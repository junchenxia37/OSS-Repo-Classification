[{"name": ".allstar", "description": null, "language": null, "license": null, "readme": "Default Allstar configuration that points to [google/allstar-config](https://github.com/google/allstar-config).\n", "release_dates": []}, {"name": ".github", "description": null, "language": null, "license": null, "readme": null, "release_dates": []}, {"name": "adanet", "description": "Fast and flexible AutoML with learning guarantees.", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AdaNet\n\n<div align=\"center\">\n  <img src=\"https://tensorflow.github.io/adanet/images/adanet_tangram_logo.png\" alt=\"adanet_tangram_logo\"><br><br>\n</div>\n\n[![Documentation Status](https://readthedocs.org/projects/adanet/badge)](https://adanet.readthedocs.io)\n[![PyPI version](https://badge.fury.io/py/adanet.svg)](https://badge.fury.io/py/adanet)\n[![Travis](https://travis-ci.org/tensorflow/adanet.svg?branch=master)](https://travis-ci.org/tensorflow/adanet)\n[![codecov](https://codecov.io/gh/tensorflow/adanet/branch/master/graph/badge.svg)](https://codecov.io/gh/tensorflow/adanet)\n[![Gitter](https://badges.gitter.im/tensorflow/adanet.svg)](https://gitter.im/tensorflow/adanet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![Downloads](https://pepy.tech/badge/adanet)](https://pepy.tech/project/adanet)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/tensorflow/adanet/blob/master/LICENSE)\n\n**AdaNet** is a lightweight TensorFlow-based framework for automatically learning high-quality models with minimal expert intervention. AdaNet builds on recent AutoML efforts to be fast and flexible while providing learning guarantees. Importantly, AdaNet provides a general framework for not only learning a neural network architecture, but also for learning to ensemble to obtain even better models.\n\nThis project is based on the _AdaNet algorithm_, presented in \u201c[AdaNet: Adaptive Structural Learning of Artificial Neural Networks](http://proceedings.mlr.press/v70/cortes17a.html)\u201d at [ICML 2017](https://icml.cc/Conferences/2017), for learning the structure of a neural network as an ensemble of subnetworks.\n\nAdaNet has the following goals:\n\n* _Ease of use_: Provide familiar APIs (e.g. Keras, Estimator) for training, evaluating, and serving models.\n* _Speed_: Scale with available compute and quickly produce high quality models.\n* _Flexibility_: Allow researchers and practitioners to extend AdaNet to novel subnetwork architectures, search spaces, and tasks.\n* _Learning guarantees_: Optimize an objective that offers theoretical learning guarantees.\n\nThe following animation shows AdaNet adaptively growing an ensemble of neural networks. At each iteration, it measures the ensemble loss for each candidate, and selects the best one to move onto the next iteration. At subsequent iterations, the blue subnetworks are frozen, and only yellow subnetworks are trained:\n\n<div align=\"center\" style=\"max-width: 450px; display: block; margin: 0 auto;\">\n  <img src=\"https://tensorflow.github.io/adanet/images/adanet_animation.gif\" alt=\"adanet_tangram_logo\"><br><br>\n</div>\n\nAdaNet was first announced on the Google AI research blog: \"[Introducing AdaNet: Fast and Flexible AutoML with Learning Guarantees](https://ai.googleblog.com/2018/10/introducing-adanet-fast-and-flexible.html)\".\n\nThis is not an official Google product.\n\n## Features\n\nAdaNet provides the following AutoML features:\n\n * Adaptive neural architecture search and ensemble learning in a single train call.\n * Regression, binary and multi-class classification, and multi-head task support.\n * A [`tf.estimator.Estimator`](https://www.tensorflow.org/guide/estimators) API for training, evaluation, prediction, and serving models.\n * The [`adanet.AutoEnsembleEstimator`](https://github.com/tensorflow/adanet/blob/master/adanet/autoensemble/estimator.py) for learning to ensemble user-defined `tf.estimator.Estimators`.\n * The ability to define subnetworks that change structure over time using [`tf.layers`](https://www.tensorflow.org/api_docs/python/tf/layers) via the [`adanet.subnetwork` API](https://github.com/tensorflow/adanet/blob/master/adanet/subnetwork/generator.py).\n * CPU, GPU, and TPU support.\n * [Distributed multi-server training](https://cloud.google.com/blog/products/gcp/easy-distributed-training-with-tensorflow-using-tfestimatortrain-and-evaluate-on-cloud-ml-engine).\n * TensorBoard integration.\n\n## Example\n\nA simple example of learning to ensemble linear and neural network models:\n\n```python\nimport adanet\nimport tensorflow as tf\n\n# Define the model head for computing loss and evaluation metrics.\nhead = MultiClassHead(n_classes=10)\n\n# Feature columns define how to process examples.\nfeature_columns = ...\n\n# Learn to ensemble linear and neural network models.\nestimator = adanet.AutoEnsembleEstimator(\n    head=head,\n    candidate_pool={\n        \"linear\":\n            tf.estimator.LinearEstimator(\n                head=head,\n                feature_columns=feature_columns,\n                optimizer=...),\n        \"dnn\":\n            tf.estimator.DNNEstimator(\n                head=head,\n                feature_columns=feature_columns,\n                optimizer=...,\n                hidden_units=[1000, 500, 100])},\n    max_iteration_steps=50)\n\nestimator.train(input_fn=train_input_fn, steps=100)\nmetrics = estimator.evaluate(input_fn=eval_input_fn)\npredictions = estimator.predict(input_fn=predict_input_fn)\n```\n\n## Getting Started\n\nTo get you started:\n\n- [API Documentation](https://adanet.readthedocs.io)\n- [Tutorials: for understanding the AdaNet algorithm and learning to use this package](./adanet/examples/tutorials)\n\n## Requirements\n\nRequires [Python](https://www.python.org/) 3.6 or above.\n\n`adanet` is built on TensorFlow 2.1. It depends on bug fixes and enhancements not present in TensorFlow releases prior to 2.1. You must install or upgrade your TensorFlow package to at least 2.1:\n\n```shell\n$ pip install \"tensorflow==2.1\"\n```\n\n## Installing with Pip\n\nYou can use the [pip package manager](https://pip.pypa.io/en/stable/installing/) to install the official `adanet` package from [PyPi](https://pypi.org/project/adanet/):\n\n```shell\n$ pip install adanet\n```\n\n## Installing from Source\n\nTo install from source first you'll need to install `bazel` following their [installation instructions](https://docs.bazel.build/versions/master/install.html).\n\nNext clone the `adanet` repository:\n\n```shell\n$ git clone https://github.com/tensorflow/adanet\n$ cd adanet\n```\n\nFrom the `adanet` root directory run the tests:\n\n```shell\n$ bazel build -c opt //...\n$ python3 -m nose\n```\n\nOnce you have verified that the tests have passed, install `adanet` from source as a [ pip package ](./adanet/pip_package/PIP.md).\n\nYou are now ready to experiment with `adanet`.\n\n```python\nimport adanet\n```\n\n## Citing this Work\n\nIf you use this AdaNet library for academic research, you are encouraged to cite the following paper from the [ICML 2019 AutoML Workshop](https://arxiv.org/abs/1905.00080):\n\n    @misc{weill2019adanet,\n        title={AdaNet: A Scalable and Flexible Framework for Automatically Learning Ensembles},\n        author={Charles Weill and Javier Gonzalvo and Vitaly Kuznetsov and Scott Yang and Scott Yak and Hanna Mazzawi and Eugen Hotaj and Ghassen Jerfel and Vladimir Macko and Ben Adlam and Mehryar Mohri and Corinna Cortes},\n        year={2019},\n        eprint={1905.00080},\n        archivePrefix={arXiv},\n        primaryClass={cs.LG}\n    }\n\n## License\n\nAdaNet is released under the [Apache License 2.0](LICENSE).\n", "release_dates": ["2020-07-09T20:53:28Z", "2019-10-02T17:31:01Z", "2019-06-26T20:41:29Z", "2019-04-29T23:01:09Z", "2019-03-29T15:47:55Z", "2019-03-28T02:22:10Z", "2018-12-17T23:18:21Z", "2018-11-30T00:05:39Z", "2018-11-07T19:32:56Z", "2018-11-02T15:12:06Z", "2018-10-23T00:25:11Z"]}, {"name": "addons", "description": "Useful extra functionality for TensorFlow 2.x maintained by SIG-addons", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "\n<h2 align=\"center\">\n :warning: :warning: :warning:\n</h2>\n\n<h4 align=\"center\">\nTensorFlow Addons (TFA) has ended development and introduction of new features.\n\nTFA has entered a minimal maintenance and release mode until a planned end of life \nin May 2024. Please modify downstream libraries to take dependencies from other \nrepositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP)\n\nFor more information see: [https://github.com/tensorflow/addons/issues/2807](https://github.com/tensorflow/addons/issues/2807)\n</h4>\n\n-----------------\n\n<div align=\"center\">\n  <img src=\"https://github.com/tensorflow/community/blob/master/sigs/logos/SIGAddons.png\" width=\"60%\"><br><br>\n</div>\n\n-----------------\n\n[![PyPI Status Badge](https://badge.fury.io/py/tensorflow-addons.svg)](https://pypi.org/project/tensorflow-addons/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/tensorflow-addons)](https://pypi.org/project/tensorflow-addons/)\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/addons/api_docs/python/tfa)\n[![Gitter chat](https://img.shields.io/badge/chat-on%20gitter-46bc99.svg)](https://gitter.im/tensorflow/sig-addons)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n### Continuous Build Status\n\n| Build      | Status |\n| ---             | ---    |\n| **Ubuntu/macOS**   | [![Status](https://github.com/tensorflow/addons/actions/workflows/release.yml/badge.svg)](https://github.com/tensorflow/addons/actions?query=workflow%3Aaddons-release) |\n| **Ubuntu GPU custom ops**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/addons/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/addons/ubuntu-gpu-py3.html) |\n\n**TensorFlow Addons** is a repository of contributions that conform to\nwell-established API patterns, but implement new functionality\nnot available in core TensorFlow. TensorFlow natively supports\na large number of operators, layers, metrics, losses, and optimizers.\nHowever, in a fast moving field like ML, there are many interesting new\ndevelopments that cannot be integrated into core TensorFlow\n(because their broad applicability is not yet clear, or it is mostly\n used by a smaller subset of the community).\n\n## Addons Subpackages\n\n* [tfa.activations](https://www.tensorflow.org/addons/api_docs/python/tfa/activations) \n* [tfa.callbacks](https://www.tensorflow.org/addons/api_docs/python/tfa/callbacks) \n* [tfa.image](https://www.tensorflow.org/addons/api_docs/python/tfa/image) \n* [tfa.layers](https://www.tensorflow.org/addons/api_docs/python/tfa/layers)\n* [tfa.losses](https://www.tensorflow.org/addons/api_docs/python/tfa/losses)\n* [tfa.metrics](https://www.tensorflow.org/addons/api_docs/python/tfa/metrics) \n* [tfa.optimizers](https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers) \n* [tfa.rnn](https://www.tensorflow.org/addons/api_docs/python/tfa/rnn) \n* [tfa.seq2seq](https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq) \n* [tfa.text](https://www.tensorflow.org/addons/api_docs/python/tfa/text) \n\n## Maintainership\nThe maintainers of TensorFlow Addons can be found in the [CODEOWNERS](.github/CODEOWNERS) file of the repo. This file \nis parsed and pull requests will automatically tag the owners using a bot. If you would\nlike to maintain something, please feel free to submit a PR. We encourage multiple \nowners for all submodules.\n\n## Installation\n#### Stable Builds\nTensorFlow Addons is available on PyPI for Linux & macOS (Windows support was dropped \ndue to [inconsistent TF2.15 whl packaging](https://github.com/tensorflow/tensorflow/issues/61830)). To install the latest version, \nrun the following:\n```\npip install tensorflow-addons\n```\n\nTo ensure you have a version of TensorFlow that is compatible with TensorFlow Addons, \nyou can specify the `tensorflow` extra requirement during install:\n\n```\npip install tensorflow-addons[tensorflow]\n```\n\nSimilar extras exist for the `tensorflow-gpu` and `tensorflow-cpu` packages.\n \n\nTo use TensorFlow Addons:\n\n```python\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n```\n\n### Python Op Compatility\nTensorFlow Addons is actively working towards forward compatibility with TensorFlow 2.x. \nHowever, there are still a few private API uses within the repository so at the moment \nwe can only guarantee compatibility with the TensorFlow versions which it was tested against. \nWarnings will be emitted when importing `tensorflow_addons` if your TensorFlow version does not match \nwhat it was tested against.\n\n#### Python Op Compatibility Matrix\n| TensorFlow Addons | TensorFlow | Python  |\n|:----------------------- |:---|:---------- |\n| tfa-nightly | 2.12, 2.13, 2.14 |3.9, 3.10, 3.11 |\n| tensorflow-addons-0.22.0 | 2.12, 2.13, 2.14 |3.9, 3.10, 3.11 |\n| tensorflow-addons-0.21.0 | 2.11, 2.12, 2.13 |3.8, 3.9, 3.10, 3.11 |\n| tensorflow-addons-0.20.0 | 2.10, 2.11, 2.12 |3.8, 3.9, 3.10, 3.11 |\n| tensorflow-addons-0.19.0 | 2.9, 2.10, 2.11 |3.7, 3.8, 3.9, 3.10 |\n| tensorflow-addons-0.18.0 | 2.8, 2.9, 2.10 |3.7, 3.8, 3.9, 3.10 |\n| tensorflow-addons-0.17.1 | 2.7, 2.8, 2.9 |3.7, 3.8, 3.9, 3.10 |\n| tensorflow-addons-0.16.1 | 2.6, 2.7, 2.8 |3.7, 3.8, 3.9, 3.10 |\n| tensorflow-addons-0.15.0 | 2.5, 2.6, 2.7 |3.7, 3.8, 3.9 |\n| tensorflow-addons-0.14.0 | 2.4, 2.5, 2.6 |3.6, 3.7, 3.8, 3.9 |\n| tensorflow-addons-0.13.0 | 2.3, 2.4, 2.5 |3.6, 3.7, 3.8, 3.9 |\n| tensorflow-addons-0.12.1 | 2.3, 2.4 |3.6, 3.7, 3.8 |\n| tensorflow-addons-0.11.2 | 2.2, 2.3 |3.5, 3.6, 3.7, 3.8 |\n| tensorflow-addons-0.10.0 | 2.2 |3.5, 3.6, 3.7, 3.8 |\n| tensorflow-addons-0.9.1 | 2.1, 2.2 |3.5, 3.6, 3.7 |\n| tensorflow-addons-0.8.3 | 2.1 |3.5, 3.6, 3.7 |\n| tensorflow-addons-0.7.1 | 2.1 | 2.7, 3.5, 3.6, 3.7 | \n| tensorflow-addons-0.6.0 | 2.0 | 2.7, 3.5, 3.6, 3.7 |\n\n### C++ Custom Op Compatibility\nTensorFlow C++ APIs are not stable and thus we can only guarantee compatibility with the \nversion TensorFlow Addons was built against. It is possible custom ops will work with multiple \nversions of TensorFlow, but there is also a chance for segmentation faults or other problematic crashes.\nWarnings will be emitted when loading a custom op if your TensorFlow version does not match \nwhat it was built against.\n\nAdditionally, custom ops registration does not have a stable ABI interface so it is \nrequired that users have a compatible installation of TensorFlow even if the versions \nmatch what we had built against. A simplification of this is that **TensorFlow Addons \ncustom ops will work with `pip`-installed TensorFlow** but will have issues when TensorFlow \nis compiled differently. A typical example of this would be `conda`-installed TensorFlow.\n[RFC #133](https://github.com/tensorflow/community/pull/133) aims to fix this.\n\n\n#### C++ Custom Op Compatibility Matrix\n| TensorFlow Addons | TensorFlow | Compiler  | cuDNN | CUDA | \n|:----------------------- |:---- |:---------|:---------|:---------|\n| tfa-nightly | 2.14  | GCC 9.3.1 | 8.6 | 11.8 |\n| tensorflow-addons-0.22.0 | 2.14  | GCC 9.3.1 | 8.6 | 11.8 |\n| tensorflow-addons-0.21.0 | 2.13  | GCC 9.3.1 | 8.6 | 11.8 |\n| tensorflow-addons-0.20.0 | 2.12  | GCC 9.3.1 | 8.6 | 11.8 |\n| tensorflow-addons-0.19.0 | 2.11  | GCC 9.3.1 | 8.1 | 11.2 |\n| tensorflow-addons-0.18.0 | 2.10  | GCC 9.3.1 | 8.1 | 11.2 |\n| tensorflow-addons-0.17.1 | 2.9  | GCC 9.3.1 | 8.1 | 11.2 |\n| tensorflow-addons-0.16.1 | 2.8  | GCC 7.3.1 | 8.1 | 11.2 |\n| tensorflow-addons-0.15.0 | 2.7  | GCC 7.3.1 | 8.1 | 11.2 |\n| tensorflow-addons-0.14.0 | 2.6  | GCC 7.3.1 | 8.1 | 11.2 |\n| tensorflow-addons-0.13.0 | 2.5  | GCC 7.3.1 | 8.1 | 11.2 |\n| tensorflow-addons-0.12.1 | 2.4  | GCC 7.3.1 | 8.0 | 11.0 |\n| tensorflow-addons-0.11.2 | 2.3  | GCC 7.3.1 | 7.6 | 10.1 |\n| tensorflow-addons-0.10.0 | 2.2  | GCC 7.3.1 | 7.6 | 10.1 |\n| tensorflow-addons-0.9.1 | 2.1  | GCC 7.3.1 | 7.6 | 10.1 |\n| tensorflow-addons-0.8.3 | 2.1  | GCC 7.3.1 | 7.6 | 10.1 |\n| tensorflow-addons-0.7.1 | 2.1  | GCC 7.3.1 | 7.6 | 10.1 |\n| tensorflow-addons-0.6.0 | 2.0  | GCC 7.3.1 | 7.4 | 10.0 |\n\n\n#### Nightly Builds\nThere are also nightly builds of TensorFlow Addons under the pip package\n`tfa-nightly`, which is built against **the latest stable version of TensorFlow**. Nightly builds\ninclude newer features, but may be less stable than the versioned releases. Contrary to \nwhat the name implies, nightly builds are not released every night, but at every commit \nof the master branch. `0.9.0.dev20200306094440` means that the commit time was \n2020/03/06 at 09:44:40 Coordinated Universal Time.\n\n```\npip install tfa-nightly\n```\n\n#### Installing from Source\nYou can also install from source. This requires the [Bazel](\nhttps://bazel.build/) build system (version >= 1.0.0).\n\n##### CPU Custom Ops\n```\ngit clone https://github.com/tensorflow/addons.git\ncd addons\n\n# This script links project with TensorFlow dependency\npython3 ./configure.py\n\nbazel build build_pip_pkg\nbazel-bin/build_pip_pkg artifacts\n\npip install artifacts/tensorflow_addons-*.whl\n```\n\n##### GPU and CPU Custom Ops\n```\ngit clone https://github.com/tensorflow/addons.git\ncd addons\n\nexport TF_NEED_CUDA=\"1\"\n\n# Set these if the below defaults are different on your system\nexport TF_CUDA_VERSION=\"12\"\nexport TF_CUDNN_VERSION=\"8\"\nexport CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\nexport CUDNN_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu\"\n\n# This script links project with TensorFlow dependency\npython3 ./configure.py\n\nbazel build build_pip_pkg\nbazel-bin/build_pip_pkg artifacts\n\npip install artifacts/tensorflow_addons-*.whl\n```\n\n## Tutorials\nSee [`docs/tutorials/`](docs/tutorials/)\nfor end-to-end examples of various addons.\n\n## Core Concepts\n\n#### Standardized API within Subpackages\nUser experience and project maintainability are core concepts in\nTensorFlow Addons. In order to achieve these we require that our additions\nconform to established API patterns seen in core TensorFlow.\n\n#### GPU and CPU Custom Ops\nTensorFlow Addons supports precompiled custom ops for CPU and GPU. However, \nGPU custom ops currently only work on Linux distributions. For this reason Windows and macOS \nwill fallback to pure TensorFlow Python implementations whenever possible.\n\nThe order of priority on macOS/Windows is:\n1) Pure TensorFlow + Python implementation (works on CPU and GPU)\n2) C++ implementation for CPU\n\nThe order of priority on Linux is:\n1) CUDA implementation\n2) C++ implementation\n3) Pure TensorFlow + Python implementation (works on CPU and GPU)\n\nIf you want to change the default priority, \"C++ and CUDA\" VS \"pure TensorFlow Python\",\nyou can set the environment variable `TF_ADDONS_PY_OPS=1` from the command line or\nrun `tfa.options.disable_custom_kernel()` in your code.\n\nFor example, if you are on Linux and you have compatibility problems with the compiled ops,\nyou can give priority to the Python implementations:\n\nFrom the command line:\n```bash\nexport TF_ADDONS_PY_OPS=1\n```\n\nor in your code:\n\n```python\nimport tensorflow_addons as tfa\ntfa.options.disable_custom_kernel()\n```\n\nThis variable defaults to `True` on Windows and macOS, and `False` on Linux.\n\n#### Proxy Maintainership\nTensorFlow Addons has been designed to compartmentalize submodules so \nthat they can be maintained by community users who have expertise, and a vested \ninterest in that component. We heavily encourage users to submit sign up to maintain a \nsubmodule by submitting your username to the [CODEOWNERS](.github/CODEOWNERS) file.\n\nFull write access will only be granted after substantial contribution \nhas been made in order to limit the number of users with write permission. \nContributions can come in the form of issue closings, bug fixes, documentation, \nnew code, or optimizing existing code. Submodule maintainership can be granted \nwith a lower barrier for entry as this will not include write permissions to \nthe repo.\n\nFor more information see [the RFC](https://github.com/tensorflow/community/blob/master/rfcs/20190308-addons-proxy-maintainership.md) \non this topic.\n\n#### Periodic Evaluation of Subpackages\nGiven the nature of this repository, submodules may become less \nand less useful to the community as time goes on. In order to keep the \nrepository sustainable, we'll be performing bi-annual reviews of our code to \nensure everything still belongs within the repo. Contributing factors to this \nreview will be:\n\n1. Number of active maintainers\n2. Amount of OSS use\n3. Amount of issues or bugs attributed to the code\n4. If a better solution is now available\n\nFunctionality within TensorFlow Addons can be categorized into three groups:\n\n* **Suggested**: well-maintained API; use is encouraged.\n* **Discouraged**: a better alternative is available; the API is kept for \nhistoric reasons; or the API requires maintenance and is the waiting period \nto be deprecated.\n* **Deprecated**: use at your own risk; subject to be deleted.\n\nThe status change between these three groups is: \nSuggested <-> Discouraged -> Deprecated.\n\nThe period between an API being marked as deprecated and being deleted will be \n90 days. The rationale being:\n\n1. In the event that TensorFlow Addons releases monthly, there will be 2-3 \nreleases before an API is deleted. The release notes could give user enough \nwarning.\n\n2. 90 days gives maintainers ample time to fix their code.\n\n\n## Contributing\nTensorFlow Addons is a community-led open source project (only a few maintainers work for Google!). \nAs such, the project depends on public contributions, bug fixes, and documentation. \nThis project adheres to [TensorFlow's code of conduct](CODE_OF_CONDUCT.md).\nBy participating, you are expected to uphold this code.\n\nDo you want to contribute but are not sure of what? Here are a few suggestions:\n1. Add a new tutorial. Located in [`docs/tutorials/`](docs/tutorials),\n  these are a great way to familiarize yourself and others with TensorFlow Addons. See\n  [the guidelines](docs/tutorials/README.md) for more information on how to add\n  examples.\n2. Improve the docstrings. The docstrings are fetched and then displayed in the documentation.\n  Do a change and hundreds of developers will see it and benefit from it. Maintainers are often focused \n  on making APIs, fixing bugs and other code related changes. The documentation will never \n  be loved enough!\n3. Solve an [existing issue](https://github.com/tensorflow/addons/issues).\n  These range from low-level software bugs to higher-level design problems.\n  Check out the label [help wanted](https://github.com/tensorflow/addons/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22). If you're a new contributor, the label [good first issue](https://github.com/tensorflow/addons/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) can be a good place to start.\n4. Review a pull request. So you're not a software engineer but you know a lot\n  about a certain field a research? That's awesome and we need your help! Many people \n  are submitting pull requests to add layers/optimizers/functions taken from recent\n  papers. Since TensorFlow Addons maintainers are not specialized in everything,\n  you can imagine how hard it is to review. It takes very long to read the paper,\n  understand it and check the math in the pull request. If you're specialized, look at \n  the [list of pull requests](https://github.com/tensorflow/addons/pulls). \n  If there is something from a paper you know, please comment on the pull request to\n  check the math is ok. If you see that everything is good, say it! It will help \n  the maintainers to sleep better at night knowing that he/she wasn't the only\n  person to approve the pull request.\n5. You have an opinion and want to share it? The docs are not very helpful for \n  a function or a class? You tried to open a pull request but you didn't manage to \n  install or test anything and you think it's too complicated? You made a pull request\n  but you didn't find the process good enough and it made no sense to you? Please \n  say it! We want feedback. Maintainers are too much the head into the code \n  to understand what it's like for someone new to open source to come to this project. \n  If you don't understand something, be aware there are no people who are \n  bad at understanding, there are just bad tutorials and bad guides.\n\nPlease see [contribution guidelines](CONTRIBUTING.md) to get started (and remember,\nif you don't understand something, open an issue, or even make a pull request to \nimprove the guide!).\n\n## Community\n* [Public Mailing List](https://groups.google.com/a/tensorflow.org/forum/#!forum/addons)\n* [SIG Monthly Meeting Notes](https://docs.google.com/document/d/1kxg5xIHWLY7EMdOJCdSGgaPu27a9YKpupUz2VTXqTJg)\n    * Join our mailing list and receive calendar invites to the meeting\n\n## License\n[Apache License 2.0](LICENSE)\n\n", "release_dates": ["2023-11-28T01:15:27Z", "2023-10-17T04:25:20Z", "2023-07-11T23:22:52Z", "2023-04-07T16:01:10Z", "2022-12-15T09:29:45Z", "2022-09-21T00:54:14Z", "2022-06-14T01:12:29Z", "2022-05-21T14:35:04Z", "2022-02-15T18:08:05Z", "2022-02-15T17:02:00Z", "2021-11-10T21:09:55Z", "2021-08-19T12:57:05Z", "2021-05-15T16:45:30Z", "2021-01-30T13:40:21Z", "2020-12-23T19:21:28Z", "2020-08-27T02:52:02Z", "2020-08-07T02:28:03Z", "2020-08-06T01:20:57Z", "2020-05-15T00:34:25Z", "2020-04-10T18:23:01Z", "2020-04-09T13:33:33Z", "2020-04-03T16:01:56Z", "2020-04-03T14:42:21Z", "2020-03-04T15:48:54Z", "2020-02-12T11:37:01Z", "2020-02-07T11:22:52Z", "2020-01-26T23:47:20Z", "2020-01-10T16:16:23Z", "2019-10-04T22:07:29Z", "2019-10-01T01:35:19Z"]}, {"name": "agents", "description": "TF-Agents: A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TF-Agents: A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning.\n\n[![PyPI tf-agents](https://badge.fury.io/py/tf-agents.svg)](https://badge.fury.io/py/tf-agents)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/tf-agents)\n\n[TF-Agents](https://github.com/tensorflow/agents) makes implementing, deploying,\nand testing new Bandits and RL algorithms easier. It provides well tested and\nmodular components that can be modified and extended. It enables fast code\niteration, with good test integration and benchmarking.\n\nTo get started, we recommend checking out one of our Colab tutorials. If you\nneed an intro to RL (or a quick recap),\n[start here](docs/tutorials/0_intro_rl.ipynb). Otherwise, check out our\n[DQN tutorial](docs/tutorials/1_dqn_tutorial.ipynb) to get an agent up and\nrunning in the Cartpole environment. API documentation for the current stable\nrelease is on\n[tensorflow.org](https://www.tensorflow.org/agents/api_docs/python/tf_agents).\n\nTF-Agents is under active development and interfaces may change at any time.\nFeedback and comments are welcome.\n\n## Table of contents\n\n<a href='#Agents'>Agents</a><br>\n<a href='#Tutorials'>Tutorials</a><br>\n<a href='#Multi-Armed Bandits'>Multi-Armed Bandits</a><br>\n<a href='#Examples'>Examples</a><br>\n<a href='#Installation'>Installation</a><br>\n<a href='#Contributing'>Contributing</a><br>\n<a href='#Releases'>Releases</a><br>\n<a href='#Principles'>Principles</a><br>\n<a href='#Contributors'>Contributors</a><br>\n<a href='#Citation'>Citation</a><br>\n<a href='#Disclaimer'>Disclaimer</a><br>\n\n<a id='Agents'></a>\n\n## Agents\n\nIn TF-Agents, the core elements of RL algorithms are implemented as `Agents`. An\nagent encompasses two main responsibilities: defining a Policy to interact with\nthe Environment, and how to learn/train that Policy from collected experience.\n\nCurrently the following algorithms are available under TF-Agents:\n\n*   [DQN: __Human level control through deep reinforcement learning__ Mnih et\n    al., 2015](https://deepmind.com/research/dqn/)\n*   [DDQN: __Deep Reinforcement Learning with Double Q-learning__ Hasselt et\n    al., 2015](https://arxiv.org/abs/1509.06461)\n*   [DDPG: __Continuous control with deep reinforcement learning__ Lillicrap et\n    al., 2015](https://arxiv.org/abs/1509.02971)\n*   [TD3: __Addressing Function Approximation Error in Actor-Critic Methods__\n    Fujimoto et al., 2018](https://arxiv.org/abs/1802.09477)\n*   [REINFORCE: __Simple Statistical Gradient-Following Algorithms for\n    Connectionist Reinforcement Learning__ Williams,\n    1992](https://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n*   [PPO: __Proximal Policy Optimization Algorithms__ Schulman et al., 2017](https://arxiv.org/abs/1707.06347)\n*   [SAC: __Soft Actor Critic__ Haarnoja et al., 2018](https://arxiv.org/abs/1812.05905)\n\n<a id='Tutorials'></a>\n\n## Tutorials\n\nSee [`docs/tutorials/`](docs/tutorials) for tutorials on the major components\nprovided.\n\n<a id='Multi-Armed Bandits'></a>\n\n## Multi-Armed Bandits\n\nThe TF-Agents library contains a comprehensive Multi-Armed Bandits suite,\nincluding Bandits environments and agents. RL agents can also be used on Bandit\nenvironments. There is a tutorial in\n[`bandits_tutorial.ipynb`](https://github.com/tensorflow/agents/tree/master/docs/tutorials/bandits_tutorial.ipynb).\nand ready-to-run examples in\n[`tf_agents/bandits/agents/examples/v2`](https://github.com/tensorflow/agents/tree/master/tf_agents/bandits/agents/examples/v2).\n\n<a id='Examples'></a>\n\n## Examples\n\nEnd-to-end examples training agents can be found under each agent directory.\ne.g.:\n\n*   DQN:\n    [`tf_agents/agents/dqn/examples/v2/train_eval.py`](https://github.com/tensorflow/agents/tree/master/tf_agents/agents/dqn/examples/v2/train_eval.py)\n\n<a id='Installation'></a>\n\n## Installation\n\nTF-Agents publishes nightly and stable builds. For a list of releases read the\n<a href='#Releases'>Releases</a> section. The commands below cover installing\nTF-Agents stable and nightly from [pypi.org](https://pypi.org) as well as from a\nGitHub clone.\n\n> :warning: If using Reverb (replay buffer), which is very common,\nTF-Agents will only work with Linux.\n\n> Note: Python 3.11 requires pygame 2.1.3+.\n\n### Stable\n\nRun the commands below to install the most recent stable release. API\ndocumentation for the release is on\n[tensorflow.org](https://www.tensorflow.org/agents/api_docs/python/tf_agents).\n\n```shell\n$ pip install --user tf-agents[reverb]\n\n# Use keras-2\n$ export TF_USE_LEGACY_KERAS=1\n# Use this tag get the matching examples and colabs.\n$ git clone https://github.com/tensorflow/agents.git\n$ cd agents\n$ git checkout v0.18.0\n```\n\nIf you want to install TF-Agents with versions of Tensorflow or\n[Reverb](https://github.com/deepmind/reverb) that are flagged as not compatible\nby the pip dependency check, use the following pattern below at your own risk.\n\n```shell\n$ pip install --user tensorflow\n$ pip install --user tf-keras\n$ pip install --user dm-reverb\n$ pip install --user tf-agents\n```\n\nIf you want to use TF-Agents with TensorFlow 1.15 or 2.0, install version 0.3.0:\n\n```shell\n# Newer versions of tensorflow-probability require newer versions of TensorFlow.\n$ pip install tensorflow-probability==0.8.0\n$ pip install tf-agents==0.3.0\n```\n\n### Nightly\n\nNightly builds include newer features, but may be less stable than the versioned\nreleases. The nightly build is pushed as `tf-agents-nightly`. We suggest\ninstalling nightly versions of TensorFlow (`tf-nightly`) and TensorFlow\nProbability (`tfp-nightly`) as those are the versions TF-Agents nightly are\ntested against.\n\nTo install the nightly build version, run the following:\n\n```shell\n# Use keras-2\n$ export TF_USE_LEGACY_KERAS=1\n\n# `--force-reinstall helps guarantee the right versions.\n$ pip install --user --force-reinstall tf-nightly\n$ pip install --user --force-reinstall tf-keras-nightly\n$ pip install --user --force-reinstall tfp-nightly\n$ pip install --user --force-reinstall dm-reverb-nightly\n\n# Installing with the `--upgrade` flag ensures you'll get the latest version.\n$ pip install --user --upgrade tf-agents-nightly\n```\n\n### From GitHub\n\nAfter cloning the repository, the dependencies can be installed by running `pip\ninstall -e .[tests]`. TensorFlow needs to be installed independently: `pip\ninstall --user tf-nightly`.\n\n<a id='Contributing'></a>\n\n## Contributing\n\nWe're eager to collaborate with you! See [`CONTRIBUTING.md`](CONTRIBUTING.md)\nfor a guide on how to contribute. This project adheres to TensorFlow's\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.\n\n<a id='Releases'></a>\n\n## Releases\n\nTF Agents has stable and nightly releases. The nightly releases are often fine\nbut can have issues due to upstream libraries being in flux. The table below\nlists the version(s) of TensorFlow that align with each TF Agents' release.\nRelease versions of interest:\n\n  * 0.19.0 supports tensorflow-2.15.0.\n  * 0.18.0 dropped Python 3.8 support.\n  * 0.16.0 is the first version to support Python 3.11.\n  * 0.15.0 is the last release compatible with Python 3.7.\n  * If using numpy < 1.19, then use TF-Agents 0.15.0 or earlier.\n  * 0.9.0 is the last release compatible with Python 3.6.\n  * 0.3.0 is the last release compatible with Python 2.x.\n\nRelease | Branch / Tag                                               | TensorFlow Version | dm-reverb Version\n------- | ---------------------------------------------------------- | ------------------ | -----------\nNightly | [master](https://github.com/tensorflow/agents)             | tf-nightly         | dm-reverb-nightly\n0.19.0  | [v0.19.0](https://github.com/tensorflow/agents/tree/v0.19.0) | 2.15.0           | 0.14.0\n0.18.0  | [v0.18.0](https://github.com/tensorflow/agents/tree/v0.18.0) | 2.14.0           | 0.13.0\n0.17.0  | [v0.17.0](https://github.com/tensorflow/agents/tree/v0.17.0) | 2.13.0           | 0.12.0\n0.16.0  | [v0.16.0](https://github.com/tensorflow/agents/tree/v0.16.0) | 2.12.0           | 0.11.0\n0.15.0  | [v0.15.0](https://github.com/tensorflow/agents/tree/v0.15.0) | 2.11.0           | 0.10.0\n0.14.0  | [v0.14.0](https://github.com/tensorflow/agents/tree/v0.14.0) | 2.10.0           | 0.9.0\n0.13.0  | [v0.13.0](https://github.com/tensorflow/agents/tree/v0.13.0) | 2.9.0            | 0.8.0\n0.12.0  | [v0.12.0](https://github.com/tensorflow/agents/tree/v0.12.0) | 2.8.0            | 0.7.0\n0.11.0  | [v0.11.0](https://github.com/tensorflow/agents/tree/v0.11.0) | 2.7.0            | 0.6.0\n0.10.0  | [v0.10.0](https://github.com/tensorflow/agents/tree/v0.10.0) | 2.6.0            |\n0.9.0   | [v0.9.0](https://github.com/tensorflow/agents/tree/v0.9.0) | 2.6.0              |\n0.8.0   | [v0.8.0](https://github.com/tensorflow/agents/tree/v0.8.0) | 2.5.0              |\n0.7.1   | [v0.7.1](https://github.com/tensorflow/agents/tree/v0.7.1) | 2.4.0              |\n0.6.0   | [v0.6.0](https://github.com/tensorflow/agents/tree/v0.6.0) | 2.3.0              |\n0.5.0   | [v0.5.0](https://github.com/tensorflow/agents/tree/v0.5.0) | 2.2.0              |\n0.4.0   | [v0.4.0](https://github.com/tensorflow/agents/tree/v0.4.0) | 2.1.0              |\n0.3.0   | [v0.3.0](https://github.com/tensorflow/agents/tree/v0.3.0) | 1.15.0 and 2.0.0.  |\n\n<a id='Principles'></a>\n\n## Principles\n\nThis project adheres to [Google's AI principles](PRINCIPLES.md). By\nparticipating, using or contributing to this project you are expected to adhere\nto these principles.\n\n\n<a id='Contributors'></a>\n\n## Contributors\n\n\nWe would like to recognize the following individuals for their code\ncontributions, discussions, and other work to make the TF-Agents library.\n\n* James Davidson\n* Ethan Holly\n* Toby Boyd\n* Summer Yue\n* Robert Ormandi\n* Kuang-Huei Lee\n* Alexa Greenberg\n* Amir Yazdanbakhsh\n* Yao Lu\n* Gaurav Jain\n* Christof Angermueller\n* Mark Daoust\n* Adam Wood\n\n\n<a id='Citation'></a>\n\n## Citation\n\nIf you use this code, please cite it as:\n\n```\n@misc{TFAgents,\n  title = {{TF-Agents}: A library for Reinforcement Learning in TensorFlow},\n  author = {Sergio Guadarrama and Anoop Korattikara and Oscar Ramirez and\n     Pablo Castro and Ethan Holly and Sam Fishman and Ke Wang and\n     Ekaterina Gonina and Neal Wu and Efi Kokiopoulou and Luciano Sbaiz and\n     Jamie Smith and G\u00e1bor Bart\u00f3k and Jesse Berent and Chris Harris and\n     Vincent Vanhoucke and Eugene Brevdo},\n  howpublished = {\\url{https://github.com/tensorflow/agents}},\n  url = \"https://github.com/tensorflow/agents\",\n  year = 2018,\n  note = \"[Online; accessed 25-June-2019]\"\n}\n```\n\n<a id='Disclaimer'></a>\n\n## Disclaimer\n\nThis is not an official Google product.\n", "release_dates": []}, {"name": "autograph", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AutoGraph\n\nThis repository contains tests and example code for [TensorFlow AutoGraph](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph). For more information, see:\n\n * [tf.function and AutoGraph guide](https://www.tensorflow.org/beta/guide/autograph)\n * [AutoGraph reference documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/index.md)\n", "release_dates": []}, {"name": "benchmarks", "description": " A benchmark framework for Tensorflow", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow benchmarks\nThis repository contains various TensorFlow benchmarks. Currently, it consists of two projects:\n\n\n1. [PerfZero](https://github.com/tensorflow/benchmarks/tree/master/perfzero): A benchmark framework for TensorFlow.\n\n2. [scripts/tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks) (no longer maintained): The TensorFlow CNN benchmarks contain TensorFlow 1 benchmarks for several convolutional neural networks.\n\nIf you want to run TensorFlow models and measure their performance, also consider the [TensorFlow Official Models](https://github.com/tensorflow/models/tree/master/official)\n\n", "release_dates": []}, {"name": "build", "description": "Build-related tools for TensorFlow", "language": "Shell", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<div align=\"center\">\n  <img src=\"https://github.com/tensorflow/community/blob/master/sigs/logos/SIGBuild.png\" width=\"60%\"><br><br>\n</div>\n\n-----------------\n\n[![Gitter chat](https://img.shields.io/badge/chat-on%20gitter-46bc99.svg)](https://gitter.im/tensorflow/sig-build)\n[![SIG Build Forum](https://img.shields.io/badge/discuss-on%20tensorflow.org-orange)](https://groups.google.com/a/tensorflow.org/g/build)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/tensorflow/build/badge)](https://api.securityscorecards.dev/projects/github.com/tensorflow/build)\n\n**TensorFlow SIG Build** is a community group dedicated to the TensorFlow build\nprocess. This repository is a showcase of resources, guides, tools, and builds\ncontributed by the community, for the community.\n\n## Group\n\n### Contributing\n\nSIG Build is a community-led open source project. As such, the project\ndepends on public contributions, bug-fixes, and documentation. Please\nsee [contribution guidelines](CONTRIBUTING.md) for a guide on how to\ncontribute. This project adheres to [TensorFlow's code of conduct](CODE_OF_CONDUCT.md).\nBy participating, you are expected to uphold this code.\n\n### Community\n\n* [Public Mailing List](https://groups.google.com/a/tensorflow.org/forum/#!forum/build)\n* [SIG Monthly Meeting Notes](https://docs.google.com/document/d/10_3IQ5aF-88ADJNLF0WOpb09bZ15x-sBnRSnDHNCNr8/edit)\n    * Join our mailing list and receive calendar invites to the meeting.\n\n### License\n[Apache License 2.0](LICENSE)\n\n## Project Showcase\n\nWant to add your own project to this list? It's easy: check out\n[CONTRIBUTING.md](CONTRIBUTING.md).\n\n### Docker\n\n* [**TF SIG Build Dockerfiles**](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/tf_sig_build_dockerfiles):\n  Standard Dockerfiles for TensorFlow builds, used internally at Google\n* [**TensorFlow Runtime Dockerfiles**](tensorflow_runtime_dockerfiles):\n  Simple Dockerfiles for running TensorFlow, with Jupyter variants.\n* [**Manylinux 2014 Docker Images**](manylinux2014_docker_images):\n  `manylinux2014` build environment for TensorFlow packages.\n* [**Distroless Dockerfiles**](https://github.com/uvarc/rivanna-docker):\n  Distroless ([info](https://github.com/GoogleContainerTools)) TensorFlow\n  images, which are smaller than TensorFlow's official images.\n* [**DevInfra Windows RBE**](devinfra_windows_rbe):\n  Static snapshot of TF DevInfra's Windows Remote Build Execution images\n\n### Language Bindings\n\n* [**Golang Install Guide**](golang_install_guide): Documentation for installing\n  the Go bindings.\n\n### Platforms\n\n* [**ppc64le Builds**](ppc64le_builds): Dockerfiles and wheel build scripts for\n  building TF on ppc64le.\n* [**Raspberry Pi Builds**](raspberry_pi_builds): TensorFlow's old official docs\n  for building on Raspberry Pi. Needs an owner.\n* [**WSL2 GPU Guide**](wsl2_gpu_guide): Instructions for enabling GPU with Tensorflow\n  on a WSL2 virtual machine.\n\n### WIP / Other\n\n* [**Directory Template**](directory_template): Example short description.\n* [**TF OSS Dashboard**](tf_oss_dashboard): Dashboard for all continuous\n  statuses on TF GitHub Commits.\n* [**Tekton CI**](tekton): perfinion's experimental directory for using Tekton \n  CI with TensorFlow\n\n## Community Supported TensorFlow Builds\n\nAmazing members of the TensorFlow community build, test, and package TensorFlow\non more platforms than are supported by the official TensorFlow team. Please\nnote that as *community* builds they are not supported by the TensorFlow team.\n\nWant to add your own community builds to this list? It's easy: check out\n[CONTRIBUTING.md](CONTRIBUTING.md).\n\n### TensorFlow Builds\n\nOwner | Build Type | Status | Artifacts\n---: | --- | :---: | :---\nAMD | **Linux AMD ROCm GPU** Nightly         | [![Build Status](http://ml-ci.amd.com:21096/job/tensorflow/job/nightly-rocmfork-develop-upstream/job/nightly-build-whl/badge/icon)](http://ml-ci.amd.com:21096/job/tensorflow/job/nightly-rocmfork-develop-upstream/job/nightly-build-whl)   | [Nightly](http://ml-ci.amd.com:21096/job/tensorflow/job/nightly-rocmfork-develop-upstream/job/nightly-build-whl/lastSuccessfulBuild/)\nAMD | **Linux AMD ROCm GPU** Stable : TF 2.x | [![Build Status](http://ml-ci.amd.com:21096/job/tensorflow/job/release-rocmfork-r212-rocm-enhanced/job/release-build-whl/badge/icon)](http://ml-ci.amd.com:21096/job/tensorflow/job/release-rocmfork-r212-rocm-enhanced/job/release-build-whl/) | [Release 2.12](http://ml-ci.amd.com:21096/job/tensorflow/job/release-rocmfork-r212-rocm-enhanced/job/release-build-whl/lastSuccessfulBuild/)\nAMD | **Linux AMD ROCm GPU** Stable : TF 1.x | [![Build Status](http://ml-ci.amd.com:21096/job/tf-develop-upstream-releases/job/tensorflow-upstream-rel1.15-enhanced-nightly/badge/icon)](http://ml-ci.amd.com:21096/job/tf-develop-upstream-releases/job/tensorflow-upstream-rel1.15-enhanced-nightly/)   | [Release 1.15](http://ml-ci.amd.com:21096/job/tf-develop-upstream-releases/job/tensorflow-upstream-rel1.15-enhanced-nightly/lastSuccessfulBuild/)\nAMD | **Linux AMD ZenDNN Plug-in CPU** Stable : TF 2.x | [![Build Status](http://ml-ci.amd.com:21096/view/ZenDNN/job/zendnn/job/tensorflow-zendnn-plugin-build-whl-release/badge/icon)](http://ml-ci.amd.com:21096/view/ZenDNN/job/zendnn/job/tensorflow-zendnn-plugin-build-whl-release/) | [Release 2.x](http://ml-ci.amd.com:21096/view/ZenDNN/job/zendnn/job/tensorflow-zendnn-plugin-build-whl-release/)\nIBM | **Linux ppc64le CPU** Nightly | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/) | [Nightly](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Nightly_Artifact/)\nIBM | **Linux ppc64le CPU** Stable: TF 1.x | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/) | Release [1.15](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/)\nIBM | **Linux ppc64le CPU** Stable: TF 2.x | [![Build Status](https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_CPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_CPU_Release_Build/) | Release [2.x](https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_CPU_Release_Build/)\nIBM | **Linux ppc64le GPU** Nightly | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/) | [Nightly](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Nightly_Artifact/)\nIBM | **Linux ppc64le GPU** Stable: TF 1.x | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/) | Release [1.15](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/)\nIBM | **Linux ppc64le GPU** Stable: TF 2.x | [![Build Status](https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_GPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_GPU_Release_Build/) | Release [2.x](https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_GPU_Release_Build/)\nIBM | **Linux s390x** Nightly | [![Build Status](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/badge/icon)](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/) | [Nightly](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/)\nIBM | **Linux s390x CPU** Stable Release | [![Build Status](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/badge/icon)](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/) | [Release](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/)\nIntel | **Linux CPU with Intel oneDNN** Stable Release 1.x | No Badge | Release [1.15](https://pypi.org/project/intel-tensorflow/1.15.2/)\nIntel | **Linux CPU with Intel oneDNN** Stable Release 2.x | No Badge | Release [2.x](https://pypi.org/project/intel-tensorflow/)\nIntel | **Windows CPU with Intel oneDNN** Stable Release 2.x | No Badge | Release [2.x](https://pypi.org/project/intel-tensorflow/)\nLinaro | **Linux aarch64 CPU** Nightly | [![Build Status](https://ci.linaro.org/jenkins/buildStatus/icon?job=ldcg-python-manylinux-tensorflow-nightly)](https://ci.linaro.org/jenkins/job/ldcg-python-manylinux-tensorflow-nightly/) | [Nightly](http://snapshots.linaro.org/ldcg/python/tensorflow-manylinux-nightly/latest/)\nLinaro | **Linux aarch64 CPU** Stable Release | [![Build Status](https://ci.linaro.org/jenkins/buildStatus/icon?job=ldcg-python-manylinux-tensorflow)](https://ci.linaro.org/jenkins/job/ldcg-python-manylinux-tensorflow/) | Release [1.x & 2.x](http://snapshots.linaro.org/ldcg/python/tensorflow-manylinux/)\n\n\n### TensorFlow Containers\n\nOwner | Container Type | Status | Artifacts\n---: | --- | :---: | :---\nLinaro | **TensorFlow aarch64 Neoverse-N1 CPU** Stable <br> Debian | Static | Release [2.3](https://hub.docker.com/r/linaro/tensorflow-arm-neoverse-n1)\nArm | **TensorFlow AArch64 Neoverse-N1 CPU** Stable | Static | [Docker Hub](https://hub.docker.com/r/armswdev/tensorflow-arm-neoverse-n1)\nAMD| **Linux ROCm GPU** Stable | Static | [Docker Hub](https://hub.docker.com/r/rocm/tensorflow)\nIntel | **Linux CPU with Intel oneDNN** Stable | Static | [Docker Hub](https://hub.docker.com/r/intel/intel-optimized-tensorflow)\n", "release_dates": []}, {"name": "cloud", "description": "The TensorFlow Cloud repository provides APIs that will allow to easily go from debugging and training your Keras and TensorFlow code in a local environment to distributed training in the cloud.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Cloud\n\nThe TensorFlow Cloud repository provides APIs that will allow to easily go from\ndebugging, training, tuning your Keras and TensorFlow code in a local\nenvironment to distributed training/tuning on Cloud.\n\n## Introduction\n\n-   [TensorFlow Cloud `run` API](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/core/README.md)\n\n-   [TensorFlow Cloud Tuner](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/tuner/README.md)\n\n## TensorFlow Cloud `run` API for GCP training/tuning\n\n### Installation\n\n#### Requirements\n\n-   Python >= 3.6\n-   [A Google Cloud project](https://cloud.google.com/ai-platform/docs/getting-started-keras#set_up_your_project)\n-   An\n    [authenticated GCP account](https://cloud.google.com/ai-platform/docs/getting-started-keras#authenticate_your_gcp_account)\n-   [Google AI platform](https://cloud.google.com/ai-platform/) APIs enabled for\n    your GCP account. We use the AI platform for deploying docker images on GCP.\n-   Either a functioning version of\n    [docker](https://docs.docker.com/engine/install/) if you want to use a local\n    docker process for your build, or\n    [create a cloud storage bucket](https://cloud.google.com/ai-platform/docs/getting-started-keras#create_a_bucket)\n    to use with [Google Cloud build](https://cloud.google.com/cloud-build) for\n    docker image build and publishing.\n\n-   [Authenticate to your Docker Container Registry](https://cloud.google.com/container-registry/docs/advanced-authentication#gcloud-helper)\n\n-   (optional) [nbconvert](https://nbconvert.readthedocs.io/en/latest/) if you\n    are using a notebook file as `entry_point` as shown in\n    [usage guide #4](#usage-guide).\n\nFor detailed end to end setup instructions, please see\n[Setup instructions](#setup-instructions).\n\n#### Install latest release\n\n```shell\npip install -U tensorflow-cloud\n```\n\n#### Install from source\n\n```shell\ngit clone https://github.com/tensorflow/cloud.git\ncd cloud\npip install src/python/.\n```\n\n### High level overview\n\nTensorFlow Cloud package provides the `run` API for training your models on GCP.\nTo start, let's walk through a simple workflow using this API.\n\n1.  Let's begin with a Keras model training code such as the following, saved as\n    `mnist_example.py`.\n\n    ```python\n    import tensorflow as tf\n\n    (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n\n    x_train = x_train.reshape((60000, 28 * 28))\n    x_train = x_train.astype('float32') / 255\n\n    model = tf.keras.Sequential([\n      tf.keras.layers.Dense(512, activation='relu', input_shape=(28 * 28,)),\n      tf.keras.layers.Dropout(0.2),\n      tf.keras.layers.Dense(10, activation='softmax')\n    ])\n\n    model.compile(loss='sparse_categorical_crossentropy',\n                  optimizer=tf.keras.optimizers.Adam(),\n                  metrics=['accuracy'])\n\n    model.fit(x_train, y_train, epochs=10, batch_size=128)\n    ```\n\n1.  After you have tested this model on your local environment for a few epochs,\n    probably with a small dataset, you can train the model on Google Cloud by\n    writing the following simple script `scale_mnist.py`.\n\n    ```python\n    import tensorflow_cloud as tfc\n    tfc.run(entry_point='mnist_example.py')\n    ```\n\n    Running `scale_mnist.py` will automatically apply TensorFlow\n    [one device strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy)\n    and train your model at scale on Google Cloud Platform. Please see the\n    [usage guide](#usage-guide) section for detailed instructions and additional\n    API parameters.\n\n1.  You will see an output similar to the following on your console. This\n    information can be used to track the training job status.\n\n    ```shell\n    user@desktop$ python scale_mnist.py\n    Job submitted successfully.\n    Your job ID is:  tf_cloud_train_519ec89c_a876_49a9_b578_4fe300f8865e\n    Please access your job logs at the following URL:\n    https://console.cloud.google.com/mlengine/jobs/tf_cloud_train_519ec89c_a876_49a9_b578_4fe300f8865e?project=prod-123\n    ```\n\n### Setup instructions\n\nEnd to end instructions to help set up your environment for Tensorflow Cloud.\nYou use one of the following notebooks to setup your project or follow the\ninstructions below.\n\n<table align=\"left\">\n    <td>\n        <a href=\"https://colab.research.google.com/github/tensorflow/cloud/blob/master/examples/google_cloud_project_setup_instructions.ipynb\">\n            <img width=\"50\" src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\">Run in Colab\n        </a>\n    </td>\n    <td>\n        <a href=\"https://github.com/tensorflow/cloud/blob/master/examples/google_cloud_project_setup_instructions.ipynb\">\n            <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">View on GitHub\n        </a>\n     </td>\n    <td>\n        <a href=\"https://www.kaggle.com/nitric/google-cloud-project-setup-instructions\">\n            <img width=\"90\" src=\"https://www.kaggle.com/static/images/site-logo.png\" alt=\"Kaggle logo\">Run in Kaggle\n        </a>\n     </td>\n</table>\n\n1.  Create a new local directory\n\n    ```shell\n    mkdir tensorflow_cloud\n    cd tensorflow_cloud\n    ```\n\n1.  Make sure you have `python >= 3.6`\n\n    ```shell\n    python -V\n    ```\n\n1.  Set up virtual environment\n\n    ```shell\n    virtualenv tfcloud --python=python3\n    source tfcloud/bin/activate\n    ```\n\n1.  [Set up your Google Cloud project](https://cloud.google.com/ai-platform/docs/getting-started-keras#set_up_your_project)\n\n    Verify that gcloud sdk is installed.\n\n    ```shell\n    which gcloud\n    ```\n\n    Set default gcloud project\n\n    ```shell\n    export PROJECT_ID=<your-project-id>\n    gcloud config set project $PROJECT_ID\n    ```\n\n1.  [Authenticate your GCP account](https://cloud.google.com/ai-platform/docs/getting-started-keras#authenticate_your_gcp_account)\n\n    Create a service account.\n\n    ```shell\n    export SA_NAME=<your-sa-name>\n    gcloud iam service-accounts create $SA_NAME\n    gcloud projects add-iam-policy-binding $PROJECT_ID \\\n        --member serviceAccount:$SA_NAME@$PROJECT_ID.iam.gserviceaccount.com \\\n        --role 'roles/editor'\n    ```\n\n    Create a key for your service account.\n\n    ```shell\n    gcloud iam service-accounts keys create ~/key.json --iam-account $SA_NAME@$PROJECT_ID.iam.gserviceaccount.com\n    ```\n\n    Create the GOOGLE_APPLICATION_CREDENTIALS environment variable.\n\n    ```shell\n    export GOOGLE_APPLICATION_CREDENTIALS=~/key.json\n    ```\n\n1.  [Create a Cloud Storage bucket](https://cloud.google.com/ai-platform/docs/getting-started-keras#create_a_bucket).\n    Using [Google Cloud build](https://cloud.google.com/cloud-build) is the\n    recommended method for building and publishing docker images, although we\n    optionally allow for local\n    [docker daemon process](https://docs.docker.com/config/daemon/#start-the-daemon-manually)\n    depending on your specific needs.\n\n    ```shell\n    BUCKET_NAME=\"your-bucket-name\"\n    REGION=\"us-central1\"\n    gcloud auth login\n    gsutil mb -l $REGION gs://$BUCKET_NAME\n    ```\n\n    (optional for local docker setup) `shell sudo dockerd`\n\n1.  Authenticate access to Google Cloud registry.\n\n    ```shell\n    gcloud auth configure-docker\n    ```\n\n1.  Install [nbconvert](https://nbconvert.readthedocs.io/en/latest/) if you plan\n    to use a notebook file `entry_point` as shown in\n    [usage guide #4](#usage-guide).\n\n    ```shell\n    pip install nbconvert\n    ```\n\n1.  Install latest release of tensorflow-cloud\n\n    ```shell\n    pip install tensorflow-cloud\n    ```\n\n### Usage guide\n\nAs described in the [high level overview](#high-level-overview), the `run` API\nallows you to train your models at scale on GCP. The\n[`run`](https://github.com/tensorflow/cloud/blob/master/src/python/core/run.py#L31)\nAPI can be used in four different ways. This is defined by where you are running\nthe API (Terminal vs IPython notebook), and your `entry_point` parameter.\n`entry_point` is an optional Python script or notebook file path to the file\nthat contains your TensorFlow Keras training code. This is the most important\nparameter in the API.\n\n```python\nrun(entry_point=None,\n    requirements_txt=None,\n    distribution_strategy='auto',\n    docker_config='auto',\n    chief_config='auto',\n    worker_config='auto',\n    worker_count=0,\n    entry_point_args=None,\n    stream_logs=False,\n    job_labels=None,\n    **kwargs)\n```\n\n1.  **Using a python file as `entry_point`.**\n\n    If you have your `tf.keras` model in a python file (`mnist_example.py`),\n    then you can write the following simple script (`scale_mnist.py`) to scale\n    your model on GCP.\n\n    ```python\n    import tensorflow_cloud as tfc\n    tfc.run(entry_point='mnist_example.py')\n    ```\n\n    Please note that all the files in the same directory tree as `entry_point`\n    will be packaged in the docker image created, along with the `entry_point`\n    file. It's recommended to create a new directory to house each cloud project\n    which includes necessary files and nothing else, to optimize image build\n    times.\n\n1.  **Using a notebook file as `entry_point`.**\n\n    If you have your `tf.keras` model in a notebook file\n    (`mnist_example.ipynb`), then you can write the following simple script\n    (`scale_mnist.py`) to scale your model on GCP.\n\n    ```python\n    import tensorflow_cloud as tfc\n    tfc.run(entry_point='mnist_example.ipynb')\n    ```\n\n    Please note that all the files in the same directory tree as `entry_point`\n    will be packaged in the docker image created, along with the `entry_point`\n    file. Like the python script `entry_point` above, we recommended creating a\n    new directory to house each cloud project which includes necessary files and\n    nothing else, to optimize image build times.\n\n1.  **Using `run` within a python script that contains the `tf.keras` model.**\n\n    You can use the `run` API from within your python file that contains the\n    `tf.keras` model (`mnist_scale.py`). In this use case, `entry_point` should\n    be `None`. The `run` API can be called anywhere and the entire file will be\n    executed remotely. The API can be called at the end to run the script\n    locally for debugging purposes (possibly with fewer epochs and other flags).\n\n    ```python\n    import tensorflow_datasets as tfds\n    import tensorflow as tf\n    import tensorflow_cloud as tfc\n\n    tfc.run(\n        entry_point=None,\n        distribution_strategy='auto',\n        requirements_txt='requirements.txt',\n        chief_config=tfc.MachineConfig(\n                cpu_cores=8,\n                memory=30,\n                accelerator_type=tfc.AcceleratorType.NVIDIA_TESLA_T4,\n                accelerator_count=2),\n        worker_count=0)\n\n    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n    mnist_train, mnist_test = datasets['train'], datasets['test']\n\n    num_train_examples = info.splits['train'].num_examples\n    num_test_examples = info.splits['test'].num_examples\n\n    BUFFER_SIZE = 10000\n    BATCH_SIZE = 64\n\n    def scale(image, label):\n        image = tf.cast(image, tf.float32)\n        image /= 255\n        return image, label\n\n    train_dataset = mnist_train.map(scale).cache()\n    train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(\n            28, 28, 1)),\n        tf.keras.layers.MaxPooling2D(),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n\n    model.compile(loss='sparse_categorical_crossentropy',\n                  optimizer=tf.keras.optimizers.Adam(),\n                  metrics=['accuracy'])\n    model.fit(train_dataset, epochs=12)\n    ```\n\n    Please note that all the files in the same directory tree as the python\n    script will be packaged in the docker image created, along with the python\n    file. It's recommended to create a new directory to house each cloud project\n    which includes necessary files and nothing else, to optimize image build\n    times.\n\n1.  **Using `run` within a notebook script that contains the `tf.keras` model.**\n\n    ![Image of colab](https://github.com/tensorflow/cloud/blob/master/images/colab.png)\n\n    In this use case, `entry_point` should be `None` and\n    `docker_config.image_build_bucket` must be specified, to ensure the build\n    can be stored and published.\n\n    ### Cluster and distribution strategy configuration\n\n    By default, `run` API takes care of wrapping your model code in a TensorFlow\n    distribution strategy based on the cluster configuration you have provided.\n\n    ***No distribution***\n\n    CPU chief config and no additional workers\n\n    ```python\n    tfc.run(entry_point='mnist_example.py',\n            chief_config=tfc.COMMON_MACHINE_CONFIGS['CPU'])\n    ```\n\n    ***OneDeviceStrategy***\n\n    1 GPU on chief (defaults to `AcceleratorType.NVIDIA_TESLA_T4`) and no\n    additional workers.\n\n    ```python\n    tfc.run(entry_point='mnist_example.py')\n    ```\n\n    ***MirroredStrategy***\n\n    Chief config with multiple GPUS (`AcceleratorType.NVIDIA_TESLA_V100`).\n\n    ```python\n    tfc.run(entry_point='mnist_example.py',\n            chief_config=tfc.COMMON_MACHINE_CONFIGS['V100_4X'])\n    ```\n\n    ***MultiWorkerMirroredStrategy***\n\n    Chief config with 1 GPU and 2 workers each with 8 GPUs\n    (`AcceleratorType.NVIDIA_TESLA_V100`).\n\n    ```python\n    tfc.run(entry_point='mnist_example.py',\n            chief_config=tfc.COMMON_MACHINE_CONFIGS['V100_1X'],\n            worker_count=2,\n            worker_config=tfc.COMMON_MACHINE_CONFIGS['V100_8X'])\n    ```\n\n    ***TPUStrategy***\n\n    Chief config with 1 CPU and 1 worker with TPU.\n\n    ```python\n    tfc.run(entry_point=\"mnist_example.py\",\n            chief_config=tfc.COMMON_MACHINE_CONFIGS[\"CPU\"],\n            worker_count=1,\n            worker_config=tfc.COMMON_MACHINE_CONFIGS[\"TPU\"])\n    ```\n\n    Please note that TPUStrategy with TensorFlow Cloud works only with TF\n    version 2.1 as this is the latest version supported by\n    [AI Platform cloud TPU](https://cloud.google.com/ai-platform/training/docs/runtime-version-list#tpu-support)\n\n    ***Custom distribution strategy***\n\n    If you would like to take care of specifying distribution strategy in your\n    model code and do not want `run` API to create a strategy, then set\n    `distribution_stategy` as `None`. This will be required for example when you\n    are using `strategy.experimental_distribute_dataset`.\n\n    ```python\n    tfc.run(entry_point='mnist_example.py',\n            distribution_strategy=None,\n            worker_count=2)\n    ```\n\n#### What happens when you call run?\n\nThe API call will encompass the following:\n\n1.  Making code entities such as a Keras script/notebook, **cloud and\n    distribution ready**.\n1.  Converting this distribution entity into a **docker container** with the\n    required dependencies.\n1.  **Deploy** this container at scale and train using TensorFlow distribution\n    strategies.\n1.  **Stream logs** and monitor them on hosted TensorBoard, manage checkpoint\n    storage.\n\nBy default, we will use local docker daemon for building and publishing docker\nimages to Google container registry. Images are published to\n`gcr.io/your-gcp-project-id`. If you specify `docker_config.image_build_bucket`,\nthen we will use [Google Cloud build](https://cloud.google.com/cloud-build) to\nbuild and publish docker images.\n\nWe use [Google AI platform](https://cloud.google.com/ai-platform/) for deploying\ndocker images on GCP.\n\nPlease note that, when `entry_point` argument is specified, all the files in the\nsame directory tree as `entry_point` will be packaged in the docker image\ncreated, along with the `entry_point` file.\n\nPlease see `run` API documentation for detailed information on the parameters\nand how you can modify the above processes to suit your needs.\n\n### End to end examples\n\n```shell\ncd src/python/tensorflow_cloud/core\npython tests/examples/call_run_on_script_with_keras_fit.py\n```\n\n-   [Using a python file as `entry_point` (Keras fit API)](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/core/tests/examples/call_run_on_script_with_keras_fit.py).\n-   [Using a python file as `entry_point` (Keras custom training loop)](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/core/tests/examples/call_run_on_script_with_keras_ctl.py).\n-   [Using a python file as `entry_point` (Keras save and load)](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/core/tests/examples/call_run_on_script_with_keras_save_and_load.py).\n-   [Using a notebook file as `entry_point`](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/core/tests/examples/call_run_on_notebook_with_keras_fit.py).\n-   [Using `run` within a python script that contains the `tf.keras` model](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/core/tests/examples/call_run_within_script_with_keras_fit.py).\n-   [Using cloud build instead of local docker](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/core/tests/examples/call_run_on_script_with_keras_fit_cloud_build.py).\n-   [Run AutoKeras with TensorFlow Cloud](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/core/tests/examples/call_run_within_script_with_autokeras.py).\n\n### Running unit tests\n\n```shell\npytest src/python/tensorflow_cloud/core/tests/unit/\n```\n\n### Local vs remote training\n\nThings to keep in mind when running your jobs remotely:\n\n[Coming soon]\n\n### Debugging workflow\n\nHere are some tips for fixing unexpected issues.\n\n#### Operation disallowed within distribution strategy scope\n\n**Error like**: Creating a generator within a strategy scope is disallowed,\nbecause there is ambiguity on how to replicate a generator (e.g. should it be\ncopied so that each replica gets the same random numbers, or 'split' so that\neach replica gets different random numbers).\n\n**Solution**: Passing `distribution_strategy='auto'` to `run` API wraps all of\nyour script in a TF distribution strategy based on the cluster configuration\nprovided. You will see the above error or something similar to it, if for some\nreason an operation is not allowed inside distribution strategy scope. To fix\nthe error, please pass `None` to the `distribution_strategy` param and create a\nstrategy instance as part of your training code as shown in\n[this](https://github.com/tensorflow/cloud/blob/master/src/python/tensorflow_cloud/core/tests/testdata/save_and_load.py)\nexample.\n\n#### Docker image build timeout\n\n**Error like**: requests.exceptions.ConnectionError: ('Connection aborted.',\ntimeout('The write operation timed out'))\n\n**Solution**: The directory being used as an entry point likely has too much\ndata for the image to successfully build, and there may be extraneous data\nincluded in the build. Reformat your directory structure such that the folder\nwhich contains the entry point only includes files necessary for the current\nproject.\n\n#### Version not supported for TPU training\n\n**Error like**: There was an error submitting the job.Field: tpu_tf_version\nError: The specified runtime version '2.3' is not supported for TPU training.\nPlease specify a different runtime version.\n\n**Solution**: Please use TF version 2.1. See TPU Strategy in\n[Cluster and distribution strategy configuration section](#cluster-and-distribution-strategy-configuration).\n\n#### TF nightly build.\n\n**Warning like**: Docker parent image '2.4.0.dev20200720' does not exist. Using\nthe latest TF nightly build.\n\n**Solution**: If you do not provide `docker_config.parent_image` param, then by\ndefault we use pre-built TF docker images as parent image. If you do not have TF\ninstalled on the environment where `run` is called, then TF docker image for the\n`latest` stable release will be used. Otherwise, the version of the docker image\nwill match the locally installed TF version. However, pre-built TF docker images\naren't available for TF nightlies except for the latest. So, if your local TF is\nan older nightly version, we upgrade to the latest nightly automatically and\nraise this warning.\n\n#### Mixing distribution strategy objects.\n\n**Error like**: RuntimeError: Mixing different tf.distribute.Strategy objects.\n\n**Solution**: Please provide `distribution_strategy=None` when you already have\na distribution strategy defined in your model code. Specifying\n`distribution_strategy'='auto'`, will wrap your code in a TensorFlow\ndistribution strategy. This will cause the above error, if there is a strategy\nobject already used in your code.\n\n### Coming up\n\n-   Distributed Keras tuner support.\n\n## Contributing\n\nWe welcome community contributions, see [CONTRIBUTING.md](CONTRIBUTING.md) and,\nfor style help,\n[Writing TensorFlow documentation](https://www.tensorflow.org/community/contribute/docs)\nguide.\n\n## License\n\n[Apache License 2.0](LICENSE)\n\n## Privacy Notice\n\nThis application reports technical and operational details of your usage of\nCloud Services in accordance with Google privacy policy, for more information\nplease refer to https://policies.google.com/privacy. If you wish to opt-out, you\nmay do so by running\ntensorflow_cloud.utils.google_api_client.optout_metrics_reporting().\n", "release_dates": ["2021-06-16T20:29:30Z", "2021-05-18T23:07:20Z", "2021-05-04T19:44:22Z", "2021-02-09T22:15:10Z", "2021-02-03T20:05:25Z", "2021-01-06T22:38:37Z", "2021-01-06T22:36:57Z", "2020-11-03T22:03:22Z", "2020-10-30T22:36:15Z", "2020-09-30T23:49:10Z", "2020-08-21T01:52:11Z", "2020-08-05T21:36:44Z", "2020-07-19T22:58:32Z", "2020-07-14T03:01:55Z", "2020-07-14T02:58:10Z", "2020-07-14T02:56:40Z", "2020-07-14T02:54:18Z"]}, {"name": "codelabs", "description": null, "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Codelabs\n\nThis repository contains sample code for several TensorFlow codelabs. Check out the Google machine learning pathways to learn more.\n\nhttps://developers.google.com/learn/pathways?category=aiandmachinelearning\n", "release_dates": []}, {"name": "community", "description": "Stores documents used by the TensorFlow developer community", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Welcome to the TensorFlow Developer Community\n\n## This Repository\n\nThe `community` repository stores documents used by the developer community.\n\n* `rfcs` - design documents used by the design review process\n* `sigs` - documentation for each TensorFlow Special Interest group (SIG)\n* `governance` - operating processes for the TensorFlow project\n\n## Contact\n\nFor questions about this repository, please file an issue or reach out\nto Thea Lamkin: thealamkin@google.com.\n\n## Further Community Resources\n\nFor a complete overview of the TensorFlow community resources,\nplease visit [tensorflow.org/community](https://tensorflow.org/community). \n", "release_dates": []}, {"name": "compression", "description": "Data compression in TensorFlow", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Compression\n\nTensorFlow Compression (TFC) contains data compression tools for TensorFlow.\n\nYou can use this library to build your own ML models with end-to-end optimized\ndata compression built in. It's useful to find storage-efficient representations\nof your data (images, features, examples, etc.) while only sacrificing a small\nfraction of model performance. Take a look at the [lossy data compression\ntutorial](https://www.tensorflow.org/tutorials/generative/data_compression) or\nthe [model compression\ntutorial](https://www.tensorflow.org/tutorials/optimization/compression) to get\nstarted.\n\nFor a more in-depth introduction from a classical data compression perspective,\nconsider our [paper on nonlinear transform\ncoding](https://arxiv.org/abs/2007.03034), or watch @jonycgn's [talk on learned\nimage compression](https://www.youtube.com/watch?v=x_q7cZviXkY). For an\nintroduction to lossy data compression from a machine learning perspective, take\na look at @yiboyang's [review paper](https://arxiv.org/abs/2202.06533).\n\nThe library contains (see the [API\ndocs](https://www.tensorflow.org/api_docs/python/tfc) for details):\n\n- Range coding (a.k.a. arithmetic coding) implementations in the form of\n  flexible TF ops written in C++. These include an optional \"overflow\"\n  functionality that embeds an Elias gamma code into the range encoded bit\n  sequence, making it possible to encode alphabets containing the entire set of\n  signed integers rather than just a finite range.\n\n- Entropy model classes which simplify the process of designing rate\u2013distortion\n  optimized codes. During training, they act like likelihood models. Once\n  training is completed, they encode floating point tensors into optimized bit\n  sequences by automating the design of range coding tables and calling the\n  range coder implementation behind the scenes.\n\n- Additional TensorFlow functions and Keras layers that are useful in the\n  context of learned data compression, such as methods to numerically find\n  quantiles of density functions, take expectations with respect to dithering\n  noise, convolution layers with more flexible padding options and support for\n  reparameterizing kernels and biases in the Fourier domain, and an\n  implementation of generalized divisive normalization (GDN).\n\n**Important update:** As of February 1, 2024, TensorFlow Compression is in\nmaintenance mode. This means concretely:\n\n- The full feature set of TFC is frozen. No new features will be developed, but\n  the repository will receive maintenance fixes.\n\n- Going forward, new TFC packages will only work with TensorFlow 2.14. This is\n  due to an incompatibility introduced in the Keras version shipped with TF\n  2.15, which would require a rewrite of our layer and entropy model classes.\n\n- To ensure existing models can still be run with TF 2.15 and later, we are\n  releasing a new package\n  [tensorflow-compression-ops](https://github.com/tensorflow/compression/tree/master/tensorflow_compression_ops),\n  which only contains the C++ ops. These will be updated as long as possible for\n  newer TF versions.\n\n- Binary packages are provided for both options on pypi.org:\n  [TFC](https://pypi.org/project/tensorflow-compression/) and\n  [TFC ops](https://pypi.org/project/tensorflow-compression-ops/).\n\n\n## Documentation & getting help\n\nRefer to [the API documentation](https://www.tensorflow.org/api_docs/python/tfc)\nfor a complete description of the classes and functions this package implements.\n\nPlease post all questions or comments on\n[Discussions](https://github.com/tensorflow/compression/discussions). Only file\n[Issues](https://github.com/tensorflow/compression/issues) for actual bugs or\nfeature requests. On Discussions, you may get a faster answer, and you help\nother people find the question or answer more easily later.\n\n## Installation\n\n***Note: Precompiled packages are currently only provided for Linux and\nDarwin/Mac OS. To use these packages on Windows, consider installing TensorFlow\nusing the [instructions for\nWSL2](https://www.tensorflow.org/install/pip#windows_1) or using a [TensorFlow\nDocker image](https://www.tensorflow.org/install/docker), and then installing\nthe Linux package.***\n\nSet up an environment in which you can install precompiled binary Python\npackages using the `pip` command. Refer to the\n[TensorFlow installation instructions](https://www.tensorflow.org/install/pip)\nfor more information on how to set up such a Python environment.\n\nThe current version of TensorFlow Compression requires TensorFlow 2. For\nversions compatible with TensorFlow 1, see our [previous\nreleases](https://github.com/tensorflow/compression/releases).\n\n### pip\n\nTo install TFC via `pip`, run the following command:\n\n```bash\npython -m pip install tensorflow-compression\n```\n\nTo test that the installation works correctly, you can run the unit tests with:\n\n```bash\npython -m tensorflow_compression.all_tests\n```\n\nOnce the command finishes, you should see a message ```OK (skipped=29)``` or\nsimilar in the last line.\n\n### Colab\n\nYou can try out TFC live in a [Colab](https://colab.research.google.com/). The\nfollowing command installs the latest version of TFC that is compatible with the\ninstalled TensorFlow version. Run it in a cell before executing your Python\ncode:\n\n```\n%pip install tensorflow-compression~=$(pip show tensorflow | perl -p -0777 -e 's/.*Version: (\\d+\\.\\d+).*/\\1.0/sg')\n```\n\nNote: The binary packages of TFC are tied to TF with the same minor version\n(e.g., TFC 2.9.1 requires TF 2.9.x), and Colab sometimes lags behind a few days\nin deploying the latest version of TensorFlow. As a result, using `pip install\ntensorflow-compression` naively might attempt to upgrade TF, which can create\nproblems.\n\n### Docker\n\nTo use a Docker container (e.g. on Windows), be sure to install Docker\n(e.g., [Docker Desktop](https://www.docker.com/products/docker-desktop)),\nuse a [TensorFlow Docker image](https://www.tensorflow.org/install/docker),\nand then run the `pip install` command inside the Docker container, not on the\nhost. For instance, you can use a command line like this:\n\n```bash\ndocker run tensorflow/tensorflow:latest bash -c \\\n    \"python -m pip install tensorflow-compression &&\n     python -m tensorflow_compression.all_tests\"\n```\n\nThis will fetch the TensorFlow Docker image if it's not already cached, install\nthe pip package and then run the unit tests to confirm that it works.\n\n### Anaconda\n\nIt seems that [Anaconda](https://www.anaconda.com/distribution/) ships its own\nbinary version of TensorFlow which is incompatible with our pip package. To\nsolve this, always install TensorFlow via `pip` rather than `conda`. For\nexample, this creates an Anaconda environment with CUDA libraries, and then\ninstalls TensorFlow and TensorFlow Compression:\n\n```bash\nconda create --name ENV_NAME python cudatoolkit cudnn\nconda activate ENV_NAME\npython -m pip install tensorflow-compression\n```\n\nDepending on the requirements of the `tensorflow` pip package, you may need to\npin the CUDA libraries to specific versions. If you aren't using a GPU, CUDA is\nof course not necessary.\n\n## Usage\n\nWe recommend importing the library from your Python code as follows:\n\n```python\nimport tensorflow as tf\nimport tensorflow_compression as tfc\n```\n\n### Using a pre-trained model to compress an image\n\nIn the\n[models directory](https://github.com/tensorflow/compression/tree/master/models),\nyou'll find a python script `tfci.py`. Download the file and run:\n\n```bash\npython tfci.py -h\n```\n\nThis will give you a list of options. Briefly, the command\n\n```bash\npython tfci.py compress <model> <PNG file>\n```\n\nwill compress an image using a pre-trained model and write a file ending in\n`.tfci`. Execute `python tfci.py models` to give you a list of supported\npre-trained models. The command\n\n```bash\npython tfci.py decompress <TFCI file>\n```\n\nwill decompress a TFCI file and write a PNG file. By default, an output file\nwill be named like the input file, only with the appropriate file extension\nappended (any existing extensions will not be removed).\n\n### Training your own model\n\nThe\n[models directory](https://github.com/tensorflow/compression/tree/master/models)\ncontains several implementations of published image compression models to enable\neasy experimentation. Note that in order to reproduce published results, more\ntuning of the code and training dataset may be necessary. Use the `tfci.py`\nscript above to access published models.\n\nThe following instructions talk about a re-implementation of the model published\nin:\n\n> \"End-to-end optimized image compression\"<br />\n> J. Ball\u00e9, V. Laparra, E. P. Simoncelli<br />\n> https://arxiv.org/abs/1611.01704\n\nNote that the models directory is not contained in the pip package. The models\nare meant to be downloaded individually. Download the file `bls2017.py` and run:\n\n```bash\npython bls2017.py -h\n```\n\nThis will list the available command line options for the implementation.\nTraining can be as simple as the following command:\n\n```bash\npython bls2017.py -V train\n```\n\nThis will use the default settings. Note that unless a custom training dataset\nis provided via `--train_glob`, the\n[CLIC dataset](https://www.tensorflow.org/datasets/catalog/clic) will be\ndownloaded using TensorFlow Datasets.\n\nThe most important training parameter is `--lambda`, which controls the\ntrade-off between bitrate and distortion that the model will be optimized for.\nThe number of channels per layer is important, too: models tuned for higher\nbitrates (or, equivalently, lower distortion) tend to require transforms with a\ngreater approximation capacity (i.e. more channels), so to optimize performance,\nyou want to make sure that the number of channels is large enough (or larger).\nThis is described in more detail in:\n\n> \"Efficient nonlinear transforms for lossy image compression\"<br />\n> J. Ball\u00e9<br />\n> https://arxiv.org/abs/1802.00847\n\nIf you wish, you can monitor progress with Tensorboard. To do this, create a\nTensorboard instance in the background before starting the training, then point\nyour web browser to [port 6006 on your machine](http://localhost:6006):\n\n```bash\ntensorboard --logdir=/tmp/train_bls2017 &\n```\n\nWhen training has finished, the Python script saves the trained model to the\ndirectory specified with `--model_path` (by default, `bls2017` in the current\ndirectory) in TensorFlow's `SavedModel` format. The script can then be used to\ncompress and decompress images as follows. The same saved model must be\naccessible to both commands.\n\n```bash\npython bls2017.py [options] compress original.png compressed.tfci\npython bls2017.py [options] decompress compressed.tfci reconstruction.png\n```\n\n## Building pip packages\n\nThis section describes the necessary steps to build your own pip packages of\nTensorFlow Compression. This may be necessary to install it on platforms for\nwhich we don't provide precompiled binaries (currently only Linux and Darwin).\n\nTo be compatible with the official TensorFlow pip package, the TFC pip package\nmust be linked against a matching version of the C libraries. For this reason,\nto build the official Linux pip packages, we use [these Docker\nimages](https://hub.docker.com/r/tensorflow/build) and use the same toolchain\nthat TensorFlow uses.\n\nInside the Docker container, the following steps need to be taken:\n\n1. Clone the `tensorflow/compression` repo from GitHub.\n2. Run `tools/build_pip_pkg.sh` inside the cloned repo.\n\nFor example:\n\n```bash\ngit clone https://github.com/tensorflow/compression.git /tensorflow_compression\ndocker run -i --rm \\\n    -v /tmp/tensorflow_compression:/tmp/tensorflow_compression\\\n    -v /tensorflow_compression:/tensorflow_compression \\\n    -w /tensorflow_compression \\\n    -e \"BAZEL_OPT=--config=manylinux_2_17_x86_64\" \\\n    tensorflow/build:latest-python3.10 \\\n    bash tools/build_pip_pkg.sh /tmp/tensorflow_compression <custom-version>\n```\n\nFor Darwin, the Docker image and specifying the toolchain is not necessary. We\njust build the package like this (note that you may want to create a clean\nPython virtual environment to do this):\n\n```bash\ngit clone https://github.com/tensorflow/compression.git /tensorflow_compression\ncd /tensorflow_compression\nBAZEL_OPT=\"--macos_minimum_os=10.14\" bash \\\n  tools/build_pip_pkg.sh \\\n  /tmp/tensorflow_compression <custom-version>\n```\n\nIn both cases, the wheel file is created inside `/tmp/tensorflow_compression`.\n\nTo test the created package, first install the resulting wheel file:\n\n```bash\npython -m pip install /tmp/tensorflow_compression/tensorflow_compression-*.whl\n```\n\nThen run the unit tests (Do not run the tests in the workspace directory where\nthe `WORKSPACE` file lives. In that case, the Python interpreter would attempt\nto import `tensorflow_compression` packages from the source tree, rather than\nfrom the installed package system directory):\n\n```bash\npushd /tmp\npython -m tensorflow_compression.all_tests\npopd\n```\n\nWhen done, you can uninstall the pip package again:\n\n```bash\npython -m pip uninstall tensorflow-compression\n```\n\n## Evaluation\n\nWe provide evaluation results for several image compression methods in terms of\ndifferent metrics in different colorspaces. Please see the\n[results subdirectory](https://github.com/tensorflow/compression/tree/master/results/image_compression)\nfor more information.\n\n## Citation\n\nIf you use this library for research purposes, please cite:\n```\n@software{tfc_github,\n  author = \"Ball\u00e9, Johannes and Hwang, Sung Jin and Agustsson, Eirikur\",\n  title = \"{T}ensor{F}low {C}ompression: Learned Data Compression\",\n  url = \"http://github.com/tensorflow/compression\",\n  version = \"2.14.1\",\n  year = \"2024\",\n}\n```\nIn the above BibTeX entry, names are top contributors sorted by number of\ncommits. Please adjust version number and year according to the version that was\nactually used.\n\nNote that this is not an officially supported Google product.\n", "release_dates": ["2024-02-02T01:53:52Z", "2024-02-02T01:42:40Z", "2023-10-13T19:56:17Z", "2023-07-26T23:21:33Z", "2023-03-23T21:44:50Z", "2022-11-23T02:02:13Z", "2022-09-07T15:27:20Z", "2022-08-13T06:18:43Z", "2022-05-30T18:44:48Z", "2022-05-16T22:44:20Z", "2022-03-02T19:12:18Z", "2022-02-09T15:42:33Z", "2022-01-26T17:13:57Z", "2021-05-14T00:38:32Z", "2021-03-11T09:05:35Z", "2021-03-06T04:46:21Z", "2021-01-06T01:50:06Z", "2020-12-04T19:38:43Z", "2019-11-20T01:04:19Z", "2019-08-15T21:10:06Z", "2019-07-17T21:42:45Z", "2019-04-17T17:58:57Z", "2019-04-02T05:39:46Z", "2019-03-14T22:11:35Z"]}, {"name": "custom-op", "description": "Guide for building custom op for TensorFlow", "language": "Smarty", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Custom Op\nThis is a guide for users who want to write custom c++ op for TensorFlow and distribute the op as a pip package. This repository serves as both a working example of the op building and packaging process, as well as a template/starting point for writing your own ops. The way this repository is set up allow you to build your custom ops from TensorFlow's pip package instead of building TensorFlow from scratch. This guarantee that the shared library you build will be binary compatible with TensorFlow's pip packages.\n\nThis guide currently supports Ubuntu and Windows custom ops, and it includes examples for both cpu and gpu ops.\n\nStarting from Aug 1, 2019, nightly previews `tf-nightly` and `tf-nightly-gpu`, as well as\nofficial releases `tensorflow` and `tensorflow-gpu` past version 1.14.0 are now built with a\ndifferent environment (Ubuntu 16.04 compared to Ubuntu 14.04, for example) as part of our effort to make TensorFlow's pip pacakges\nmanylinux2010 compatible. To help you building custom ops on linux, here we provide our toolchain in the format of a combination of a Docker image and bazel configurations.  Please check the table below for the Docker image name needed to build your custom ops.\n\n|          |          CPU custom op          |          GPU custom op         |\n|----------|:-------------------------------:|:------------------------------:|\n| TF nightly  |    nightly-custom-op-ubuntu16   | nightly-custom-op-gpu-ubuntu16 |\n| TF >= 2.3   |   2.3.0-custom-op-ubuntu16  |    2.3.0-custom-op-gpu-ubuntu16    |\n| TF 1.5, 2.0 | custom-op-ubuntu16-cuda10.0 |       custom-op-gpu-ubuntu16       |\n| TF <= 1.4   |        custom-op-ubuntu14       |     custom-op-gpu-ubuntu14     |\n\n\nNote: all above Docker images have prefix `tensorflow/tensorflow:`\n\nThe bazel configurations are included as part of this repository.\n\n## Build Example zero_out Op (CPU only)\nIf you want to try out the process of building a pip package for custom op, you can use the source code from this repository following the instructions below.\n\n### For Windows Users\nYou can skip this section if you are not building on Windows. If you are building custom ops for Windows platform, you will need similar setup as building TensorFlow from source mentioned [here](https://www.tensorflow.org/install/source_windows). Additionally, you can skip all the Docker steps from the instructions below. Otherwise, the bazel commands to build and test custom ops stay the same.\n\n### Setup Docker Container\nYou are going to build the op inside a Docker container. Pull the provided Docker image from TensorFlow's Docker hub and start a container.\n\nUse the following command if the TensorFlow pip package you are building\nagainst is not yet manylinux2010 compatible:\n```bash\n  docker pull tensorflow/tensorflow:custom-op-ubuntu14\n  docker run -it tensorflow/tensorflow:custom-op-ubuntu14 /bin/bash\n```\nAnd the following instead if it is manylinux2010 compatible:\n\n```bash\n  docker pull tensorflow/tensorflow:custom-op-ubuntu16\n  docker run -it tensorflow/tensorflow:custom-op-ubuntu16 /bin/bash\n```\n\nInside the Docker container, clone this repository. The code in this repository came from the [Adding an op](https://www.tensorflow.org/extend/adding_an_op) guide.\n```bash\ngit clone https://github.com/tensorflow/custom-op.git\ncd custom-op\n```\n\n### Build PIP Package\nYou can build the pip package with either Bazel or make.\n\nWith bazel:\n```bash\n  ./configure.sh\n  bazel build build_pip_pkg\n  bazel-bin/build_pip_pkg artifacts\n```\n\nWith Makefile:\n```bash\n  make zero_out_pip_pkg\n```\n\n### Install and Test PIP Package\nOnce the pip package has been built, you can install it with,\n```bash\npip3 install artifacts/*.whl\n```\nThen test out the pip package\n```bash\ncd ..\npython3 -c \"import tensorflow as tf;import tensorflow_zero_out;print(tensorflow_zero_out.zero_out([[1,2], [3,4]]))\"\n```\nAnd you should see the op zeroed out all input elements except the first one:\n```bash\n[[1 0]\n [0 0]]\n```\n\n## Create and Distribute Custom Ops\nNow you are ready to write and distribute your own ops. The example in this repository has done the boiling plate work for setting up build systems and package files needed for creating a pip package. We recommend using this repository as a template. \n\n\n### Template Overview\nFirst let's go through a quick overview of the folder structure of this template repository.\n```\n\u251c\u2500\u2500 gpu  # Set up crosstool and CUDA libraries for Nvidia GPU, only needed for GPU ops\n\u2502   \u251c\u2500\u2500 crosstool/\n\u2502   \u251c\u2500\u2500 cuda/\n\u2502   \u251c\u2500\u2500 BUILD\n\u2502   \u2514\u2500\u2500 cuda_configure.bzl\n|\n\u251c\u2500\u2500 tensorflow_zero_out  # A CPU only op\n\u2502   \u251c\u2500\u2500 cc\n\u2502   \u2502   \u251c\u2500\u2500 kernels  # op kernel implementation\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zero_out_kernels.cc\n\u2502   \u2502   \u2514\u2500\u2500 ops  # op interface definition\n\u2502   \u2502       \u2514\u2500\u2500 zero_out_ops.cc\n\u2502   \u251c\u2500\u2500 python\n\u2502   \u2502   \u251c\u2500\u2500 ops\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 zero_out_ops.py   # Load and extend the ops in python\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zero_out_ops_test.py  # tests for ops\n\u2502   \u2502   \u2514\u2500\u2500 __init__.py\n|   |\n\u2502   \u251c\u2500\u2500 BUILD  # BUILD file for all op targets\n\u2502   \u2514\u2500\u2500 __init__.py  # top level __init__ file that imports the custom op\n\u2502\n\u251c\u2500\u2500 tensorflow_time_two  # A GPU op\n\u2502   \u251c\u2500\u2500 cc\n\u2502   \u2502   \u251c\u2500\u2500 kernels  # op kernel implementation\n\u2502   \u2502   \u2502   |\u2500\u2500 time_two.h\n\u2502   \u2502   \u2502   |\u2500\u2500 time_two_kernels.cc\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 time_two_kernels.cu.cc  # GPU kernel\n\u2502   \u2502   \u2514\u2500\u2500 ops  # op interface definition\n\u2502   \u2502       \u2514\u2500\u2500 time_two_ops.cc\n\u2502   \u251c\u2500\u2500 python\n\u2502   \u2502   \u251c\u2500\u2500 ops\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 time_two_ops.py   # Load and extend the ops in python\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 time_two_ops_test.py  # tests for ops\n\u2502   \u2502   \u2514\u2500\u2500 __init__.py\n|   |\n\u2502   \u251c\u2500\u2500 BUILD  # BUILD file for all op targets\n\u2502   \u2514\u2500\u2500 __init__.py  # top level __init__ file that imports the custom op\n|\n\u251c\u2500\u2500 tf  # Set up TensorFlow pip package as external dependency for Bazel\n\u2502   \u251c\u2500\u2500 BUILD\n\u2502   \u251c\u2500\u2500 BUILD.tpl\n\u2502   \u2514\u2500\u2500 tf_configure.bzl\n|\n\u251c\u2500\u2500 BUILD  # top level Bazel BUILD file that contains pip package build target\n\u251c\u2500\u2500 build_pip_pkg.sh  # script to build pip package for Bazel and Makefile\n\u251c\u2500\u2500 configure.sh  # script to install TensorFlow and setup action_env for Bazel\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile  # Makefile for building shared library and pip package\n\u251c\u2500\u2500 setup.py  # file for creating pip package\n\u251c\u2500\u2500 MANIFEST.in  # files for creating pip package\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 WORKSPACE  # Used by Bazel to specify tensorflow pip package as an external dependency\n\n```\nThe op implementation, including both c++ and python code, goes under `tensorflow_zero_out` dir for CPU only ops, or `tensorflow_time_two` dir for GPU ops. You will want to replace either directory with the corresponding content of your own ops. `tf` folder contains the code for setting up TensorFlow pip package as an external dependency for Bazel only. You shouldn't need to change the content of this folder. You also don't need this folder if you are using other build systems, such as Makefile. The `gpu` folder contains the code for setting up CUDA libraries and toolchain. You only need the `gpu` folder if you are writing a GPU op and using bazel. To build a pip package for your op, you will also need to update a few files at the top level of the template, for example, `setup.py`, `MANIFEST.in` and `build_pip_pkg.sh`.\n\n### Setup\nFirst, clone this template repo.\n```bash\ngit clone https://github.com/tensorflow/custom-op.git my_op\ncd my_op\n```\n\n#### Docker\nNext, set up a Docker container using the provided Docker image for building and testing the ops. We provide two sets of Docker images for different versions of pip packages. If the pip package you are building against was released before Aug 1, 2019 and has manylinux1 tag, please use Docker images `tensorflow/tensorflow:custom-op-ubuntu14` and `tensorflow/tensorflow:custom-op-gpu-ubuntu14`, which are based on Ubuntu 14.04. Otherwise, for the newer manylinux2010 packages, please use Docker images `tensorflow/tensorflow:custom-op-ubuntu16` and `tensorflow/tensorflow:custom-op-gpu-ubuntu16` instead. All Docker images come with Bazel pre-installed, as well as the corresponding toolchain used for building the released TensorFlow pacakges. We have seen many cases where dependency version differences and ABI incompatibilities cause the custom op extension users build to not work properly with TensorFlow's released pip packages. Therefore, it is *highly recommended* to use the provided Docker image to build your custom op. To get the CPU Docker image, run one of the following command based on which pip package you are building against:\n```bash\n# For pip packages labeled manylinux1\ndocker pull tensorflow/tensorflow:custom-op-ubuntu14\n\n# For manylinux2010\ndocker pull tensorflow/tensorflow:custom-op-ubuntu16\n```\n\nFor GPU, run \n```bash\n# For pip packages labeled manylinux1\ndocker pull tensorflow/tensorflow:custom-op-gpu-ubuntu14\n\n# For manylinux2010\ndocker pull tensorflow/tensorflow:custom-op-gpu-ubuntu16\n```\n\nYou might want to use Docker volumes to map a `work_dir` from host to the container, so that you can edit files on the host, and build with the latest changes in the Docker container. To do so, run the following for CPU\n```bash\n# For pip packages labeled manylinux1\ndocker run -it -v ${PWD}:/working_dir -w /working_dir  tensorflow/tensorflow:custom-op-ubuntu14\n\n# For manylinux2010\ndocker run -it -v ${PWD}:/working_dir -w /working_dir  tensorflow/tensorflow:custom-op-ubuntu16\n```\n\nFor GPU, you want to use `nvidia-docker`:\n```bash\n# For pip packages labeled manylinux1\ndocker run --runtime=nvidia --privileged  -it -v ${PWD}:/working_dir -w /working_dir  tensorflow/tensorflow:custom-op-gpu-ubuntu14\n\n# For manylinux2010\ndocker run --runtime=nvidia --privileged  -it -v ${PWD}:/working_dir -w /working_dir  tensorflow/tensorflow:custom-op-gpu-ubuntu16\n\n```\n\n#### Run configure.sh\nLast step before starting implementing the ops, you want to set up the build environment. The custom ops will need to depend on TensorFlow headers and shared library libtensorflow_framework.so, which are distributed with TensorFlow official pip package. If you would like to use Bazel to build your ops, you might also want to set a few action_envs so that Bazel can find the installed TensorFlow. We provide a `configure` script that does these for you. Simply run `./configure.sh` in the docker container and you are good to go.\n\n\n### Add Op Implementation\nNow you are ready to implement your op. Following the instructions at [Adding a New Op](https://www.tensorflow.org/extend/adding_an_op), add definition of your op interface under `<your_op>/cc/ops/` and kernel implementation under `<your_op>/cc/kernels/`.\n\n\n### Build and Test CPU Op\n\n#### Bazel\nTo build the custom op shared library with Bazel, follow the cc_binary example in [`tensorflow_zero_out/BUILD`](https://github.com/tensorflow/custom-op/blob/master/tensorflow_zero_out/BUILD#L5). You will need to depend on the header files and libtensorflow_framework.so from TensorFlow pip package to build your op. Earlier we mentioned that the template has already setup TensorFlow pip package as an external dependency in `tf` directory, and the pip package is listed as `local_config_tf` in [`WORKSPACE`](https://github.com/tensorflow/custom-op/blob/master/WORKSPACE) file. Your op can depend directly on TensorFlow header files and 'libtensorflow_framework.so' with the following:\n```python\n    deps = [\n        \"@local_config_tf//:libtensorflow_framework\",\n        \"@local_config_tf//:tf_header_lib\",\n    ],\n```\n\nYou will need to keep both above dependencies for your op. To build the shared library with Bazel, run the following command in your Docker container\n```bash\nbazel build tensorflow_zero_out:python/ops/_zero_out_ops.so\n```\n\n#### Makefile\nTo build the custom op shared library with make, follow the example in [`Makefile`](https://github.com/tensorflow/custom-op/blob/master/Makefile) for `_zero_out_ops.so` and run the following command in your Docker container:\n```bash\nmake op\n```\n\n#### Extend and Test the Op in Python\nOnce you have built your custom op shared library, you can follow the example in [`tensorflow_zero_out/python/ops`](https://github.com/tensorflow/custom-op/tree/master/tensorflow_zero_out/python/ops), and instructions [here](https://www.tensorflow.org/extend/adding_an_op#use_the_op_in_python) to create a module in Python for your op. Both guides use TensorFlow API `tf.load_op_library`, which loads the shared library and registers the ops with the TensorFlow framework.\n```python\nfrom tensorflow.python.framework import load_library\nfrom tensorflow.python.platform import resource_loader\n\n_zero_out_ops = load_library.load_op_library(\n    resource_loader.get_path_to_datafile('_zero_out_ops.so'))\nzero_out = _zero_out_ops.zero_out\n\n```\n\nYou can also add Python tests like what we have done in `tensorflow_zero_out/python/ops/zero_out_ops_test.py` to check that your op is working as intended.\n\n\n##### Run Tests with Bazel\nTo add the python library and tests targets to Bazel, please follow the examples for `py_library` target `tensorflow_zero_out:zero_out_ops_py` and `py_test` target `tensorflow_zero_out:zero_out_ops_py_test` in `tensorflow_zero_out/BUILD` file. To run your test with bazel, do the following in Docker container,\n\n```bash\nbazel test tensorflow_zero_out:zero_out_ops_py_test\n```\n\n##### Run Tests with Make\nTo add the test target to make, please follow the example in `Makefile`. To run your python test, simply run the following in Docker container,\n```bash\nmake test_zero_out\n```\n\n### Build and Test GPU Op\n\n#### Bazel\nTo build the custom GPU op shared library with Bazel, follow the cc_binary example in [`tensorflow_time_two/BUILD`](https://github.com/tensorflow/custom-op/blob/master/tensorflow_time_two/BUILD#L29). Similar to CPU custom ops, you can directly depend on TensorFlow header files and 'libtensorflow_framework.so' with the following:\n```python\n    deps = [\n        \"@local_config_tf//:libtensorflow_framework\",\n        \"@local_config_tf//:tf_header_lib\",\n    ],\n```\n\nAdditionally, when you ran configure inside the GPU container, `config=cuda` will be set for bazel command, which will also automatically include cuda shared library and cuda headers as part of the dependencies only for GPU version of the op: `if_cuda_is_configured([\":cuda\",  \"@local_config_cuda//cuda:cuda_headers\"])`.\n\nTo build the shared library with Bazel, run the following command in your Docker container\n```bash\nbazel build tensorflow_time_two:python/ops/_time_two_ops.so\n```\n\n#### Makefile\nTo build the custom op shared library with make, follow the example in [`Makefile`](https://github.com/tensorflow/custom-op/blob/master/Makefile) for `_time_two_ops.so` and run the following command in your Docker container:\n```bash\nmake time_two_op\n```\n\n#### Extend and Test the Op in Python\nOnce you have built your custom op shared library, you can follow the example in [`tensorflow_time_two/python/ops`](https://github.com/tensorflow/custom-op/tree/master/tensorflow_time_two/python/ops), and instructions [here](https://www.tensorflow.org/extend/adding_an_op#use_the_op_in_python) to create a module in Python for your op. This part is the same as CPU custom op as shown above.\n\n\n##### Run Tests with Bazel\nSimilar to CPU custom op, to run your test with bazel, do the following in Docker container,\n\n```bash\nbazel test tensorflow_time_two:time_two_ops_py_test\n```\n\n##### Run Tests with Make\nTo add the test target to make, please follow the example in `Makefile`. To run your python test, simply run the following in Docker container,\n```bash\nmake time_two_test\n```\n\n\n\n\n### Build PIP Package\nNow your op works, you might want to build a pip package for it so the community can also benefit from your work. This template provides the basic setup needed to build your pip package. First, you will need to update the following top level files based on your op.\n\n- `setup.py` contains information about your package (such as the name and version) as well as which code files to include. \n- `MANIFEST.in` contains the list of additional files you want to include in the source distribution. Here you want to make sure the shared library for your custom op is included in the pip package.\n- `build_pip_pkg.sh` creates the package hierarchy, and calls `bdist_wheel` to assemble your pip package.\n\nYou can use either Bazel or Makefile to build the pip package.\n\n\n#### Build with Bazel\nYou can find the target for pip package in the top level `BUILD` file. Inside the data list of this `build_pip_pkg` target, you want to include the python library target ` //tensorflow_zero_out:zero_out_py` in addition to the top level files. To build the pip package builder, run the following command in Docker container,\n```bash\nbazel build :build_pip_pkg\n```\n\nThe bazel build command creates a binary named build_pip_package, which you can use to build the pip package. For example, the following builds your .whl package in the `artifacts` directory:\n```bash\nbazel-bin/build_pip_pkg artifacts\n```\n\n#### Build with make\nBuilding with make also invoke the same `build_pip_pkg.sh` script. You can run,\n```bash\nmake pip_pkg\n```\n\n### Test PIP Package\nBefore publishing your pip package, test your pip package.\n```bash\npip3 install artifacts/*.whl\npython3 -c \"import tensorflow as tf;import tensorflow_zero_out;print(tensorflow_zero_out.zero_out([[1,2], [3,4]]))\"\n```\n\n\n### Publish PIP Package\nOnce your pip package has been thoroughly tested, you can distribute your package by uploading your package to the Python Package Index. Please follow the [official instruction](https://packaging.python.org/tutorials/packaging-projects/#uploading-the-distribution-archives) from Pypi.\n\n\n### FAQ\n\nHere are some issues our users have ran into and possible solutions. Feel free to send us a PR to add more entries.\n\n\n| Issue  |  How to? |\n|---|---|\n|  Do I need both the toolchain and the docker image? | Yes, you will need both to get the same setup we use to build TensorFlow's official pip package. |\n|  How do I also create a manylinux2010 binary? | You can use [auditwheel](https://github.com/pypa/auditwheel) version 2.0.0 or newer.  |\n|  What do I do if I get `ValueError: Cannot repair wheel, because required library \"libtensorflow_framework.so.1\" could not be located` or `ValueError: Cannot repair wheel, because required library \"libtensorflow_framework.so.2\" could not be located` with auditwheel? | Please see [this related issue](https://github.com/tensorflow/tensorflow/issues/31807).  |\n| What do I do if I get `In file included from tensorflow_time_two/cc/kernels/time_two_kernels.cu.cc:21:0: /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/util/gpu_kernel_helper.h:22:10: fatal error: third_party/gpus/cuda/include/cuda_fp16.h: No such file or directory` | Copy the CUDA header files to target directory. `mkdir -p /usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/gpus/cuda/include && cp -r /usr/local/cuda/targets/x86_64-linux/include/* /usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/gpus/cuda/include` |\n", "release_dates": []}, {"name": "data-validation", "description": "Library for exploring and validating machine learning data", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<!-- See: www.tensorflow.org/tfx/data_validation/ -->\n\n# TensorFlow Data Validation\n\n[![Python](https://img.shields.io/badge/python%7C3.9%7C3.10%7C3.11-blue)](https://github.com/tensorflow/data-validation)\n[![PyPI](https://badge.fury.io/py/tensorflow-data-validation.svg)](https://badge.fury.io/py/tensorflow-data-validation)\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv)\n\n*TensorFlow Data Validation* (TFDV) is a library for exploring and validating\nmachine learning data. It is designed to be highly scalable\nand to work well with TensorFlow and [TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx).\n\nTF Data Validation includes:\n\n*    Scalable calculation of summary statistics of training and test data.\n*    Integration with a viewer for data distributions and statistics, as well\n     as faceted comparison of pairs of features ([Facets](https://github.com/PAIR-code/facets))\n*    Automated [data-schema](https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/schema.proto)\n     generation to describe expectations about data\n     like required values, ranges, and vocabularies\n*    A schema viewer to help you inspect the schema.\n*    Anomaly detection to identify [anomalies](https://github.com/tensorflow/data-validation/blob/master/g3doc/anomalies.md),\n     such as missing features,\n     out-of-range values, or wrong feature types, to name a few.\n*    An anomalies viewer so that you can see what features have anomalies and\n     learn more in order to correct them.\n\nFor instructions on using TFDV, see the [get started guide](https://github.com/tensorflow/data-validation/blob/master/g3doc/get_started.md)\nand try out the [example notebook](https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/data_validation/tfdv_basic.ipynb).\nSome of the techniques implemented in TFDV are described in a\n[technical paper published in SysML'19](https://mlsys.org/Conferences/2019/doc/2019/167.pdf).\n\n## Installing from PyPI\n\nThe recommended way to install TFDV is using the\n[PyPI package](https://pypi.org/project/tensorflow-data-validation/):\n\n```bash\npip install tensorflow-data-validation\n```\n### Nightly Packages\n\nTFDV also hosts nightly packages on Google Cloud. To install the latest nightly\npackage, please use the following command:\n\n```bash\nexport TFX_DEPENDENCY_SELECTOR=NIGHTLY\npip install --extra-index-url https://pypi-nightly.tensorflow.org/simple tensorflow-data-validation\n```\n\nThis will install the nightly packages for the major dependencies of TFDV such\nas TFX Basic Shared Libraries (TFX-BSL) and TensorFlow Metadata (TFMD).\n\nSometimes TFDV uses those dependencies' most recent changes, which are not yet\nreleased. Because of this, it is safer to use nightly versions of those\ndependent libraries when using nightly TFDV. Export the\n`TFX_DEPENDENCY_SELECTOR` environment variable to do so.\n\nNOTE: These nightly packages are unstable and breakages are likely to happen.\nThe fix could often take a week or more depending on the complexity involved.\n\n## Build with Docker\n\nThis is the recommended way to build TFDV under Linux, and is continuously\ntested at Google.\n\n### 1. Install Docker\n\nPlease first install `docker` and `docker-compose` by following the directions:\n[docker](https://docs.docker.com/install/);\n[docker-compose](https://docs.docker.com/compose/install/).\n\n### 2. Clone the TFDV repository\n\n```shell\ngit clone https://github.com/tensorflow/data-validation\ncd data-validation\n```\n\nNote that these instructions will install the latest master branch of TensorFlow\nData Validation. If you want to install a specific branch (such as a release\nbranch), pass `-b <branchname>` to the `git clone` command.\n\n### 3. Build the pip package\n\nThen, run the following at the project root:\n\n```bash\nsudo docker-compose build manylinux2010\nsudo docker-compose run -e PYTHON_VERSION=${PYTHON_VERSION} manylinux2010\n```\nwhere `PYTHON_VERSION` is one of `{39, 310, 311}`.\n\nA wheel will be produced under `dist/`.\n\n### 4. Install the pip package\n\n```shell\npip install dist/*.whl\n```\n\n## Build from source\n\n### 1. Prerequisites\n\nTo compile and use TFDV, you need to set up some prerequisites.\n\n#### Install NumPy\n\nIf NumPy is not installed on your system, install it now by following [these\ndirections](https://www.scipy.org/scipylib/download.html).\n\n#### Install Bazel\n\nIf Bazel is not installed on your system, install it now by following [these\ndirections](https://bazel.build/versions/master/docs/install.html).\n\n### 2. Clone the TFDV repository\n\n```shell\ngit clone https://github.com/tensorflow/data-validation\ncd data-validation\n```\n\nNote that these instructions will install the latest master branch of TensorFlow\nData Validation. If you want to install a specific branch (such as a release\nbranch), pass `-b <branchname>` to the `git clone` command.\n\n### 3. Build the pip package\n\n`TFDV` wheel is Python version dependent -- to build the pip package that\nworks for a specific Python version, use that Python binary to run:\n\n```shell\npython setup.py bdist_wheel\n```\n\nYou can find the generated `.whl` file in the `dist` subdirectory.\n\n### 4. Install the pip package\n\n```shell\npip install dist/*.whl\n```\n\n## Supported platforms\n\nTFDV is tested on the following 64-bit operating systems:\n\n  * macOS 12.5 (Monterey) or later.\n  * Ubuntu 20.04 or later.\n\n## Notable Dependencies\n\nTensorFlow is required.\n\n[Apache Beam](https://beam.apache.org/) is required; it's the way that efficient\ndistributed computation is supported. By default, Apache Beam runs in local\nmode but can also run in distributed mode using\n[Google Cloud Dataflow](https://cloud.google.com/dataflow/) and other Apache\nBeam\n[runners](https://beam.apache.org/documentation/runners/capability-matrix/).\n\n[Apache Arrow](https://arrow.apache.org/) is also required. TFDV uses Arrow to\nrepresent data internally in order to make use of vectorized numpy functions.\n\n## Compatible versions\n\nThe following table shows the  package versions that are\ncompatible with each other. This is determined by our testing framework, but\nother *untested* combinations may also work.\n\ntensorflow-data-validation                                                            | apache-beam[gcp] | pyarrow | tensorflow        | tensorflow-metadata | tensorflow-transform | tfx-bsl\n------------------------------------------------------------------------------------- | ---------------- | ------- | ----------------- | ------------------- | -------------------- | -------\n[GitHub master](https://github.com/tensorflow/data-validation/blob/master/RELEASE.md) | 2.47.0           | 10.0.0  | nightly (2.x)     | 1.14.0              | n/a                  | 1.14.0\n[1.14.0](https://github.com/tensorflow/data-validation/blob/v1.14.0/RELEASE.md)       | 2.47.0           | 10.0.0  | 2.13              | 1.14.0              | n/a                  | 1.14.0\n[1.13.0](https://github.com/tensorflow/data-validation/blob/v1.13.0/RELEASE.md)       | 2.40.0           | 6.0.0   | 2.12              | 1.13.1              | n/a                  | 1.13.0\n[1.12.0](https://github.com/tensorflow/data-validation/blob/v1.12.0/RELEASE.md)       | 2.40.0           | 6.0.0   | 2.11              | 1.12.0              | n/a                  | 1.12.0\n[1.11.0](https://github.com/tensorflow/data-validation/blob/v1.11.0/RELEASE.md)       | 2.40.0           | 6.0.0   | 1.15 / 2.10       | 1.11.0              | n/a                  | 1.11.0\n[1.10.0](https://github.com/tensorflow/data-validation/blob/v1.10.0/RELEASE.md)       | 2.40.0           | 6.0.0   | 1.15 / 2.9        | 1.10.0              | n/a                  | 1.10.1\n[1.9.0](https://github.com/tensorflow/data-validation/blob/v1.9.0/RELEASE.md)         | 2.38.0           | 5.0.0   | 1.15 / 2.9        | 1.9.0               | n/a                  | 1.9.0\n[1.8.0](https://github.com/tensorflow/data-validation/blob/v1.8.0/RELEASE.md)         | 2.38.0           | 5.0.0   | 1.15 / 2.8        | 1.8.0               | n/a                  | 1.8.0\n[1.7.0](https://github.com/tensorflow/data-validation/blob/v1.7.0/RELEASE.md)         | 2.36.0           | 5.0.0   | 1.15 / 2.8        | 1.7.0               | n/a                  | 1.7.0\n[1.6.0](https://github.com/tensorflow/data-validation/blob/v1.6.0/RELEASE.md)         | 2.35.0           | 5.0.0   | 1.15 / 2.7        | 1.6.0               | n/a                  | 1.6.0\n[1.5.0](https://github.com/tensorflow/data-validation/blob/v1.5.0/RELEASE.md)         | 2.34.0           | 5.0.0   | 1.15 / 2.7        | 1.5.0               | n/a                  | 1.5.0\n[1.4.0](https://github.com/tensorflow/data-validation/blob/v1.4.0/RELEASE.md)         | 2.32.0           | 4.0.1   | 1.15 / 2.6        | 1.4.0               | n/a                  | 1.4.0\n[1.3.0](https://github.com/tensorflow/data-validation/blob/v1.3.0/RELEASE.md)         | 2.32.0           | 2.0.0   | 1.15 / 2.6        | 1.2.0               | n/a                  | 1.3.0\n[1.2.0](https://github.com/tensorflow/data-validation/blob/v1.2.0/RELEASE.md)         | 2.31.0           | 2.0.0   | 1.15 / 2.5        | 1.2.0               | n/a                  | 1.2.0\n[1.1.1](https://github.com/tensorflow/data-validation/blob/v1.1.1/RELEASE.md)         | 2.29.0           | 2.0.0   | 1.15 / 2.5        | 1.1.0               | n/a                  | 1.1.1\n[1.1.0](https://github.com/tensorflow/data-validation/blob/v1.1.0/RELEASE.md)         | 2.29.0           | 2.0.0   | 1.15 / 2.5        | 1.1.0               | n/a                  | 1.1.0\n[1.0.0](https://github.com/tensorflow/data-validation/blob/v1.0.0/RELEASE.md)         | 2.29.0           | 2.0.0   | 1.15 / 2.5        | 1.0.0               | n/a                  | 1.0.0\n[0.30.0](https://github.com/tensorflow/data-validation/blob/v0.30.0/RELEASE.md)       | 2.28.0           | 2.0.0   | 1.15 / 2.4        | 0.30.0              | n/a                  | 0.30.0\n[0.29.0](https://github.com/tensorflow/data-validation/blob/v0.29.0/RELEASE.md)       | 2.28.0           | 2.0.0   | 1.15 / 2.4        | 0.29.0              | n/a                  | 0.29.0\n[0.28.0](https://github.com/tensorflow/data-validation/blob/v0.28.0/RELEASE.md)       | 2.28.0           | 2.0.0   | 1.15 / 2.4        | 0.28.0              | n/a                  | 0.28.1\n[0.27.0](https://github.com/tensorflow/data-validation/blob/v0.27.0/RELEASE.md)       | 2.27.0           | 2.0.0   | 1.15 / 2.4        | 0.27.0              | n/a                  | 0.27.0\n[0.26.1](https://github.com/tensorflow/data-validation/blob/v0.26.1/RELEASE.md)       | 2.28.0           | 0.17.0  | 1.15 / 2.3        | 0.26.0              | 0.26.0               | 0.26.0\n[0.26.0](https://github.com/tensorflow/data-validation/blob/v0.26.0/RELEASE.md)       | 2.25.0           | 0.17.0  | 1.15 / 2.3        | 0.26.0              | 0.26.0               | 0.26.0\n[0.25.0](https://github.com/tensorflow/data-validation/blob/v0.25.0/RELEASE.md)       | 2.25.0           | 0.17.0  | 1.15 / 2.3        | 0.25.0              | 0.25.0               | 0.25.0\n[0.24.1](https://github.com/tensorflow/data-validation/blob/v0.24.1/RELEASE.md)       | 2.24.0           | 0.17.0  | 1.15 / 2.3        | 0.24.0              | 0.24.1               | 0.24.1\n[0.24.0](https://github.com/tensorflow/data-validation/blob/v0.24.0/RELEASE.md)       | 2.23.0           | 0.17.0  | 1.15 / 2.3        | 0.24.0              | 0.24.0               | 0.24.0\n[0.23.1](https://github.com/tensorflow/data-validation/blob/v0.23.1/RELEASE.md)       | 2.24.0           | 0.17.0  | 1.15 / 2.3        | 0.23.0              | 0.23.0               | 0.23.0\n[0.23.0](https://github.com/tensorflow/data-validation/blob/v0.23.0/RELEASE.md)       | 2.23.0           | 0.17.0  | 1.15 / 2.3        | 0.23.0              | 0.23.0               | 0.23.0\n[0.22.2](https://github.com/tensorflow/data-validation/blob/v0.22.2/RELEASE.md)       | 2.20.0           | 0.16.0  | 1.15 / 2.2        | 0.22.0              | 0.22.0               | 0.22.1\n[0.22.1](https://github.com/tensorflow/data-validation/blob/v0.22.1/RELEASE.md)       | 2.20.0           | 0.16.0  | 1.15 / 2.2        | 0.22.0              | 0.22.0               | 0.22.1\n[0.22.0](https://github.com/tensorflow/data-validation/blob/v0.22.0/RELEASE.md)       | 2.20.0           | 0.16.0  | 1.15 / 2.2        | 0.22.0              | 0.22.0               | 0.22.0\n[0.21.5](https://github.com/tensorflow/data-validation/blob/v0.21.5/RELEASE.md)       | 2.17.0           | 0.15.0  | 1.15 / 2.1        | 0.21.0              | 0.21.1               | 0.21.3\n[0.21.4](https://github.com/tensorflow/data-validation/blob/v0.21.4/RELEASE.md)       | 2.17.0           | 0.15.0  | 1.15 / 2.1        | 0.21.0              | 0.21.1               | 0.21.3\n[0.21.2](https://github.com/tensorflow/data-validation/blob/v0.21.2/RELEASE.md)       | 2.17.0           | 0.15.0  | 1.15 / 2.1        | 0.21.0              | 0.21.0               | 0.21.0\n[0.21.1](https://github.com/tensorflow/data-validation/blob/v0.21.1/RELEASE.md)       | 2.17.0           | 0.15.0  | 1.15 / 2.1        | 0.21.0              | 0.21.0               | 0.21.0\n[0.21.0](https://github.com/tensorflow/data-validation/blob/v0.21.0/RELEASE.md)       | 2.17.0           | 0.15.0  | 1.15 / 2.1        | 0.21.0              | 0.21.0               | 0.21.0\n[0.15.0](https://github.com/tensorflow/data-validation/blob/v0.15.0/RELEASE.md)       | 2.16.0           | 0.14.0  | 1.15 / 2.0        | 0.15.0              | 0.15.0               | 0.15.0\n[0.14.1](https://github.com/tensorflow/data-validation/blob/v0.14.1/RELEASE.md)       | 2.14.0           | 0.14.0  | 1.14              | 0.14.0              | 0.14.0               | n/a\n[0.14.0](https://github.com/tensorflow/data-validation/blob/v0.14.0/RELEASE.md)       | 2.14.0           | 0.14.0  | 1.14              | 0.14.0              | 0.14.0               | n/a\n[0.13.1](https://github.com/tensorflow/data-validation/blob/v0.13.1/RELEASE.md)       | 2.11.0           | n/a     | 1.13              | 0.12.1              | 0.13.0               | n/a\n[0.13.0](https://github.com/tensorflow/data-validation/blob/v0.13.0/RELEASE.md)       | 2.11.0           | n/a     | 1.13              | 0.12.1              | 0.13.0               | n/a\n[0.12.0](https://github.com/tensorflow/data-validation/blob/v0.12.0/RELEASE.md)       | 2.10.0           | n/a     | 1.12              | 0.12.1              | 0.12.0               | n/a\n[0.11.0](https://github.com/tensorflow/data-validation/blob/v0.11.0/RELEASE.md)       | 2.8.0            | n/a     | 1.11              | 0.9.0               | 0.11.0               | n/a\n[0.9.0](https://github.com/tensorflow/data-validation/blob/v0.9.0/RELEASE.md)         | 2.6.0            | n/a     | 1.9               | n/a                 | n/a                  | n/a\n\n## Questions\n\nPlease direct any questions about working with TF Data Validation to\n[Stack Overflow](https://stackoverflow.com) using the\n[tensorflow-data-validation](https://stackoverflow.com/questions/tagged/tensorflow-data-validation)\ntag.\n\n## Links\n\n  * [TensorFlow Data Validation Getting Started Guide](https://www.tensorflow.org/tfx/data_validation/get_started)\n  * [TensorFlow Data Validation Notebook](https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/data_validation/tfdv_basic.ipynb)\n  * [TensorFlow Data Validation API Documentation](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv)\n  * [TensorFlow Data Validation Blog Post](https://medium.com/tensorflow/introducing-tensorflow-data-validation-data-understanding-validation-and-monitoring-at-scale-d38e3952c2f0)\n  * [TensorFlow Data Validation PyPI](https://pypi.org/project/tensorflow-data-validation/)\n  * [TensorFlow Data Validation Paper](https://mlsys.org/Conferences/2019/doc/2019/167.pdf)\n  * [TensorFlow Data Validation Slides](https://conf.slac.stanford.edu/xldb2018/sites/xldb2018.conf.slac.stanford.edu/files/Tues_09.45_NeoklisPolyzotis_Data%20Analysis%20and%20Validation%20(1).pdf)\n\n", "release_dates": ["2023-08-14T06:29:46Z", "2023-04-14T17:05:11Z", "2022-12-08T22:22:56Z", "2022-11-16T06:37:37Z", "2022-08-29T22:43:57Z", "2022-06-29T17:34:01Z", "2022-05-16T07:40:43Z", "2022-03-02T20:48:55Z", "2022-01-21T02:09:31Z", "2021-12-01T23:06:45Z", "2021-10-27T23:36:05Z", "2021-09-20T17:17:02Z", "2021-07-28T23:14:32Z", "2021-07-26T17:21:20Z", "2021-06-22T20:17:51Z", "2021-05-24T18:30:11Z", "2021-05-10T18:26:00Z", "2021-04-21T22:08:37Z", "2021-03-24T21:18:20Z", "2021-02-24T01:58:44Z", "2021-01-28T20:05:49Z", "2020-12-17T00:22:01Z", "2020-11-05T18:21:17Z", "2020-09-24T22:30:26Z", "2020-09-24T16:13:45Z", "2020-09-14T22:02:12Z", "2020-08-14T21:34:12Z", "2020-06-29T22:30:10Z", "2020-06-24T23:31:51Z", "2020-05-15T23:36:31Z"]}, {"name": "datasets", "description": "TFDS is a collection of datasets ready to use with TensorFlow, Jax, ...", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Datasets\n\nTensorFlow Datasets provides many public datasets as `tf.data.Datasets`.\n\n[![Unittests](https://github.com/tensorflow/datasets/actions/workflows/pytest.yml/badge.svg)](https://github.com/tensorflow/datasets/actions/workflows/pytest.yml)\n[![PyPI version](https://badge.fury.io/py/tensorflow-datasets.svg)](https://badge.fury.io/py/tensorflow-datasets)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![Tutorial](https://img.shields.io/badge/doc-tutorial-blue.svg)](https://www.tensorflow.org/datasets/overview)\n[![API](https://img.shields.io/badge/doc-api-blue.svg)](https://www.tensorflow.org/datasets/api_docs/python/tfds)\n[![Catalog](https://img.shields.io/badge/doc-datasets-blue.svg)](https://www.tensorflow.org/datasets/catalog/overview#all_datasets)\n\n## Documentation\n\nTo install and use TFDS, we strongly encourage to start with our\n[**getting started guide**](https://www.tensorflow.org/datasets/overview). Try\nit interactively in a\n[Colab notebook](https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb).\n\nOur documentation contains:\n\n* [Tutorials and guides](https://www.tensorflow.org/datasets/overview)\n* List of all [available datasets](https://www.tensorflow.org/datasets/catalog/overview#all_datasets)\n* The [API reference](https://www.tensorflow.org/datasets/api_docs/python/tfds)\n\n```python\n# !pip install tensorflow-datasets\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\n# Construct a tf.data.Dataset\nds = tfds.load('mnist', split='train', as_supervised=True, shuffle_files=True)\n\n# Build your input pipeline\nds = ds.shuffle(1000).batch(128).prefetch(10).take(5)\nfor image, label in ds:\n  pass\n```\n\n## TFDS core values\n\nTFDS has been built with these principles in mind:\n\n* **Simplicity**: Standard use-cases should work out-of-the box\n* **Performance**: TFDS follows\n  [best practices](https://www.tensorflow.org/guide/data_performance)\n  and can achieve state-of-the-art speed\n* **Determinism/reproducibility**: All users get the same examples in the same\n  order\n* **Customisability**: Advanced users can have fine-grained control\n\nIf those use cases are not satisfied, please send us\n[feedback](https://github.com/tensorflow/datasets/issues).\n\n## Want a certain dataset?\n\nAdding a dataset is really straightforward by following\n[our guide](https://www.tensorflow.org/datasets/add_dataset).\n\nRequest a dataset by opening a\n[Dataset request GitHub issue](https://github.com/tensorflow/datasets/issues/new?assignees=&labels=dataset+request&template=dataset-request.md&title=%5Bdata+request%5D+%3Cdataset+name%3E).\n\nAnd vote on the current\n[set of requests](https://github.com/tensorflow/datasets/labels/dataset%20request)\nby adding a thumbs-up reaction to the issue.\n\n### Citation\n\nPlease include the following citation when using `tensorflow-datasets` for a\npaper, in addition to any citation specific to the used datasets.\n\n```bibtex\n@misc{TFDS,\n  title = {{TensorFlow Datasets}, A collection of ready-to-use datasets},\n  howpublished = {\\url{https://www.tensorflow.org/datasets}},\n}\n```\n\n#### *Disclaimers*\n\n*This is a utility library that downloads and prepares public datasets. We do*\n*not host or distribute these datasets, vouch for their quality or fairness, or*\n*claim that you have license to use the dataset. It is your responsibility to*\n*determine whether you have permission to use the dataset under the dataset's*\n*license.*\n\n*If you're a dataset owner and wish to update any part of it (description,*\n*citation, etc.), or do not want your dataset to be included in this*\n*library, please get in touch through a GitHub issue. Thanks for your*\n*contribution to the ML community!*\n\n*If you're interested in learning more about responsible AI practices, including*\n*fairness, please see Google AI's [Responsible AI Practices](https://ai.google/education/responsible-ai-practices).*\n\n*`tensorflow/datasets` is Apache 2.0 licensed. See the\n[`LICENSE`](https://github.com/tensorflow/datasets/blob/master/LICENSE) file.*\n", "release_dates": ["2023-12-18T13:28:11Z", "2023-09-08T09:07:53Z", "2023-04-13T11:21:10Z", "2023-04-11T13:16:52Z", "2023-04-05T07:30:49Z", "2023-02-27T11:46:00Z", "2023-01-17T20:41:36Z", "2023-01-02T18:30:24Z", "2022-12-21T11:09:44Z", "2022-10-05T10:23:01Z", "2022-06-02T09:21:23Z", "2022-01-31T15:45:29Z", "2022-01-31T12:10:06Z", "2022-01-26T09:44:08Z", "2021-07-28T12:29:08Z", "2021-05-07T13:09:09Z", "2021-01-06T15:41:12Z", "2020-11-04T12:02:52Z", "2020-10-09T17:45:17Z", "2020-10-06T19:15:50Z", "2020-08-12T10:05:15Z", "2020-07-10T21:39:26Z", "2020-04-30T00:18:42Z", "2020-04-16T03:03:50Z", "2020-02-25T21:51:43Z", "2020-01-24T20:02:14Z", "2019-10-24T16:12:44Z", "2019-08-20T08:26:58Z", "2019-07-22T21:24:25Z", "2019-05-01T20:26:45Z"]}, {"name": "decision-forests", "description": "A collection of state-of-the-art algorithms for the training, serving and interpretation of Decision Forest models in Keras.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<p align=\"center\">\n<img src=\"documentation/image/logo.png\"  />\n</p>\n\n**TensorFlow Decision Forests** (**TF-DF**) is a library to train, run and\ninterpret [decision forest](https://ydf.readthedocs.io/en/latest/intro_df.html)\nmodels (e.g., Random Forests, Gradient Boosted Trees) in TensorFlow. TF-DF\nsupports classification, regression and ranking.\n\n**TF-DF** is powered by\n[Yggdrasil Decision Forest](https://github.com/google/yggdrasil-decision-forests)\n(**YDF**, a library to train and use decision forests in C++, JavaScript, CLI,\nand Go. TF-DF models are\n[compatible](https://ydf.readthedocs.io/en/latest/convert_model.html#convert-a-a-tensorflow-decision-forests-model-to-a-yggdrasil-model)\nwith YDF' models, and vice versa.\n\nTensorflow Decision Forests is available on Linux and Mac. Windows users can use\nthe library through WSL+Linux.\n\n## Usage example\n\nA minimal end-to-end run looks as follows:\n\n```python\nimport tensorflow_decision_forests as tfdf\nimport pandas as pd\n\n# Load the dataset in a Pandas dataframe.\ntrain_df = pd.read_csv(\"project/train.csv\")\ntest_df = pd.read_csv(\"project/test.csv\")\n\n# Convert the dataset into a TensorFlow dataset.\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"my_label\")\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, label=\"my_label\")\n\n# Train the model\nmodel = tfdf.keras.RandomForestModel()\nmodel.fit(train_ds)\n\n# Look at the model.\nmodel.summary()\n\n# Evaluate the model.\nmodel.evaluate(test_ds)\n\n# Export to a TensorFlow SavedModel.\n# Note: the model is compatible with Yggdrasil Decision Forests.\nmodel.save(\"project/model\")\n```\n\n## Google I/O Presentation\n\n<div align=\"center\">\n    <a href=\"https://youtu.be/5qgk9QJ4rdQ\">\n        <img src=\"https://img.youtube.com/vi/5qgk9QJ4rdQ/0.jpg\"></img>\n    </a>\n</div>\n\n## Documentation & Resources\n\nThe following resources are available:\n\n-   [TF-DF on TensorFlow.org](https://tensorflow.org/decision_forests) (API\n    Reference, Guides and Tutorials)\n-   [Tutorials](https://www.tensorflow.org/decision_forests/tutorials) (on\n    tensorflow.org)\n-   [YDF documentation](https://ydf.readthedocs.io) (also applicable to TF-DF)\n-   [Issue tracker](https://github.com/tensorflow/decision-forests/issues)\n-   [Known issues](documentation/known_issues.md)\n-   [Changelog](CHANGELOG.md)\n-   [TensorFlow Forum](https://discuss.tensorflow.org) (on\n    discuss.tensorflow.org)\n-   [More examples](documentation/more_examples.md)\n\n## Installation\n\nTo install TensorFlow Decision Forests, run:\n\n```shell\npip3 install tensorflow_decision_forests --upgrade\n```\n\nSee the [installation](documentation/installation.md) page for more details,\ntroubleshooting and alternative installation solutions.\n\n## Contributing\n\nContributions to TensorFlow Decision Forests and Yggdrasil Decision Forests are\nwelcome. If you want to contribute, make sure to review the\n[developer manual](documentation/developer_manual.md) and\n[contribution guidelines](CONTRIBUTING.md).\n\n## Citation\n\nIf you us Tensorflow Decision Forests in a scientific publication, please cite\nthe following paper:\n[Yggdrasil Decision Forests: A Fast and Extensible Decision Forests Library](https://doi.org/10.1145/3580305.3599933).\n\n**Bibtex**\n\n```\n@inproceedings{GBBSP23,\n  author       = {Mathieu Guillame{-}Bert and\n                  Sebastian Bruch and\n                  Richard Stotz and\n                  Jan Pfeifer},\n  title        = {Yggdrasil Decision Forests: {A} Fast and Extensible Decision Forests\n                  Library},\n  booktitle    = {Proceedings of the 29th {ACM} {SIGKDD} Conference on Knowledge Discovery\n                  and Data Mining, {KDD} 2023, Long Beach, CA, USA, August 6-10, 2023},\n  pages        = {4068--4077},\n  year         = {2023},\n  url          = {https://doi.org/10.1145/3580305.3599933},\n  doi          = {10.1145/3580305.3599933},\n}\n```\n\n**Raw**\n\nYggdrasil Decision Forests: A Fast and Extensible Decision Forests Library,\nGuillame-Bert et al., KDD 2023: 4068-4077. doi:10.1145/3580305.3599933\n\n## Contact\n\nYou can contact the core development team at\n[decision-forests-contact@google.com](mailto:decision-forests-contact@google.com).\n\n## Credits\n\nTensorFlow Decision Forests was developed by:\n\n-   Mathieu Guillame-Bert (gbm AT google DOT com)\n-   Jan Pfeifer (janpf AT google DOT com)\n-   Richard Stotz (richardstotz AT google DOT com)\n-   Sebastian Bruch (sebastian AT bruch DOT io)\n-   Arvind Srinivasan (arvnd AT google DOT com)\n\n## License\n\n[Apache License 2.0](LICENSE)\n", "release_dates": ["2023-11-17T17:03:16Z", "2023-11-17T14:54:52Z", "2023-09-28T14:21:12Z", "2023-07-24T05:56:14Z", "2023-07-04T11:11:15Z", "2023-03-24T13:14:43Z", "2023-01-25T12:47:14Z", "2022-11-18T18:34:15Z", "2022-11-10T07:50:15Z", "2022-09-20T09:13:19Z", "2022-09-07T16:22:53Z", "2022-09-16T15:01:09Z", "2022-08-26T13:55:07Z", "2022-07-17T11:23:29Z", "2022-06-01T09:43:25Z", "2022-05-19T19:38:45Z", "2022-01-27T19:24:12Z", "2021-12-15T17:13:46Z", "2021-11-08T09:44:57Z", "2021-11-01T16:54:54Z", "2021-08-31T11:50:49Z", "2021-08-25T13:56:52Z", "2021-07-29T14:55:06Z", "2021-06-24T13:06:02Z", "2021-06-08T07:37:26Z", "2021-05-26T14:48:36Z", "2021-05-21T18:52:25Z", "2021-05-18T17:25:03Z", "2021-05-17T12:43:16Z"]}, {"name": "deepmath", "description": "Experiments towards neural network theorem proving", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Deepmath\n\nThe Deepmath project seeks to improve automated theorem proving using deep\nlearning and other machine learning techniques.  Deepmath is a collaboration\nbetween [Google Research](https://research.google.com) and several universities.\n\n## DISCLAIMER:\n\nThe source code in this repository is not an official Google product, but\nis a research collaboration with external research teams.\n\n## Installation\n\nDeepmath depends on TensorFlow, which is included as a submodule.  Use, or\nsee, the Dockerfile for build instructions for `deephol`, our neural prover. It\nrequires connecting to a proof assistant server. See\nhttps://github.com/brain-research/hol-light for a server implementation.\n", "release_dates": []}, {"name": "docs", "description": "TensorFlow documentation", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Documentation\n\n<div align=\"center\">\n  <img src=\"https://www.tensorflow.org/images/tf_logo_horizontal.png\"><br><br>\n</div>\n\nThese are the source files for the guide and tutorials on\n[tensorflow.org](https://www.tensorflow.org/overview).\n\nTo contribute to the TensorFlow documentation, please read\n[CONTRIBUTING.md](CONTRIBUTING.md), the\n[TensorFlow docs contributor guide](https://www.tensorflow.org/community/contribute/docs),\nand the [style guide](https://www.tensorflow.org/community/contribute/docs_style).\n\nTo file a docs issue, use the issue tracker in the\n[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md) repo.\n\nAnd join the TensorFlow documentation contributors on the\n[TensorFlow Forum](https://discuss.tensorflow.org/).\n\n## Community translations\n\n[Community translations](https://www.tensorflow.org/community/contribute/docs#community_translations)\nare located in the\n[tensorflow/docs-l10n](https://github.com/tensorflow/docs-l10n) repo. These docs\nare contributed, reviewed, and maintained by the community as *best-effort*. To\nparticipate as a translator or reviewer, see the `site/<lang>/README.md`, join\nthe language mailing list, and submit a pull request.\n\n## License\n\n[Apache License 2.0](LICENSE)\n", "release_dates": ["2023-05-25T22:42:23Z"]}, {"name": "docs-l10n", "description": "Translations of TensorFlow documentation", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Docs Translations\n\nThis project contains translations of the technical content and Jupyter\nnotebooks published on [tensorflow.org](https://www.tensorflow.org/guide).\n\nPlease file issues under the *documentation* component of the\n[TensorFlow issue tracker](https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md).\nQuestions about TensorFlow usage are better addressed on the\n[TensorFlow Forum](https://discuss.tensorflow.org/).\n\n## Contributing\n\nContributors are encouraged to use our GitLocalize project to submit pull\nrequests and reviews: https://gitlocalize.com/tensorflow/docs-l10n\n\nGeneral docs instructions are available in the\n[TensorFlow docs contributor guide](https://www.tensorflow.org/community/contribute/docs).\n\nPlease sign a\n[Contributor License Agreement](https://cla.developers.google.com/) (CLA) to\ncontribute to this Google open source project. Check\n[your existing CLA](https://cla.developers.google.com/clas) and verify that\nyour [email is set on git commits](https://docs.github.com/en/github/setting-up-and-managing-your-github-user-account/setting-your-commit-email-address).\n\n## Content\n\nTo view translated content on tensorflow.org, select the in-page language\nswitcher or append `?hl=<lang>` to the URL. For example, the\n[TensorFlow quickstart for beginners](https://www.tensorflow.org/tutorials/quickstart/beginner?hl=en)\ntutorial is available in:\n\n* Korean: https://www.tensorflow.org/tutorials/quickstart/beginner?hl=ko,\n* Spanish: https://www.tensorflow.org/tutorials/quickstart/beginner?hl=es-419,\n* Or any of the supported languages in [site/&lt;lang&gt;](./site/).\n\nIf a human-translation does not exist, some pages fall back to a *machine\ntranslation* (MT). An MT page is indicated with a banner at the top of the page.\nIf the MT page is not useful or confusing, please click the *Switch to English*\nbutton (and consider providing a human translation through the\n[GitLocalize project](https://gitlocalize.com/tensorflow/docs-l10n)).\n\n### Source\n\nSource content is aggregated from multiple GitHub repos into the\n[/site/en-snapshot/](./site/en-snapshot/) directory used for translations.\nTranslations are published to the website on a periodic basis (usually weekly or\nbi-weekly). If you find an error in the source content, please submit a fix to\nthe [upstream repo](./site/en-snapshot/README.md) and *not* the\n`/site/en-snapshot/` directory in this repo.\n\n### Do not translate\n\nNot all content on tensorflow.org is translated in this project (or at all).\nOverview pages and navigation files are translated using another process.\ntensorflow.org does not translate the API reference, old versions, images, or\ntime-sensitive sections like the\n[installation instructions](https://www.tensorflow.org/install). Non-translated\npages are automatically filtered in the\n[GitLocalize](https://gitlocalize.com/tensorflow/docs-l10n) interface.\n\n## Style\n\nThe [TensorFlow docs notebook tools](https://github.com/tensorflow/docs/tree/master/tools/tensorflow_docs/tools)\nare used for formatting and style consistency. This is integrated into the pull\nrequest workflow and can be run locally.\n\nPlease follow the\n[TensorFlow documentation style guide](https://www.tensorflow.org/community/contribute/docs_style)\nand the\n[Google developer docs style guide](https://developers.google.com/style/highlights),\nwhen applicable.\n\n## Languages\n\nOfficial language support is determined by a number of factors including\u2014but not\nlimited to\u2014site metrics and demand, community support,\n[English proficiency](https://en.wikipedia.org/wiki/EF_English_Proficiency_Index),\naudience preference, and other indicators. Since each supported language incurs\na cost, unmaintained languages are removed. Support for new languages will be\nannounced on the [TensorFlow blog](https://blog.tensorflow.org/) or\n[Twitter](https://twitter.com/TensorFlow).\n\nThe [community branch](https://github.com/tensorflow/docs-l10n/tree/community/site)\ncontains community contributed content for languages that are not officially\nsupported by the TensorFlow team. This is an unmaintained archive that you can\nuse for your own open source fork if your preferred language is not supported.\nPlease let us know if you maintain a language! These docs are not published to\ntensorflow.org.\n\n## License\n\n[Apache License 2.0](LICENSE)\n", "release_dates": []}, {"name": "dtensor-gcp-examples", "description": "Using DTensor on Google Cloud", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# DTensor GCP Examples\n\nThis project contains examples of using multi-client DTensor on GCP with a\ncluster of GPUs or TPUs.\n\n\n## Prerequisites\n\n1. gcloud environment on the local console:\n  ```\n  gcloud auth login ...\n  gcloud config set project  ...\n  ```\n\n2. A GCS bucket that the GCE service account can write into. The bucket is used\n  to demo checkpointing. Set the prefix paths name with\n  ```\n  export GCS_BUCKET=<bucket_name>\n  ```\n  or edit bootstrap.sh.\n\n\n# For maintainers\n\nSince this requires tf-nightly, periodically update requirements.txt in\neach directory to known 'good' versions for these examples.\n", "release_dates": []}, {"name": "ecosystem", "description": "Integration of TensorFlow with other open-source frameworks", "language": "Scala", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Ecosystem\n\nThis repository contains examples for integrating TensorFlow with other\nopen-source frameworks. The examples are minimal and intended for use as\ntemplates. Users can tailor the templates for their own use-cases.\n\nIf you have any additions or improvements, please create an issue or pull\nrequest.\n\n## Contents\n\n- [docker](docker) - Docker configuration for running TensorFlow on\n  cluster managers.\n- [kubeflow](https://github.com/kubeflow/kubeflow) - A Kubernetes native platform for ML\n\t* A K8s custom resource for running distributed [TensorFlow jobs](https://github.com/kubeflow/kubeflow/blob/master/user_guide.md#submitting-a-tensorflow-training-job) \n\t* Jupyter images for different versions of TensorFlow\n\t* [TFServing](https://github.com/kubeflow/kubeflow/blob/master/user_guide.md#serve-a-model-using-tensorflow-serving) Docker images and K8s templates\n- [kubernetes](kubernetes) - Templates for running distributed TensorFlow on\n  Kubernetes.\n- [marathon](marathon) - Templates for running distributed TensorFlow using\n  Marathon, deployed on top of Mesos.\n- [hadoop](hadoop) - TFRecord file InputFormat/OutputFormat for Hadoop MapReduce\n  and Spark.\n- [spark-tensorflow-connector](spark/spark-tensorflow-connector) - Spark TensorFlow Connector\n- [spark-tensorflow-distributor](spark/spark-tensorflow-distributor) - Python package that helps users do distributed training with TensorFlow on their Spark clusters.\n\n## Distributed TensorFlow\n\nSee the [Distributed TensorFlow](https://www.tensorflow.org/deploy/distributed)\ndocumentation for a description of how it works. The examples in this\nrepository focus on the most common form of distributed training: between-graph\nreplication with asynchronous updates.\n\n### Common Setup for distributed training\n\nEvery distributed training program has some common setup. First, define flags so\nthat the worker knows about other workers and knows what role it plays in\ndistributed training:\n\n```python\n# Flags for configuring the task\nflags.DEFINE_integer(\"task_index\", None,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Worker task index, should be >= 0. task_index=0 is \"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"the master worker task the performs the variable \"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"initialization.\")\nflags.DEFINE_string(\"ps_hosts\", None,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"worker_hosts\", None,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"job_name\", None, \"job name: worker or ps\")\n```\n\nThen, start your server. Since worker and parameter servers (ps jobs) usually\nshare a common program, parameter servers should stop at this point and so they\nare joined with the server.\n\n```python\n# Construct the cluster and start the server\nps_spec = FLAGS.ps_hosts.split(\",\")\nworker_spec = FLAGS.worker_hosts.split(\",\")\n\ncluster = tf.train.ClusterSpec({\n\u00a0\u00a0\u00a0\u00a0\"ps\": ps_spec,\n\u00a0\u00a0\u00a0\u00a0\"worker\": worker_spec})\n\nserver = tf.train.Server(\n\u00a0\u00a0\u00a0\u00a0cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\n\nif FLAGS.job_name == \"ps\":\n\u00a0\u00a0server.join()\n```\n\nAfterwards, your code varies depending on the form of distributed training you\nintend on doing. The most common form is between-graph replication.\n\n### Between-graph Replication\n\nIn this mode, each worker separately constructs the exact same graph. Each\nworker then runs the graph in isolation, only sharing gradients with the\nparameter servers. This set up is illustrated by the following diagram. Please\nnote that each dashed box indicates a task.\n![Diagram for Between-graph replication](images/between-graph_replication.png \"Between-graph Replication\")\n\nYou must explicitly set the device before graph construction for this mode of\ntraining. The following code snippet from the\n[Distributed TensorFlow tutorial](https://www.tensorflow.org/deploy/distributed)\ndemonstrates the setup:\n\n```python\nwith tf.device(tf.train.replica_device_setter(\n    worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n    cluster=cluster)):\n  # Construct the TensorFlow graph.\n\n# Run the TensorFlow graph.\n```\n\n### Requirements To Run the Examples\n\nTo run our examples, [Jinja templates](http://jinja.pocoo.org/) must be installed:\n\n```sh\n# On Ubuntu\nsudo apt-get install python-jinja2\n\n# On most other platforms\nsudo pip install Jinja2\n```\n\nJinja is used for template expansion. There are other framework-specific\nrequirements, please refer to the README page of each framework.\n", "release_dates": []}, {"name": "embedding-projector-standalone", "description": null, "language": "HTML", "license": null, "readme": null, "release_dates": []}, {"name": "estimator", "description": "TensorFlow Estimator", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "-----------------\n| **`Documentation`** |\n|-----------------|\n| [![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/python/tf/estimator) |\n\nTensorFlow Estimator is a high-level TensorFlow API that greatly simplifies machine learning programming.\nEstimators encapsulate training, evaluation, prediction, and exporting for your model.\n\n## Getting Started\n\nSee our Estimator\n[getting started guide](https://www.tensorflow.org/guide/estimator) for an\nintroduction to the Estimator APIs.\n\n## Installation\n\n`tf.estimator` is installed when you install the TensorFlow pip package. See\n[Installing TensorFlow](https://www.tensorflow.org/install) for instructions.\n\n## Developing\n\nIf you want to build TensorFlow Estimator locally, you will need to\n[install Bazel](https://docs.bazel.build/versions/master/install.html) and\n[install TensorFlow](https://www.tensorflow.org/install/pip).\n\n```sh\n# To build TensorFlow Estimator whl file.\nbazel build //tensorflow_estimator/tools/pip_package:build_pip_package\nbazel-bin/tensorflow_estimator/tools/pip_package/build_pip_package /tmp/estimator_pip\n\n# To run all Estimator tests\nbazel test //tensorflow_estimator/...\n```\n\n## Contribution guidelines\n\nIf you want to contribute to TensorFlow Estimator, be sure to review the [contribution\nguidelines](CONTRIBUTING.md).\n\n**Note that this repository is included as a component of the main TensorFlow\npackage, and any issues encountered while using Estimators should be filed under\n[TensorFlow GitHub Issues](https://github.com/tensorflow/tensorflow/issues),\nas we do not separately track issues in this repository. You can link this\nrepository in any issues created as necessary.**\n\nPlease see\n[TensorFlow Discuss](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss) for general questions\nand discussion and please direct specific questions to\n[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).\n\n## License\n\n[Apache License 2.0](LICENSE)\n", "release_dates": ["2023-11-07T01:02:58Z", "2023-10-17T17:23:10Z", "2023-09-06T23:01:56Z", "2023-08-04T16:55:54Z", "2023-06-27T22:44:35Z", "2023-05-03T00:57:58Z", "2023-03-20T19:07:30Z", "2023-02-03T21:27:31Z", "2022-11-24T14:05:54Z", "2022-10-18T23:35:44Z", "2022-09-02T22:30:43Z", "2022-08-04T15:15:51Z", "2022-05-13T17:26:43Z", "2022-01-31T18:21:12Z", "2021-12-22T21:00:27Z", "2021-10-29T23:23:27Z", "2021-10-08T19:29:05Z", "2021-08-09T17:50:19Z", "2021-06-30T16:58:11Z", "2021-05-14T00:00:20Z", "2021-04-02T18:59:20Z", "2020-12-15T02:25:40Z", "2020-11-03T00:47:01Z", "2020-07-23T00:27:12Z", "2020-06-29T21:09:19Z", "2020-04-16T19:42:42Z", "2020-02-27T20:30:16Z", "2020-03-11T17:13:54Z", "2019-12-13T18:23:59Z", "2019-12-07T00:34:43Z"]}, {"name": "examples", "description": "TensorFlow examples", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Examples\n\n<div align=\"center\">\n  <img src=\"https://www.tensorflow.org/images/tf_logo_social.png\" /><br /><br />\n</div>\n\n<h2>Most important links!</h2>\n\n* [Community examples](./community)\n* [Course materials](./courses/udacity_deep_learning) for the [Deep Learning](https://www.udacity.com/course/deep-learning--ud730) class on Udacity\n\nIf you are looking to learn TensorFlow, don't miss the\n[core TensorFlow documentation](http://github.com/tensorflow/docs)\nwhich is largely runnable code.\nThose notebooks can be opened in Colab from\n[tensorflow.org](https://tensorflow.org).\n\n<h2>What is this repo?</h2>\n\nThis is the TensorFlow example repo.  It has several classes of material:\n\n* Showcase examples and documentation for our fantastic [TensorFlow Community](https://tensorflow.org/community)\n* Provide examples mentioned on TensorFlow.org\n* Publish material supporting official TensorFlow courses\n* Publish supporting material for the [TensorFlow Blog](https://blog.tensorflow.org) and [TensorFlow YouTube Channel](https://youtube.com/tensorflow)\n\nWe welcome community contributions, see [CONTRIBUTING.md](CONTRIBUTING.md) and, for style help,\n[Writing TensorFlow documentation](https://www.tensorflow.org/community/contribute/docs_style)\nguide.\n\nTo file an issue, use the tracker in the\n[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md) repo.\n\n## License\n\n[Apache License 2.0](LICENSE)\n", "release_dates": ["2021-05-16T15:33:53Z", "2021-05-12T08:39:12Z", "2021-02-05T06:54:52Z"]}, {"name": "fairness-indicators", "description": "Tensorflow's Fairness Evaluation and Visualization Toolkit", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Fairness Indicators\n\n![Fairness_Indicators](https://raw.githubusercontent.com/tensorflow/fairness-indicators/master/fairness_indicators/images/fairnessIndicators.png)\n\nFairness Indicators is designed to support teams in evaluating, improving, and comparing models for fairness concerns in partnership with the broader Tensorflow toolkit.\n\nThe tool is currently actively used internally by many of our products. We would love to partner with you to understand where Fairness Indicators is most useful, and where added functionality would be valuable. Please reach out at tfx@tensorflow.org. You can provide feedback and feature requests [here](https://github.com/tensorflow/fairness-indicators/issues/new/choose).\n\n## Key links\n* [Introductory Video](https://www.youtube.com/watch?v=pHT-ImFXPQo)\n* [Fairness Indicators Case Study](https://developers.google.com/machine-learning/practica/fairness-indicators?utm_source=github&utm_medium=github&utm_campaign=fi-practicum&utm_term=&utm_content=repo-body)\n* [Fairness Indicators Example Colab](https://colab.research.google.com/github/tensorflow/fairness-indicators/blob/master/g3doc/tutorials/Fairness_Indicators_Example_Colab.ipynb)\n* [Pandas DataFrame to Fairness Indicators Case Study](https://colab.research.google.com/github/tensorflow/fairness-indicators/blob/master/g3doc/tutorials/Fairness_Indicators_Pandas_Case_Study.ipynb)\n* [Fairness Indicators: Thinking about Fairness Evaluation](https://github.com/tensorflow/fairness-indicators/blob/master/g3doc/guide/guidance.md)\n\n## What is Fairness Indicators?\nFairness Indicators enables easy computation of commonly-identified fairness metrics for **binary** and **multiclass** classifiers.\n\nMany existing tools for evaluating fairness concerns don\u2019t work well on large-scale datasets and models. At Google, it is important for us to have tools that can work on billion-user systems. Fairness Indicators will allow you to evaluate fairenss metrics across any size of use case.\n\nIn particular, Fairness Indicators includes the ability to:\n\n* Evaluate the distribution of datasets\n* Evaluate model performance, sliced across defined groups of users\n  * Feel confident about your results with confidence intervals and evals at multiple thresholds\n* Dive deep into individual slices to explore root causes and opportunities for improvement\n\nThis [case study](https://developers.google.com/machine-learning/practica/fairness-indicators?utm_source=github&utm_medium=github&utm_campaign=fi-practicum&utm_term=&utm_content=repo-body), complete with [videos](https://www.youtube.com/watch?v=pHT-ImFXPQo) and programming exercises, demonstrates how Fairness Indicators can be used on one of your own products to evaluate fairness concerns over time.\n\n[![](http://img.youtube.com/vi/pHT-ImFXPQo/0.jpg)](http://www.youtube.com/watch?v=pHT-ImFXPQo \"\")\n\n## [Installation](https://pypi.org/project/fairness-indicators/)\n\n`pip install fairness-indicators`\n\nThe pip package includes:\n\n* [**Tensorflow Data Validation (TFDV)**](https://github.com/tensorflow/data-validation) - analyze the distribution of your dataset\n* [**Tensorflow Model Analysis (TFMA)**](https://github.com/tensorflow/model-analysis) - analyze model performance\n  * **Fairness Indicators** - an addition to TFMA that adds fairness metrics and easy performance comparison across slices\n* **The What-If Tool (WIT)**](https://github.com/PAIR-code/what-if-tool - an interactive visual interface designed to probe your models better\n\n### Nightly Packages\n\nFairness Indicators also hosts nightly packages at\nhttps://pypi-nightly.tensorflow.org on Google Cloud. To install the latest\nnightly package, please use the following command:\n\n```bash\npip install --extra-index-url https://pypi-nightly.tensorflow.org/simple fairness-indicators\n```\n\nThis will install the nightly packages for the major dependencies of Fairness\nIndicators such as TensorFlow Data Validation (TFDV), TensorFlow Model Analysis\n(TFMA).\n\n## How can I use Fairness Indicators?\nTensorflow Models\n\n* Access Fairness Indicators as part of the Evaluator component in Tensorflow Extended \\[[docs](https://www.tensorflow.org/tfx/guide/evaluator)]\n* Access Fairness Indicators in Tensorboard when evaluating other real-time metrics \\[[docs](https://github.com/tensorflow/tensorboard/blob/master/docs/fairness-indicators.md)]\n\nNot using existing Tensorflow tools? No worries!\n\n* Download the Fairness Indicators pip package, and use Tensorflow Model Analysis as a standalone tool \\[[docs](https://www.tensorflow.org/tfx/guide/fairness_indicators)]\n* Model Agnostic TFMA enables you to compute Fairness Indicators based on the output of any model \\[[docs](https://www.tensorflow.org/tfx/guide/fairness_indicators)]\n\n## [Examples](https://github.com/tensorflow/fairness-indicators/tree/master/g3doc/tutorials) directory contains several examples.\n\n* [Fairness_Indicators_Example_Colab.ipynb](https://github.com/tensorflow/fairness-indicators/blob/master/g3doc/tutorials/Fairness_Indicators_Example_Colab.ipynb) gives an overview of Fairness Indicators in [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/guide/tfma) and how to use it with a real dataset. This notebook also goes over [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) and [What-If Tool](https://pair-code.github.io/what-if-tool/), two tools for analyzing TensorFlow models that are packaged with Fairness Indicators.\n* [Fairness_Indicators_on_TF_Hub.ipynb](https://github.com/tensorflow/fairness-indicators/blob/master/g3doc/tutorials/Fairness_Indicators_on_TF_Hub_Text_Embeddings.ipynb) demonstrates how to use Fairness Indicators to compare models trained on different [text embeddings](https://en.wikipedia.org/wiki/Word_embedding). This notebook uses text embeddings from [TensorFlow Hub](https://www.tensorflow.org/hub), TensorFlow's library to publish, discover, and reuse model components.\n* [Fairness_Indicators_TensorBoard_Plugin_Example_Colab.ipynb](https://github.com/tensorflow/fairness-indicators/blob/master/g3doc/tutorials/Fairness_Indicators_TensorBoard_Plugin_Example_Colab.ipynb)\ndemonstrates how to visualize Fairness Indicators in TensorBoard.\n\n## More questions?\nFor more information on how to think about fairness evaluation in the context of your use case, see [this link](https://github.com/tensorflow/fairness-indicators/blob/master/g3doc/guide/guidance.md).\n\nIf you have found a bug in Fairness Indicators, please file a [GitHub issue](https://github.com/tensorflow/fairness-indicators/issues/new/choose) with as much supporting information as you can provide.\n\n## Compatible versions\n\nThe following table shows the  package versions that are\ncompatible with each other. This is determined by our testing framework, but\nother *untested* combinations may also work.\n\n|fairness-indicators                                                                        | tensorflow         | tensorflow-data-validation | tensorflow-model-analysis |\n|-------------------------------------------------------------------------------------------|--------------------|----------------------------|---------------------------|\n|[GitHub master](https://github.com/tensorflow/fairness-indicators/blob/master/RELEASE.md)  | nightly (1.x/2.x)  | 1.13.0                     | 0.44.0                    |\n|[v0.44.0](https://github.com/tensorflow/fairness-indicators/blob/v0.44.0/RELEASE.md)       | 2.12               | 1.13.0                     | 0.44.0                    |\n|[v0.43.0](https://github.com/tensorflow/fairness-indicators/blob/v0.43.0/RELEASE.md)       | 2.11               | 1.12.0                     | 0.43.0                    |\n|[v0.42.0](https://github.com/tensorflow/fairness-indicators/blob/v0.42.0/RELEASE.md)       | 1.15.5 / 2.10      | 1.11.0                     | 0.42.0                    |\n|[v0.41.0](https://github.com/tensorflow/fairness-indicators/blob/v0.41.0/RELEASE.md)       | 1.15.5 / 2.9       | 1.10.0                     | 0.41.0                    |\n|[v0.40.0](https://github.com/tensorflow/fairness-indicators/blob/v0.40.0/RELEASE.md)       | 1.15.5 / 2.9       | 1.9.0                      | 0.40.0                    |\n|[v0.39.0](https://github.com/tensorflow/fairness-indicators/blob/v0.39.0/RELEASE.md)       | 1.15.5 / 2.8       | 1.8.0                      | 0.39.0                    |\n|[v0.38.0](https://github.com/tensorflow/fairness-indicators/blob/v0.38.0/RELEASE.md)       | 1.15.5 / 2.8       | 1.7.0                      | 0.38.0                    |\n|[v0.37.0](https://github.com/tensorflow/fairness-indicators/blob/v0.37.0/RELEASE.md)       | 1.15.5 / 2.7       | 1.6.0                      | 0.37.0                    |\n|[v0.36.0](https://github.com/tensorflow/fairness-indicators/blob/v0.36.0/RELEASE.md)       | 1.15.2 / 2.7       | 1.5.0                      | 0.36.0                    |\n|[v0.35.0](https://github.com/tensorflow/fairness-indicators/blob/v0.35.0/RELEASE.md)       | 1.15.2 / 2.6       | 1.4.0                      | 0.35.0                    |\n|[v0.34.0](https://github.com/tensorflow/fairness-indicators/blob/v0.34.0/RELEASE.md)       | 1.15.2 / 2.6       | 1.3.0                      | 0.34.0                    |\n|[v0.33.0](https://github.com/tensorflow/fairness-indicators/blob/v0.33.0/RELEASE.md)       | 1.15.2 / 2.5       | 1.2.0                      | 0.33.0                    |\n|[v0.30.0](https://github.com/tensorflow/fairness-indicators/blob/v0.30.0/RELEASE.md)       | 1.15.2 / 2.4       | 0.30.0                     | 0.30.0                    |\n|[v0.29.0](https://github.com/tensorflow/fairness-indicators/blob/v0.29.0/RELEASE.md)       | 1.15.2 / 2.4       | 0.29.0                     | 0.29.0                    |\n|[v0.28.0](https://github.com/tensorflow/fairness-indicators/blob/v0.28.0/RELEASE.md)       | 1.15.2 / 2.4       | 0.28.0                     | 0.28.0                    |\n|[v0.27.0](https://github.com/tensorflow/fairness-indicators/blob/v0.27.0/RELEASE.md)       | 1.15.2 / 2.4       | 0.27.0                     | 0.27.0                    |\n|[v0.26.0](https://github.com/tensorflow/fairness-indicators/blob/v0.26.0/RELEASE.md)       | 1.15.2 / 2.3       | 0.26.0                     | 0.26.0                    |\n|[v0.25.0](https://github.com/tensorflow/fairness-indicators/blob/v0.25.0/RELEASE.md)       | 1.15.2 / 2.3       | 0.25.0                     | 0.25.0                    |\n|[v0.24.0](https://github.com/tensorflow/fairness-indicators/blob/v0.24.0/RELEASE.md)       | 1.15.2 / 2.3       | 0.24.0                     | 0.24.0                    |\n|[v0.23.0](https://github.com/tensorflow/fairness-indicators/blob/v0.23.0/RELEASE.md)       | 1.15.2 / 2.3       | 0.23.0                     | 0.23.0                    |\n", "release_dates": ["2023-05-03T22:54:39Z", "2022-12-13T18:57:59Z", "2022-11-22T20:10:46Z", "2022-10-05T23:04:26Z", "2022-07-20T07:04:00Z", "2022-05-27T17:02:06Z", "2022-03-07T20:04:18Z", "2022-02-02T23:04:26Z", "2021-12-03T20:24:45Z", "2021-11-12T23:47:14Z", "2021-09-21T18:23:08Z", "2021-08-09T17:33:50Z", "2021-05-25T23:30:54Z", "2021-05-17T19:48:11Z", "2021-05-14T19:27:09Z", "2021-04-08T23:18:26Z", "2021-03-09T20:20:07Z", "2021-01-28T22:02:14Z", "2020-12-17T03:24:04Z", "2020-11-05T19:40:34Z", "2020-09-15T20:10:29Z", "2020-08-25T22:14:47Z", "2020-08-18T01:21:33Z"]}, {"name": "federated", "description": "A framework for implementing federated learning", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Federated\n\nTensorFlow Federated (TFF) is an open-source framework for machine learning and\nother computations on decentralized data. TFF has been developed to facilitate\nopen research and experimentation with\n[Federated Learning (FL)](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html),\nan approach to machine learning where a shared global model is trained across\nmany participating clients that keep their training data locally. For example,\nFL has been used to train\n[prediction models for mobile keyboards](https://arxiv.org/abs/1811.03604)\nwithout uploading sensitive typing data to servers.\n\nTFF enables developers to use the included federated learning algorithms with\ntheir models and data, as well as to experiment with novel algorithms. The\nbuilding blocks provided by TFF can also be used to implement non-learning\ncomputations, such as aggregated analytics over decentralized data.\n\nTFF's interfaces are organized in two layers:\n\n*   [Federated Learning (FL) API](https://github.com/tensorflow/federated/blob/main/docs/federated_learning.md)\n    The `tff.learning` layer offers a set of high-level interfaces that allow\n    developers to apply the included implementations of federated training and\n    evaluation to their existing TensorFlow models.\n\n*   [Federated Core (FC) API](https://github.com/tensorflow/federated/blob/main/docs/federated_core.md)\n    At the core of the system is a set of lower-level interfaces for concisely\n    expressing novel federated algorithms by combining TensorFlow with\n    distributed communication operators within a strongly-typed functional\n    programming environment. This layer also serves as the foundation upon which\n    we've built `tff.learning`.\n\nTFF enables developers to declaratively express federated computations, so they\ncould be deployed to diverse runtime environments. Included with TFF is a\nsingle-machine simulation runtime for experiments. Please visit the tutorials\nand try it out yourself!\n\n## Installation\n\nSee the\n[install](https://github.com/tensorflow/federated/blob/main/docs/install.md)\ndocumentation for instructions on how to install TensorFlow Federated as a\npackage or build TensorFlow Federated from source.\n\n## Getting Started\n\nSee the\n[get started](https://github.com/tensorflow/federated/blob/main/docs/get_started.md)\ndocumentation for instructions on how to use TensorFlow Federated.\n\n## Contributing\n\nThere are a number of ways to contribute depending on what you're interested in:\n\n*   If you are interested in developing new federated learning algorithms, the\n    best way to start would be to study the implementations of federated\n    averaging and evaluation in `tff.learning`, and to think of extensions to\n    the existing implementation (or alternative approaches). If you have a\n    proposal for a new algorithm, we recommend starting by staging your project\n    in the `research` directory and including a colab notebook to showcase the\n    new features.\n\n    You may want to also develop new algorithms in your own repository. We are\n    happy to feature pointers to academic publications and/or repos using TFF on\n    [tensorflow.org/federated](http://www.tensorflow.org/federated).\n\n*   If you are interested in applying federated learning, consider contributing\n    a tutorial, a new federated dataset, or an example model that others could\n    use for experiments and testing, or writing helper classes that others can\n    use in setting up simulations.\n\n*   If you are interested in helping us improve the developer experience, the\n    best way to start would be to study the implementations behind the\n    `tff.learning` API, and to reflect on how we could make the code more\n    streamlined. You could contribute helper classes that build upon the FC API\n    or suggest extensions to the FC API itself.\n\n*   If you are interested in helping us develop runtime infrastructure for\n    simulations and beyond, please wait for a future release in which we will\n    introduce interfaces and guidelines for contributing to a simulation\n    infrastructure.\n\nPlease be sure to review the\n[contribution](https://github.com/tensorflow/federated/blob/main/CONTRIBUTING.md#guidelines)\nguidelines on how to contribute.\n\n## Issues\n\nUse [GitHub issues](https://github.com/tensorflow/federated/issues) for tracking\nrequests and bugs.\n\n## Questions\n\nPlease direct questions to [Stack Overflow](https://stackoverflow.com) using the\n[tensorflow-federated](https://stackoverflow.com/questions/tagged/tensorflow-federated)\ntag.\n", "release_dates": ["2024-02-27T16:50:10Z", "2024-02-13T21:25:43Z", "2024-02-02T19:02:30Z", "2024-01-23T19:39:17Z", "2024-01-09T02:42:07Z", "2023-12-15T22:00:38Z", "2023-11-30T15:46:45Z", "2023-11-08T17:41:59Z", "2023-10-06T20:49:05Z", "2023-08-14T22:14:38Z", "2023-07-18T18:00:11Z", "2023-06-30T22:48:10Z", "2023-06-05T23:37:12Z", "2023-05-18T19:04:15Z", "2023-05-09T15:05:41Z", "2023-04-20T16:35:21Z", "2023-04-11T23:31:13Z", "2023-04-04T17:22:44Z", "2023-03-28T01:28:12Z", "2023-03-20T19:49:10Z", "2023-03-13T17:00:41Z", "2023-02-23T18:16:42Z", "2023-02-14T16:21:46Z", "2023-01-31T17:33:18Z", "2023-01-23T23:18:33Z", "2023-01-18T22:08:24Z", "2023-01-13T00:20:42Z", "2022-12-27T20:00:06Z", "2022-12-17T00:17:30Z", "2022-12-14T18:00:36Z"]}, {"name": "flutter-tflite", "description": null, "language": "Dart", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": " <p align=\"center\">\n    <br>\n    <img src=\"https://github.com/am15h/tflite_flutter_plugin/raw/update_readme/docs/tflite_flutter_cover.png\"/>\n    </br>\n</p>\n<p align=\"center\">\n \n   <a href=\"https://flutter.dev\">\n     <img src=\"https://img.shields.io/badge/Platform-Flutter-02569B?logo=flutter\"\n       alt=\"Platform\" />\n   </a>\n   <a href=\"https://pub.dartlang.org/packages/tflite_flutter\">\n     <img src=\"https://img.shields.io/pub/v/tflite_flutter.svg\"\n       alt=\"Pub Package\" />\n   </a>\n    <a href=\"https://pub.dev/documentation/tflite_flutter/latest/tflite_flutter/tflite_flutter-library.html\">\n        <img alt=\"Docs\" src=\"https://readthedocs.org/projects/hubdb/badge/?version=latest\">\n    </a>\n    <a href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\"></a>\n\n\n</a>\n</p>\n\n## Announcement\n\nUpdate: 26 April, 2023\n\nThis repo is a TensorFlow managed fork of the [tflite_flutter_plugin](https://github.com/am15h/tflite_flutter_plugin) project by the amazing Amish Garg. The goal of this project is to support our Flutter community in creating machine-learning backed apps with the TensorFlow Lite framework.\n\nThis project is currently a work-in-progress as we update it to create a working plugin that meets the latest and greatest Flutter and TensorFlow Lite standards. That said, *pull requests and contributions are more than welcome* and will be reviewed by TensorFlow or Flutter team members. We thank you for your understanding as we make progress on this update.\n\nFeel free to reach out to us by posting in the issues or discussion areas.\n\nThanks!\n\n- PaulTR\n\n## Overview\n\nTensorFlow Lite Flutter plugin provides a flexible and fast solution for accessing TensorFlow Lite interpreter and performing inference. The API is similar to the TFLite Java and Swift APIs. It directly binds to TFLite C API making it efficient (low-latency). Offers acceleration support using NNAPI, GPU delegates on Android, Metal and CoreML delegates on iOS, and XNNPack delegate on Desktop platforms.\n\n\n## Key Features\n\n* Multi-platform Support for Android and iOS\n* Flexibility to use any TFLite Model.\n* Acceleration using multi-threading.\n* Similar structure as TensorFlow Lite Java API.\n* Inference speeds close to native Android Apps built using the Java API.\n* Run inference in different isolates to prevent jank in UI thread.\n\n\n## (Important) Initial setup : Add dynamic libraries to your app\n\n### Android & iOS\n\nExamples and support now support dynamic library downloads! iOS samples can be run with the commands\n\n`flutter build ios` & `flutter install ios` from their respective iOS folders.\n\nAndroid can be run with the commands\n\n`flutter build android` & `flutter install android`\n\nwhile devices are plugged in.\n\nNote: This requires a device with a minimum API level of 26.\n\nNote: TFLite may not work in the iOS simulator. It's recommended that you test with a physical device.\n\nWhen creating a release archive (IPA), the symbols are stripped by Xcode, so the command `flutter build ipa` may throw a `Failed to lookup symbol ... symbol not found` error. To work around this:\n\n1. In Xcode, go to **Target Runner > Build Settings > Strip Style**\n2. Change from **All Symbols** to **Non-Global Symbols**\n\n### MacOS\n\nFor MacOS a TensorFlow Lite dynamic library needs to be added to the project manually.\nFor this, first a `.dylib` needs to be built. You can follow the [Bazel build guide](https://www.tensorflow.org/lite/guide/build_arm) or the [CMake build guide](https://www.tensorflow.org/lite/guide/build_cmake) to build the libraries.\n\n**CMake Note:**\n\n- cross compiling in CMake can be achieved using:\n`-DCMAKE_OSX_ARCHITECTURES=x86_64|arm64`\n\n- bundling two architectures (arm / x86) using lipo:\n`lipo -create arm64/libtensorflowlite_c.dylib x86/libtensorflowlite_c.dylib -output libtensorflowlite_c.dylib`\n\nAs a second step, the library needs to be added to your application's XCode project. For this, you can follow the step 1 and 2 of the [official Flutter guide on adding dynamic libraries](https://docs.flutter.dev/platform-integration/macos/c-interop#compiled-dynamic-library-macos).\n\n### Linux\n\nFor Linux a TensorFlow Lite dynamic library needs to be added to the project manually.\nFor this, first a `.so` needs to be built. You can follow the [Bazel build guide](https://www.tensorflow.org/lite/guide/build_arm) or the [CMake build guide](https://www.tensorflow.org/lite/guide/build_cmake) to build the libraries.\n\nAs a second step, the library needs to be added to your application's project. This is a simple procedure\n\n1. Create a folder called `blobs` in the top level of your project\n2. Copy the `libtensorflowlite_c-linux.so` to this folder\n3. Append following lines to your `linux/CMakeLists.txt`\n\n``` Make\n...\n\n# get tf lite binaries\ninstall(\n  FILES ${PROJECT_BUILD_DIR}/../blobs/libtensorflowlite_c-linux.so\n  DESTINATION ${INSTALL_BUNDLE_DATA_DIR}/../blobs/\n)\n```\n\n### Windows\n\nFor Windows a TensorFlow Lite dynamic library needs to be added to the project manually.\nFor this, first a `.dll` needs to be built. You can follow the [Bazel build guide](https://www.tensorflow.org/lite/guide/build_arm) or the [CMake build guide](https://www.tensorflow.org/lite/guide/build_cmake) to build the libraries.\n\nAs a second step, the library needs to be added to your application's project. This is a simple procedure\n\n1. Create a folder called `blobs` in the top level of your project\n2. Copy the `libtensorflowlite_c-win.dll` to this folder\n3. Append following lines to your `windows/CMakeLists.txt`\n\n``` Make\n...\n\n# get tf lite binaries\ninstall(\n  FILES ${PROJECT_BUILD_DIR}/../blobs/libtensorflowlite_c-win.dll \n  DESTINATION ${INSTALL_BUNDLE_DATA_DIR}/../blobs/\n)\n```\n\n## TFLite Flutter Helper Library\n\nThe helper library has been deprecated. New development underway for a replacement at https://github.com/google/flutter-mediapipe. Current timeline is to have wide support by the end of August, 2023.\n\n## Import\n\n```dart\nimport 'package:tflite_flutter/tflite_flutter.dart';\n```\n\n## Usage instructions\n\n### Import the libraries\nIn the dependency section of `pubspec.yaml` file, add `tflite_flutter: ^0.10.1` (adjust the version accordingly based on the latest release)\n\n### Creating the Interpreter\n\n* **From asset**\n\n    Place `your_model.tflite` in `assets` directory. Make sure to include assets in `pubspec.yaml`.\n\n    ```dart\n    final interpreter = await tfl.Interpreter.fromAsset('assets/your_model.tflite');\n    ```\n\nRefer to the documentation for info on creating interpreter from buffer or file.\n\n### Performing inference\n\n* **For single input and output**\n\n    Use `void run(Object input, Object output)`.\n    ```dart\n    // For ex: if input tensor shape [1,5] and type is float32\n    var input = [[1.23, 6.54, 7.81, 3.21, 2.22]];\n\n    // if output tensor shape [1,2] and type is float32\n    var output = List.filled(1*2, 0).reshape([1,2]);\n\n    // inference\n    interpreter.run(input, output);\n\n    // print the output\n    print(output);\n    ```\n  \n* **For multiple inputs and outputs**\n\n    Use `void runForMultipleInputs(List<Object> inputs, Map<int, Object> outputs)`.\n\n    ```dart\n    var input0 = [1.23];  \n    var input1 = [2.43];  \n\n    // input: List<Object>\n    var inputs = [input0, input1, input0, input1];  \n\n    var output0 = List<double>.filled(1, 0);  \n    var output1 = List<double>.filled(1, 0);\n\n    // output: Map<int, Object>\n    var outputs = {0: output0, 1: output1};\n\n    // inference  \n    interpreter.runForMultipleInputs(inputs, outputs);\n\n    // print outputs\n    print(outputs)\n    ```\n\n### Closing the interpreter\n\n```dart\ninterpreter.close();\n```\n\n### Asynchronous Inference with `IsolateInterpreter`\n\nTo utilize asynchronous inference, first create your `Interpreter` and then wrap it with `IsolateInterpreter`.\n\n```dart\nfinal interpreter = await Interpreter.fromAsset('assets/your_model.tflite');\nfinal isolateInterpreter =\n        await IsolateInterpreter.create(address: interpreter.address);\n```\n\nBoth `run` and `runForMultipleInputs` methods of `isolateInterpreter` are asynchronous:\n\n```dart\nawait isolateInterpreter.run(input, output);\nawait isolateInterpreter.runForMultipleInputs(inputs, outputs);\n```\n\nBy using `IsolateInterpreter`, the inference runs in a separate isolate. This ensures that the main isolate, responsible for UI tasks, remains unblocked and responsive.\n", "release_dates": []}, {"name": "fold", "description": "Deep learning with dynamic computation graphs in TensorFlow", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Fold\n\nTensorFlow Fold is a library for\ncreating [TensorFlow](https://www.tensorflow.org) models that consume structured\ndata, where the structure of the computation graph depends on the structure of\nthe input data. For example, [this model](tensorflow_fold/g3doc/sentiment.ipynb)\nimplements [TreeLSTMs](https://arxiv.org/abs/1503.00075) for sentiment analysis\non parse trees of arbitrary shape/size/depth.\n\nFold implements [*dynamic batching*](https://arxiv.org/abs/1702.02181).\nBatches of arbitrarily shaped computation graphs are transformed to produce a\nstatic computation graph. This graph has the same structure regardless of what\ninput it receives, and can be executed efficiently by TensorFlow.\n\n* [Download and Setup](tensorflow_fold/g3doc/setup.md)\n* [Quick Start Notebook](tensorflow_fold/g3doc/quick.ipynb)\n* [Documentation](tensorflow_fold/g3doc/index.md)\n\n![animation](tensorflow_fold/g3doc/animation.gif)  \n\nThis animation shows a [recursive neural network](https://en.wikipedia.org/wiki/Recursive_neural_network) run with dynamic batching. Operations of the same type appearing at the same depth in the computation graph (indicated by color in the animiation) are batched together regardless of whether or not they appear in the same parse tree. The [Embed](tensorflow_fold/g3doc/py/td.md#td.Embedding) operation converts [words to vector representations](https://www.tensorflow.org/tutorials/word2vec/). The fully connected ([FC](tensorflow_fold/g3doc/py/td.md#td.FC)) operation combines word vectors to form vector representations of phrases. The output of the network is a vector representation of an entire sentence.  Although only a single parse tree of a sentence is shown, the same network can run, and batch together operations, over multiple parse trees of arbitrary shapes and sizes. The TensorFlow `concat`, `while_loop`, and `gather` ops are created once, prior to variable initialization, by [Loom](tensorflow_fold/g3doc/py/loom.md), the low-level API for TensorFlow Fold.\n\nIf you'd like to contribute to TensorFlow Fold, please review the\n[contribution guidelines](CONTRIBUTING.md).\n  \nTensorFlow Fold is not an official Google product.\n", "release_dates": []}, {"name": "gan", "description": "Tooling for GANs in TensorFlow", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow-GAN (TF-GAN)\n\nTF-GAN is a lightweight library for training and evaluating\n[Generative Adversarial Networks (GANs)](https://arxiv.org/abs/1406.2661).\n\n\n*   Can be installed with `pip` using `pip install tensorflow-gan`, and used\n    with `import tensorflow_gan as tfgan`\n*   [Well-tested examples](https://github.com/tensorflow/gan/tree/master/tensorflow_gan/examples/)\n*   [Interactive introduction to TF-GAN](https://github.com/tensorflow/gan/blob/master/tensorflow_gan/examples/colab_notebooks/tfgan_tutorial.ipynb) in\n\n## Structure of the TF-GAN Library\n\nTF-GAN is composed of several parts, which are designed to exist independently:\n\n*   [Core](https://github.com/tensorflow/gan/tree/master/tensorflow_gan/python/train.py):\n    the main infrastructure needed to train a GAN. Set up training with any\n    combination of TF-GAN library calls, custom-code, native TF code, and other\n    frameworks\n*   [Features](https://github.com/tensorflow/gan/tree/master/tensorflow_gan/python/features/):\n    common GAN operations and normalization techniques, such as instance\n    normalization and conditioning.\n*   [Losses](https://github.com/tensorflow/gan/tree/master/tensorflow_gan/python/losses/):\n    losses and penalties, such as the Wasserstein loss, gradient penalty, mutual\n    information penalty, etc.\n*   [Evaluation](https://github.com/tensorflow/gan/tree/master/tensorflow_gan/python/eval/):\n    standard GAN evaluation metrics. Use `Inception Score`, `Frechet Distance`,\n    or `Kernel Distance` with a pretrained Inception network to evaluate your\n    unconditional generative model. You can also use your own pretrained\n    classifier for more specific performance numbers, or use other methods for\n    evaluating conditional generative models.\n*   [Examples](https://github.com/tensorflow/gan/tree/master/tensorflow_gan/):\n    simple examples on how to use TF-GAN, and more complicated state-of-the-art\n    examples\n\n## Who uses TF-GAN?\n\nNumerous projects inside Google. The following are some published papers that\nuse TF-GAN:\n\n*   [Self-Attention Generative Adversarial Networks](https://arxiv.org/abs/1805.08318)\n*   [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096)\n*   [GANSynth: Adversarial Neural Audio Synthesis](https://arxiv.org/abs/1902.08710)\n*   [Boundless: Generative Adversarial Networks for Image Extension](http://arxiv.org/abs/1908.07007)\n*   [NetGAN: Generating Graphs via Random Walks](https://arxiv.org/abs/1803.00816)\n*   [Discriminator rejection sampling](https://arxiv.org/abs/1810.06758)\n*   [Generative Models for Effective ML on Private, Decentralized Datasets](https://arxiv.org/pdf/1911.06679.pdf)\n*   [Semantic Pyramid for Image Generation](https://arxiv.org/abs/2003.06221)\n*   [GAN-Mediated Cell Images Batch Equalization](https://www.biorxiv.org/content/10.1101/2020.02.07.939215v1.full)\n\nThe framework [Compare GAN](https://github.com/google/compare_gan) uses TF-GAN,\nespecially the evaluation metrics.\n[Their papers](https://github.com/google/compare_gan#compare-gan) use TF-GAN to\nensure consistent and comparable evaluation metrics. Some of those papers are:\n\n*   [Are GANs Created Equal? A Large-Scale Study](https://arxiv.org/abs/1711.10337)\n*   [The GAN Landscape: Losses, Architectures, Regularization, and Normalization](https://arxiv.org/abs/1807.04720)\n*   [Assessing Generative Models via Precision and Recall](https://arxiv.org/abs/1806.00035)\n*   [High-Fidelity Image Generation With Fewer Labels](https://arxiv.org/abs/1903.02271)\n\n## Training a GAN model\n\nTraining in TF-GAN typically consists of the following steps:\n\n1.  Specify the input to your networks.\n1.  Set up your generator and discriminator using a `GANModel`.\n1.  Specify your loss using a `GANLoss`.\n1.  Create your train ops using a `GANTrainOps`.\n1.  Run your train ops.\n\nAt each stage, you can either use TF-GAN's convenience functions, or you can\nperform the step manually for fine-grained control.\n\nThere are various types of GAN setup. For instance, you can train a generator to\nsample unconditionally from a learned distribution, or you can condition on\nextra information such as a class label. TF-GAN is compatible with many setups,\nand we demonstrate in the well-tested\n[examples directory](https://github.com/tensorflow/gan/tree/master/tensorflow_gan/examples/)\n\n\n## Maintainers\n\n*   (Documentation) David Westbrook, westbrook@google.com\n*   Joel Shor, joelshor@google.com, [github](https://github.com/joel-shor)\n*   Aaron Sarna, sarna@google.com, [github](https://github.com/aaronsarna)\n*   Yoel Drori, dyoel@google.com, [github](https://github.com/yoeldr)\n\n## Authors\n\n*   Joel Shor, joelshor@google.com, [github](https://github.com/joel-shor)\n", "release_dates": []}, {"name": "gnn", "description": "TensorFlow GNN is a library to build Graph Neural Networks on the TensorFlow platform.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow GNN\n\n## Summary\n\nTensorFlow GNN is a library to build\n[Graph Neural Networks](tensorflow_gnn/docs/guide/intro.md) on the TensorFlow platform.\nIt provides...\n\n  * a [`tfgnn.GraphTensor`](tensorflow_gnn/docs/guide/graph_tensor.md) type to represent\n    graphs with a [heterogeneous schema](tensorflow_gnn/docs/guide/schema.md), that is,\n    multiple types of nodes and edges;\n  * tools for [data preparation](tensorflow_gnn/docs/guide/data_prep.md),\n    notably a [graph sampler](tensorflow_gnn/docs/guide/beam_sampler.md)\n    to convert a huge database into a stream of reasonably-sized subgraphs for\n    training and inference;\n  * a collection of [ready-to-use models](tensorflow_gnn/models/README.md)\n    and Keras layers to do your own [GNN modeling](tensorflow_gnn/docs/guide/gnn_modeling.md);\n  * a high-level API for training [orchestration](tensorflow_gnn/docs/guide/runner.md).\n\nThis library is an OSS port of a Google-internal library used in a broad variety\nof contexts, on homogeneous and heterogeneous graphs, and in conjunction with\nother scalable graph mining tools.\n\nFor background and discussion, please see O. Ferludin et al.:\n[TF-GNN: Graph Neural Networks in TensorFlow](https://arxiv.org/abs/2207.03522),\n2023 (full citation below).\n\n## Quickstart\n\nGoogle Colab lets you run TF-GNN demos from your browser, no installation\nrequired:\n\n  * [Molecular Graph\n    Classification](https://colab.research.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/intro_mutag_example.ipynb)\n     with the MUTAG dataset.\n  * [Solving OGBN-MAG\n    end-to-end](https://colab.research.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb)\n    trains a model on heterogeneous sampled subgraphs from the popular\n    [OGBN-MAG](https://ogb.stanford.edu/docs/nodeprop/#ogbn-mag) benchmark.\n  * [Learning shortest paths with\n    GraphNetworks](https://colab.research.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/graph_network_shortest_path.ipynb)\n    demonstrates an advanced Encoder/Process/Decoder architecture for predicting\n    the edges of a shortest path.\n\nFor all colabs and user guides, please see the\n[Documentation overview](tensorflow_gnn/docs/guide/overview.md)\npage, which also links to the\n[API docs](tensorflow_gnn/docs/api_docs/README.md).\n\n## Installation Instructions\n\nThe latest stable release of TensorFlow GNN is available from\n\n```\npip install tensorflow_gnn\n```\n\nFor installation from source, see our [Developer\nGuide](tensorflow_gnn/docs/guide/developer.md).\n\nKey platform requirements:\n\n  * TensorFlow 2.12, 2.13, 2.14 or 2.15, and any GPU drivers it needs\n    [[instructions](https://www.tensorflow.org/install)].\n  * Keras v2, as traditionally included with TensorFlow 2.x.\n    (TF-GNN does not work with the new multi-backend Keras v3.)\n  * Apache Beam for distributed graph sampling.\n\nTF-GNN is developed and tested on Linux. Running on other platforms supported\nby TensorFlow may be possible.\n\n## Citation\n\nWhen referencing this library in a paper, please cite the\n[TF-GNN paper](https://arxiv.org/abs/2207.03522):\n\n```\n@article{tfgnn,\n  author  = {Oleksandr Ferludin and Arno Eigenwillig and Martin Blais and\n             Dustin Zelle and Jan Pfeifer and Alvaro Sanchez{-}Gonzalez and\n             Wai Lok Sibon Li and Sami Abu{-}El{-}Haija and Peter Battaglia and\n             Neslihan Bulut and Jonathan Halcrow and\n             Filipe Miguel Gon{\\c{c}}alves de Almeida and Pedro Gonnet and\n             Liangze Jiang and Parth Kothari and Silvio Lattanzi and \n             Andr{\\'{e}} Linhares and Brandon Mayer and Vahab Mirrokni and\n             John Palowitch and Mihir Paradkar and Jennifer She and\n             Anton Tsitsulin and Kevin Villela and Lisa Wang and David Wong and\n             Bryan Perozzi},\n  title   = {{TF-GNN:} Graph Neural Networks in TensorFlow},\n  journal = {CoRR},\n  volume  = {abs/2207.03522},\n  year    = {2023},\n  url     = {http://arxiv.org/abs/2207.03522},\n}\n```\n", "release_dates": ["2024-02-06T16:38:26Z", "2024-02-06T14:17:48Z", "2024-02-02T09:24:51Z", "2023-12-19T08:51:17Z", "2023-12-14T16:59:34Z", "2023-12-13T17:08:28Z", "2023-12-06T16:26:35Z", "2023-12-06T07:06:25Z", "2023-07-27T21:21:28Z", "2023-07-26T13:25:21Z", "2023-07-20T17:54:57Z", "2023-06-12T14:34:15Z", "2023-01-30T16:15:36Z", "2023-01-27T18:11:16Z", "2023-01-06T17:09:38Z", "2022-12-16T01:45:42Z", "2022-11-27T13:04:01Z", "2022-11-27T00:44:34Z", "2022-11-23T23:05:59Z", "2022-09-20T18:15:01Z", "2022-07-19T22:32:24Z", "2022-07-04T17:02:37Z", "2022-06-29T16:38:20Z", "2022-05-31T21:42:07Z"]}, {"name": "graphics", "description": "TensorFlow Graphics: Differentiable Graphics Layers for TensorFlow", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Graphics\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Build](https://github.com/tensorflow/graphics/workflows/Build/badge.svg?branch=master)](https://github.com/tensorflow/graphics/actions)\n[![Code coverage](https://img.shields.io/coveralls/github/tensorflow/graphics.svg)](https://coveralls.io/github/tensorflow/graphics)\n[![PyPI project status](https://img.shields.io/pypi/status/tensorflow-graphics.svg)](https://pypi.org/project/tensorflow-graphics/)\n[![Supported Python version](https://img.shields.io/pypi/pyversions/tensorflow-graphics.svg)](https://pypi.org/project/tensorflow-graphics/)\n[![PyPI release version](https://img.shields.io/pypi/v/tensorflow-graphics.svg)](https://pypi.org/project/tensorflow-graphics/)\n[![Downloads](https://pepy.tech/badge/tensorflow-graphics)](https://pepy.tech/project/tensorflow-graphics)\n\nThe last few years have seen a rise in novel differentiable graphics layers\nwhich can be inserted in neural network architectures. From spatial transformers\nto differentiable graphics renderers, these new layers leverage the knowledge\nacquired over years of computer vision and graphics research to build new and\nmore efficient network architectures. Explicitly modeling geometric priors and\nconstraints into neural networks opens up the door to architectures that can be\ntrained robustly, efficiently, and more importantly, in a self-supervised\nfashion.\n\n## Overview\n\nAt a high level, a computer graphics pipeline requires a representation of 3D\nobjects and their absolute positioning in the scene, a description of the\nmaterial they are made of, lights and a camera. This scene description is then\ninterpreted by a renderer to generate a synthetic rendering.\n\n<div align=\"center\">\n  <img border=\"0\"  src=\"https://storage.googleapis.com/tensorflow-graphics/git/readme/graphics.jpg\" width=\"600\">\n</div>\n\nIn comparison, a computer vision system would start from an image and try to\ninfer the parameters of the scene. This allows the prediction of which objects\nare in the scene, what materials they are made of, and their three-dimensional\nposition and orientation.\n\n<div align=\"center\">\n  <img border=\"0\"  src=\"https://storage.googleapis.com/tensorflow-graphics/git/readme/cv.jpg\" width=\"600\">\n</div>\n\nTraining machine learning systems capable of solving these complex 3D vision\ntasks most often requires large quantities of data. As labelling data is a\ncostly and complex process, it is important to have mechanisms to design machine\nlearning models that can comprehend the three dimensional world while being\ntrained without much supervision. Combining computer vision and computer\ngraphics techniques provides a unique opportunity to leverage the vast amounts\nof readily available unlabelled data. As illustrated in the image below, this\ncan, for instance, be achieved using analysis by synthesis where the vision\nsystem extracts the scene parameters and the graphics system renders back an\nimage based on them. If the rendering matches the original image, the vision\nsystem has accurately extracted the scene parameters. In this setup, computer\nvision and computer graphics go hand in hand, forming a single machine learning\nsystem similar to an autoencoder, which can be trained in a self-supervised\nmanner.\n\n<div align=\"center\">\n  <img border=\"0\"  src=\"https://storage.googleapis.com/tensorflow-graphics/git/readme/cv_graphics.jpg\" width=\"600\">\n</div>\n\nTensorflow Graphics is being developed to help tackle these types of challenges\nand to do so, it provides a set of differentiable graphics and geometry layers\n(e.g. cameras, reflectance models, spatial transformations, mesh convolutions)\nand 3D viewer functionalities (e.g. 3D TensorBoard) that can be used to train\nand debug your machine learning models of choice.\n\n## Installing TensorFlow Graphics\n\nSee the [install](https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/g3doc/install.md)\ndocumentation for instructions on how to install TensorFlow Graphics.\n\n## API Documentation\n\nYou can find the API documentation\n[here](https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/g3doc/api_docs/python/tfg.md).\n\n## Compatibility\n\nTensorFlow Graphics is fully compatible with the latest stable release of\nTensorFlow, tf-nightly, and tf-nightly-2.0-preview. All the functions are\ncompatible with graph and eager execution.\n\n## Debugging\n\nTensorflow Graphics heavily relies on L2 normalized tensors, as well as having\nthe inputs to specific function be in a pre-defined range. Checking for all of\nthis takes cycles, and hence is not activated by default. It is recommended to\nturn these checks on during a couple epochs of training to make sure that\neverything behaves as expected. This\n[page](https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/g3doc/debug_mode.md)\nprovides the instructions to enable these checks.\n\n## Colab tutorials\n\nTo help you get started with some of the functionalities provided by TF\nGraphics, some Colab notebooks are available below and roughly ordered by\ndifficulty. These Colabs touch upon a large range of topics including, object\npose estimation, interpolation, object materials, lighting, non-rigid surface\ndeformation, spherical harmonics, and mesh convolutions.\n\nNOTE: the tutorials are maintained carefully. However, they are not considered\npart of the API and they can change at any time without warning. It is not\nadvised to write code that takes dependency on them.\n\n### Beginner\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <th style=\"text-align:center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/6dof_alignment.ipynb\">Object pose estimation</a></th>\n      <th style=\"text-align:center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/intrinsics_optimization.ipynb\">Camera intrinsics optimization</a></th>\n    </tr>\n    <tr>\n      <td align=\"center\">\n        <a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/6dof_alignment.ipynb\"><img border=\"0\"  src=\"https://storage.googleapis.com/tensorflow-graphics/notebooks/6dof_pose/thumbnail.jpg\" width=\"200\" height=\"200\">\n        </a>\n      </td>\n      <td align=\"center\">\n              <a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/intrinsics_optimization.ipynb\"><img border=\"0\" src=\"https://storage.googleapis.com/tensorflow-graphics/notebooks/intrinsics/intrinsics_thumbnail.png\" width=\"200\" height=\"200\">\n        </a>\n      </td>\n    </tr>\n  </table>\n</div>\n\n### Intermediate\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <th style=\"text-align:center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/interpolation.ipynb\">B-spline and slerp interpolation</a></th>\n      <th style=\"text-align:center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/reflectance.ipynb\">Reflectance</a></th>\n      <th style=\"text-align:center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/non_rigid_deformation.ipynb\">Non-rigid surface deformation</a></th>\n    </tr>\n    <tr>\n      <td align=\"center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/interpolation.ipynb\"><img border=\"0\" src=\"https://storage.googleapis.com/tensorflow-graphics/notebooks/interpolation/thumbnail.png\" width=\"200\" height=\"200\"> </td>\n      <td align=\"center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/reflectance.ipynb\"><img border=\"0\" src=\"https://storage.googleapis.com/tensorflow-graphics/notebooks/reflectance/thumbnail.png\" width=\"200\" height=\"200\"></td>\n      <td align=\"center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/non_rigid_deformation.ipynb\"><img border=\"0\" src=\"https://storage.googleapis.com/tensorflow-graphics/notebooks/non_rigid_deformation/thumbnail.jpg\" width=\"200\" height=\"200\">\n      </a></td>\n    </tr>\n  </table>\n</div>\n\n### Advanced\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <th style=\"text-align:center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/spherical_harmonics_approximation.ipynb\">Spherical harmonics rendering</a></th>\n      <th style=\"text-align:center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/spherical_harmonics_optimization.ipynb\">Environment map optimization</a></th>\n      <th style=\"text-align:center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/mesh_segmentation_demo.ipynb\">Semantic mesh segmentation</a></th>\n    </tr>\n    <tr>\n      <td align=\"center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/spherical_harmonics_approximation.ipynb\"><img border=\"0\" src=\"https://storage.googleapis.com/tensorflow-graphics/notebooks/sh_rendering/thumbnail.png\" width=\"200\" height=\"200\">\n      </a></td>\n      <td align=\"center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/spherical_harmonics_optimization.ipynb\"><img border=\"0\" src=\"https://storage.googleapis.com/tensorflow-graphics/notebooks/environment_lighting/thumbnail.png\" width=\"200\" height=\"200\">\n      </a></td>\n      <td align=\"center\"><a href=\"https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/mesh_segmentation_demo.ipynb\"><img border=\"0\" src=\"https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/thumbnail.jpg\" width=\"200\" height=\"200\">\n      </a></td>\n    </tr>\n  </table>\n</div>\n\n## TensorBoard 3D\n\nVisual debugging is a great way to assess whether an experiment is going in the\nright direction. To this end, TensorFlow Graphics comes with a TensorBoard\nplugin to interactively visualize 3D meshes and point clouds.\n[This demo](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/tensorboard/plugins/mesh/Mesh_Plugin_Tensorboard.ipynb)\nshows how to use the plugin. Follow\n[these instructions](https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/g3doc/tensorboard.md)\nto install and configure TensorBoard 3D. Note that TensorBoard 3D is currently\nnot compatible with eager execution nor TensorFlow 2.\n\n<div align=\"center\">\n  <img border=\"0\"  src=\"https://storage.googleapis.com/tensorflow-graphics/git/readme/tensorboard_plugin.jpg\" width=\"1280\">\n</div>\n\n## Coming next...\n\nAmong many things, we are hoping to release resamplers, additional 3D\nconvolution and pooling operators, and a differentiable rasterizer!\n\nFollow us on [Twitter](https://twitter.com/_TFGraphics_) to hear about the\nlatest updates!\n\n## Additional Information\n\nYou may use this software under the\n[Apache 2.0 License](https://github.com/tensorflow/graphics/blob/master/LICENSE).\n\n## Community\n\nAs part of TensorFlow, we're committed to fostering an open and welcoming\nenvironment.\n\n*   [Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow): Ask\n    or answer technical questions.\n*   [GitHub](https://github.com/tensorflow/graphics/issues): Report bugs or make\n    feature requests.\n*   [TensorFlow Blog](https://blog.tensorflow.org/): Stay up to date on content\n    from the TensorFlow team and best articles from the community.\n*   [Youtube Channel](http://youtube.com/tensorflow/): Follow TensorFlow shows.\n\n## References\n\nIf you use TensorFlow Graphics in your research, please reference it as:\n\n    @inproceedings{TensorflowGraphicsIO2019,\n       author = {Valentin, Julien and Keskin, Cem and Pidlypenskyi, Pavel and Makadia, Ameesh and Sud, Avneesh and Bouaziz, Sofien},\n       title = {TensorFlow Graphics: Computer Graphics Meets Deep Learning},\n       year = {2019}\n    }\n\n### Contact\n\nWant to reach out? E-mail us at tf-graphics-contact@google.com!\n\n### Contributors - in alphabetical order\n\n-   Sofien Bouaziz (sofien@google.com)\n-   Jay Busch\n-   Forrester Cole\n-   Ambrus Csaszar\n-   Boyang Deng\n-   Ariel Gordon\n-   Christian H\u00e4ne\n-   Cem Keskin\n-   Ameesh Makadia\n-   Cengiz \u00d6ztireli\n-   Rohit Pandey\n-   Romain Pr\u00e9vost\n-   Pavel Pidlypenskyi\n-   Stefan Popov\n-   Konstantinos Rematas\n-   Omar Sanseviero\n-   Aviv Segal\n-   Avneesh Sud\n-   Andrea Tagliasacchi\n-   Anastasia Tkach\n-   Julien Valentin\n-   He Wang\n-   Yinda Zhang\n", "release_dates": ["2019-05-09T10:06:22Z"]}, {"name": "haskell", "description": "Haskell bindings for TensorFlow", "language": "Haskell", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![Build Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/haskell/github.png)\n\nThe tensorflow-haskell package provides Haskell bindings to\n[TensorFlow](https://www.tensorflow.org/).\n\nThis is not an official Google product.\n\n# Documentation\n\nhttps://tensorflow.github.io/haskell/haddock/\n\n[TensorFlow.Core](https://tensorflow.github.io/haskell/haddock/tensorflow-0.3.0.0/TensorFlow-Core.html)\nis a good place to start.\n\n# Examples\n\nNeural network model for the MNIST dataset: [code](tensorflow-mnist/app/Main.hs)\n\nToy example of a linear regression model\n([full code](tensorflow-ops/tests/RegressionTest.hs)):\n\n```haskell\nimport Control.Monad (replicateM, replicateM_)\nimport System.Random (randomIO)\nimport Test.HUnit (assertBool)\n\nimport qualified TensorFlow.Core as TF\nimport qualified TensorFlow.GenOps.Core as TF\nimport qualified TensorFlow.Minimize as TF\nimport qualified TensorFlow.Ops as TF hiding (initializedVariable)\nimport qualified TensorFlow.Variable as TF\n\nmain :: IO ()\nmain = do\n    -- Generate data where `y = x*3 + 8`.\n    xData <- replicateM 100 randomIO\n    let yData = [x*3 + 8 | x <- xData]\n    -- Fit linear regression model.\n    (w, b) <- fit xData yData\n    assertBool \"w == 3\" (abs (3 - w) < 0.001)\n    assertBool \"b == 8\" (abs (8 - b) < 0.001)\n\nfit :: [Float] -> [Float] -> IO (Float, Float)\nfit xData yData = TF.runSession $ do\n    -- Create tensorflow constants for x and y.\n    let x = TF.vector xData\n        y = TF.vector yData\n    -- Create scalar variables for slope and intercept.\n    w <- TF.initializedVariable 0\n    b <- TF.initializedVariable 0\n    -- Define the loss function.\n    let yHat = (x `TF.mul` TF.readValue w) `TF.add` TF.readValue b\n        loss = TF.square (yHat `TF.sub` y)\n    -- Optimize with gradient descent.\n    trainStep <- TF.minimizeWith (TF.gradientDescent 0.001) loss [w, b]\n    replicateM_ 1000 (TF.run trainStep)\n    -- Return the learned parameters.\n    (TF.Scalar w', TF.Scalar b') <- TF.run (TF.readValue w, TF.readValue b)\n    return (w', b')\n```\n\n# Installation Instructions\n\nNote: building this repository with `stack` requires version `2.3.1` or newer.\nCheck your stack version with `stack --version` in a terminal.\n\n## Build with Docker on Linux\n\nAs an expedient we use [docker](https://www.docker.com/) for building. Once you have docker\nworking, the following commands will compile and run the tests.\n\n```\ngit clone --recursive https://github.com/tensorflow/haskell.git tensorflow-haskell\ncd tensorflow-haskell\ndocker build -t tensorflow/haskell:2.12.0 docker\n# TODO: move the setup step to the docker script.\nstack --docker setup\nstack --docker test\n```\n\nThere is also a demo application:\n\n```\ncd tensorflow-mnist\nstack --docker build --exec Main\n```\n\n### Stack + Docker + GPU\n\nIf you want to use GPU you can do:\n\n```\nIMAGE_NAME=tensorflow/haskell:2.12.0-gpu\ndocker build -t $IMAGE_NAME docker/gpu\n# TODO: move the setup step to the docker script.\nstack --docker --docker-image=$IMAGE_NAME setup\nstack --docker --docker-image=$IMAGE_NAME test\n```\n\n### Using nvidia-docker version 2\nSee [Nvidia docker 2 install instructions](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0))\n\n```\nstack --docker --docker-image=$IMAGE_NAME setup\nstack --docker --docker-run-args \"--runtime=nvidia\" --docker-image=$IMAGE_NAME test\n```\n\n### Using nvidia-docker classic\n\nStack needs to use `nvidia-docker` instead of the normal `docker` for GPU support. We must wrap 'docker' with a script. This script will shadow the normal `docker` command.\n\n```\nln -s `pwd`/tools/nvidia-docker-wrapper.sh <somewhere in your path>/docker\nstack --docker --docker-image=$IMAGE_NAME setup\nstack --docker --docker-image=$IMAGE_NAME test\n```\n\n## Build on macOS\n\nRun the [install_macos_dependencies.sh](./tools/install_macos_dependencies.sh)\nscript in the `tools/` directory. The script installs dependencies\nvia [Homebrew](https://brew.sh/) and then downloads and installs the TensorFlow\nlibrary on your machine under `/usr/local`.\n\nAfter running the script to install system dependencies, build the project with stack:\n\n    stack test\n\n## Build on NixOS\n\nThe `stack.yaml` file describes a NixOS environment containing the necessary\ndependencies. To build, run:\n\n    $ stack --nix build\n\n## Installation on CentOS\n\n[Xiaokui Shu (@subbyte)](https://github.com/subbyte) maintains [separate instructions for installation on CentOS](https://github.com/subbyte/haskell-learn/blob/master/tensorflow_setup.md).\n\n# Related Projects\n\n## Statically validated tensor shapes\n\nhttps://github.com/helq/tensorflow-haskell-deptyped is experimenting with using dependent types to statically validate tensor shapes. May be merged with this repository in the future.\n\nExample:\n\n```haskell\n{-# LANGUAGE DataKinds, ScopedTypeVariables #-}\n\nimport Data.Maybe (fromJust)\nimport Data.Vector.Sized (Vector, fromList)\nimport TensorFlow.DepTyped\n\ntest :: IO (Vector 8 Float)\ntest = runSession $ do\n  (x :: Placeholder \"x\" '[4,3] Float) <- placeholder\n\n  let elems1 = fromJust $ fromList [1,2,3,4,1,2]\n      elems2 = fromJust $ fromList [5,6,7,8]\n      (w :: Tensor '[3,2] '[] Build Float) = constant elems1\n      (b :: Tensor '[4,1] '[] Build Float) = constant elems2\n      y = (x `matMul` w) `add` b -- y shape: [4,2] (b shape is [4.1] but `add` broadcasts it to [4,2])\n\n  let (inputX :: TensorData \"x\" [4,3] Float) =\n          encodeTensorData . fromJust $ fromList [1,2,3,4,1,0,7,9,5,3,5,4]\n\n  runWithFeeds (feed x inputX :~~ NilFeedList) y\n\nmain :: IO ()\nmain = test >>= print\n```\n\n# License\nThis project is licensed under the terms of the [Apache 2.0 license](LICENSE).\n", "release_dates": []}, {"name": "hub", "description": "A library for transfer learning by reusing parts of TensorFlow models.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<!-- Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================-->\n\n<!--\nThis file is rendered on github.com/tensorflow/hub.\ng3doc/_index.yaml is rendered on tensorflow.org/hub.\nBoth link to g3doc/overview.md and g3doc/*.md for detailed docs.\n-->\n\n**Warning: unmigrated tfhub.dev model artifacts will be deleted on March 18,\n2024.**\n\nAs of November 15th 2023, most [tfhub.dev](https://tfhub.dev) URLs and model\nhandles are now redirecting to their migrated/equivalent counterpart on Kaggle\nModels.\n\nOn March 18, 2024, all unmigrated model assets previously surfaced on tfhub.dev\nwill be deleted \u2013 after this date, `hub.load` and `hub.KerasLayer` calls to\nthese tfhub.dev handles will fail permanently. See the list of unmigrated model\nassets here:\n\n-   [inaturalist/vision/embedder/inaturalist_V2](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/inaturalist/models/vision/embedder/inaturalist_V2)\n-   [nvidia/unet/industrial/class_1](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_1)\n-   [nvidia/unet/industrial/class_2](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_2)\n-   [nvidia/unet/industrial/class_3](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_3)\n-   [nvidia/unet/industrial/class_4](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_4)\n-   [nvidia/unet/industrial/class_5](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_5)\n-   [nvidia/unet/industrial/class_6](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_6)\n-   [nvidia/unet/industrial/class_7](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_7)\n-   [nvidia/unet/industrial/class_8](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_8)\n-   [nvidia/unet/industrial/class_9](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_9)\n-   [nvidia/unet/industrial/class_10](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_10)\n-   [silero/silero-stt/de](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/silero/models/silero-stt/de)\n-   [silero/silero-stt/en](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/silero/models/silero-stt/en)\n-   [silero/silero-stt/es](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/silero/models/silero-stt/es)\n-   [svampeatlas/vision/classifier/fungi_mobile_V1](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/svampeatlas/models/vision/classifier/fungi_mobile_V1)\n-   [svampeatlas/vision/embedder/fungi_V2](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/svampeatlas/models/vision/embedder/fungi_V2)\n\n**If you are an owner of an unmigrated model, please get in touch with us at\nkaggle-models@google.com if you'd like to migrate your model. If you take no\naction, your model(s) will be deleted on March 18, 2024 and not retrievable\n(either by you or other users).**\n\nFor models with a Kaggle Models copy, there will be no impact on the\navailability/functionality of models that were copied from tfhub.dev \u2013\n`tensorflow_hub` will continue to support downloading models that were initially\nuploaded to tfhub.dev via\ne.g. `hub.load(\"https://tfhub.dev/<publisher>/<model>/<version>\")`. To see if a\ntfhub.dev model has been migrated, enter the model handle in your URL bar \u2013 if\nthe redirect is successful, it has already been migrated, otherwise it is an\nunmigrated model and will be subject to deletion.\n\nAlthough no migration or code rewrites are explicitly required, we recommend\nreplacing tfhub.dev links with their Kaggle Models counterparts to improve code\nhealth and debuggability.\n\nSee FAQs [here](https://kaggle.com/tfhub-dev-faqs).\n\n# tensorflow_hub\n\nThis GitHub repository hosts the `tensorflow_hub` Python library to download\nand reuse SavedModels in your TensorFlow program with a minimum amount of code,\nas well as other associated code and documentation.\n\n## Getting Started\n\n  * [Introduction](https://www.tensorflow.org/hub/)\n  * The asset types of [tfhub.dev](https://tfhub.dev/)\n      * [SavedModels for TensorFlow 2](docs/tf2_saved_model.md)\n        and the [Reusable SavedModel interface](docs/reusable_saved_models.md).\n      * Deprecated: [Models in TF1 Hub format](docs/tf1_hub_module.md) and\n        their [Common Signatures](docs/common_signatures/index.md) collection.\n  * Using the library\n      * [Installation](docs/installation.md)\n      * [Caching model downloads](docs/caching.md)\n      * [Migration to TF2](docs/migration_tf2.md)\n      * [Model compatibility for TF1/TF2](docs/model_compatibility.md)\n      * [Common issues](docs/common_issues.md)\n      * [Build from source](docs/build_from_source.md)\n      * [Hosting a module](docs/hosting.md)\n  * Tutorials\n      * [TF2 Image Retraining](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb)\n      * [TF2 Text Classification](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_text_classification.ipynb)\n      * [Additional TF1 and TF2 examples](examples/README.md)\n\n\n## Contributing\n\nIf you'd like to contribute to TensorFlow Hub, be sure to review the\n[contribution guidelines](CONTRIBUTING.md). To contribute code to the\nlibrary itself (not examples), you will probably need to\n[build from source](docs/build_from_source.md).\n\nThis project adheres to TensorFlow's\n[code of conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md).\nBy participating, you are expected to uphold this code.\n\nWe use [GitHub issues](https://github.com/tensorflow/hub/issues) for tracking\nrequests and bugs.\n\n\n## License\n\n[Apache License 2.0](LICENSE)\n", "release_dates": ["2024-01-30T15:53:29Z", "2024-01-22T17:23:22Z", "2023-10-06T10:06:37Z", "2023-07-13T14:35:04Z", "2023-03-14T19:14:17Z", "2021-04-14T13:17:26Z", "2021-01-06T15:48:17Z", "2020-10-29T15:10:15Z", "2020-08-19T11:54:27Z", "2020-04-01T16:13:04Z", "2019-10-31T10:28:44Z", "2019-08-29T07:39:24Z", "2019-06-25T08:49:53Z", "2019-04-03T08:30:15Z", "2019-03-01T16:22:36Z", "2018-12-05T09:51:16Z", "2018-11-29T14:51:13Z", "2018-11-28T14:26:12Z", "2018-07-24T15:27:53Z", "2018-04-03T12:04:15Z"]}, {"name": "io", "description": "Dataset, streaming, and file system extensions maintained by TensorFlow SIG-IO", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<div align=\"center\">\n  <img src=\"https://github.com/tensorflow/community/blob/master/sigs/logos/SIGIO.png\" width=\"60%\"><br><br>\n</div>\n\n-----------------\n\n# TensorFlow I/O\n\n[![GitHub CI](https://github.com/tensorflow/io/workflows/GitHub%20CI/badge.svg?branch=master)](https://github.com/tensorflow/io/actions?query=branch%3Amaster)\n[![PyPI](https://badge.fury.io/py/tensorflow-io.svg)](https://pypi.org/project/tensorflow-io/)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/tensorflow/io/blob/master/LICENSE)\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/io)\n\nTensorFlow I/O is a collection of file systems and file formats that are not\navailable in TensorFlow's built-in support. A full list of supported file systems\nand file formats by TensorFlow I/O can be found [here](https://www.tensorflow.org/io/api_docs/python/tfio).\n\nThe use of tensorflow-io is straightforward with keras. Below is an example\nto [Get Started with TensorFlow](https://www.tensorflow.org/tutorials/quickstart/beginner) with\nthe data processing aspect replaced by tensorflow-io:\n\n```python\nimport tensorflow as tf\nimport tensorflow_io as tfio\n\n# Read the MNIST data into the IODataset.\ndataset_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\nd_train = tfio.IODataset.from_mnist(\n    dataset_url + \"train-images-idx3-ubyte.gz\",\n    dataset_url + \"train-labels-idx1-ubyte.gz\",\n)\n\n# Shuffle the elements of the dataset.\nd_train = d_train.shuffle(buffer_size=1024)\n\n# By default image data is uint8, so convert to float32 using map().\nd_train = d_train.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y))\n\n# prepare batches the data just like any other tf.data.Dataset\nd_train = d_train.batch(32)\n\n# Build the model.\nmodel = tf.keras.models.Sequential(\n    [\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\n        tf.keras.layers.Dense(512, activation=tf.nn.relu),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n    ]\n)\n\n# Compile the model.\nmodel.compile(\n    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)\n\n# Fit the model.\nmodel.fit(d_train, epochs=5, steps_per_epoch=200)\n```\n\nIn the above [MNIST](http://yann.lecun.com/exdb/mnist/) example, the URL's\nto access the dataset files are passed directly to the `tfio.IODataset.from_mnist` API call.\nThis is due to the inherent support that `tensorflow-io` provides for `HTTP`/`HTTPS` file system,\nthus eliminating the need for downloading and saving datasets on a local directory.\n\nNOTE: Since `tensorflow-io` is able to detect and uncompress the MNIST dataset automatically if needed,\nwe can pass the URL's for the compressed files (gzip) to the API call as is.\n\nPlease check the official [documentation](https://www.tensorflow.org/io) for more\ndetailed and interesting usages of the package.\n\n## Installation\n\n### Python Package\n\nThe `tensorflow-io` Python package can be installed with pip directly using:\n```sh\n$ pip install tensorflow-io\n```\n\nPeople who are a little more adventurous can also try our nightly binaries:\n```sh\n$ pip install tensorflow-io-nightly\n```\n\nTo ensure you have a version of TensorFlow that is compatible with TensorFlow-IO,\nyou can specify the `tensorflow` extra requirement during install:\n\n```\npip install tensorflow-io[tensorflow]\n```\n\nSimilar extras exist for the `tensorflow-gpu`, `tensorflow-cpu` and `tensorflow-rocm`\npackages.\n\n### Docker Images\n\nIn addition to the pip packages, the docker images can be used to quickly get started.\n\nFor stable builds:\n```sh\n$ docker pull tfsigio/tfio:latest\n$ docker run -it --rm --name tfio-latest tfsigio/tfio:latest\n```\n\nFor nightly builds:\n```sh\n$ docker pull tfsigio/tfio:nightly\n$ docker run -it --rm --name tfio-nightly tfsigio/tfio:nightly\n```\n\n### R Package\n\nOnce the `tensorflow-io` Python package has been successfully installed, you\ncan install the development version of the R package from GitHub via the following:\n```r\nif (!require(\"remotes\")) install.packages(\"remotes\")\nremotes::install_github(\"tensorflow/io\", subdir = \"R-package\")\n```\n\n### TensorFlow Version Compatibility\n\nTo ensure compatibility with TensorFlow, it is recommended to install a matching\nversion of TensorFlow I/O according to the table below. You can find the list\nof releases [here](https://github.com/tensorflow/io/releases).\n\n| TensorFlow I/O Version | TensorFlow Compatibility | Release Date |\n| --- | --- | --- |\n| 0.36.0 | 2.15.x | Feb 02, 2024 |\n| 0.35.0 | 2.14.x | Dec 18, 2023 |\n| 0.34.0 | 2.13.x | Sep 08, 2023 |\n| 0.33.0 | 2.13.x | Aug 01, 2023 |\n| 0.32.0 | 2.12.x | Mar 28, 2023 |\n| 0.31.0 | 2.11.x | Feb 25, 2023 |\n| 0.30.0 | 2.11.x | Jan 20, 2023 |\n| 0.29.0 | 2.11.x | Dec 18, 2022 |\n| 0.28.0 | 2.11.x | Nov 21, 2022 |\n| 0.27.0 | 2.10.x | Sep 08, 2022 |\n| 0.26.0 | 2.9.x | May 17, 2022 |\n| 0.25.0 | 2.8.x | Apr 19, 2022 |\n| 0.24.0 | 2.8.x | Feb 04, 2022 |\n| 0.23.1 | 2.7.x | Dec 15, 2021 |\n| 0.23.0 | 2.7.x | Dec 14, 2021 |\n| 0.22.0 | 2.7.x | Nov 10, 2021 |\n| 0.21.0 | 2.6.x | Sep 12, 2021 |\n| 0.20.0 | 2.6.x | Aug 11, 2021 |\n| 0.19.1 | 2.5.x | Jul 25, 2021 |\n| 0.19.0 | 2.5.x | Jun 25, 2021 |\n| 0.18.0 | 2.5.x | May 13, 2021 |\n| 0.17.1 | 2.4.x | Apr 16, 2021 |\n| 0.17.0 | 2.4.x | Dec 14, 2020 |\n| 0.16.0 | 2.3.x | Oct 23, 2020 |\n| 0.15.0 | 2.3.x | Aug 03, 2020 |\n| 0.14.0 | 2.2.x | Jul 08, 2020 |\n| 0.13.0 | 2.2.x | May 10, 2020 |\n| 0.12.0 | 2.1.x | Feb 28, 2020 |\n| 0.11.0 | 2.1.x | Jan 10, 2020 |\n| 0.10.0 | 2.0.x | Dec 05, 2019 |\n| 0.9.1 | 2.0.x | Nov 15, 2019 |\n| 0.9.0 | 2.0.x | Oct 18, 2019 |\n| 0.8.1 | 1.15.x | Nov 15, 2019 |\n| 0.8.0 | 1.15.x | Oct 17, 2019 |\n| 0.7.2 | 1.14.x | Nov 15, 2019 |\n| 0.7.1 | 1.14.x | Oct 18, 2019 |\n| 0.7.0 | 1.14.x | Jul 14, 2019 |\n| 0.6.0 | 1.13.x | May 29, 2019 |\n| 0.5.0 | 1.13.x | Apr 12, 2019 |\n| 0.4.0 | 1.13.x | Mar 01, 2019 |\n| 0.3.0 | 1.12.0 | Feb 15, 2019 |\n| 0.2.0 | 1.12.0 | Jan 29, 2019 |\n| 0.1.0 | 1.12.0 | Dec 16, 2018 |\n\n\n## Performance Benchmarking\n\nWe use [github-pages](https://tensorflow.github.io/io/dev/bench/) to document the results of API performance benchmarks. The benchmark job is triggered on every commit to `master` branch and\nfacilitates tracking performance w.r.t commits.\n\n## Contributing\n\nTensorflow I/O is a community led open source project. As such, the project\ndepends on public contributions, bug-fixes, and documentation. Please see:\n\n- [contribution guidelines](CONTRIBUTING.md) for a guide on how to contribute.\n- [development doc](docs/development.md) for instructions on the development environment setup.\n- [tutorials](docs/tutorials) for a list of tutorial notebooks and instructions on how to write one.\n\n### Build Status and CI\n\n| Build | Status |\n| --- | --- |\n| Linux CPU Python 2 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-py2.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-py2.html) |\n| Linux CPU Python 3 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-py3.html) |\n| Linux GPU Python 2| [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-gpu-py2.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-gpu-py2.html) |\n| Linux GPU Python 3| [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-gpu-py3.html) |\n\nBecause of manylinux2010 requirement, TensorFlow I/O is built with\nUbuntu:16.04 + Developer Toolset 7 (GCC 7.3) on Linux. Configuration\nwith Ubuntu 16.04 with Developer Toolset 7 is not exactly straightforward.\nIf the system have docker installed, then the following command\nwill automatically build manylinux2010 compatible whl package:\n\n```sh\n#!/usr/bin/env bash\n\nls dist/*\nfor f in dist/*.whl; do\n  docker run -i --rm -v $PWD:/v -w /v --net=host quay.io/pypa/manylinux2010_x86_64 bash -x -e /v/tools/build/auditwheel repair --plat manylinux2010_x86_64 $f\ndone\nsudo chown -R $(id -nu):$(id -ng) .\nls wheelhouse/*\n```\n\nIt takes some time to build, but once complete, there will be python\n`3.5`, `3.6`, `3.7` compatible whl packages available in `wheelhouse`\ndirectory.\n\nOn macOS, the same command could be used. However, the script expects `python` in shell\nand will only generate a whl package that matches the version of `python` in shell. If\nyou want to build a whl package for a specific python then you have to alias this version\nof python to `python` in shell. See [.github/workflows/build.yml](.github/workflows/build.yml)\nAuditwheel step for instructions how to do that.\n\nNote the above command is also the command we use when releasing packages for Linux and macOS.\n\nTensorFlow I/O uses both GitHub Workflows and Google CI (Kokoro) for continuous integration.\nGitHub Workflows is used for macOS build and test. Kokoro is used for Linux build and test.\nAgain, because of the manylinux2010 requirement, on Linux whl packages are always\nbuilt with Ubuntu 16.04 + Developer Toolset 7. Tests are done on a variatiy of systems\nwith different python3 versions to ensure a good coverage:\n\n| Python | Ubuntu 18.04| Ubuntu 20.04 | macOS + osx9 | Windows-2019 |\n| ------- | ----- | ------- | ------- | --------- |\n| 2.7 |  :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | N/A |\n| 3.7 |  :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| 3.8 |  :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n\n\nTensorFlow I/O has integrations with many systems and cloud vendors such as\nPrometheus, Apache Kafka, Apache Ignite, Google Cloud PubSub, AWS Kinesis,\nMicrosoft Azure Storage, Alibaba Cloud OSS etc.\n\nWe tried our best to test against those systems in our continuous integration\nwhenever possible. Some tests such as Prometheus, Kafka, and Ignite\nare done with live systems, meaning we install Prometheus/Kafka/Ignite on CI machine before\nthe test is run. Some tests such as Kinesis, PubSub, and Azure Storage are done\nthrough official or non-official emulators. Offline tests are also performed whenever\npossible, though systems covered through offine tests may not have the same\nlevel of coverage as live systems or emulators.\n\n\n|  | Live System | Emulator| CI Integration |  Offline |\n| ------- | ----- | ----- | ----- | ----- |\n| Apache Kafka | :heavy_check_mark:  | | :heavy_check_mark:| |\n| Apache Ignite |  :heavy_check_mark: | |:heavy_check_mark:| |\n| Prometheus |  :heavy_check_mark: | |:heavy_check_mark:| |\n| Google PubSub |   | :heavy_check_mark: |:heavy_check_mark:| |\n| Azure Storage |   | :heavy_check_mark: |:heavy_check_mark:| |\n| AWS Kinesis |   | :heavy_check_mark: |:heavy_check_mark:| |\n| Alibaba Cloud OSS |   | | |  :heavy_check_mark: |\n| Google BigTable/BigQuery |   | to be added | | |\n| Elasticsearch (experimental) |  :heavy_check_mark: | |:heavy_check_mark:| |\n| MongoDB (experimental) |  :heavy_check_mark: | |:heavy_check_mark:| |\n\n\nReferences for emulators:\n- Official [PubSub Emulator](https://cloud.google.com/sdk/gcloud/reference/beta/emulators/pubsub/) by Google Cloud for Cloud PubSub.\n- Official [Azurite Emulator](https://github.com/Azure/Azurite) by Azure for Azure Storage.\n- None-official [LocalStack emulator](https://github.com/localstack/localstack) by LocalStack for AWS Kinesis.\n\n\n## Community\n\n* SIG IO [Google Group](https://groups.google.com/a/tensorflow.org/forum/#!forum/io) and mailing list: [io@tensorflow.org](io@tensorflow.org)\n* SIG IO [Monthly Meeting Notes](https://docs.google.com/document/d/1CB51yJxns5WA4Ylv89D-a5qReiGTC0GYum6DU-9nKGo/edit)\n* Gitter room: [tensorflow/sig-io](https://gitter.im/tensorflow/sig-io)\n\n## Additional Information\n\n* [Streaming Machine Learning with Tiered Storage and Without a Data Lake](https://www.confluent.io/blog/streaming-machine-learning-with-tiered-storage/) - [Kai Waehner](https://github.com/kaiwaehner)\n* [TensorFlow with Apache Arrow Datasets](https://medium.com/tensorflow/tensorflow-with-apache-arrow-datasets-cdbcfe80a59f) - [Bryan Cutler](https://github.com/BryanCutler)\n* [How to build a custom Dataset for Tensorflow](https://towardsdatascience.com/how-to-build-a-custom-dataset-for-tensorflow-1fe3967544d8) - [Ivelin Ivanov](https://github.com/ivelin)\n* [TensorFlow on Apache Ignite](https://medium.com/tensorflow/tensorflow-on-apache-ignite-99f1fc60efeb) - [Anton Dmitriev](https://github.com/dmitrievanthony)\n\n## License\n\n[Apache License 2.0](LICENSE)\n", "release_dates": ["2024-02-06T04:35:56Z", "2023-09-08T15:59:39Z", "2023-08-02T02:27:26Z", "2023-03-29T02:39:56Z", "2023-02-25T19:32:38Z", "2023-01-21T14:21:58Z", "2022-12-19T02:08:17Z", "2022-11-22T04:30:58Z", "2022-09-08T23:03:47Z", "2022-05-18T01:03:25Z", "2022-04-21T16:25:12Z", "2022-02-04T22:19:51Z", "2021-12-15T23:04:19Z", "2021-12-14T15:10:25Z", "2021-11-10T18:53:06Z", "2021-09-13T03:24:54Z", "2021-08-12T03:38:51Z", "2021-07-25T13:38:20Z", "2021-06-25T13:22:04Z", "2021-05-14T06:06:47Z", "2021-04-16T17:14:10Z", "2020-12-15T00:45:37Z", "2020-10-23T18:42:52Z", "2020-08-03T18:30:24Z", "2020-07-08T19:20:17Z", "2020-05-11T02:35:38Z", "2020-02-28T19:05:37Z", "2020-01-10T20:56:25Z", "2019-12-05T21:09:52Z", "2019-11-15T20:34:53Z"]}, {"name": "java", "description": "Java bindings for TensorFlow", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow for Java\n\n## Welcome to the Java world of TensorFlow!\n\nTensorFlow can run on any JVM for building, training and running machine learning models. It comes with \na series of utilities and frameworks that help achieve most of the tasks common to data scientists \nand developers working in this domain. Java and other JVM languages, such as Scala or Kotlin, are \nfrequently used in small-to-large enterprises all over the world, which makes TensorFlow a strategic \nchoice for adopting machine learning at a large scale.\n\n## This Repository\n\nIn the early days, the Java language bindings for TensorFlow were hosted in the [main repository](https://github.com/tensorflow/tensorflow)\nand released only when a new version of the core library was ready to be distributed, which happens only\na few times a year. Now, all Java-related code has been moved to this repository so that it can evolve and \nbe released independently from official TensorFlow releases. In addition, most of the build tasks have been\nmigrated from Bazel to Maven, which is more familiar for most Java developers.\n\nThe following describes the layout of the repository and its different artifacts:\n\n* `tensorflow-core`\n  * All artifacts that build up the core language bindings of TensorFlow for Java\n  * Intended audience: projects that provide their own APIs or frameworks on top of \n    TensorFlow and just want a thin layer to access the TensorFlow native library from the JVM\n    \n* `tensorflow-framework`\n  * Primary API for building and training neural networks with TensorFlow\n  * Intended audience: neural network developers\n  * For more information: [tensorflow-framework/README.md](tensorflow-framework/README.md)\n\n*Note: The NdArray Library module has now its own [repository](https://github.com/tensorflow/java-ndarray) and has been moved out of TensorFlow Java.*\n\n## Communication\n\nThis repository is maintained by TensorFlow JVM Special Interest Group (SIG). You can easily contact the group\nby posting to the [TensorFlow Forum](https://discuss.tensorflow.org), adding the `sig_jvm` tag, or by writing to us on\nthe [sig-jvm Gitter channel](https://gitter.im/tensorflow/sig-jvm). You can also simply send pull requests \nand raise issues to this repository.\n\n## Building Sources\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md#building).\n\n## Using Maven Artifacts\n\nThere are two options for adding TensorFlow Java as a dependency to your Maven project: with individual dependencies \nfor each targeted platforms or with a single dependency that target them all.\n\n### Individual dependencies\n\nWith this option, you must first add a dependency to `tensorflow-core-api` and then one or multiple\ndependencies to `tensorflow-core-native` with a classifier targeting a specific platform. This option is preferred as \nit minimize the size of your application by only including the TensorFlow builds you need, at the cost of being more \nrestrictive. \n\nWhile TensorFlow Java can be compiled for [multiple platforms](https://github.com/tensorflow/java/blob/master/tensorflow-core/pom.xml#L54),\nonly binaries for the followings are being **supported and distributed** by this project:\n\n- `linux-x86_64`: Linux platforms on Intel chips\n- `linux-x86_64-gpu`: Linux platforms on Intel chips with Cuda GPU support\n- `macosx-x86_64`: MacOS X platforms on Intel chips\n- `macosx-arm64`: MacOS X platforms on Apple Silicon chips\n- `windows-x86_64`: Windows platforms on Intel chips\n\nFor example, for building a JAR that uses TensorFlow and is targeted to be deployed only on Linux\nsystems with no GPU support, you should add the following dependencies:\n```xml\n<dependency>\n  <groupId>org.tensorflow</groupId>\n  <artifactId>tensorflow-core-api</artifactId>\n  <version>0.5.0</version>\n</dependency>\n<dependency>\n  <groupId>org.tensorflow</groupId>\n  <artifactId>tensorflow-core-native</artifactId>\n  <version>0.5.0</version>\n  <classifier>linux-x86_64</classifier>\n</dependency>\n```\n\nOn the other hand, if you plan to deploy your JAR on more platforms, you need additional\nnative dependencies as follows:\n```xml\n<dependency>\n  <groupId>org.tensorflow</groupId>\n  <artifactId>tensorflow-core-api</artifactId>\n  <version>0.5.0</version>\n</dependency>\n<dependency>\n  <groupId>org.tensorflow</groupId>\n  <artifactId>tensorflow-core-native</artifactId>\n  <version>0.5.0</version>\n  <classifier>linux-x86_64-gpu</classifier>\n</dependency>\n<dependency>\n  <groupId>org.tensorflow</groupId>\n  <artifactId>tensorflow-core-native</artifactId>\n  <version>0.5.0</version>\n  <classifier>macosx-arm64</classifier>\n</dependency>\n<dependency>\n  <groupId>org.tensorflow</groupId>\n  <artifactId>tensorflow-core-native</artifactId>\n  <version>0.5.0</version>\n  <classifier>windows-x86_64</classifier>\n</dependency>\n```\n\nOnly one dependency can be added per platform, meaning that you cannot add native dependencies to both `linux-x86_64` and \n`linux-x86_64-gpu` within the same project.\n\n### Single dependency\n\nIn some cases, it might be preferable to add a single dependency that includes transitively all the artifacts \nrequired to run TensorFlow Java on any [supported platforms](README.md#individual-dependencies)\n\n- `tensorflow-core-platform`: Includes `tensorflow-core-api`, plus native artifacts for `linux-x86_64`, `macosx-arm64`, `macosx-x86_64` and `windows-x86_64`\n\nFor example, to run TensorFlow Java on any platform for which a binary is being distributed by this project, you can \nsimply add this dependency to your application:\n```xml\n<dependency>\n  <groupId>org.tensorflow</groupId>\n  <artifactId>tensorflow-core-platform</artifactId>\n  <version>0.5.0</version>\n</dependency>\n```\n\nBe aware though that the builds of TensorFlow are quite voluminous and including too many native dependencies may\nsignificantly increase the size of your application. So it is good practice to limit your dependencies to\nthe platforms you are targeting. For this purpose these artifacts include profiles that follow\nthe conventions established on this page:\n* [Reducing the Number of Dependencies](https://github.com/bytedeco/javacpp-presets/wiki/Reducing-the-Number-of-Dependencies)\n\n### Snapshots\n\nSnapshots of TensorFlow Java artifacts are automatically distributed after each update in the code. To use them, you need\nto add Sonatype OSS repository in your pom.xml, like the following\n\n```xml\n<repositories>\n    <repository>\n        <id>tensorflow-snapshots</id>\n        <url>https://oss.sonatype.org/content/repositories/snapshots/</url>\n        <snapshots>\n            <enabled>true</enabled>\n        </snapshots>\n    </repository>\n</repositories>\n<dependencies>\n    <!-- Example of dependency, see section above for more options -->\n    <dependency>\n        <groupId>org.tensorflow</groupId>\n        <artifactId>tensorflow-core-platform</artifactId>\n        <version>1.0.0-SNAPSHOT</version>\n    </dependency>\n</dependencies>\n```\n\n## TensorFlow/Java Version Support\n\nThis table shows the mapping between TensorFlow, TensorFlow Java and minimum supported Java versions.\n\n| TensorFlow Java Version | TensorFlow Version | Minimum Java Version |\n|-------------------------|--------------------| --------------- |\n| 0.2.0                   | 2.3.1              | 8 |\n| 0.3.0                   | 2.4.1              | 8 |\n| 0.3.1                   | 2.4.1              | 8 |\n| 0.3.2                   | 2.4.1              | 8 |\n| 0.3.3                   | 2.4.1              | 8 |\n| 0.4.0                   | 2.7.0              | 8 |\n| 0.4.1                   | 2.7.1              | 8 |\n| 0.4.2                   | 2.7.4              | 8 |\n| 0.5.0                   | 2.10.1             | 11 |\n| 0.6.0-SNAPSHOT          | 2.10.1             | 11 |\n| 1.0.0-SNAPSHOT          | 2.15.0             | 11 |\n\n## How to Contribute?\n\nContributions are welcome, guidelines are located in [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code and Usage Examples\n\nPlease look at this repository: https://github.com/tensorflow/java-models\n", "release_dates": ["2023-02-21T03:00:11Z", "2022-09-23T15:55:35Z", "2022-03-29T13:41:36Z", "2021-11-28T17:15:49Z", "2021-08-31T17:57:58Z", "2021-07-12T13:07:30Z", "2021-03-15T14:32:23Z", "2020-10-07T15:54:02Z"]}, {"name": "java-models", "description": "Models in Java", "language": "Java", "license": null, "readme": null, "release_dates": []}, {"name": "java-ndarray", "description": null, "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# NdArray Java Library\n\n## Introduction\n\nNdArray is a library exposing utilities for manipulating data in a n-dimensional space in Java. \nUnlike other Java artifacts distributed by TensorFlow, this library does not depend on the TensorFlow\nruntime, therefore is very lightweight and can be used by any kind of Java project.\n\nTo import the NdArray library in your project, simply add the following dependency:\n```xml\n<dependency>\n  <groupId>org.tensorflow</groupId>\n  <artifactId>ndarray</artifactId>\n  <version>1.0.0-rc.1</version>\n</dependency>\n```\n\n### Data Buffers\n\nInstances of `DataBuffer` map contiguous segments of memory with 64-bits indexing and supports \ngeneric parametrization while still allowing direct access to primitive types. Such segments \ncould be standard Java arrays, JDK NIO buffers or native memory. In addition, it can serialize and \ndeserialize data of any type (and not only primitive types, as with `java.util.nio`).\n\n```java\n// Allocate a buffer of 4K int values\nIntDataBuffer bufferA = DataBuffers.ofInts(4096L);\nassertEquals(4096L, bufferA.size());\n\n// Write an int array at the beginning of the buffer\nbufferA.write(new int[] { 1, 2, 3 });\nassertEquals(3, bufferA.getInt(2));\n\n// Slice buffer after its first value\nIntDataBuffer bufferB = bufferA.offset(1);\nassertEquals(4095L, bufferB.size());\nassertEquals(2, bufferB.getInt(0));\n\n// Resize a buffer to 10 elements\nIntDataBuffer bufferC = bufferB.narrow(10);\nassertEquals(10L, bufferB.size());\nassertEquals(2, bufferB.getInt(0));\n```\n\n### ND Arrays\n\nInstances of `NdArray` are used to view memory segments stored in a `DataBuffer` as a \nmultidimensional arrays and to provide an API for traversing, reading, writing and slicing\ntheir data. The goal of these tools is to replace the usage of standard multidimensional Java arrays \n(e.g. `new int[][][]`) since those results in slow performances, from the non-contiguous \nstorage of their data and the multiple dereferences required to access their values. \n\n```java\n// Allocating a 3D matrix of 2x3x2\nIntNdArray matrix3d = NdArrays.ofInts(Shape.of(2, 3, 2));\nassertEquals(3, matrix3d.rank());\n\n// Initializing 3D matrix data with vectors from the first dimension (index 0)\nmatrix3d.elements(0).forEach(matrix -> {\n    assertEquals(2, matrix.rank());\n    assertEquals(Shape.of(3, 2), matrix.shape());\n    matrix\n      .set(NdArrays.vectorOf(1, 2), 0)\n      .set(NdArrays.vectorOf(3, 4), 1)\n      .set(NdArrays.vectorOf(5, 6), 2);\n});\n\n// Visit all scalars of 3D matrix, printing their coordinates and value\nmatrix3d.scalars().forEachIdx((coords, scalar) ->\n    System.out.println(\"Scalar at \" + Arrays.toString(coords) + \" has value \" + scalar.getInt())\n);\n\n// Retrieving the second vector of the first matrix\nIntNdArray vector = matrix3d.get(0, 1);\nassertEquals(1, vector.rank());\n\n// Rewriting the values of the vector using a primitive array\nvector.copyFrom(DataBuffers.of(new int[] { 7, 8 }));\nassertEquals(7, matrix3d.getInt(0, 1, 0));\nassertEquals(8, matrix3d.getInt(0, 1, 1));\n\n// Slicing the 3D matrix so we only keep the second element of the second dimension\nIntNdArray slice = matrix3d.slice(all(), at(1));\nassertEquals(2, slice.rank());\nassertEquals(Shape.of(2, 2), slice.shape());\nassertEquals(7, slice.getInt(0, 0));  // (0, 1, 0) in the original matrix\nassertEquals(3, slice.getInt(1, 0));  // (1, 1, 0) in the original matrix\n```\n\n## Integration with TensorFlow\n\nThe NdArray library is independent of the TensorFlow runtime library, making it a good choice for\nmanipulating multi-dimensional data structures from anywhere. But as an example, here\nis how it is actually being used by the [TensorFlow Java API](https://github.com/tensorflow/java/):\n\n```java\n// Allocate a tensor of 32-bits integer of the shape (2, 3, 2)\nTInt32 tensor = TInt32.ofShape(2, 3, 2);\n\n// Access tensor memory directly\nassertEquals(3, tensor.rank());\nassertEquals(12, tensor.size());\n\ntry (EagerSession session = EagerSession.create()) {\n  Ops tf = Ops.create(session);\n\n  // Initialize tensor memory with zeros and take a snapshot\n  tensor.scalars().forEach(scalar -> scalar.setInt(0));\n  Constant<T> x = tf.constant(tensor);\n\n  // Initialize the same tensor memory with ones and take a snapshot\n  tensor.scalars().forEach(scalar -> scalar.setInt(1));\n  Constant<T> y = tf.constant(tensor);\n\n  // Subtract y from x and validate the result\n  Sub<T> sub = tf.math.sub(x, y);\n  sub.asTensor().scalars().forEach(scalar ->\n      assertEquals(-1, scalar.getInt())\n  );\n}\n```\n", "release_dates": ["2023-01-26T22:00:20Z"]}, {"name": "kfac", "description": "An implementation of KFAC for TensorFlow", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# K-FAC: Kronecker-Factored Approximate Curvature\n\n[![Travis](https://img.shields.io/travis/tensorflow/kfac.svg)](https://travis-ci.org/tensorflow/kfac)\n\n**K-FAC in TensorFlow** is an implementation of [K-FAC][kfac-paper], an\napproximate second-order optimization method, in TensorFlow.\n\n[kfac-paper]: https://arxiv.org/abs/1503.05671\n\n## Installation\n\n`kfac` is compatible with Python 2 and 3 and can be installed directly via\n`pip`,\n\n```shell\n# Assumes tensorflow or tensorflow-gpu installed\n$ pip install kfac\n\n# Installs with tensorflow-gpu requirement\n$ pip install 'kfac[tensorflow_gpu]'\n\n# Installs with tensorflow (cpu) requirement\n$ pip install 'kfac[tensorflow]'\n```\n\n## KFAC DOCS\n\nPlease check [KFAC docs][kfac_docs] for a detailed description with examples\nof how to use KFAC. Check the [Keras KFAC docs][keras_docs] for information on\nusing KFAC with Keras.\n\n[kfac_docs]: https://github.com/tensorflow/kfac/tree/master/docs/index.md\n[keras_docs]: https://github.com/tensorflow/kfac/tree/master/kfac/python/keras/README.md\n", "release_dates": []}, {"name": "lattice", "description": "Lattice methods in TensorFlow", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<!-- Copyright 2020 The TensorFlow Lattice Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n=============================================================================-->\n# TensorFlow Lattice\n\nTensorFlow Lattice is a library that implements constrained and interpretable\nlattice based models. It is an implementation of\n[Monotonic Calibrated Interpolated Look-Up Tables](http://jmlr.org/papers/v17/15-243.html)\nin [TensorFlow](https://www.tensorflow.org).\n\nThe library enables you to inject domain knowledge into\nthe learning process through common-sense or policy-driven shape constraints.\nThis is done using a collection of Keras layers that can satisfy constraints\nsuch as monotonicity, convexity and pairwise trust:\n\n* PWLCalibration: piecewise linear calibration of signals.\n* CategoricalCalibration: mapping of categorical inputs into real values.\n* Lattice: interpolated look-up table implementation.\n* Linear: linear function with monotonicity and norm constraints.\n\nThe library also provides easy to setup canned estimators for common use cases:\n\n* Calibrated Linear\n* Calibrated Lattice\n* Random Tiny Lattices (RTL)\n* Crystals\n\nWith TF Lattice you can use domain knowledge to better extrapolate to the parts\nof the input space not covered by the training dataset. This helps avoid\nunexpected model behaviour when the serving distribution is different from the\ntraining distribution.\n\n<div align=\"center\">\n  <img src=\"docs/images/model_comparison.png\">\n</div>\n\nYou can install our prebuilt pip package using\n\n```bash\npip install tensorflow-lattice\n```\n", "release_dates": ["2023-09-28T21:15:44Z", "2023-08-22T21:10:40Z", "2022-10-20T23:44:18Z", "2022-01-13T20:10:11Z", "2021-09-30T20:41:34Z", "2021-02-17T00:45:48Z", "2020-12-14T23:44:09Z", "2020-08-10T22:56:40Z", "2020-06-15T23:12:23Z", "2020-04-14T22:25:43Z", "2020-03-06T20:42:30Z", "2020-02-08T02:33:20Z", "2020-02-04T00:27:04Z", "2020-01-28T22:51:10Z", "2019-07-31T07:02:09Z", "2018-10-08T16:07:54Z", "2018-07-30T22:27:34Z", "2018-02-15T21:33:57Z", "2018-02-01T23:11:34Z", "2017-11-09T03:57:42Z", "2017-10-18T16:57:49Z"]}, {"name": "lingvo", "description": "Lingvo", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Lingvo\n\n[![PyPI](https://badge.fury.io/py/lingvo.svg)](https://badge.fury.io/py/lingvo)\n[![Python](https://img.shields.io/pypi/pyversions/lingvo)](https://badge.fury.io/py/tensorflow)\n\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://tensorflow.github.io/lingvo)\n\n[![License](https://img.shields.io/github/license/tensorflow/lingvo)](LICENSE)\n\n## What is it?\n\nLingvo is a framework for building neural networks in Tensorflow, particularly\nsequence models.\n\nA list of publications using Lingvo can be found [here](PUBLICATIONS.md).\n\n## Table of Contents\n\n*   [Releases](#releases)\n    *   [Major breaking changes](#major-breaking-changes)\n*   [Quick start](#quick-start)\n    *   [Installation](#installation)\n    *   [Running the MNIST image model](#running-the-mnist-image-model)\n    *   [Running the machine translation model](#running-the-machine-translation-model)\n    *   [Running the GShard transformer based giant language model](#running-the-gshard-transformer-based-giant-language-model)\n    *   [Running the 3d object detection model](#running-the-3d-object-detection-model)\n*   [Models](#models)\n    *   [Automatic Speech Recognition](#automatic-speech-recognition)\n    *   [Car](#car)\n    *   [Image](#image)\n    *   [Language Modelling](#language-modelling)\n    *   [Machine Translation](#machine-translation)\n*   [References](#references)\n*   [License](#license)\n\n## Releases\n\nPyPI Version | Commit\n------------ | ----------------------------------------\n0.12.4       | --\n0.11.0       | 6fae10077756f54beacd5c454959f20b33fd65e2\n0.10.0       | 075fd1d88fa6f92681f58a2383264337d0e737ee\n0.9.1        | c1124c5aa7af13d2dd2b6d43293c8ca6d022b008\n0.9.0        | f826e99803d1b51dccbbbed1ef857ba48a2bbefe\n\n<details><summary>\n<b>Older releases</b>\n</summary><p>\n\nPyPI Version | Commit\n------------ | ----------------------------------------\n0.8.2        | 93e123c6788e934e6b7b1fd85770371becf1e92e\n0.7.2        | b05642fe386ee79e0d88aa083565c9a93428519e\n\nDetails for older releases are unavailable.\n\n</p></details>\n\n### Major breaking changes\n\n**NOTE: this is not a comprehensive list. Lingvo releases do not offer any\nguarantees regarding backwards compatibility.**\n\n#### HEAD\n\nNothing here.\n\n#### 0.12.0\n\n*   **General**\n    *   Tensorflow 2.9 is now required.\n    *   Python 3.7 support has been removed.\n    *   Compatible with (up to) Tensorflow 2.10 and Python 3.10\n\n#### 0.11.0\n\n*   **General**\n    *   Tensorflow 2.7 is now the required version.\n    *   Python 3.6 support has been removed.\n\n#### 0.10.0\n\n*   **General**\n    *   Tensorflow 2.6 is now the required version.\n    *   The theta_fn arg to CreateVariable() has been removed.\n\n#### 0.9.1\n\n*   **General**\n    *   Python 3.9 is now supported.\n    *   ops.beam_search_step now takes and returns an additional arg\n        `beam_done`.\n    *   The namedtuple beam_search_helper.BeamSearchDecodeOutput now removes the\n        field `done_hyps`.\n\n#### 0.9.0\n\n*   **General**\n    *   Tensorflow 2.5 is now the required version.\n    *   Python 3.5 support has been removed.\n    *   py_utils.AddGlobalVN and py_utils.AddPerStepVN have been combined into\n        py_utils.AddVN.\n    *   BaseSchedule().Value() no longer takes a step arg.\n    *   Classes deriving from BaseSchedule should implement Value() not FProp().\n    *   theta.global_step has been removed in favor of py_utils.GetGlobalStep().\n    *   py_utils.GenerateStepSeedPair() no longer takes a global_step arg.\n    *   PostTrainingStepUpdate() no longer takes a global_step arg.\n    *   The fatal_errors argument to custom input ops now takes error message\n        substrings rather than integer error codes.\n\n<details><summary>\n<b>Older releases</b>\n</summary><p>\n\n#### 0.8.2\n\n*   **General**\n    *   NestedMap Flatten/Pack/Transform/Filter etc now expand descendent dicts\n        as well.\n    *   Subclasses of BaseLayer extending from `abc.ABCMeta` should now extend\n        `base_layer.ABCLayerMeta` instead.\n    *   Trying to call self.CreateChild outside of `__init__` now raises an\n        error.\n    *   `base_layer.initializer` has been removed. Subclasses no longer need to\n        decorate their `__init__` function.\n    *   Trying to call self.CreateVariable outside of `__init__` or\n        `_CreateLayerVariables` now raises an error.\n    *   It is no longer possible to access self.vars or self.theta inside of\n        `__init__`. Refactor by moving the variable creation and access to\n        `_CreateLayerVariables`. The variable scope is set automatically\n        according to the layer name in `_CreateLayerVariables`.\n\nDetails for older releases are unavailable.\n\n</p></details>\n\n## Quick start\n\n### Installation\n\nThere are two ways to set up Lingvo: installing a fixed version through pip, or\ncloning the repository and building it with bazel. Docker configurations are\nprovided for each case.\n\nIf you would just like to use the framework as-is, it is easiest to just install\nit through pip. This makes it possible to develop and train custom models using\na frozen version of the Lingvo framework. However, it is difficult to modify the\nframework code or implement new custom ops.\n\nIf you would like to develop the framework further and potentially contribute\npull requests, you should avoid using pip and clone the repository instead.\n\n**pip:**\n\nThe [Lingvo pip package](https://pypi.org/project/lingvo) can be installed with\n`pip3 install lingvo`.\n\nSee the\n[codelab](https://colab.research.google.com/github/tensorflow/lingvo/blob/master/codelabs/introduction.ipynb)\nfor how to get started with the pip package.\n\n**From sources:**\n\nThe prerequisites are:\n\n*   a TensorFlow 2.7 [installation](https://www.tensorflow.org/install/),\n*   a `C++` compiler (only g++ 7.3 is officially supported), and\n*   the bazel build system.\n\nRefer to [docker/dev.Dockerfile](docker/dev.Dockerfile) for a set of working\nrequirements.\n\n`git clone` the repository, then use bazel to build and run targets directly.\nThe `python -m module` commands in the codelab need to be mapped onto `bazel\nrun` commands.\n\n**docker:**\n\nDocker configurations are available for both situations. Instructions can be\nfound in the comments on the top of each file.\n\n*   [lib.dockerfile](docker/lib.dockerfile) has the Lingvo pip package\n    preinstalled.\n*   [dev.Dockerfile](docker/dev.Dockerfile) can be used to build Lingvo from\n    sources.\n\n[How to install docker.](https://docs.docker.com/install/linux/docker-ce/ubuntu/)\n\n### Running the MNIST image model\n\n#### Preparing the input data\n\n**pip:**\n\n```shell\nmkdir -p /tmp/mnist\npython3 -m lingvo.tools.keras2ckpt --dataset=mnist\n```\n\n**bazel:**\n\n```shell\nmkdir -p /tmp/mnist\nbazel run -c opt //lingvo/tools:keras2ckpt -- --dataset=mnist\n```\n\nThe following files will be created in `/tmp/mnist`:\n\n*   `mnist.data-00000-of-00001`: 53MB.\n*   `mnist.index`: 241 bytes.\n\n#### Running the model\n\n**pip:**\n\n```shell\ncd /tmp/mnist\ncurl -O https://raw.githubusercontent.com/tensorflow/lingvo/master/lingvo/tasks/image/params/mnist.py\npython3 -m lingvo.trainer --run_locally=cpu --mode=sync --model=mnist.LeNet5 --logdir=/tmp/mnist/log\n```\n\n**bazel:**\n\n```shell\n(cpu) bazel build -c opt //lingvo:trainer\n(gpu) bazel build -c opt --config=cuda //lingvo:trainer\nbazel-bin/lingvo/trainer --run_locally=cpu --mode=sync --model=image.mnist.LeNet5 --logdir=/tmp/mnist/log --logtostderr\n```\n\nAfter about 20 seconds, the loss should drop below 0.3 and a checkpoint will be\nsaved, like below. Kill the trainer with Ctrl+C.\n\n```\ntrainer.py:518] step:   205, steps/sec: 11.64 ... loss:0.25747201 ...\ncheckpointer.py:115] Save checkpoint\ncheckpointer.py:117] Save checkpoint done: /tmp/mnist/log/train/ckpt-00000205\n```\n\nSome artifacts will be produced in `/tmp/mnist/log/control`:\n\n*   `params.txt`: hyper-parameters.\n*   `model_analysis.txt`: model sizes for each layer.\n*   `train.pbtxt`: the training `tf.GraphDef`.\n*   `events.*`: a tensorboard events file.\n\nAs well as in `/tmp/mnist/log/train`:\n\n*   `checkpoint`: a text file containing information about the checkpoint files.\n*   `ckpt-*`: the checkpoint files.\n\nNow, let's evaluate the model on the \"Test\" dataset. In the normal training\nsetup the trainer and evaler should be run at the same time as two separate\nprocesses.\n\n**pip:**\n\n```shell\npython3 -m lingvo.trainer --job=evaler_test --run_locally=cpu --mode=sync --model=mnist.LeNet5 --logdir=/tmp/mnist/log\n```\n\n**bazel:**\n\n```shell\nbazel-bin/lingvo/trainer --job=evaler_test --run_locally=cpu --mode=sync --model=image.mnist.LeNet5 --logdir=/tmp/mnist/log --logtostderr\n```\n\nKill the job with Ctrl+C when it starts waiting for a new checkpoint.\n\n```\nbase_runner.py:177] No new check point is found: /tmp/mnist/log/train/ckpt-00000205\n```\n\nThe evaluation accuracy can be found slightly earlier in the logs.\n\n```\nbase_runner.py:111] eval_test: step:   205, acc5: 0.99775392, accuracy: 0.94150388, ..., loss: 0.20770954, ...\n```\n\n### Running the machine translation model\n\nTo run a more elaborate model, you'll need a cluster with GPUs. Please refer to\n[`third_party/py/lingvo/tasks/mt/README.md`](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/mt/README.md)\nfor more information.\n\n### Running the GShard transformer based giant language model\n\nTo train a GShard language model with one trillion parameters on GCP using\nCloudTPUs v3-512 using 512-way model parallelism, please refer to\n[`third_party/py/lingvo/tasks/lm/README.md`](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/lm/README.md)\nfor more information.\n\n### Running the 3d object detection model\n\nTo run the StarNet model using CloudTPUs on GCP, please refer to\n[`third_party/py/lingvo/tasks/car/README.md`](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/car/README.md).\n\n## Models\n\n### Automatic Speech Recognition\n\n*   [Listen, Attend and Spell](https://arxiv.org/pdf/1508.01211.pdf).<br/>\n    William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals. ICASSP 2016.\n\n    [End-to-end Continuous Speech Recognition using Attention-based Recurrent\n    NN: First Results](https://arxiv.org/pdf/1412.1602.pdf).<br/>Jan Chorowski,\n    Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. arXiv 2014.\n\n    *   [asr.librispeech.Librispeech960Grapheme](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/asr/params/librispeech.py)\n    *   [asr.librispeech.Librispeech960Wpm](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/asr/params/librispeech.py)\n\n### Car\n*   [DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection](https://arxiv.org/pdf/2203.08195.pdf).<br/>\n    Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang Shen, Bo Wu, Yifeng Lu, Denny \n    Zhou, Quoc V. Le, Alan Yuille, Mingxing Tan. CVPR 2022.\n    *   [car.waymo_deepfusion.DeepFusionCenterPointPed](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/car/params/waymo_deepfusion.py)\n\n*   [StarNet: Targeted Computation for Object Detection in Point Clouds](https://arxiv.org/pdf/1908.11069.pdf).<br/>\n    Jiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang, Yuning Chai, Pei Sun, Yin\n    Zhou, Xi Yi, Ouais Alsharif, Patrick Nguyen, Zhifeng Chen, Jonathon Shlens,\n    and Vijay Vasudevan. arXiv 2019.\n\n    *   [car.kitti.StarNetCarModel0701](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/car/params/kitti.py)\n    *   [car.kitti.StarNetPedCycModel0704](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/car/params/kitti.py)\n    *   [car.waymo.StarNetVehicle](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/car/params/waymo.py)\n    *   [car.waymo.StarNetPed](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/car/params/waymo.py)\n\n### Image\n\n*   [Gradient-based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf).<br/>\n    Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. IEEE 1998.\n\n    *   [image.mnist.LeNet5](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/image/params/mnist.py)\n\n### Language Modelling\n\n*   [Exploring the Limits of Language Modeling](https://arxiv.org/pdf/1602.02410.pdf).<br/>\n    Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui\n    Wu. arXiv, 2016.\n\n    *   [lm.one_billion_wds.WordLevelOneBwdsSimpleSampledSoftmax](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/lm/params/one_billion_wds.py)\n\n*   [GShard: Scaling Giant Models with Conditional Computation and Automatic\n    Sharding](https://arxiv.org/pdf/2006.16668.pdf).<br/>\n    Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun,\n    Noam Shazeer and Zhifeng Chen arXiv, 2020.\n\n    *   [lm.synthetic_packed_input.DenseLm1T16x16](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/lm/params/synthetic_packed_input.py)\n\n### Machine Translation\n\n*   [The Best of Both Worlds: Combining Recent Advances in Neural Machine\n    Translation](http://aclweb.org/anthology/P18-1008).<br/>\n    Mia X. Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion\n    Jones, Mike Schuster, Noam Shazeer, Niki Parmar, Ashish Vaswani, Jakob\n    Uszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui Wu, and Macduff Hughes.\n    ACL 2018.\n\n    *   [mt.wmt14_en_de.WmtEnDeTransformerBase](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/mt/params/wmt14_en_de.py)\n    *   [mt.wmt14_en_de.WmtEnDeRNMT](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/mt/params/wmt14_en_de.py)\n    *   [mt.wmtm16_en_de.WmtCaptionEnDeTransformer](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/mt/params/wmtm16_en_de.py)\n\n*   [Self-supervised and Supervised Joint Training for Resource-rich Neural\n    Machine Translation](https://arxiv.org/pdf/2106.04060.pdf).<br/>\n    Yong Cheng, Wei Wang, Lu Jiang, and Wolfgang Macherey. ICML 2021.\n\n    *   [mt.xendec.wmt14_en_de.WmtEnDeXEnDec](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/mt/params/xendec/wmt14_en_de.py)\n\n## References\n\n*   [API Docs](https://tensorflow.github.io/lingvo/)\n*   [Codelab](https://colab.research.google.com/github/tensorflow/lingvo/blob/master/codelabs/introduction.ipynb)\n\nPlease cite this [paper](https://arxiv.org/abs/1902.08295) when referencing\nLingvo.\n\n```\n@misc{shen2019lingvo,\n    title={Lingvo: a Modular and Scalable Framework for Sequence-to-Sequence Modeling},\n    author={Jonathan Shen and Patrick Nguyen and Yonghui Wu and Zhifeng Chen and others},\n    year={2019},\n    eprint={1902.08295},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n```\n\n## License\n\n[Apache License 2.0](LICENSE)\n", "release_dates": []}, {"name": "lucid", "description": "A collection of infrastructure and tools for research in neural network interpretability.", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<img src=\"https://storage.googleapis.com/lucid-static/common/stickers/channels-visualizations.jpg\" width=\"782\"></img>\n\n# Lucid\n\n<!--*DeepDream, but sane. Home of cats, dreams, and interpretable neural networks.*-->\n\n[![PyPI project status](https://img.shields.io/pypi/status/Lucid.svg)]()\n[![Travis build status](https://img.shields.io/travis/tensorflow/lucid.svg)](https://travis-ci.org/tensorflow/lucid)\n[![Code coverage](https://img.shields.io/coveralls/github/tensorflow/lucid.svg)](https://coveralls.io/github/tensorflow/lucid)\n[![Supported Python version](https://img.shields.io/pypi/pyversions/Lucid.svg)]()\n[![PyPI release version](https://img.shields.io/pypi/v/Lucid.svg)](https://pypi.org/project/Lucid/)\n\n\nLucid is a collection of infrastructure and tools for research in neural\nnetwork interpretability.\n\n**We're not currently supporting tensorflow 2!**\n\nIf you'd like to use lucid in colab which defaults to tensorflow 2, add this magic to a cell before you import tensorflow:\n\n```%tensorflow_version 1.x```\n\n**Lucid is research code, not production code. We provide no guarantee it will work for your use case. Lucid is maintained by volunteers who are unable to provide significant technical support.**\n\n* [\ud83d\udcd3\u2002**Notebooks**](#notebooks) -- Get started without any setup!\n* [\ud83d\udcda\u2002**Reading**](#recomended-reading) -- Learn more about visualizing neural nets.\n* [\ud83d\udcac\u2002**Community**](#community) -- Want to get involved? Please reach out!\n* [\ud83d\udd27\u2002**Additional Information**](#additional-information) -- Licensing, code style, etc.\n* [\ud83d\udd2c\u2002**Start Doing Research!**](https://github.com/tensorflow/lucid/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3Aresearch) -- Want to get involved? We're trying to research openly!\n* [\ud83d\udce6 **Visualize your own model**](https://github.com/tensorflow/lucid/wiki/Importing-Models-into-Lucid) -- How to import your own model for visualization\n\n<br>\n\n# Notebooks\n\nStart visualizing neural networks ***with no setup***. The following notebooks\nrun right from your browser, thanks to [Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb). It's a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud.\n\nYou can run the notebooks on your local machine, too. Clone the repository and find them in the `notebooks` subfolder. You will need to run a local instance of the [Jupyter notebook environment](http://jupyter.org/install.html) to execute them.\n\n## Tutorial Notebooks\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/tutorial.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/common/stickers/colab-tutorial.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/modelzoo.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/common/stickers/colab-modelzoo.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<!--If you want to study techniques for visualizing and understanding neural networks, it's important to be able to try your experiments on multiple models. As of lucid v0.3, we provide a consistent API for interacting with 27 different vision models.-->\n\n## Feature Visualization Notebooks\n*Notebooks corresponding to the [Feature Visualization](https://distill.pub/2017/feature-visualization/) article*\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/feature-visualization/stickers/colab-neuron-negative.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/neuron_diversity.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/feature-visualization/stickers/colab-neuron-diversity.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/neuron_interaction.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/feature-visualization/stickers/colab-neuron-interaction.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/regularization.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/feature-visualization/stickers/colab-regularization.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n## Building Blocks Notebooks\n*Notebooks corresponding to the [Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/) article*\n\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/SemanticDictionary.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/building-blocks/stickers/colab-semantic-dict.png\" width=\"500\" alt=\"\"></img>\n</a>\n<br>\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/ActivationGrid.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/building-blocks/stickers/colab-grid.png\" width=\"500\" alt=\"\"></img>\n</a>\n<br>\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/AttrSpatial.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/building-blocks/stickers/colab-spatial-attr.png\" width=\"500\" alt=\"\"></img>\n</a>\n<br>\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/AttrChannel.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/building-blocks/stickers/colab-channel-attr.png\" width=\"500\" alt=\"\"></img>\n</a>\n<br>\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/NeuronGroups.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/building-blocks/stickers/colab-neuron-groups.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n\n## Differentiable Image Parameterizations Notebooks\n*Notebooks corresponding to the [Differentiable Image Parameterizations](https://distill.pub/2018/differentiable-parameterizations/) article*\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/aligned_interpolation.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/differentiable-parameterizations/stickers/colab-interpolate.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/style_transfer_2d.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/differentiable-parameterizations/stickers/colab-style-beyond-vgg.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/xy2rgb.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/differentiable-parameterizations/stickers/colab-xy2rgb.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/transparency.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/differentiable-parameterizations/stickers/colab-transparent.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/texture_synth_3d.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/differentiable-parameterizations/stickers/colab-3d-texture.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/style_transfer_3d.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/differentiable-parameterizations/stickers/colab-3d-style.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<br>\n\n## Activation Atlas Notebooks\n*Notebooks corresponding to the [Activation Atlas](https://distill.pub/2019/activation-atlas/) article*\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/activation-atlas/activation-atlas-collect.ipynb\">\n<img src=\"https://storage.googleapis.com/modelzoo/tmp/activation-atlas/stickers/lucid-notebook-1-collect.png\" width=\"500\" alt=\"Collecting activations\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/activation-atlas/activation-atlas-simple.ipynb\">\n<img src=\"https://storage.googleapis.com/modelzoo/tmp/activation-atlas/stickers/lucid-notebook-2-atlas.png\" width=\"500\" alt=\"Simple activation atlas\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/activation-atlas/class-activation-atlas.ipynb\">\n<img src=\"https://storage.googleapis.com/modelzoo/tmp/activation-atlas/stickers/lucid-notebook-3-class-atlas.png\" width=\"500\" alt=\"Class activation atlas\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/activation-atlas/activation-atlas-adversarial.ipynb\">\n<img src=\"https://storage.googleapis.com/modelzoo/tmp/activation-atlas/stickers/lucid-notebook-4-patches.png\" width=\"500\" alt=\"Activation atlas patches\"></img>\n</a>\n\n## Miscellaneous Notebooks\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/misc/feature_inversion_caricatures.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/misc/stickers/colab-feature-inversion.ipynb.png\" width=\"500\" alt=\"\"></img>\n</a>\n<br>\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/misc/neuron_interaction_grids.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/misc/stickers/colab-interaction-grid.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<br> \n\n# Recomended Reading\n\n* [Feature Visualization](https://distill.pub/2017/feature-visualization/)\n* [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/)\n* [Using Arti\ufb01cial Intelligence to Augment Human Intelligence](https://distill.pub/2017/aia/)\n* [Visualizing Representations: Deep Learning and Human Beings](http://colah.github.io/posts/2015-01-Visualizing-Representations/)\n* [Differentiable Image Parameterizations](https://distill.pub/2018/differentiable-parameterizations/)\n* [Activation Atlas](https://distill.pub/2019/activation-atlas/)\n\n## Related Talks\n* [Lessons from a year of Distill ML Research](https://www.youtube.com/watch?v=jlZsgUZaIyY) (Shan Carter, OpenVisConf)\n* [Machine Learning for Visualization](https://www.youtube.com/watch?v=6n-kCYn0zxU) (Ian Johnson, OpenVisConf)\n\n# Community\n\nWe're in `#proj-lucid` on the Distill slack ([join link](http://slack.distill.pub)).\n\nWe'd love to see more people doing research in this space!\n\n<br>\n\n# Additional Information\n\n## License and Disclaimer\n\nYou may use this software under the Apache 2.0 License. See [LICENSE](LICENSE).\n\nThis project is research code. It is not an official Google product.\n\n## Special consideration for TensorFlow dependency\n\nLucid requires `tensorflow`, but does not explicitly depend on it in `setup.py`. Due to the way [tensorflow is packaged](https://github.com/tensorflow/tensorflow/issues/7166) and some deficiencies in how pip handles dependencies, specifying either the GPU or the non-GPU version of tensorflow will conflict with the version of tensorflow your already may have installed.\n\nIf you don't want to add your own dependency on tensorflow, you can specify which tensorflow version you want lucid to install by selecting from `extras_require` like so: `lucid[tf]` or `lucid[tf_gpu]`.\n\n**In actual practice, we recommend you use your already installed version of tensorflow.**\n", "release_dates": ["2021-03-19T15:59:35Z", "2019-01-17T19:46:43Z", "2018-12-20T00:25:24Z", "2018-12-19T20:30:43Z", "2018-11-29T04:49:51Z", "2018-11-29T04:10:31Z", "2018-11-03T00:39:15Z", "2018-08-08T23:12:42Z", "2018-07-13T19:10:31Z", "2018-05-21T20:16:18Z", "2018-05-21T20:16:04Z", "2018-04-30T21:36:58Z", "2018-04-17T03:48:27Z", "2018-04-17T03:49:05Z", "2018-03-06T04:52:17Z"]}, {"name": "mesh", "description": "Mesh TensorFlow: Model Parallelism Made Easier", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Mesh TensorFlow - Model Parallelism Made Easier\n\n[![PyPI\nversion](https://badge.fury.io/py/mesh-tensorflow.svg)](https://badge.fury.io/py/mesh-tensorflow)\n[![GitHub\nIssues](https://img.shields.io/github/issues/tensorflow/mesh.svg)](https://github.com/tensorflow/mesh/issues)\n[![Contributions\nwelcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Build Status](https://github.com/tensorflow/mesh/workflows/build/badge.svg)](https://github.com/tensorflow/mesh/actions?query=workflow%3Abuild)\n\n\n# Introduction\n\nMesh TensorFlow (`mtf`) is a language for distributed deep learning, capable of\nspecifying a broad class of distributed tensor computations.  The purpose of\nMesh TensorFlow is to formalize and implement distribution strategies for your\ncomputation graph over your hardware/processors. For example: \"Split the batch\nover rows of processors and split the units in the hidden layer across columns\nof processors.\" Mesh TensorFlow is implemented as a layer over TensorFlow.\n\nWatch our [YouTube video](https://www.youtube.com/watch?v=HgGyWS40g-g).\n\n\n## Do I need Mesh TensorFlow?\n\nIf you just want data-parallel training (batch-splitting), then you do not need\nMesh TensorFlow, though Mesh TensorFlow can do this.  The most common reasons\nfor more sophisticated parallel computation are:\n\n* The parameters of the model do not fit on one device - e.g. a\n5-billion-parameter language model.\n\n* An example is so large that the activations do not fit on one device. - e.g.\nlarge 3D image model([`experimental/unet.py`](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/experimental/unet.py)).\n\n* Lower-latency parallel inference (at batch size 1).\n\n## The Mesh TensorFlow Approach to Distributed Computation\n\n* A \"Mesh\" is an n-dimensional array of processors, connected by a network.\n\n* Each tensor is distributed (split and/or replicated) across all processors\n  in a mesh.\n\n* Tensor dimensions and mesh dimensions are named.  The layouts of all tensors\n  follow from a set of user-defined layout rules which specify which\n  tensor-dimensions are split across which mesh-dimensions.  This ensures that\n  the corresponding dimensions in different tensors are split in the same\n  manner.\n\n* Layouts do not affect results - only performance.\n\n* The implementation of an operation involves parallel computation on all\n  processors in the mesh, and sometimes also collective communication.  A\n  processor usually just manipulates the slices of the input tensors already\n  resident on that processor, and produces the slice of the output that goes on\n  that processor.\n\n## Getting Started\n\n### Installation\n\nTo install the latest stable version, run\n\n```sh\npip install mesh-tensorflow\n```\n\nTo install the latest development version, run\n\n```sh\npip install -e \"git+https://github.com/tensorflow/mesh.git#egg=mesh-tensorflow\"\n```\n\nInstalling `mesh-tensorflow` does not automatically install or update\nTensorFlow. We recommend installing it via `pip install tensorflow` or `pip\ninstall tensorflow-gpu`. See TensorFlow\u2019s\n[installation instructions for details](https://www.tensorflow.org/install/).\nIf you're using a development version of Mesh TensorFlow, you may need to\nuse TensorFlow's nightly package (`tf-nightly`).\n\n### Example Network (MNIST)\n\nTo illustrate, let us consider a simple model for the MNIST image-classification\ntask.  Our network has one hidden layer with 1024 units, and an output layer\nwith 10 units (corresponding to the 10 digit classes).\n\nThe code consists of two parts, the first describing the mathematical\noperations, and the second describing the devices and tensor/computation layout.\nFor the full example, see [`examples/mnist.py`](\nhttps://github.com/tensorflow/mesh/blob/master/examples/mnist.py).\nTODO(noam): verify that this code works.\n\n```Python\n# tf_images is a tf.Tensor with shape [100, 28, 28] and dtype tf.float32\n# tf_labels is a tf.Tensor with shape [100] and dtype tf.int32\ngraph = mtf.Graph()\nmesh = mtf.Mesh(graph, \"my_mesh\")\nbatch_dim = mtf.Dimension(\"batch\", 100)\nrows_dim = mtf.Dimension(\"rows\", 28)\ncols_dim = mtf.Dimension(\"cols\", 28)\nhidden_dim = mtf.Dimension(\"hidden\", 1024)\nclasses_dim = mtf.Dimension(\"classes\", 10)\nimages = mtf.import_tf_tensor(\n    mesh, tf_images, shape=[batch_dim, rows_dim, cols_dim])\nlabels = mtf.import_tf_tensor(mesh, tf_labels, [batch_dim])\nw1 = mtf.get_variable(mesh, \"w1\", [rows_dim, cols_dim, hidden_dim])\nw2 = mtf.get_variable(mesh, \"w2\", [hidden_dim, classes_dim])\n# einsum is a generalization of matrix multiplication (see numpy.einsum)\nhidden = mtf.relu(mtf.einsum(images, w1, output_shape=[batch_dim, hidden_dim]))\nlogits = mtf.einsum(hidden, w2, output_shape=[batch_dim, classes_dim])\nloss = mtf.reduce_mean(mtf.layers.softmax_cross_entropy_with_logits(\n    logits, mtf.one_hot(labels, classes_dim), classes_dim))\nw1_grad, w2_grad = mtf.gradients([loss], [w1, w2])\nupdate_w1_op = mtf.assign(w1, w1 - w1_grad * 0.001)\nupdate_w2_op = mtf.assign(w2, w2 - w2_grad * 0.001)\n```\n\nIn the code above, we have built a Mesh TensorFlow graph, which is simply\na Python structure.  We have completely defined the mathematical operations.\nIn the code below, we specify the mesh of processors and the layout of the\ncomputation.\n\n```Python\ndevices = [\"gpu:0\", \"gpu:1\", \"gpu:2\", \"gpu:3\"]\nmesh_shape = [(\"all_processors\", 4)]\nlayout_rules = [(\"batch\", \"all_processors\")]\nmesh_impl = mtf.placement_mesh_impl.PlacementMeshImpl(\n    mesh_shape, layout_rules, devices)\nlowering = mtf.Lowering(graph, {mesh:mesh_impl})\ntf_update_ops = [lowering.lowered_operation(update_w1_op),\n                 lowering.lowered_operation(update_w2_op)]\n```\n\nThe particular layout above implements data-parallelism, splitting the batch of\nexamples evenly across all four processors.  Any Tensor with a \"batch\" dimension\n(e.g. `images`, `h`, `logits`, and their gradients) is split in that dimension\nacross all processors, while any tensor without a \"batch\" dimension (e.g. the\nmodel parameters) is replicated identically on every processor.\n\nAlternatively, for model-parallelism, we can set\n`layout_rules=[(\"hidden\", \"all_processors\")]`.  In this case,\nany tensor with a \"hidden\" dimension (e.g. `hidden`, `w1`, `w2`)  is split,\nwhile any other tensor (e.g. `image`, `logits`) is fully replicated.\n\nWe can even combine data-parallelism and model-parallelism on a 2-dimensional\nmesh of processors.  We split the batch along one dimension of the mesh, and the\nunits in the hidden layer along the other dimension of the mesh, as below.  In\nthis case, the hidden layer is actually tiled between the four processors, being\nsplit in both the \"batch\" and \"hidden_units\" dimensions.\n\n```Python\nmesh_shape = [(\"processor_rows\", 2), (\"processor_cols\", 2)]\nlayout_rules = [(\"batch\", \"processor_rows\"), (\"hidden\", \"processor_cols\")]\n```\n\n## Where does the network communication happen?\n\nSome Mesh TensorFlow operations cause network communication.  For example, an\neinsum (generalized matrix multiplication) is computed as follows:\n\n* On each processor, compute the einsum of the slices of the two operands that\n  are local to that processor.\n* If no reduced-out dimensions are split, then we are done.\n* If reduced-out dimensions are split, then perform an \"allreduce\" operation \n  on the resulting slices - summing across any mesh dimensions over which the\n  reduced-out dimensions are split.\n\nWhere the allreduces happen depends will depend on the computation layout.\nFor example, in a data-parallel layout where the \"batch\" dimension is split,\nallreduces will happen when computing the parameter gradients, since this\ninvolves matrix multiplications which reduce out the \"batch\" dimension.\n\n## How do I pick a layout?\n\nWhile results do not depend on layout (except in the realm of roundoff errors\nand random seeds), performance and memory consumption depend heavily on layout.\nFortunately, the auto_mtf subpackage provides a method for automatically\nchoosing a layout.  For more information about what auto_mtf is doing to choose\na layout, see its [README](mesh_tensorflow/auto_mtf/README.md) file.\n\n```Python\nimport mesh_tensorflow.auto_mtf\n\ngraph = mtf.Graph()\nmesh = mtf.Mesh(graph, \"my_mesh\")\n# Insert model code here.\noutputs = [logits, loss]  # iterable of mtf.Tensor, the outputs you're computing\nmesh_shape = [(\"processor_rows\", 2), (\"processor_cols\", 2)]\nlayout_rules = mtf.auto_mtf.layout(graph, mesh_shape, outputs)\n```\n\nIt is possible for advanced users to eke out additional performance by tuning\nthe layout (and model) further.  Mesh TensorFlow helps by accumulating and\nprinting counters of computation/communication.  To start, here are some\ntricks/guidelines.\n\n* It is illegal for two dimensions of the same tensor to be split across the\n  same mesh dimension.\n* For any compute-intense operation (e.g. einsum), make sure that all\n  mesh-dimensions are used to split dimensions of the inputs or outputs.\n  Otherwise, computation is duplicated.\n* To keep the ratio of compute/communication high (i.e. not be bandwidth-bound),\n  split dimensions into large chunks.  This should be familiar in the\n  data-parallelism case, where we want a large batch size per processor to avoid\n  spending most of our time communicating.\n\n# The Mesh TensorFlow Language\n\nMesh TensorFlow (v0.0) is implemented as a Python library which can generate\npart of a TensorFlow graph.  The user first builds a `mtf.Graph` (the analog of\na TensorFlow graph) made up of `mtf.Tensor`s and `mtf.Operation`s.  As in\nTensorFlow, this graph consists of simple Python objects.  The user then creates\na `mtf.Lowering` object, which lowers the `mtf.Graph` into TensorFlow, adding to\nthe default TensorFlow graph.\n\nThe Mesh TensorFlow language is nearly identical to TensorFlow, with the\nfamiliar notion of a Graph, Tensors, Operations, and automatic gradient\ncomputation.  The principal differences are as follows:\n\n## Meshes replace devices\n\nA `Mesh` is a n-dimensional array of processors with named dimensions.  Each\n`Tensor` is assigned to a `Mesh`, instead of a device.\n\n## Tensor dimensions are named\n\nEach `Tensor` has a static `Shape`, which is a tuple of different \"Dimensions\".\nA `Dimension` is a `(name, size)` pair. For example, the shape of a `Tensor`\nrepresenting a batch of images might be:\n\n`[(\"batch\", 100), (\"rows\", 28\"), (\"cols\", 28), (\"channels\", 3)]`.\n\n## Layouts\n\nA `Tensor` is laid out on its mesh with one slice on each processor.  A `Tensor`\n\"layout\", is an injective partial map specifying which dimensions of the tensor\nare (evenly) split across which dimensions of the mesh.  No dimension of a\ntensor may be split across two dimensions of its mesh and no two dimensions of a\ntensor may be split across the same dimension of its mesh.  The user defines a\nglobal set of layout rules in the form of (tensor-dimension-name,\nmesh-dimension-name) pairs.  A dimension of a tensor is split across a dimension\nof its mesh if there is a matching rule.\n\n### Example Layouts\n\nTake our example `Tensor` `image_batch` with shape: \n`[(\"batch\", 100), (\"rows\", 28\"), (\"cols\", 28), (\"channels\", 3)]`\n\nAssume that this `Tensor` is assigned to a mesh of 8 processors with shape:\n`[(\"processor_rows\", 2), (\"processor_cols\", 4)]`\n\n* If we use an empty set of layout rules `[]`, we get no splitting.  Each\n  processor contains the whole `Tensor`.\n\n* If we use the layout rules `\"batch:processor_cols\"`, then the `\"batch\"`\n  dimension of the `Tensor` is split across the `\"processor_cols\"` dimension of\n  the batch.  This means that each processor contains a Tensor slice with shape\n  `[25, 28, 28, 3]`.  For example, processors (0, 3) and (1, 3) contain\n  identical slices - `image_batch[75:100, :, :, :]`.\n\n* If we use the layout rules `\"rows:processor_rows;cols:processor_cols\"`, \n  then the image is split in two dimensions, with each processor containing one\n  spatial tile with shape `[100, 14, 7, 3]`.   For example, processor (0, 1)\n  contains the slice `image_batch[:, 0:14, 7:14, :]`.\n\nSome layout rules would lead to illegal layouts:\n\n* `\"batch:processor_rows;rows:processor_rows\"` is illegal because two tensor\n  dimensions could not be split across the same mesh dimension.\n\n* `\"channels:processor_rows\"` is illegal because the size of the tensor\n  dimension is not evenly divisible by the size of the mesh dimension.\n\n## Einsum\n\nMesh TensorFlow uses Einstein-summation notation, `mtf.einsum(inputs,\noutput_shape)`, using the (named) `Dimensions` as the symbols.  Matrix\nmultiplication, broadcast, sum-reduction, and transposition can all be expressed\nas special cases of `mtf.einsum`, though the familiar interfaces are also\nsupported.  The operation is lowered to slice-wise `tf.einsum`s, followed by\nallreduce across any mesh-dimensions corresponding to the summed-out Tensor\ndimensions.\n\n## Reshape can be expensive\n\n`mtf.reshape(x, new_shape)` is used to change a `Tensor`'s shape, potentially\nleading to a new tensor layout and hence network communication.\n\n# CPU/GPU/TPU implementations\n\nMesh TensorFlow works on CPU, GPU and TPU.  The TPU implementation is very\ndifferent from the CPU/GPU implementation.\n\nMulti-CPU/GPU meshes are implemented with `PlacementMeshImpl`.  In this case\nMesh TensorFlow emits separate TensorFlow operations placed on the different\ndevices, all in one big TensorFlow graph.\n\nTPU meshes are implemented in with `SimdMeshImpl`.  In this case,\nMesh TensorFlow emits TensorFlow operations (and communication collectives) from\nthe perspective of one core, and this same program runs on every core, relying\non the fact that each core actually performs the same operations.  This\npiggy-backs on the TPU data-parallelism infrastructure, which operates the same\nway.  This \"SIMD\" approach keeps the TensorFlow and XLA graphs from growing with\nthe number of cores.  The differences between cores are as follows:\n\n* different slices of the variables (this works now)\n* different positions in the collective communication (this works now)\n* different slices of the infed and outfed tensors.  We currently work around\n  this by requiring that all imported/exported tensors be fully-replicated.  In\n  the future, we should handle this correctly.\n\n# Experimental features\n\nThe input pipeline of Mesh Tensorflow models might become a bottleneck, when\ntraining with large input (e.g., high resolution images). We provide new APIs\nand a new input pipeline for you to run Mesh Tensorflow models. You can find\nthem under the [`experimental/`](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/experimental/)\nfolder. We suggest that you give them a try when your input is so large that\nrunning Mesh Tensorflow models with the default APIs is almost infeasible.\nTo be more specific:\n\n* The BROADCAST mode in TPUEstimator does not scale up to large inputs (images\n  of tens of millions of pixels). We provide a new input pipeline:\n  [`experimental/input_reader.py`](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/experimental/input_reader.py).\n  See [`experimental/model_executor.py`](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/experimental/model_executor.py)\n  on how to use it.\n* If your model takes images as input and has convolution layers. You cannot\n  directly map image height and width dimensions to mesh dimensions, due to the\n  sliding-window nature of convolution. Instead, you should use spatial\n  partitioning. We provide examples in\n  [`experimental/unet.py`](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/experimental/unet.py).\n* If you want more control on the training and evaluation loop, instead of using\n  the default API (TPUEstimator) to run your model, you can use low level APIs\n  in [`experimental/model_executor.py`](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/experimental/model_executor.py).\n\nNote that we did not test the experimental code on GPUs. We ran them on TPUs.\nWe believe that some debugging would be required for it to work on GPUs.\n\n# Instructions for running on cloud-tpu\n\nNote: It requires `tensorflow>=1.11.0`.\n\n## Prerequisite\n\nPlease go through the\n[Transformer tutorial](https://cloud.google.com/tpu/docs/tutorials/transformer).\n\n## Create VM and TPU instance in Cloud console\n\nTODO(trandustin,ylc): update given mtf pypi package\n\n```sh\nctpu up -name=ylc-mtf-donut -tf-version=nightly -tpu-size=v2-8 -zone=us-central1-b\n```\n\n## SSH into VM\n\n```sh\ngit clone https://github.com/tensorflow/mesh.git\ncd mesh/\npip install --user .\n```\n\n## Run the Transfomer model (no Tensor2Tensor dependencies)\n\n```sh\npip install tensorflow_datasets\n\ncd mesh/\nDATA_DIR=gs://noam-mtf/data\nMODEL_DIR=gs://noam-mtf/transformer_standalone\nTPU=noam-mtf-donut\n\n# MODEL HPARAMS AND DIRECTORY  (uncomment one)\n# base model\nMODEL=./transformer/gin/model_base.gin\n# 5B parameters (too big for this dataset, only trains with model-parallelism)\n# MODEL=./transformer/gin/model_5b.gin\n\n# UNCOMMENT ONE OF THESE\n# Data-parallelism\nLAYOUT=./transformer/gin/layout_data_parallel.gin\n# Model-parallelism\n# LAYOUT=./transformer/gin/layout_model_parallel.gin\n# Data-parallelism and Model-Parallelism\n# LAYOUT=./transformer/gin/layout_data_and_model_parallel.gin\n\n# TRAIN\npython examples/transformer_standalone.py \\\n  --tpu=$TPU --data_dir=$DATA_DIR --model_dir=$MODEL_DIR --gin_file=$MODEL \\\n  --gin_file=$LAYOUT --gin_param=\"run.mode='train'\"\n\n# EVAL\npython examples/transformer_standalone.py \\\n  --tpu=$TPU --data_dir=$DATA_DIR --model_dir=$MODEL_DIR --gin_file=$MODEL \\\n  --gin_file=$LAYOUT --gin_param=\"run.mode='evaluate'\"\n```\n\nThe above code will train on the LM1B language modeling benchmark, as specified\nin `examples/transformer_standalone_defaults.gin`. To train a\nsequence-to-sequence model on WMT14 en-de, change `utils.run.dataset` to\n`wmt_translate_ende/ende_subwords8k_t2t` and set `utils.run.mode` to `True`.\nNote that the `wmt_translate_ende/ende_subwords8k_t2t` dataset was removed from\nTensorFlow Datasets in\n[commit 211cb6f](https://github.com/tensorflow/datasets/commit/211cb6f082c5cc3c482e37d70234142a8fda2db3),\nso in order to train a model using this dataset you need to install a version of\nTFDS before this commit. Then, you can decode the WMT en-de development set\nand evaluate it using [SacreBLEU](https://github.com/mjpost/sacreBLEU) like so:\n\n```\n# INFER\npip3 install sacrebleu\nmkdir ~/input ~/output\nDECODE_INPUT=/home/$USER/input/ende.dev\nDECODE_OUTPUT=/home/$USER/output/ende.dev.out\n~/.local/bin/sacrebleu -t wmt13 -l en-de --echo src > $DECODE_INPUT\npython examples/transformer_standalone.py \\\n  --tpu=$TPU --data_dir=$DATA_DIR --model_dir=$MODEL_DIR --gin_file=$MODEL \\\n  --gin_file=$LAYOUT \\\n  --gin_param=\"decode_from_file.input_filename='$DECODE_INPUT'\" \\\n  --gin_param=\"decode_from_file.output_filename='$DECODE_OUTPUT'\" \\\n  --gin_param=\"run.mode='infer'\"\n\n# Compute BLEU score for dev set\ncat $DECODE_OUTPUT | ~/.local/bin/sacrebleu -t wmt13 -l en-de -tok intl\n```\n\n\n## Run the Transfomer model with Tensor2Tensor config\n```sh\ngit clone https://github.com/tensorflow/tensor2tensor.git\ncd tensor2tensor/\npip install --user  .\n```\n\nBefore running the model, you need to prepare the training data and bucket for\nstoring checkpoints. Refer to the\n[Transformer tutorial](https://cloud.google.com/tpu/docs/tutorials/transformer)\nto learn how to generate the training data and create buckets.\n\n```sh\nCONF=mtf_transformer_paper_tr_0_mesh_8\nNAME=ende_$CONF\\_0828\nMODEL=mtf_transformer\nPROBLEM=translate_ende_wmt32k_packed\n\nDATA_DIR=gs://xxxx\nOUT_DIR=gs://xxxx\nTPU_NAME=ylc-mtf-donut\n\ntensor2tensor/bin/t2t-trainer \\\n  --model=$MODEL \\\n  --hparams_set=$CONF \\\n  --problem=$PROBLEM \\\n  --train_steps=10000 \\\n  --eval_steps=200 \\\n  --data_dir=$DATA_DIR \\\n  --output_dir=$OUT_DIR \\\n  --use_tpu=True \\\n  --cloud_tpu_name=$TPU_NAME\n```\n\n\n## Run the toy model without Tensor2Tensor dependencies\n\n  This toy model contains two fully-connected layers which aim to train a\n  identity function: f(x) = x. Since there are 8 TPU cores, we can arbitrary\n  change the FLAGS.mesh_shape and FLAGS.layout to achieve different\n  data-parallelism and model-parallelism strategies.\n\n```sh\nMODEL_DIR=gs://xxxx\nTPU_NAME=ylc-mtf-donut\n\n# 2 ways data-parallelism and 4 ways model-parallelism.\n# In this configuration, we split the batch dimension into 2 cores and the\n# hidden dimension into 4 cores.\npython examples/toy_model_tpu.py \\\n  --tpu=$TPU \\\n  --model_dir=$MODEL_DIR \\\n  --io_size=8 \\\n  --hidden_size=8 \\\n  --mesh_shape='x:2;y:4' \\\n  --layout='batch:x;hidden:y'\n\n# 8 ways model-parallelism.\n# In this configuration, We split the hidden dimension into 8 cores.\npython examples/toy_model_tpu.py \\\n  --tpu=$TPU \\\n  --model_dir=$MODEL_DIR \\\n  --io_size=8 \\\n  --hidden_size=8 \\\n  --mesh_shape='all:8' \\\n  --layout='hidden:all'\n```\n\n## References\n\n> N. Shazeer, Y. Cheng, N. Parmar, D. Tran, A. Vaswani, P. Koanantakool,\n> P. Hawkins, H. Lee, M. Hong, C. Young, R. Sepassi, and B. Hechtman.\n> [Mesh-TensorFlow: Deep learning for supercomputers.](https://arxiv.org/abs/1811.02084)\n> In _Neural Information Processing Systems_, 2018.\n\n```none\n@inproceedings{shazeer2018mesh,\n  author = {Noam Shazeer and Youlong Cheng and Niki Parmar and Dustin Tran and Ashish Vaswani and Penporn Koanantakool and Peter Hawkins and HyoukJoong Lee and Mingsheng Hong and Cliff Young and Ryan Sepassi and Blake Hechtman},\n  title = {{Mesh-TensorFlow}: Deep Learning for Supercomputers},\n  booktitle = {Neural Information Processing Systems},\n  year = {2018},\n}\n```\n", "release_dates": ["2018-12-11T00:09:43Z", "2018-11-13T04:37:41Z", "2018-10-30T00:58:19Z", "2018-10-30T00:57:34Z"]}, {"name": "metadata", "description": "Utilities for passing TensorFlow-related metadata between tools", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Metadata\n\n[![Python](https://img.shields.io/badge/python%7C3.9%7C3.10%7C3.11-blue)](https://github.com/tensorflow/metadata)\n[![PyPI](https://badge.fury.io/py/tensorflow-metadata.svg)](https://badge.fury.io/py/tensorflow-metadata)\n\nTensorFlow Metadata provides standard representations for metadata that are\nuseful when training machine learning models with TensorFlow.\n\nThe metadata serialization formats include:\n\n* A schema describing tabular data (e.g., tf.Examples).\n* A collection of summary statistics over such datasets.\n* A problem statement quantifying the objectives of a model.\n\nThe metadata may be produced by hand or automatically during input data\nanalysis, and may be consumed for data validation, exploration, and\ntransformation.\n", "release_dates": ["2023-08-09T21:39:16Z", "2023-04-11T23:23:23Z", "2023-04-03T21:53:20Z", "2022-12-05T21:01:03Z", "2022-11-10T23:00:05Z", "2022-08-22T18:19:29Z", "2022-06-21T18:20:19Z", "2022-05-09T22:21:44Z", "2022-02-28T19:22:56Z", "2022-01-19T18:03:26Z", "2021-11-29T19:14:48Z", "2021-10-26T19:52:58Z", "2021-07-27T18:49:08Z", "2021-06-21T19:00:45Z", "2021-05-20T22:41:56Z", "2021-04-19T22:27:18Z", "2021-03-22T20:33:15Z", "2021-02-22T20:59:56Z", "2021-01-25T19:01:54Z", "2020-12-10T18:47:15Z", "2020-11-02T19:00:17Z", "2020-09-09T16:42:11Z", "2020-08-10T17:41:27Z", "2020-06-05T18:24:18Z", "2020-05-27T18:57:59Z", "2020-05-11T19:52:23Z", "2020-04-14T18:41:44Z", "2020-01-27T23:56:47Z", "2020-01-07T18:23:09Z", "2019-12-19T19:10:29Z"]}, {"name": "minigo", "description": "An open-source implementation of the AlphaGoZero algorithm", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "Minigo: A minimalist Go engine modeled after AlphaGo Zero, built on MuGo\n==================================================\n\nThis is an implementation of a neural-network based Go AI, using TensorFlow.\nWhile inspired by DeepMind's AlphaGo algorithm, this project is not\na DeepMind project nor is it affiliated with the official AlphaGo project.\n\n### This is NOT an official version of AlphaGo ###\n\nRepeat, *this is not the official AlphaGo program by DeepMind*.  This is an\nindependent effort by Go enthusiasts to replicate the results of the AlphaGo\nZero paper (\"Mastering the Game of Go without Human Knowledge,\" *Nature*), with\nsome resources generously made available by Google.\n\nMinigo is based off of Brian Lee's \"[MuGo](https://github.com/brilee/MuGo)\"\n-- a pure Python implementation of the first AlphaGo paper\n[\"Mastering the Game of Go with Deep Neural Networks and\nTree Search\"](https://www.nature.com/articles/nature16961) published in\n*Nature*. This implementation adds features and architecture changes present in\nthe more recent AlphaGo Zero paper, [\"Mastering the Game of Go without Human\nKnowledge\"](https://www.nature.com/articles/nature24270). More recently, this\narchitecture was extended for Chess and Shogi in [\"Mastering Chess and Shogi by\nSelf-Play with a General Reinforcement Learning\nAlgorithm\"](https://arxiv.org/abs/1712.01815).  These papers will often be\nabridged in Minigo documentation as *AG* (for AlphaGo), *AGZ* (for AlphaGo\nZero), and *AZ* (for AlphaZero) respectively.\n\n\nGoals of the Project\n==================================================\n\n1. Provide a clear set of learning examples using Tensorflow, Kubernetes, and\n   Google Cloud Platform for establishing Reinforcement Learning pipelines on\n   various hardware accelerators.\n\n2. Reproduce the methods of the original DeepMind AlphaGo papers as faithfully\n   as possible, through an open-source implementation and open-source pipeline\n   tools.\n\n3. Provide our data, results, and discoveries in the open to benefit the Go,\n   machine learning, and Kubernetes communities.\n\nAn explicit non-goal of the project is to produce a competitive Go program that\nestablishes itself as the top Go AI. Instead, we strive for a readable,\nunderstandable implementation that can benefit the community, even if that\nmeans our implementation is not as fast or efficient as possible.\n\nWhile this product might produce such a strong model, we hope to focus on the\nprocess.  Remember, getting there is half the fun. :)\n\nWe hope this project is an accessible way for interested developers to have\naccess to a strong Go model with an easy-to-understand platform of python code\navailable for extension, adaptation, etc.\n\nIf you'd like to read about our experiences training models, see [RESULTS.md](RESULTS.md).\n\nTo see our guidelines for contributing, see [CONTRIBUTING.md](CONTRIBUTING.md).\n\nGetting Started\n===============\n\nThis project assumes you have the following:\n\n- virtualenv / virtualenvwrapper\n- Python 3.5+\n- [Docker](https://docs.docker.com/install/)\n- [Cloud SDK](https://cloud.google.com/sdk/downloads)\n\nThe [Hitchhiker's guide to\npython](http://docs.python-guide.org/en/latest/dev/virtualenvs/) has a good\nintro to python development and virtualenv usage. The instructions after this\npoint haven't been tested in environments that are not using virtualenv.\n\n```shell\npip3 install virtualenv\npip3 install virtualenvwrapper\n```\n\nInstall Bazel\n------------------\n\n```shell\nBAZEL_VERSION=0.24.1\nwget https://github.com/bazelbuild/bazel/releases/download/${BAZEL_VERSION}/bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh\nchmod 755 bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh\nsudo ./bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh\n```\n\nInstall TensorFlow\n------------------\nFirst set up and enter your virtualenv and then the shared requirements:\n\n```\npip3 install -r requirements.txt\n```\n\nThen, you'll need to choose to install the GPU or CPU tensorflow requirements:\n\n- GPU: `pip3 install \"tensorflow-gpu==1.15.0\"`.\n  - *Note*: You must install [CUDA 10.0](https://developer.nvidia.com/cuda-10.0-download-archive). for Tensorflow\n    1.13.0+.\n- CPU: `pip3 install \"tensorflow==1.15.0\"`.\n\nSetting up the Environment\n--------------------------\n\nYou may want to use a cloud project for resources. If so set:\n\n```shell\nPROJECT=foo-project\n```\n\nThen, running\n\n```shell\nsource cluster/common.sh\n```\n\nwill set up other environment variables defaults.\n\nRunning unit tests\n------------------\n```\n./test.sh\n```\n\nTo run individual modules\n\n```\nBOARD_SIZE=9 python3 tests/run_tests.py test_go\nBOARD_SIZE=19 python3 tests/run_tests.py test_mcts\n```\n\nAutomated Tests\n----------------\n\n[Test Dashboard](https://k8s-testgrid.appspot.com/sig-big-data#tf-minigo-presubmit)\n\nTo automatically test PRs, Minigo uses\n[Prow](https://github.com/kubernetes/test-infra/tree/master/prow), which is a\ntest framework created by the Kubernetes team for testing changes in a hermetic\nenvironment. We use prow for running unit tests, linting our code, and\nlaunching our test Minigo Kubernetes clusters.\n\nYou can see the status of our automated tests by looking at the Prow and\nTestgrid UIs:\n\n- Testgrid (Test Results Dashboard): https://k8s-testgrid.appspot.com/sig-big-data\n- Prow (Test-runner dashboard): https://prow.k8s.io/?repo=tensorflow%2Fminigo\n\nBasics\n======\n\nAll commands are compatible with either Google Cloud Storage as a remote file\nsystem, or your local file system. The examples here use GCS, but local file\npaths will work just as well.\n\nTo use GCS, set the `BUCKET_NAME` variable and authenticate via `gcloud login`.\nOtherwise, all commands fetching files from GCS will hang.\n\nFor instance, this would set a bucket, authenticate, and then look for the most\nrecent model.\n\n```shell\n# When you first start we recommend using our minigo-pub bucket.\n# Later you can setup your own bucket and store data there.\nexport BUCKET_NAME=minigo-pub/v9-19x19\ngcloud auth application-default login\ngsutil ls gs://$BUCKET_NAME/models | tail -4\n```\n\nWhich might look like:\n\n```\ngs://$BUCKET_NAME/models/000737-fury.data-00000-of-00001\ngs://$BUCKET_NAME/models/000737-fury.index\ngs://$BUCKET_NAME/models/000737-fury.meta\ngs://$BUCKET_NAME/models/000737-fury.pb\n```\n\nThese four files comprise the model. Commands that take a model as an\nargument usually need the path to the model basename, e.g.\n`gs://$BUCKET_NAME/models/000737-fury`\n\nYou'll need to copy them to your local disk.  This fragment copies the files\nassociated with `$MODEL_NAME` to the directory specified by `MINIGO_MODELS`:\n\n```shell\nMODEL_NAME=000737-fury\nMINIGO_MODELS=$HOME/minigo-models\nmkdir -p $MINIGO_MODELS/models\ngsutil ls gs://$BUCKET_NAME/models/$MODEL_NAME.* | \\\n       gsutil cp -I $MINIGO_MODELS/models\n```\n\nSelfplay\n--------\nTo watch Minigo play a game, you need to specify a model. Here's an example\nto play using the latest model in your bucket\n\n```shell\npython3 selfplay.py \\\n  --verbose=2 \\\n  --num_readouts=400 \\\n  --load_file=$MINIGO_MODELS/models/$MODEL_NAME\n```\n\nwhere `READOUTS` is how many searches to make per move.  Timing information and\nstatistics will be printed at each move.  Setting verbosity to 3 or\nhigher will print a board at each move.\n\nPlaying Against Minigo\n----------------------\n\nMinigo uses the\n[GTP Protocol](http://www.lysator.liu.se/~gunnar/gtp/gtp2-spec-draft2/gtp2-spec.html),\nand you can use any gtp-compliant program with it.\n\n```shell\n# Latest model should look like: /path/to/models/000123-something\nLATEST_MODEL=$(ls -d $MINIGO_MODELS/* | tail -1 | cut -f 1 -d '.')\npython3 gtp.py --load_file=$LATEST_MODEL --num_readouts=$READOUTS --verbose=3\n```\n\nAfter some loading messages, it will display `GTP engine ready`, at which point\nit can receive commands.  GTP cheatsheet:\n\n```\ngenmove [color]             # Asks the engine to generate a move for a side\nplay [color] [coordinate]   # Tells the engine that a move should be played for `color` at `coordinate`\nshowboard                   # Asks the engine to print the board.\n```\n\nOne way to play via GTP is to use gogui-display (which implements a UI that\nspeaks GTP.) You can download the gogui set of tools at\n[http://gogui.sourceforge.net/](http://gogui.sourceforge.net/). See also\n[documentation on interesting ways to use\nGTP](http://gogui.sourceforge.net/doc/reference-twogtp.html).\n\n```shell\ngogui-twogtp -black 'python3 gtp.py --load_file=$LATEST_MODEL' -white 'gogui-display' -size 19 -komi 7.5 -verbose -auto\n```\n\nAnother way to play via GTP is to watch it play against GnuGo, while\nspectating the games:\n\n```shell\nBLACK=\"gnugo --mode gtp\"\nWHITE=\"python3 gtp.py --load_file=$LATEST_MODEL\"\nTWOGTP=\"gogui-twogtp -black \\\"$BLACK\\\" -white \\\"$WHITE\\\" -games 10 \\\n  -size 19 -alternate -sgffile gnugo\"\ngogui -size 19 -program \"$TWOGTP\" -computer-both -auto\n```\n\nTraining Minigo\n======================\n\nOverview\n--------\n\nThe following sequence of commands will allow you to do one iteration of\nreinforcement learning on 9x9. These are the basic commands used to produce the\nmodels and games referenced above.\n\nThe commands are\n\n - bootstrap: initializes a random model\n - selfplay: plays games with the latest model, producing data used for training\n - train: trains a new model with the selfplay results from the most recent N\n   generations.\n\nTraining works via tf.Estimator; a working directory manages checkpoints and\ntraining logs, and the latest checkpoint is periodically exported to GCS, where\nit gets picked up by selfplay workers.\n\nConfiguration for things like \"where do debug SGFs get written\", \"where does\ntraining data get written\", \"where do the latest models get published\" are\nmanaged by the helper scripts in the rl\\_loop directory. Those helper scripts\nexecute the same commands as demonstrated below. Configuration for things like\n\"what size network is being used?\" or \"how many readouts during selfplay\" can\nbe passed in as flags. The mask\\_flags.py utility helps ensure all parts of the\npipeline are using the same network configuration.\n\nAll local paths in the examples can be replaced with `gs://` GCS paths, and the\nKubernetes-orchestrated version of the reinforcement learning loop uses GCS.\n\nBootstrap\n---------\n\nThis command initializes your working directory for the trainer and a random\nmodel. This random model is also exported to `--model-save-path` so that\nselfplay can immediately start playing with this random model.\n\nIf these directories don't exist, bootstrap will create them for you.\n\n```shell\nexport MODEL_NAME=000000-bootstrap\npython3 bootstrap.py \\\n  --work_dir=estimator_working_dir \\\n  --export_path=outputs/models/$MODEL_NAME\n```\n\nSelf-play\n---------\n\nThis command starts self-playing, outputting its raw game data as tf.Examples\nas well as in SGF form in the directories.\n\n\n```shell\npython3 selfplay.py \\\n  --load_file=outputs/models/$MODEL_NAME \\\n  --num_readouts 10 \\\n  --verbose 3 \\\n  --selfplay_dir=outputs/data/selfplay \\\n  --holdout_dir=outputs/data/holdout \\\n  --sgf_dir=outputs/sgf\n```\n\nTraining\n--------\n\nThis command takes a directory of tf.Example files from selfplay and trains a\nnew model, starting from the latest model weights in the `estimator_working_dir`\nparameter.\n\nRun the training job:\n\n```shell\npython3 train.py \\\n  outputs/data/selfplay/* \\\n  --work_dir=estimator_working_dir \\\n  --export_path=outputs/models/000001-first_generation\n```\n\nAt the end of training, the latest checkpoint will be exported to.\nAdditionally, you can follow along with the training progress with TensorBoard.\nIf you point TensorBoard at the estimator working directory, it will find the\ntraining log files and display them.\n\n```shell\ntensorboard --logdir=estimator_working_dir\n```\n\nValidation\n----------\n\nIt can be useful to set aside some games to use as a 'validation set' for\ntracking the model overfitting.  One way to do this is with the `validate`\ncommand.\n\n### Validating on holdout data\n\nBy default, Minigo will hold out 5% of selfplay games for validation. This can\nbe changed by adjusting the `holdout_pct` flag on the `selfplay` command.\n\nWith this setup, `rl_loop/train_and_validate.py` will validate on the same\nwindow of games that were used to train, writing TensorBoard logs to the\nestimator working directory.\n\n### Validating on a different set of data\n\nThis might be useful if you have some known set of 'good data' to test your\nnetwork against, e.g., a set of pro games.\nAssuming you've got a set of .sgfs with the proper komi & boardsizes, you'll\nwant to preprocess them into the .tfrecord files, by running something similar\nto\n\n```python\nimport preprocessing\nfilenames = [generate a list of filenames here]\nfor f in filenames:\n    try:\n        preprocessing.make_dataset_from_sgf(f, f.replace(\".sgf\", \".tfrecord.zz\"))\n    except:\n        print(f)\n```\n\nOnce you've collected all the files in a directory, producing validation is as\neasy as\n\n```shell\npython3 validate.py \\\n  validation_files/ \\\n  --work_dir=estimator_working_dir \\\n  --validation_name=pro_dataset\n```\n\nThe validate.py will glob all the .tfrecord.zz files under the\ndirectories given as positional arguments and compute the validation error\nfor the positions from those files.\n\n\nRetraining a model\n======================\n\nThe training data for most of Minigo's models up to v13 is publicly available in\nthe `minigo-pub` Cloud storage bucket, e.g.:\n\n```shell\ngsutil ls gs://minigo-pub/v13-19x19/data/golden_chunks/\n```\n\nFor models v14 and onwards, we started using Cloud BigTable and are still\nworking on making that data public.\n\nHere's how to retrain your own model from this source data using a Cloud TPU:\n\n```shell\n# I wrote these notes using our existing TPU-enabled project, so they're missing\n# a few preliminary steps, like setting up a Cloud account, creating a project,\n# etc. New users will also need to enable Cloud TPU on their project using the\n# TPUs panel.\n\n###############################################################################\n\n# Note that you will be billed for any storage you use and also while you have\n# VMs running. Remember to shut down your VMs when you're not using them!\n\n# To use a Cloud TPU on GCE, you need to create a special TPU-enabled VM using\n# the `ctpu` tool. First, set up some environment variables:\n#   GCE_PROJECT=<your project name>\n#   GCE_VM_NAME=<your VM's name>\n#   GCE_ZONE<the zone in which you want to bring uo your VM, e.g. us-central1-f>\n\n# In this example, we will use the following values:\nGCE_PROJECT=example-project\nGCE_VM_NAME=minigo-etpu-test\nGCE_ZONE=us-central1-f\n\n# Create the Cloud TPU enabled VM.\nctpu up \\\n  --project=\"${GCE_PROJECT}\" \\\n  --zone=\"${GCE_ZONE}\" \\\n  --name=\"${GCE_VM_NAME}\" \\\n  --tf-version=1.13\n\n# This will take a few minutes and you should see output similar to the\n# following:\n#   ctpu will use the following configuration values:\n#         Name:                 minigo-etpu-test\n#         Zone:                 us-central1-f\n#         GCP Project:          example-project\n#         TensorFlow Version:   1.13\n#  OK to create your Cloud TPU resources with the above configuration? [Yn]: y\n#  2019/04/09 10:50:04 Creating GCE VM minigo-etpu-test (this may take a minute)...\n#  2019/04/09 10:50:04 Creating TPU minigo-etpu-test (this may take a few minutes)...\n#  2019/04/09 10:50:11 GCE operation still running...\n#  2019/04/09 10:50:12 TPU operation still running...\n\n# Once the Cloud TPU is created, `ctpu` will have SSHed you into the machine.\n\n# Remember to set the same environment variables on your VM.\nGCE_PROJECT=example-project\nGCE_VM_NAME=minigo-etpu-test\nGCE_ZONE=us-central1-f\n\n# Clone the Minigo Github repository:\ngit clone --depth 1 https://github.com/tensorflow/minigo\ncd minigo\n\n# Install virtualenv.\npip3 install virtualenv virtualenvwrapper\n\n# Create a virtual environment\nvirtualenv -p /usr/bin/python3 --system-site-packages \"${HOME}/.venvs/minigo\"\n\n# Activate the virtual environment.\nsource \"${HOME}/.venvs/minigo/bin/activate\"\n\n# Install Minigo dependencies (TensorFlow for Cloud TPU is already installed as\n# part of the VM image).\npip install -r requirements.txt\n\n# When training on a Cloud TPU, the training work directory must be on Google Cloud Storage.\n# You'll need to choose your own globally unique bucket name.\n# The bucket location should be close to your VM.\nGCS_BUCKET_NAME=minigo_test_bucket\nGCE_BUCKET_LOCATION=us-central1\ngsutil mb -p \"${GCE_PROJECT}\" -l \"${GCE_BUCKET_LOCATION}\" \"gs://${GCS_BUCKET_NAME}\"\n\n# Run the training script and note the location of the training work_dir\n# it reports, e.g.\n#    Writing to gs://minigo_test_bucket/train/2019-04-25-18\n./oneoffs/train.sh \"${GCS_BUCKET_NAME}\"\n\n# Launch tensorboard, pointing it at the work_dir reported by the train.sh script.\ntensorboard --logdir=gs://minigo_test_bucket/train/2019-04-25-18\n\n# After a few minutes, TensorBoard should start updating.\n# Interesting graphs to look at are value_cost_normalized, policy_cost and policy_entropy.\n```\n\nRunning Minigo on a Kubernetes Cluster\n==============================\n\nSee more at [cluster/README.md](https://github.com/tensorflow/minigo/tree/master/cluster/README.md)\n", "release_dates": ["2018-01-30T01:18:32Z"]}, {"name": "mlir", "description": "\"Multi-Level Intermediate Representation\" Compiler Infrastructure", "language": null, "license": null, "readme": "# 301 - Moved\n\nMLIR is now part of LLVM, more information on https://mlir.llvm.org\n\nThe code from this repository can now be found at\nhttps://github.com/llvm/llvm-project/tree/main/mlir/\n\n# Migration\n\nIf you have a local fork of this repository or pull-requests that need to be\nmigrated to the LLVM monorepo, the following recipe may help you:\n\n```\n# From your local MLIR clone:\n$ git clone git@github.com:newren/git-filter-repo.git /tmp/git-filter-repo\n$ /tmp/git-filter-repo/git-filter-repo --path-rename :mlir/ --force  --message-callback 'return re.sub(b\"(#[0-9]+)\", b\"tensorflow/mlir\\\\1\", message)' --refs <branch name>\n```\n\nAfter this, all the commits from the previous upstream MLIR should match the\nones in the monorepo now. If you don't provide the `--refs` option, this\nwill rewrite *all the branches* in your repo.\n\nFrom there you should be able to rebase any of your branch/commits on top of\nthe LLVM monorepo:\n\n```\n$ git remote set-url origin git@github.com:llvm/llvm-project.git\n$ git fetch origin\n$ git rebase origin/main -i\n```\n\nCherry-picking commits should also work, if you checkout the main branch from\nthe monorepo you can `git cherry-pick <sha1>` from your (rewritten) branches.\n\nYou can also export patches with `git format-patch <range>` and re-apply it on\nthe monorepo using `git am <patch file>`.\n", "release_dates": []}, {"name": "mlir-hlo", "description": null, "language": "MLIR", "license": null, "readme": "# MLIR-HLO: A Standalone \"HLO\" MLIR-based Compiler\n\nThe code here exists in two places:\n\n*   https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/xla/mlir_hlo;\n    this is the canonical location and where contributions should be made using\n    GitHub pull-requests.\n*   https://github.com/tensorflow/mlir-hlo; this is a standalone repository with\n    a view to the same code to allow other projects to use this without\n    depending on the entire TF monorepo.\n\nThis implements a self-contained compiler for a linear algebra set of operations\ninspired by XLA\n[HLO IR](https://www.tensorflow.org/xla/architecture#how_does_xla_work) using\nMLIR components. It is designed to provide an end-to-end flow independent of\nTensorFlow and XLA, but usable inside of these projects.\n\nCoding practice and conventions in this repository follow the\n[MLIR Developer Guide](https://mlir.llvm.org/getting_started/DeveloperGuide/) in\nthis repo as part of the intent to act as an incubator for technology to\nupstream.\n\n## QuickStart: building and testing\n\nThese instructions work on Linux, you may have to adjust for your platform.\n\nTo build the code in this repository, you need a clone of the LLVM/MLIR git\nrepository:\n\n    $ git clone https://github.com/llvm/llvm-project.git\n\n\nYou need to make sure you have the right commit checked out in the LLVM\nrepository (you need to do this every time you pull from this repo):\n\n    $ (cd llvm-project && git checkout $(cat ../build_tools/llvm_version.txt))\n\nWe provide a script to configure and build LLVM/MLIR:\n\n    $ build_tools/build_mlir.sh ${PWD}/llvm-project/ ${PWD}/llvm-build\n\nAgain this is something to do every time you pull from this repository and the\nLLVM revision changes.\n\nFinally you can build and test this repository:\n\n    $ mkdir build && cd build\n    $ cmake .. -GNinja \\\n       -DLLVM_ENABLE_LLD=ON \\\n       -DCMAKE_BUILD_TYPE=Release \\\n       -DLLVM_ENABLE_ASSERTIONS=On \\\n       -DMLIR_DIR=${PWD}/../llvm-build/lib/cmake/mlir\n    $ ninja check-mlir-hlo\n\n\n## Overview\n\nMLIR-HLO aims to provide an end-to-end compiler for CPU and GPU, as well as\nbuilding reusable blocks for other accelerators. This is heavily inspired by the\nsuccess of XLA.\n\n[XLA](https://www.tensorflow.org/xla/) (Accelerated Linear Algebra) is a\ndomain-specific compiler framework and execution environment for linear algebra,\nwhich powers code-generation for ML frameworks like TensorFlow, JAX, and others.\n\nA cornerstone of XLA is the HLO (High Level Optimizer) IR, which offers a\ncarefully fixed selected list of operations, mostly orthogonal to each other. It\nprovides an efficient optimizer for computations expressed with this set of\noperations and generate codes for hardware platforms like CPU, GPU, and TPUs.\nIts goal is to provide a uniform interface to compile and execute these\noptimized HLO programs independently of the targeted device. It is not a\nfront-end ML system like TensorFlow or JAX, rather it is a backend framework\nthat optimizes HLO and lowers to machine code.\n\nThe HLO set of operations is closed and has well defined semantics. HLO\noperations operate on immutable Tensors with static shapes (actually bounded\nshapes to be exact) and explicit broadcasts.\n\n[MLIR](https://mlir.llvm.org/) is a compiler infrastructure which intends to\ncome with \"battery included\", as such it intends to provide all the blocks\nrequired to assemble graph optimization and codegen pipelines. The longer term\nroadmap for MLIR is to provide a\n[Tensor Compute Primitive](https://llvm.discourse.group/c/mlir/MLIR-TCP-WG/36)\n(TCP) dialect, which should hopefully be general enough to model what HLO\nrepresents today (see\n[slides](https://drive.google.com/open?id=1iljcpTQ5NPaMfGpoPDFml1XkYxjK_6A4) and\n[recording](https://drive.google.com/open?id=1jSPa8TwPKUt0WuLquGc8OgSUVYJHMvWZ)\nfor a technical discussion on this topic).\n\nThe work on MLIR-HLO can be seen as a stepping stone towards building TCP, while\nintegrating intermediate components into XLA itself by relying on the\nwell-proven HLO IR and introducing more pieces from upstream MLIR\n([Linalg](https://mlir.llvm.org/docs/Dialects/Linalg/),\n[Vector](https://mlir.llvm.org/docs/Dialects/Vector/),\n[GPU](https://mlir.llvm.org/docs/Dialects/GPU/) dialect, ...).\n[This document](https://www.tensorflow.org/mlir/xla_gpu_codegen?hl=zh-cn)\nprovides more information on the current migration of the XLA GPU codegen.\n\n## MLIR Dialects for XLA-style compilation\n\nThis repository defines three dialects to support a HLO-like compilation\npipeline using MLIR:\n\n*   `chlo`: the \"client\" HLO dialect, intended to be closer to the frontend\n    (including implicit broadcast semantics).\n*   `mhlo`: \"meta\"-HLO dialect ; similar to `xla_hlo`, but with extensions for\n    dynamic shape support.\n*   `lmhlo`: \"late\"-\"meta\"-HLO, it is the IR after buffer allocation is\n    performed. In XLA the buffer allocation is a side-data structure which keeps\n    track of these informations, while this separate dialect materializes it in\n    the IR.\n\nWe describe these in more details below.\n\n### HLO Client Dialect: `chlo`.\n\n*   It was originally designed to map the\n    [XLA client APIs](https://www.tensorflow.org/xla/operation_semantics) (e.g.,\n    ops supports implicit broadcast and roughly modeled on XlaBuilder API)\n    modulo support for dynamic shapes and additional ops required to support\n    dynamic client side HLOs.\n*   Ops can be from either the XlaBuilder or XLA helper functions can be\n    converted into ops (e.g., given ambiguity in what constitutes these ops,\n    there is some freedom to decide), the goal of this dialect is to correspond\n    close to client level and enable a thin layer between client use and op\n    construction (making it cheap to construct and optimizations on the dialect\n    close to optimizations on the client ops).\n\nEntry:\n\n*   The vast majority of old \"client\" interactions are via the XlaBuilder APIs.\n    These APIs are used by TF2XLA kernels, JAX, PyTorch bridge and directly. The\n    legalization path (described below) can also reuse the XlaBuilder's APIs to\n    construct XLA Client HLO ops directly (this uses MlirXlaBuilder which is a\n    subclass of XlaBuilder).\n*   The other entry point is during legalization from TensorFlow ops in the TF\n    Graph Compiler and other tools (e.g., SavedModel lowering and TFCompile).\n\nExit:\n\n*   MHLO\n*   May be exported to xla::HloInstructionProto by invoking the XlaBuilder APIs\n    (with regular XlaBuilder)\n\nThe `chlo` dialect started originally as mapping to the XLA client Builder APIs.\nIt enables it to both be constructed and converted back to existing XLA\ninterfaces using the XlaBuilder API. Due to the way that translation into and\nout of the dialect works, there is no expectation that this dialect roundtrips\nto XLA (e.g., it is only intended to be translated to MLIR and then legalized to\nanother dialect or translated to HloInstructionProto).\n\nThe export approach of reusing the XlaBuilders enables reusing a lot of logic\nthat was already implemented in terms of computing shapes, inserting broadcasts\netc.\n\nAn important topic here is that XLA Client HLO ops are not a well defined set.\nAnd in particular what some would consider helper functions, others would\nconsider ops. It should be easy to move between these and so define a new op\nalong with the helper function or autogenerate the helper functions from the\ndescriptions of the ops. For the former, a simple approach would be to simply\nconsider the context in which the op is being constructed and if an MLIR one,\nconstruct a op in the client dialect instead of further calls into XlaBuilder.\nThe latter could be implemented by adding the op and a legalization of the op to\nother known ops, from which a helper function can get generated that could be\nused as regular.\n\nStatus: Exists but need to be cleaned up.\n\n### Meta HLO Dialect `mhlo`\n\n*   Dialect is closer to current HLO server ops (e.g., no implicit broadcast)\n*   MHLO dialect where we can deviate from the requirements of the client or\n    server dialect, in particular:\n    *   Control flow ops with implicit capture to enable simpler optimizations\n        (e.g., generic LICM, unroll & jam, etc.)\n    *   Multiple results ops (e.g., no tuples)\n    *   More ops (for example, unique op or assert op), and ops that don't need\n        to be added to either client or server dialect.\n    *   Op set not constrained by implementation (e.g., hlo.add operating on say\n        i79 or !mydialect.weird_type is allowed even though no XLA backend\n        supports it). Verification on types happening at the boundaries.\n    *   It does not need to preserve some deprecated XLA constructs (e.g.\n        stateful RNG HLO).\n    *   More dynamic shape support ops without need for updating all\n        users/backends.\n*   This dialect enables evolving HLO independently from XLA in order to\n    experiment with features we'd like to upstream in MLIR TCP. In particular it\n    intends to be user-extensible through\n    [interfaces](https://mlir.llvm.org/docs/Interfaces/).\n*   It should have no TensorFlow, or proto, or other Google internal\n    dependencies.\n*   It need not be a complete superset of ops compared to XLA HLO dialect.\n\nEntry:\n\n*   Legalization from `chlo` dialect or conversion from XLA HLO.\n*   Directly emitted from TF Graph Compiler;\n*   Builder call (e.g., EDSL);\n\nExit:\n\n*   LMHLO, Linalg IREE, directly used in codegen.\n*   XLA HLO.\n\nThe MHLO dialect has no direct export format, it is only meant as an\nintermediate optimization dialect/format. It is also where we can experiment\ncheaply with new ops. This format will be where the representation would differ\nfrom existing endpoints.\n\nStatus: Exists but need to be cleaned up and evolved, in particular with respect\nto supporting dynamic shapes.\n\nMHLO differs from XLA HLO op set in multiple ways, including:\n1. MHLO While accepts multiple operands and may produce multiple results\n   instead;\n\n### LMHLO\n\nLMHLO corresponds to late `mhlo` and operates on buffer domain (e.g., memref)\nwith side-effecting operations. The lowering from `mhlo` dialect proceeds by way\nof scheduling, memory and buffer allocation. The current mapping is directly on\nXLA Client HLOs but without implicit broadcast and with operation on memrefs.\nThis dialect will instead be rebased on `mhlo` dialect but operating on buffers\nstill.\n\nEntry:\n\n*   Post buffer assignment on `mhlo` dialect, or from XLA after buffer\n    assignment.\n\nExit:\n\n*   Codegen (LLVM IR in the common cases at the moment)\n\n## End-to-End pipeline\n\nTODO\n\n## Alternative build setups\n\n### Building Python API\n\nBuilding the MHLO Python API requires building as an LLVM external project.\nThe below instructions presume that you have this `mlir-hlo` repo and an\n`llvm-project` repo checked out side by side.\n\nNote that the python package produced by this procedure includes the `mlir`\npackage and is not suitable for deployment as-is (but it can be included into\na larger aggregate).\n\n```\nmkdir build && cd build\ncmake -GNinja -B. ${LLVM_SRC_DIR}/llvm \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DLLVM_ENABLE_PROJECTS=mlir \\\n    -DLLVM_EXTERNAL_PROJECTS=mlir_hlo \\\n    -DLLVM_EXTERNAL_MLIR_HLO_SOURCE_DIR=${MLIR_HLO_SRC_DIR} \\\n    -DLLVM_TARGETS_TO_BUILD=host \\\n    -DPython3_EXECUTABLE=$(which python) \\\n    -DMLIR_ENABLE_BINDINGS_PYTHON=ON \\\n    -DMHLO_ENABLE_BINDINGS_PYTHON=ON\n\nninja MLIRHLOPythonModules\nexport PYTHONPATH=$PWD/tools/mlir_hlo/python_packages/mlir_hlo\npython -c \"import mlir.dialects.mhlo\"\n```\n\n## External projects that depend on mlir-hlo\n\nExternal projects that need to depend on `mlir-hlo` (for example via a git\nsubmodule) can use the following setting in their cmake configuration in order\nfor `find_package(MHLO)` to import all mlir-hlo cmake targets into their build\nsetup and have access to the required include and lib variables (see generated\n`MHLOConfig.cmake`).\n\n```\n...\n   -DMHLO_DIR=<path to mlir-hlo build dir>/lib/cmake/mlir-hlo\n   ...\n```\n", "release_dates": []}, {"name": "model-analysis", "description": "Model analysis tools for TensorFlow", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<!-- See: www.tensorflow.org/tfx/model_analysis/ -->\n\n# TensorFlow Model Analysis\n\n[![Python](https://img.shields.io/badge/python%20-3.8%7C3.9-blue)](https://github.com/tensorflow/model-analysis)\n[![PyPI](https://badge.fury.io/py/tensorflow-model-analysis.svg)](https://badge.fury.io/py/tensorflow-model-analysis)\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma)\n\n*TensorFlow Model Analysis* (TFMA) is a library for evaluating TensorFlow\nmodels.  It allows users to evaluate their models on large amounts of data in a\ndistributed manner, using the same metrics defined in their trainer. These\nmetrics can be computed over different slices of data and visualized in Jupyter\nnotebooks.\n\n![TFMA Slicing Metrics Browser](https://raw.githubusercontent.com/tensorflow/model-analysis/master/g3doc/images/tfma-slicing-metrics-browser.gif)\n\nCaution: TFMA may introduce backwards incompatible changes before version 1.0.\n\n## Installation\n\nThe recommended way to install TFMA is using the\n[PyPI package](https://pypi.org/project/tensorflow-model-analysis/):\n\n<pre class=\"devsite-terminal devsite-click-to-copy\">\npip install tensorflow-model-analysis\n</pre>\n\npip install from https://pypi-nightly.tensorflow.org\n\n<pre class=\"devsite-terminal devsite-click-to-copy\">\npip install -i https://pypi-nightly.tensorflow.org/simple tensorflow-model-analysis\n</pre>\n\npip install from the HEAD of the git:\n\n<pre class=\"devsite-terminal devsite-click-to-copy\">\npip install git+https://github.com/tensorflow/model-analysis.git#egg=tensorflow_model_analysis\n</pre>\n\npip install from a released version directly from git:\n\n<pre class=\"devsite-terminal devsite-click-to-copy\">\npip install git+https://github.com/tensorflow/model-analysis.git@v0.21.3#egg=tensorflow_model_analysis\n</pre>\n\nIf you have cloned the repository locally, and want to test your local change,\npip install from a local folder.\n\n<pre class=\"devsite-terminal devsite-click-to-copy\">\npip install -e $FOLDER_OF_THE_LOCAL_LOCATION\n</pre>\n\nNote that protobuf must be installed correctly for the above option since it is\nbuilding TFMA from source and it requires protoc and all of its includes\nreference-able. Please see [protobuf install instruction](https://github.com/protocolbuffers/protobuf#protocol-compiler-installation)\nfor see the latest install instructions.\n\nCurrently, TFMA requires that TensorFlow is installed but does not have an\nexplicit dependency on the TensorFlow PyPI package. See the\n[TensorFlow install guides](https://www.tensorflow.org/install/) for\ninstructions.\n\n### Build TFMA from source\n\nTo build from source follow the following steps:\n\nInstall the protoc as per the link mentioned:\n[protoc](https://grpc.io/docs/protoc-installation/#install-pre-compiled-binaries-any-os)\n\nCreate a virtual environment by running the commands\n\n```\npython3 -m venv <virtualenv_name>\nsource <virtualenv_name>/bin/activate\npip3 install setuptools wheel\ngit clone https://github.com/tensorflow/model-analysis.git\ncd model-analysis\npython3 setup.py bdist_wheel\n```\nThis will build the TFMA wheel in the dist directory. To install the wheel from\ndist directory run the commands\n\n```\ncd dist\npip3 install tensorflow_model_analysis-<version>-py3-none-any.whl\n```\n\n### Jupyter Lab\n\nAs of writing, because of https://github.com/pypa/pip/issues/9187, `pip install`\nmight never finish. In that case, you should revert pip to version 19 instead of\n20: `pip install \"pip<20\"`.\n\nUsing a JupyterLab extension requires installing dependencies on the command\nline. You can do this within the console in the JupyterLab UI or on the command\nline. This includes separately installing any pip package dependencies and\nJupyterLab labextension plugin dependencies, and the version numbers must be\ncompatible.  JupyterLab labextension packages refer to npm packages\n(eg, [tensorflow_model_analysis](https://www.npmjs.com/package/tensorflow_model_analysis).\n\nThe examples below use 0.32.0. Check available [versions](#compatible-versions)\nbelow to use the latest.\n\n\n#### Jupyter Lab 3.0.x\n\n```Shell\npip install tensorflow_model_analysis==0.32.0\njupyter labextension install tensorflow_model_analysis@0.32.0\npip install jupyterlab_widgets==1.0.0\n```\n\n\n#### Jupyter Lab 2.2.x\n\n```Shell\npip install tensorflow_model_analysis==0.32.0\njupyter labextension install tensorflow_model_analysis@0.32.0\njupyter labextension install @jupyter-widgets/jupyterlab-manager@2\n```\n\n#### Jupyter Lab 1.2.x\n\n```Shell\npip install tensorflow_model_analysis==0.32.0\njupyter labextension install tensorflow_model_analysis@0.32.0\njupyter labextension install @jupyter-widgets/jupyterlab-manager@1.1\n```\n\n#### Classic Jupyter Notebook\n\nTo enable TFMA visualization in the classic Jupyter Notebook (either through\n`jupyter notebook` or\n[through the JupyterLab UI](https://jupyterlab.readthedocs.io/en/stable/getting_started/starting.html)),\nyou'll also need to run:\n\n```shell\njupyter nbextension enable --py widgetsnbextension\njupyter nbextension enable --py tensorflow_model_analysis\n```\n\nNote: If Jupyter notebook is already installed in your home directory, add\n`--user` to these commands. If Jupyter is installed as root, or using a virtual\nenvironment, the parameter `--sys-prefix` might be required.\n\n#### Building TFMA from source\n\nIf you want to build TFMA from source and use the UI in JupyterLab, you'll need\nto make sure that the source contains valid version numbers.  Check that the\nPython package version number and npm package version number are exactly the\nsame, and that both valid version numbers (eg, remove the `-dev` suffix).\n\n\n#### Troubleshooting\n\nCheck pip packages:\n\n```Shell\npip list\n```\n\nCheck JupyterLab extensions:\n\n```Shell\njupyter labextension list  # for JupyterLab\njupyter nbextension list  # for classic Jupyter Notebook\n```\n\n### Standalone HTML page with `embed_minimal_html`\n\nTFMA notebook extension can be built into a standalone HTML file that also\nbundles data into the HTML file.  See the Jupyter Widgets docs on\n[embed_minimal_html](https://ipywidgets.readthedocs.io/en/latest/embedding.html#python-interface).\n\n\n### Kubeflow Pipelines\n\n[Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/sdk/output-viewer/)\nincludes integrations that embed the TFMA notebook extension ([code](https://github.com/kubeflow/pipelines/blob/1.5.0-rc.2/backend/src/apiserver/visualization/types/tfma.py#L17)).\nThis integration relies on network access at runtime to load a variant of the\nJavaScript build published on unpkg.com (see [config](https://github.com/tensorflow/model-analysis/blob/v0.29.0/tensorflow_model_analysis/notebook/jupyter/js/webpack.config.js#L78)\nand [loader code](https://github.com/tensorflow/model-analysis/blob/v0.29.0/tensorflow_model_analysis/notebook/jupyter/js/lib/widget.js#L23)).\n\n\n### Notable Dependencies\n\nTensorFlow is required.\n\n[Apache Beam](https://beam.apache.org/) is required; it's the way that efficient\ndistributed computation is supported. By default, Apache Beam runs in local\nmode but can also run in distributed mode using\n[Google Cloud Dataflow](https://cloud.google.com/dataflow/) and other Apache\nBeam\n[runners](https://beam.apache.org/documentation/runners/capability-matrix/).\n\n[Apache Arrow](https://arrow.apache.org/) is also required. TFMA uses Arrow to\nrepresent data internally in order to make use of vectorized numpy functions.\n\n## Getting Started\n\nFor instructions on using TFMA, see the [get started\nguide](https://github.com/tensorflow/model-analysis/blob/master/g3doc/get_started.md).\n\n## Compatible Versions\n\nThe following table is the TFMA package versions that are compatible with each\nother. This is determined by our testing framework, but other *untested*\ncombinations may also work.\n\n|tensorflow-model-analysis                                                            |apache-beam[gcp]|pyarrow   |tensorflow         |tensorflow-metadata |tfx-bsl   |\n|------------------------------------------------------------------------------------ |----------------|----------|-------------------|--------------------|----------|\n|[GitHub master](https://github.com/tensorflow/model-analysis/blob/master/RELEASE.md) | 2.47.0         | 10.0.0   | nightly (2.x)     | 1.14.0             | 1.14.0   |\n|[0.45.0](https://github.com/tensorflow/model-analysis/blob/v0.45.0/RELEASE.md)       | 2.47.0         | 10.0.0   | 2.13              | 1.14.0             | 1.14.0   |\n|[0.44.0](https://github.com/tensorflow/model-analysis/blob/v0.44.0/RELEASE.md)       | 2.40.0         | 6.0.0    | 2.12              | 1.13.1             | 1.13.0   |\n|[0.43.0](https://github.com/tensorflow/model-analysis/blob/v0.43.0/RELEASE.md)       | 2.40.0         | 6.0.0    | 2.11              | 1.12.0             | 1.12.0   |\n|[0.42.0](https://github.com/tensorflow/model-analysis/blob/v0.42.0/RELEASE.md)       | 2.40.0         | 6.0.0    | 1.15.5 / 2.10     | 1.11.0             | 1.11.1   |\n|[0.41.0](https://github.com/tensorflow/model-analysis/blob/v0.41.0/RELEASE.md)       | 2.40.0         | 6.0.0    | 1.15.5 / 2.9      | 1.10.0             | 1.10.1   |\n|[0.40.0](https://github.com/tensorflow/model-analysis/blob/v0.40.0/RELEASE.md)       | 2.38.0         | 5.0.0    | 1.15.5 / 2.9      | 1.9.0              | 1.9.0    |\n|[0.39.0](https://github.com/tensorflow/model-analysis/blob/v0.39.0/RELEASE.md)       | 2.38.0         | 5.0.0    | 1.15.5 / 2.8      | 1.8.0              | 1.8.0    |\n|[0.38.0](https://github.com/tensorflow/model-analysis/blob/v0.38.0/RELEASE.md)       | 2.36.0         | 5.0.0    | 1.15.5 / 2.8      | 1.7.0              | 1.7.0    |\n|[0.37.0](https://github.com/tensorflow/model-analysis/blob/v0.37.0/RELEASE.md)       | 2.35.0         | 5.0.0    | 1.15.5 / 2.7      | 1.6.0              | 1.6.0    |\n|[0.36.0](https://github.com/tensorflow/model-analysis/blob/v0.36.0/RELEASE.md)       | 2.34.0         | 5.0.0    | 1.15.5 / 2.7      | 1.5.0              | 1.5.0    |\n|[0.35.0](https://github.com/tensorflow/model-analysis/blob/v0.35.0/RELEASE.md)       | 2.33.0         | 5.0.0    | 1.15 / 2.6        | 1.4.0              | 1.4.0    |\n|[0.34.1](https://github.com/tensorflow/model-analysis/blob/v0.34.1/RELEASE.md)       | 2.32.0         | 2.0.0    | 1.15 / 2.6        | 1.2.0              | 1.3.0    |\n|[0.34.0](https://github.com/tensorflow/model-analysis/blob/v0.34.0/RELEASE.md)       | 2.31.0         | 2.0.0    | 1.15 / 2.6        | 1.2.0              | 1.3.1    |\n|[0.33.0](https://github.com/tensorflow/model-analysis/blob/v0.33.0/RELEASE.md)       | 2.31.0         | 2.0.0    | 1.15 / 2.5        | 1.2.0              | 1.2.0    |\n|[0.32.1](https://github.com/tensorflow/model-analysis/blob/v0.32.1/RELEASE.md)       | 2.29.0         | 2.0.0    | 1.15 / 2.5        | 1.1.0              | 1.1.1    |\n|[0.32.0](https://github.com/tensorflow/model-analysis/blob/v0.32.0/RELEASE.md)       | 2.29.0         | 2.0.0    | 1.15 / 2.5        | 1.1.0              | 1.1.0    |\n|[0.31.0](https://github.com/tensorflow/model-analysis/blob/v0.31.0/RELEASE.md)       | 2.29.0         | 2.0.0    | 1.15 / 2.5        | 1.0.0              | 1.0.0    |\n|[0.30.0](https://github.com/tensorflow/model-analysis/blob/v0.30.0/RELEASE.md)       | 2.28.0         | 2.0.0    | 1.15 / 2.4        | 0.30.0             | 0.30.0   |\n|[0.29.0](https://github.com/tensorflow/model-analysis/blob/v0.29.0/RELEASE.md)       | 2.28.0         | 2.0.0    | 1.15 / 2.4        | 0.29.0             | 0.29.0   |\n|[0.28.0](https://github.com/tensorflow/model-analysis/blob/v0.28.0/RELEASE.md)       | 2.28.0         | 2.0.0    | 1.15 / 2.4        | 0.28.0             | 0.28.0   |\n|[0.27.0](https://github.com/tensorflow/model-analysis/blob/v0.27.0/RELEASE.md)       | 2.27.0         | 2.0.0    | 1.15 / 2.4        | 0.27.0             | 0.27.0   |\n|[0.26.1](https://github.com/tensorflow/model-analysis/blob/v0.26.1/RELEASE.md)       | 2.28.0         | 0.17.0   | 1.15 / 2.3        | 0.26.0             | 0.26.0   |\n|[0.26.0](https://github.com/tensorflow/model-analysis/blob/v0.26.0/RELEASE.md)       | 2.25.0         | 0.17.0   | 1.15 / 2.3        | 0.26.0             | 0.26.0   |\n|[0.25.0](https://github.com/tensorflow/model-analysis/blob/v0.25.0/RELEASE.md)       | 2.25.0         | 0.17.0   | 1.15 / 2.3        | 0.25.0             | 0.25.0   |\n|[0.24.3](https://github.com/tensorflow/model-analysis/blob/v0.24.3/RELEASE.md)       | 2.24.0         | 0.17.0   | 1.15 / 2.3        | 0.24.0             | 0.24.1   |\n|[0.24.2](https://github.com/tensorflow/model-analysis/blob/v0.24.2/RELEASE.md)       | 2.23.0         | 0.17.0   | 1.15 / 2.3        | 0.24.0             | 0.24.0   |\n|[0.24.1](https://github.com/tensorflow/model-analysis/blob/v0.24.1/RELEASE.md)       | 2.23.0         | 0.17.0   | 1.15 / 2.3        | 0.24.0             | 0.24.0   |\n|[0.24.0](https://github.com/tensorflow/model-analysis/blob/v0.24.0/RELEASE.md)       | 2.23.0         | 0.17.0   | 1.15 / 2.3        | 0.24.0             | 0.24.0   |\n|[0.23.0](https://github.com/tensorflow/model-analysis/blob/v0.23.0/RELEASE.md)       | 2.23.0         | 0.17.0   | 1.15 / 2.3        | 0.23.0             | 0.23.0   |\n|[0.22.2](https://github.com/tensorflow/model-analysis/blob/v0.22.2/RELEASE.md)       | 2.20.0         | 0.16.0   | 1.15 / 2.2        | 0.22.2             | 0.22.0   |\n|[0.22.1](https://github.com/tensorflow/model-analysis/blob/v0.22.1/RELEASE.md)       | 2.20.0         | 0.16.0   | 1.15 / 2.2        | 0.22.2             | 0.22.0   |\n|[0.22.0](https://github.com/tensorflow/model-analysis/blob/v0.22.0/RELEASE.md)       | 2.20.0         | 0.16.0   | 1.15 / 2.2        | 0.22.0             | 0.22.0   |\n|[0.21.6](https://github.com/tensorflow/model-analysis/blob/v0.21.6/RELEASE.md)       | 2.19.0         | 0.15.0   | 1.15 / 2.1        | 0.21.0             | 0.21.3   |\n|[0.21.5](https://github.com/tensorflow/model-analysis/blob/v0.21.5/RELEASE.md)       | 2.19.0         | 0.15.0   | 1.15 / 2.1        | 0.21.0             | 0.21.3   |\n|[0.21.4](https://github.com/tensorflow/model-analysis/blob/v0.21.4/RELEASE.md)       | 2.19.0         | 0.15.0   | 1.15 / 2.1        | 0.21.0             | 0.21.3   |\n|[0.21.3](https://github.com/tensorflow/model-analysis/blob/v0.21.3/RELEASE.md)       | 2.17.0         | 0.15.0   | 1.15 / 2.1        | 0.21.0             | 0.21.0   |\n|[0.21.2](https://github.com/tensorflow/model-analysis/blob/v0.21.2/RELEASE.md)       | 2.17.0         | 0.15.0   | 1.15 / 2.1        | 0.21.0             | 0.21.0   |\n|[0.21.1](https://github.com/tensorflow/model-analysis/blob/v0.21.1/RELEASE.md)       | 2.17.0         | 0.15.0   | 1.15 / 2.1        | 0.21.0             | 0.21.0   |\n|[0.21.0](https://github.com/tensorflow/model-analysis/blob/v0.21.0/RELEASE.md)       | 2.17.0         | 0.15.0   | 1.15 / 2.1        | 0.21.0             | 0.21.0   |\n|[0.15.4](https://github.com/tensorflow/model-analysis/blob/v0.15.4/RELEASE.md)       | 2.16.0         | 0.15.0   | 1.15 / 2.0        | n/a                | 0.15.1   |\n|[0.15.3](https://github.com/tensorflow/model-analysis/blob/v0.15.3/RELEASE.md)       | 2.16.0         | 0.15.0   | 1.15 / 2.0        | n/a                | 0.15.1   |\n|[0.15.2](https://github.com/tensorflow/model-analysis/blob/v0.15.2/RELEASE.md)       | 2.16.0         | 0.15.0   | 1.15 / 2.0        | n/a                | 0.15.1   |\n|[0.15.1](https://github.com/tensorflow/model-analysis/blob/v0.15.1/RELEASE.md)       | 2.16.0         | 0.15.0   | 1.15 / 2.0        | n/a                | 0.15.0   |\n|[0.15.0](https://github.com/tensorflow/model-analysis/blob/v0.15.0/RELEASE.md)       | 2.16.0         | 0.15.0   | 1.15              | n/a                | n/a      |\n|[0.14.0](https://github.com/tensorflow/model-analysis/blob/v0.14.0/RELEASE.md)       | 2.14.0         | n/a      | 1.14              | n/a                | n/a      |\n|[0.13.1](https://github.com/tensorflow/model-analysis/blob/v0.13.1/RELEASE.md)       | 2.11.0         | n/a      | 1.13              | n/a                | n/a      |\n|[0.13.0](https://github.com/tensorflow/model-analysis/blob/v0.13.0/RELEASE.md)       | 2.11.0         | n/a      | 1.13              | n/a                | n/a      |\n|[0.12.1](https://github.com/tensorflow/model-analysis/blob/v0.12.1/RELEASE.md)       | 2.10.0         | n/a      | 1.12              | n/a                | n/a      |\n|[0.12.0](https://github.com/tensorflow/model-analysis/blob/v0.12.0/RELEASE.md)       | 2.10.0         | n/a      | 1.12              | n/a                | n/a      |\n|[0.11.0](https://github.com/tensorflow/model-analysis/blob/v0.11.0/RELEASE.md)       | 2.8.0          | n/a      | 1.11              | n/a                | n/a      |\n|[0.9.2](https://github.com/tensorflow/model-analysis/blob/v0.9.2/RELEASE.md)         | 2.6.0          | n/a      | 1.9               | n/a                | n/a      |\n|[0.9.1](https://github.com/tensorflow/model-analysis/blob/v0.9.1/RELEASE.md)         | 2.6.0          | n/a      | 1.10              | n/a                | n/a      |\n|[0.9.0](https://github.com/tensorflow/model-analysis/blob/v0.9.0/RELEASE.md)         | 2.5.0          | n/a      | 1.9               | n/a                | n/a      |\n|[0.6.0](https://github.com/tensorflow/model-analysis/blob/v0.6.0/RELEASE.md)         | 2.4.0          | n/a      | 1.6               | n/a                | n/a      |\n\n## Questions\n\nPlease direct any questions about working with TFMA to\n[Stack Overflow](https://stackoverflow.com) using the\n[tensorflow-model-analysis](https://stackoverflow.com/questions/tagged/tensorflow-model-analysis)\ntag.\n", "release_dates": ["2023-08-14T20:20:00Z", "2023-04-14T19:45:09Z", "2022-12-09T19:15:41Z", "2022-11-16T23:31:35Z", "2022-10-07T18:51:57Z", "2022-09-09T00:25:38Z", "2022-07-01T08:38:27Z", "2022-05-16T03:23:38Z", "2022-03-04T23:47:25Z", "2022-01-24T17:50:57Z", "2021-12-02T04:29:18Z", "2021-11-02T18:54:42Z", "2021-09-20T17:03:29Z", "2021-08-30T20:40:08Z", "2021-07-28T20:55:20Z", "2021-07-16T22:14:47Z", "2021-06-24T00:08:32Z", "2021-05-24T17:55:57Z", "2021-05-14T00:22:55Z", "2021-04-21T20:35:18Z", "2021-03-24T22:14:51Z", "2021-02-23T18:55:05Z", "2021-01-28T16:09:36Z", "2020-12-16T20:41:19Z", "2020-11-04T18:38:50Z", "2020-09-24T21:58:01Z", "2020-09-19T01:21:49Z", "2020-09-11T23:01:07Z", "2020-09-10T01:55:35Z", "2020-08-24T16:00:47Z"]}, {"name": "model-card-toolkit", "description": "A toolkit that streamlines and automates the generation of model cards ", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Model Card Toolkit\n\n[![CI][ci_badge]][ci_link]\n[![PyPI][pypi_badge]][pypi_link]\n[![Documentation][docs_badge]][docs_link]\n\nThe Model Card Toolkit (MCT) streamlines and automates generation of\n[Model Cards](https://modelcards.withgoogle.com/about) [1], machine learning documents\nthat provide context and transparency into a model's development and performance.\nIntegrating the MCT into your ML pipeline enables you to share model metadata and\nmetrics with researchers, developers, reporters, and more.\n\nSome use cases of model cards include:\n\n* Facilitating the exchange of information between model builders and product developers.\n* Informing users of ML models to make better-informed decisions about how to use them (or how not to use them).\n* Providing model information required for effective public oversight and accountability.\n\n![Generated model card image](https://raw.githubusercontent.com/tensorflow/model-card-toolkit/main/model_card_toolkit/documentation/guide/images/model_card.png)\n\n## Installation\n\nThe Model Card Toolkit is hosted on [PyPI](https://pypi.org/project/model-card-toolkit/),\nand requires Python 3.7 or later.\n\nInstalling the basic, framework agnostic package:\n\n```sh\npip install model-card-toolkit\n```\n\nIf you are generating model cards for TensorFlow models, install the optional\nTensorFlow dependencies to use Model Card Toolkit's TensorFlow utilities:\n\n```sh\npip install model-card-toolkit[tensorflow]\n```\n\nYou may need to append the `--use-deprecated=legacy-resolver` flag when running\nversions of pip starting with 20.3.\n\nSee [the installation guide](model_card_toolkit/documentation/guide/install.md)\nfor more installation options.\n\n## Getting Started\n\n    import model_card_toolkit as mct\n\n    # Initialize the Model Card Toolkit with a path to store generate assets\n    model_card_output_path = ...\n    toolkit = mct.ModelCardToolkit(model_card_output_path)\n\n    # Initialize the ModelCard, which can be freely populated\n    model_card = toolkit.scaffold_assets()\n    model_card.model_details.name = 'My Model'\n\n    # Write the model card data to a proto file\n    toolkit.update_model_card(model_card)\n\n    # Return the model card document as an HTML page\n    html = toolkit.export_format()\n\n## Model Card Generation on TFX\n\nIf you are using [TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx), you can\nincorporate model card generation into your TFX pipeline via the `ModelCardGenerator`\ncomponent.\n\nThe `ModelCardGenerator` component has moved to the\n[TFX Addons](https://github.com/tensorflow/tfx-addons) library and is no longer\npackaged in Model Card Toolkit from version 2.0.0. Before you can use the\ncomponent, you will need to install the `tfx-addons` package:\n\n```sh\npip install tfx-addons[model_card_generator]\n```\n\nSee the [ModelCardGenerator guide](https://github.com/tensorflow/tfx-addons/blob/main/tfx_addons/model_card_generator/README.md)\nand run the [case study notebook](https://github.com/tensorflow/tfx-addons/blob/main/examples/model_card_generator/MLMD_Model_Card_Toolkit_Demo.ipynb)\nto learn more about the component.\n\n## Schema\n\nModel cards are stored in proto as an intermediate format. You can see the model\ncard JSON schema in the `schema` directory.\n\n## References\n\n[1] https://arxiv.org/abs/1810.03993\n\n\n[ci_badge]: https://github.com/tensorflow/model-card-toolkit/actions/workflows/ci.yml/badge.svg\n[ci_link]: https://github.com/tensorflow/model-card-toolkit/actions/workflows/ci.yml\n\n[pypi_badge]: https://badge.fury.io/py/model-card-toolkit.svg\n[pypi_link]: https://badge.fury.io/py/model-card-toolkit\n\n[docs_badge]: https://img.shields.io/badge/TensorFow-page-orange\n[docs_link]: https://www.tensorflow.org/responsible_ai/model_card_toolkit/guide\n", "release_dates": ["2023-07-07T05:27:51Z", "2023-04-03T17:58:08Z", "2023-04-02T21:31:20Z", "2022-02-24T21:00:01Z", "2022-01-06T17:29:20Z", "2021-09-02T22:08:38Z", "2021-08-03T00:08:17Z", "2021-02-05T20:56:01Z", "2021-01-27T22:23:56Z"]}, {"name": "model-optimization", "description": "A toolkit to optimize ML models for deployment for Keras and TensorFlow, including quantization and pruning.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Model Optimization Toolkit\n\nThe **TensorFlow Model Optimization Toolkit** is a suite of tools that users,\nboth novice and advanced, can use to optimize machine learning models for\ndeployment and execution.\n\nSupported techniques include quantization and pruning for sparse weights.\nThere are APIs built specifically for Keras.\n\nFor an overview of this project and individual tools, the optimization gains,\nand our roadmap refer to\n[tensorflow.org/model_optimization](https://www.tensorflow.org/model_optimization).\nThe website also provides various tutorials and API docs.\n\nThe toolkit provides stable Python APIs.\n\n## Installation\nFor installation instructions, see\n[tensorflow.org/model_optimization/guide/install](https://www.tensorflow.org/model_optimization/guide/install).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow Model Optimization, be sure to review\nthe [contribution guidelines](CONTRIBUTING.md). This project adheres to\nTensorFlow's\n[code of conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md).\nBy participating, you are expected to uphold this code.**\n\n**We use\n[GitHub issues](https://github.com/tensorflow/model-optimization/issues) for\ntracking requests and bugs.**\n\n## Maintainers\n\n<table>\n  <tr>\n    <th>Subpackage</th>\n    <th>Maintainers</th>\n  </tr>\n  <tr>\n    <td>tfmot.clustering</td>\n    <td>Arm ML Tooling</td>\n  </tr>\n  <tr>\n    <td>tfmot.quantization</td>\n    <td>TensorFlow Model Optimization</td>\n  </tr>\n  <tr>\n    <td>tfmot.sparsity</td>\n    <td>TensorFlow Model Optimization</td>\n  </tr>\n</table>\n\n## Community\n\nAs part of TensorFlow, we're committed to fostering an open and welcoming\nenvironment.\n\n*   [TensorFlow Blog](https://blog.tensorflow.org): Stay up to date on content\n    from the TensorFlow team and best articles from the community.\n", "release_dates": ["2024-02-08T02:06:46Z", "2023-05-26T02:36:01Z", "2023-04-03T01:49:37Z", "2022-07-20T20:42:08Z", "2022-03-18T17:04:59Z", "2022-02-09T08:07:26Z", "2021-09-30T10:12:08Z", "2021-06-18T06:40:44Z", "2020-09-14T04:46:03Z", "2020-08-05T23:38:06Z", "2020-07-28T17:55:10Z", "2020-04-07T20:30:52Z", "2019-12-18T23:37:57Z", "2019-12-18T01:49:23Z", "2019-08-23T22:12:08Z", "2019-07-12T20:52:22Z", "2019-05-13T20:59:45Z", "2019-05-13T17:15:24Z", "2019-05-09T18:53:07Z", "2019-05-06T16:25:20Z", "2019-05-02T21:15:38Z"]}, {"name": "model-remediation", "description": "Model Remediation is a library that provides solutions for machine learning practitioners working to create and train models in a way that reduces or eliminates user harm resulting from underlying performance biases.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Model Remediation\n\n\nTensorFlow Model Remediation is a library that provides solutions for machine\nlearning practitioners working to create and train models in a way that reduces\nor eliminates user harm resulting from underlying performance biases.\n\n[![PyPI version](https://badge.fury.io/py/tensorflow-model-remediation.svg)](https://badge.fury.io/py/tensorflow-model-remediation)\n\n[![Tutorial](https://img.shields.io/badge/doc-tutorial-blue.svg)](https://www.tensorflow.org/responsible_ai/model_remediation/min_diff/tutorials/min_diff_keras)\n\n[![Overview](https://img.shields.io/badge/doc-overview-blue.svg)](https://www.tensorflow.org/responsible_ai/model_remediation)\n\n## Installation\n\nYou can install the package from `pip`:\n\n```shell\n$ pip install tensorflow-model-remediation\n```\n\nNote: Make sure you are using TensorFlow 2.x.\n\n## Documentation\n\nThis library contains a collection of machine learning remediation techniques\nfor addressing potential bias in a model.\n\nCurrently TensorFlow Model Remediation contains the below techniques:\n\n*   MinDiff technique: Typically used to ensure that a model predicts the\n    preferred label equally well for all values of a sensitive attribute.\n    Helpful when trying to achieve [equality of\n    opportunity](https://developers.google.com/machine-learning/glossary/fairness#equality-of-opportunity).\n\n*   Counterfactual Logit Pairing technique: Typically used to ensure that a\n    model\u2019s prediction does not change between \u201ccounterfactual pairs\u201d, where the\n    sensitive attribute referenced in a feature is different. Helpful when\n    trying to achieve\n    [counterfactual fairness](https://developers.google.com/machine-learning/glossary/fairness#counterfactual-fairness).\n\nWe recommend starting with the\n[overview guide](https://www.tensorflow.org/responsible_ai/model_remediation) to\nget an idea of TensorFlow Model Remediation. Next try one of our interactive\nguides like the\n\n[MinDiff tutorial notebook](https://www.tensorflow.org/responsible_ai/model_remediation/min_diff/tutorials/min_diff_keras).\n\n[Counterfactual tutorial notebook](https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_keras).\n\n\n```python\n\nimport tensorflow_model_remediation as tfmr\n\nimport tensorflow as tf\n\n# Start by defining a Keras model.\n\noriginal_model = ...\n\n# Next pick the remediation technique you'd like to use. For example, a\n# MinDiff implementation might look like the below:\n# Set the MinDiff weight and choose a loss.\n\nmin_diff_loss = tfmr.min_diff.losses.MMDLoss()\n\nmin_diff_weight = 1.0  # Hyperparamater to be tuned.\n\n# Create a MinDiff model.\n\nmin_diff_model = tfmr.min_diff.keras.MinDiffModel(\n\n   original_model, min_diff_loss, min_diff_weight)\n\n# Compile the MinDiff model as you normally would do with the original model.\n\nmin_diff_model.compile(...)\n\n# Create a MinDiff Dataset and train the min_diff_model on it.\n\nmin_diff_model.fit(min_diff_dataset, ...)\n\n```\n\n#### *Disclaimers*\n\n*If you're interested in learning more about responsible AI practices, including*\n\n*fairness, please see Google AI's [Responsible AI Practices](https://ai.google/education/responsible-ai-practices).*\n\n*`tensorflow/model_remediation` is Apache 2.0 licensed. See the\n[`LICENSE`](LICENSE) file.*\n", "release_dates": ["2022-05-05T00:33:35Z", "2022-03-28T17:14:15Z", "2021-03-29T18:49:25Z", "2021-03-09T23:36:09Z", "2021-02-09T00:02:25Z", "2021-02-01T20:17:39Z"]}, {"name": "models", "description": "Models and examples built with TensorFlow", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "<div align=\"center\">\n  <img src=\"https://storage.googleapis.com/tf_model_garden/tf_model_garden_logo.png\">\n</div>\n\n[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://badge.fury.io/py/tensorflow)\n[![tf-models-official PyPI](https://badge.fury.io/py/tf-models-official.svg)](https://badge.fury.io/py/tf-models-official)\n\n\n# Welcome to the Model Garden for TensorFlow\n\nThe TensorFlow Model Garden is a repository with a number of different\nimplementations of state-of-the-art (SOTA) models and modeling solutions for\nTensorFlow users. We aim to demonstrate the best practices for modeling so that\nTensorFlow users can take full advantage of TensorFlow for their research and\nproduct development.\n\nTo improve the transparency and reproducibility of our models, training logs on\n[TensorBoard.dev](https://tensorboard.dev) are also provided for models to the\nextent possible though not all models are suitable.\n\n| Directory | Description |\n|-----------|-------------|\n| [official](official) | \u2022 A collection of example implementations for SOTA models using the latest TensorFlow 2's high-level APIs<br />\u2022 Officially maintained, supported, and kept up to date with the latest TensorFlow 2 APIs by TensorFlow<br />\u2022 Reasonably optimized for fast performance while still being easy to read<br /> For more details on the capabilities, check the guide on the [Model-garden](https://www.tensorflow.org/tfmodels)|\n| [research](research) | \u2022 A collection of research model implementations in TensorFlow 1 or 2 by researchers<br />\u2022 Maintained and supported by researchers |\n| [community](community) | \u2022 A curated list of the GitHub repositories with machine learning models and implementations powered by TensorFlow 2 |\n| [orbit](orbit) | \u2022 A flexible and lightweight library that users can easily use or fork when writing customized training loop code in TensorFlow 2.x. It seamlessly integrates with `tf.distribute` and supports running on different device types (CPU, GPU, and TPU). |\n\n## Installation\n\nTo install the current release of tensorflow-models, please follow any one of the methods described below.\n\n#### Method 1: Install the TensorFlow Model Garden pip package\n\n<details>\n\n**tf-models-official** is the stable Model Garden package. Please check out the [releases](https://github.com/tensorflow/models/releases) to see what are available modules.\n\npip3 will install all models and dependencies automatically.\n\n```shell\npip3 install tf-models-official\n```\n\nPlease check out our examples:\n  - [basic library import](https://github.com/tensorflow/models/blob/master/tensorflow_models/tensorflow_models_pypi.ipynb)\n  - [nlp model building](https://github.com/tensorflow/models/blob/master/docs/nlp/index.ipynb)\nto learn how to use a PIP package.\n\nNote that **tf-models-official** may not include the latest changes in the master branch of this\ngithub repo. To include latest changes, you may install **tf-models-nightly**,\nwhich is the nightly Model Garden package created daily automatically.\n\n```shell\npip3 install tf-models-nightly\n```\n\n</details>\n\n\n#### Method 2: Clone the source\n\n<details>\n\n1. Clone the GitHub repository:\n\n```shell\ngit clone https://github.com/tensorflow/models.git\n```\n\n2. Add the top-level ***/models*** folder to the Python path.\n\n```shell\nexport PYTHONPATH=$PYTHONPATH:/path/to/models\n```\n\nIf you are using in a Windows environment, you may need to use the following command with PowerShell:\n```shell\n$env:PYTHONPATH += \":\\path\\to\\models\"\n```\n\nIf you are using a Colab notebook, please set the Python path with os.environ.\n\n```python\nimport os\nos.environ['PYTHONPATH'] += \":/path/to/models\"\n```\n\n3. Install other dependencies\n\n```shell\npip3 install --user -r models/official/requirements.txt\n```\n\nFinally, if you are using nlp packages, please also install\n**tensorflow-text-nightly**:\n\n```shell\npip3 install tensorflow-text-nightly\n```\n\n</details>\n\n\n## Announcements\n\nPlease check [this page](https://github.com/tensorflow/models/wiki/Announcements) for recent announcements.\n\n## Contributions\n\n[![help wanted:paper implementation](https://img.shields.io/github/issues/tensorflow/models/help%20wanted%3Apaper%20implementation)](https://github.com/tensorflow/models/labels/help%20wanted%3Apaper%20implementation)\n\nIf you want to contribute, please review the [contribution guidelines](https://github.com/tensorflow/models/wiki/How-to-contribute).\n\n## License\n\n[Apache License 2.0](LICENSE)\n\n## Citing TensorFlow Model Garden\n\nIf you use TensorFlow Model Garden in your research, please cite this repository.\n\n```\n@misc{tensorflowmodelgarden2020,\n  author = {Hongkun Yu, Chen Chen, Xianzhi Du, Yeqing Li, Abdullah Rashwan, Le Hou, Pengchong Jin, Fan Yang,\n            Frederick Liu, Jaeyoun Kim, and Jing Li},\n  title = {{TensorFlow Model Garden}},\n  howpublished = {\\url{https://github.com/tensorflow/models}},\n  year = {2020}\n}\n```\n", "release_dates": ["2023-11-15T20:39:27Z", "2023-10-16T23:05:59Z", "2023-10-13T21:53:22Z", "2023-10-06T20:28:43Z", "2023-09-13T19:10:58Z", "2023-07-21T22:11:34Z", "2023-07-07T23:52:08Z", "2023-07-05T21:25:32Z", "2023-05-04T00:38:52Z", "2023-03-31T00:03:15Z", "2023-03-23T20:10:02Z", "2023-03-21T20:33:28Z", "2023-01-18T23:51:43Z", "2023-01-18T16:52:42Z", "2022-11-23T00:09:34Z", "2022-11-17T05:15:35Z", "2022-09-19T21:45:22Z", "2022-07-08T06:48:39Z", "2022-05-20T04:55:27Z", "2022-05-17T23:01:15Z", "2022-02-03T17:33:15Z", "2022-02-04T17:43:04Z", "2021-12-15T03:38:11Z", "2021-11-16T06:26:26Z", "2021-08-16T18:27:03Z", "2021-07-24T23:17:00Z", "2021-05-18T03:04:05Z", "2020-12-21T01:36:45Z", "2020-07-31T03:46:14Z", "2020-07-10T00:12:44Z"]}, {"name": "moonlight", "description": "Optical music recognition in TensorFlow", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<img align=\"center\" width=\"400\" height=\"94,358\" src=\"https://user-images.githubusercontent.com/34600369/40580500-74088e4a-6137-11e8-9705-ecac1499b1ce.png\">\n\n# Moonlight Optical Music Recognition (OMR) [![Build Status](https://travis-ci.org/tensorflow/moonlight.svg?branch=master)](https://travis-ci.org/tensorflow/moonlight)\n\nAn experimental [optical music\nrecognition](https://en.wikipedia.org/wiki/Optical_music_recognition) engine.\n\nMoonlight reads PNG image(s) containing sheet music and outputs\n[MusicXML](https://www.musicxml.com/) or a\n[NoteSequence message](https://github.com/tensorflow/magenta/blob/master/magenta/protobuf/music.proto).\nMusicXML is a standard sheet music interchange format, and `NoteSequence` is\nused by [Magenta](http://magenta.tensorflow.org) for training generative music\nmodels.\n\nMoonlight is not an officially supported Google product.\n\n### Command-Line Usage\n\n    git clone https://github.com/tensorflow/moonlight\n    cd moonlight\n    # You may want to run this inside a virtualenv.\n    pip install -r requirements.txt\n    # Build the OMR command-line tool.\n    bazel build moonlight:omr\n    # Prints a Score message.\n    bazel-bin/moonlight/omr moonlight/testdata/IMSLP00747-000.png\n    # Scans several pages and prints a NoteSequence message.\n    bazel-bin/moonlight/omr --output_type=NoteSequence IMSLP00001-*.png\n    # Writes MusicXML to ~/mozart.xml.\n    bazel-bin/moonlight/omr --output_type=MusicXML --output=$HOME/mozart.xml \\\n        corpus/56/IMSLP56442-*.png\n\nThe `omr` CLI will print a [`Score`](moonlight/protobuf/musicscore.proto)\nmessage by default, or [MusicXML](https://www.musicxml.com/) or a\n`NoteSequence` message if specified.\n\nMoonlight is intended to be run in bulk, and will not offer a full UI for\ncorrecting the score. The main entry point will be an Apache Beam pipeline that\nprocesses an entire corpus of images.\n\nThere is no release yet, and Moonlight is not ready for end users. To run\ninteractively or import the module, you can use the [sandbox\ndirectory](sandbox/README.md). Moonlight will be used offline for digitizing\na scanned corpus (it can be installed on all Cloud Compute platforms, and OS\ncompatibility is not a priority).\n\n### Dependencies\n\n* Linux\n  - Note: Our Google dep versions are fragile, and updating them or updating other OS may break directory structure in fragile ways.\n* [Protobuf 3.6.1](https://pypi.org/project/protobuf/3.6.1/)\n* [Bazel 0.20.0](https://github.com/bazelbuild/bazel/releases/tag/0.20.0). We\n  encountered some errors using Bazel 0.21.0 to build Protobuf 3.6.1, which is\n  the latest Protobuf release at the time of writing.\n* Python version supported by TensorFlow (Python 3.5-3.7)\n* Python dependencies specified in the [requirements](requirements.txt).\n\n### Resources\n\n[Forum](https://groups.google.com/forum/#!forum/moonlight-omr)\n", "release_dates": ["2020-08-14T19:48:13Z", "2018-05-16T17:38:49Z"]}, {"name": "networking", "description": "Enhanced networking support for TensorFlow. Maintained by SIG-networking.", "language": "C++", "license": null, "readme": "# TensorFlow Networking\n\n\nThis repository is for platform-specific networking extensions to core TensorFlow and related\nutilities (e.g. testing).\n\nThe design goal is to work towards separately compilable plugins, but initially we'll just be porting the\nnetworking related contrib directories since TensorFlow 2.0 will be dropping contrib.\n\n## Building\n\nCurrently support building GDR, VERBS, and MPI extensions:\n\n#### GDR\n\nUsing Bazel:\n\n```bash\nbazel build -c opt //tensorflow_networking/gdr:gdr_server_lib\n```\n\nUsing Docker:\n\n```bash\ndocker build -t tf_networking -f tensorflow_networking/gdr/Dockerfile .\n```\n\n#### VERBS\n\nUsing Bazel:\n\n```bash\nbazel build -c opt //tensorflow_networking/verbs:verbs_server_lib\n```\n\nUsing Docker:\n\n```bash\ndocker build -t tf_networking -f tensorflow_networking/verbs/Dockerfile .\n```\n\n####  MPI\n\n\nFor the MPI extensions the location to the MPI library has to be configured. The `configure` script is used to setup this configuration. The script will attempt to find the location of the `mpirun` binary and from there deduce the include and library paths. You can use the `MPI_HOME` environment variable if `mpirun` is not installed in your PATH or you want to use another base path for the MPI library. The configure script will create symbolic links inside the `third_party/mpi` folder to the relevant MPI header and library files. Furthermore the script will determine if your MPI installation is based on `OpenMPI` or on `MPICH` and sets this in the `.tf_networking_configure.bazelrc` file.\n\n#####  `grpc+mpi` extension\n\nUsing Bazel:\n\nBy manually answering the relevant configuration questions\n```bash\n./configure\nbazel build -c opt //tensorflow_networking/mpi:mpi_server_lib\n```\nor by preset answers to the configuration questions\n```bash\nMPI_HOME=<path to mpi folder root> TF_NEED_MPI=1 ./configure\nbazel build -c opt //tensorflow_networking/mpi:mpi_server_lib\n```\n\nUsing Docker:\n\n```bash\ndocker build -t tf_networking -f tensorflow_networking/mpi/Dockerfile .\n```\n\n\n#####  `MPI collectives` extension\n\nUsing Bazel:\n\nBy manually answering the relevant configuration questions\n```bash\n./configure\nbazel build -c opt //tensorflow_networking/mpi_collectives:all\n```\n\nUsing Docker:\n\n```bash\ndocker build -t tf_networking -f tensorflow_networking/mpi_collectives/Dockerfile .\n```\n\n#####  `grpc+seastar` extension\n\nUsing Bazel:\n\n```bash\nbazel build -c opt --copt='-std=gnu++14' //tensorflow_networking:libtensorflow_networking.so\n", "release_dates": []}, {"name": "neural-structured-learning", "description": "Training neural models with structured signals.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Neural Structured Learning in TensorFlow\n\n![](g3doc/images/nsl_overview.png)\n\n**Neural Structured Learning (NSL)** is a new learning paradigm to train neural\nnetworks by leveraging structured signals in addition to feature inputs.\nStructure can be explicit as represented by a graph [1,2,5] or implicit as\ninduced by adversarial perturbation [3,4].\n\nStructured signals are commonly used to represent relations or similarity among\nsamples that may be labeled or unlabeled. Leveraging these signals during neural\nnetwork training harnesses both labeled and unlabeled data, which can improve\nmodel accuracy, particularly when **the amount of labeled data is relatively\nsmall**. Additionally, models trained with samples that are generated by\nadversarial perturbation have been shown to be **robust against malicious\nattacks**, which are designed to mislead a model's prediction or classification.\n\nNSL generalizes to Neural Graph Learning [1] as well as to Adversarial Learning\n[3]. The NSL framework in TensorFlow provides the following easy-to-use APIs and\ntools for developers to train models with structured signals:\n\n*   **Keras APIs** to enable training with graphs (explicit structure) and\n    adversarial perturbations (implicit structure).\n\n*   **TF ops and functions** to enable training with structure when using\n    lower-level TensorFlow APIs\n\n*   **Tools** to build graphs and construct graph inputs for training\n\nThe NSL framework is designed to be flexible and can be used to train any kind\nof neural network. For example, feed-forward, convolution, and recurrent neural\nnetworks can all be trained using the NSL framework. In addition to supervised\nand semi-supervised learning (a low amount of supervision), NSL can in theory be\ngeneralized to unsupervised learning. Incorporating structured signals is done\nonly during training, so the performance of the serving/inference workflow\nremains unchanged. Please check out our tutorials for a practical introduction\nto NSL.\n\n## Getting started\n\nYou can install the prebuilt NSL pip package by running:\n\n```bash\npip install neural-structured-learning\n```\n\nFor more detailed instructions on how to install NSL as a package or to build it\nfrom source in various environments, please see the\n[installation guide](g3doc/install.md)\n\nNote that NSL requires a TensorFlow version of 1.15 or higher. NSL also supports\nTensorFlow 2.x with the exception of v2.1, which contains a bug that is\nincompatible with NSL.\n\n## Videos and Colab Tutorials\n\nGet a jump-start on NSL by watching our video series on YouTube! It gives a\ncomplete overview of the framework as well as discusses several aspects of\nlearning with structured signals.\n\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=N_IS3x5wFNI\"\n   target=\"_blank\"><img src=\"http://img.youtube.com/vi/N_IS3x5wFNI/0.jpg\"\n                        alt=\"Overall Framework\" width=\"180\"  border=\"2\" /></a>\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=pJRRdtJ-rPU\"\n   target=\"_blank\"><img src=\"http://img.youtube.com/vi/pJRRdtJ-rPU/0.jpg\"\n                        alt=\"Natural Graphs\" width=\"180\" border=\"2\" /></a>\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=3RQqTTOY0U0\"\n   target=\"_blank\"><img src=\"http://img.youtube.com/vi/3RQqTTOY0U0/0.jpg\"\n                        alt=\"Synthetic Graphs\" width=\"180\" border=\"2\" /></a>\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=Js2WJkhdU7k\"\n   target=\"_blank\"><img src=\"http://img.youtube.com/vi/Js2WJkhdU7k/0.jpg\"\n                        alt=\"Adversarial Learning\" width=\"180\" border=\"2\" /></a>\n\nWe've also created hands-on colab-based tutorials that will allow you to\ninteractively explore NSL. Here are a few:\n\n*   [training with natural graphs](https://github.com/tensorflow/neural-structured-learning/blob/master/g3doc/tutorials/graph_keras_mlp_cora.ipynb)\n*   [training with synthesized graphs](https://github.com/tensorflow/neural-structured-learning/blob/master/g3doc/tutorials/graph_keras_lstm_imdb.ipynb)\n*   [adversarial learning](https://github.com/tensorflow/neural-structured-learning/blob/master/g3doc/tutorials/adversarial_keras_cnn_mnist.ipynb)\n\nYou can find more examples and tutorials under the\n[examples](neural_structured_learning/examples) directory.\n\n## Contributing to NSL\n\nContributions are welcome and highly appreciated - there are several ways to\ncontribute to TF Neural Structured Learning:\n\n*   Case studies: If you are interested in applying NSL, consider wrapping up\n    your usage as a tutorial, a new dataset, or an example model that others\n    could use for experiments and/or development. The [examples](examples)\n    directory could be a good destination for such contributions.\n\n*   Product excellence: If you are interested in improving NSL's product\n    excellence and developer experience, the best way is to clone this repo,\n    make changes directly on the implementation in your local repo, and then\n    send us pull request to integrate your changes.\n\n*   New algorithms: If you are interested in developing new algorithms for NSL,\n    the best way is to study the implementations of NSL libraries, and to think\n    of extensions to the existing implementation (or alternative approaches). If\n    you have a proposal for a new algorithm, we recommend starting by staging\n    your project in the [research](research) directory and including a colab\n    notebook to showcase the new features. If you develop new algorithms in your\n    own repository, we would be happy to feature pointers to academic\n    publications and/or repositories using NSL from this repository.\n\nPlease be sure to review the [contribution guidelines](CONTRIBUTING.md).\n\n## Research\n\nSee our [research](research) directory for research projects in Neural\nStructured Learning:\n\n*   [Low-Dimensional Hyperbolic Knowledge Graph Embeddings](research/kg_hyp_emb)\n*   [A2N: Attending to Neighbors for Knowledge Graph Inference](research/a2n)\n*   [GAM: Graph Agreement Models for Semi-Supervised Learning](research/gam)\n*   [Neural Clustering Processes](research/neural_clustering)\n*   [CARLS: Cross-platform Asynchronous Representation Learning System](research/carls)\n*   [Denoised Smoothing: A Provable Defense for Pretrained Classifiers](research/third_party/denoised_smoothing)\n\n## Featured Usage\n\nPlease see the [usage page](usage.md) to learn more about how NSL is being\ndiscussed and used in the open source community.\n\n## Issues, Questions, and Feedback\n\nPlease use\n[GitHub issues](https://github.com/tensorflow/neural-structured-learning/issues)\nto file issues, bugs, and feature requests. For questions, please direct them to\n[Stack Overflow](https://stackoverflow.com) with the\n[\"nsl\"](https://stackoverflow.com/questions/tagged/nsl) tag. For feedback,\nplease fill this\n[form](https://docs.google.com/forms/d/1AQEcPSgmwWBJj3H2haEytF4C_fr1aotWaHjCEXpPm2A);\nwe would love to hear from you.\n\n## Release Notes\n\nPlease see the [release notes](RELEASE.md) for detailed version updates.\n\n## References\n\n[[1] T. Bui, S. Ravi and V. Ramavajjala. \"Neural Graph Learning: Training Neural\nNetworks Using Graphs.\" WSDM 2018](https://research.google/pubs/pub46568.pdf)\n\n[[2] T. Kipf and M. Welling. \"Semi-supervised classification with graph\nconvolutional networks.\" ICLR 2017](https://arxiv.org/pdf/1609.02907.pdf)\n\n[[3] I. Goodfellow, J. Shlens and C. Szegedy. \"Explaining and harnessing\nadversarial examples.\" ICLR 2015](https://arxiv.org/pdf/1412.6572.pdf)\n\n[[4] T. Miyato, S. Maeda, M. Koyama and S. Ishii. \"Virtual Adversarial Training:\na Regularization Method for Supervised and Semi-supervised Learning.\" ICLR\n2016](https://arxiv.org/pdf/1704.03976.pdf)\n\n[[5] D. Juan, C. Lu, Z. Li, F. Peng, A. Timofeev, Y. Chen, Y. Gao, T. Duerig, A.\nTomkins and S. Ravi \"Graph-RISE: Graph-Regularized Image Semantic Embedding.\"\nWSDM 2020](https://arxiv.org/abs/1902.10814)\n", "release_dates": ["2022-07-29T20:57:47Z", "2020-08-18T00:30:03Z", "2020-07-31T00:18:01Z", "2020-06-10T22:08:58Z", "2019-10-15T17:37:04Z", "2019-09-18T18:17:38Z", "2019-09-03T21:09:11Z"]}, {"name": "ngraph-bridge", "description": "TensorFlow-nGraph bridge", "language": "C++", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "\n<p align=\"center\">\n  <img src=\"images/ngraph-logo.png\">\n</p>\n\n# Intel\u00ae nGraph\u2122 Compiler and Runtime for TensorFlow*\n\nThis repository contains the code needed to enable Intel(R) nGraph(TM) Compiler and \nruntime engine for TensorFlow. Use it to speed up your TensorFlow training and \ninference workloads. The nGraph Library and runtime suite can also be used to \ncustomize and deploy Deep Learning inference models that will \"just work\" with \na variety of nGraph-enabled backends: CPU, and custom silicon like the \n[Intel(R) Nervana(TM) NNP](https://itpeernetwork.intel.com/inteldcisummit-artificial-intelligence/).\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/tensorflow/ngraph-bridge/blob/master/LICENSE)\n[![Build Status](https://badge.buildkite.com/180bbf814f1a884219849b4838cbda5fa1e03715e494185be3.svg?branch=master)](https://buildkite.com/ngraph/cpu)\n[![Build Status](https://badge.buildkite.com/ae8d39ef4a18eb238b58ab0637fb97e85b86e85822a08b96d1.svg?branch=master)](https://buildkite.com/ngraph/models-cpu)\n[![Build Status](https://badge.buildkite.com/0aeaff43e378d387a160d30083f203f7147f010e3fb15b01d1.svg?branch=master)](https://buildkite.com/ngraph/cpu-intel-tf)\n\n\n#### *** This repository is currently undergoing heavy refactoring for optimization of inference use-cases. If you are looking for the latest stable baseline, please use the following tag: [v0.22.0-rc4](https://github.com/tensorflow/ngraph-bridge/tree/v0.22.0-rc4) ***\n\n## Installation\n\n### Requirements\n\n|Using pre-built packages| Building from source|\n| -----------------------|-------------------|\n|Python 3| Python 3|\n|TensorFlow v2.2.0|GCC 7.5 (Ubuntu), Clang/LLVM (macOS)|\n|        |`cmake` 3.4 or higher|\n|        |Bazelisk|\n|        |`virtualenv` 16.0.0+|\n|        |`patchelf`|\n\n### Use pre-built packages\n\n nGraph bridge enables you to use the nGraph Library with TensorFlow.\n Complete the following steps to install a pre-built nGraph bridge for\n TensorFlow.\n\n1. Ensure the following pip version is being used:\n\n        pip install --upgrade pip==19.3.1\n\n2. Install TensorFlow:\n\n        pip install -U tensorflow==1.14.0\n\n3. Install `ngraph-tensorflow-bridge`:\n\n        pip install -U ngraph-tensorflow-bridge\n\n### Build nGraph from source\n\nTo use the latest version of nGraph Library, complete the following steps to\nbuild nGraph bridge from source. \n\n#### Note to macOS users\n\nThe build and installation instructions are identical for Ubuntu 16.04 and\nmacOS. However, the Python setup may vary across different versions of Mac OS.\nTensorFlow build instructions recommend using Homebrew but developers often use\nPyenv. Some users prefer Anaconda/Miniconda. Before building nGraph, ensure that\nyou can successfully build TensorFlow on macOS with a suitable Python\nenvironment.\n\nThe requirements for building nGraph bridge are identical to the requirements for \nbuilding TensorFlow from source. For more information, review the [TensorFlow configuration] \ndetails. \n\n##### Prepare your build environment\n\nInstall the following requirements before building the `ngraph-bridge`. \n\nInstall [Bazelisk](https://github.com/bazelbuild/bazelisk):\n\n        wget https://github.com/bazelbuild/bazelisk/releases/download/v1.7.4/bazelisk-linux-amd64\n        mv bazelisk-linux-amd64 ~/bin/bazel\n        chmod +x ~/bin/bazel\n\nAdd and source the `bin` path to your `~/.bashrc` file to call\nbazel:\n\n        export PATH=$PATH:~/bin\n        source ~/.bashrc   \n\nInstall `cmake`, `virtualenv`, and `gcc`.\n\n##### Build nGraph bridge\n\nOnce TensorFlow's dependencies are installed, clone the `ngraph-bridge` repo:\n\n        git clone https://github.com/tensorflow/ngraph-bridge.git\n        cd ngraph-bridge\n\nRun the following Python script to build TensorFlow, nGraph, and the bridge. Use Python 3:\n\n        python3 build_ngtf.py --use_prebuilt_tensorflow\n\nWhen the build finishes, a new `virtualenv` directory is created in `build_cmake/venv-tf-py3`. Build artifacts (i.e., the `ngraph_tensorflow_bridge-<VERSION>-py2.py3-none-manylinux1_x86_64.whl`) are created in the `build_cmake/artifacts` directory. \n\nFor more build options:\n        \n        python3 build_ngtf.py --help\n\nTo use the `ngraph-tensorflow-bridge`, activate the following `virtualenv` to start using nGraph with TensorFlow. \n\n        source build_cmake/venv-tf-py3/bin/activate\n \nAlternatively, you can also install the TensorFlow and nGraph bridge outside of a `virtualenv`. The Python `whl` files are located in the `build_cmake/artifacts/` and `build_cmake/artifacts/tensorflow` directories, respectively.\n\nSelect the help option of `build_ngtf.py` script to learn more about various build options. \n\nVerify that `ngraph-bridge` installed correctly:\n\n    python -c \"import tensorflow as tf; print('TensorFlow version: ',tf.__version__);\\\n                import ngraph_bridge; print(ngraph_bridge.__version__)\"\n\nThis will produce something like this:\n\n        TensorFlow version:  2.2.0\n        nGraph bridge version: b'0.22.0-rc3'\n        nGraph version used for this build: b'0.28.0-rc.1+d2cd873'\n        TensorFlow version used for this build: v2.2.0-0-2b96f3662b\n        CXX11_ABI flag used for this build: 1\n        nGraph bridge built with Grappler: False\n\n\nNote: The version of the ngraph-tensorflow-bridge is not going to be exactly \nthe same as when you build from source. This is due to delay in the source \nrelease and publishing the corresponding Python wheel.\n\nTest the installation:\n\n        python3 test_ngtf.py\n\nThis command runs all C++ and Python unit tests from the `ngraph-bridge` source tree. It also runs various TensorFlow Python tests using nGraph.\n\n### Build and run nGraph in Docker\n\nA shell script and dockerfiles are provided in the [`tools`](/tools) directory for easy setup in a Docker container. \nSee [this README](/tools) if you want to use Docker.\n\n## Classify an image\n\nOnce you have installed nGraph bridge, you can use TensorFlow to train a neural network or run inference using a trained model.\nThe only change required to a script is adding\n\n    import ngraph_bridge\n\nUse `infer_image.py` in the [examples] directory to classify an image.\n\nNote: The script downloads the inceptionV3 model and sample image.\n\n    python examples/infer_image.py\n\nThis will print the following results:\n\n    military uniform 0.8343056\n    mortarboard 0.021869544\n    academic gown 0.010358088\n    pickelhaube 0.008008157\n    bulletproof vest 0.005350913\n\nTo classify your own images, modify the `infer_image.py` file.\n\n#### Measure the time\nnGraph is a Just In Time (JIT) compiler meaning that the TensorFlow computation graph is compiled to nGraph during the first instance of the execution. From the second time onwards, the execution speeds up significantly. \n\nAdd the following Python code to measure the computation time:\n\n```python\n# Warmup\nsess.run(output_operation.outputs[0], {\n        input_operation.outputs[0]: t})\n# Run\nimport time\nstart = time.time()\nresults = sess.run(output_operation.outputs[0], {\n        input_operation.outputs[0]: t\n        })      \nelapsed = time.time() - start\nprint('Time elapsed: %f seconds' % elapsed)\n```\nObserve that the output time runs faster than TensorFlow native (i.e., without nGraph).\n\n#### Add additional backends\n\nYou can substitute the default CPU backend with a different backend. \nUse the following API:\n\n    ngraph_bridge.set_backend('backend_name')\n\nTo determine what backends are available on your system, use the following API:\n\n    ngraph_bridge.list_backends()\n\nMore detailed examples on how to use ngraph_bridge are located in the [examples] directory.\n\n## Debugging \n\nDuring the build, often there are missing configuration steps for building TensorFlow. If you run into build issues, first ensure that you can build TensorFlow. For debugging run time issues, see the instructions provided in the [diagnostics] directory.\n\n## Support\n\nPlease submit your questions, feature requests and bug reports via [GitHub issues].\n\n## How to Contribute\n\nWe welcome community contributions to nGraph. If you have an idea for how to \nimprove it:\n\n* Share your proposal via [GitHub issues].\n* Ensure you can build the product and run all the examples with your patch.\n* In the case of a larger feature, create a test.\n* Submit a [pull request].\n* We will review your contribution and, if any additional fixes or\n  modifications are necessary, may provide feedback to guide you. When\n  accepted, your pull request will be merged to the repository.\n\n\n## About Intel\u00ae nGraph\u2122\n\nSee the [full documentation] here.\n\n[TensorFlow]:https://github.com/tensorflow/tensorflow.git\n[Github issues]: https://github.com/tensorflow/ngraph-bridge/issues\n[pull request]: https://github.com/tensorflow/ngraph-bridge/pulls\n[bazel version]: https://github.com/bazelbuild/bazel/releases/tag/0.25.2\n[TensorFlow configuration]: https://www.tensorflow.org/install/source\n[diagnostics]:diagnostics/README.md\n[examples]:examples/README.md\n[nGraph]:https://docs.openvinotoolkit.org/latest/openvino_docs_nGraph_DG_Introduction.html\n[full documentation]:https://docs.openvinotoolkit.org/latest/openvino_docs_nGraph_DG_Introduction.html\n[frozen model]: https://www.tensorflow.org/guide/extend/model_files#freezing\n[TensorFlow C++ and Python Image Recognition Demo]: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image\n", "release_dates": ["2020-12-18T01:41:53Z", "2020-01-27T18:46:12Z", "2020-01-24T01:39:17Z", "2020-01-22T01:45:08Z", "2020-01-15T17:12:16Z", "2020-01-14T06:37:54Z", "2019-12-26T18:40:16Z", "2019-12-24T01:59:35Z", "2019-12-12T21:10:24Z", "2019-12-10T23:51:07Z", "2019-12-05T22:18:42Z", "2019-11-19T04:12:37Z", "2019-11-18T21:19:44Z", "2019-10-03T23:23:30Z", "2019-09-26T21:11:48Z", "2019-09-25T05:04:52Z", "2019-09-19T18:00:39Z", "2019-09-19T17:12:19Z", "2019-09-13T05:56:00Z", "2019-08-22T05:22:37Z", "2019-08-20T17:08:05Z", "2019-08-17T02:34:37Z", "2019-08-16T00:23:51Z", "2019-08-12T07:22:07Z", "2019-08-10T00:31:22Z", "2019-08-09T19:14:39Z", "2019-08-08T19:51:05Z", "2019-08-08T03:59:27Z", "2019-08-08T23:09:58Z", "2019-08-01T22:11:14Z"]}, {"name": "nmt", "description": "TensorFlow Neural Machine Translation Tutorial", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Neural Machine Translation (seq2seq) Tutorial\n\n*Authors: Thang Luong, Eugene Brevdo, Rui Zhao ([Google Research Blogpost](https://research.googleblog.com/2017/07/building-your-own-neural-machine.html), [Github](https://github.com/tensorflow/nmt))*\n\n*This version of the tutorial requires [TensorFlow Nightly](https://github.com/tensorflow/tensorflow/#installation).\nFor using the stable TensorFlow versions, please consider other branches such as\n[tf-1.4](https://github.com/tensorflow/nmt/tree/tf-1.4).*\n\n*If make use of this codebase for your research, please cite\n[this](#bibtex).*\n\n- [Introduction](#introduction)\n- [Basic](#basic)\n   - [Background on Neural Machine Translation](#background-on-neural-machine-translation)\n   - [Installing the Tutorial](#installing-the-tutorial)\n   - [Training \u2013 *How to build our first NMT system*](#training--how-to-build-our-first-nmt-system)\n      - [Embedding](#embedding)\n      - [Encoder](#encoder)\n      - [Decoder](#decoder)\n      - [Loss](#loss)\n      - [Gradient computation & optimization](#gradient-computation--optimization)\n   - [Hands-on \u2013 *Let's train an NMT model*](#hands-on--lets-train-an-nmt-model)\n   - [Inference \u2013 *How to generate translations*](#inference--how-to-generate-translations)\n- [Intermediate](#intermediate)\n   - [Background on the Attention Mechanism](#background-on-the-attention-mechanism)\n   - [Attention Wrapper API](#attention-wrapper-api)\n   - [Hands-on \u2013 *Building an attention-based NMT model*](#hands-on--building-an-attention-based-nmt-model)\n- [Tips & Tricks](#tips--tricks)\n   - [Building Training, Eval, and Inference Graphs](#building-training-eval-and-inference-graphs)\n   - [Data Input Pipeline](#data-input-pipeline)\n   - [Other details for better NMT models](#other-details-for-better-nmt-models)\n      - [Bidirectional RNNs](#bidirectional-rnns)\n      - [Beam search](#beam-search)\n      - [Hyperparameters](#hyperparameters)\n      - [Multi-GPU training](#multi-gpu-training)\n- [Benchmarks](#benchmarks)\n   - [IWSLT English-Vietnamese](#iwslt-english-vietnamese)\n   - [WMT German-English](#wmt-german-english)\n   - [WMT English-German &mdash; *Full Comparison*](#wmt-english-german--full-comparison)\n   - [Standard HParams](#standard-hparams)\n- [Other resources](#other-resources)\n- [Acknowledgment](#acknowledgment)\n- [References](#references)\n- [BibTex](#bibtex)\n\n\n# Introduction\n\nSequence-to-sequence (seq2seq) models\n([Sutskever et al., 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf),\n[Cho et al., 2014](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)) have\nenjoyed great success in a variety of tasks such as machine translation, speech\nrecognition, and text summarization. This tutorial gives readers a full\nunderstanding of seq2seq models and shows how to build a competitive seq2seq\nmodel from scratch. We focus on the task of Neural Machine Translation (NMT)\nwhich was the very first testbed for seq2seq models with\nwild\n[success](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html). The\nincluded code is lightweight, high-quality, production-ready, and incorporated\nwith the latest research ideas. We achieve this goal by:\n\n1. Using the recent decoder / attention\n   wrapper\n   [API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops),\n   TensorFlow 1.2 data iterator\n1. Incorporating our strong expertise in building recurrent and seq2seq models\n1. Providing tips and tricks for building the very best NMT models and replicating\n   [Google\u2019s NMT (GNMT) system](https://research.google.com/pubs/pub45610.html).\n\nWe believe that it is important to provide benchmarks that people can easily\nreplicate. As a result, we have provided full experimental results and\npretrained our models on the following publicly available datasets:\n\n1. *Small-scale*: English-Vietnamese parallel corpus of TED talks (133K sentence\n   pairs) provided by\n   the\n   [IWSLT Evaluation Campaign](https://sites.google.com/site/iwsltevaluation2015/).\n1. *Large-scale*: German-English parallel corpus (4.5M sentence pairs) provided\n   by the [WMT Evaluation Campaign](http://www.statmt.org/wmt16/translation-task.html).\n\nWe first build up some basic knowledge about seq2seq models for NMT, explaining\nhow to build and train a vanilla NMT model. The second part will go into details\nof building a competitive NMT model with attention mechanism. We then discuss\ntips and tricks to build the best possible NMT models (both in speed and\ntranslation quality) such as TensorFlow best practices (batching, bucketing),\nbidirectional RNNs, beam search, as well as scaling up to multiple GPUs using GNMT attention.\n\n# Basic\n\n## Background on Neural Machine Translation\n\nBack in the old days, traditional phrase-based translation systems performed\ntheir task by breaking up source sentences into multiple chunks and then\ntranslated them phrase-by-phrase. This led to disfluency in the translation\noutputs and was not quite like how we, humans, translate. We read the entire\nsource sentence, understand its meaning, and then produce a translation. Neural\nMachine Translation (NMT) mimics that!\n\n<p align=\"center\">\n<img width=\"80%\" src=\"nmt/g3doc/img/encdec.jpg\" />\n<br>\nFigure 1. <b>Encoder-decoder architecture</b> \u2013 example of a general approach for\nNMT. An encoder converts a source sentence into a \"meaning\" vector which is\npassed through a <i>decoder</i> to produce a translation.\n</p>\n\nSpecifically, an NMT system first reads the source sentence using an *encoder*\nto build\na\n[\"thought\" vector](https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence),\na sequence of numbers that represents the sentence meaning; a *decoder*, then,\nprocesses the sentence vector to emit a translation, as illustrated in\nFigure 1. This is often referred to as the *encoder-decoder architecture*. In\nthis manner, NMT addresses the local translation problem in the traditional\nphrase-based approach: it can capture *long-range dependencies* in languages,\ne.g., gender agreements; syntax structures; etc., and produce much more fluent\ntranslations as demonstrated\nby\n[Google Neural Machine Translation systems](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html).\n\nNMT models vary in terms of their exact architectures. A natural choice for\nsequential data is the recurrent neural network (RNN), used by most NMT models.\nUsually an RNN is used for both the encoder and decoder. The RNN models,\nhowever, differ in terms of: (a) *directionality* \u2013 unidirectional or\nbidirectional; (b) *depth* \u2013 single- or multi-layer; and (c) *type* \u2013 often\neither a vanilla RNN, a Long Short-term Memory (LSTM), or a gated recurrent unit\n(GRU). Interested readers can find more information about RNNs and LSTM on\nthis [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n\nIn this tutorial, we consider as examples a *deep multi-layer RNN* which is\nunidirectional and uses LSTM as a recurrent unit. We show an example of such a\nmodel in Figure 2. In this example, we build a model to translate a source\nsentence \"I am a student\" into a target sentence \"Je suis \u00e9tudiant\". At a high\nlevel, the NMT model consists of two recurrent neural networks: the *encoder*\nRNN simply consumes the input source words without making any prediction; the\n*decoder*, on the other hand, processes the target sentence while predicting the\nnext words.\n\nFor more information, we refer readers\nto [Luong (2016)](https://github.com/lmthang/thesis) which this tutorial is\nbased on.\n\n<p align=\"center\">\n<img width=\"48%\" src=\"nmt/g3doc/img/seq2seq.jpg\" />\n<br>\nFigure 2. <b>Neural machine translation</b> \u2013 example of a deep recurrent\narchitecture proposed by for translating a source sentence \"I am a student\" into\na target sentence \"Je suis \u00e9tudiant\". Here, \"&lts&gt\" marks the start of the\ndecoding process while \"&lt/s&gt\" tells the decoder to stop.\n</p>\n\n## Installing the Tutorial\n\nTo install this tutorial, you need to have TensorFlow installed on your system.\nThis tutorial requires TensorFlow Nightly. To install TensorFlow, follow\nthe [installation instructions here](https://www.tensorflow.org/install/).\n\nOnce TensorFlow is installed, you can download the source code of this tutorial\nby running:\n\n``` shell\ngit clone https://github.com/tensorflow/nmt/\n```\n\n## Training \u2013 How to build our first NMT system\n\nLet's first dive into the heart of building an NMT model with concrete code\nsnippets through which we will explain Figure 2 in more detail. We defer data\npreparation and the full code to later. This part refers to\nfile\n[**model.py**](nmt/model.py).\n\nAt the bottom layer, the encoder and decoder RNNs receive as input the\nfollowing: first, the source sentence, then a boundary marker \"\\<s\\>\" which\nindicates the transition from the encoding to the decoding mode, and the target\nsentence.  For *training*, we will feed the system with the following tensors,\nwhich are in time-major format and contain word indices:\n\n-  **encoder_inputs** [max_encoder_time, batch_size]: source input words.\n-  **decoder_inputs** [max_decoder_time, batch_size]: target input words.\n-  **decoder_outputs** [max_decoder_time, batch_size]: target output words,\n   these are decoder_inputs shifted to the left by one time step with an\n   end-of-sentence tag appended on the right.\n\nHere for efficiency, we train with multiple sentences (batch_size) at\nonce. Testing is slightly different, so we will discuss it later.\n\n### Embedding\n\nGiven the categorical nature of words, the model must first look up the source\nand target embeddings to retrieve the corresponding word representations. For\nthis *embedding layer* to work, a vocabulary is first chosen for each language.\nUsually, a vocabulary size V is selected, and only the most frequent V words are\ntreated as unique.  All other words are converted to an \"unknown\" token and all\nget the same embedding.  The embedding weights, one set per language, are\nusually learned during training.\n\n``` python\n# Embedding\nembedding_encoder = variable_scope.get_variable(\n    \"embedding_encoder\", [src_vocab_size, embedding_size], ...)\n# Look up embedding:\n#   encoder_inputs: [max_time, batch_size]\n#   encoder_emb_inp: [max_time, batch_size, embedding_size]\nencoder_emb_inp = embedding_ops.embedding_lookup(\n    embedding_encoder, encoder_inputs)\n```\n\nSimilarly, we can build *embedding_decoder* and *decoder_emb_inp*. Note that one\ncan choose to initialize embedding weights with pretrained word representations\nsuch as word2vec or Glove vectors. In general, given a large amount of training\ndata we can learn these embeddings from scratch.\n\n### Encoder\n\nOnce retrieved, the word embeddings are then fed as input into the main network,\nwhich consists of two multi-layer RNNs \u2013 an encoder for the source language and\na decoder for the target language. These two RNNs, in principle, can share the\nsame weights; however, in practice, we often use two different RNN parameters\n(such models do a better job when fitting large training datasets). The\n*encoder* RNN uses zero vectors as its starting states and is built as follows:\n\n``` python\n# Build RNN cell\nencoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n\n# Run Dynamic RNN\n#   encoder_outputs: [max_time, batch_size, num_units]\n#   encoder_state: [batch_size, num_units]\nencoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n    encoder_cell, encoder_emb_inp,\n    sequence_length=source_sequence_length, time_major=True)\n```\n\nNote that sentences have different lengths to avoid wasting computation, we tell\n*dynamic_rnn* the exact source sentence lengths through\n*source_sequence_length*. Since our input is time major, we set\n*time_major=True*. Here, we build only a single layer LSTM, *encoder_cell*. We\nwill describe how to build multi-layer LSTMs, add dropout, and use attention in\na later section.\n\n### Decoder\n\nThe *decoder* also needs to have access to the source information, and one\nsimple way to achieve that is to initialize it with the last hidden state of the\nencoder, *encoder_state*. In Figure 2, we pass the hidden state at the source\nword \"student\" to the decoder side.\n\n``` python\n# Build RNN cell\ndecoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n```\n\n``` python\n# Helper\nhelper = tf.contrib.seq2seq.TrainingHelper(\n    decoder_emb_inp, decoder_lengths, time_major=True)\n# Decoder\ndecoder = tf.contrib.seq2seq.BasicDecoder(\n    decoder_cell, helper, encoder_state,\n    output_layer=projection_layer)\n# Dynamic decoding\noutputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\nlogits = outputs.rnn_output\n```\n\nHere, the core part of this code is the *BasicDecoder* object, *decoder*, which\nreceives *decoder_cell* (similar to encoder_cell), a *helper*, and the previous\n*encoder_state* as inputs. By separating out decoders and helpers, we can reuse\ndifferent codebases, e.g., *TrainingHelper* can be substituted with\n*GreedyEmbeddingHelper* to do greedy decoding. See more\nin\n[helper.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/helper.py).\n\nLastly, we haven't mentioned *projection_layer* which is a dense matrix to turn\nthe top hidden states to logit vectors of dimension V. We illustrate this\nprocess at the top of Figure 2.\n\n``` python\nprojection_layer = layers_core.Dense(\n    tgt_vocab_size, use_bias=False)\n```\n\n### Loss\n\nGiven the *logits* above, we are now ready to compute our training loss:\n\n``` python\ncrossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n    labels=decoder_outputs, logits=logits)\ntrain_loss = (tf.reduce_sum(crossent * target_weights) /\n    batch_size)\n```\n\nHere, *target_weights* is a zero-one matrix of the same size as\n*decoder_outputs*. It masks padding positions outside of the target sequence\nlengths with values 0.\n\n***Important note***: It's worth pointing out that we divide the loss by\n*batch_size*, so our hyperparameters are \"invariant\" to batch_size. Some people\ndivide the loss by (*batch_size* * *num_time_steps*), which plays down the\nerrors made on short sentences. More subtly, our hyperparameters (applied to the\nformer way) can't be used for the latter way. For example, if both approaches\nuse SGD with a learning of 1.0, the latter approach effectively uses a much\nsmaller learning rate of 1 / *num_time_steps*.\n\n### Gradient computation & optimization\n\nWe have now defined the forward pass of our NMT model. Computing the\nbackpropagation pass is just a matter of a few lines of code:\n\n``` python\n# Calculate and clip gradients\nparams = tf.trainable_variables()\ngradients = tf.gradients(train_loss, params)\nclipped_gradients, _ = tf.clip_by_global_norm(\n    gradients, max_gradient_norm)\n```\n\nOne of the important steps in training RNNs is gradient clipping. Here, we clip\nby the global norm.  The max value, *max_gradient_norm*, is often set to a value\nlike 5 or 1. The last step is selecting the optimizer.  The Adam optimizer is a\ncommon choice.  We also select a learning rate.  The value of *learning_rate*\ncan is usually in the range 0.0001 to 0.001; and can be set to decrease as\ntraining progresses.\n\n``` python\n# Optimization\noptimizer = tf.train.AdamOptimizer(learning_rate)\nupdate_step = optimizer.apply_gradients(\n    zip(clipped_gradients, params))\n```\n\nIn our own experiments, we use standard SGD (tf.train.GradientDescentOptimizer)\nwith a decreasing learning rate schedule, which yields better performance. See\nthe [benchmarks](#benchmarks).\n\n## Hands-on \u2013 Let's train an NMT model\n\nLet's train our very first NMT model, translating from Vietnamese to English!\nThe entry point of our code\nis\n[**nmt.py**](nmt/nmt.py).\n\nWe will use a *small-scale parallel corpus of TED talks* (133K training\nexamples) for this exercise. All data we used here can be found\nat:\n[https://nlp.stanford.edu/projects/nmt/](https://nlp.stanford.edu/projects/nmt/). We\nwill use tst2012 as our dev dataset, and tst2013 as our test dataset.\n\nRun the following command to download the data for training NMT model:\\\n\t`nmt/scripts/download_iwslt15.sh /tmp/nmt_data`\n\nRun the following command to start the training:\n\n``` shell\nmkdir /tmp/nmt_model\npython -m nmt.nmt \\\n    --src=vi --tgt=en \\\n    --vocab_prefix=/tmp/nmt_data/vocab  \\\n    --train_prefix=/tmp/nmt_data/train \\\n    --dev_prefix=/tmp/nmt_data/tst2012  \\\n    --test_prefix=/tmp/nmt_data/tst2013 \\\n    --out_dir=/tmp/nmt_model \\\n    --num_train_steps=12000 \\\n    --steps_per_stats=100 \\\n    --num_layers=2 \\\n    --num_units=128 \\\n    --dropout=0.2 \\\n    --metrics=bleu\n```\n\nThe above command trains a 2-layer LSTM seq2seq model with 128-dim hidden units\nand embeddings for 12 epochs. We use a dropout value of 0.2 (keep probability\n0.8). If no error, we should see logs similar to the below with decreasing\nperplexity values as we train.\n\n```\n# First evaluation, global step 0\n  eval dev: perplexity 17193.66\n  eval test: perplexity 17193.27\n# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017\n  sample train data:\n    src_reverse: </s> </s> \u0110i\u1ec1u \u0111\u00f3 , d\u0129 nhi\u00ean , l\u00e0 c\u00e2u chuy\u1ec7n tr\u00edch ra t\u1eeb h\u1ecdc thuy\u1ebft c\u1ee7a Karl Marx .\n    ref: That , of course , was the <unk> distilled from the theories of Karl Marx . </s> </s> </s>\n  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00\n  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00\n  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00\n  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00\n  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00\n```\n\nSee [**train.py**](nmt/train.py) for more details.\n\nWe can start Tensorboard to view the summary of the model during training:\n\n``` shell\ntensorboard --port 22222 --logdir /tmp/nmt_model/\n```\n\nTraining the reverse direction from English and Vietnamese can be done simply by changing:\\\n\t`--src=en --tgt=vi`\n\n## Inference \u2013 How to generate translations\n\nWhile you're training your NMT models (and once you have trained models), you\ncan obtain translations given previously unseen source sentences. This process\nis called inference. There is a clear distinction between training and inference\n(*testing*): at inference time, we only have access to the source sentence,\ni.e., *encoder_inputs*. There are many ways to perform decoding.  Decoding\nmethods include greedy, sampling, and beam-search decoding. Here, we will\ndiscuss the greedy decoding strategy.\n\nThe idea is simple and we illustrate it in Figure 3:\n\n1. We still encode the source sentence in the same way as during training to\n   obtain an *encoder_state*, and this *encoder_state* is used to initialize the\n   decoder.\n2. The decoding (translation) process is started as soon as the decoder receives\n   a starting symbol \"\\<s\\>\" (refer as *tgt_sos_id* in our code);\n3. For each timestep on the decoder side, we treat the RNN's output as a set of\n   logits.  We choose the most likely word, the id associated with the maximum\n   logit value, as the emitted word (this is the \"greedy\" behavior).  For\n   example in Figure 3, the word \"moi\" has the highest translation probability\n   in the first decoding step.  We then feed this word as input to the next\n   timestep.\n4. The process continues until the end-of-sentence marker \"\\</s\\>\" is produced as\n   an output symbol (refer as *tgt_eos_id* in our code).\n\n<p align=\"center\">\n<img width=\"40%\" src=\"nmt/g3doc/img/greedy_dec.jpg\" />\n<br>\nFigure 3. <b>Greedy decoding</b> \u2013 example of how a trained NMT model produces a\ntranslation for a source sentence \"Je suis \u00e9tudiant\" using greedy search.\n</p>\n\nStep 3 is what makes inference different from training. Instead of always\nfeeding the correct target words as an input, inference uses words predicted by\nthe model. Here's the code to achieve greedy decoding.  It is very similar to\nthe training decoder.\n\n``` python\n# Helper\nhelper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n    embedding_decoder,\n    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)\n\n# Decoder\ndecoder = tf.contrib.seq2seq.BasicDecoder(\n    decoder_cell, helper, encoder_state,\n    output_layer=projection_layer)\n# Dynamic decoding\noutputs, _ = tf.contrib.seq2seq.dynamic_decode(\n    decoder, maximum_iterations=maximum_iterations)\ntranslations = outputs.sample_id\n```\n\nHere, we use *GreedyEmbeddingHelper* instead of *TrainingHelper*. Since we do\nnot know the target sequence lengths in advance, we use *maximum_iterations* to\nlimit the translation lengths. One heuristic is to decode up to two times the\nsource sentence lengths.\n\n``` python\nmaximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)\n```\n\nHaving trained a model, we can now create an inference file and translate some\nsentences:\n\n``` shell\ncat > /tmp/my_infer_file.vi\n# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)\n\npython -m nmt.nmt \\\n    --out_dir=/tmp/nmt_model \\\n    --inference_input_file=/tmp/my_infer_file.vi \\\n    --inference_output_file=/tmp/nmt_model/output_infer\n\ncat /tmp/nmt_model/output_infer # To view the inference as output\n```\n\nNote the above commands can also be run while the model is still being trained\nas long as there exists a training\ncheckpoint. See [**inference.py**](nmt/inference.py) for more details.\n\n# Intermediate\n\nHaving gone through the most basic seq2seq model, let's get more advanced! To\nbuild state-of-the-art neural machine translation systems, we will need more\n\"secret sauce\": the *attention mechanism*, which was first introduced\nby [Bahdanau et al., 2015](https://arxiv.org/abs/1409.0473), then later refined\nby [Luong et al., 2015](https://arxiv.org/abs/1508.04025) and others. The key\nidea of the attention mechanism is to establish direct short-cut connections\nbetween the target and the source by paying \"attention\" to relevant source\ncontent as we translate. A nice byproduct of the attention mechanism is an\neasy-to-visualize alignment matrix between the source and target sentences (as\nshown in Figure 4).\n\n<p align=\"center\">\n<img width=\"40%\" src=\"nmt/g3doc/img/attention_vis.jpg\" />\n<br>\nFigure 4. <b>Attention visualization</b> \u2013 example of the alignments between source\nand target sentences. Image is taken from (Bahdanau et al., 2015).\n</p>\n\nRemember that in the vanilla seq2seq model, we pass the last source state from\nthe encoder to the decoder when starting the decoding process. This works well\nfor short and medium-length sentences; however, for long sentences, the single\nfixed-size hidden state becomes an information bottleneck. Instead of discarding\nall of the hidden states computed in the source RNN, the attention mechanism\nprovides an approach that allows the decoder to peek at them (treating them as a\ndynamic memory of the source information). By doing so, the attention mechanism\nimproves the translation of longer sentences. Nowadays, attention mechanisms are\nthe defacto standard and have been successfully applied to many other tasks\n(including image caption generation, speech recognition, and text\nsummarization).\n\n## Background on the Attention Mechanism\n\nWe now describe an instance of the attention mechanism proposed in (Luong et\nal., 2015), which has been used in several state-of-the-art systems including\nopen-source toolkits such as [OpenNMT](http://opennmt.net/about/) and in the TF\nseq2seq API in this tutorial. We will also provide connections to other variants\nof the attention mechanism.\n\n<p align=\"center\">\n<img width=\"48%\" src=\"nmt/g3doc/img/attention_mechanism.jpg\" />\n<br>\nFigure 5. <b>Attention mechanism</b> \u2013 example of an attention-based NMT system\nas described in (Luong et al., 2015) . We highlight in detail the first step of\nthe attention computation. For clarity, we don't show the embedding and\nprojection layers in Figure (2).\n</p>\n\nAs illustrated in Figure 5, the attention computation happens at every decoder\ntime step.  It consists of the following stages:\n\n1. The current target hidden state is compared with all source states to derive\n   *attention weights* (can be visualized as in Figure 4).\n1. Based on the attention weights we compute a *context vector* as the weighted\n   average of the source states.\n1. Combine the context vector with the current target hidden state to yield the\n   final *attention vector*\n1. The attention vector is fed as an input to the next time step (*input\n   feeding*).  The first three steps can be summarized by the equations below:\n\n<p align=\"center\">\n<img width=\"80%\" src=\"nmt/g3doc/img/attention_equation_0.jpg\" />\n<br>\n</p>\n\nHere, the function `score` is used to compared the target hidden state $$h_t$$\nwith each of the source hidden states $$\\overline{h}_s$$, and the result is normalized to\nproduced attention weights (a distribution over source positions). There are\nvarious choices of the scoring function; popular scoring functions include the\nmultiplicative and additive forms given in Eq. (4). Once computed, the attention\nvector $$a_t$$ is used to derive the softmax logit and loss.  This is similar to the\ntarget hidden state at the top layer of a vanilla seq2seq model. The function\n`f` can also take other forms.\n\n<p align=\"center\">\n<img width=\"80%\" src=\"nmt/g3doc/img/attention_equation_1.jpg\" />\n<br>\n</p>\n\nVarious implementations of attention mechanisms can be found\nin\n[attention_wrapper.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py).\n\n***What matters in the attention mechanism?***\n\nAs hinted in the above equations, there are many different attention variants.\nThese variants depend on the form of the scoring function and the attention\nfunction, and on whether the previous state $$h_{t-1}$$ is used instead of\n$$h_t$$ in the scoring function as originally suggested in (Bahdanau et al.,\n2015). Empirically, we found that only certain choices matter. First, the basic\nform of attention, i.e., direct connections between target and source, needs to\nbe present. Second, it's important to feed the attention vector to the next\ntimestep to inform the network about past attention decisions as demonstrated in\n(Luong et al., 2015). Lastly, choices of the scoring function can often result\nin different performance. See more in the [benchmark results](#benchmarks)\nsection.\n\n## Attention Wrapper API\n\nIn our implementation of\nthe\n[AttentionWrapper](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py),\nwe borrow some terminology\nfrom [(Weston et al., 2015)](https://arxiv.org/abs/1410.3916) in their work on\n*memory networks*. Instead of having readable & writable memory, the attention\nmechanism presented in this tutorial is a *read-only* memory. Specifically, the\nset of source hidden states (or their transformed versions, e.g.,\n$$W\\overline{h}_s$$ in Luong's scoring style or $$W_2\\overline{h}_s$$ in\nBahdanau's scoring style) is referred to as the *\"memory\"*. At each time step,\nwe use the current target hidden state as a *\"query\"* to decide on which parts\nof the memory to read.  Usually, the query needs to be compared with keys\ncorresponding to individual memory slots. In the above presentation of the\nattention mechanism, we happen to use the set of source hidden states (or their\ntransformed versions, e.g., $$W_1h_t$$ in Bahdanau's scoring style) as\n\"keys\". One can be inspired by this memory-network terminology to derive other\nforms of attention!\n\nThanks to the attention wrapper, extending our vanilla seq2seq code with\nattention is trivial. This part refers to\nfile [**attention_model.py**](nmt/attention_model.py)\n\nFirst, we need to define an attention mechanism, e.g., from (Luong et al.,\n2015):\n\n``` python\n# attention_states: [batch_size, max_time, num_units]\nattention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n\n# Create an attention mechanism\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\n    num_units, attention_states,\n    memory_sequence_length=source_sequence_length)\n```\n\nIn the previous [Encoder](#encoder) section, *encoder_outputs* is the set of all\nsource hidden states at the top layer and has the shape of *[max_time,\nbatch_size, num_units]* (since we use *dynamic_rnn* with *time_major* set to\n*True* for efficiency). For the attention mechanism, we need to make sure the\n\"memory\" passed in is batch major, so we need to transpose\n*attention_states*. We pass *source_sequence_length* to the attention mechanism\nto ensure that the attention weights are properly normalized (over non-padding\npositions only).\n\nHaving defined an attention mechanism, we use *AttentionWrapper* to wrap the\ndecoding cell:\n\n``` python\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n    decoder_cell, attention_mechanism,\n    attention_layer_size=num_units)\n```\n\nThe rest of the code is almost the same as in the Section [Decoder](#decoder)!\n\n## Hands-on \u2013 building an attention-based NMT model\n\nTo enable attention, we need to use one of `luong`, `scaled_luong`, `bahdanau`\nor `normed_bahdanau` as the value of the `attention` flag during training. The\nflag specifies which attention mechanism we are going to use. In addition, we\nneed to create a new directory for the attention model, so we don't reuse the\npreviously trained basic NMT model.\n\nRun the following command to start the training:\n\n``` shell\nmkdir /tmp/nmt_attention_model\n\npython -m nmt.nmt \\\n    --attention=scaled_luong \\\n    --src=vi --tgt=en \\\n    --vocab_prefix=/tmp/nmt_data/vocab  \\\n    --train_prefix=/tmp/nmt_data/train \\\n    --dev_prefix=/tmp/nmt_data/tst2012  \\\n    --test_prefix=/tmp/nmt_data/tst2013 \\\n    --out_dir=/tmp/nmt_attention_model \\\n    --num_train_steps=12000 \\\n    --steps_per_stats=100 \\\n    --num_layers=2 \\\n    --num_units=128 \\\n    --dropout=0.2 \\\n    --metrics=bleu\n```\n\nAfter training, we can use the same inference command with the new out_dir for\ninference:\n\n``` shell\npython -m nmt.nmt \\\n    --out_dir=/tmp/nmt_attention_model \\\n    --inference_input_file=/tmp/my_infer_file.vi \\\n    --inference_output_file=/tmp/nmt_attention_model/output_infer\n```\n\n# Tips & Tricks\n\n## Building Training, Eval, and Inference Graphs\n\nWhen building a machine learning model in TensorFlow, it's often best to build\nthree separate graphs:\n\n-  The Training graph, which:\n    -  Batches, buckets, and possibly subsamples input data from a set of\n       files/external inputs.\n    -  Includes the forward and backprop ops.\n    -  Constructs the optimizer, and adds the training op.\n\n-  The Eval graph, which:\n    -  Batches and buckets input data from a set of files/external inputs.\n    -  Includes the training forward ops, and additional evaluation ops that\n       aren't used for training.\n\n-  The Inference graph, which:\n    -  May not batch input data.\n    -  Does not subsample or bucket input data.\n    -  Reads input data from placeholders (data can be fed directly to the graph\n       via *feed_dict* or from a C++ TensorFlow serving binary).\n    -  Includes a subset of the model forward ops, and possibly additional\n       special inputs/outputs for storing state between session.run calls.\n\nBuilding separate graphs has several benefits:\n\n-  The inference graph is usually very different from the other two, so it makes\n   sense to build it separately.\n-  The eval graph becomes simpler since it no longer has all the additional\n   backprop ops.\n-  Data feeding can be implemented separately for each graph.\n-  Variable reuse is much simpler.  For example, in the eval graph there's no\n   need to reopen variable scopes with *reuse=True* just because the Training\n   model created these variables already.  So the same code can be reused\n   without sprinkling *reuse=* arguments everywhere.\n-  In distributed training, it is commonplace to have separate workers perform\n   training, eval, and inference.  These need to build their own graphs anyway.\n   So building the system this way prepares you for distributed training.\n\nThe primary source of complexity becomes how to share Variables across the three\ngraphs in a single machine setting. This is solved by using a separate session\nfor each graph. The training session periodically saves checkpoints, and the\neval session and the infer session restore parameters from checkpoints. The\nexample below shows the main differences between the two approaches.\n\n**Before: Three models in a single graph and sharing a single Session**\n\n``` python\nwith tf.variable_scope('root'):\n  train_inputs = tf.placeholder()\n  train_op, loss = BuildTrainModel(train_inputs)\n  initializer = tf.global_variables_initializer()\n\nwith tf.variable_scope('root', reuse=True):\n  eval_inputs = tf.placeholder()\n  eval_loss = BuildEvalModel(eval_inputs)\n\nwith tf.variable_scope('root', reuse=True):\n  infer_inputs = tf.placeholder()\n  inference_output = BuildInferenceModel(infer_inputs)\n\nsess = tf.Session()\n\nsess.run(initializer)\n\nfor i in itertools.count():\n  train_input_data = ...\n  sess.run([loss, train_op], feed_dict={train_inputs: train_input_data})\n\n  if i % EVAL_STEPS == 0:\n    while data_to_eval:\n      eval_input_data = ...\n      sess.run([eval_loss], feed_dict={eval_inputs: eval_input_data})\n\n  if i % INFER_STEPS == 0:\n    sess.run(inference_output, feed_dict={infer_inputs: infer_input_data})\n```\n\n**After: Three models in three graphs, with three Sessions sharing the same Variables**\n\n``` python\ntrain_graph = tf.Graph()\neval_graph = tf.Graph()\ninfer_graph = tf.Graph()\n\nwith train_graph.as_default():\n  train_iterator = ...\n  train_model = BuildTrainModel(train_iterator)\n  initializer = tf.global_variables_initializer()\n\nwith eval_graph.as_default():\n  eval_iterator = ...\n  eval_model = BuildEvalModel(eval_iterator)\n\nwith infer_graph.as_default():\n  infer_iterator, infer_inputs = ...\n  infer_model = BuildInferenceModel(infer_iterator)\n\ncheckpoints_path = \"/tmp/model/checkpoints\"\n\ntrain_sess = tf.Session(graph=train_graph)\neval_sess = tf.Session(graph=eval_graph)\ninfer_sess = tf.Session(graph=infer_graph)\n\ntrain_sess.run(initializer)\ntrain_sess.run(train_iterator.initializer)\n\nfor i in itertools.count():\n\n  train_model.train(train_sess)\n\n  if i % EVAL_STEPS == 0:\n    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)\n    eval_model.saver.restore(eval_sess, checkpoint_path)\n    eval_sess.run(eval_iterator.initializer)\n    while data_to_eval:\n      eval_model.eval(eval_sess)\n\n  if i % INFER_STEPS == 0:\n    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)\n    infer_model.saver.restore(infer_sess, checkpoint_path)\n    infer_sess.run(infer_iterator.initializer, feed_dict={infer_inputs: infer_input_data})\n    while data_to_infer:\n      infer_model.infer(infer_sess)\n```\n\nNotice how the latter approach is \"ready\" to be converted to a distributed\nversion.\n\nOne other difference in the new approach is that instead of using *feed_dicts*\nto feed data at each *session.run* call (and thereby performing our own\nbatching, bucketing, and manipulating of data), we use stateful iterator\nobjects.  These iterators make the input pipeline much easier in both the\nsingle-machine and distributed setting. We will cover the new input data\npipeline (as introduced in TensorFlow 1.2) in the next section.\n\n## Data Input Pipeline\n\nPrior to TensorFlow 1.2, users had two options for feeding data to the\nTensorFlow training and eval pipelines:\n\n1. Feed data directly via *feed_dict* at each training *session.run* call.\n1. Use the queueing mechanisms in *tf.train* (e.g. *tf.train.batch*) and\n   *tf.contrib.train*.\n1. Use helpers from a higher level framework like *tf.contrib.learn* or\n   *tf.contrib.slim* (which effectively use #2).\n\nThe first approach is easier for users who aren't familiar with TensorFlow or\nneed to do exotic input modification (i.e., their own minibatch queueing) that\ncan only be done in Python.  The second and third approaches are more standard\nbut a little less flexible; they also require starting multiple python threads\n(queue runners).  Furthermore, if used incorrectly queues can lead to deadlocks\nor opaque error messages.  Nevertheless, queues are significantly more efficient\nthan using *feed_dict* and are the standard for both single-machine and\ndistributed training.\n\nStarting in TensorFlow 1.2, there is a new system available for reading data\ninto TensorFlow models: dataset iterators, as found in the **tf.data**\nmodule. Data iterators are flexible, easy to reason about and to manipulate, and\nprovide efficiency and multithreading by leveraging the TensorFlow C++ runtime.\n\nA **dataset** can be created from a batch data Tensor, a filename, or a Tensor\ncontaining multiple filenames.  Some examples:\n\n``` python\n# Training dataset consists of multiple files.\ntrain_dataset = tf.data.TextLineDataset(train_files)\n\n# Evaluation dataset uses a single file, but we may\n# point to a different file for each evaluation round.\neval_file = tf.placeholder(tf.string, shape=())\neval_dataset = tf.data.TextLineDataset(eval_file)\n\n# For inference, feed input data to the dataset directly via feed_dict.\ninfer_batch = tf.placeholder(tf.string, shape=(num_infer_examples,))\ninfer_dataset = tf.data.Dataset.from_tensor_slices(infer_batch)\n```\n\nAll datasets can be treated similarly via input processing.  This includes\nreading and cleaning the data, bucketing (in the case of training and eval),\nfiltering, and batching.\n\nTo convert each sentence into vectors of word strings, for example, we use the\ndataset map transformation:\n\n``` python\ndataset = dataset.map(lambda string: tf.string_split([string]).values)\n```\n\nWe can then switch each sentence vector into a tuple containing both the vector\nand its dynamic length:\n\n``` python\ndataset = dataset.map(lambda words: (words, tf.size(words))\n```\n\nFinally, we can perform a vocabulary lookup on each sentence.  Given a lookup\ntable object table, this map converts the first tuple elements from a vector of\nstrings to a vector of integers.\n\n\n``` python\ndataset = dataset.map(lambda words, size: (table.lookup(words), size))\n```\n\nJoining two datasets is also easy.  If two files contain line-by-line\ntranslations of each other and each one is read into its own dataset, then a new\ndataset containing the tuples of the zipped lines can be created via:\n\n``` python\nsource_target_dataset = tf.data.Dataset.zip((source_dataset, target_dataset))\n```\n\nBatching of variable-length sentences is straightforward. The following\ntransformation batches *batch_size* elements from *source_target_dataset*, and\nrespectively pads the source and target vectors to the length of the longest\nsource and target vector in each batch.\n\n``` python\nbatched_dataset = source_target_dataset.padded_batch(\n        batch_size,\n        padded_shapes=((tf.TensorShape([None]),  # source vectors of unknown size\n                        tf.TensorShape([])),     # size(source)\n                       (tf.TensorShape([None]),  # target vectors of unknown size\n                        tf.TensorShape([]))),    # size(target)\n        padding_values=((src_eos_id,  # source vectors padded on the right with src_eos_id\n                         0),          # size(source) -- unused\n                        (tgt_eos_id,  # target vectors padded on the right with tgt_eos_id\n                         0)))         # size(target) -- unused\n```\n\nValues emitted from this dataset will be nested tuples whose tensors have a\nleftmost dimension of size *batch_size*.  The structure will be:\n\n-  iterator[0][0] has the batched and padded source sentence matrices.\n-  iterator[0][1] has the batched source size vectors.\n-  iterator[1][0] has the batched and padded target sentence matrices.\n-  iterator[1][1] has the batched target size vectors.\n\nFinally, bucketing that batches similarly-sized source sentences together is\nalso possible.  Please see the\nfile\n[utils/iterator_utils.py](nmt/utils/iterator_utils.py) for\nmore details and the full implementation.\n\nReading data from a Dataset requires three lines of code: create the iterator,\nget its values, and initialize it.\n\n``` python\nbatched_iterator = batched_dataset.make_initializable_iterator()\n\n((source, source_lengths), (target, target_lengths)) = batched_iterator.get_next()\n\n# At initialization time.\nsession.run(batched_iterator.initializer, feed_dict={...})\n```\n\nOnce the iterator is initialized, every *session.run* call that accesses source\nor target tensors will request the next minibatch from the underlying dataset.\n\n## Other details for better NMT models\n\n### Bidirectional RNNs\n\nBidirectionality on the encoder side generally gives better performance (with\nsome degradation in speed as more layers are used). Here, we give a simplified\nexample of how to build an encoder with a single bidirectional layer:\n\n``` python\n# Construct forward and backward cells\nforward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\nbackward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n\nbi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n    forward_cell, backward_cell, encoder_emb_inp,\n    sequence_length=source_sequence_length, time_major=True)\nencoder_outputs = tf.concat(bi_outputs, -1)\n```\n\nThe variables *encoder_outputs* and *encoder_state* can be used in the same way\nas in Section Encoder. Note that, for multiple bidirectional layers, we need to\nmanipulate the encoder_state a bit, see [model.py](nmt/model.py), method\n*_build_bidirectional_rnn()* for more details.\n\n### Beam search\n\nWhile greedy decoding can give us quite reasonable translation quality, a beam\nsearch decoder can further boost performance. The idea of beam search is to\nbetter explore the search space of all possible translations by keeping around a\nsmall set of top candidates as we translate. The size of the beam is called\n*beam width*; a minimal beam width of, say size 10, is generally sufficient. For\nmore information, we refer readers to Section 7.2.3\nof [Neubig, (2017)](https://arxiv.org/abs/1703.01619). Here's an example of how\nbeam search can be done:\n\n``` python\n# Replicate encoder infos beam_width times\ndecoder_initial_state = tf.contrib.seq2seq.tile_batch(\n    encoder_state, multiplier=hparams.beam_width)\n\n# Define a beam-search decoder\ndecoder = tf.contrib.seq2seq.BeamSearchDecoder(\n        cell=decoder_cell,\n        embedding=embedding_decoder,\n        start_tokens=start_tokens,\n        end_token=end_token,\n        initial_state=decoder_initial_state,\n        beam_width=beam_width,\n        output_layer=projection_layer,\n        length_penalty_weight=0.0,\n        coverage_penalty_weight=0.0)\n\n# Dynamic decoding\noutputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\n```\n\nNote that the same *dynamic_decode()* API call is used, similar to the\nSection [Decoder](#decoder). Once decoded, we can access the translations as\nfollows:\n\n``` python\ntranslations = outputs.predicted_ids\n# Make sure translations shape is [batch_size, beam_width, time]\nif self.time_major:\n   translations = tf.transpose(translations, perm=[1, 2, 0])\n```\n\nSee [model.py](nmt/model.py), method *_build_decoder()* for more details.\n\n### Hyperparameters\n\nThere are several hyperparameters that can lead to additional\nperformances. Here, we list some based on our own experience [ Disclaimers:\nothers might not agree on things we wrote! ].\n\n***Optimizer***: while Adam can lead to reasonable results for \"unfamiliar\"\narchitectures, SGD with scheduling will generally lead to better performance if\nyou can train with SGD.\n\n***Attention***: Bahdanau-style attention often requires bidirectionality on the\nencoder side to work well; whereas Luong-style attention tends to work well for\ndifferent settings. For this tutorial code, we recommend using the two improved\nvariants of Luong & Bahdanau-style attentions: *scaled_luong* & *normed\nbahdanau*.\n\n### Multi-GPU training\n\nTraining a NMT model may take several days. Placing different RNN layers on\ndifferent GPUs can improve the training speed. Here\u2019s an example to create\nRNN layers on multiple GPUs.\n\n``` python\ncells = []\nfor i in range(num_layers):\n  cells.append(tf.contrib.rnn.DeviceWrapper(\n      tf.contrib.rnn.LSTMCell(num_units),\n      \"/gpu:%d\" % (num_layers % num_gpus)))\ncell = tf.contrib.rnn.MultiRNNCell(cells)\n```\n\nIn addition, we need to enable the `colocate_gradients_with_ops` option in\n`tf.gradients` to parallelize the gradients computation.\n\nYou may notice the speed improvement of the attention based NMT model is very\nsmall as the number of GPUs increases. One major drawback of the standard\nattention architecture is using the top (final) layer\u2019s output to query\nattention at each time step. That means each decoding step must wait its\nprevious step completely finished; hence, we can\u2019t parallelize the decoding\nprocess by simply placing RNN layers on multiple GPUs.\n\nThe [GNMT attention architecture](https://arxiv.org/pdf/1609.08144.pdf)\nparallelizes the decoder's computation by using the bottom (first) layer\u2019s\noutput to query attention. Therefore, each decoding step can start as soon as\nits previous step's first layer and attention computation finished. We\nimplemented the architecture in\n[GNMTAttentionMultiCell](nmt/gnmt_model.py),\na subclass of *tf.contrib.rnn.MultiRNNCell*. Here\u2019s an example of how to create\na decoder cell with the *GNMTAttentionMultiCell*.\n\n``` python\ncells = []\nfor i in range(num_layers):\n  cells.append(tf.contrib.rnn.DeviceWrapper(\n      tf.contrib.rnn.LSTMCell(num_units),\n      \"/gpu:%d\" % (num_layers % num_gpus)))\nattention_cell = cells.pop(0)\nattention_cell = tf.contrib.seq2seq.AttentionWrapper(\n    attention_cell,\n    attention_mechanism,\n    attention_layer_size=None,  # don't add an additional dense layer.\n    output_attention=False,)\ncell = GNMTAttentionMultiCell(attention_cell, cells)\n```\n\n# Benchmarks\n\n## IWSLT English-Vietnamese\n\nTrain: 133K examples, vocab=vocab.(vi|en), train=train.(vi|en)\ndev=tst2012.(vi|en),\ntest=tst2013.(vi|en), [download script](nmt/scripts/download_iwslt15.sh).\n\n***Training details***. We train 2-layer LSTMs of 512 units with bidirectional\nencoder (i.e., 1 bidirectional layers for the encoder), embedding dim\nis 512. LuongAttention (scale=True) is used together with dropout keep_prob of\n0.8. All parameters are uniformly. We use SGD with learning rate 1.0 as follows:\ntrain for 12K steps (~ 12 epochs); after 8K steps, we start halving learning\nrate every 1K step.\n\n***Results***.\n\nBelow are the averaged results of 2 models\n([model 1](http://download.tensorflow.org/models/nmt/envi_model_1.zip),\n[model 2](http://download.tensorflow.org/models/nmt/envi_model_2.zip)).\\\nWe measure the translation quality in terms of BLEU scores [(Papineni et al., 2002)](http://www.aclweb.org/anthology/P02-1040.pdf).\n\nSystems | tst2012 (dev) | test2013 (test)\n--- | :---: | :---:\nNMT (greedy) | 23.2 | 25.5\nNMT (beam=10) | 23.8 | **26.1**\n[(Luong & Manning, 2015)](https://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf) | - | 23.3\n\n**Training Speed**: (0.37s step-time, 15.3K wps) on *K40m* & (0.17s step-time, 32.2K wps) on *TitanX*.\\\nHere, step-time means the time taken to run one mini-batch (of size 128). For wps, we count words on both the source and target.\n\n## WMT German-English\n\nTrain: 4.5M examples, vocab=vocab.bpe.32000.(de|en),\ntrain=train.tok.clean.bpe.32000.(de|en), dev=newstest2013.tok.bpe.32000.(de|en),\ntest=newstest2015.tok.bpe.32000.(de|en),\n[download script](nmt/scripts/wmt16_en_de.sh)\n\n***Training details***. Our training hyperparameters are similar to the\nEnglish-Vietnamese experiments except for the following details. The data is\nsplit into subword units using [BPE](https://github.com/rsennrich/subword-nmt)\n(32K operations). We train 4-layer LSTMs of 1024 units with bidirectional\nencoder (i.e., 2 bidirectional layers for the encoder), embedding dim\nis 1024. We train for 350K steps (~ 10 epochs); after 170K steps, we start\nhalving learning rate every 17K step.\n\n***Results***.\n\nThe first 2 rows are the averaged results of 2 models\n([model 1](http://download.tensorflow.org/models/nmt/deen_model_1.zip),\n[model 2](http://download.tensorflow.org/models/nmt/deen_model_2.zip)).\nResults in the third row is with GNMT attention\n([model](http://download.tensorflow.org/models/nmt/10122017/deen_gnmt_model_4_layer.zip))\n; trained with 4 GPUs.\n\nSystems | newstest2013 (dev) | newstest2015\n--- | :---: | :---:\nNMT (greedy) | 27.1 | 27.6\nNMT (beam=10) | 28.0 | 28.9\nNMT + GNMT attention (beam=10) | 29.0 | **29.9**\n[WMT SOTA](http://matrix.statmt.org/) | - | 29.3\n\nThese results show that our code builds strong baseline systems for NMT.\\\n(Note that WMT systems generally utilize a huge amount monolingual data which we currently do not.)\n\n**Training Speed**: (2.1s step-time, 3.4K wps) on *Nvidia K40m* & (0.7s step-time, 8.7K wps) on *Nvidia TitanX* for standard models.\\\nTo see the speed-ups with GNMT attention, we benchmark on *K40m* only:\n\nSystems | 1 gpu | 4 gpus | 8 gpus\n--- | :---: | :---: | :---:\nNMT (4 layers) | 2.2s, 3.4K | 1.9s, 3.9K | -\nNMT (8 layers) | 3.5s, 2.0K | - | 2.9s, 2.4K\nNMT + GNMT attention (4 layers) | 2.6s, 2.8K | 1.7s, 4.3K | -\nNMT + GNMT attention (8 layers) | 4.2s, 1.7K | - | 1.9s, 3.8K\n\nThese results show that without GNMT attention, the gains from using multiple gpus are minimal.\\\nWith GNMT attention, we obtain from 50%-100% speed-ups with multiple gpus.\n\n## WMT English-German &mdash; Full Comparison\nThe first 2 rows are our models with GNMT\nattention:\n[model 1 (4 layers)](http://download.tensorflow.org/models/nmt/10122017/ende_gnmt_model_4_layer.zip),\n[model 2 (8 layers)](http://download.tensorflow.org/models/nmt/10122017/ende_gnmt_model_8_layer.zip).\n\nSystems | newstest2014 | newstest2015\n--- | :---: | :---:\n*Ours* &mdash; NMT + GNMT attention (4 layers) | 23.7 | 26.5\n*Ours* &mdash; NMT + GNMT attention (8 layers) | 24.4 | **27.6**\n[WMT SOTA](http://matrix.statmt.org/) | 20.6 | 24.9\nOpenNMT [(Klein et al., 2017)](https://arxiv.org/abs/1701.02810) | 19.3 | -\ntf-seq2seq [(Britz et al., 2017)](https://arxiv.org/abs/1703.03906) | 22.2 | 25.2\nGNMT [(Wu et al., 2016)](https://research.google.com/pubs/pub45610.html) | **24.6** | -\n\nThe above results show our models are very competitive among models of similar architectures.\\\n[Note that OpenNMT uses smaller models and the current best result (as of this writing) is 28.4 obtained by the Transformer network [(Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) which has a significantly different architecture.]\n\n\n## Standard HParams\n\nWe have provided\n[a set of standard hparams](nmt/standard_hparams/)\nfor using pre-trained checkpoint for inference or training NMT architectures\nused in the Benchmark.\n\nWe will use the WMT16 German-English data, you can download the data by the\nfollowing command.\n\n```\nnmt/scripts/wmt16_en_de.sh /tmp/wmt16\n```\n\nHere is an example command for loading the pre-trained GNMT WMT German-English\ncheckpoint for inference.\n\n```\npython -m nmt.nmt \\\n    --src=de --tgt=en \\\n    --ckpt=/path/to/checkpoint/translate.ckpt \\\n    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \\\n    --out_dir=/tmp/deen_gnmt \\\n    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \\\n    --inference_input_file=/tmp/wmt16/newstest2014.tok.bpe.32000.de \\\n    --inference_output_file=/tmp/deen_gnmt/output_infer \\\n    --inference_ref_file=/tmp/wmt16/newstest2014.tok.bpe.32000.en\n```\n\nHere is an example command for training the GNMT WMT German-English model.\n\n```\npython -m nmt.nmt \\\n    --src=de --tgt=en \\\n    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \\\n    --out_dir=/tmp/deen_gnmt \\\n    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \\\n    --train_prefix=/tmp/wmt16/train.tok.clean.bpe.32000 \\\n    --dev_prefix=/tmp/wmt16/newstest2013.tok.bpe.32000 \\\n    --test_prefix=/tmp/wmt16/newstest2015.tok.bpe.32000\n```\n\n\n# Other resources\n\nFor deeper reading on Neural Machine Translation and sequence-to-sequence\nmodels, we highly recommend the following materials\nby\n[Luong, Cho, Manning, (2016)](https://sites.google.com/site/acl16nmt/);\n[Luong, (2016)](https://github.com/lmthang/thesis);\nand [Neubig, (2017)](https://arxiv.org/abs/1703.01619).\n\nThere's a wide variety of tools for building seq2seq models, so we pick one per\nlanguage:\\\nStanford NMT\n[https://nlp.stanford.edu/projects/nmt/](https://nlp.stanford.edu/projects/nmt/)\n*[Matlab]* \\\ntf-seq2seq\n[https://github.com/google/seq2seq](https://github.com/google/seq2seq)\n*[TensorFlow]* \\\nNemantus\n[https://github.com/rsennrich/nematus](https://github.com/rsennrich/nematus)\n*[Theano]* \\\nOpenNMT [http://opennmt.net/](http://opennmt.net/) *[Torch]*\\\nOpenNMT-py [https://github.com/OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py) *[PyTorch]*\n\n\n\n# Acknowledgment\nWe would like to thank Denny Britz, Anna Goldie, Derek Murray, and Cinjon Resnick for their work bringing new features to TensorFlow and the seq2seq library. Additional thanks go to Lukasz Kaiser for the initial help on the seq2seq codebase; Quoc Le for the suggestion to replicate GNMT; Yonghui Wu and Zhifeng Chen for details on the GNMT systems; as well as the Google Brain team for their support and feedback!\n\n# References\n\n-  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua\n   Bengio. 2015.[ Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf). ICLR.\n-  Minh-Thang Luong, Hieu Pham, and Christopher D\n   Manning. 2015.[ Effective approaches to attention-based neural machine translation](https://arxiv.org/pdf/1508.04025.pdf). EMNLP.\n-  Ilya Sutskever, Oriol Vinyals, and Quoc\n   V. Le. 2014.[ Sequence to sequence learning with neural networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf). NIPS.\n\n# BibTex\n\n```\n@article{luong17,\n  author  = {Minh{-}Thang Luong and Eugene Brevdo and Rui Zhao},\n  title   = {Neural Machine Translation (seq2seq) Tutorial},\n  journal = {https://github.com/tensorflow/nmt},\n  year    = {2017},\n}\n```\n", "release_dates": []}, {"name": "oss-fuzz", "description": "OSS-Fuzz - continuous fuzzing for open source software.", "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# OSS-Fuzz: Continuous Fuzzing for Open Source Software\n\n[Fuzz testing] is a well-known technique for uncovering programming errors in\nsoftware. Many of these detectable errors, like [buffer overflow], can have\nserious security implications. Google has found [thousands] of security\nvulnerabilities and stability bugs by deploying [guided in-process fuzzing of\nChrome components], and we now want to share that service with the open source\ncommunity.\n\n[Fuzz testing]: https://en.wikipedia.org/wiki/Fuzz_testing\n[buffer overflow]: https://en.wikipedia.org/wiki/Buffer_overflow\n[thousands]: https://bugs.chromium.org/p/chromium/issues/list?q=label%3AStability-LibFuzzer%2CStability-AFL%20-status%3ADuplicate%2CWontFix&can=1\n[guided in-process fuzzing of Chrome components]: https://security.googleblog.com/2016/08/guided-in-process-fuzzing-of-chrome.html\n\nIn cooperation with the [Core Infrastructure Initiative] and the [OpenSSF],\nOSS-Fuzz aims to make common open source software more secure and stable by\ncombining modern fuzzing techniques with scalable, distributed execution.\nProjects that do not qualify for OSS-Fuzz (e.g. closed source) can run their own\ninstances of [ClusterFuzz] or [ClusterFuzzLite].\n\n[Core Infrastructure Initiative]: https://www.coreinfrastructure.org/\n[OpenSSF]: https://www.openssf.org/\n\nWe support the [libFuzzer], [AFL++], and [Honggfuzz] fuzzing engines in\ncombination with [Sanitizers], as well as [ClusterFuzz], a distributed fuzzer\nexecution environment and reporting tool.\n\n[libFuzzer]: https://llvm.org/docs/LibFuzzer.html\n[AFL++]: https://github.com/AFLplusplus/AFLplusplus\n[Honggfuzz]: https://github.com/google/honggfuzz\n[Sanitizers]: https://github.com/google/sanitizers\n[ClusterFuzz]: https://github.com/google/clusterfuzz\n[ClusterFuzzLite]: https://google.github.io/clusterfuzzlite/\n\nCurrently, OSS-Fuzz supports C/C++, Rust, Go, Python and Java/JVM code. Other languages\nsupported by [LLVM] may work too. OSS-Fuzz supports fuzzing x86_64 and i386\nbuilds.\n\n[LLVM]: https://llvm.org\n\n## Overview\n![OSS-Fuzz process diagram](docs/images/process.png)\n\n## Documentation\nRead our [detailed documentation] to learn how to use OSS-Fuzz.\n\n[detailed documentation]: https://google.github.io/oss-fuzz\n\n## Trophies\nAs of July 2022, OSS-Fuzz has found over [40,500] bugs in [650] open source\nprojects.\n\n[40,500]: https://bugs.chromium.org/p/oss-fuzz/issues/list?q=-status%3AWontFix%2CDuplicate%20-component%3AInfra&can=1\n[650]: https://github.com/google/oss-fuzz/tree/master/projects\n\n## Blog posts\n* 2016-12-01 - [Announcing OSS-Fuzz: Continuous fuzzing for open source software]\n* 2017-05-08 - [OSS-Fuzz: Five months later, and rewarding projects]\n* 2018-11-06 - [A New Chapter for OSS-Fuzz]\n* 2020-10-09 - [Fuzzing internships for Open Source Software]\n* 2020-12-07 - [Improving open source security during the Google summer internship program]\n\n[Announcing OSS-Fuzz: Continuous fuzzing for open source software]: https://opensource.googleblog.com/2016/12/announcing-oss-fuzz-continuous-fuzzing.html\n[OSS-Fuzz: Five months later, and rewarding projects]: https://opensource.googleblog.com/2017/05/oss-fuzz-five-months-later-and.html\n[A New Chapter for OSS-Fuzz]: https://security.googleblog.com/2018/11/a-new-chapter-for-oss-fuzz.html\n[Fuzzing internships for Open Source Software]: https://security.googleblog.com/2020/10/fuzzing-internships-for-open-source.html\n[Improving open source security during the Google summer internship program]: https://security.googleblog.com/2020/12/improving-open-source-security-during.html\n", "release_dates": []}, {"name": "playground", "description": "Play with neural networks!", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Deep playground\n\nDeep playground is an interactive visualization of neural networks, written in\nTypeScript using d3.js. We use GitHub issues for tracking new requests and bugs.\nYour feedback is highly appreciated!\n\n**If you'd like to contribute, be sure to review the [contribution guidelines](CONTRIBUTING.md).**\n\n## Development\n\nTo run the visualization locally, run:\n- `npm i` to install dependencies\n- `npm run build` to compile the app and place it in the `dist/` directory\n- `npm run serve` to serve from the `dist/` directory and open a page on your browser.\n\nFor a fast edit-refresh cycle when developing run `npm run serve-watch`.\nThis will start an http server and automatically re-compile the TypeScript,\nHTML and CSS files whenever they change.\n\n## For owners\nTo push to production: `git subtree push --prefix dist origin gh-pages`.\n\nThis is not an official Google product.\n", "release_dates": []}, {"name": "privacy", "description": "Library for training machine learning models with privacy for training data", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Privacy\n\nThis repository contains the source code for TensorFlow Privacy, a Python\nlibrary that includes implementations of TensorFlow optimizers for training\nmachine learning models with differential privacy. The library comes with\ntutorials and analysis tools for computing the privacy guarantees provided.\n\nThe TensorFlow Privacy library is under continual development, always welcoming\ncontributions. In particular, we always welcome help towards resolving the\nissues currently open.\n\n## Latest Updates\n\n2024-02-14: As of version 0.9.0, the TensorFlow Privacy github repository will\nbe published as two separate PyPI packages. The first will inherit the name\ntensorflow-privacy and contain the parts related to training of DP models. The\nsecond, tensorflow-empirical-privacy, will contain the parts related to testing\nfor empirical privacy.\n\n2023-02-21: A new implementation of efficient per-example gradient clipping is\nnow available for\n[DP keras models](https://github.com/tensorflow/privacy/tree/master/tensorflow_privacy/privacy/keras_models)\nconsisting only of Dense and Embedding layers. The models use the fast gradient\ncalculation results of [this paper](https://arxiv.org/abs/1510.01799). The\nimplementation should allow for doing DP training without any meaningful memory\nor runtime overhead. It also removes the need for tuning the number of\nmicrobatches as it clips the gradient with respect to each example.\n\n## Setting up TensorFlow Privacy\n\n### Dependencies\n\nThis library uses [TensorFlow](https://www.tensorflow.org/) to define machine\nlearning models. Therefore, installing TensorFlow (>= 1.14) is a pre-requisite.\nYou can find instructions [here](https://www.tensorflow.org/install/). For\nbetter performance, it is also recommended to install TensorFlow with GPU\nsupport (detailed instructions on how to do this are available in the TensorFlow\ninstallation documentation).\n\n### Installing TensorFlow Privacy\n\nIf you only want to use TensorFlow Privacy as a library, you can simply execute\n\n`pip install tensorflow-privacy`\n\nOtherwise, you can clone this GitHub repository into a directory of your choice:\n\n```\ngit clone https://github.com/tensorflow/privacy\n```\n\nYou can then install the local package in \"editable\" mode in order to add it to\nyour `PYTHONPATH`:\n\n```\ncd privacy\npip install -e .\n```\n\nIf you'd like to make contributions, we recommend first forking the repository\nand then cloning your fork rather than cloning this repository directly.\n\n## Contributing\n\nContributions are welcomed! Bug fixes and new features can be initiated through\nGitHub pull requests. To speed the code review process, we ask that:\n\n*   When making code contributions to TensorFlow Privacy, you follow the `PEP8\n    with two spaces` coding style (the same as the one used by TensorFlow) in\n    your pull requests. In most cases this can be done by running `autopep8 -i\n    --indent-size 2 <file>` on the files you have edited.\n\n*   You should also check your code with pylint and TensorFlow's pylint\n    [configuration file](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/ci_build/pylintrc)\n    by running `pylint --rcfile=/path/to/the/tf/rcfile <edited file.py>`.\n\n*   When making your first pull request, you\n    [sign the Google CLA](https://cla.developers.google.com/clas)\n\n*   We do not accept pull requests that add git submodules because of\n    [the problems that arise when maintaining git submodules](https://medium.com/@porteneuve/mastering-git-submodules-34c65e940407)\n\n## Tutorials directory\n\nTo help you get started with the functionalities provided by this library, we\nprovide a detailed walkthrough [here](tutorials/walkthrough/README.md) that will\nteach you how to wrap existing optimizers (e.g., SGD, Adam, ...) into their\ndifferentially private counterparts using TensorFlow (TF) Privacy. You will also\nlearn how to tune the parameters introduced by differentially private\noptimization and how to measure the privacy guarantees provided using analysis\ntools included in TF Privacy.\n\nIn addition, the `tutorials/` folder comes with scripts demonstrating how to use\nthe library features. The list of tutorials is described in the README included\nin the tutorials directory.\n\nNOTE: the tutorials are maintained carefully. However, they are not considered\npart of the API and they can change at any time without warning. You should not\nwrite 3rd party code that imports the tutorials and expect that the interface\nwill not break.\n\n## Research directory\n\nThis folder contains code to reproduce results from research papers related to\nprivacy in machine learning. It is not maintained as carefully as the tutorials\ndirectory, but rather intended as a convenient archive.\n\n## TensorFlow 2.x\n\nTensorFlow Privacy now works with TensorFlow 2! You can use the new Keras-based\nestimators found in\n`privacy/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras.py`.\n\nFor this to work with `tf.keras.Model` and `tf.estimator.Estimator`, however,\nyou need to install TensorFlow 2.4 or later.\n\n## Remarks\n\nThe content of this repository supersedes the following existing folder in the\ntensorflow/models\n[repository](https://github.com/tensorflow/models/tree/master/research/differential_privacy)\n\n## Contacts\n\nIf you have any questions that cannot be addressed by raising an issue, feel\nfree to contact:\n\n*   Galen Andrew (@galenmandrew)\n*   Steve Chien (@schien1729)\n*   Nicolas Papernot (@npapernot)\n\n## Copyright\n\nCopyright 2019 - Google LLC\n", "release_dates": ["2024-02-14T19:18:00Z", "2023-10-10T20:00:05Z", "2023-10-05T19:04:32Z", "2023-06-28T19:58:57Z", "2023-03-13T18:22:34Z", "2022-08-31T20:56:52Z", "2022-07-27T21:35:20Z", "2022-07-20T20:56:35Z", "2022-02-22T20:37:34Z", "2021-07-14T18:55:00Z", "2020-09-03T01:13:47Z", "2020-05-01T23:44:28Z", "2020-03-20T18:04:59Z", "2019-10-31T02:09:08Z", "2019-10-22T20:41:45Z", "2019-10-15T21:10:36Z", "2019-10-02T22:07:30Z", "2019-08-23T16:22:10Z"]}, {"name": "probability", "description": "Probabilistic reasoning and statistical analysis in TensorFlow", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Probability\n\nTensorFlow Probability is a library for probabilistic reasoning and statistical\nanalysis in TensorFlow. As part of the TensorFlow ecosystem, TensorFlow\nProbability provides integration of probabilistic methods with deep networks,\ngradient-based inference via automatic differentiation, and scalability to\nlarge datasets and models via hardware acceleration (e.g., GPUs) and distributed\ncomputation.\n\n__TFP also works as \"Tensor-friendly Probability\" in pure JAX!__:\n`from tensorflow_probability.substrates import jax as tfp` --\nLearn more [here](https://www.tensorflow.org/probability/examples/TensorFlow_Probability_on_JAX).\n\nOur probabilistic machine learning tools are structured as follows.\n\n__Layer 0: TensorFlow.__ Numerical operations. In particular, the LinearOperator\nclass enables matrix-free implementations that can exploit special structure\n(diagonal, low-rank, etc.) for efficient computation. It is built and maintained\nby the TensorFlow Probability team and is now part of\n[`tf.linalg`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/ops/linalg)\nin core TF.\n\n__Layer 1: Statistical Building Blocks__\n\n* Distributions ([`tfp.distributions`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/distributions)):\n  A large collection of probability\n  distributions and related statistics with batch and\n  [broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n  semantics. See the\n  [Distributions Tutorial](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Distributions_Tutorial.ipynb).\n* Bijectors ([`tfp.bijectors`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/bijectors)):\n  Reversible and composable transformations of random variables. Bijectors\n  provide a rich class of transformed distributions, from classical examples\n  like the\n  [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution)\n  to sophisticated deep learning models such as\n  [masked autoregressive flows](https://arxiv.org/abs/1705.07057).\n\n__Layer 2: Model Building__\n\n* Joint Distributions (e.g., [`tfp.distributions.JointDistributionSequential`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/distributions/joint_distribution_sequential.py)):\n    Joint distributions over one or more possibly-interdependent distributions.\n    For an introduction to modeling with TFP's `JointDistribution`s, check out\n    [this colab](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Modeling_with_JointDistribution.ipynb)\n* Probabilistic Layers ([`tfp.layers`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/layers)):\n  Neural network layers with uncertainty over the functions they represent,\n  extending TensorFlow Layers.\n\n__Layer 3: Probabilistic Inference__\n\n* Markov chain Monte Carlo ([`tfp.mcmc`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/mcmc)):\n  Algorithms for approximating integrals via sampling. Includes\n  [Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo),\n  random-walk Metropolis-Hastings, and the ability to build custom transition\n  kernels.\n* Variational Inference ([`tfp.vi`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/vi)):\n  Algorithms for approximating integrals via optimization.\n* Optimizers ([`tfp.optimizer`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/optimizer)):\n  Stochastic optimization methods, extending TensorFlow Optimizers. Includes\n  [Stochastic Gradient Langevin Dynamics](http://www.icml-2011.org/papers/398_icmlpaper.pdf).\n* Monte Carlo ([`tfp.monte_carlo`](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/python/monte_carlo)):\n  Tools for computing Monte Carlo expectations.\n\nTensorFlow Probability is under active development. Interfaces may change at any\ntime.\n\n## Examples\n\nSee [`tensorflow_probability/examples/`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/)\nfor end-to-end examples. It includes tutorial notebooks such as:\n\n* [Linear Mixed Effects Models](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Linear_Mixed_Effects_Models.ipynb).\n  A hierarchical linear model for sharing statistical strength across examples.\n* [Eight Schools](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Eight_Schools.ipynb).\n  A hierarchical normal model for exchangeable treatment effects.\n* [Hierarchical Linear Models](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/HLM_TFP_R_Stan.ipynb).\n  Hierarchical linear models compared among TensorFlow Probability, R, and Stan.\n* [Bayesian Gaussian Mixture Models](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Bayesian_Gaussian_Mixture_Model.ipynb).\n  Clustering with a probabilistic generative model.\n* [Probabilistic Principal Components Analysis](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_PCA.ipynb).\n  Dimensionality reduction with latent variables.\n* [Gaussian Copulas](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Gaussian_Copula.ipynb).\n  Probability distributions for capturing dependence across random variables.\n* [TensorFlow Distributions: A Gentle Introduction](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Distributions_Tutorial.ipynb).\n  Introduction to TensorFlow Distributions.\n* [Understanding TensorFlow Distributions Shapes](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Understanding_TensorFlow_Distributions_Shapes.ipynb).\n  How to distinguish between samples, batches, and events for arbitrarily shaped\n  probabilistic computations.\n* [TensorFlow Probability Case Study: Covariance Estimation](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Probability_Case_Study_Covariance_Estimation.ipynb).\n  A user's case study in applying TensorFlow Probability to estimate covariances.\n\nIt also includes example scripts such as:\n\n  Representation learning with a latent code and variational inference.\n* [Vector-Quantized Autoencoder](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/vq_vae.py).\n  Discrete representation learning with vector quantization.\n* [Disentangled Sequential Variational Autoencoder](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/disentangled_vae.py)\n  Disentangled representation learning over sequences with variational inference.\n* [Bayesian Neural Networks](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/bayesian_neural_network.py).\n  Neural networks with uncertainty over their weights.\n* [Bayesian Logistic Regression](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/logistic_regression.py).\n  Bayesian inference for binary classification.\n\n## Installation\n\nFor additional details on installing TensorFlow, guidance installing\nprerequisites, and (optionally) setting up virtual environments, see the\n[TensorFlow installation guide](https://www.tensorflow.org/install).\n\n### Stable Builds\n\nTo install the latest stable version, run the following:\n\n```shell\n# Notes:\n\n# - The `--upgrade` flag ensures you'll get the latest version.\n# - The `--user` flag ensures the packages are installed to your user directory\n#   rather than the system directory.\n# - TensorFlow 2 packages require a pip >= 19.0\npython -m pip install --upgrade --user pip\npython -m pip install --upgrade --user tensorflow tensorflow_probability\n```\n\nFor CPU-only usage (and a smaller install), install with `tensorflow-cpu`.\n\nTo use a pre-2.0 version of TensorFlow, run:\n\n```shell\npython -m pip install --upgrade --user \"tensorflow<2\" \"tensorflow_probability<0.9\"\n```\n\nNote: Since [TensorFlow](https://www.tensorflow.org/install) is *not* included\nas a dependency of the TensorFlow Probability package (in `setup.py`), you must\nexplicitly install the TensorFlow package (`tensorflow` or `tensorflow-cpu`).\nThis allows us to maintain one package instead of separate packages for CPU and\nGPU-enabled TensorFlow. See the\n[TFP release notes](https://github.com/tensorflow/probability/releases) for more\ndetails about dependencies between TensorFlow and TensorFlow Probability.\n\n\n### Nightly Builds\n\nThere are also nightly builds of TensorFlow Probability under the pip package\n`tfp-nightly`, which depends on one of `tf-nightly` or `tf-nightly-cpu`.\nNightly builds include newer features, but may be less stable than the\nversioned releases. Both stable and nightly docs are available\n[here](https://www.tensorflow.org/probability/api_docs/python/tfp?version=nightly).\n\n```shell\npython -m pip install --upgrade --user tf-nightly tfp-nightly\n```\n\n### Installing from Source\n\nYou can also install from source. This requires the [Bazel](\nhttps://bazel.build/) build system. It is highly recommended that you install\nthe nightly build of TensorFlow (`tf-nightly`) before trying to build\nTensorFlow Probability from source. The most recent version of Bazel that TFP\ncurrently supports is 6.4.0; support for 7.0.0+ is WIP.\n\n```shell\n# sudo apt-get install bazel git python-pip  # Ubuntu; others, see above links.\npython -m pip install --upgrade --user tf-nightly\ngit clone https://github.com/tensorflow/probability.git\ncd probability\nbazel build --copt=-O3 --copt=-march=native :pip_pkg\nPKGDIR=$(mktemp -d)\n./bazel-bin/pip_pkg $PKGDIR\npython -m pip install --upgrade --user $PKGDIR/*.whl\n```\n\n## Community\n\nAs part of TensorFlow, we're committed to fostering an open and welcoming\nenvironment.\n\n* [Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow): Ask\n  or answer technical questions.\n* [GitHub](https://github.com/tensorflow/probability/issues): Report bugs or\n  make feature requests.\n* [TensorFlow Blog](https://blog.tensorflow.org/): Stay up to date on content\n  from the TensorFlow team and best articles from the community.\n* [Youtube Channel](http://youtube.com/tensorflow/): Follow TensorFlow shows.\n* [tfprobability@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfprobability):\n  Open mailing list for discussion and questions.\n\nSee the [TensorFlow Community](https://www.tensorflow.org/community/) page for\nmore details. Check out our latest publicity here:\n\n+ [Coffee with a Googler: Probabilistic Machine Learning in TensorFlow](\n  https://www.youtube.com/watch?v=BjUkL8DFH5Q)\n+ [Introducing TensorFlow Probability](\n  https://medium.com/tensorflow/introducing-tensorflow-probability-dca4c304e245)\n\n## Contributing\n\nWe're eager to collaborate with you! See [`CONTRIBUTING.md`](CONTRIBUTING.md)\nfor a guide on how to contribute. This project adheres to TensorFlow's\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.\n\n## References\n\nIf you use TensorFlow Probability in a paper, please cite:\n\n+ _TensorFlow Distributions._ Joshua V. Dillon, Ian Langmore, Dustin Tran,\nEugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt\nHoffman, Rif A. Saurous.\n[arXiv preprint arXiv:1711.10604, 2017](https://arxiv.org/abs/1711.10604).\n\n(We're aware there's a lot more to TensorFlow Probability than Distributions, but the Distributions paper lays out our vision and is a fine thing to cite for now.)\n", "release_dates": ["2023-11-20T23:32:05Z", "2023-10-23T16:37:21Z", "2023-10-02T16:03:27Z", "2023-08-04T17:50:20Z", "2023-05-08T20:13:58Z", "2022-12-06T22:34:10Z", "2022-09-12T15:46:51Z", "2022-06-07T18:01:34Z", "2022-02-14T17:24:33Z", "2021-11-18T15:49:59Z", "2021-09-30T23:00:59Z", "2021-09-21T04:38:21Z", "2021-06-18T21:06:22Z", "2021-05-24T14:04:59Z", "2021-04-19T23:03:58Z", "2020-12-29T18:40:25Z", "2020-12-29T18:40:22Z", "2020-12-09T16:35:36Z", "2020-11-21T00:35:12Z", "2020-11-11T05:11:40Z", "2020-11-10T05:23:06Z", "2020-10-09T23:49:08Z", "2020-07-28T22:39:48Z", "2020-07-20T19:40:38Z", "2020-07-16T22:30:14Z", "2020-07-06T22:41:05Z", "2020-05-14T23:20:57Z", "2020-04-30T20:22:17Z", "2020-04-15T19:28:12Z", "2020-01-15T05:30:42Z"]}, {"name": "profiler", "description": "A profiling and performance analysis tool for TensorFlow", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Profiler\nThe profiler includes a suite of tools. These tools help you understand, debug and optimize TensorFlow programs to run on CPUs, GPUs and TPUs.\n\n## Demo\nFirst time user? Come and check out this [Colab Demo](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras).\n\n## Prerequisites\n* TensorFlow >= 2.2.0\n* TensorBoard >= 2.2.0\n* tensorboard-plugin-profile >= 2.2.0\n\nNote: The TensorFlow Profiler requires access to the Internet to load the [Google Chart library](https://developers.google.com/chart/interactive/docs/basic_load_libs#basic-library-loading).\nSome charts and tables may be missing if you run TensorBoard entirely offline on\nyour local machine, behind a corporate firewall, or in a datacenter.\n\nTo profile on a **single GPU** system, the following NVIDIA software must be installed on your system:\n\n1. NVIDIA GPU drivers and CUDA Toolkit:\n    * CUDA 10.1 requires 418.x and higher.\n2. Ensure that CUPTI 10.1 exists on the path.\n\n   ```shell\n   $ /sbin/ldconfig -N -v $(sed 's/:/ /g' <<< $LD_LIBRARY_PATH) | grep libcupti\n   ```\n\n   If you don't see `libcupti.so.10.1` on the path, prepend its installation directory to the $LD_LIBRARY_PATH environmental variable:\n\n   ```shell\n   $ export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\n   ```\n   Run the ldconfig command above again to verify that the CUPTI 10.1 library is found.\n\n   If this doesn't work, try:\n   ```shell\n   $ sudo apt-get install libcupti-dev\n   ```\n\nTo profile a system with **multiple GPUs**, see this [guide](docs/profile_multi_gpu.md) for details.\n\nTo profile multi-worker GPU configurations, profile individual workers independently.\n\nTo profile cloud TPUs, you must have access to Google Cloud TPUs.\n\n## Quick Start\nInstall nightly version of profiler by downloading and running the `install_and_run.py` script from this directory.\n```\n$ git clone https://github.com/tensorflow/profiler.git profiler\n$ mkdir profile_env\n$ python3 profiler/install_and_run.py --envdir=profile_env --logdir=profiler/demo\n```\nGo to `localhost:6006/#profile` of your browser, you should now see the demo overview page show up.\n![Overview Page](docs/images/overview_page.png)\nCongratulations! You're now ready to capture a profile.\n\n## Next Steps\n* GPU Profiling Guide:  https://tensorflow.org/guide/profiler\n* Cloud TPU Profiling Guide: https://cloud.google.com/tpu/docs/cloud-tpu-tools\n* Colab Tutorial: https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras\n", "release_dates": []}, {"name": "profiler-ui", "description": "[Deprecated] The TensorFlow Profiler (TFProf) UI provides a visual interface for profiling TensorFlow models.", "language": "HTML", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Note: This project has been deprecated in favor of TensorBoard\n\nhttps://www.tensorflow.org/tensorboard/r2/tensorboard_profiling_keras\n\n# TensorFlow Profiler UI\n\nThe TensorFlow Profiler (TFProf) UI provides a visual interface for profiling TensorFlow models.\n\n# Installation\n1) Install Python dependencies.\n   ```s\n   pip install --user -r requirements.txt\n   ```\n2) Install [pprof](https://github.com/google/pprof#building-pprof).\n3) Create a profile context file using the [tf.contrib.tfprof.ProfileContext](https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/python/profiler/profile_context.py#L110-L148) class.\n3) Start the UI.\n   ```s\n   python ui.py --profile_context_path=/path/to/your/profile.context\n   ```\n\n# Learn more\nYou can learn more about the TensorFlow Profiler's Python API and CLI [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/README.md#quick-start).\n\n# Screenshot\n<img src=\"docs/images/preview.png\">\n\n# Browser support\nCurrently only [Chrome](https://www.google.com/chrome/) is supported.\n\n# Contributing\nPlease see [our contributor's guide](/CONTRIBUTING.md)\n\n# Feature requests\nWant ideas for ways to contribute to the TensorFlow Profiler UI? Here are some requested features: \n- Support multiple profile contexts at once ([#11](https://github.com/tensorflow/profiler-ui/issues/11))\n", "release_dates": ["2018-07-04T18:19:38Z", "2018-05-17T18:28:34Z", "2018-05-14T19:58:38Z", "2018-04-27T00:56:38Z"]}, {"name": "quantum", "description": "Hybrid Quantum-Classical Machine Learning in TensorFlow", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![TensorFlow Quantum](./docs/images/logo/tf_quantum_circle.jpg)\n\n---\n\n[TensorFlow Quantum](https://www.tensorflow.org/quantum) (TFQ) is a Python\nframework for hybrid quantum-classical machine learning that is primarily\nfocused on modeling quantum data. TFQ is an application framework developed to\nallow quantum algorithms researchers and machine learning applications\nresearchers to explore computing workflows that leverage Google\u2019s quantum\ncomputing offerings, all from within TensorFlow.\n\n\n## Motivation\n\nQuantum computing at Google has hit an exciting milestone with the achievement\nof [Quantum Supremacy](https://www.nature.com/articles/s41586-019-1666-5).\nIn the wake of this demonstration, Google is now turning its attention to\ndeveloping and implementing new algorithms to run on its Quantum Computer\nthat have real world [applications](https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html).\n\nTo provide users with the tools they need to program and simulate a quantum\ncomputer, Google is working on [Cirq](https://github.com/quantumlib/Cirq). Cirq\nis designed for quantum computing researchers who are interested in running and\ndesigning algorithms that leverage existing (imperfect) quantum computers.\n\nTensorFlow Quantum provides users with the tools they need to interleave quantum\nalgorithms and logic designed in Cirq with the powerful and performant ML tools\nfrom TensorFlow. With this connection we hope to unlock new and exciting paths\nfor Quantum Computing research that would not have otherwise been possible.\n\n\n## Installation\n\nSee the [installation instructions](https://github.com/tensorflow/quantum/blob/master/docs/install.md).\n\n\n## Examples\n\nAll of our examples can be found here in the form of\n[Python notebook tutorials](https://github.com/tensorflow/quantum/tree/master/docs/tutorials)\n\n\n## Report issues\n\nReport bugs or feature requests using the\n[TensorFlow Quantum issue tracker](https://github.com/tensorflow/quantum/issues).\n\nWe also have a [Stack Overflow tag](https://stackoverflow.com/questions/tagged/tensorflow-quantum)\nfor more general TFQ related discussions.\n\nIn the meantime check out the [install instructions](./docs/install.md) to get\nthe experimental code running!\n\n\n## Contributing\n\nWe are eager to collaborate with you! TensorFlow Quantum is still a very young code base,\nif you have ideas for features that you would like added feel free to check out our\n[Contributor Guidelines](https://github.com/tensorflow/quantum/blob/master/CONTRIBUTING.md)\nto get started.\n\n\n## References\n\nIf you use TensorFlow Quantum in your research, please cite:\n\nTensorFlow Quantum: A Software Framework for Quantum Machine Learning\n[arXiv:2003.02989, 2020](https://arxiv.org/abs/2003.02989).\n", "release_dates": ["2023-01-31T19:32:07Z", "2022-02-03T17:48:58Z", "2022-02-03T09:02:12Z", "2021-06-05T00:42:37Z", "2021-05-17T00:20:12Z", "2020-10-06T20:28:48Z", "2020-08-07T21:14:56Z", "2020-05-11T21:07:41Z", "2020-02-28T19:06:17Z"]}, {"name": "ranking", "description": "Learning to Rank in TensorFlow", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Ranking\n\nTensorFlow Ranking is a library for Learning-to-Rank (LTR) techniques on the\nTensorFlow platform. It contains the following components:\n\n*   Commonly used loss functions including pointwise, pairwise, and listwise\n    losses.\n*   Commonly used ranking metrics like\n    [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)\n    and\n    [Normalized Discounted Cumulative Gain (NDCG)](https://en.wikipedia.org/wiki/Discounted_cumulative_gain).\n*   [Multi-item (also known as groupwise) scoring functions](https://arxiv.org/abs/1811.04415).\n*   [LambdaLoss](https://ai.google/research/pubs/pub47258) implementation for\n    direct ranking metric optimization.\n*   [Unbiased Learning-to-Rank](http://www.cs.cornell.edu/people/tj/publications/joachims_etal_17a.pdf)\n    from biased feedback data.\n\nWe envision that this library will provide a convenient open platform for\nhosting and advancing state-of-the-art ranking models based on deep learning\ntechniques, and thus facilitate both academic research and industrial\napplications.\n\n## Tutorial Slides\n\nTF-Ranking was presented at premier conferences in Information Retrieval,\n[SIGIR 2019](https://sigir.org/sigir2019/program/tutorials/) and\n[ICTIR 2019](http://ictir2019.org/program/#tutorials)! The slides are available\n[here](http://bendersky.github.io/res/TF-Ranking-ICTIR-2019.pdf).\n\n## Demos\n\nWe provide a demo, with no installation required, to get started on using\nTF-Ranking. This demo runs on a\n[colaboratory notebook](https://research.google.com/colaboratory/faq.html), an\ninteractive Python environment. Using sparse features and embeddings in\nTF-Ranking\n[![Run in Google Colab](https://www.tensorflow.org/images/colab_logo_32px.png)](https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb).\nThis demo demonstrates how to:\n\n*   Use sparse/embedding features\n*   Process data in TFRecord format\n*   Tensorboard integration in colab notebook, for Estimator API\n\nAlso see [Running Scripts](#running-scripts) for executable scripts.\n\n## Linux Installation\n\n### Stable Builds\n\nTo install the latest version from\n[PyPI](https://pypi.org/project/tensorflow-ranking/), run the following:\n\n```shell\n# Installing with the `--upgrade` flag ensures you'll get the latest version.\npip install --user --upgrade tensorflow_ranking\n```\n\nTo force a Python 3-specific install, replace `pip` with `pip3` in the above\ncommands. For additional installation help, guidance installing prerequisites,\nand (optionally) setting up virtual environments, see the\n[TensorFlow installation guide](https://www.tensorflow.org/install).\n\nNote: Since TensorFlow is now included as a dependency of the TensorFlow Ranking\npackage (in `setup.py`). If you wish to use different versions of TensorFlow\n(e.g., `tensorflow-gpu`), you may need to uninstall the existing verison and\nthen install your desired version:\n\n```shell\n$ pip uninstall tensorflow\n$ pip install tensorflow-gpu\n```\n\n### Installing from Source\n\n1.  To build TensorFlow Ranking locally, you will need to install:\n\n    *   [Bazel](https://docs.bazel.build/versions/master/install.html), an open\n        source build tool.\n\n        ```shell\n        $ sudo apt-get update && sudo apt-get install bazel\n        ```\n\n    *   [Pip](https://pypi.org/project/pip/), a Python package manager.\n\n        ```shell\n        $ sudo apt-get install python-pip\n        ```\n\n    *   [VirtualEnv](https://virtualenv.pypa.io/en/stable/installation/), a tool\n        to create isolated Python environments.\n\n        ```shell\n        $ pip install --user virtualenv\n        ```\n\n2.  Clone the TensorFlow Ranking repository.\n\n    ```shell\n    $ git clone https://github.com/tensorflow/ranking.git\n    ```\n\n3.  Build TensorFlow Ranking wheel file and store them in `/tmp/ranking_pip`\n    folder.\n\n    ```shell\n    $ cd ranking  # The folder which was cloned in Step 2.\n    $ bazel build //tensorflow_ranking/tools/pip_package:build_pip_package\n    $ bazel-bin/tensorflow_ranking/tools/pip_package/build_pip_package /tmp/ranking_pip\n    ```\n\n4.  Install the wheel package using pip. Test in virtualenv, to avoid clash with\n    any system dependencies.\n\n    ```shell\n    $ ~/.local/bin/virtualenv -p python3 /tmp/tfr\n    $ source /tmp/tfr/bin/activate\n    (tfr) $ pip install /tmp/ranking_pip/tensorflow_ranking*.whl\n    ```\n\n    In some cases, you may want to install a specific version of tensorflow,\n    e.g., `tensorflow-gpu` or `tensorflow==2.0.0`. To do so you can either\n\n    ```shell\n    (tfr) $ pip uninstall tensorflow\n    (tfr) $ pip install tensorflow==2.0.0\n    ```\n\n    or\n\n    ```shell\n    (tfr) $ pip uninstall tensorflow\n    (tfr) $ pip install tensorflow-gpu\n    ```\n\n5.  Run all TensorFlow Ranking tests.\n\n    ```shell\n    (tfr) $ bazel test //tensorflow_ranking/...\n    ```\n\n6.  Invoke TensorFlow Ranking package in python (within virtualenv).\n\n    ```shell\n    (tfr) $ python -c \"import tensorflow_ranking\"\n    ```\n\n## Running Scripts\n\nFor ease of experimentation, we also provide\n[a TFRecord example](https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/tf_ranking_tfrecord.py)\nand\n[a LIBSVM example](https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/tf_ranking_libsvm.py)\nin the form of executable scripts. This is particularly useful for\nhyperparameter tuning, where the hyperparameters are supplied as flags to the\nscript.\n\n### TFRecord Example\n\n1.  Set up the data and directory.\n\n    ```shell\n    MODEL_DIR=/tmp/tf_record_model && \\\n    TRAIN=tensorflow_ranking/examples/data/train_elwc.tfrecord && \\\n    EVAL=tensorflow_ranking/examples/data/eval_elwc.tfrecord && \\\n    VOCAB=tensorflow_ranking/examples/data/vocab.txt\n    ```\n\n2.  Build and run.\n\n    ```shell\n    rm -rf $MODEL_DIR && \\\n    bazel build -c opt \\\n    tensorflow_ranking/examples/tf_ranking_tfrecord_py_binary && \\\n    ./bazel-bin/tensorflow_ranking/examples/tf_ranking_tfrecord_py_binary \\\n    --train_path=$TRAIN \\\n    --eval_path=$EVAL \\\n    --vocab_path=$VOCAB \\\n    --model_dir=$MODEL_DIR \\\n    --data_format=example_list_with_context\n    ```\n\n### LIBSVM Example\n\n1.  Set up the data and directory.\n\n    ```shell\n    OUTPUT_DIR=/tmp/libsvm && \\\n    TRAIN=tensorflow_ranking/examples/data/train.txt && \\\n    VALI=tensorflow_ranking/examples/data/vali.txt && \\\n    TEST=tensorflow_ranking/examples/data/test.txt\n    ```\n\n2.  Build and run.\n\n    ```shell\n    rm -rf $OUTPUT_DIR && \\\n    bazel build -c opt \\\n    tensorflow_ranking/examples/tf_ranking_libsvm_py_binary && \\\n    ./bazel-bin/tensorflow_ranking/examples/tf_ranking_libsvm_py_binary \\\n    --train_path=$TRAIN \\\n    --vali_path=$VALI \\\n    --test_path=$TEST \\\n    --output_dir=$OUTPUT_DIR \\\n    --num_features=136 \\\n    --num_train_steps=100\n    ```\n\n### TensorBoard\n\nThe training results such as loss and metrics can be visualized using\n[Tensorboard](https://github.com/tensorflow/tensorboard/blob/master/README.md).\n\n1.  (Optional) If you are working on remote server, set up port forwarding with\n    this command.\n\n    ```shell\n    $ ssh <remote-server> -L 8888:127.0.0.1:8888\n    ```\n\n2.  Install Tensorboard and invoke it with the following commands.\n\n    ```shell\n    (tfr) $ pip install tensorboard\n    (tfr) $ tensorboard --logdir $OUTPUT_DIR\n    ```\n\n### Jupyter Notebook\n\nAn example jupyter notebook is available in\n`tensorflow_ranking/examples/handling_sparse_features.ipynb`.\n\n1.  To run this notebook, first follow the steps in installation to set up\n    `virtualenv` environment with tensorflow_ranking package installed.\n\n2.  Install jupyter within virtualenv.\n\n    ```shell\n    (tfr) $ pip install jupyter\n    ```\n\n3.  Start a jupyter notebook instance on remote server.\n\n    ```shell\n    (tfr) $ jupyter notebook tensorflow_ranking/examples/handling_sparse_features.ipynb \\\n            --NotebookApp.allow_origin='https://colab.research.google.com' \\\n            --port=8888\n    ```\n\n4.  (Optional) If you are working on remote server, set up port forwarding with\n    this command.\n\n    ```shell\n    $ ssh <remote-server> -L 8888:127.0.0.1:8888\n    ```\n\n5.  Running the notebook.\n\n    *   Start jupyter notebook on your local machine at\n        [http://localhost:8888/](http://localhost:8888/) and browse to the\n        ipython notebook.\n\n    *   An alternative is to use colaboratory notebook via\n        [colab.research.google.com](http://colab.research.google.com) and open\n        the notebook in the browser. Choose local runtime and link to port 8888.\n\n## References\n\n+   Rama Kumar Pasumarthi, Sebastian Bruch, Xuanhui Wang, Cheng Li, Michael\n    Bendersky, Marc Najork, Jan Pfeifer, Nadav Golbandi, Rohan Anil, Stephan\n    Wolf. _TF-Ranking: Scalable TensorFlow Library for Learning-to-Rank._\n    [KDD 2019.](https://ai.google/research/pubs/pub48160)\n\n+   Qingyao Ai, Xuanhui Wang, Sebastian Bruch, Nadav Golbandi, Michael\n    Bendersky, Marc Najork. _Learning Groupwise Scoring Functions Using Deep\n    Neural Networks._ [ICTIR 2019](https://ai.google/research/pubs/pub48348)\n\n+   Xuanhui Wang, Michael Bendersky, Donald Metzler, and Marc Najork. _Learning\n    to Rank with Selection Bias in Personal Search._\n    [SIGIR 2016.](https://ai.google/research/pubs/pub45286)\n\n+   Xuanhui Wang, Cheng Li, Nadav Golbandi, Mike Bendersky, Marc Najork. _The\n    LambdaLoss Framework for Ranking Metric Optimization_.\n    [CIKM 2018.](https://ai.google/research/pubs/pub47258)\n\n### Citation\n\nIf you use TensorFlow Ranking in your research and would like to cite it, we\nsuggest you use the following citation:\n\n    @inproceedings{TensorflowRankingKDD2019,\n       author = {Rama Kumar Pasumarthi and Sebastian Bruch and Xuanhui Wang and Cheng Li and Michael Bendersky and Marc Najork and Jan Pfeifer and Nadav Golbandi and Rohan Anil and Stephan Wolf},\n       title = {TF-Ranking: Scalable TensorFlow Library for Learning-to-Rank},\n       booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n       year = {2019},\n       pages = {2970--2978},\n       location = {Anchorage, AK}\n    }\n", "release_dates": ["2023-06-03T06:14:49Z", "2023-02-01T02:48:12Z", "2022-10-26T23:51:05Z", "2021-11-16T23:49:54Z", "2021-07-22T23:55:13Z", "2021-05-25T17:36:24Z", "2021-02-02T16:58:47Z", "2020-08-19T21:58:05Z", "2020-06-01T23:00:54Z", "2020-03-24T00:54:39Z", "2020-03-06T00:53:48Z", "2020-01-17T07:42:45Z", "2019-12-18T21:16:07Z", "2019-10-22T20:22:52Z", "2019-10-22T05:57:55Z", "2019-09-24T23:09:10Z", "2019-09-05T21:50:34Z", "2019-06-20T21:00:46Z"]}, {"name": "recommenders", "description": "TensorFlow Recommenders is a library for building recommender system models using TensorFlow.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Recommenders\n\n![TensorFlow Recommenders logo](assets/full_logo.png)\n\n![TensorFlow Recommenders build badge](https://github.com/tensorflow/recommenders/actions/workflows/test.yaml/badge.svg)\n[![PyPI badge](https://img.shields.io/pypi/v/tensorflow-recommenders.svg)](https://pypi.python.org/pypi/tensorflow-recommenders/)\n\nTensorFlow Recommenders is a library for building recommender system models\nusing [TensorFlow](https://www.tensorflow.org).\n\nIt helps with the full workflow of building a recommender system: data\npreparation, model formulation, training, evaluation, and deployment.\n\nIt's built on Keras and aims to have a gentle learning curve while still giving\nyou the flexibility to build complex models.\n\n## Installation\n\nMake sure you have TensorFlow 2.x installed, and install from `pip`:\n\n```shell\npip install tensorflow-recommenders\n```\n\n## Documentation\n\nHave a look at our\n[tutorials](https://tensorflow.org/recommenders/examples/quickstart) and\n[API reference](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/).\n\n## Quick start\n\nBuilding a factorization model for the Movielens 100K dataset is very simple\n([Colab](https://tensorflow.org/recommenders/examples/quickstart)):\n\n```python\nfrom typing import Dict, Text\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport tensorflow_recommenders as tfrs\n\n# Ratings data.\nratings = tfds.load('movielens/100k-ratings', split=\"train\")\n# Features of all the available movies.\nmovies = tfds.load('movielens/100k-movies', split=\"train\")\n\n# Select the basic features.\nratings = ratings.map(lambda x: {\n    \"movie_id\": tf.strings.to_number(x[\"movie_id\"]),\n    \"user_id\": tf.strings.to_number(x[\"user_id\"])\n})\nmovies = movies.map(lambda x: tf.strings.to_number(x[\"movie_id\"]))\n\n# Build a model.\nclass Model(tfrs.Model):\n\n  def __init__(self):\n    super().__init__()\n\n    # Set up user representation.\n    self.user_model = tf.keras.layers.Embedding(\n        input_dim=2000, output_dim=64)\n    # Set up movie representation.\n    self.item_model = tf.keras.layers.Embedding(\n        input_dim=2000, output_dim=64)\n    # Set up a retrieval task and evaluation metrics over the\n    # entire dataset of candidates.\n    self.task = tfrs.tasks.Retrieval(\n        metrics=tfrs.metrics.FactorizedTopK(\n            candidates=movies.batch(128).map(self.item_model)\n        )\n    )\n\n  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n\n    user_embeddings = self.user_model(features[\"user_id\"])\n    movie_embeddings = self.item_model(features[\"movie_id\"])\n\n    return self.task(user_embeddings, movie_embeddings)\n\n\nmodel = Model()\nmodel.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n\n# Randomly shuffle data and split between train and test.\ntf.random.set_seed(42)\nshuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n\ntrain = shuffled.take(80_000)\ntest = shuffled.skip(80_000).take(20_000)\n\n# Train.\nmodel.fit(train.batch(4096), epochs=5)\n\n# Evaluate.\nmodel.evaluate(test.batch(4096), return_dict=True)\n```\n", "release_dates": ["2023-02-03T02:08:24Z", "2022-09-28T22:56:14Z", "2022-07-12T23:32:26Z", "2022-07-12T16:40:05Z", "2021-08-23T23:20:47Z", "2021-07-16T21:22:22Z", "2021-05-17T16:31:38Z", "2021-05-07T17:47:08Z", "2021-01-20T22:51:14Z", "2020-12-22T19:21:09Z", "2020-12-22T18:48:27Z", "2020-11-19T01:39:42Z", "2020-10-15T20:06:55Z", "2020-09-22T21:54:59Z", "2020-09-17T17:20:15Z", "2020-09-17T00:54:15Z"]}, {"name": "recommenders-addons", "description": "Additional utils and helpers to extend TensorFlow when build recommendation systems, contributed and maintained by SIG Recommenders.", "language": "Cuda", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Recommenders Addons\n-----------------\n![TensorFlow Recommenders logo](assets/SIGRecommendersAddons.png)\n[![PyPI Status Badge](https://badge.fury.io/py/tensorflow-recommenders-addons.svg)](https://pypi.org/project/tensorflow-recommenders-addons/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/tensorflow-recommenders-addons)](https://pypi.org/project/tensorflow-recommenders-addons/)\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](docs/api_docs/)\n\nTensorFlow Recommenders Addons(TFRA) are a collection of projects related to large-scale recommendation systems \nbuilt upon TensorFlow by introducing the **Dynamic Embedding Technology** to TensorFlow \nthat makes TensorFlow more suitable for training models of **Search, Recommendations, and Advertising** and \nmakes building, evaluating, and serving sophisticated recommenders models easy. \nSee approved TensorFlow RFC #[313](https://github.com/tensorflow/community/pull/313).\nThose contributions will be complementary to TensorFlow Core and TensorFlow Recommenders etc. \n\nFor Apple silicon(M1), please refer to [Apple Silicon Support](#apple-silicon-support-beta-release).\n\n## Main Features\n\n- Make key-value data structure (dynamic embedding) trainable in TensorFlow\n- Get better recommendation effect compared to static embedding mechanism with no hash conflicts\n- Compatible with all native TensorFlow optimizers and initializers\n- Compatible with native TensorFlow CheckPoint and SavedModel format\n- Fully support train and inference recommenders models on GPUs\n- Support [TF serving](https://github.com/tensorflow/serving) and [Triton Inference Server](https://github.com/triton-inference-server/server) as inference framework\n- Support variant Key-Value implements as dynamic embedding storage and easy to extend\n  - [cuckoohash_map](https://github.com/efficient/libcuckoo) (from Efficient Computing at Carnegie Mellon, on CPU)\n  - [nvhash](https://github.com/rapidsai/cudf) (from NVIDIA, on GPU)\n  - [Redis](https://github.com/redis/redis)\n- Support half synchronous training based on Horovod\n  - Synchronous training for dense weights\n  - Asynchronous training for sparse weights\n\n## Subpackages\n\n* [tfra.dynamic_embedding](docs/api_docs/tfra/dynamic_embedding.md), [RFC](rfcs/20200424-sparse-domain-isolation.md)\n* [tfra.embedding_variable](https://github.com/tensorflow/recommenders-addons/blob/master/docs/tutorials/embedding_variable_tutorial.ipynb), [RFC](https://docs.google.com/document/d/1odez6-69YH-eFcp8rKndDHTNGxZgdFFRJufsW94_gl4)\n\n## Contributors\n\nTensorFlow Recommenders-Addons depends on public contributions, bug fixes, and documentation.\nThis project exists thanks to all the people and organizations who contribute. [[Contribute](CONTRIBUTING.md)]\n\n<a href=\"https://github.com/tensorflow/recommenders-addons/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=tensorflow/recommenders-addons\" />\n</a>\n\n\n\\\n<a href=\"https://github.com/tencent\">\n  <kbd> <img src=\"./assets/tencent.png\" height=\"70\" /> </kbd>\n</a><a href=\"https://github.com/alibaba\">\n  <kbd> <img src=\"./assets/alibaba.jpg\" height=\"70\" /> </kbd>\n</a><a href=\"https://vip.com/\"> \n  <kbd> <img src=\"./assets/vips.jpg\" height=\"70\" /> </kbd>\n</a><a href=\"https://www.zhipin.com//\">\n  <kbd> <img src=\"./assets/boss.svg\" height=\"70\" /> </kbd>\n</a>\n\n\\\nA special thanks to [NVIDIA Merlin Team](https://github.com/NVIDIA-Merlin) and NVIDIA China DevTech Team, \nwho have provided GPU acceleration technology support and code contribution.\n\n<a href=\"https://github.com/NVIDIA-Merlin\">\n  <kbd> <img src=\"./assets/merilin.png\" height=\"70\" /> </kbd>\n</a>\n\n\n## Tutorials & Demos\nSee [tutorials](docs/tutorials/) and [demo](demo/) for end-to-end examples of each subpackages.\n\n## Installation\n#### Stable Builds\nTensorFlow Recommenders-Addons is available on PyPI for Linux, macOS. To install the latest version, \nrun the following:\n```\npip install tensorflow-recommenders-addons\n```\n\nBy default, CPU version will be installed. To install GPU version, run the following:\n```\npip install tensorflow-recommenders-addons-gpu\n```\n\nTo use TensorFlow Recommenders-Addons:\n\n```python\nimport tensorflow as tf\nimport tensorflow_recommenders_addons as tfra\n```\n\n### Compatibility with Tensorflow\nTensorFlow C++ APIs are not stable and thus we can only guarantee compatibility with the \nversion TensorFlow Recommenders-Addons(TFRA) was built against. It is possible TFRA will work with \nmultiple versions of TensorFlow, but there is also a chance for segmentation faults or other problematic \ncrashes. Warnings will be emitted if your TensorFlow version does not match what it was built against.\n\nAdditionally, TFRA custom ops registration does not have a stable ABI interface so it is \nrequired that users have a compatible installation of TensorFlow even if the versions \nmatch what we had built against. A simplification of this is that **TensorFlow Recommenders-Addons \ncustom ops will work with `pip`-installed TensorFlow** but will have issues when TensorFlow \nis compiled differently. A typical example of this would be `conda`-installed TensorFlow.\n[RFC #133](https://github.com/tensorflow/community/pull/133) aims to fix this.\n\n\n#### Compatibility Matrix\n*GPU is supported by version `0.2.0` and later.*\n\n| TFRA  | TensorFlow | Compiler   | CUDA | CUDNN | Compute Capability           | CPU           |\n|:------|:-----------|:-----------|:-----|:------|:-----------------------------|:--------------|\n| 0.6.0 | 2.8.3      | GCC 7.3.1  | 11.2 | 8.1   | 6.0, 6.1, 7.0, 7.5, 8.0, 8.6 | x86 |\n| 0.6.0 | 2.6.0      | Xcode 13.1 | -    | -     | -                            | Apple M1      |\n| 0.5.1 | 2.8.3      | GCC 7.3.1  | 11.2 | 8.1   | 6.0, 6.1, 7.0, 7.5, 8.0, 8.6 | x86 |\n| 0.5.1 | 2.6.0      | Xcode 13.1 | -    | -     | -                            | Apple M1      |\n| 0.5.0 | 2.8.3      | GCC 7.3.1  | 11.2 | 8.1   | 6.0, 6.1, 7.0, 7.5, 8.0, 8.6 | x86 |\n| 0.5.0 | 2.6.0      | Xcode 13.1 | -    | -     | -                            | Apple M1      |\n| 0.4.0 | 2.5.1      | GCC 7.3.1  | 11.2 | 8.1   | 6.0, 6.1, 7.0, 7.5, 8.0, 8.6 | x86 |\n| 0.4.0 | 2.5.0      | Xcode 13.1 | -    | -     | -                            | Apple M1      |\n| 0.3.1 | 2.5.1      | GCC 7.3.1  | 11.2 | 8.1   | 6.0, 6.1, 7.0, 7.5, 8.0, 8.6 | x86           |\n| 0.2.0 | 2.4.1      | GCC 7.3.1  | 11.0 | 8.0   | 6.0, 6.1, 7.0, 7.5, 8.0      | x86           |\n| 0.2.0 | 1.15.2     | GCC 7.3.1  | 10.0 | 7.6   | 6.0, 6.1, 7.0, 7.5           | x86           |\n| 0.1.0 | 2.4.1      | GCC 7.3.1  | -    | -     | -                            | x86           |\n\nCheck [nvidia-support-matrix](https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html) for more details.\n\n**NOTICE**\n\n- The release packages have a strict version binding relationship with TensorFlow. \n- Due to the significant changes in the Tensorflow API, we can only ensure version 0.2.0 compatibility with TF1.15.2 on CPU & GPU, \n  but **there are no official releases**, you can only get it through compiling by the following:\n```sh\nPY_VERSION=\"3.7\" \\\nTF_VERSION=\"1.15.2\" \\\nTF_NEED_CUDA=1 \\\nsh .github/workflows/make_wheel_Linux_x86.sh\n\n# .whl file will be created in ./wheelhouse/\n```\n\n- If you need to work with TensorFlow 1.14.x or older version, we suggest you give up,\nbut maybe this doc can help you : [Extract headers from TensorFlow compiling directory](./build_deps/tf_header/README.md).\nAt the same time, we find some OPs used by TRFA have better performance, so we highly recommend you update TensorFlow to 2.x.\n\n### Installing from Source\n\nFor all developers, we recommend you use the development docker containers which are all GPU enabled:\n```sh\ndocker pull tfra/dev_container:latest-python3.8  # \"3.7\", \"3.9\" are all avaliable.\ndocker run --privileged --gpus all -it --rm -v $(pwd):$(pwd) tfra/dev_container:latest-3.8\n```\n\n#### CPU Only\nYou can also install from source. This requires the [Bazel](https://bazel.build/) build system (version == 5.1.1).\nPlease install a TensorFlow on your compiling machine, The compiler needs to know the version of Tensorflow and \nits headers according to the installed TensorFlow. \n\n```sh\nexport TF_VERSION=\"2.8.3\"  # \"2.6.3\" are well tested.\npip install tensorflow[-gpu]==$TF_VERSION\n\ngit clone https://github.com/tensorflow/recommenders-addons.git\ncd recommenders-addons\n\n# This script links project with TensorFlow dependency\npython configure.py\n\nbazel build --enable_runfiles build_pip_pkg\nbazel-bin/build_pip_pkg artifacts\n\npip install artifacts/tensorflow_recommenders_addons-*.whl\n```\n#### GPU Support\nOnly `TF_NEED_CUDA=1` is required and other environment variables are optional:\n```sh\nexport TF_VERSION=\"2.8.3\"  # \"2.6.3\" is well tested.\nexport PY_VERSION=\"3.8\" \nexport TF_NEED_CUDA=1\nexport TF_CUDA_VERSION=11.2\nexport TF_CUDNN_VERSION=8.1\nexport CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\nexport CUDNN_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu\"\n\npython configure.py\n```\nAnd then build the pip package and install:\n```sh\nbazel build --enable_runfiles build_pip_pkg\nbazel-bin/build_pip_pkg artifacts\npip install artifacts/tensorflow_recommenders_addons_gpu-*.whl\n```\n\n#### Apple Silicon Support\nRequirements:\n\n- macOS 12.0.0+\n- Python 3.9\n- tensorflow-macos 2.9.0\n- bazel 5.1.1\n\nThe natively supported TensorFlow is maintained by Apple. Please see the instruction [Get started with tensorflow-metal](https://developer.apple.com/metal/tensorflow-plugin/) to install the Tensorflow on apple silicon devices.\n\n\n**Install TFRA on Apple Silicon via PIP**\n```sh\npython -m pip install tensorflow-recommenders-addons --no-deps\n```\n\n**Build TFRA on Apple Silicon from Source**\n\n```sh\n# Install bazelisk\nbrew install bazelisk\n\n# Build wheel from source\nPY_VERSION=3.9.0 TF_VERSION=2.9.0 TF_NEED_CUDA=\"0\" sh .github/workflows/make_wheel_macOS_arm64.sh\n\n# Install the wheel\npython -m pip install --no-deps ./artifacts/*.whl\n```\n\n**Known Issues:**\n\nThe Apple silicon version of TFRA doesn't support: \n\n* Data type **float16**\n* Synchronous training based on **Horovod**\n* `save_to_file_system`\n* `load_from_file_system` \n* `warm_start_util`\n\n`save_to_file_system` and `load_from_file_system` are not supported because TFIO is not supported on apple silicon devices. Horovod and `warm_start_util` are not supported because the natively supported tensorflow-macos doesn't support V1 Tensorflow networks.\n\nThese issues may be fixed in the future release.\n\n\n##### Data Type Matrix for `tfra.dynamic_embedding.Variable` \n\n|  Values \\\\ Keys  | int64  | int32 | string |\n|:----:|:----:|:----:|:----:| \n| float  | CPU, GPU | CPU, GPU | CPU |\n| half  | CPU, GPU | - | CPU |\n| int32  | CPU, GPU | CPU | CPU |\n| int8  | CPU, GPU | - | CPU |\n| int64  | CPU | - | CPU |\n| double  | CPU, CPU | CPU | CPU |\n| bool  | - | - | CPU |\n| string  | CPU | - | - |\n\n##### To use GPU by `tfra.dynamic_embedding.Variable`\nThe `tfra.dynamic_embedding.Variable` will ignore the device placement mechanism of TensorFlow, \nyou should specify the `devices` onto GPUs explicitly for it.\n\n```python\nimport tensorflow as tf\nimport tensorflow_recommenders_addons as tfra\n\nde = tfra.dynamic_embedding.get_variable(\"VariableOnGpu\",\n                                         devices=[\"/job:ps/task:0/GPU:0\", ],\n                                         # ...\n                                         )\n```\n\n**Usage restrictions on GPU**\n- Only work on Nvidia GPU with cuda compute capability 6.0 or higher.\n- Considering the size of the .whl file, currently `dim` only supports less than or equal to 200, if you need longer `dim`, please submit an issue.\n- Only `dynamic_embedding` APIs and relative OPs support running on GPU.\n- For GPU HashTables manage GPU memory independently, TensorFlow should be configured to allow GPU memory growth by the following:\n```python\nsess_config.gpu_options.allow_growth = True\n```\n\n## Inference \n\n### With TensorFlow Serving\n\n#### Compatibility Matrix\n| TFRA  | TensorFlow | Serving branch | Compiler  | CUDA | CUDNN | Compute Capability |\n|:------|:-----------|:---------------|:---------| :------------ | :---- | :------------ |\n| 0.6.0 | 2.8.3      | r2.8           | GCC 7.3.1 | 11.2| 8.1 | 6.0, 6.1, 7.0, 7.5, 8.0, 8.6 |\n| 0.5.1 | 2.8.3      | r2.8           | GCC 7.3.1 | 11.2| 8.1 | 6.0, 6.1, 7.0, 7.5, 8.0, 8.6 |\n| 0.5.0 | 2.8.3      | r2.8           | GCC 7.3.1 | 11.2| 8.1 | 6.0, 6.1, 7.0, 7.5, 8.0, 8.6 |\n| 0.4.0 | 2.5.1      | r2.5           | GCC 7.3.1 | 11.2| 8.1 | 6.0, 6.1, 7.0, 7.5, 8.0, 8.6 |\n| 0.3.1 | 2.5.1      | r2.5           | GCC 7.3.1 | 11.2| 8.1 | 6.0, 6.1, 7.0, 7.5, 8.0, 8.6 |\n| 0.2.0 | 2.4.1      | r2.4           | GCC 7.3.1 | 11.0 | 8.0 | 6.0, 6.1, 7.0, 7.5, 8.0 |\n| 0.2.0 | 1.15.2     | r1.15          | GCC 7.3.1 | 10.0 | 7.6 | 6.0, 6.1, 7.0, 7.5 |\n| 0.1.0 | 2.4.1      | r2.4           | GCC 7.3.1 | - | - | - |\n\nServing TFRA-enable models by custom ops in TensorFlow Serving. \n\n```sh\n## If enable GPU OPs\nexport SERVING_WITH_GPU=1 \n\n## Specifiy the branch of TFRA\nexport TFRA_BRANCH=\"master\" # The `master` and `r0.6` are available.\n\n## Create workspace, modify the directory as you prefer to.\nexport TFRA_SERVING_WORKSPACE=~/tfra_serving_workspace/\nmkdir -p $TFRA_SERVING_WORKSPACE && cd $TFRA_SERVING_WORKSPACE\n\n## Clone the release branches of serving and TFRA according to `Compatibility Matrix`.\ngit clone -b r2.8 https://github.com/tensorflow/serving.git\ngit clone -b $TFRA_BRANCH https://github.com/tensorflow/recommenders-addons.git\n\n## Run config shell script\ncd $TFRA_SERVING_WORKSPACE/recommenders-addons/tools\nbash config_tfserving.sh $TFRA_BRANCH $TFRA_SERVING_WORKSPACE/serving $SERVING_WITH_GPU\n\n## Build serving with TFRA OPs.\ncd $TFRA_SERVING_WORKSPACE/serving\n./tools/run_in_docker.sh bazel build tensorflow_serving/model_servers:tensorflow_model_server\n\n```\n\nFor more detail, please refer to the shell script `./tools/config_tfserving.sh`.\n\n**NOTICE**\n- Distributed inference is only supported when using Redis as Key-Value storage. \n- Reference documents: https://www.tensorflow.org/tfx/serving/custom_op\n\n### With Triton\nWhen building the custom operations shared library it is important to\nuse the same version of TensorFlow as is being used in Triton. You can\nfind the TensorFlow version in the [Triton Release\nNotes](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/index.html). A\nsimple way to ensure you are using the correct version of TensorFlow\nis to use the [NGC TensorFlow\ncontainer](https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow)\ncorresponding to the Triton container. For example, if you are using\nthe 23.05 version of Triton, use the 23.05 version of the TensorFlow\ncontainer.\n```bash\ndocker pull nvcr.io/nvidia/tritonserver:22.05-py3\n\nexport TFRA_BRANCH=\"master\"\ngit clone -b $TFRA_BRANCH https://github.com/tensorflow/recommenders-addons.git\ncd recommenders-addons\n\npython configure.py\nbazel build //tensorflow_recommenders_addons/dynamic_embedding/core:_cuckoo_hashtable_ops.so ##bazel 5.1.1 is well tested\nmkdir /tmp/so\n#you can also use the so file from pip install package file from \"(PYTHONPATH)/site-packages/tensorflow_recommenders_addons/dynamic_embedding/core/_cuckoo_hashtable_ops.so\"\ncp bazel-bin/tensorflow_recommenders_addons/dynamic_embedding/core/_cuckoo_hashtable_ops.so /tmp/so\n\n#tfra saved_model directory \"/models/model_repository\"\ndocker run --net=host -v /models/model_repository:/models nvcr.io/nvidia/tritonserver:22.05-py3 bash -c \\\n  \"LD_PRELOAD=/tmp/so/_cuckoo_hashtable_ops.so:${LD_PRELOAD} tritonserver --model-repository=/models/ --backend-config=tensorflow,version=2 --strict-model-config=false\"\n```\n\n**NOTICE**\n- The above LD_LIBRARY_PATH and backend-config must be set Because the default backend is tf1.\n\n\n## Community\n\n* SIG Recommenders mailing list:\n[recommenders@tensorflow.org](https://groups.google.com/a/tensorflow.org/g/recommenders)\n\n## Acknowledgment\nWe are very grateful to the maintainers of [tensorflow/addons](https://github.com/tensorflow/addons) for borrowing a lot of code from [tensorflow/addons](https://github.com/tensorflow/addons) to build our workflow and documentation system.\nWe also want to extend a thank you to the Google team members who have helped with CI setup and reviews!\n\n## License\nApache License 2.0\n\n\n\n", "release_dates": ["2023-04-04T07:14:36Z", "2023-04-03T05:50:58Z", "2023-03-28T13:05:58Z", "2023-03-11T10:37:06Z", "2022-11-08T07:59:26Z", "2022-03-30T04:55:26Z", "2021-10-27T06:45:02Z", "2021-10-14T13:00:07Z", "2021-06-04T06:37:17Z", "2021-03-25T05:32:47Z"]}, {"name": "runtime", "description": "A performant and modular runtime for TensorFlow", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TFRT: A New TensorFlow Runtime\n\nTFRT is a new TensorFlow runtime. It aims to provide a unified, extensible\ninfrastructure layer with best-in-class performance across a wide variety of\ndomain specific hardware. It provides efficient use of multithreaded host CPUs,\nsupports fully asynchronous programming models, and focuses on low-level\nefficiency.\n\nTFRT will benefit a broad range of users, but it will be of particular interest\nto you if you are a:\n\n*   Researcher looking to experiment with complex new models and add custom\n    operations to TensorFlow\n*   Application developer looking for improved performance when serving models\n    in production\n*   Hardware maker looking to plug hardware into TensorFlow, including edge and\n    datacenter devices\n\n...or you are simply curious about cool ML infrastructure and low-level runtime\ntechnology!\n\nTo learn more about TFRT\u2019s early progress and wins, check out our\n[Tensorflow Dev Summit 2020 presentation](https://www.youtube.com/watch?v=15tiQoPpuZ8)\nwhere we provided a performance benchmark for small-batch GPU inference on\nResNet 50, and our\n[MLIR Open Design Deep Dive presentation](https://drive.google.com/drive/folders/1fkLJuVP-tIk4GENBu2AgemF3oXYGr2PB)\nwhere we provided a detailed overview of TFRT\u2019s core components, low-level\nabstractions, and general design principles.\n\n**Note:** TFRT is an early stage project and is not yet ready for general use.\n\n## Getting started\n\n**TLDR:** This section describes how to set up a development environment for\nTFRT, as well as instructions to build and test TFRT components.\n\nTFRT currently supports Ubuntu-16.04. Future supported platforms include MacOS,\nWindows, etc. Bazel and clang are required to build and test TFRT. NVIDIA's CUDA\nToolkit and cuDNN libraries are required for the GPU backend.\n\nTo describe the TFRT build and test workflows, we will build and run the\nfollowing binaries for graph execution.\n\nRecall from our Dev Summit presentation that for graph execution, a TensorFlow\nuser passes into TFRT a TensorFlow graph created via high-level TensorFlow APIs,\nand TFRT then calls the [MLIR](https://www.tensorflow.org/mlir)-based graph\ncompiler to optimize and lower the graph into\n[BEF](documents/binary_executable_format.md), a Binary Executable Format for\nTFRT graph execution (MLIR is the compiler infrastructure that we use to\nrepresent TFRT host programs). The blue arrows in the simplified TensorFlow\ntraining stack diagram below show this flow.\n\n<div align=\"center\">\n<img src=\"documents/img/TFRT_overview.svg\" alt=\"TFRT Overview\" width=\"800\">\n</div>\n\nThe two binaries introduced next focus on the backend of the graph execution\nworkflow. After the graph compiler has optimized the TensorFlow graph and\nproduced a low-level TFRT Host Program represented in MLIR, `tfrt_translate`\ngenerates a `BEF` file from that host program and `bef_executor` runs the `BEF`\nfile. The progression from TFRT Host Program to `bef_executor` via\n`tfrt_translate` is depicted in the expanded TensorFlow training stack diagram\nbelow. Note that the blue arrow between TFRT Host Program and `BEF` file\nrepresents `tfrt_translate`. Both programs are built in the `tools` directory.\n\n<div align=\"center\">\n<img src=\"documents/img/BEF_conversion.svg\" alt=\"BEF Conversion\" width=\"480\">\n</div>\n\n#### tfrt_translate\n\nThe `tfrt_translate` program does round trip translation between MLIR and BEF,\nsimilar to an assembler and disassembler.\n\n#### bef_executor\n\nThe `bef_executor` program is the execution driver of `BEF` files. It reads in a\n`BEF` file, sets up runtime, and asynchronously executes function(s) in that\nfile.\n\n### Prerequisites\n\n#### Install Bazel\n\nTo build TFRT, you need to install Bazel. TFRT is built and verified with Bazel\n4.0. Follow\n[the Bazel installation instructions](https://docs.bazel.build/versions/master/install-ubuntu.html)\nto install Bazel. Verify the installation with\n\n```shell\n$ bazel --version\nbazel 4.0.0\n```\n\n#### Install clang\n\nFollow [the clang installation instructions](https://apt.llvm.org/) to install\nclang. The automatic installation script that installs clang, lldb, and lld, is\nrecommended. TFRT is built and verified with clang 11.1.\n\nIf you have multiple versions of clang installed, ensure that the right version\nof clang is the default. On Ubuntu based systems, you can use\n`update-alternatives` to select the default version. The following example\ncommands assume you installed clang-11:\n\n```shell\n$ sudo update-alternatives --install /usr/bin/clang clang /usr/bin/clang-11 11\n$ sudo update-alternatives --install /usr/bin/clang++ clang++ /usr/bin/clang++-11 11\n```\n\nVerify the installation with\n\n```shell\n$ clang --version\nclang version 11.1.0\n```\n\n#### Install libstdc++\n\nTFRT requires libstdc++8 or greater. Check clang's selected version with\n\n```shell\n$ clang++ -v |& grep \"Selected GCC\"\nSelected GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/10\n```\n\nIn the example above, the *10* at the end of the path indicates that clang will\nuse libstdc++<em>10</em>, which is compatible with TFRT.\n\nIf you need to upgrade, the easiest way is to install gcc-8. Run the following\ncommand to install:\n\n```shell\n$ sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test\n$ sudo apt-get update\n$ sudo apt-get install -y gcc-8 g++-8\n```\n\nTo verify installation, re-run the `clang++ -v` check above.\n\n#### GPU prerequisites\n\n**Note:** You can skip this section if you don't want to build the GPU backend.\nRemember to exclude `//backends/gpu/...` from your Bazel target patterns though.\n\nBuilding and running the GPU backend requires installing additional components.\n\nInstall clang Python bindings using pip with\n\n```shell\n$ pip install libclang\n```\n\nInstall NVIDIA's CUDA Toolkit v11.2 (see\n[installation guide](https://docs.nvidia.com/cuda/cuda-installation-guide-linux)\nfor details) in a single directory from NVIDIA\u2019s `.run` package with\n\n```shell\n$ wget http://developer.download.nvidia.com/compute/cuda/11.2.2/local_installers/cuda_11.2.2_460.32.03_linux.run\n$ sudo sh cuda_11.2.2_460.32.03_linux.run --toolkit --installpath=<path>\n```\n\nRegister the path to CUDA shared objects with\n\n```shell\n$ sudo echo '<path>/lib64' > '/etc/ld.so.conf.d/cuda.conf'\n$ sudo ldconfig\n```\n\nInstall NVIDIA's cuDNN libraries (see\n[installation guide](http://docs.nvidia.com/deeplearning/sdk/cudnn-install) for\ndetails) with\n\n```shell\n$ wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/libcudnn8_8.0.4.30-1+cuda11.1_amd64.deb\n$ sudo apt install ./libcudnn8_8.0.4.30-1+cuda11.1_amd64.deb\n```\n\n**Note:** The above package is intended for CUDA 11.1, but is compatible with\nCUDA 11.2. TFRT is built and verified with cuDNN 8.1 for CUDA 11.2. Access to\nthat package requires a (free) NVIDIA developer account.\n\n### Building and running TFRT\n\nTo build TFRT, `cd` to the root directory (where `WORKSPACE` file is located) of\nthe TFRT workspace. A set of build configurations is in `.bazelrc` file. You can\ncreate a `user.bazelrc` in the repository root with extra Bazel configs that may\nbe useful. Build `tfrt_translate` and `bef_executor` with the following\ncommands:\n\n```shell\n$ bazel build //tools:bef_executor\n$ bazel build //tools:tfrt_translate\n```\n\nThe above commands build the binaries with `opt` compilation mode. Check\n[Bazel's documentation](https://docs.bazel.build/versions/master/command-line-reference.html#build-options)\nfor more build options. Bazel will notify the output location at the end of a\nsuccessful build (default is `bazel-bin`).\n\nAfter `tfrt_translate` and `bef_executor` are built, run an `.mlir` program with\nthe following command:\n\n```shell\n$ bazel-bin/tools/tfrt_translate -mlir-to-bef path/to/program.mlir | bazel-bin/tools/bef_executor\n```\n\nTFRT provides a series of .mlir test programs. For example:\n\n```shell\n$ bazel-bin/tools/tfrt_translate -mlir-to-bef mlir_tests/bef_executor/async.mlir | bazel-bin/tools/bef_executor\n```\n\nAny output will be printed out to the terminal.\n\n### Adding GPU support\n\nAdd `--config=cuda` to the Bazel command to link the GPU backend to the above\ntargets.\n\nCustom CUDA Toolkit locations can be specified with\n`--repo_env=CUDA_PATH=<path>`. The default is `/usr/local/cuda`.\n\n### Testing\n\nTFRT utilizes LLVM\u2019s [LIT](https://llvm.org/docs/CommandGuide/lit.html)\ninfrastructure and\n[FileCheck](https://llvm.org/docs/CommandGuide/FileCheck.html) utility tool to\nconstruct MLIR-based check tests. These tests verify that some set of string\ntags appear in the test\u2019s output. More introduction and guidelines on testing\ncan be found\n[here](https://mlir.llvm.org/getting_started/TestingGuide/#check-tests). An\nexample test is shown below:\n\n```c++\n// RUN: tfrt_translate -mlir-to-bef %s | bef_executor | FileCheck %s\n// RUN: tfrt_opt %s | tfrt_opt\n\n// CHECK-LABEL: --- Running 'basic_tensor'\nfunc @basic_tensor() {\n  %c0 = tfrt.new.chain\n\n  %a = dht.create_uninitialized_tensor.i32.2 [3 : i64, 2 : i64]\n  %c1 = dht.fill_tensor_with_constant.i32 %a, %c0 0 : i32\n\n  // CHECK: shape = [3, 2], values = [0, 0, 0, 0, 0, 0]\n  %c2 = dht.print_tensor %a, %c1\n\n  tfrt.return\n}\n```\n\nTo run a test, simply invoke `bazel test`:\n\n```shell\n$ bazel test //mlir_tests/bef_executor:basics.mlir.test\n```\n\nMost tests under `//backends/gpu/...` need to be built with `--config=cuda` so\nthat the GPU backend is linked to the bef_executor:\n\n```shell\n$ bazel test --config=cuda //backends/gpu/mlir_tests/core_runtime:get_device.mlir.test\n```\n\nUse Bazel\n[target patterns](https://docs.bazel.build/versions/master/guide.html#specifying-targets-to-build)\nto run multiple tests:\n\n```shell\n$ bazel test -- //... -//third_party/... -//backends/gpu/...  # All CPU tests.\n$ bazel test --config=cuda //backends/gpu/...                 # All GPU tests.\n```\n\n### Next Steps\n\nTry our [tutorial](documents/tutorial.md) for some hands-on experience with\nTFRT.\n\nSee [host runtime design](documents/tfrt_host_runtime_design.md) for more\ndetails on TFRT's design.\n\n## Repository Overview\n\nThe three key directories under the TFRT root directory are\n\n*   `lib/`: Contains core TFRT infrastructure code\n*   `backends/`: Contains device specific infrastructure and op/kernel\n    implementations\n*   `include/`: Contains public header files for core TFRT infrastructure\n\n<table>\n  <tr>\n   <td><strong>Top level directory</strong>\n   </td>\n   <td><strong>Sub-directory</strong>\n   </td>\n   <td><strong>Description</strong>\n   </td>\n  </tr>\n  <tr>\n   <td colspan=\"2\" ><strong><code>include/</code></strong>\n   </td>\n   <td>TFRT infrastructure public headers\n   </td>\n  </tr>\n  <tr>\n   <td colspan=\"2\" ><strong><code>lib/</code></strong>\n   </td>\n   <td>TFRT infrastructure common for host runtime and all device runtime\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>basic_kernels/</code>\n   </td>\n   <td>Common infrastructure kernels, e.g. control flow kernels\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>bef_executor/</code>\n   </td>\n   <td>BEFFile and BEFExecutor implementation\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>bef_executor_driver/</code>\n   </td>\n   <td>Driver code for running BEFExecutor for an input MLIR file\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>bef_converter/</code>\n   </td>\n   <td>Converter between MLIR and BEF (bef_to_mlir and mlir_to_bef)\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>core_runtime/</code>\n   </td>\n   <td>TFRT Core Runtime infrastructure\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>distributed_runtime/</code>\n   </td>\n   <td>TFRT Distributed Runtime infrastructure\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>data/</code>\n   </td>\n   <td>TFRT infrastructure for TF input pipelines\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>host_context/</code>\n   </td>\n   <td>Host TFRT data structure, e.g. HostContext, AsyncValue, ConcurrentWorkQueue\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>metrics/</code>\n   </td>\n   <td>ML metric integration\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>support/</code>\n   </td>\n   <td>Basic utilities, e.g. hash_util, string_util\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>tensor/</code>\n   </td>\n   <td>Base Tensor class and host tensor implementations\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>test_kernels/</code>\n   </td>\n   <td>Testing kernel implementations\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>tracing/</code>\n   </td>\n   <td>Tracing/profiling support\n   </td>\n  </tr>\n  <tr>\n   <td colspan=\"2\" ><strong><code>cpp_tests/</code></strong>\n   </td>\n   <td>C++ unit tests\n   </td>\n  </tr>\n  <tr>\n   <td colspan=\"2\" ><strong><code>mlir_tests/</code></strong>\n   </td>\n   <td>MLIR-based unit tests\n   </td>\n  </tr>\n  <tr>\n   <td colspan=\"2\" ><strong><code>utils/</code></strong>\n   </td>\n   <td>Miscellaneous utilities, such as scripts for generating test ML models.\n   </td>\n  </tr>\n  <tr>\n   <td colspan=\"2\" ><strong><code>tools/</code></strong>\n   </td>\n   <td>Binaries including bef_executor, tfrt_translate etc.\n   </td>\n  </tr>\n  <tr>\n   <td colspan=\"2\" ><strong><code>backends/common/</code></strong>\n   </td>\n   <td>Library shared for different backends, e.g. eigen, dnn_op_utils.h\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>ops/</code>\n   </td>\n   <td>Shared library for op implementations across devices, e.g. metadata functions\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>compat/eigen/</code>\n   </td>\n   <td>Adapter library for eigen, used by multiple backends\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>utils/</code>\n   </td>\n   <td>Miscellaneous utilities, such as scripts for generating MLIR test code.\n   </td>\n  </tr>\n  <tr>\n   <td colspan=\"2\" ><strong><code>backends/cpu/</code></strong>\n   </td>\n   <td>CPU device infra and CPU ops and kernels\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>include/</code>\n   </td>\n   <td>CPU related public headers\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>lib/core_runtime/</code>\n   </td>\n   <td>CPU core_runtime infra, e.g. cpu_device\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>lib/ops</code>\n   </td>\n   <td>CPU ops\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>lib/kernels</code>\n   </td>\n   <td>CPU kernels\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>cpp_tests/</code>\n   </td>\n   <td>CPU infra unit tests\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>mlir_tests/</code>\n   </td>\n   <td>CPU mlir based tests\n   </td>\n  </tr>\n  <td colspan=\"2\" ><strong><code>backends/gpu/</code></strong>\n   </td>\n   <td>GPU infra and op/kernel implementations. We might split this directory into a separate repository at some point after the interface with the rest of TFRT infra becomes stable.\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>include/</code>\n   </td>\n   <td>GPU related public headers\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>lib/core_runtime/</code>\n   </td>\n   <td>GPU Core runtime infra\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>lib/memory</code>\n   </td>\n   <td>GPU memory abstraction\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>lib/stream</code>\n   </td>\n   <td>GPU stream abstraction and wrappers\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>lib/tensor</code>\n   </td>\n   <td>GPU tensor\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>lib/ops</code>\n   </td>\n   <td>GPU ops\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>lib/kernels</code>\n   </td>\n   <td>GPU kernels\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>lib/data</code>\n   </td>\n   <td>GPU kernels for input pipeline infrastructure\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>cpp_tests/</code>\n   </td>\n   <td>GPU infra unit tests\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>mlir_tests/</code>\n   </td>\n   <td>GPU mlir based tests\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td><code>tools/</code>\n   </td>\n   <td>Miscellaneous utilities\n   </td>\n  </tr>\n</table>\n\n## Contribution guidelines\n\nIf you want to contribute to TFRT, be sure to review the\n[contribution guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's\n[code of conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md).\nBy participating, you are expected to uphold this code of conduct.\n\n**Note:** TFRT is currently not open to contributions. TFRT developers are\ncurrently developing workflows and continuous integration for accepting\ncontributions. Once we are ready, we will update this page.\n\n## Continuous build status\n\n[![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/tf_runtime/ubuntu-clang.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/tf_runtime/ubuntu-clang.html)\n[![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/tf_runtime/ubuntu-gcc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/tf_runtime/ubuntu-gcc.html)\n\n## Contact\n\nSubscribe to the\n[TFRT mailing list](https://groups.google.com/a/tensorflow.org/d/forum/tfrt) for\ngeneral discussions about the runtime.\n\nWe use GitHub [issues](https://github.com/tensorflow/runtime/issues) to track\nbugs and feature requests.\n\n## License\n\n[Apache License 2.0](LICENSE)\n", "release_dates": []}, {"name": "rust", "description": "Rust language bindings for TensorFlow", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# <img alt=\"SIG Rust TensorFlow\" src=\"https://github.com/tensorflow/community/blob/master/sigs/logos/SIGRust.png\" width=\"340\"/>\n[![Version](https://img.shields.io/crates/v/tensorflow.svg)](https://crates.io/crates/tensorflow)\n[![Build status](https://github.com/tensorflow/rust/actions/workflows/ci.yml/badge.svg)](https://github.com/tensorflow/rust/actions/workflows/ci.yml)\n\nTensorFlow Rust provides idiomatic [Rust](https://www.rust-lang.org) language\nbindings for [TensorFlow](https://www.tensorflow.org).\n\n**Notice:** This project is still under active development and not guaranteed to have a\nstable API.\n\n* [Documentation](https://tensorflow.github.io/rust/tensorflow/)\n* [TensorFlow Rust Google Group](https://groups.google.com/a/tensorflow.org/forum/#!forum/rust)\n* [TensorFlow website](https://www.tensorflow.org)\n* [TensorFlow GitHub page](https://github.com/tensorflow/tensorflow)\n\n## Getting Started\nSince this crate depends on the TensorFlow C API, it needs to be downloaded or compiled first. This\ncrate will automatically download or compile the TensorFlow shared libraries for you, but it is also\npossible to manually install TensorFlow and the crate will pick it up accordingly.\n\n### Prerequisites\nIf the TensorFlow shared libraries can already be found on your system, they will be used.  If your\nsystem is x86-64 Linux or Mac, a prebuilt binary will be downloaded, and no special prerequisites\nare needed.\n\nOtherwise, the following dependencies are needed to compile and build this crate, which involves\ncompiling TensorFlow itself:\n\n - git\n - [bazel](https://bazel.build/)\n - Python Dependencies `numpy`, `dev`, `pip` and `wheel`\n - Optionally, CUDA packages to support GPU-based processing\n\nThe TensorFlow website provides detailed instructions on how to obtain and install said dependencies,\nso if you are unsure please [check out the docs](https://www.tensorflow.org/install/source)\n for further details.\n\nSome of the examples use TensorFlow code written in Python and require a full TensorFlow\ninstallation.\n\nThe minimum supported Rust version is 1.58.\n\n### Usage\nAdd this to your `Cargo.toml`:\n\n```toml\n[dependencies]\ntensorflow = \"0.21.0\"\n```\n\nand this to your crate root:\n\n```rust\nextern crate tensorflow;\n```\n\nThen run `cargo build -j 1`. The tensorflow-sys crate's \n[`build.rs`](https://github.com/tensorflow/rust/blob/f204b39/tensorflow-sys/build.rs#L44-L52)\nnow either downloads a pre-built, basic CPU only binary\n([the default](https://github.com/tensorflow/rust/pull/65))\nor compiles TensorFlow if forced to by an environment variable. If TensorFlow\nis compiled during this process, since the full compilation is very memory\nintensive, we recommend using the `-j 1` flag which tells cargo to use only one\ntask, which in turn tells TensorFlow to build with only one task. Though, if\nyou have a lot of RAM, you can obviously use a higher value.\n\nTo include the especially unstable API (which is currently the `expr` module),\nuse `--features tensorflow_unstable`.\n\nFor now, please see the [Examples](https://github.com/tensorflow/rust/tree/master/examples) for more\ndetails on how to use this binding.\n\n## Tensor Max Display\nWhen printing or debugging a tensor, it will print every element by default, this\ncan be modified by changing an environment variable:\n```bash\nTF_RUST_DISPLAY_MAX=5\n```\nWhich will truncate the values if they exceed the limit:\n\n```rust\nlet values: Vec<u64> = (0..100000).collect();\nlet t = Tensor::new(&[2, 50000]).with_values(&values).unwrap();\ndbg!(t);\n```\n```\nt = Tensor<u64> {\n    values: [\n        [0, 1, 2, 3, 4, ...],\n        ...\n    ],\n    dtype: uint64,\n    shape: [2, 50000]\n}\n```\n\n## GPU Support\n\nTo enable GPU support, use the `tensorflow_gpu` feature in your Cargo.toml:\n\n```\n[dependencies]\ntensorflow = { version = \"0.21.0\", features = [\"tensorflow_gpu\"] }\n```\n\n## Manual TensorFlow Compilation\n\nIf you want to work against unreleased/unsupported TensorFlow versions or use a build optimized for\nyour machine, manual compilation is the way to go.\n\nSee [tensorflow-sys/README.md](tensorflow-sys/README.md) for details.\n\n## FAQ's\n\n### Why does the compiler say that parts of the API don't exist?\nThe especially unstable parts of the API (which is currently the `expr` module) are\nfeature-gated behind the feature `tensorflow_unstable` to prevent accidental\nuse. See http://doc.crates.io/manifest.html#the-features-section.\n(We would prefer using an `#[unstable]` attribute, but that\n[doesn't exist](https://github.com/rust-lang/rfcs/issues/1491) yet.)\n\n### How do I...?\nTry the [documentation](https://tensorflow.github.io/rust/tensorflow/) first, and see if it answers\nyour question.  If not, take a look at the examples folder.  Note that there may not be an example\nfor your exact question, but it may be answered by an example demonstrating something else.\n\nIf none of the above help, you can ask your question on\n[TensorFlow Rust Google Group](https://groups.google.com/a/tensorflow.org/forum/#!forum/rust).\n\n## Contributing\nDevelopers and users are welcome to join the\n[TensorFlow Rust Google Group](https://groups.google.com/a/tensorflow.org/forum/#!forum/rust).\n\nPlease read the [contribution guidelines](CONTRIBUTING.md) on how to contribute code.\n\nThis is not an official Google product.\n\nRFCs are [issues tagged with RFC](https://github.com/tensorflow/rust/labels/rfc).\nCheck them out and comment. Discussions are welcomed. After all, that is the purpose of\nRequest For Comment!\n\n## License\nThis project is licensed under the terms of the [Apache 2.0 license](LICENSE).\n", "release_dates": []}, {"name": "serving", "description": "A flexible, high-performance serving system for machine learning models", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Serving\n\n[![Ubuntu Build Status](https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/ubuntu.svg)](https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/ubuntu.html)\n[![Ubuntu Build Status at TF HEAD](https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/ubuntu-tf-head.svg)](https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/ubuntu-tf-head.html)\n![Docker CPU Nightly Build Status](https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/docker-cpu-nightly.svg)\n![Docker GPU Nightly Build Status](https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/docker-gpu-nightly.svg)\n\n----\nTensorFlow Serving is a flexible, high-performance serving system for\nmachine learning models, designed for production environments. It deals with\nthe *inference* aspect of machine learning, taking models after *training* and\nmanaging their lifetimes, providing clients with versioned access via\na high-performance, reference-counted lookup table.\nTensorFlow Serving provides out-of-the-box integration with TensorFlow models,\nbut can be easily extended to serve other types of models and data.\n\nTo note a few features:\n\n-   Can serve multiple models, or multiple versions of the same model\n    simultaneously\n-   Exposes both gRPC as well as HTTP inference endpoints\n-   Allows deployment of new model versions without changing any client code\n-   Supports canarying new versions and A/B testing experimental models\n-   Adds minimal latency to inference time due to efficient, low-overhead\n    implementation\n-   Features a scheduler that groups individual inference requests into batches\n    for joint execution on GPU, with configurable latency controls\n-   Supports many *servables*: Tensorflow models, embeddings, vocabularies,\n    feature transformations and even non-Tensorflow-based machine learning\n    models\n\n## Serve a Tensorflow model in 60 seconds\n```bash\n# Download the TensorFlow Serving Docker image and repo\ndocker pull tensorflow/serving\n\ngit clone https://github.com/tensorflow/serving\n# Location of demo models\nTESTDATA=\"$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata\"\n\n# Start TensorFlow Serving container and open the REST API port\ndocker run -t --rm -p 8501:8501 \\\n    -v \"$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two\" \\\n    -e MODEL_NAME=half_plus_two \\\n    tensorflow/serving &\n\n# Query the model using the predict API\ncurl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n    -X POST http://localhost:8501/v1/models/half_plus_two:predict\n\n# Returns => { \"predictions\": [2.5, 3.0, 4.5] }\n```\n\n## End-to-End Training & Serving Tutorial\n\nRefer to the official Tensorflow documentations site for [a complete tutorial to train and serve a Tensorflow Model](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple).\n\n\n## Documentation\n\n### Set up\n\nThe easiest and most straight-forward way of using TensorFlow Serving is with\nDocker images. We highly recommend this route unless you have specific needs\nthat are not addressed by running in a container.\n\n*   [Install Tensorflow Serving using Docker](tensorflow_serving/g3doc/docker.md)\n    *(Recommended)*\n*   [Install Tensorflow Serving without Docker](tensorflow_serving/g3doc/setup.md)\n    *(Not Recommended)*\n*   [Build Tensorflow Serving from Source with Docker](tensorflow_serving/g3doc/building_with_docker.md)\n*   [Deploy Tensorflow Serving on Kubernetes](tensorflow_serving/g3doc/serving_kubernetes.md)\n\n### Use\n\n#### Export your Tensorflow model\n\nIn order to serve a Tensorflow model, simply export a SavedModel from your\nTensorflow program.\n[SavedModel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md)\nis a language-neutral, recoverable, hermetic serialization format that enables\nhigher-level systems and tools to produce, consume, and transform TensorFlow\nmodels.\n\nPlease refer to [Tensorflow documentation](https://www.tensorflow.org/guide/saved_model#save_and_restore_models)\nfor detailed instructions on how to export SavedModels.\n\n#### Configure and Use Tensorflow Serving\n\n* [Follow a tutorial on Serving Tensorflow models](tensorflow_serving/g3doc/serving_basic.md)\n* [Configure Tensorflow Serving to make it fit your serving use case](tensorflow_serving/g3doc/serving_config.md)\n* Read the [Performance Guide](tensorflow_serving/g3doc/performance.md)\nand learn how to [use TensorBoard to profile and optimize inference requests](tensorflow_serving/g3doc/tensorboard.md)\n* Read the [REST API Guide](tensorflow_serving/g3doc/api_rest.md)\nor [gRPC API definition](https://github.com/tensorflow/serving/tree/master/tensorflow_serving/apis)\n* [Use SavedModel Warmup if initial inference requests are slow due to lazy initialization of graph](tensorflow_serving/g3doc/saved_model_warmup.md)\n* [If encountering issues regarding model signatures, please read the SignatureDef documentation](tensorflow_serving/g3doc/signature_defs.md)\n* If using a model with custom ops, [learn how to serve models with custom ops](tensorflow_serving/g3doc/custom_op.md)\n\n### Extend\n\nTensorflow Serving's architecture is highly modular. You can use some parts\nindividually (e.g. batch scheduling) and/or extend it to serve new use cases.\n\n* [Ensure you are familiar with building Tensorflow Serving](tensorflow_serving/g3doc/building_with_docker.md)\n* [Learn about Tensorflow Serving's architecture](tensorflow_serving/g3doc/architecture.md)\n* [Explore the Tensorflow Serving C++ API reference](https://www.tensorflow.org/tfx/serving/api_docs/cc/)\n* [Create a new type of Servable](tensorflow_serving/g3doc/custom_servable.md)\n* [Create a custom Source of Servable versions](tensorflow_serving/g3doc/custom_source.md)\n\n## Contribute\n\n\n**If you'd like to contribute to TensorFlow Serving, be sure to review the\n[contribution guidelines](CONTRIBUTING.md).**\n\n\n## For more information\n\nPlease refer to the official [TensorFlow website](http://tensorflow.org) for\nmore information.\n", "release_dates": ["2023-11-21T22:22:47Z", "2023-10-31T16:19:29Z", "2023-10-23T20:23:39Z", "2023-09-29T21:29:45Z", "2023-10-19T21:51:41Z", "2023-07-24T04:28:54Z", "2023-07-20T20:40:06Z", "2023-07-18T20:47:53Z", "2023-07-18T18:40:28Z", "2023-07-14T20:30:46Z", "2023-05-02T04:07:15Z", "2023-04-27T21:24:06Z", "2023-03-23T22:14:21Z", "2022-11-23T23:21:43Z", "2022-11-22T21:55:47Z", "2022-11-22T21:55:00Z", "2022-11-23T23:21:39Z", "2022-11-03T17:33:48Z", "2022-10-26T22:25:44Z", "2022-10-21T06:01:32Z", "2022-09-12T17:36:01Z", "2022-09-12T21:59:43Z", "2022-09-12T17:35:16Z", "2022-09-12T17:34:13Z", "2022-08-31T18:41:58Z", "2022-08-24T18:15:48Z", "2022-08-16T20:34:21Z", "2022-08-05T18:10:26Z", "2022-08-03T04:29:04Z", "2022-07-11T18:05:37Z"]}, {"name": "sig-tfjs", "description": null, "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow.JS SIG repo \n\nA community repo for TFJS SIG projects.\n", "release_dates": []}, {"name": "similarity", "description": "TensorFlow Similarity is a python package focused on making similarity learning quick and easy.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Similarity: Metric Learning for Humans\n\nTensorFlow Similarity is a [TensorFlow](https://tensorflow.org) library for [similarity learning](https://en.wikipedia.org/wiki/Similarity_learning) which includes techniques such as self-supervised learning, metric learning, similarity learning, and contrastive learning. TensorFlow Similarity is still in beta and we may push breaking changes.\n\n## Introduction\n\nTensorflow Similarity offers state-of-the-art algorithms for metric learning along with all the necessary components to research, train, evaluate, and serve similarity and contrastive based models. These components include models, losses, metrics, samplers, visualizers, and indexing subsystems to make this quick and easy.\n\n![Example of nearest neighbors search performed on the embedding generated by a similarity model trained on the Oxford IIIT Pet Dataset.](https://raw.githubusercontent.com/tensorflow/similarity/master/assets/images/similar-cats-and-dogs.jpg)\n\nWith Tensorflow Similarity you can train two main types of models:\n\n1. **Self-supervised models**: Used to learn general data representations on unlabeled data to boost the accuracy of downstream tasks where you have few labels. For example, you can pre-train a model on a large number of unlabled images using one of the supported contrastive methods supported by TensorFlow Similarity, and then fine-tune it on a small labeled dataset to achieve higher accuracy. To get started training your own self-supervised model see this [notebook](examples/unsupervised_hello_world.ipynb).\n\n2. **Similarity models**: Output embeddings that allow you to find and cluster similar examples such as images representing the same object within a large corpus of examples. For instance, as visible above, you can train a similarity model to find and cluster similar looking, unseen cat and dog images from the [Oxford IIIT Pet Dataset](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet) while only training on a few of the dataset classes. To get started training your own similarity model see this [notebook](examples/supervised/visualization.ipynb).\n\n## What's new\n\n- [Mar 2023]: 0.17 more losses and metric and massive refactoring \n   * Added VicReg Loss to contrastive losses.\n   * Added metrics used in retrieval papers such as Precision@K\n   * Native support for distributed training e.g SimClr now works correctly with distributed training.\n   * Multi-modal embedding initial support (CLIP)\n\nFor more details and previous releases information - see [the changelog](./releases.md)\n\n## Getting Started\n\n### Installation\n\nUse pip to install the library.\n\n**NOTE**: The Tensorflow extra_require key can be omitted if you already have tensorflow>=2.4 installed.\n\n```shell\npip install --upgrade-strategy=only-if-needed tensorflow_similarity[tensorflow] \n```\n\n### Documentation\n\nThe detailed and narrated [notebooks](examples/) are a good way to get started with TensorFlow Similarity. There is likely to be one that is similar to your data or your problem (if not, let us know). You can start working with the examples immediately in Google Colab by clicking the Google Colab icon.\n\nFor more information about specific functions, you can [check the API documentation](api/)\n\nFor contributing to the project please check out the [contribution guidelines](CONTRIBUTING.md)\n\n### Minimal Example: MNIST similarity\n<details>\n   <summary> Click to expand and see how to train a supervised similarity model on mnist using TF.Similarity</summary>\n\nHere is a bare bones example demonstrating how to train a TensorFlow Similarity model on the MNIST data. This example illustrates some of the main components provided by TensorFlow Similarity and how they fit together. Please refer to the [hello_world notebook](examples/supervised_hello_world.ipynb) for a more detailed introduction.\n\n### Preparing data\n\nTensorFlow Similarity provides [data samplers](api/TFSimilarity/samplers/), for various dataset types, that balance the batches to ensure smoother training.\nIn this example, we are using the multi-shot sampler that integrates directly from the TensorFlow dataset catalog.\n\n```python\nfrom tensorflow_similarity.samplers import TFDatasetMultiShotMemorySampler\n\n# Data sampler that generates balanced batches from MNIST dataset\nsampler = TFDatasetMultiShotMemorySampler(dataset_name='mnist', classes_per_batch=10)\n```\n\n### Building a Similarity model\n\nBuilding a TensorFlow Similarity model is similar to building a standard Keras model, except the output layer is usually a [`MetricEmbedding()`](api/TFSimilarity/layers/) layer that enforces L2 normalization and the model is instantiated as a specialized subclass [`SimilarityModel()`](api/TFSimilarity/models/SimilarityModel.md) that supports additional functionality.\n\n```python\nfrom tensorflow.keras import layers\nfrom tensorflow_similarity.layers import MetricEmbedding\nfrom tensorflow_similarity.models import SimilarityModel\n\n# Build a Similarity model using standard Keras layers\ninputs = layers.Input(shape=(28, 28, 1))\nx = layers.experimental.preprocessing.Rescaling(1/255)(inputs)\nx = layers.Conv2D(64, 3, activation='relu')(x)\nx = layers.Flatten()(x)\nx = layers.Dense(64, activation='relu')(x)\noutputs = MetricEmbedding(64)(x)\n\n# Build a specialized Similarity model\nmodel = SimilarityModel(inputs, outputs)\n```\n\n### Training model via contrastive learning\n\nTo output a metric embedding, that are searchable via approximate nearest neighbor search, the model needs to be trained using a similarity loss. Here we are using the `MultiSimilarityLoss()`, which is one of the most efficient loss functions.\n\n```python\nfrom tensorflow_similarity.losses import MultiSimilarityLoss\n\n# Train Similarity model using contrastive loss\nmodel.compile('adam', loss=MultiSimilarityLoss())\nmodel.fit(sampler, epochs=5)\n```\n\n### Building images index and querying it\n\nOnce the model is trained, reference examples must be indexed via the model index API to be searchable. After indexing, you can use the model lookup API to search the index for the K most similar items.\n\n```python\nfrom tensorflow_similarity.visualization import viz_neigbors_imgs\n\n# Index 100 embedded MNIST examples to make them searchable\nsx, sy = sampler.get_slice(0,100)\nmodel.index(x=sx, y=sy, data=sx)\n\n# Find the top 5 most similar indexed MNIST examples for a given example\nqx, qy = sampler.get_slice(3713, 1)\nnns = model.single_lookup(qx[0])\n\n# Visualize the query example and its top 5 neighbors\nviz_neigbors_imgs(qx[0], qy[0], nns)\n```\n</details>\n\n## Supported Algorithms\n\n### Self-Supervised Models\n\n- SimCLR \n- SimSiam\n- Barlow Twins\n\n### Supervised Losses\n\n- Triplet Loss\n- PN Loss\n- Multi Sim Loss\n- Circle Loss\n- Soft Nearest Neighbor Loss\n\n### Metrics\n\nTensorflow Similarity offers many of the most common metrics used for [classification](api/TFSimilarity/classification_metrics/) and [retrieval](api/TFSimilarity/retrieval_metrics/) evaluation. Including:\n\n| Name | Type | Description |\n| ---- | ---- | ----------- |\n| Precision | Classification | |\n| Recall | Classification | |\n| F1 Score | Classification | |\n| Recall@K | Retrieval | |\n| Binary NDCG | Retrieval | |\n\n## Citing\n\nPlease cite this reference if you use any part of TensorFlow similarity in your research:\n\n```bibtex\n@article{EBSIM21,\n  title={TensorFlow Similarity: A Usable, High-Performance Metric Learning Library},\n  author={Elie Bursztein, James Long, Shun Lin, Owen Vallis, Francois Chollet},\n  journal={Fixme},\n  year={2021}\n}\n```\n\n## Disclaimer\n\nThis is not an official Google product.\n", "release_dates": ["2023-09-11T23:23:00Z", "2023-03-19T19:34:49Z", "2022-05-27T21:10:50Z", "2022-01-21T19:28:02Z", "2021-10-09T00:40:03Z", "2021-09-13T19:03:04Z"]}, {"name": "skflow", "description": "Simplified interface for TensorFlow (mimicking Scikit Learn) for Deep Learning", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2016-02-14T06:08:29Z"]}, {"name": "swift", "description": "Swift for TensorFlow", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<p align=\"center\">\n  <img src=\"images/logo.png\">\n</p>\n\n# Swift for TensorFlow (Archived)\n\nSwift for TensorFlow was an experiment in the next-generation platform for machine learning, incorporating the latest research across machine learning, compilers, differentiable programming, systems design, and beyond.  It was archived in February 2021.  Some significant achievements from this project include:\n\n* Added [language-integrated differentiable programming](https://forums.swift.org/t/differentiable-programming-for-gradient-based-machine-learning/42147) into the Swift language.  This work continues in the official Swift compiler. \n* Developed a mutable-value-semantics-oriented [deep learning API](https://github.com/tensorflow/swift-apis).\n* Fostered the development of [a model garden](https://github.com/tensorflow/swift-models) with more than [30 models from a variety of deep learning disciplines](https://github.com/tensorflow/swift-models#examples).\n* Enabled novel research that [combines deep learning with probabilistic graphical models](https://github.com/borglab/SwiftFusion) for 3D motion tracking and beyond.\n* Powered a(n almost) pure-Swift prototype of a [GPU+CPU runtime supporting parallel map](https://github.com/ewconnell/swiftrt).\n* Spun off multiple open source side efforts which continue to be under active development:\n  * [PythonKit](https://github.com/pvieito/PythonKit): Python interoperability with Swift.\n  * [swift-jupyter](https://github.com/google/swift-jupyter): Enables use of Swift within Jupyter notebooks.\n  * [swift-benchmark](https://github.com/google/swift-benchmark): Provides a robust benchmarking suite for Swift code.\n* Spun off several other open source efforts:\n  * [penguin](https://github.com/saeta/penguin): Parallel programming, data structures, graph algorithms, and more.\n  * [Tensors Fitting Perfectly](https://github.com/google-research/swift-tfp): Static analysis of tensor shape mismatches.\n* Swift Evolution proposals pitched, implemented, and accepted:\n  * [SE-0195](https://github.com/apple/swift-evolution/blob/main/proposals/0195-dynamic-member-lookup.md): User-defined \"Dynamic Member Lookup\" Types (`@dynamicMemberLookup`)\n  * [SE-0216](https://github.com/apple/swift-evolution/blob/main/proposals/0216-dynamic-callable.md): Introduce user-defined dynamically \"callable\" types (`@dynamicCallable`)\n  * [SE-0233](https://github.com/apple/swift-evolution/blob/main/proposals/0233-additive-arithmetic-protocol.md): Make `Numeric` refine a new `AdditiveArithmetic` protocol\n  * [SE-0253](https://github.com/apple/swift-evolution/blob/main/proposals/0253-callable.md): Callable values of user-defined nominal types (`func callAsFunction`)\n\nThis site will not receive further updates.  The API documentation and binary downloads will continue to be accessible as well as the [Open Design Review meeting recordings](https://docs.google.com/document/d/1Fm56p5rV1t2Euh6WLtBFKGqI43ozC3EIjReyLk-LCLU/edit).\n\n\n## Getting started\n\n### Using Swift for TensorFlow\n\n- **Google Colaboratory**: The fastest way to get started is to try out Swift\n   for TensorFlow right in your browser. Just open up [a tutorial](#tutorials-),\n   or start from a [blank notebook][blank_notebook]!\n   Read more in our [usage guide](Usage.md).\n\n- **Install locally**: You can [download a pre-built Swift for TensorFlow\n   package](Installation.md). After installation, you can follow these\n   [step-by-step instructions](Usage.md) to build and execute a Swift script on\n   your computer.\n\n- **Run on GCP**: You can spin up a GCE instance using a Swift for TensorFlow\n  [Deep Learning VM][dlvm] image, with all drivers and the toolchain\n  pre-installed. Instructions can be found in the\n  [Installation Guide](Installation.md).\n\n- **Compile from source**: If you'd like to customize Swift for TensorFlow or\n   contribute back, follow our [instructions][instructions]\n   on building Swift for TensorFlow from source.\n\n### Tutorials ![](https://www.tensorflow.org/images/colab_logo_32px.png)\n\nTutorial | Last Updated |\n-------- | ------------ |\n[A Swift Tour](https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/a_swift_tour.ipynb) | March 2019\n[Protocol-Oriented Programming & Generics](https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/protocol_oriented_generics.ipynb) | August 2019\n[Python Interoperability](https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/python_interoperability.ipynb) | March 2019\n[Custom Differentiation](https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/custom_differentiation.ipynb) | March 2019\n[Sharp Edges in Differentiability](https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/Swift_autodiff_sharp_edges.ipynb) | November 2020\n[Model Training Walkthrough](https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/model_training_walkthrough.ipynb) | March 2019\n[Raw TensorFlow Operators](https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/raw_tensorflow_operators.ipynb) | December 2019\n[Introducing X10, an XLA-Based Backend](https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/introducing_x10.ipynb) | May 2020\n\n### Resources\n\n- [Models and Examples](https://github.com/tensorflow/swift-models)\n- [TensorFlow Swift API Reference](https://www.tensorflow.org/swift/api_docs/Structs/Tensor)\n- [Release Notes](RELEASES.md)\n- [Known Issues](KNOWN_ISSUES.md)\n- [Frequently Asked Questions](FAQ.md)\n- [TensorFlow Blog Posts](https://blog.tensorflow.org/search?label=Swift)\n\n### Forums\n\nThe discussions happened on the\n[swift@tensorflow.org mailing list](https://groups.google.com/a/tensorflow.org/d/forum/swift).\n\n## Why Swift for TensorFlow?\n\nSwift for TensorFlow is a new way to develop machine learning models. It\ngives you the power of\n[TensorFlow](https://www.tensorflow.org) directly integrated into the\n[Swift programming language](https://swift.org/about). We believe that\nmachine learning paradigms are so important that they deserve\n**first-class language and compiler support**.\n\nA fundamental primitive in machine learning is gradient-based optimization:\ncomputing function derivatives to optimize parameters. With Swift for\nTensorFlow, you can easily differentiate functions using differential\noperators like [`gradient(of:)`](https://www.tensorflow.org/swift/api_docs/Functions#/s:10TensorFlow8gradient2of15CotangentVectorQzxcq_xc_tAA14DifferentiableRzSFR_AaFR_AdaFPQy_Rs_r0_lF), or differentiate with respect to an entire\nmodel by calling method [`gradient(in:)`](https://www.tensorflow.org/swift/api_docs/Protocols/Differentiable#/s:10TensorFlow14DifferentiablePAAE8gradient2in15CotangentVectorQzqd__xXE_tSFRd__AaBRd__AfCQyd__Rsd__lF). These differentiation APIs\nare not just available for `Tensor`-related concepts\u2014they are\ngeneralized for all types that conform to the [`Differentiable`](https://www.tensorflow.org/swift/api_docs/Protocols/Differentiable)\nprotocol, including `Float`, `Double`, SIMD vectors, and your own data\nstructures.\n\n```swift\n// Custom differentiable type.\nstruct Model: Differentiable {\n    var w: Float\n    var b: Float\n    func applied(to input: Float) -> Float {\n        return w * input + b\n    }\n}\n\n// Differentiate using `gradient(at:_:in:)`.\nlet model = Model(w: 4, b: 3)\nlet input: Float = 2\nlet (\ud835\udec1model, \ud835\udec1input) = gradient(at: model, input) { model, input in\n    model.applied(to: input)\n}\n\nprint(\ud835\udec1model) // Model.TangentVector(w: 2.0, b: 1.0)\nprint(\ud835\udec1input) // 4.0\n```\n\nBeyond derivatives, the Swift for TensorFlow project comes with a sophisticated toolchain\nto make users more productive. You can run Swift interactively in a Jupyter\nnotebook, and get helpful autocomplete suggestions to help you explore the\nmassive API surface of a modern deep learning library. You can [get started\nright in your browser in\nseconds](https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/model_training_walkthrough.ipynb)!\n\nMigrating to Swift for TensorFlow is really easy thanks to Swift's powerful\nPython integration. You can incrementally migrate your Python code over (or\ncontinue to use your favorite Python libraries), because you can easily call\nyour favorite Python library with a familiar syntax:\n\n```swift\nimport TensorFlow\nimport Python\n\nlet np = Python.import(\"numpy\")\n\nlet array = np.arange(100).reshape(10, 10)  // Create a 10x10 numpy array.\nlet tensor = Tensor<Float>(numpy: array)  // Seamless integration!\n```\n\n## Documentation\n\n> Beware: the project is moving very quickly, and thus some of these documents\n> are slightly out of date as compared to the current state-of-the-art.\n\n### Overview\n\nDocument | Last Updated | Status |\n-------- | ------------ | ------ |\n[Why *Swift* for TensorFlow?](docs/WhySwiftForTensorFlow.md) | April 2018 | Current\n[Swift for TensorFlow Design Overview](docs/DesignOverview.md) | April 2018 | Outdated\n[Supported Backends](docs/SupportedBackends.md) | May 2020 | Current\n\n### Technology deep dive\n\nThe Swift for TensorFlow project builds on top of powerful theoretical\nfoundations. For insight into some of the underlying technologies, check\nout the following documentation.\n\nDocument | Last Updated | Status |\n-------- | ------------ | ------ |\n[Swift Differentiable Programming Manifesto](https://github.com/apple/swift/blob/main/docs/DifferentiableProgramming.md) | January 2020 | Current\n[Swift Differentiable Programming Implementation Overview](https://docs.google.com/document/d/1_BirmTqdotglwNTOcYAW-ib6mx_jl-gH9Dbg4WmHZh0) | August 2019 | Current\n[Swift Differentiable Programming Design Overview](https://docs.google.com/document/d/1bPepWLfRQa6CtXqKA8CDQ87uZHixNav-TFjLSisuKag/edit?usp=sharing) | June 2019 | Outdated\n[Differentiable Types](docs/DifferentiableTypes.md) | March 2019 | Outdated\n[Differentiable Functions and Differentiation APIs](docs/DifferentiableFunctions.md) | March 2019 | Outdated\n[Dynamic Property Iteration using Key Paths](docs/DynamicPropertyIteration.md) | March 2019 | Current\n[Hierarchical Parameter Iteration and Optimization](docs/ParameterOptimization.md) | March 2019 | Current\n[First-Class Automatic Differentiation in Swift: A Manifesto](https://gist.github.com/rxwei/30ba75ce092ab3b0dce4bde1fc2c9f1d) | October 2018 | Outdated\n[Automatic Differentiation Whitepaper](docs/AutomaticDifferentiation.md) | April 2018 | Outdated\n[Python Interoperability](docs/PythonInteroperability.md) | April 2018 | Current\n[Graph Program Extraction](docs/GraphProgramExtraction.md) | April 2018 | Outdated\n\n## Source code\n\nCompiler and standard library development happens on the `main` branch of the\n[apple/swift](https://github.com/apple/swift/tree/main) repository.\n\nAdditional code repositories that make up the core of the project include:\n\n - [Deep learning library](https://github.com/tensorflow/swift-apis): high-level\n   API familiar to Keras users.\n\n> Swift for TensorFlow is **no longer** a fork of the official Swift language;\n> development was previously done on the `tensorflow` branch of the\n> [apple/swift](https://github.com/apple/swift/tree/tensorflow) repository.\n> Language additions were designed to fit with the direction of Swift and are\n> going through the [Swift Evolution](https://github.com/apple/swift-evolution)\n> process.\n\n### Jupyter Notebook support\n\n[Jupyter Notebook](http://jupyter.org/) support for Swift is under development at\n[google/swift-jupyter](https://github.com/google/swift-jupyter).\n\n### Model garden\n\n[tensorflow/swift-models](https://github.com/tensorflow/swift-models) is a\nrepository of machine learning models built with Swift for TensorFlow. It\nintended to provide examples of how to use Swift for TensorFlow, to allow for\nend-to-end tests of machine learning APIs, and to host model benchmarking\ninfrastructure.\n\n### SwiftAI\n\n[fastai/swiftai](https://github.com/fastai/swiftai) is a high-level API for\nSwift for TensorFlow, modeled after the\n[fastai Python library](https://github.com/fastai/fastai).\n\n## Community\n\nSwift for TensorFlow discussions happen on the\n[swift@tensorflow.org mailing list](https://groups.google.com/a/tensorflow.org/d/forum/swift).\n\n### Bugs reports and feature requests\n\nBefore reporting an issue, please check the [Frequently Asked Questions](FAQ.md)\nto see if your question has already been addressed.\n\nFor questions about general use or feature requests, please send an email to\nthe [mailing list](mailto:swift@tensorflow.org) or search for relevant issues\nin the [JIRA issue tracker](https://bugs.swift.org/projects/TF/issues/?filter=allopenissues).\n\nFor the most part, the core team's development is also tracked in\n[JIRA](https://bugs.swift.org/secure/RapidBoard.jspa?rapidView=17&projectKey=TF&view=planning).\n\n### Contributing\n\nWe welcome contributions from everyone. Read the [contributing\nguide](Contributing.md) for information on how to get started.\n\n### Code of conduct\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of\nexperience, education, socio-economic status, nationality, personal appearance,\nrace, religion, or sexual identity and orientation.\n\nThe Swift for TensorFlow community is guided by our [Code of\nConduct](CODE_OF_CONDUCT.md), which we encourage everybody to read before\nparticipating.\n\n[blank_notebook]: https://colab.research.google.com/notebook#create=true&language=swift\n[dlvm]: https://cloud.google.com/ai-platform/deep-learning-vm/docs\n[instructions]: https://github.com/apple/swift/tree/tensorflow#building-swift-for-tensorflow\n", "release_dates": []}, {"name": "swift-apis", "description": "Swift for TensorFlow Deep Learning Library", "language": "Swift", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Swift for TensorFlow Deep Learning Library\n\nGet a taste of *protocol-oriented differentiable programming*.\n\nThis repository hosts [Swift for TensorFlow][s4tf]'s deep learning library,\navailable both as a part of Swift for TensorFlow toolchains and as a Swift\npackage.\n\n## Usage\n\nThis library is being [automatically integrated][integrated] in Swift for\nTensorFlow toolchains. You do not need to add this library as a Swift Package\nManager dependency.\n\n### Use Google Colaboratory\n\n[**Open an empty Colaboratory now**][blank_colab] to try out Swift,\nTensorFlow, differentiable programming, and deep learning.\n\n> For detailed usage and troubleshooting, see [Usage][usage] on the Swift for\nTensorFlow project homepage.\n\n#### Define a model\n\nSimply import `TensorFlow` to get the full power of TensorFlow.\n\n```swift\nimport TensorFlow\n\nlet hiddenSize: Int = 10\n\nstruct Model: Layer {\n    var layer1 = Dense<Float>(inputSize: 4, outputSize: hiddenSize, activation: relu)\n    var layer2 = Dense<Float>(inputSize: hiddenSize, outputSize: hiddenSize, activation: relu)\n    var layer3 = Dense<Float>(inputSize: hiddenSize, outputSize: 3, activation: identity)\n    \n    @differentiable\n    func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n        return input.sequenced(through: layer1, layer2, layer3)\n    }\n}\n```\n\n#### Initialize a model and an optimizer\n\n```swift\nvar classifier = Model()\nlet optimizer = SGD(for: classifier, learningRate: 0.02)\nContext.local.learningPhase = .training\n// Dummy data.\nlet x: Tensor<Float> = Tensor(randomNormal: [100, 4])\nlet y: Tensor<Int32> = Tensor(randomUniform: [100])\n```\n\n#### Run a training loop\n\nOne way to define a training epoch is to use the\n[`gradient(at:in:)`][gradient] function.\n\n```swift\nfor _ in 0..<1000 {\n    let \ud835\udec1model = gradient(at: classifier) { classifier -> Tensor<Float> in\n        let \u0177 = classifier(x)\n        let loss = softmaxCrossEntropy(logits: \u0177, labels: y)\n        print(\"Loss: \\(loss)\")\n        return loss\n    }\n    optimizer.update(&classifier, along: \ud835\udec1model)\n}\n```\n\nAnother way is to make use of methods on `Differentiable` or `Layer` that\nproduce a backpropagation function. This allows you to compose your derivative\ncomputation with great flexibility.\n\n```swift\nfor _ in 0..<1000 {\n    let (\u0177, backprop) = classifier.appliedForBackpropagation(to: x)\n    let (loss, \ud835\udec1\u0177) = valueWithGradient(at: \u0177) { \u0177 in softmaxCrossEntropy(logits: \u0177, labels: y) }\n    print(\"Model output: \\(\u0177), Loss: \\(loss)\")\n    let (\ud835\udec1model, _) = backprop(\ud835\udec1\u0177)\n    optimizer.update(&classifier, along: \ud835\udec1model)\n}\n```\n\nFor more models, go to [**tensorflow/swift-models**][swift-models].\n\n## Development\n\nDocumentation covering development can be found in the [Developer Guide](Documentation/Development.md).\n\n## Bugs\n\nPlease report bugs and feature requests using GitHub issues in this repository.\n\n## Community\n\nDiscussion about Swift for TensorFlow happens on the\n[swift@tensorflow.org][forum]\nmailing list.\n\n## Contributing\n\nWe welcome contributions: please read the [Contributor Guide](CONTRIBUTING.md)\nto get started. It's always a good idea to discuss your plans on the mailing\nlist before making any major submissions.\n\n## Code of Conduct\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of\nexperience, education, socio-economic status, nationality, personal appearance,\nrace, religion, or sexual identity and orientation.\n\nThe Swift for TensorFlow community is guided by our [Code of\nConduct](CODE_OF_CONDUCT.md), which we encourage everybody to read before\nparticipating.\n\n[s4tf]: https://github.com/tensorflow/swift\n[integrated]: https://github.com/apple/swift/tree/tensorflow#customize-tensorflow-support\n[blank_colab]: https://colab.research.google.com/notebook#create=true&language=swift\n[usage]: https://github.com/tensorflow/swift/blob/main/Usage.md\n[gradient]: https://www.tensorflow.org/swift/api_docs/Functions#/s:10TensorFlow8gradient2at2in13TangentVectorQzx_AA0A0Vyq_GxXEtAA14DifferentiableRzAA0aB13FloatingPointR_r0_lF\n[swift-models]: https://github.com/tensorflow/swift-models\n[toolchain]: https://github.com/tensorflow/swift/blob/main/Installation.md\n[forum]: https://groups.google.com/a/tensorflow.org/d/forum/swift\n", "release_dates": ["2019-02-28T23:37:57Z", "2019-02-28T19:39:10Z"]}, {"name": "swift-bindings", "description": null, "language": "Swift", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Swift for TensorFlow Ops Bindings\n\nThis repository is now deprecated. Bindings have been moved to [tensorflow/swift-apis](https://github.com/tensorflow/swift-apis/tree/master/Sources/TensorFlow/Bindings).\n", "release_dates": []}, {"name": "swift-models", "description": "Models and examples built with Swift for TensorFlow", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Swift for TensorFlow Models\n\nThis repository contains many examples of how Swift for TensorFlow can be used to build machine\nlearning applications, as well as the models, datasets, and other components required to build them.\nThese examples are intended to demonstrate best practices for the use of \n[Swift for TensorFlow APIs](https://github.com/tensorflow/swift-apis) and act as end-to-end tests\nto validate the function and performance of those APIs.\n\nActive development occurs on the `main` branch, and that is kept current against the `main` branch\nof [the Swift compiler](https://github.com/apple/swift) and the `main` branch of [the Swift for TensorFlow APIs](https://github.com/tensorflow/swift-apis).\n\nFor stable snapshots, use the ```tensorflow-xx``` branch that corresponds to the toolchain you are using from the [Swift for TensorFlow releases](https://github.com/tensorflow/swift/blob/master/Installation.md#releases).  For example, for the 0.12 release, use the ```tensorflow-0.12``` branch.\n\nTo learn more about Swift for TensorFlow development, please visit\n[tensorflow/swift](https://github.com/tensorflow/swift).\n\n## Examples\n\nThe examples within this repository are all designed to be run as standalone applications. The easiest way to do this is to use Swift Package Manager to build and run individual examples. This \ncan be accomplished by changing to the root directory of the project and typing something like\n\n```bash\nswift run -c release [Example] [Options]\n```\n\nFor Windows, an additional flag may be required:\n\n```cmd\nswift run -Xswiftc -use-ld=lld -c release [Example] [Options]\n```\n\nThis will build and run a specific example in the release configuration. Due to significant\nperformance differences between debug and release builds in Swift, we highly recommend running the\nexamples from a release build. Some examples have additional command-line options, and those will\nbe described in the example's README.\n\nThe following is a catalog of the current examples, grouped by subject area, with links to their\nlocation within the project. Each example should have documentation for what it is demonstrating\nand how to use it.\n\n### Image classification\n\n- [A custom model training against CIFAR-10](Examples/Custom-CIFAR10)\n- [LeNet-5 training against MNIST](Examples/LeNet-MNIST)\n- [MobileNet V1 training against Imagenette](Examples/MobileNetV1-Imagenette)\n- [MobileNet V2 training against Imagenette](Examples/MobileNetV2-Imagenette)\n- [ResNet-56 training against CIFAR-10](Examples/ResNet-CIFAR10)\n- [ResNet-50 training against ImageNet](Examples/ResNet50-ImageNet)\n- [VGG-16 training against Imagewoof](Examples/VGG-Imagewoof)\n\n### Text\n\n- [BERT training against CoLA](Examples/BERT-CoLA)\n- [Pretrained GPT-2 performing text generation](Examples/GPT2-Inference)\n- [GPT-2 training against WikiText-2](Examples/GPT2-WikiText2)\n- [WordSeg](Examples/WordSeg)\n\n### Generative models\n\n- [1-D Autoencoder](Autoencoder/Autoencoder1D)\n- [2-D Autoencoder](Autoencoder/Autoencoder2D)\n- [1-D Variational Autoencoder](Autoencoder/VAE1D)\n- [CycleGAN](CycleGAN)\n- [GAN](GAN)\n- [DCGAN](DCGAN)\n- [pix2pix](pix2pix)\n\n### Reinforcement learning\n\n- [Blackjack](Gym)\n- [CartPole](Gym)\n- [Catch](Catch)\n- [FrozenLake](Gym)\n- [MiniGo](MiniGo)\n\n### Standalone\n\n- [Differentiable Shallow Water PDE Solver](Examples/Shallow-Water-PDE)\n- [Fast Style Transfer](FastStyleTransfer)\n- [Fractals](Examples/Fractals)\n- [Growing Neural Cellular Automata](Examples/GrowingNeuralCellularAutomata)\n- [Neural Collaborative Filtering using MovieLens](Examples/NeuMF-MovieLens)\n- [PersonLab Human Pose Estimator](PersonLab)\n- [Regression using BostonHousing](Examples/Regression-BostonHousing)\n\n## Components\n\nBeyond examples that use Swift for TensorFlow, this repository also contains reusable components\nfor constructing machine learning applications. These components reside in modules that can be\nimported into separate Swift projects and used by themselves.\n\nThese components provide standalone machine learning models, datasets, image loading and saving,\nTensorBoard integration, and a training loop abstraction, among other capabilities.\n\nThe Swift for TensorFlow models repository has acted as a staging ground for experimental\ncapabilities, letting us evaluate new components and interfaces before elevating them into the core\nSwift for TensorFlow APIs. As a result, the design and interfaces of these components may change\nregularly.\n\n### Models\n\nSeveral modules are provided that contain reusable Swift models for image classification, text\nprocessing, and more. These modules are used within the example applications to demonstrate the\ncapabilities of these models, but they can also be imported into many other projects.\n\n#### Image classification\n\nMany common image classification models are present within\n[the ImageClassificationModels module](Models/ImageClassification). To use them within a Swift\nproject, add ImageClassificationModels as a dependency and import the module:\n\n```swift\nimport ImageClassificationModels\n```\n\nThese models include:\n\n- DenseNet121\n- EfficientNet\n- LeNet-5\n- MobileNetV1\n- MobileNetV2\n- MobileNetV3\n- ResNet\n- ResNetV2\n- ShuffleNetV2\n- SqueezeNet\n- VGG\n- WideResNet\n- Xception\n\n#### Recommendation\n\nSeveral recommendation models are present within\n[the RecommendationModels module](Models/Recommendation). To use them within a Swift\nproject, add RecommendationModels as a dependency and import the module:\n\n```swift\nimport RecommendationModels\n```\n\nThese models include:\n\n- DLRM\n- MLP\n- NeuMF\n\n#### Text\n\nSeveral text models are present within\n[the TextModels module](Models/Text). To use them within a Swift\nproject, add TextModels as a dependency and import the module:\n\n```swift\nimport TextModels\n```\n\nThese models include:\n\n- [BERT](Models/Text/BERT)\n- [GPT-2](Models/Text/GPT2)\n- [WordSeg](Models/Text/WordSeg)\n\n### Datasets\n\nIn addition to the machine learning model itself, a dataset is usually required when training.\nSwift wrappers have been built for many common datasets to ease their use within machine learning\napplications. Most of these use the\n[Epochs](https://github.com/tensorflow/swift-apis/tree/main/Sources/TensorFlow/Epochs) API that \nprovides a generalized abstraction of common dataset operations.\n\nThe [Datasets](Datasets) module provides these wrappers. To use them within a Swift\nproject, add Datasets as a dependency and import the module:\n\n```swift\nimport Datasets\n```\n\nThese are the currently provided dataset wrappers:\n\n- [BostonHousing](Datasets/BostonHousing)\n- [CIFAR-10](Datasets/CIFAR10)\n- [MS COCO](Datasets/COCO)\n- [CoLA](Datasets/CoLA)\n- [ImageNet](Datasets/Imagenette)\n- [Imagenette](Datasets/Imagenette)\n- [Imagewoof](Datasets/Imagenette)\n- [FashionMNIST](Datasets/MNIST)\n- [KuzushijiMNIST](Datasets/MNIST)\n- [MNIST](Datasets/MNIST)\n- [MovieLens](Datasets/MovieLens)\n- [Oxford-IIIT Pet](Datasets/OxfordIIITPets)\n- [WordSeg](Datasets/WordSeg)\n\n### Model checkpoints\n\nModel saving and loading is provided by the [Checkpoints](Checkpoints) module. To use the model\ncheckpointing functionality, add Checkpoints as a dependency and import the module:\n\n```swift\nimport Checkpoints\n```\n\n### Image loading and saving\n\nThe [ModelSupport](Support) module contains many shared utilites that are needed within the Swift\nmachine learning examples. This includes the loading, saving, and processing of\n[still images](Support/Image.swift) via the\n[stb_image](https://github.com/nothings/stb) library.\n[Animated images](Support/AnimatedImage.swift) can also be written out as GIF files from multiple\ntensors.\n\nExperimental support for libjpeg-turbo as an accelerated image loader [is present](ImageLoader), \nbut has not yet been incorporated into the main image loading capabilities.\n\n### Generalized training loop\n\nA generalized training loop that can be customized via callbacks is provided within the \n[TrainingLoop](TrainingLoop) module. All of the image classification examples use this training\nloop, with the exception of the Custom-CIFAR10 example that demonstrates how to define your own\ntraining loop from scratch. Other examples are being gradually converted to use this training loop.\n\n### TensorBoard integration\n\n[TensorBoard](https://www.tensorflow.org/tensorboard) integration is provided in the\n[TensorBoard module](TensorBoard) as a callback for the generalized training loop. TensorBoard \nlets you visualize the progress of your model as it trains by plotting model statistics as they\nupdate, or to review the training process afterward.\n\nThe [GPT2-WikiText2](Examples/GPT2-WikiText2) example demonstrates how this can be used when\ntraining your own models.\n\n## Benchmarks and tests\n\nA core goal of this repository is to validate the proper function of the Swift for TensorFlow APIs.\nIn addition to the models and end-to-end applications present within this project, a suite of\nbenchmarks and unit tests reside here.\n\nThe benchmarks are split into a core of functionality, the\n[SwiftModelsBenchmarksCore](SwiftModelsBenchmarksCore) module, and a \n[Benchmarks](SwiftModelsBenchmarks) command-line application for running these benchmarks. Refer to\nthe [documentation](SwiftModelsBenchmarks) for how to run the benchmarks on your system.\n\nThe [unit tests](Tests) verify functionality within models, datasets and other components. To run\nthem using Swift Package Manager on macOS or Linux:\n\n```bash\nswift test\n```\n\nand to run them on Windows:\n\n```cmd\nswift test -Xswiftc -use-ld=lld -c debug\n```\n\n## Using CMake for Development\n\nIn addition to Swift Package Manager, CMake can be used to build and run Swift for TensorFlow\nmodels.\n\n### *Experimental* CMake Support\n\nThere is experimental support for building with CMake.  This can be used to cross-compile the models and the demo programs.\n\nIt is highly recommended that you use CMake 3.16 or newer to ensure that `-B`\nand parallel builds function properly in the example commands below. To install\nthis version on Ubuntu, we recommend following the instructions at\n[Kitware's apt repo](https://apt.kitware.com/).\n\n**Prerequisite:** [Ninja build tool](https://ninja-build.org/). Find\ninstallation commands for your favorite package manager\n[here](https://github.com/ninja-build/ninja/wiki/Pre-built-Ninja-packages).\n\nmacOS:\n\n```\n# Configure\ncmake                                                              \\\n  -B /BinaryCache/tensorflow-swift-models                          \\\n  -D BUILD_TESTING=YES                                             \\\n  -D CMAKE_BUILD_TYPE=Release                                      \\\n  -D CMAKE_Swift_COMPILER=$(TOOLCHAINS=tensorflow xcrun -f swiftc) \\\n  -G Ninja                                                         \\\n  -S /SourceCache/tensorflow-swift-models\n# Build\ncmake --build /BinaryCache/tensorflow-swift-models\n# Test\ncmake --build /BinaryCache/tensorflow-swift-models --target test\n```\n\nLinux:\n\n```\n# Configure\ncmake                                     \\\n  -B /BinaryCache/tensorflow-swift-models \\\n  -D BUILD_TESTING=NO                     \\\n  -D CMAKE_BUILD_TYPE=Release             \\\n  -D CMAKE_Swift_COMPILER=$(which swiftc) \\\n  -G Ninja                                \\\n  -S /SourceCache/tensorflow-swift-models\n# Build\ncmake --build /BinaryCache/tensorflow-swift-models\n```\n\nWindows:\n\n```\nset DEVELOPER_LIBRARY_DIR=%SystemDrive%/Library/Developer/Platforms/Windows.platform/Developer/Library\n:: Configure\n\"%ProgramFiles%\\CMake\\bin\\cmake.exe\"                                                                                                                                                   ^\n  -B %SystemDrive%/BinaryCache/tensorflow-swift-models                                                                                                                                 ^\n  -D BUILD_SHARED_LIBS=YES                                                                                                                                                             ^\n  -D BUILD_TESTING=YES                                                                                                                                                                 ^\n  -D CMAKE_BUILD_TYPE=Release                                                                                                                                                          ^\n  -D CMAKE_Swift_COMPILER=%SystemDrive%/Library/Developer/Toolchains/unknown-Asserts-development.xctoolchain/usr/bin/swiftc.exe                                                        ^\n  -D CMAKE_Swift_FLAGS=\"-sdk %SDKROOT% -I %DEVELOPER_LIBRARY_DIR%/XCTest-development/usr/lib/swift/windows/x86_64 -L %DEVELOPER_LIBRARY_DIR%/XCTest-development/usr/lib/swift/windows\" ^\n  -G Ninja                                                                                                                                                                             ^\n  -S %SystemDrive%/SourceCache/tensorflow-swift-models\n:: Build\n\"%ProgramFiles%\\CMake\\bin\\cmake.exe\" --build %SystemDrive%/BinaryCache/tensorflow-swift-models\n:: Test\n\"%ProgramFiles%\\CMake\\bin\\cmake.exe\" --build %SystemDrive%/BinaryCache/tensorflow-swift-models --target test\n```\n\n## Bugs\n\nPlease report model-related bugs and feature requests using GitHub issues in\nthis repository.\n\n## Community\n\nDiscussion about Swift for TensorFlow happens on the\n[swift@tensorflow.org](https://groups.google.com/a/tensorflow.org/d/forum/swift)\nmailing list.\n\n## Contributing\n\nWe welcome contributions: please read the [Contributor Guide](CONTRIBUTING.md)\nto get started. It's always a good idea to discuss your plans on\n[the mailing list](https://groups.google.com/a/tensorflow.org/d/forum/swift) before making\nany major submissions.\n\nWe have labeled some issues as [\"good first issue\"](https://github.com/tensorflow/swift-models/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)\nor [\"help wanted\"](https://github.com/tensorflow/swift-models/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\nto provide some suggestions for where new contributors might be able to start.\n\n## Code of Conduct\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of\nexperience, education, socio-economic status, nationality, personal appearance,\nrace, religion, or sexual identity and orientation.\n\nThe Swift for TensorFlow community is guided by our [Code of\nConduct](CODE_OF_CONDUCT.md), which we encourage everybody to read before\nparticipating.\n", "release_dates": []}, {"name": "tcav", "description": "Code for the TCAV ML interpretability project", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) \n\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda\nViegas, Rory Sayres\n\nICML Paper: https://arxiv.org/abs/1711.11279\n\n## What is TCAV?\n\nTesting with Concept Activation Vectors (TCAV) is a new interpretability method\nto understand what signals your neural networks models uses for prediction.\n\n### What's special about TCAV compared to other methods?\n\nTypical interpretability methods show importance weights in each input feature\n(e.g, pixel). TCAV instead shows importance of high level concepts (e.g., color,\ngender, race) for a prediction class - this is how humans communicate!\n\nTypical interpretability methods require you to have one particular image that\nyou are interested in understanding. TCAV gives an explanation that is generally\ntrue for a class of interest, beyond one image (global explanation).\n\nFor example, for a given class, we can show how much race or gender was\nimportant for classifications in InceptionV3. Even though neither race nor\ngender labels were part of the training input!\n\n### Cool, where do these concepts come from?\n\nTCAV learns concepts from examples. For instance, TCAV needs a couple of\nexamples of female, and something not female to learn a \"gender\" concept. We\nhave tested a variety of concepts: color, gender, race, textures and many\nothers.\n\n### Why use high level concepts instead of input features?\n\nHumans think and communicate using concepts, and not using numbers (e.g.,\nweights to each feature). When there are lots of numbers to combine and reason\nabout (many features), it becomes harder and harder for humans to make sense of\nthe information they are accounting for. TCAV instead delivers explanations in\nthe way humans communicate to each other.\n\n### The consumer of the explanation may not know machine learning too well. Can they understand the explanation?\n\nYes. TCAV is designed to make sense to everyone - as long as they can understand\nthe high level concept!\n\n### Sounds good. Do I need to change my network to use TCAV?\nNo. You don't need to change or retrain your network to use TCAV.\n\n## Installation\n\nTensorflow must be installed to use TCAV. But it isn't included in the TCAV pip\npackage install_requires as a user may wish to use it with either the tensorflow\nor tensorflow-gpu package. So please pip install tensorflow or tensorflow-gpu as\nwell as the tcav package.\n\n> pip install tcav\n\n### Requirements\n\nSee requirements.txt for a list of python dependencies used in testing TCAV.\nThese will all be installed during pip installation of tcav with the exception\nof tensorflow, as mentioned above.\n\n## How to use TCAV\n\nSee Run TCAV.ipynb for step by step guide, after pip installing the tcav\npackage.\n\n```python\nmytcav = tcav.TCAV(sess,\n                   target,\n                   concepts,\n                   bottlenecks,\n                   act_gen,\n                   alphas,\n                   cav_dir=cav_dir,\n                   num_random_exp=2)\n\nresults = mytcav.run()\n```\n\n## TCAV for discrete models\n\nWe provide a simple example of how to run TCAV on models trained on discrete,\nnon-image data. Please see\n\n```\ncd tcav/tcav_examples/discrete/\n```\n\nYou can also find a Jupyter notebook for a model trained on KDD99 in here:\n\n```\ntcav/tcav_examples/discrete/kdd99_discrete_example.ipynb.\n```\n\n## Requirements\n\n-   tensorflow\n-   numpy\n-   Pillow\n-   matplotlib\n-   scikit-learn\n-   scipy\n\n## How to run unit tests\n\n`python -m tcav.cav_test`\n\n`python -m tcav.model_test`\n\n`python -m tcav.tcav_test`\n\n`python -m tcav.utils_test`\n\n## How to create a new version of the pip package\n\n1.  Ensure the version in setup.py has been updated to a new version.\n2.  Run `python setup.py bdist_wheel --python-tag py3` and `python setup.py\n    bdist_wheel --python-tag py2`.\n3.  Run `twine upload dist/*` to upload the py2 and py3 pip packages to PyPi.\n    \n", "release_dates": ["2018-11-21T15:34:40Z", "2018-11-14T20:05:11Z"]}, {"name": "tensor2tensor", "description": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Tensor2Tensor\n\n[![PyPI\nversion](https://badge.fury.io/py/tensor2tensor.svg)](https://badge.fury.io/py/tensor2tensor)\n[![GitHub\nIssues](https://img.shields.io/github/issues/tensorflow/tensor2tensor.svg)](https://github.com/tensorflow/tensor2tensor/issues)\n[![Contributions\nwelcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/tensor2tensor/Lobby)\n[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Travis](https://img.shields.io/travis/tensorflow/tensor2tensor.svg)](https://travis-ci.org/tensorflow/tensor2tensor)\n[![Run on FH](https://static.floydhub.com/button/button-small.svg)](https://floydhub.com/run)\n\n[Tensor2Tensor](https://github.com/tensorflow/tensor2tensor), or\n[T2T](https://github.com/tensorflow/tensor2tensor) for short, is a library\nof deep learning models and datasets designed to make deep learning more\naccessible and [accelerate ML\nresearch](https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html).\n\n\nT2T was developed by researchers and engineers in the\n[Google Brain team](https://research.google.com/teams/brain/) and a community\nof users. It is now deprecated &mdash; we keep it running and welcome\nbug-fixes, but encourage users to use the successor library [Trax](https://github.com/google/trax).\n\n### Quick Start\n\n[This iPython notebook](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)\nexplains T2T and runs in your browser using a free VM from Google,\nno installation needed. Alternatively, here is a one-command version that\ninstalls T2T, downloads MNIST, trains a model and evaluates it:\n\n```\npip install tensor2tensor && t2t-trainer \\\n  --generate_data \\\n  --data_dir=~/t2t_data \\\n  --output_dir=~/t2t_train/mnist \\\n  --problem=image_mnist \\\n  --model=shake_shake \\\n  --hparams_set=shake_shake_quick \\\n  --train_steps=1000 \\\n  --eval_steps=100\n```\n\n### Contents\n\n* [Suggested Datasets and Models](#suggested-datasets-and-models)\n  * [Mathematical Language Understanding](#mathematical-language-understanding)\n  * [Story, Question and Answer](#story-question-and-answer)\n  * [Image Classification](#image-classification)\n  * [Image Generation](#image-generation)\n  * [Language Modeling](#language-modeling)\n  * [Sentiment Analysis](#sentiment-analysis)\n  * [Speech Recognition](#speech-recognition)\n  * [Summarization](#summarization)\n  * [Translation](#translation)\n* [Basics](#basics)\n  * [Walkthrough](#walkthrough)\n  * [Installation](#installation)\n  * [Features](#features)\n* [T2T Overview](#t2t-overview)\n  * [Datasets](#datasets)\n  * [Problems and Modalities](#problems-and-modalities)\n  * [Models](#models)\n  * [Hyperparameter Sets](#hyperparameter-sets)\n  * [Trainer](#trainer)\n* [Adding your own components](#adding-your-own-components)\n* [Adding a dataset](#adding-a-dataset)\n* [Papers](#papers)\n* [Run on FloydHub](#run-on-floydhub)\n\n## Suggested Datasets and Models\n\nBelow we list a number of tasks that can be solved with T2T when\nyou train the appropriate model on the appropriate problem.\nWe give the problem and model below and we suggest a setting of\nhyperparameters that we know works well in our setup. We usually\nrun either on Cloud TPUs or on 8-GPU machines; you might need\nto modify the hyperparameters if you run on a different setup.\n\n### Mathematical Language Understanding\n\nFor evaluating mathematical expressions at the character level involving addition, subtraction and multiplication of both positive and negative decimal numbers with variable digits assigned to symbolic variables, use\n\n* the [MLU](https://art.wangperawong.com/mathematical_language_understanding_train.tar.gz) data-set:\n `--problem=algorithmic_math_two_variables`\n\nYou can try solving the problem with different transformer models and hyperparameters as described in the [paper](https://arxiv.org/abs/1812.02825):\n* Standard transformer:\n`--model=transformer`\n`--hparams_set=transformer_tiny`\n* Universal transformer:\n`--model=universal_transformer`\n`--hparams_set=universal_transformer_tiny`\n* Adaptive universal transformer:\n`--model=universal_transformer`\n`--hparams_set=adaptive_universal_transformer_tiny`\n\n### Story, Question and Answer\n\nFor answering questions based on a story, use\n\n* the [bAbi](https://research.fb.com/downloads/babi/) data-set:\n `--problem=babi_qa_concat_task1_1k`\n\nYou can choose the bAbi task from the range [1,20] and the subset from 1k or\n10k. To combine test data from all tasks into a single test set, use\n`--problem=babi_qa_concat_all_tasks_10k`\n\n### Image Classification\n\nFor image classification, we have a number of standard data-sets:\n\n* ImageNet (a large data-set): `--problem=image_imagenet`, or one\n   of the re-scaled versions (`image_imagenet224`, `image_imagenet64`,\n   `image_imagenet32`)\n* CIFAR-10: `--problem=image_cifar10` (or\n    `--problem=image_cifar10_plain` to turn off data augmentation)\n* CIFAR-100: `--problem=image_cifar100`\n* MNIST: `--problem=image_mnist`\n\nFor ImageNet, we suggest to use the ResNet or Xception, i.e.,\nuse `--model=resnet --hparams_set=resnet_50` or\n`--model=xception --hparams_set=xception_base`.\nResnet should get to above 76% top-1 accuracy on ImageNet.\n\nFor CIFAR and MNIST, we suggest to try the shake-shake model:\n`--model=shake_shake --hparams_set=shakeshake_big`.\nThis setting trained for `--train_steps=700000` should yield\nclose to 97% accuracy on CIFAR-10.\n\n### Image Generation\n\nFor (un)conditional image generation, we have a number of standard data-sets:\n\n* CelebA: `--problem=img2img_celeba` for image-to-image translation, namely,\n    superresolution from 8x8 to 32x32.\n* CelebA-HQ: `--problem=image_celeba256_rev` for a downsampled 256x256.\n* CIFAR-10: `--problem=image_cifar10_plain_gen_rev` for class-conditional\n    32x32 generation.\n* LSUN Bedrooms: `--problem=image_lsun_bedrooms_rev`\n* MS-COCO: `--problem=image_text_ms_coco_rev` for text-to-image generation.\n* Small ImageNet (a large data-set): `--problem=image_imagenet32_gen_rev` for\n    32x32 or `--problem=image_imagenet64_gen_rev` for 64x64.\n\nWe suggest to use the Image Transformer, i.e., `--model=imagetransformer`, or\nthe Image Transformer Plus, i.e., `--model=imagetransformerpp` that uses\ndiscretized mixture of logistics, or variational auto-encoder, i.e.,\n`--model=transformer_ae`.\nFor CIFAR-10, using `--hparams_set=imagetransformer_cifar10_base` or\n`--hparams_set=imagetransformer_cifar10_base_dmol` yields 2.90 bits per\ndimension. For Imagenet-32, using\n`--hparams_set=imagetransformer_imagenet32_base` yields 3.77 bits per dimension.\n\n### Language Modeling\n\nFor language modeling, we have these data-sets in T2T:\n\n* PTB (a small data-set): `--problem=languagemodel_ptb10k` for\n    word-level modeling and `--problem=languagemodel_ptb_characters`\n    for character-level modeling.\n* LM1B (a billion-word corpus): `--problem=languagemodel_lm1b32k` for\n    subword-level modeling and `--problem=languagemodel_lm1b_characters`\n    for character-level modeling.\n\nWe suggest to start with `--model=transformer` on this task and use\n`--hparams_set=transformer_small` for PTB and\n`--hparams_set=transformer_base` for LM1B.\n\n### Sentiment Analysis\n\nFor the task of recognizing the sentiment of a sentence, use\n\n* the IMDB data-set: `--problem=sentiment_imdb`\n\nWe suggest to use `--model=transformer_encoder` here and since it is\na small data-set, try `--hparams_set=transformer_tiny` and train for\nfew steps (e.g., `--train_steps=2000`).\n\n### Speech Recognition\n\nFor speech-to-text, we have these data-sets in T2T:\n\n* Librispeech (US English): `--problem=librispeech` for\n    the whole set and `--problem=librispeech_clean` for a smaller\n    but nicely filtered part.\n\n* Mozilla Common Voice (US English): `--problem=common_voice` for the whole set\n    `--problem=common_voice_clean` for a quality-checked subset.\n\n### Summarization\n\nFor summarizing longer text into shorter one we have these data-sets:\n\n* CNN/DailyMail articles summarized into a few sentences:\n  `--problem=summarize_cnn_dailymail32k`\n\nWe suggest to use `--model=transformer` and\n`--hparams_set=transformer_prepend` for this task.\nThis yields good ROUGE scores.\n\n### Translation\n\nThere are a number of translation data-sets in T2T:\n\n* English-German: `--problem=translate_ende_wmt32k`\n* English-French: `--problem=translate_enfr_wmt32k`\n* English-Czech: `--problem=translate_encs_wmt32k`\n* English-Chinese: `--problem=translate_enzh_wmt32k`\n* English-Vietnamese: `--problem=translate_envi_iwslt32k`\n* English-Spanish: `--problem=translate_enes_wmt32k`\n\nYou can get translations in the other direction by appending `_rev` to\nthe problem name, e.g., for German-English use\n`--problem=translate_ende_wmt32k_rev`\n(note that you still need to download the original data with t2t-datagen\n`--problem=translate_ende_wmt32k`).\n\nFor all translation problems, we suggest to try the Transformer model:\n`--model=transformer`. At first it is best to try the base setting,\n`--hparams_set=transformer_base`. When trained on 8 GPUs for 300K steps\nthis should reach a BLEU score of about 28 on the English-German data-set,\nwhich is close to state-of-the art. If training on a single GPU, try the\n`--hparams_set=transformer_base_single_gpu` setting. For very good results\nor larger data-sets (e.g., for English-French), try the big model\nwith `--hparams_set=transformer_big`.\n\nSee this [example](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/Transformer_translate.ipynb) to know how the translation works.\n\n## Basics\n\n### Walkthrough\n\nHere's a walkthrough training a good English-to-German translation\nmodel using the Transformer model from [*Attention Is All You\nNeed*](https://arxiv.org/abs/1706.03762) on WMT data.\n\n```\npip install tensor2tensor\n\n# See what problems, models, and hyperparameter sets are available.\n# You can easily swap between them (and add new ones).\nt2t-trainer --registry_help\n\nPROBLEM=translate_ende_wmt32k\nMODEL=transformer\nHPARAMS=transformer_base_single_gpu\n\nDATA_DIR=$HOME/t2t_data\nTMP_DIR=/tmp/t2t_datagen\nTRAIN_DIR=$HOME/t2t_train/$PROBLEM/$MODEL-$HPARAMS\n\nmkdir -p $DATA_DIR $TMP_DIR $TRAIN_DIR\n\n# Generate data\nt2t-datagen \\\n  --data_dir=$DATA_DIR \\\n  --tmp_dir=$TMP_DIR \\\n  --problem=$PROBLEM\n\n# Train\n# *  If you run out of memory, add --hparams='batch_size=1024'.\nt2t-trainer \\\n  --data_dir=$DATA_DIR \\\n  --problem=$PROBLEM \\\n  --model=$MODEL \\\n  --hparams_set=$HPARAMS \\\n  --output_dir=$TRAIN_DIR\n\n# Decode\n\nDECODE_FILE=$DATA_DIR/decode_this.txt\necho \"Hello world\" >> $DECODE_FILE\necho \"Goodbye world\" >> $DECODE_FILE\necho -e 'Hallo Welt\\nAuf Wiedersehen Welt' > ref-translation.de\n\nBEAM_SIZE=4\nALPHA=0.6\n\nt2t-decoder \\\n  --data_dir=$DATA_DIR \\\n  --problem=$PROBLEM \\\n  --model=$MODEL \\\n  --hparams_set=$HPARAMS \\\n  --output_dir=$TRAIN_DIR \\\n  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n  --decode_from_file=$DECODE_FILE \\\n  --decode_to_file=translation.en\n\n# See the translations\ncat translation.en\n\n# Evaluate the BLEU score\n# Note: Report this BLEU score in papers, not the internal approx_bleu metric.\nt2t-bleu --translation=translation.en --reference=ref-translation.de\n```\n\n### Installation\n\n\n```\n# Assumes tensorflow or tensorflow-gpu installed\npip install tensor2tensor\n\n# Installs with tensorflow-gpu requirement\npip install tensor2tensor[tensorflow_gpu]\n\n# Installs with tensorflow (cpu) requirement\npip install tensor2tensor[tensorflow]\n```\n\nBinaries:\n\n```\n# Data generator\nt2t-datagen\n\n# Trainer\nt2t-trainer --registry_help\n```\n\nLibrary usage:\n\n```\npython -c \"from tensor2tensor.models.transformer import Transformer\"\n```\n\n### Features\n\n* Many state of the art and baseline models are built-in and new models can be\n  added easily (open an issue or pull request!).\n* Many datasets across modalities - text, audio, image - available for\n  generation and use, and new ones can be added easily (open an issue or pull\n  request for public datasets!).\n* Models can be used with any dataset and input mode (or even multiple); all\n  modality-specific processing (e.g. embedding lookups for text tokens) is done\n  with `bottom` and `top` transformations, which are specified per-feature in the\n  model.\n* Support for multi-GPU machines and synchronous (1 master, many workers) and\n  asynchronous (independent workers synchronizing through a parameter server)\n  [distributed training](https://tensorflow.github.io/tensor2tensor/distributed_training.html).\n* Easily swap amongst datasets and models by command-line flag with the data\n  generation script `t2t-datagen` and the training script `t2t-trainer`.\n* Train on [Google Cloud ML](https://tensorflow.github.io/tensor2tensor/cloud_mlengine.html) and [Cloud TPUs](https://tensorflow.github.io/tensor2tensor/cloud_tpu.html).\n\n## T2T overview\n\n### Problems\n\n**Problems** consist of features such as inputs and targets, and metadata such\nas each feature's modality (e.g. symbol, image, audio) and vocabularies. Problem\nfeatures are given by a dataset, which is stored as a `TFRecord` file with\n`tensorflow.Example` protocol buffers. All\nproblems are imported in\n[`all_problems.py`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/all_problems.py)\nor are registered with `@registry.register_problem`. Run\n[`t2t-datagen`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen)\nto see the list of available problems and download them.\n\n### Models\n\n**`T2TModel`s** define the core tensor-to-tensor computation. They apply a\ndefault transformation to each input and output so that models may deal with\nmodality-independent tensors (e.g. embeddings at the input; and a linear\ntransform at the output to produce logits for a softmax over classes). All\nmodels are imported in the\n[`models` subpackage](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/models/__init__.py),\ninherit from [`T2TModel`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/t2t_model.py),\nand are registered with\n[`@registry.register_model`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/registry.py).\n\n### Hyperparameter Sets\n\n**Hyperparameter sets** are encoded in\n[`HParams`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/hparam.py)\nobjects, and are registered with\n[`@registry.register_hparams`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/registry.py).\nEvery model and problem has a `HParams`. A basic set of hyperparameters are\ndefined in\n[`common_hparams.py`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/layers/common_hparams.py)\nand hyperparameter set functions can compose other hyperparameter set functions.\n\n### Trainer\n\nThe **trainer** binary is the entrypoint for training, evaluation, and\ninference. Users can easily switch between problems, models, and hyperparameter\nsets by using the `--model`, `--problem`, and `--hparams_set` flags. Specific\nhyperparameters can be overridden with the `--hparams` flag. `--schedule` and\nrelated flags control local and distributed training/evaluation\n([distributed training documentation](https://github.com/tensorflow/tensor2tensor/tree/master/docs/distributed_training.md)).\n\n## Adding your own components\n\nT2T's components are registered using a central registration mechanism that\nenables easily adding new ones and easily swapping amongst them by command-line\nflag. You can add your own components without editing the T2T codebase by\nspecifying the `--t2t_usr_dir` flag in `t2t-trainer`.\n\nYou can do so for models, hyperparameter sets, modalities, and problems. Please\ndo submit a pull request if your component might be useful to others.\n\nSee the [`example_usr_dir`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/test_data/example_usr_dir)\nfor an example user directory.\n\n## Adding a dataset\n\nTo add a new dataset, subclass\n[`Problem`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py)\nand register it with `@registry.register_problem`. See\n[`TranslateEndeWmt8k`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py)\nfor an example. Also see the [data generators\nREADME](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md).\n\n## Run on FloydHub\n\n[![Run on FloydHub](https://static.floydhub.com/button/button.svg)](https://floydhub.com/run)\n\nClick this button to open a [Workspace](https://blog.floydhub.com/workspaces/) on [FloydHub](https://www.floydhub.com/?utm_medium=readme&utm_source=tensor2tensor&utm_campaign=jul_2018). You can use the workspace to develop and test your code on a fully configured cloud GPU machine.\n\nTensor2Tensor comes preinstalled in the environment, you can simply open a [Terminal](https://docs.floydhub.com/guides/workspace/#using-terminal) and run your code.\n\n```bash\n# Test the quick-start on a Workspace's Terminal with this command\nt2t-trainer \\\n  --generate_data \\\n  --data_dir=./t2t_data \\\n  --output_dir=./t2t_train/mnist \\\n  --problem=image_mnist \\\n  --model=shake_shake \\\n  --hparams_set=shake_shake_quick \\\n  --train_steps=1000 \\\n  --eval_steps=100\n```\n\nNote: Ensure compliance with the FloydHub [Terms of Service](https://www.floydhub.com/about/terms).\n\n## Papers\n\nWhen referencing Tensor2Tensor, please cite [this\npaper](https://arxiv.org/abs/1803.07416).\n\n```\n@article{tensor2tensor,\n  author    = {Ashish Vaswani and Samy Bengio and Eugene Brevdo and\n    Francois Chollet and Aidan N. Gomez and Stephan Gouws and Llion Jones and\n    \\L{}ukasz Kaiser and Nal Kalchbrenner and Niki Parmar and Ryan Sepassi and\n    Noam Shazeer and Jakob Uszkoreit},\n  title     = {Tensor2Tensor for Neural Machine Translation},\n  journal   = {CoRR},\n  volume    = {abs/1803.07416},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1803.07416},\n}\n```\n\nTensor2Tensor was used to develop a number of state-of-the-art models\nand deep learning methods. Here we list some papers that were based on T2T\nfrom the start and benefited from its features and architecture in ways\ndescribed in the [Google Research Blog post introducing\nT2T](https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html).\n\n* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n* [Depthwise Separable Convolutions for Neural Machine\n   Translation](https://arxiv.org/abs/1706.03059)\n* [One Model To Learn Them All](https://arxiv.org/abs/1706.05137)\n* [Discrete Autoencoders for Sequence Models](https://arxiv.org/abs/1801.09797)\n* [Generating Wikipedia by Summarizing Long\n   Sequences](https://arxiv.org/abs/1801.10198)\n* [Image Transformer](https://arxiv.org/abs/1802.05751)\n* [Training Tips for the Transformer Model](https://arxiv.org/abs/1804.00247)\n* [Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155)\n* [Fast Decoding in Sequence Models using Discrete Latent Variables](https://arxiv.org/abs/1803.03382)\n* [Adafactor: Adaptive Learning Rates with Sublinear Memory Cost](https://arxiv.org/abs/1804.04235)\n* [Universal Transformers](https://arxiv.org/abs/1807.03819)\n* [Attending to Mathematical Language with Transformers](https://arxiv.org/abs/1812.02825)\n* [The Evolved Transformer](https://arxiv.org/abs/1901.11117)\n* [Model-Based Reinforcement Learning for Atari](https://arxiv.org/abs/1903.00374)\n* [VideoFlow: A Flow-Based Generative Model for Video](https://arxiv.org/abs/1903.01434)\n\n*NOTE: This is not an official Google product.*\n", "release_dates": ["2020-06-17T16:10:01Z", "2020-06-02T15:03:13Z", "2020-04-18T18:14:22Z", "2020-01-11T01:06:45Z", "2020-01-10T22:39:54Z", "2019-11-23T08:52:29Z", "2019-11-23T08:19:30Z", "2019-11-22T16:15:10Z", "2019-10-03T21:04:09Z", "2019-08-21T22:03:23Z", "2019-05-08T16:04:02Z", "2019-05-08T01:23:49Z", "2019-04-08T19:03:21Z", "2019-03-22T00:35:08Z", "2019-03-22T01:00:29Z", "2019-01-11T23:46:06Z", "2018-11-15T06:16:44Z", "2018-10-30T06:48:48Z", "2018-09-08T01:30:08Z", "2018-08-20T17:36:29Z", "2018-08-10T03:54:48Z", "2018-06-26T21:42:37Z", "2018-06-15T20:49:49Z", "2018-05-21T23:18:34Z", "2018-05-08T03:23:02Z", "2018-04-26T23:43:26Z", "2018-04-20T22:59:30Z", "2018-04-13T21:29:38Z", "2018-04-05T20:47:48Z", "2018-03-10T02:16:11Z"]}, {"name": "tensorboard", "description": "TensorFlow's Visualization Toolkit", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorBoard [![GitHub Actions CI](https://github.com/tensorflow/tensorboard/workflows/CI/badge.svg)](https://github.com/tensorflow/tensorboard/actions?query=workflow%3ACI+branch%3Amaster+event%3Apush) [![GitHub Actions Nightly CI](https://github.com/tensorflow/tensorboard/workflows/nightly-release/badge.svg)](https://github.com/tensorflow/tensorboard/actions?query=workflow%3Anightly-release+branch%3Amaster) [![PyPI](https://badge.fury.io/py/tensorboard.svg)](https://badge.fury.io/py/tensorboard)\n\nTensorBoard is a suite of web applications for inspecting and understanding your\nTensorFlow runs and graphs.\n\nThis README gives an overview of key concepts in TensorBoard, as well as how to\ninterpret the visualizations TensorBoard provides. For an in-depth example of\nusing TensorBoard, see the tutorial: [TensorBoard: Getting Started][].\nDocumentation on how to use TensorBoard to work with images, graphs, hyper\nparameters, and more are linked from there, along with tutorial walk-throughs in\nColab.\n\nTensorBoard is designed to run entirely offline, without requiring any access\nto the Internet. For instance, this may be on your local machine, behind a\ncorporate firewall, or in a datacenter.\n\n[TensorBoard: Getting Started]: https://www.tensorflow.org/tensorboard/get_started\n[TensorBoard.dev]: https://tensorboard.dev\n[This experiment]: https://tensorboard.dev/experiment/EDZb7XgKSBKo6Gznh3i8hg/#scalars\n\n# Usage\n\nBefore running TensorBoard, make sure you have generated summary data in a log\ndirectory by creating a summary writer:\n\n``` python\n# sess.graph contains the graph definition; that enables the Graph Visualizer.\n\nfile_writer = tf.summary.FileWriter('/path/to/logs', sess.graph)\n```\n\nFor more details, see\n[the TensorBoard tutorial](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\nOnce you have event files, run TensorBoard and provide the log directory. If\nyou're using a precompiled TensorFlow package (e.g. you installed via pip), run:\n\n```\ntensorboard --logdir path/to/logs\n```\n\nOr, if you are building from source:\n\n```bash\nbazel build tensorboard:tensorboard\n./bazel-bin/tensorboard/tensorboard --logdir path/to/logs\n\n# or even more succinctly\nbazel run tensorboard -- --logdir path/to/logs\n```\n\nThis should print that TensorBoard has started. Next, connect to\nhttp://localhost:6006.\n\nTensorBoard requires a `logdir` to read logs from. For info on configuring\nTensorBoard, run `tensorboard --help`.\n\nTensorBoard can be used in Google Chrome or Firefox. Other browsers might\nwork, but there may be bugs or performance issues.\n\n# Key Concepts\n\n### Summary Ops: How TensorBoard gets data from TensorFlow\n\nThe first step in using TensorBoard is acquiring data from your TensorFlow run.\nFor this, you need\n[summary ops](https://www.tensorflow.org/api_docs/python/tf/summary).\nSummary ops are ops, just like\n[`tf.matmul`](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul)\nand\n[`tf.nn.relu`](https://www.tensorflow.org/api_docs/python/tf/nn/relu),\nwhich means they take in tensors, produce tensors, and are evaluated from within\na TensorFlow graph. However, summary ops have a twist: the Tensors they produce\ncontain serialized protobufs, which are written to disk and sent to TensorBoard.\nTo visualize the summary data in TensorBoard, you should evaluate the summary\nop, retrieve the result, and then write that result to disk using a\nsummary.FileWriter. A full explanation, with examples, is in [the\ntutorial](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\n\nThe supported summary ops include:\n* [`tf.summary.scalar`](https://www.tensorflow.org/api_docs/python/tf/summary/scalar)\n* [`tf.summary.image`](https://www.tensorflow.org/api_docs/python/tf/summary/image)\n* [`tf.summary.audio`](https://www.tensorflow.org/api_docs/python/tf/summary/audio)\n* [`tf.summary.text`](https://www.tensorflow.org/api_docs/python/tf/summary/text)\n* [`tf.summary.histogram`](https://www.tensorflow.org/api_docs/python/tf/summary/histogram)\n\n### Tags: Giving names to data\n\nWhen you make a summary op, you will also give it a `tag`. The tag is basically\na name for the data recorded by that op, and will be used to organize the data\nin the frontend. The scalar and histogram dashboards organize data by tag, and\ngroup the tags into folders according to a directory/like/hierarchy. If you have\na lot of tags, we recommend grouping them with slashes.\n\n### Event Files & LogDirs: How TensorBoard loads the data\n\n`summary.FileWriters` take summary data from TensorFlow, and then write them to a\nspecified directory, known as the `logdir`. Specifically, the data is written to\nan append-only record dump that will have \"tfevents\" in the filename.\nTensorBoard reads data from a full directory, and organizes it into the history\nof a single TensorFlow execution.\n\nWhy does it read the whole directory, rather than an individual file? You might\nhave been using\n[supervisor.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py)\nto run your model, in which case if TensorFlow crashes, the supervisor will\nrestart it from a checkpoint. When it restarts, it will start writing to a new\nevents file, and TensorBoard will stitch the various event files together to\nproduce a consistent history of what happened.\n\n### Runs: Comparing different executions of your model\n\nYou may want to visually compare multiple executions of your model; for example,\nsuppose you've changed the hyperparameters and want to see if it's converging\nfaster. TensorBoard enables this through different \"runs\". When TensorBoard is\npassed a `logdir` at startup, it recursively walks the directory tree rooted at\n`logdir` looking for subdirectories that contain tfevents data. Every time it\nencounters such a subdirectory, it loads it as a new `run`, and the frontend\nwill organize the data accordingly.\n\nFor example, here is a well-organized TensorBoard log directory, with two runs,\n\"run1\" and \"run2\".\n\n```\n/some/path/mnist_experiments/\n/some/path/mnist_experiments/run1/\n/some/path/mnist_experiments/run1/events.out.tfevents.1456525581.name\n/some/path/mnist_experiments/run1/events.out.tfevents.1456525585.name\n/some/path/mnist_experiments/run2/\n/some/path/mnist_experiments/run2/events.out.tfevents.1456525385.name\n/tensorboard --logdir /some/path/mnist_experiments\n```\n\n#### Logdir & Logdir_spec (Legacy Mode)\n\nYou may also pass a comma separated list of log directories, and TensorBoard\nwill watch each directory. You can also assign names to individual log\ndirectories by putting a colon between the name and the path, as in\n\n```\ntensorboard --logdir_spec name1:/path/to/logs/1,name2:/path/to/logs/2\n```\n\n_This flag (`--logdir_spec`) is discouraged and can usually be avoided_. TensorBoard walks log directories recursively; for finer-grained control, prefer using a symlink tree. _Some features may not work when using `--logdir_spec` instead of `--logdir`._\n\n# The Visualizations\n\n### Scalar Dashboard\n\nTensorBoard's Scalar Dashboard visualizes scalar statistics that vary over time;\nfor example, you might want to track the model's loss or learning rate. As\ndescribed in *Key Concepts*, you can compare multiple runs, and the data is\norganized by tag. The line charts have the following interactions:\n\n* Clicking on the small blue icon in the lower-left corner of each chart will\nexpand the chart\n\n* Dragging a rectangular region on the chart will zoom in\n\n* Double clicking on the chart will zoom out\n\n* Mousing over the chart will produce crosshairs, with data values recorded in\nthe run-selector on the left.\n\nAdditionally, you can create new folders to organize tags by writing regular\nexpressions in the box in the top-left of the dashboard.\n\n### Histogram Dashboard\n\nThe Histogram Dashboard displays how the statistical distribution of a Tensor\nhas varied over time. It visualizes data recorded via `tf.summary.histogram`.\nEach chart shows temporal \"slices\" of data, where each slice is a histogram of\nthe tensor at a given step. It's organized with the oldest timestep in the back,\nand the most recent timestep in front. By changing the Histogram Mode from\n\"offset\" to \"overlay\", the perspective will rotate so that every histogram slice\nis rendered as a line and overlaid with one another.\n\n### Distribution Dashboard\n\nThe Distribution Dashboard is another way of visualizing histogram data from\n`tf.summary.histogram`. It shows some high-level statistics on a distribution.\nEach line on the chart represents a percentile in the distribution over the\ndata: for example, the bottom line shows how the minimum value has changed over\ntime, and the line in the middle shows how the median has changed. Reading from\ntop to bottom, the lines have the following meaning: `[maximum, 93%, 84%, 69%,\n50%, 31%, 16%, 7%, minimum]`\n\nThese percentiles can also be viewed as standard deviation boundaries on a\nnormal distribution: `[maximum, \u03bc+1.5\u03c3, \u03bc+\u03c3, \u03bc+0.5\u03c3, \u03bc, \u03bc-0.5\u03c3, \u03bc-\u03c3, \u03bc-1.5\u03c3,\nminimum]` so that the colored regions, read from inside to outside, have widths\n`[\u03c3, 2\u03c3, 3\u03c3]` respectively.\n\n### Image Dashboard\n\nThe Image Dashboard can display pngs that were saved via a `tf.summary.image`.\nThe dashboard is set up so that each row corresponds to a different tag, and\neach column corresponds to a run. Since the image dashboard supports arbitrary\npngs, you can use this to embed custom visualizations (e.g. matplotlib\nscatterplots) into TensorBoard. This dashboard always shows you the latest image\nfor each tag.\n\n### Audio Dashboard\n\nThe Audio Dashboard can embed playable audio widgets for audio saved via a\n`tf.summary.audio`. The dashboard is set up so that each row corresponds to a\ndifferent tag, and each column corresponds to a run. This dashboard always\nembeds the latest audio for each tag.\n\n### Graph Explorer\n\nThe Graph Explorer can visualize a TensorBoard graph, enabling inspection of the\nTensorFlow model. To get best use of the graph visualizer, you should use name\nscopes to hierarchically group the ops in your graph - otherwise, the graph may\nbe difficult to decipher. For more information, including examples, see the\n[examining the TensorFlow graph](https://www.tensorflow.org/tensorboard/graphs)\ntutorial.\n\n### Embedding Projector\n\nThe Embedding Projector allows you to visualize high-dimensional data; for\nexample, you may view your input data after it has been embedded in a high-\ndimensional space by your model. The embedding projector reads data from your\nmodel checkpoint file, and may be configured with additional metadata, like\na vocabulary file or sprite images. For more details, see [the embedding\nprojector tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings).\n\n### Text Dashboard\n\nThe Text Dashboard displays text snippets saved via `tf.summary.text`. Markdown\nfeatures including hyperlinks, lists, and tables are all supported.\n\n### Time Series Dashboard\n\nThe Time Series Dashboard shows a unified interface containing all your Scalars,\nHistograms, and Images saved via `tf.summary.scalar`, `tf.summary.image`, or\n`tf.summary.histogram`. It enables viewing your 'accuracy' line chart side by\nside with activation histograms and training example images, for example.\n\nFeatures include:\n\n* Custom run colors: click on the colored circles in the run selector to change\na run's color.\n\n* Pinned cards: click the 'pin' icon on any card to add it to the pinned section\nat the top for quick comparison.\n\n* Settings: the right pane offers settings for charts and other visualizations.\nImportant settings will persist across TensorBoard sessions, when hosted at the\nsame URL origin.\n\n* Autocomplete in tag filter: search for specific charts more easily.\n\n# Frequently Asked Questions\n\n### My TensorBoard isn't showing any data! What's wrong?\n\nFirst, check that the directory passed to `--logdir` is correct. You can also\nverify this by navigating to the Scalars dashboard (under the \"Inactive\" menu)\nand looking for the log directory path at the bottom of the left sidebar.\n\nIf you're loading from the proper path, make sure that event files are present.\nTensorBoard will recursively walk its logdir, it's fine if the data is nested\nunder a subdirectory. Ensure the following shows at least one result:\n\n`find DIRECTORY_PATH | grep tfevents`\n\nYou can also check that the event files actually have data by running\ntensorboard in inspect mode to inspect the contents of your event files.\n\n`tensorboard --inspect --logdir DIRECTORY_PATH`\n\nThe output for an event file corresponding to a blank TensorBoard may\nstill sometimes show a few steps, representing a few initial events that\naren't shown by TensorBoard (for example, when using the Keras TensorBoard callback):\n\n```\ntensor\n   first_step           0\n   last_step            2\n   max_step             2\n   min_step             0\n   num_steps            2\n   outoforder_steps     [(2, 0), (2, 0), (2, 0)]\n```\n\nIn contrast, the output for an event file with more data might look like this:\n\n```\ntensor\n   first_step           0\n   last_step            55\n   max_step             250\n   min_step             0\n   num_steps            60\n   outoforder_steps     [(2, 0), (2, 0), (2, 0), (2, 0), (50, 9), (100, 19), (150, 29), (200, 39), (250, 49)]\n```\n\n### TensorBoard is showing only some of my data, or isn't properly updating!\n\n> **Update:** After [2.3.0 release][2-3-0], TensorBoard no longer auto reloads\n> every 30 seconds. To re-enable the behavior, please open the settings by\n> clicking the gear icon in the top-right of the TensorBoard web interface, and\n> enable \"Reload data\".\n\n> **Update:** the [experimental `--reload_multifile=true` option][pr-1867] can\n> now be used to poll all \"active\" files in a directory for new data, rather\n> than the most recent one as described below. A file is \"active\" as long as it\n> received new data within `--reload_multifile_inactive_secs` seconds ago,\n> defaulting to 86400.\n\nThis issue usually comes about because of how TensorBoard iterates through the\n`tfevents` files: it progresses through the events file in timestamp order, and\nonly reads one file at a time. Let's suppose we have files with timestamps `a`\nand `b`, where `a<b`. Once TensorBoard has read all the events in `a`, it will\nnever return to it, because it assumes any new events are being written in the\nmore recent file. This could cause an issue if, for example, you have two\n`FileWriters` simultaneously writing to the same directory. If you have\nmultiple summary writers, each one should be writing to a separate directory.\n\n### Does TensorBoard support multiple or distributed summary writers?\n\n> **Update:** the [experimental `--reload_multifile=true` option][pr-1867] can\n> now be used to poll all \"active\" files in a directory for new data, defined as\n> any file that received new data within `--reload_multifile_inactive_secs`\n> seconds ago, defaulting to 86400.\n\nNo. TensorBoard expects that only one events file will be written to at a time,\nand multiple summary writers means multiple events files. If you are running a\ndistributed TensorFlow instance, we encourage you to designate a single worker\nas the \"chief\" that is responsible for all summary processing. See\n[supervisor.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py)\nfor an example.\n\n### I'm seeing data overlapped on itself! What gives?\n\nIf you are seeing data that seems to travel backwards through time and overlap\nwith itself, there are a few possible explanations.\n\n* You may have multiple execution of TensorFlow that all wrote to the same log\ndirectory. Please have each TensorFlow run write to its own logdir.\n\n  > **Update:** the [experimental `--reload_multifile=true` option][pr-1867] can\n  > now be used to poll all \"active\" files in a directory for new data, defined\n  > as any file that received new data within `--reload_multifile_inactive_secs`\n  > seconds ago, defaulting to 86400.\n\n* You may have a bug in your code where the global_step variable (passed\nto `FileWriter.add_summary`) is being maintained incorrectly.\n\n* It may be that your TensorFlow job crashed, and was restarted from an earlier\ncheckpoint. See *How to handle TensorFlow restarts*, below.\n\nAs a workaround, try changing the x-axis display in TensorBoard from `steps` to\n`wall_time`. This will frequently clear up the issue.\n\n### How should I handle TensorFlow restarts?\n\nTensorFlow is designed with a mechanism for graceful recovery if a job crashes\nor is killed: TensorFlow can periodically write model checkpoint files, which\nenable you to restart TensorFlow without losing all your training progress.\n\nHowever, this can complicate things for TensorBoard; imagine that TensorFlow\nwrote a checkpoint at step `a`, and then continued running until step `b`, and\nthen crashed and restarted at timestamp `a`. All of the events written between\n`a` and `b` were \"orphaned\" by the restart event and should be removed.\n\nTo facilitate this, we have a `SessionLog` message in\n`tensorflow/core/util/event.proto` which can record `SessionStatus.START` as an\nevent; like all events, it may have a `step` associated with it. If TensorBoard\ndetects a `SessionStatus.START` event with step `a`, it will assume that every\nevent with a step greater than `a` was orphaned, and it will discard those\nevents. This behavior may be disabled with the flag\n`--purge_orphaned_data false` (in versions after 0.7).\n\n### How can I export data from TensorBoard?\n\nThe Scalar Dashboard supports exporting data; you can click the \"enable\ndownload links\" option in the left-hand bar. Then, each plot will provide\ndownload links for the data it contains.\n\nIf you need access to the full dataset, you can read the event files that\nTensorBoard consumes by using the [`summary_iterator`](\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/train/summary_iterator)\nmethod.\n\n### Can I make my own plugin?\n\nYes! You can clone and tinker with one of the [examples][plugin-examples] and\nmake your own, amazing visualizations. More documentation on the plugin system\nis described in the [ADDING_A_PLUGIN](./ADDING_A_PLUGIN.md) guide. Feel free to\nfile feature requests or questions about plugin functionality.\n\nOnce satisfied with your own groundbreaking new plugin, see the\n[distribution section][plugin-distribution] on how to publish to PyPI and share\nit with the community.\n\n[plugin-examples]: ./tensorboard/examples/plugins\n[plugin-distribution]: ./ADDING_A_PLUGIN.md#distribution\n\n### Can I customize which lines appear in a plot?\n\nUsing the [custom scalars plugin](tensorboard/plugins/custom_scalar), you can\ncreate scalar plots with lines for custom run-tag pairs. However, within the\noriginal scalars dashboard, each scalar plot corresponds to data for a specific\ntag and contains lines for each run that includes that tag.\n\n### Can I visualize margins above and below lines?\n\nMargin plots (that visualize lower and upper bounds) may be created with the\n[custom scalars plugin](tensorboard/plugins/custom_scalar). The original\nscalars plugin does not support visualizing margins.\n\n### Can I create scatterplots (or other custom plots)?\n\nThis isn't yet possible. As a workaround, you could create your custom plot in\nyour own code (e.g. matplotlib) and then write it into an `SummaryProto`\n(`core/framework/summary.proto`) and add it to your `FileWriter`. Then, your\ncustom plot will appear in the TensorBoard image tab.\n\n### Is my data being downsampled? Am I really seeing all the data?\n\nTensorBoard uses [reservoir\nsampling](https://en.wikipedia.org/wiki/Reservoir_sampling) to downsample your\ndata so that it can be loaded into RAM. You can modify the number of elements it\nwill keep per tag by using the `--samples_per_plugin` command line argument (ex:\n`--samples_per_plugin=scalars=500,images=20`).\nSee this [Stack Overflow question](http://stackoverflow.com/questions/43702546/tensorboard-doesnt-show-all-data-points/)\nfor some more information.\n\n### I get a network security popup every time I run TensorBoard on a mac!\n\nVersions of TensorBoard prior to TensorBoard 2.0 would by default serve on host\n`0.0.0.0`, which is publicly accessible. For those versions of TensorBoard, you\ncan stop the popups by specifying `--host localhost` at startup.\n\nIn TensorBoard 2.0 and up, `--host localhost` is the default. Use `--bind_all`\nto restore the old behavior of serving to the public network on both IPv4 and\nIPv6.\n\n### Can I run `tensorboard` without a TensorFlow installation?\n\nTensorBoard 1.14+ can be run with a reduced feature set if you do not have\nTensorFlow installed. The primary limitation is that as of 1.14, only the\nfollowing plugins are supported: scalars, custom scalars, image, audio,\ngraph, projector (partial), distributions, histograms, text, PR curves, mesh.\nIn addition, there is no support for log directories on Google Cloud Storage.\n\n### How can I contribute to TensorBoard development?\n\nSee [DEVELOPMENT.md](DEVELOPMENT.md).\n\n### I have a different issue that wasn't addressed here!\n\nFirst, try searching our [GitHub\nissues](https://github.com/tensorflow/tensorboard/issues) and\n[Stack Overflow][stack-overflow]. It may be\nthat someone else has already had the same issue or question.\n\nGeneral usage questions (or problems that may be specific to your local setup)\nshould go to [Stack Overflow][stack-overflow].\n\nIf you have found a bug in TensorBoard, please [file a GitHub issue](\nhttps://github.com/tensorflow/tensorboard/issues/new) with as much supporting\ninformation as you can provide (e.g. attaching events files, including the output\nof `tensorboard --inspect`, etc.).\n\n[stack-overflow]: https://stackoverflow.com/questions/tagged/tensorboard\n[pr-1867]: https://github.com/tensorflow/tensorboard/pull/1867\n[2-3-0]: https://github.com/tensorflow/tensorboard/releases/tag/2.3.0\n", "release_dates": ["2024-02-16T19:55:52Z", "2024-02-14T15:52:50Z", "2024-02-12T21:46:33Z", "2024-02-09T10:29:43Z", "2023-11-02T18:45:45Z", "2023-10-19T19:31:58Z", "2023-09-27T23:07:32Z", "2023-08-08T22:34:52Z", "2023-05-04T22:43:16Z", "2023-05-02T19:25:52Z", "2023-04-19T13:42:39Z", "2023-03-31T21:48:19Z", "2023-02-09T23:01:25Z", "2023-01-13T20:21:22Z", "2023-01-13T01:03:07Z", "2022-11-09T01:31:25Z", "2022-09-26T15:14:27Z", "2022-08-10T19:44:48Z", "2022-06-13T11:14:01Z", "2022-04-29T19:39:04Z", "2022-01-20T21:08:58Z", "2021-10-13T16:15:14Z", "2021-08-06T18:11:42Z", "2021-04-19T18:42:18Z", "2021-01-14T20:44:05Z", "2020-11-12T03:15:42Z", "2020-07-21T22:38:12Z", "2020-05-28T17:50:49Z", "2020-04-15T17:57:45Z", "2020-03-24T15:57:18Z"]}, {"name": "tensorboard-plugin-example", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Example is deprecated. Please see [example at tensorflow/tensorboard](https://github.com/tensorflow/tensorboard/blob/javascript/tensorboard/examples/plugins/example_basic/README.md) for the latest example instead.\n\nThis repository has example that works with TensorBoard version 1.13 and prior.\n\n", "release_dates": []}, {"name": "tensorflow", "description": "An Open Source Machine Learning Framework for Everyone", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<div align=\"center\">\n  <img src=\"https://www.tensorflow.org/images/tf_logo_horizontal.png\">\n</div>\n\n[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![PyPI](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg)](https://doi.org/10.5281/zenodo.4724125)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/tensorflow/tensorflow/badge)](https://securityscorecards.dev/viewer/?uri=github.com/tensorflow/tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow-py.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow-py)\n[![OSSRank](https://shields.io/endpoint?url=https://ossrank.com/shield/44)](https://ossrank.com/p/44)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)\n[![TF Official Continuous](https://tensorflow.github.io/build/TF%20Official%20Continuous.svg)](https://tensorflow.github.io/build#TF%20Official%20Continuous)\n[![TF Official Nightly](https://tensorflow.github.io/build/TF%20Official%20Nightly.svg)](https://tensorflow.github.io/build#TF%20Official%20Nightly)\n\n**`Documentation`** |\n------------------- |\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |\n\n[TensorFlow](https://www.tensorflow.org/) is an end-to-end open source platform\nfor machine learning. It has a comprehensive, flexible ecosystem of\n[tools](https://www.tensorflow.org/resources/tools),\n[libraries](https://www.tensorflow.org/resources/libraries-extensions), and\n[community](https://www.tensorflow.org/community) resources that lets\nresearchers push the state-of-the-art in ML and developers easily build and\ndeploy ML-powered applications.\n\nTensorFlow was originally developed by researchers and engineers working within\nthe Machine Intelligence team at Google Brain to conduct research in machine\nlearning and neural networks. However, the framework is versatile enough to be\nused in other areas as well.\n\nTensorFlow provides stable [Python](https://www.tensorflow.org/api_docs/python)\nand [C++](https://www.tensorflow.org/api_docs/cc) APIs, as well as a\nnon-guaranteed backward compatible API for\n[other languages](https://www.tensorflow.org/api_docs).\n\nKeep up-to-date with release announcements and security updates by subscribing\nto\n[announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).\nSee all the [mailing lists](https://www.tensorflow.org/community/forums).\n\n## Install\n\nSee the [TensorFlow install guide](https://www.tensorflow.org/install) for the\n[pip package](https://www.tensorflow.org/install/pip), to\n[enable GPU support](https://www.tensorflow.org/install/gpu), use a\n[Docker container](https://www.tensorflow.org/install/docker), and\n[build from source](https://www.tensorflow.org/install/source).\n\nTo install the current release, which includes support for\n[CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and\nWindows)*:\n\n```\n$ pip install tensorflow\n```\n\nOther devices (DirectX and MacOS-metal) are supported using\n[Device plugins](https://www.tensorflow.org/install/gpu_plugins#available_devices).\n\nA smaller CPU-only package is also available:\n\n```\n$ pip install tensorflow-cpu\n```\n\nTo update TensorFlow to the latest version, add `--upgrade` flag to the above\ncommands.\n\n*Nightly binaries are available for testing using the\n[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPi.*\n\n#### *Try your first TensorFlow program*\n\n```shell\n$ python\n```\n\n```python\n>>> import tensorflow as tf\n>>> tf.add(1, 2).numpy()\n3\n>>> hello = tf.constant('Hello, TensorFlow!')\n>>> hello.numpy()\nb'Hello, TensorFlow!'\n```\n\nFor more examples, see the\n[TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow, be sure to review the\n[contribution guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, please see\n[TensorFlow Forum](https://discuss.tensorflow.org/) for general questions and\ndiscussion, and please direct specific questions to\n[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**\n\nThe TensorFlow project strives to abide by generally accepted best practices in\nopen-source software development.\n\n## Patching guidelines\n\nFollow these steps to patch a specific version of TensorFlow, for example, to\napply fixes to bugs or security vulnerabilities:\n\n*   Clone the TensorFlow repo and switch to the corresponding branch for your\n    desired TensorFlow version, for example, branch `r2.8` for version 2.8.\n*   Apply (that is, cherry-pick) the desired changes and resolve any code\n    conflicts.\n*   Run TensorFlow tests and ensure they pass.\n*   [Build](https://www.tensorflow.org/install/source) the TensorFlow pip\n    package from source.\n\n## Continuous build status\n\nYou can find more community-supported platforms and configurations in the\n[TensorFlow SIG Build community builds table](https://github.com/tensorflow/build#community-supported-tensorflow-builds).\n\n### Official Builds\n\nBuild Type                    | Status                                                                                                                                                                           | Artifacts\n----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------\n**Linux CPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html)           | [PyPI](https://pypi.org/project/tf-nightly/)\n**Linux GPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html) | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Linux XLA**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html)         | TBA\n**macOS**                     | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html)     | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows CPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html)       | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows GPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html)       | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Android**                   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html)               | [Download](https://bintray.com/google/tensorflow/tensorflow/_latestVersion)\n**Raspberry Pi 0 and 1**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl)\n**Raspberry Pi 2 and 3**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)\n**Libtensorflow MacOS CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/macos/latest/macos_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Linux CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/cpu/ubuntu_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Linux GPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/gpu/ubuntu_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Windows CPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/cpu/windows_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Windows GPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/gpu/windows_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n\n## Resources\n\n*   [TensorFlow.org](https://www.tensorflow.org)\n*   [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)\n*   [TensorFlow Official Models](https://github.com/tensorflow/models/tree/master/official)\n*   [TensorFlow Examples](https://github.com/tensorflow/examples)\n*   [TensorFlow Codelabs](https://codelabs.developers.google.com/?cat=TensorFlow)\n*   [TensorFlow Blog](https://blog.tensorflow.org)\n*   [Learn ML with TensorFlow](https://www.tensorflow.org/resources/learn-ml)\n*   [TensorFlow Twitter](https://twitter.com/tensorflow)\n*   [TensorFlow YouTube](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)\n*   [TensorFlow model optimization roadmap](https://www.tensorflow.org/model_optimization/guide/roadmap)\n*   [TensorFlow White Papers](https://www.tensorflow.org/about/bib)\n*   [TensorBoard Visualization Toolkit](https://github.com/tensorflow/tensorboard)\n*   [TensorFlow Code Search](https://cs.opensource.google/tensorflow/tensorflow)\n\nLearn more about the\n[TensorFlow community](https://www.tensorflow.org/community) and how to\n[contribute](https://www.tensorflow.org/community/contribute).\n\n## Courses\n\n* [Coursera](https://www.coursera.org/search?query=TensorFlow)\n* [Udacity](https://www.udacity.com/courses/all?search=TensorFlow)\n* [Edx](https://www.edx.org/search?q=TensorFlow)\n\n## License\n\n[Apache License 2.0](LICENSE)\n", "release_dates": ["2024-02-26T22:54:41Z", "2023-11-14T18:46:08Z", "2023-11-14T18:42:20Z", "2023-11-03T18:09:42Z", "2023-10-25T23:15:20Z", "2023-09-26T22:45:05Z", "2023-09-26T20:34:00Z", "2023-08-31T21:42:28Z", "2023-08-17T18:09:07Z", "2023-07-05T17:32:13Z", "2023-07-05T19:49:02Z", "2023-06-22T18:05:19Z", "2023-05-30T17:38:45Z", "2023-05-09T19:47:35Z", "2023-03-23T15:50:02Z", "2023-03-20T17:03:21Z", "2023-03-07T19:10:44Z", "2023-02-15T00:41:50Z", "2022-11-18T06:01:33Z", "2022-11-16T20:31:35Z", "2022-11-16T19:57:28Z", "2022-11-16T19:22:47Z", "2022-11-02T16:59:54Z", "2022-10-19T16:26:28Z", "2022-10-18T18:19:02Z", "2022-09-06T19:44:26Z", "2022-09-03T03:27:32Z", "2022-09-02T21:26:19Z", "2022-09-02T20:03:49Z", "2022-08-29T18:01:52Z"]}, {"name": "tensorrt", "description": "TensorFlow/TensorRT integration", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Documentation for TensorRT in TensorFlow (TF-TRT)\n\nTensorFlow-TensorRT (TF-TRT) is an integration of TensorFlow and TensorRT that leverages inference optimization on NVIDIA GPUs within the TensorFlow ecosystem. It provides a simple API that delivers substantial performance gains on NVIDIA GPUs with minimal effort. The documentation on how to accelerate inference in TensorFlow with TensorRT (TF-TRT) is here: https://docs.nvidia.com/deeplearning/dgx/tf-trt-user-guide/index.html\n\nCheck out this [gentle introduction](https://www.youtube.com/watch?v=w7871kMiAs8) to TensorFlow TensorRT or watch this [quick walkthrough](https://www.youtube.com/watch?v=O-_K42EAlP0) example for more!\n# Examples for TensorRT in TensorFlow (TF-TRT)\n\nThis repository contains a number of different examples\nthat show how to use TF-TRT.\nTF-TRT is a part of TensorFlow\nthat optimizes TensorFlow graphs using\n[TensorRT](https://developer.nvidia.com/tensorrt).\nWe have used these examples to verify the accuracy and\nperformance of TF-TRT. For more information see\n[Verified Models](https://docs.nvidia.com/deeplearning/dgx/tf-trt-user-guide/index.html#verified-models).\n\n## Examples\n\n* [Image Classification](tftrt/benchmarking-python/image_classification)\n* [Object Detection](tftrt/benchmarking-python/object_detection)\n\n\n# Using TensorRT in TensorFlow (TF-TRT)\n\nThis module provides necessary bindings and introduces\n`TRTEngineOp` operator that wraps a subgraph in TensorRT.\nThis module is under active development.\n\n\n## Installing TF-TRT\n\nCurrently Tensorflow nightly builds include TF-TRT by default,\nwhich means you don't need to install TF-TRT separately.\nYou can pull the latest TF containers from docker hub or\ninstall the latest TF pip package to get access to the latest TF-TRT.\n\nIf you want to use TF-TRT on NVIDIA Jetson platform, you can find\nthe download links for the relevant Tensorflow pip packages here:\nhttps://docs.nvidia.com/deeplearning/dgx/index.html#installing-frameworks-for-jetson\n\nYou can also use [NVIDIA's Tensorflow container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow)(tested and published monthly).\n\n## Installing TensorRT\n\nIn order to make use of TF-TRT, you will need a local installation\nof TensorRT from the\n[NVIDIA Developer website](https://developer.nvidia.com/tensorrt).\nInstallation instructions for compatibility with TensorFlow are provided on the\n[TensorFlow GPU support](https://www.tensorflow.org/install/gpu) guide.\n\n\n## Documentation\n\n[TF-TRT documentaion](https://docs.nvidia.com/deeplearning/dgx/tf-trt-user-guide/index.html)\ngives an overview of the supported functionalities, provides tutorials\nand verified models, explains best practices with troubleshooting guides.\n\n\n## Tests\n\nTF-TRT includes both Python tests and C++ unit tests.\nMost of Python tests are located in the test directory\nand they can be executed uring `bazel test` or directly\nwith the Python command. Most of the C++ unit tests are\nused to test the conversion functions that convert each TF op to\na number of TensorRT layers.\n\n\n## Compilation\n\nIn order to compile the module, you need to have a local TensorRT installation\n(libnvinfer.so and respective include files). During the configuration step,\nTensorRT should be enabled and installation path should be set. If installed\nthrough package managers (deb,rpm), configure script should find the necessary\ncomponents from the system automatically. If installed from tar packages, user\nhas to set path to location where the library is installed during configuration.\n\n```shell\nbazel build --config=cuda --config=opt //tensorflow/tools/pip_package:build_pip_package\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/\n```\n\n\n## License\n\n[Apache License 2.0](LICENSE)\n", "release_dates": []}, {"name": "tensorstore", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorStore\n\nThis project is a placeholder for TensorBoard storage code.\n\nThis is not an official Google product.\n", "release_dates": []}, {"name": "text", "description": "Making text a first-class citizen in TensorFlow.", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/tensorflow/text/master/docs/include/tftext.png\" width=\"60%\"><br><br>\n</div>\n\n-----------------\n\n[![PyPI version](https://img.shields.io/pypi/v/tensorflow-text)](https://badge.fury.io/py/tensorflow-text)\n[![PyPI nightly version](https://img.shields.io/pypi/v/tensorflow-text-nightly?color=informational&label=pypi%20%40%20nightly)](https://badge.fury.io/py/tensorflow-text-nightly)\n[![PyPI Python version](https://img.shields.io/pypi/pyversions/tensorflow-text)](https://pypi.org/project/tensorflow-text/)\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://github.com/tensorflow/text/blob/master/docs/api_docs/python/index.md)\n[![Contributions\nwelcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)\n\n<!-- TODO(broken):  Uncomment when badges are made public.\n### Continuous Integration Test Status\n\n| Build      | Status |\n| ---             | ---    |\n| **Linux**   | [![Status](https://storage.googleapis.com/tf-text-badges/ubuntu-gpu-py3.svg)] |\n| **MacOS**   | [![Status](https://storage.googleapis.com/tf-text-badges/ubuntu-gpu-py3.svg)] |\n| **Windows**   | [![Status](https://storage.googleapis.com/tf-text-badges/ubuntu-gpu-py3.svg)] |\n-->\n\n# TensorFlow Text - Text processing in Tensorflow\n\n**IMPORTANT**: When installing TF Text with `pip install`, please note the\nversion of TensorFlow you are running, as you should specify the corresponding\nminor version of TF Text (eg. for tensorflow==2.3.x use tensorflow_text==2.3.x).\n\n## INDEX\n* [Introduction](#introduction)\n* [Documentation](#documentation)\n* [Unicode](#unicode)\n* [Normalization](#normalization)\n* [Tokenization](#tokenization)\n  * [Whitespace Tokenizer](#whitespacetokenizer)\n  * [UnicodeScript Tokenizer](#unicodescripttokenizer)\n  * [Unicode split](#unicode-split)\n  * [Offsets](#offsets)\n  * [TF.Data Example](#tfdata-example)\n  * [Keras API](#keras-api)\n* [Other Text Ops](#other-text-ops)\n  * [Wordshape](#wordshape)\n  * [N-grams & Sliding Window](#n-grams--sliding-window)\n* [Installation](#installation)\n  * [Install using PIP](#install-using-pip)\n  * [Build from source steps:](#build-from-source-steps)\n\n## Introduction\n\nTensorFlow Text provides a collection of text related classes and ops ready to\nuse with TensorFlow 2.0. The library can perform the preprocessing regularly\nrequired by text-based models, and includes other features useful for sequence\nmodeling not provided by core TensorFlow.\n\nThe benefit of using these ops in your text preprocessing is that they are done\nin the TensorFlow graph. You do not need to worry about tokenization in\ntraining being different than the tokenization at inference, or managing\npreprocessing scripts.\n\n## Documentation\n\nPlease visit [http://tensorflow.org/text](http://tensorflow.org/text) for all\ndocumentation. This site includes API docs, guides for working with TensorFlow\nText, as well as tutorials for building specific models.\n\n## Unicode\n\nMost ops expect that the strings are in UTF-8. If you're using a different\nencoding, you can use the core tensorflow transcode op to transcode into UTF-8.\nYou can also use the same op to coerce your string to structurally valid UTF-8\nif your input could be invalid.\n\n```python\ndocs = tf.constant([u'Everything not saved will be lost.'.encode('UTF-16-BE'),\n                    u'Sad\u2639'.encode('UTF-16-BE')])\nutf8_docs = tf.strings.unicode_transcode(docs, input_encoding='UTF-16-BE',\n                                         output_encoding='UTF-8')\n```\n\n## Normalization\n\nWhen dealing with different sources of text, it's important that the same words\nare recognized to be identical. A common technique for case-insensitive matching\nin Unicode is case folding (similar to lower-casing). (Note that case folding\ninternally applies NFKC normalization.)\n\nWe also provide Unicode normalization ops for transforming strings into a\ncanonical representation of characters, with Normalization Form KC being the\ndefault ([NFKC](http://unicode.org/reports/tr15/)).\n\n```python\nprint(text.case_fold_utf8(['Everything not saved will be lost.']))\nprint(text.normalize_utf8(['\u00c4ffin']))\nprint(text.normalize_utf8(['\u00c4ffin'], 'nfkd'))\n```\n\n```sh\ntf.Tensor(['everything not saved will be lost.'], shape=(1,), dtype=string)\ntf.Tensor(['\\xc3\\x84ffin'], shape=(1,), dtype=string)\ntf.Tensor(['A\\xcc\\x88ffin'], shape=(1,), dtype=string)\n```\n\n## Tokenization\n\nTokenization is the process of breaking up a string into tokens. Commonly, these\ntokens are words, numbers, and/or punctuation.\n\nThe main interfaces are `Tokenizer` and `TokenizerWithOffsets` which each have a\nsingle method `tokenize` and `tokenizeWithOffsets` respectively. There are\nmultiple implementing tokenizers available now. Each of these implement\n`TokenizerWithOffsets` (which extends `Tokenizer`) which includes an option for\ngetting byte offsets into the original string. This allows the caller to know\nthe bytes in the original string the token was created from.\n\nAll of the tokenizers return RaggedTensors with the inner-most dimension of\ntokens mapping to the original individual strings. As a result, the resulting\nshape's rank is increased by one. Please review the ragged tensor guide if you\nare unfamiliar with them. https://www.tensorflow.org/guide/ragged_tensor\n\n### WhitespaceTokenizer\n\nThis is a basic tokenizer that splits UTF-8 strings on ICU defined whitespace\ncharacters (eg. space, tab, new line).\n\n```python\ntokenizer = text.WhitespaceTokenizer()\ntokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad\u2639'.encode('UTF-8')])\nprint(tokens.to_list())\n```\n\n```sh\n[['everything', 'not', 'saved', 'will', 'be', 'lost.'], ['Sad\\xe2\\x98\\xb9']]\n```\n\n### UnicodeScriptTokenizer\n\nThis tokenizer splits UTF-8 strings based on Unicode script boundaries. The\nscript codes used correspond to International Components for Unicode (ICU)\nUScriptCode values. See: http://icu-project.org/apiref/icu4c/uscript_8h.html\n\nIn practice, this is similar to the `WhitespaceTokenizer` with the most apparent\ndifference being that it will split punctuation (USCRIPT_COMMON) from language\ntexts (eg. USCRIPT_LATIN, USCRIPT_CYRILLIC, etc) while also separating language\ntexts from each other.\n\n```python\ntokenizer = text.UnicodeScriptTokenizer()\ntokens = tokenizer.tokenize(['everything not saved will be lost.',\n                             u'Sad\u2639'.encode('UTF-8')])\nprint(tokens.to_list())\n```\n\n```sh\n[['everything', 'not', 'saved', 'will', 'be', 'lost', '.'],\n ['Sad', '\\xe2\\x98\\xb9']]\n```\n\n### Unicode split\n\nWhen tokenizing languages without whitespace to segment words, it is common to\njust split by character, which can be accomplished using the\n[unicode_split](https://www.tensorflow.org/api_docs/python/tf/strings/unicode_split)\nop found in core.\n\n```python\ntokens = tf.strings.unicode_split([u\"\u4ec5\u4eca\u5e74\u524d\".encode('UTF-8')], 'UTF-8')\nprint(tokens.to_list())\n```\n\n```sh\n[['\\xe4\\xbb\\x85', '\\xe4\\xbb\\x8a', '\\xe5\\xb9\\xb4', '\\xe5\\x89\\x8d']]\n```\n\n### Offsets\n\nWhen tokenizing strings, it is often desired to know where in the original\nstring the token originated from. For this reason, each tokenizer which\nimplements `TokenizerWithOffsets` has a *tokenize_with_offsets* method that will\nreturn the byte offsets along with the tokens. The start_offsets lists the bytes\nin the original string each token starts at (inclusive), and the end_offsets\nlists the bytes where each token ends at (exclusive, i.e., first byte *after*\nthe token).\n\n```python\ntokenizer = text.UnicodeScriptTokenizer()\n(tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets(\n    ['everything not saved will be lost.', u'Sad\u2639'.encode('UTF-8')])\nprint(tokens.to_list())\nprint(start_offsets.to_list())\nprint(end_offsets.to_list())\n```\n\n```sh\n[['everything', 'not', 'saved', 'will', 'be', 'lost', '.'],\n ['Sad', '\\xe2\\x98\\xb9']]\n[[0, 11, 15, 21, 26, 29, 33], [0, 3]]\n[[10, 14, 20, 25, 28, 33, 34], [3, 6]]\n```\n\n### TF.Data Example\n\nTokenizers work as expected with the tf.data API. A simple example is provided\nbelow.\n\n```python\ndocs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'],\n                                           [\"It's a trap!\"]])\ntokenizer = text.WhitespaceTokenizer()\ntokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\niterator = tokenized_docs.make_one_shot_iterator()\nprint(iterator.get_next().to_list())\nprint(iterator.get_next().to_list())\n```\n\n```sh\n[['Never', 'tell', 'me', 'the', 'odds.']]\n[[\"It's\", 'a', 'trap!']]\n```\n\n### Keras API\n\nWhen you use different tokenizers and ops to preprocess your data, the resulting\noutputs are Ragged Tensors. The Keras API makes it easy now to train a model\nusing Ragged Tensors without having to worry about padding or masking the data,\nby either using the ToDense layer which handles all of these for you or relying\non Keras built-in layers support for natively working on ragged data.\n\n```python\nmodel = tf.keras.Sequential([\n  tf.keras.layers.InputLayer(input_shape=(None,), dtype='int32', ragged=True)\n  text.keras.layers.ToDense(pad_value=0, mask=True),\n  tf.keras.layers.Embedding(100, 16),\n  tf.keras.layers.LSTM(32),\n  tf.keras.layers.Dense(32, activation='relu'),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\n```\n\n## Other Text Ops\n\nTF.Text packages other useful preprocessing ops. We will review a couple below.\n\n### Wordshape\n\nA common feature used in some natural language understanding models is to see\nif the text string has a certain property. For example, a sentence breaking\nmodel might contain features which check for word capitalization or if a\npunctuation character is at the end of a string.\n\nWordshape defines a variety of useful regular expression based helper functions\nfor matching various relevant patterns in your input text. Here are a few\nexamples.\n\n```python\ntokenizer = text.WhitespaceTokenizer()\ntokens = tokenizer.tokenize(['Everything not saved will be lost.',\n                             u'Sad\u2639'.encode('UTF-8')])\n\n# Is capitalized?\nf1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n# Are all letters uppercased?\nf2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n# Does the token contain punctuation?\nf3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n# Is the token a number?\nf4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n\nprint(f1.to_list())\nprint(f2.to_list())\nprint(f3.to_list())\nprint(f4.to_list())\n```\n\n```sh\n[[True, False, False, False, False, False], [True]]\n[[False, False, False, False, False, False], [False]]\n[[False, False, False, False, False, True], [True]]\n[[False, False, False, False, False, False], [False]]\n```\n\n### N-grams & Sliding Window\n\nN-grams are sequential words given a sliding window size of *n*. When combining\nthe tokens, there are three reduction mechanisms supported. For text, you would\nwant to use `Reduction.STRING_JOIN` which appends the strings to each other.\nThe default separator character is a space, but this can be changed with the\nstring_separater argument.\n\nThe other two reduction methods are most often used with numerical values, and\nthese are `Reduction.SUM` and `Reduction.MEAN`.\n\n```python\ntokenizer = text.WhitespaceTokenizer()\ntokens = tokenizer.tokenize(['Everything not saved will be lost.',\n                             u'Sad\u2639'.encode('UTF-8')])\n\n# Ngrams, in this case bi-gram (n = 2)\nbigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n\nprint(bigrams.to_list())\n```\n\n```sh\n[['Everything not', 'not saved', 'saved will', 'will be', 'be lost.'], []]\n```\n\n## Installation\n\n### Install using PIP\n\nWhen installing TF Text with `pip install`, please note the version\nof TensorFlow you are running, as you should specify the corresponding version\nof TF Text. For example, if you're using TF 2.0, install the 2.0 version of TF\nText, and if you're using TF 1.15, install the 1.15 version of TF Text.\n\n```bash\npip install -U tensorflow-text==<version>\n```\n\n### A note about different operating system packages\n\nAfter version 2.10, we will only be providing pip packages for Linux x86_64 and\nIntel-based Macs. TensorFlow Text has always leveraged the release\ninfrastructure of the core TensorFlow package to more easily maintain compatible\nreleases with minimal maintenance, allowing the team to focus on TF Text itself\nand contributions to other parts of the TensorFlow ecosystem.\n\nFor other systems like Windows, Aarch64, and Apple Macs, TensorFlow relies on\n[build collaborators](https://blog.tensorflow.org/2022/09/announcing-tensorflow-official-build-collaborators.html),\nand so we will not be providing packages for them. However, we will continue to\naccept PRs to make building for these OSs easy for users, and will try to point\nto community efforts related to them.\n\n\n### Build from source steps:\n\nNote that TF Text needs to be built in the same environment as TensorFlow. Thus,\nif you manually build TF Text, it is highly recommended that you also build\nTensorFlow.\n\nIf building on MacOS, you must have coreutils installed. It is probably easiest\nto do with Homebrew.\n\n1. [build and install TensorFlow](https://www.tensorflow.org/install/source).\n1. Clone the TF Text repo:\n   ```Shell\n   git clone https://github.com/tensorflow/text.git\n   cd text\n   ```\n1. Run the build script to create a pip package:\n   ```Shell\n   ./oss_scripts/run_build.sh\n   ```\n   After this step, there should be a `*.whl` file in current directory. File name similar to `tensorflow_text-2.5.0rc0-cp38-cp38-linux_x86_64.whl`.\n1. Install the package to environment:\n   ```Shell\n   pip install ./tensorflow_text-*-*-*-os_platform.whl\n   ```\n\n### Build or test using TensorFlow's SIG docker image:\n\n1.  Pull image from\n    [Tensorflow SIG docker builds](https://hub.docker.com/r/tensorflow/build/tags).\n\n1.  Run a container based with the pulled image and create a bash session.\n    This can be done by running `docker run -it {image_name} bash`. <br />\n    `{image_name}` can be any name with `{tf_verison}-python{python_version}` format.\n    An example for python 3.10 and TF version 2.10 :- `2.10-python3.10`.\n1.  Clone the TF-Text Github repository inside container:  `git clone https://github.com/tensorflow/text.git`. <br />\n    Once cloned, change to the working directory using `cd text/`.\n1.  Run the configuration script(s): `./oss_scripts/configure.sh` and `./oss_scripts/prepare_tf_dep.sh`. <br />\n    This will update bazel and TF dependencies to installed tensorflow in the container.\n1.  To run the tests, use the bazel command: `bazel test --test_output=errors tensorflow_text:all`. This will run all the tests declared in the `BUILD` file. <br />\n    To run a specific test, modify the above command replacing `:all` with the test name (for example `:fast_bert_normalizer`).\n    \n1.  Build the pip package/wheel: \\\n    `bazel build --config=release_cpu_linux\n    oss_scripts/pip_package:build_pip_package` \\\n    `./bazel-bin/oss_scripts/pip_package/build_pip_package\n    /{wheel_dir}` <br />\n\n    Once the build is complete, you should see the wheel available under\n    `{wheel_dir}` directory.\n", "release_dates": ["2024-02-28T06:51:28Z", "2023-11-15T19:00:05Z", "2023-10-26T09:17:11Z", "2023-10-06T19:23:09Z", "2023-08-18T04:02:00Z", "2023-07-06T20:43:53Z", "2023-05-16T16:44:53Z", "2023-04-12T22:52:51Z", "2023-03-27T16:25:14Z", "2023-02-21T19:40:44Z", "2022-11-21T19:15:26Z", "2022-10-19T20:10:27Z", "2022-09-08T08:15:31Z", "2022-08-04T05:46:04Z", "2022-05-18T02:27:52Z", "2022-05-12T17:28:37Z", "2022-04-21T02:58:05Z", "2022-04-15T02:06:55Z", "2022-04-14T16:59:38Z", "2022-02-04T11:02:37Z", "2022-01-31T20:21:28Z", "2021-11-19T12:59:24Z", "2021-11-12T06:43:48Z", "2021-11-04T22:39:02Z", "2021-10-15T18:06:04Z", "2021-08-18T20:58:42Z", "2021-07-02T22:59:00Z", "2021-05-24T19:16:25Z", "2021-04-06T23:41:14Z", "2021-01-13T04:36:01Z"]}, {"name": "tf-build-actions", "description": "An Open Source Machine Learning Framework for Everyone", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "> **Warning**\n>\n> This is a temporary fork of TensorFlow, used by the TensorFlow team at Google to test new build configurations and Github Actions. Please go to https://github.com/tensorflow/tensorflow to contribute to or use TensorFlow.\n\n<div align=\"center\">\n  <img src=\"https://www.tensorflow.org/images/tf_logo_horizontal.png\">\n</div>\n\n[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://badge.fury.io/py/tensorflow)\n[![PyPI](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg)](https://doi.org/10.5281/zenodo.4724125)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/tensorflow/tensorflow/badge)](https://api.securityscorecards.dev/projects/github.com/tensorflow/tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow-py.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow-py)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)\n\n**`Documentation`** |\n------------------- |\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |\n\n[TensorFlow](https://www.tensorflow.org/) is an end-to-end open source platform\nfor machine learning. It has a comprehensive, flexible ecosystem of\n[tools](https://www.tensorflow.org/resources/tools),\n[libraries](https://www.tensorflow.org/resources/libraries-extensions), and\n[community](https://www.tensorflow.org/community) resources that lets\nresearchers push the state-of-the-art in ML and developers easily build and\ndeploy ML-powered applications.\n\nTensorFlow was originally developed by researchers and engineers working on the\nGoogle Brain team within Google's Machine Intelligence Research organization to\nconduct machine learning and deep neural networks research. The system is\ngeneral enough to be applicable in a wide variety of other domains, as well.\n\nTensorFlow provides stable [Python](https://www.tensorflow.org/api_docs/python)\nand [C++](https://www.tensorflow.org/api_docs/cc) APIs, as well as\nnon-guaranteed backward compatible API for\n[other languages](https://www.tensorflow.org/api_docs).\n\nKeep up-to-date with release announcements and security updates by subscribing\nto\n[announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).\nSee all the [mailing lists](https://www.tensorflow.org/community/forums).\n\n## Install\n\nSee the [TensorFlow install guide](https://www.tensorflow.org/install) for the\n[pip package](https://www.tensorflow.org/install/pip), to\n[enable GPU support](https://www.tensorflow.org/install/gpu), use a\n[Docker container](https://www.tensorflow.org/install/docker), and\n[build from source](https://www.tensorflow.org/install/source).\n\nTo install the current release, which includes support for\n[CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and\nWindows)*:\n\n```\n$ pip install tensorflow\n```\n\nOther devices (DirectX and MacOS-metal) are supported using\n[Device plugins](https://www.tensorflow.org/install/gpu_plugins#available_devices).\n\nA smaller CPU-only package is also available:\n\n```\n$ pip install tensorflow-cpu\n```\n\nTo update TensorFlow to the latest version, add `--upgrade` flag to the above\ncommands.\n\n*Nightly binaries are available for testing using the\n[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPi.*\n\n#### *Try your first TensorFlow program*\n\n```shell\n$ python\n```\n\n```python\n>>> import tensorflow as tf\n>>> tf.add(1, 2).numpy()\n3\n>>> hello = tf.constant('Hello, TensorFlow!')\n>>> hello.numpy()\nb'Hello, TensorFlow!'\n```\n\nFor more examples, see the\n[TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow, be sure to review the\n[contribution guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, please see\n[TensorFlow Discuss](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)\nfor general questions and discussion, and please direct specific questions to\n[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**\n\nThe TensorFlow project strives to abide by generally accepted best practices in\nopen-source software development.\n\n## Patching guidelines\n\nFollow these steps to patch a specific version of TensorFlow, for example, to\napply fixes to bugs or security vulnerabilities:\n\n*   Clone the TensorFlow repo and switch to the corresponding branch for your\n    desired TensorFlow version, for example, branch `r2.8` for version 2.8.\n*   Apply (that is, cherry pick) the desired changes and resolve any code\n    conflicts.\n*   Run TensorFlow tests and ensure they pass.\n*   [Build](https://www.tensorflow.org/install/source) the TensorFlow pip\n    package from source.\n\n## Continuous build status\n\nYou can find more community-supported platforms and configurations in the\n[TensorFlow SIG Build community builds table](https://github.com/tensorflow/build#community-supported-tensorflow-builds).\n\n### Official Builds\n\nBuild Type                    | Status                                                                                                                                                                           | Artifacts\n----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------\n**Linux CPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html)           | [PyPI](https://pypi.org/project/tf-nightly/)\n**Linux GPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html) | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Linux XLA**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html)         | TBA\n**macOS**                     | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html)     | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows CPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html)       | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows GPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html)       | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Android**                   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html)               | [Download](https://bintray.com/google/tensorflow/tensorflow/_latestVersion)\n**Raspberry Pi 0 and 1**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl)\n**Raspberry Pi 2 and 3**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)\n**Libtensorflow MacOS CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/macos/latest/macos_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Linux CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/cpu/ubuntu_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Linux GPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/gpu/ubuntu_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Windows CPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/cpu/windows_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Windows GPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/gpu/windows_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n\n## Resources\n\n*   [TensorFlow.org](https://www.tensorflow.org)\n*   [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)\n*   [TensorFlow Official Models](https://github.com/tensorflow/models/tree/master/official)\n*   [TensorFlow Examples](https://github.com/tensorflow/examples)\n*   [TensorFlow Codelabs](https://codelabs.developers.google.com/?cat=TensorFlow)\n*   [TensorFlow Blog](https://blog.tensorflow.org)\n*   [Learn ML with TensorFlow](https://www.tensorflow.org/resources/learn-ml)\n*   [TensorFlow Twitter](https://twitter.com/tensorflow)\n*   [TensorFlow YouTube](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)\n*   [TensorFlow model optimization roadmap](https://www.tensorflow.org/model_optimization/guide/roadmap)\n*   [TensorFlow White Papers](https://www.tensorflow.org/about/bib)\n*   [TensorBoard Visualization Toolkit](https://github.com/tensorflow/tensorboard)\n*   [TensorFlow Code Search](https://cs.opensource.google/tensorflow/tensorflow)\n\nLearn more about the\n[TensorFlow community](https://www.tensorflow.org/community) and how to\n[contribute](https://www.tensorflow.org/community/contribute).\n\n## Courses\n\n*   [Deep Learning with Tensorflow from Edx](https://www.edx.org/course/deep-learning-with-tensorflow)\n*   [DeepLearning.AI TensorFlow Developer Professional Certificate from Coursera](https://www.coursera.org/specializations/tensorflow-in-practice)\n*   [TensorFlow: Data and Deployment from Coursera](https://www.coursera.org/specializations/tensorflow-data-and-deployment)\n*   [Getting Started with TensorFlow 2 from Coursera](https://www.coursera.org/learn/getting-started-with-tensor-flow2)\n*   [TensorFlow: Advanced Techniques from Coursera](https://www.coursera.org/specializations/tensorflow-advanced-techniques)\n*   [TensorFlow 2 for Deep Learning Specialization from Coursera](https://www.coursera.org/specializations/tensorflow2-deeplearning)\n*   [Intro to TensorFlow for A.I, M.L, and D.L from Coursera](https://www.coursera.org/learn/introduction-tensorflow)\n*   [Machine Learning with TensorFlow on GCP from Coursera](https://www.coursera.org/specializations/machine-learning-tensorflow-gcp)\n*   [Intro to TensorFlow for Deep Learning from Udacity](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187)\n*   [Introduction to TensorFlow Lite from Udacity](https://www.udacity.com/course/intro-to-tensorflow-lite--ud190)\n\n## License\n\n[Apache License 2.0](LICENSE)\n", "release_dates": []}, {"name": "tfhub.dev", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "**Warning: unmigrated tfhub.dev model artifacts will be deleted on March 18,\n2024.**\n\nAs of November 15th 2023, most [tfhub.dev](https://tfhub.dev) URLs and model\nhandles are now redirecting to their migrated/equivalent counterpart on Kaggle\nModels.\n\nOn March 18, 2024, all unmigrated model assets previously surfaced on tfhub.dev\nwill be deleted \u2013 after this date, `hub.load` and `hub.KerasLayer` calls to\nthese tfhub.dev handles will fail permanently. See the list of unmigrated model\nassets here:\n\n-   [inaturalist/vision/embedder/inaturalist_V2](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/inaturalist/models/vision/embedder/inaturalist_V2)\n-   [nvidia/unet/industrial/class_1](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_1)\n-   [nvidia/unet/industrial/class_2](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_2)\n-   [nvidia/unet/industrial/class_3](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_3)\n-   [nvidia/unet/industrial/class_4](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_4)\n-   [nvidia/unet/industrial/class_5](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_5)\n-   [nvidia/unet/industrial/class_6](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_6)\n-   [nvidia/unet/industrial/class_7](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_7)\n-   [nvidia/unet/industrial/class_8](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_8)\n-   [nvidia/unet/industrial/class_9](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_9)\n-   [nvidia/unet/industrial/class_10](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/nvidia/models/unet/industrial/class_10)\n-   [silero/silero-stt/de](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/silero/models/silero-stt/de)\n-   [silero/silero-stt/en](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/silero/models/silero-stt/en)\n-   [silero/silero-stt/es](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/silero/models/silero-stt/es)\n-   [svampeatlas/vision/classifier/fungi_mobile_V1](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/svampeatlas/models/vision/classifier/fungi_mobile_V1)\n-   [svampeatlas/vision/embedder/fungi_V2](https://github.com/tensorflow/tfhub.dev/tree/master/assets/docs/svampeatlas/models/vision/embedder/fungi_V2)\n\n**If you are an owner of an unmigrated model, please get in touch with us at\nkaggle-models@google.com if you'd like to migrate your model. If you take no\naction, your model(s) will be deleted on March 18, 2024 and not retrievable\n(either by you or other users).**\n\nFor models with a Kaggle Models copy, there will be no impact on the\navailability/functionality of models that were copied from tfhub.dev \u2013\n`tensorflow_hub` will continue to support downloading models that were initially\nuploaded to tfhub.dev via\ne.g. `hub.load(\"https://tfhub.dev/<publisher>/<model>/<version>\")`. To see if a\ntfhub.dev model has been migrated, enter the model handle in your URL bar \u2013 if\nthe redirect is successful, it has already been migrated, otherwise it is an\nunmigrated model and will be subject to deletion.\n\nAlthough no migration or code rewrites are explicitly required, we recommend\nreplacing tfhub.dev links with their Kaggle Models counterparts to improve code\nhealth and debuggability.\n\nSee FAQs [here](https://kaggle.com/tfhub-dev-faqs).\n", "release_dates": []}, {"name": "tfjs", "description": "A WebGL accelerated JavaScript library for training and deploying ML models.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow.js\n\nTensorFlow.js is an open-source hardware-accelerated JavaScript library for\ntraining and deploying machine learning models.\n\n\n**Develop ML in the Browser** <br/>\nUse flexible and intuitive APIs to build models from scratch using the low-level\nJavaScript linear algebra library or the high-level layers API.\n\n**Develop ML in Node.js** <br/>\nExecute native TensorFlow with the same TensorFlow.js API under the Node.js\nruntime.\n\n**Run Existing models** <br/>\nUse TensorFlow.js model converters to run pre-existing TensorFlow models right\nin the browser.\n\n**Retrain Existing models** <br/>\nRetrain pre-existing ML models using sensor data connected to the browser or\nother client-side data.\n\n## About this repo\n\nThis repository contains the logic and scripts that combine\nseveral packages.\n\nAPIs:\n- [TensorFlow.js Core](/tfjs-core),\n  a flexible low-level API for neural networks and numerical computation.\n- [TensorFlow.js Layers](/tfjs-layers),\n  a high-level API which implements functionality similar to\n  [Keras](https://keras.io/).\n- [TensorFlow.js Data](/tfjs-data),\n  a simple API to load and prepare data analogous to\n  [tf.data](https://www.tensorflow.org/guide/datasets).\n- [TensorFlow.js Converter](/tfjs-converter),\n  tools to import a TensorFlow SavedModel to TensorFlow.js\n- [TensorFlow.js Vis](/tfjs-vis),\n  in-browser visualization for TensorFlow.js models\n- [TensorFlow.js AutoML](/tfjs-automl),\n  Set of APIs to load and run models produced by\n  [AutoML Edge](https://cloud.google.com/vision/automl/docs/edge-quickstart).\n\n\nBackends/Platforms:\n- [TensorFlow.js CPU Backend](/tfjs-backend-cpu), pure-JS backend for Node.js and the browser.\n- [TensorFlow.js WebGL Backend](/tfjs-backend-webgl), WebGL backend for the browser.\n- [TensorFlow.js WASM Backend](/tfjs-backend-wasm), WebAssembly backend for the browser.\n- [TensorFlow.js WebGPU](/tfjs-backend-webgpu), WebGPU backend for the browser.\n- [TensorFlow.js Node](/tfjs-node), Node.js platform via TensorFlow C++ adapter.\n- [TensorFlow.js React Native](/tfjs-react-native), React Native platform via expo-gl adapter.\n\nIf you care about bundle size, you can import those packages individually.\n\nIf you are looking for Node.js support, check out the [TensorFlow.js Node directory](/tfjs-node).\n\n## Examples\n\nCheck out our\n[examples repository](https://github.com/tensorflow/tfjs-examples)\nand our [tutorials](https://js.tensorflow.org/tutorials/).\n\n## Gallery\n\nBe sure to check out [the gallery](GALLERY.md) of all projects related to TensorFlow.js.\n\n## Pre-trained models\n\nBe sure to also check out our [models repository](https://github.com/tensorflow/tfjs-models) where we host pre-trained models\non NPM.\n\n## Benchmarks\n\n* [Local benchmark tool](https://tfjs-benchmarks.web.app/). Use this webpage tool to collect the performance related metrics (speed, memory, etc) of TensorFlow.js models and kernels **on your local device** with CPU, WebGL or WASM backends. You can benchmark custom models by following this [guide](https://github.com/tensorflow/tfjs/blob/master/e2e/benchmarks/local-benchmark/README.md).\n* [Multi-device benchmark tool](https://github.com/tensorflow/tfjs/tree/master/e2e/benchmarks/browserstack-benchmark/README.md). Use this tool to collect the same performance related metrics **on a collection of remote devices**.\n\n## Getting started\n\nThere are two main ways to get TensorFlow.js in your JavaScript project:\nvia <a href=\"https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_JavaScript_within_a_webpage\" target=\"_blank\">script tags</a> <strong>or</strong> by installing it from <a href=\"https://www.npmjs.com/\" target=\"_blank\">NPM</a>\nand using a build tool like <a href=\"https://parceljs.org/\" target=\"_blank\">Parcel</a>,\n<a href=\"https://webpack.js.org/\" target=\"_blank\">WebPack</a>, or <a href=\"https://rollupjs.org/guide/en\" target=\"_blank\">Rollup</a>.\n\n### via Script Tag\n\nAdd the following code to an HTML file:\n\n```html\n<html>\n  <head>\n    <!-- Load TensorFlow.js -->\n    <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js\"> </script>\n\n\n    <!-- Place your code in the script tag below. You can also use an external .js file -->\n    <script>\n      // Notice there is no 'import' statement. 'tf' is available on the index-page\n      // because of the script tag above.\n\n      // Define a model for linear regression.\n      const model = tf.sequential();\n      model.add(tf.layers.dense({units: 1, inputShape: [1]}));\n\n      // Prepare the model for training: Specify the loss and the optimizer.\n      model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n\n      // Generate some synthetic data for training.\n      const xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);\n      const ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);\n\n      // Train the model using the data.\n      model.fit(xs, ys).then(() => {\n        // Use the model to do inference on a data point the model hasn't seen before:\n        // Open the browser devtools to see the output\n        model.predict(tf.tensor2d([5], [1, 1])).print();\n      });\n    </script>\n  </head>\n\n  <body>\n  </body>\n</html>\n```\n\nOpen up that HTML file in your browser, and the code should run!\n\n### via NPM\n\nAdd TensorFlow.js to your project using <a href=\"https://yarnpkg.com/en/\" target=\"_blank\">yarn</a> <em>or</em> <a href=\"https://docs.npmjs.com/cli/npm\" target=\"_blank\">npm</a>. <b>Note:</b> Because\nwe use ES2017 syntax (such as `import`), this workflow assumes you are using a modern browser or a bundler/transpiler\nto convert your code to something older browsers understand. See our\n<a href='https://github.com/tensorflow/tfjs-examples' target=\"_blank\">examples</a>\nto see how we use <a href=\"https://parceljs.org/\" target=\"_blank\">Parcel</a> to build\nour code. However, you are free to use any build tool that you prefer.\n\n\n\n```js\nimport * as tf from '@tensorflow/tfjs';\n\n// Define a model for linear regression.\nconst model = tf.sequential();\nmodel.add(tf.layers.dense({units: 1, inputShape: [1]}));\n\n// Prepare the model for training: Specify the loss and the optimizer.\nmodel.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n\n// Generate some synthetic data for training.\nconst xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);\nconst ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);\n\n// Train the model using the data.\nmodel.fit(xs, ys).then(() => {\n  // Use the model to do inference on a data point the model hasn't seen before:\n  model.predict(tf.tensor2d([5], [1, 1])).print();\n});\n```\n\nSee our <a href=\"https://js.tensorflow.org/tutorials/\" target=\"_blank\">tutorials</a>, <a href=\"https://github.com/tensorflow/tfjs-examples\" target=\"_blank\">examples</a>\nand <a href=\"https://js.tensorflow.org/api/latest/\">documentation</a> for more details.\n\n## Importing pre-trained models\n\nWe support porting pre-trained models from:\n- [TensorFlow SavedModel](https://www.tensorflow.org/js/tutorials/conversion/import_saved_model)\n- [Keras](https://js.tensorflow.org/tutorials/import-keras.html)\n\n## Various ops supported in different backends\n\nPlease refer below :\n- [TFJS Ops Matrix](https://docs.google.com/spreadsheets/d/1D25XtWaBrmUEErbGQB0QmNhH-xtwHo9LDl59w0TbxrI/edit#gid=0)\n\n## Find out more\n\n[TensorFlow.js](https://js.tensorflow.org) is a part of the\n[TensorFlow](https://www.tensorflow.org) ecosystem. For more info:\n- For help from the community, use the `tfjs` tag on the [TensorFlow Forum](https://discuss.tensorflow.org/tag/tfjs).\n- [TensorFlow.js Website](https://js.tensorflow.org)\n- [Tutorials](https://js.tensorflow.org/tutorials)\n- [API reference](https://js.tensorflow.org/api/latest/)\n- [TensorFlow.js Blog](https://blog.tensorflow.org/search?label=TensorFlow.js)\n\nThanks, <a href=\"https://www.browserstack.com/\">BrowserStack</a>, for providing testing support.\n", "release_dates": ["2024-01-31T00:08:32Z", "2024-01-11T02:07:13Z", "2023-12-13T22:23:39Z", "2023-11-30T20:34:44Z", "2023-11-30T21:43:46Z", "2023-11-09T18:27:49Z", "2023-10-31T04:12:41Z", "2023-10-18T18:19:38Z", "2023-10-13T20:22:26Z", "2023-09-13T18:50:31Z", "2023-08-01T20:27:30Z", "2023-07-24T18:01:12Z", "2023-07-19T19:42:21Z", "2023-06-22T00:51:53Z", "2023-06-06T20:56:00Z", "2023-05-17T22:11:48Z", "2023-05-05T18:12:52Z", "2023-04-07T18:25:02Z", "2023-03-20T19:37:05Z", "2023-01-04T00:24:43Z", "2022-11-21T21:49:00Z", "2022-10-13T22:03:38Z", "2022-10-06T21:29:00Z", "2022-10-03T23:44:12Z", "2022-08-24T00:00:05Z", "2022-07-22T23:45:25Z", "2022-05-23T17:35:08Z", "2022-05-16T21:55:43Z", "2022-04-20T18:56:58Z", "2022-03-23T21:25:52Z"]}, {"name": "tfjs-converter", "description": "Convert TensorFlow SavedModel and Keras models to TensorFlow.js", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# This repository has been archived in favor of tensorflow/tfjs.\n\nThis repo will remain around for some time to keep history but all future PRs should be sent to [tensorflow/tfjs](https://github.com/tensorflow/tfjs) inside the tfjs-core folder.\n\nAll history and contributions have been preserved in the monorepo.", "release_dates": ["2018-05-10T02:44:49Z", "2018-05-06T14:44:56Z", "2018-04-26T06:34:39Z", "2018-04-19T21:11:30Z"]}, {"name": "tfjs-core", "description": "WebGL-accelerated ML // linear algebra // automatic differentiation for JavaScript.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# This repository has been archived in favor of tensorflow/tfjs.\n\nThis repo will remain around for some time to keep history but all future PRs should be sent to [tensorflow/tfjs](https://github.com/tensorflow/tfjs) inside the tfjs-core folder.\n\nAll history and contributions have been preserved in the monorepo.", "release_dates": ["2018-03-30T14:13:51Z", "2018-02-17T03:07:34Z", "2017-10-09T20:41:26Z", "2017-09-07T20:18:02Z", "2017-08-04T17:45:12Z"]}, {"name": "tfjs-data", "description": "Simple APIs to load and prepare data for use in machine learning models", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# This repository has been archived in favor of tensorflow/tfjs.\n\nThis repo will remain around for some time to keep history but all future PRs should be sent to [tensorflow/tfjs](https://github.com/tensorflow/tfjs) inside the tfjs-data folder.\n\nAll history and contributions have been preserved in the monorepo.", "release_dates": []}, {"name": "tfjs-examples", "description": "Examples built with TensorFlow.js", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow.js Examples\n\nThis repository contains a set of examples implemented in\n[TensorFlow.js](http://js.tensorflow.org).\n\nEach example directory is standalone so the directory can be copied\nto another project.\n\n# Overview of Examples\n\n<table>\n  <tr>\n    <th>Example name</th>\n    <th>Demo link</th>\n    <th>Input data type</th>\n    <th>Task type</th>\n    <th>Model type</th>\n    <th>Training</th>\n    <th>Inference</th>\n    <th>API type</th>\n    <th>Save-load operations</th>\n  <tr>\n    <td><a href=\"./abalone-node\">abalone-node</a></td>\n    <td></td>\n    <td>Numeric</td>\n    <td>Loading data from local file and training in Node.js</td>\n    <td>Multilayer perceptron</td>\n    <td>Node.js</td>\n    <td>Node.js</td>\n    <td>Layers</td>\n    <td>Saving to filesystem and loading in Node.js</td>\n  </tr>\n  <tr>\n    <td><a href=\"./addition-rnn\">addition-rnn</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/addition-rnn/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Text</td>\n    <td>Sequence-to-sequence</td>\n    <td>RNN: SimpleRNN, GRU and LSTM</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./addition-rnn-webworker\">addition-rnn-webworker</a></td>\n    <td></td>\n    <td>Text</td>\n    <td>Sequence-to-sequence</td>\n    <td>RNN: SimpleRNN, GRU and LSTM</td>\n    <td>Browser: Web Worker</td>\n    <td>Browser: Web Worker</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./angular-predictive-prefetching\">angular-predictive-prefetching</a></td>\n    <td></td>\n    <td>Numeric</td>\n    <td>Multiclass predictor</td>\n    <td>DNN</td>\n    <td></td>\n    <td>Browser: Service Worker</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./baseball-node\">baseball-node</a></td>\n    <td></td>\n    <td>Numeric</td>\n    <td>Multiclass classification</td>\n    <td>Multilayer perceptron</td>\n    <td>Node.js</td>\n    <td>Node.js</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./boston-housing\">boston-housing</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/boston-housing/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Numeric</td>\n    <td>Regression</td>\n    <td>Multilayer perceptron</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./cart-pole\">cart-pole</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/cart-pole/dist/index.html\">\ud83d\udd17</a></td>\n    <td></td>\n    <td>Reinforcement learning</td>\n    <td>Policy gradient</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td>IndexedDB</td>\n  </tr>\n  <tr>\n    <td><a href=\"./chrome-extension\">chrome-extension</a></td>\n    <td></td>\n    <td>Image</td>\n    <td>(Deploying TF.js in Chrome extension)</td>\n    <td>Convnet</td>\n    <td></td>\n    <td>Browser</td>\n    <td></td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./custom-layer\">custom-layer</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/custom-layer/dist/index.html\">\ud83d\udd17</a></td>\n    <td></td>\n    <td>(Defining a custom Layer subtype)</td>\n    <td></td>\n    <td></td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./data-csv\">data-csv</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/data-csv/dist/index.html\">\ud83d\udd17</a></td>\n    <td></td>\n    <td>Building a tf.data.Dataset from a remote CSV</td>\n    <td></td>\n    <td></td>\n    <td></td>\n    <td></td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./data-generator\">data-generator</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/data-generator/dist/index.html\">\ud83d\udd17</a></td>\n    <td></td>\n    <td>Building a tf.data.Dataset using a generator</td>\n    <td>Regression</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./date-conversion-attention\">date-conversion-attention</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/date-conversion-attention/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Text</td>\n    <td>Text-to-text conversion</td>\n    <td>Attention mechanism, RNN</td>\n    <td>Node.js</td>\n    <td>Browser and Node.js</td>\n    <td>Layers</td>\n    <td>Saving to filesystem and loading in browser</td>\n  </tr>\n  <tr>\n    <td><a href=\"./electron\">electron</a></td>\n    <td></td>\n    <td>Image</td>\n    <td>(Deploying TF.js in Electron-based desktop apps)</td>\n    <td>Convnet</td>\n    <td></td>\n    <td>Node.js</td>\n    <td></td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./fashion-mnist-vae\">fashion-mnist-vae</a></td>\n    <td></td>\n    <td>Image</td>\n    <td>Generative</td>\n    <td>Variational autoencoder (VAE)</td>\n    <td>Node.js</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td>Export trained model from tfjs-node and load it in browser</td>\n  </tr>\n  <tr>\n    <td><a href=\"./interactive-visualizers\">interactive-visualizers</a></td>\n    <td></td>\n    <td>Image</td>\n    <td>Multiclass classification, object detection, segmentation</td>\n    <td></td>\n    <td></td>\n    <td>Browser</td>\n    <td></td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./iris\">iris</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/iris/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Numeric</td>\n    <td>Multiclass classification</td>\n    <td>Multilayer perceptron</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./iris-fitDataset\">iris-fitDataset</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/iris-fitDataset/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Numeric</td>\n    <td>Multiclass classification</td>\n    <td>Multilayer perceptron</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./jena-weather\">jena-weather</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/jena-weather/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Sequence</td>\n    <td>Sequence-to-prediction</td>\n    <td>MLP and RNNs</td>\n    <td>Browser and Node</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./lstm-text-generation\">lstm-text-generation</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/lstm-text-generation/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Text</td>\n    <td>Sequence prediction</td>\n    <td>RNN: LSTM</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td>IndexedDB</td>\n  </tr>\n  <tr>\n    <td><a href=\"./mnist\">mnist</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/mnist/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Image</td>\n    <td>Multiclass classification</td>\n    <td>Convolutional neural network</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./mnist-acgan\">mnist-acgan</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/mnist-acgan/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Image</td>\n    <td>Generative Adversarial Network (GAN)</td>\n    <td>Convolutional neural network; GAN</td>\n    <td>Node.js</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td>Saving to filesystem from Node.js and loading it in the browser</td>\n  </tr>\n  <tr>\n    <td><a href=\"./mnist-core\">mnist-core</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/mnist-core/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Image</td>\n    <td>Multiclass classification</td>\n    <td>Convolutional neural network</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Core (Ops)</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./mnist-node\">mnist-node</a></td>\n    <td></td>\n    <td>Image</td>\n    <td>Multiclass classification</td>\n    <td>Convolutional neural network</td>\n    <td>Node.js</td>\n    <td>Node.js</td>\n    <td>Layers</td>\n    <td>Saving to filesystem</td>\n  </tr>\n  <tr>\n    <td><a href=\"./mnist-transfer-cnn\">mnist-transfer-cnn</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/mnist-transfer-cnn/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Image</td>\n    <td>Multiclass classification (transfer learning)</td>\n    <td>Convolutional neural network</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td>Loading pretrained model</td>\n  </tr>\n  <tr>\n    <td><a href=\"./mobilenet\">mobilenet</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/mobilenet/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Image</td>\n    <td>Multiclass classification</td>\n    <td>Convolutional neural network</td>\n    <td></td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td>Loading pretrained model</td>\n  </tr>\n  <tr>\n    <td><a href=\"./polynomial-regression\">polynomial-regression</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/polynomial-regression/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Numeric</td>\n    <td>Regression</td>\n    <td>Shallow neural network</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./polynomial-regression-core\">polynomial-regression-core</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/polynomial-regression-core/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Numeric</td>\n    <td>Regression</td>\n    <td>Shallow neural network</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Core (Ops)</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./quantization\">quantization</a></td>\n    <td></td>\n    <td>Various</td>\n    <td>Demonstrates the effect of post-training weight quantization</td>\n    <td>Various</td>\n    <td>Node.js</td>\n    <td>Node.js</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./sentiment\">sentiment</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/sentiment/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Text</td>\n    <td>Sequence-to-binary-prediction</td>\n    <td>LSTM, 1D convnet</td>\n    <td>Node.js or Python</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td>Load model from Keras and tfjs-node</td>\n  </tr>\n  <tr>\n    <td><a href=\"./simple-object-detection\">simple-object-detection</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/simple-object-detection/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Image</td>\n    <td>Object detection</td>\n    <td>Convolutional neural network (transfer learning)</td>\n    <td>Node.js</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td>Export trained model from tfjs-node and load it in browser</td>\n  </tr>\n  <tr>\n    <td><a href=\"./snake-dqn\">snake-dqn</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/snake-dqn/index.html\">\ud83d\udd17</a></td>\n    <td></td>\n    <td>Reinforcement learning</td>\n    <td>Deep Q-Network (DQN)</td>\n    <td>Node.js</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td>Export trained model from tfjs-node and load it in browser</td>\n  </tr>\n  <tr>\n    <td><a href=\"./translation\">translation</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/translation/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Text</td>\n    <td>Sequence-to-sequence</td>\n    <td>LSTM encoder and decoder</td>\n    <td>Node.js or Python</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td>Load model converted from Keras</td>\n  </tr>\n  <tr>\n    <td><a href=\"./tsne-mnist-canvas\">tsne-mnist-canvas</a></td>\n    <td></td>\n    <td></td>\n    <td>Dimension reduction and data visualization</td>\n    <td>tSNE</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Core (Ops)</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td><a href=\"./webcam-transfer-learning\">webcam-transfer-learning</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/webcam-transfer-learning/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Image</td>\n    <td>Multiclass classification (transfer learning)</td>\n    <td>Convolutional neural network</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td>Loading pretrained model</td>\n  </tr>\n  <tr>\n    <td><a href=\"./website-phishing\">website-phishing</a></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-examples/website-phishing/dist/index.html\">\ud83d\udd17</a></td>\n    <td>Numeric</td>\n    <td>Binary classification</td>\n    <td>Multilayer perceptron</td>\n    <td>Browser</td>\n    <td>Browser</td>\n    <td>Layers</td>\n    <td></td>\n  </tr>\n</table>\n\n# Dependencies\n\nExcept for `getting_started`, all the examples require the following dependencies to be installed.\n\n - Node.js version 8.9 or higher\n - [NPM cli](https://docs.npmjs.com/cli/npm) OR [Yarn](https://yarnpkg.com/en/)\n\n## How to build an example\n`cd` into the directory\n\nIf you are using `yarn`:\n\n```sh\ncd mnist-core\nyarn\nyarn watch\n```\n\nIf you are using `npm`:\n```sh\ncd mnist-core\nnpm install\nnpm run watch\n```\n\n### Details\n\nThe convention is that each example contains two scripts:\n\n- `yarn watch` or `npm run watch`: starts a local development HTTP server which watches the\nfilesystem for changes so you can edit the code (JS or HTML) and see changes when you refresh the page immediately.\n\n- `yarn build` or `npm run build`: generates a `dist/` folder which contains the build artifacts and\ncan be used for deployment.\n\n## Contributing\n\nIf you want to contribute an example, please reach out to us on\n[Github issues](https://github.com/tensorflow/tfjs/issues)\nbefore sending us a pull request as we are trying to keep this set of examples\nsmall and highly curated.\n\n### Running Presubmit Tests\n\nBefore you send a pull request, it is a good idea to run the presubmit tests\nand make sure they all pass. To do that, execute the following commands in the\nroot directory of tfjs-examples:\n\n```sh\nyarn\nyarn presubmit\n```\n\nThe `yarn presubmit` command executes the unit tests and lint checks of all\nthe exapmles that contain the `yarn test` and/or `yarn lint` scripts. You\nmay also run the tests for individual exampls by cd'ing into their respective\nsubdirectory and executing `yarn`, followed by `yarn test` and/or `yarn lint`.\n", "release_dates": []}, {"name": "tfjs-layers", "description": "TensorFlow.js high-level layers API", "language": "TypeScript", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# This repository has been archived in favor of tensorflow/tfjs.\n\nThis repo will remain around for some time to keep history but all future PRs should be sent to [tensorflow/tfjs](https://github.com/tensorflow/tfjs) inside the [tfjs-layers](https://github.com/tensorflow/tfjs/tree/master/tfjs-layers) folder.\n\nAll history and contributions have been preserved in the monorepo.\n", "release_dates": []}, {"name": "tfjs-models", "description": "Pretrained models for TensorFlow.js", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Pre-trained TensorFlow.js models\n\nThis repository hosts a set of pre-trained models that have been ported to\nTensorFlow.js.\n\nThe models are hosted on NPM and unpkg so they can be used in any project out of the box. They can be used directly or used in a transfer learning\nsetting with TensorFlow.js.\n\nTo find out about APIs for models, look at the README in each of the respective\ndirectories. In general, we try to hide tensors so the API can be used by\nnon-machine learning experts.\n\nFor those interested in contributing a model, please file a [GitHub issue on tfjs](https://github.com/tensorflow/tfjs/issues) to gauge\ninterest. We are trying to add models that complement the existing set of models\nand can be used as building blocks in other apps.\n\n## Models\n\n<table style=\"max-width:100%;table-layout:auto;\">\n  <tr style=\"text-align:center;\">\n    <th>Type</th>\n    <th>Model</th>\n    <th>Demo</th>\n    <th>Details</th>\n    <th>Install</th>\n  </tr>\n  <!-- Images -->\n  <!-- ** MobileNet -->\n  <tr>\n    <td rowspan=\"12\"><b>Images</b></td>\n    <td rowspan=\"2\"><b><a style=\"white-space:nowrap; display:inline-block;\" href=\"./mobilenet\"><div style='vertical-align:middle; display:inline;'>MobileNet</div></a></b></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-models/demos/mobilenet/index.html\">live</a></td>\n    <td rowspan=\"2\">Classify images with labels from the <a href=\"http://www.image-net.org/\">ImageNet database</a>.</td>\n    <td rowspan=\"2\"><code>npm i @tensorflow-models/mobilenet</code></td>\n  </tr>\n  <tr>\n    <td><a href=\"./mobilenet/demo/index.html\">source</a></td>\n  </tr>\n  <!-- ** Hand -->\n  <tr>\n    <td rowspan=\"2\"><b><a style=\"white-space:nowrap; display:inline-block;\" href=\"./hand-pose-detection\"><div style='vertical-align:middle; display:inline;'>Hand</div></a></b></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-models/demos/hand-pose-detection/index.html?model=mediapipe_hands\">live</a></td>\n    <td rowspan=\"2\">Real-time hand pose detection in the browser using TensorFlow.js.</td>\n    <td rowspan=\"2\"><code>npm i @tensorflow-models/hand-pose-detection</code></td>\n  </tr>\n  <tr>\n    <td><a href=\"./hand-pose-detection/demos/live_video/index.html\">source</a></td>\n  </tr>\n    <!-- ** Pose -->\n  <tr>\n    <td rowspan=\"2\"><b><a style=\"white-space:nowrap; display:inline-block;\" href=\"./pose-detection\"><div style='vertical-align:middle; display:inline;'>Pose</div></a></b></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=movenet\">live</a></td>\n    <td rowspan=\"2\">An API for real-time human pose detection in the browser.</td>\n    <td rowspan=\"2\"><code>npm i @tensorflow-models/pose-detection</code></td>\n  </tr>\n  <tr>\n    <td><a href=\"./pose-detection/demos/live_video/index.html\">source</a></td>\n  </tr>\n  <!-- ** Coco SSD -->\n  <tr>\n    <td rowspan=\"2\"><b><a style=\"white-space:nowrap; display:inline-block;\" href=\"./coco-ssd\"><div style='vertical-align:middle; display:inline;'>Coco SSD</div></a></b></td>\n    <td><a href=\"\"></a></td>\n    <td rowspan=\"2\">Object detection model that aims to localize and identify multiple objects in a single image. Based on the <a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/README.md\">TensorFlow object detection API</a>.</td>\n    <td rowspan=\"2\"><code>npm i @tensorflow-models/coco-ssd</code></td>\n  </tr>\n  <tr>\n    <td><a href=\"./coco-ssd/demo\">source</a></td>\n  </tr>\n    <td rowspan=\"2\"><b><a style=\"white-space:nowrap; display:inline-block;\" href=\"./deeplab\"><div style='vertical-align:middle; display:inline;'>DeepLab v3</div></a></b></td>\n    <td><a href=\"\"></a></td>\n    <td rowspan=\"2\">Semantic segmentation</td>\n    <td rowspan=\"2\"><code>npm i @tensorflow-models/deeplab</code></td>\n  </tr>\n  <tr>\n    <td><a href=\"./deeplab/demo\">source</a></td>\n  </tr>\n    <!-- ** Face Landmark Detection -->\n  <tr>\n    <td rowspan=\"2\"><b><a style=\"white-space:nowrap; display:inline-block;\" href=\"./face-landmarks-detection\"><div style='vertical-align:middle; display:inline;'>Face Landmark Detection</div></a></b></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-models/demos/face-landmarks-detection/index.html?model=mediapipe_face_mesh\">live</a></td>\n    <td rowspan=\"2\">Real-time 3D facial landmarks detection to infer the approximate surface geometry of a human face</td>\n    <td rowspan=\"2\"><code>npm i @tensorflow-models/face-landmarks-detection</code></td>\n  </tr>\n  <tr>\n    <td><a href=\"./face-landmarks-detection/demos\">source</a></td>\n  </tr>\n\n  <!-- * Audio -->\n  <!-- ** Speech Commands -->\n  <tr>\n    <td rowspan=\"2\"><b>Audio</b></td>\n    <td rowspan=\"2\"><b><a style=\"white-space:nowrap; display:inline-block;\" href=\"./speech-commands\"><div style='vertical-align:middle; display:inline;'>Speech Commands</div></a></b></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-speech-model-test/2019-01-03a/dist/index.html\">live</a></td>\n    <td rowspan=\"2\">Classify 1 second audio snippets from the <a href=\"https://www.tensorflow.org/tutorials/sequences/audio_recognition\">speech commands dataset</a>.</td>\n    <td rowspan=\"2\"><code>npm i @tensorflow-models/speech-commands</code></td>\n  </tr>\n  <tr>\n    <td><a href=\"./speech-commands/demo/index.html\">source</a></td>\n  </tr>\n  <!-- * Text -->\n  <!-- ** Universal Sentence Encoder -->\n  <tr>\n    <td rowspan=\"4\"><b>Text</b></td>\n    <td rowspan=\"2\"><b><a style=\"white-space:nowrap; display:inline-block;\" href=\"./universal-sentence-encoder\"><div style='vertical-align:middle; display:inline;'>Universal Sentence Encoder</div></a></b></td>\n    <td><a href=\"\"></a></td>\n    <td rowspan=\"2\">Encode text into a 512-dimensional embedding to be used as inputs to natural language processing tasks such as sentiment classification and textual similarity.</td>\n    <td rowspan=\"2\"><code>npm i @tensorflow-models/universal-sentence-encoder</code></td>\n  </tr>\n  <tr>\n    <td><a href=\"./universal-sentence-encoder/demo\">source</a></td>\n  </tr>\n  <!-- ** Text Toxicity -->\n  <tr>\n    <td rowspan=\"2\"><b><a style=\"white-space:nowrap; display:inline-block;\" href=\"./toxicity\"><div style='vertical-align:middle; display:inline;'>Text Toxicity</div></a></b></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-models/demos/toxicity/index.html\">live</a></td>\n    <td rowspan=\"2\">Score the perceived impact a comment might have on a conversation, from \"Very toxic\" to \"Very healthy\".</td>\n    <td rowspan=\"2\"><code>npm i @tensorflow-models/toxicity</code></td>\n  </tr>\n  <tr>\n    <td><a href=\"./toxicity/demo/index.html\">source</a></td>\n  </tr>\n  <!-- * Depth Estimation -->\n  <!-- ** Portrait Depth -->\n  <tr>\n    <td rowspan=\"2\"><b>Depth Estimation</b></td>\n    <td rowspan=\"2\"><b><a style=\"white-space:nowrap; display:inline-block;\" href=\"./depth-estimation\"><div style='vertical-align:middle; display:inline;'>Portrait Depth</div></a></b></td>\n    <td><a href=\"https://storage.googleapis.com/tfjs-models/demos/3dphoto/index.html\">live</a></td>\n    <td rowspan=\"2\">Estimate per-pixel depth (the distance to the camera center) for a single portrait image, which can be further used for creative applications such as <a href=\"https://blog.tensorflow.org/2022/05/portrait-depth-api-turning-single-image.html?linkId=8063793\">3D photo</a> and <a href=\"https://storage.googleapis.com/tfjs-models/demos/relighting/index.html\">relighting</a>.</td>\n    <td rowspan=\"2\"><code>npm i @tensorflow-models/depth-estimation</code></td>\n  </tr>\n  <tr>\n    <td><a href=\"./depth-estimation/demos/3d_photo/index.html\">source</a></td>\n  </tr>\n  <!-- * General Utilities -->\n  <tr>\n    <td rowspan=\"2\"><b>General Utilities</b></td>\n  <!-- ** KNN Classifier -->\n    <td rowspan=\"2\"><b><a style=\"white-space:nowrap; display:inline-block;\" href=\"./knn-classifier\"><div style='vertical-align:middle; display:inline;'>KNN Classifier</div></a></b></td>\n    <td><a href=\"\"></a></td>\n    <td rowspan=\"2\">This package provides a utility for creating a classifier using the K-Nearest Neighbors algorithm. Can be used for transfer learning.</td>\n    <td rowspan=\"2\"><code>npm i @tensorflow-models/knn-classifier</code></td>\n  </tr>\n  <tr>\n    <td><a href=\"./knn-classifier/demo\">source</a></td>\n  </tr>\n</table>\n\n## Development\n\nYou can run the unit tests for any of the models by running the following\ninside a directory:\n\n`yarn test`\n\nNew models should have a test NPM script (see [this](./mobilenet/package.json) `package.json` and `run_tests.ts` [helper](./mobilenet/run_tests.ts) for reference).\n\nTo run all of the tests, you can run the following command from the root of this\nrepo:\n\n`yarn presubmit`\n", "release_dates": ["2021-05-13T18:52:28Z", "2021-04-08T17:36:51Z"]}, {"name": "tfjs-node", "description": "TensorFlow powered JavaScript library for training and deploying ML models on Node.js.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# This repository has been archived in favor of tensorflow/tfjs.\n\nThis repo will remain around for some time to keep history but all future PRs should be sent to [tensorflow/tfjs](https://github.com/tensorflow/tfjs) inside the [tfjs-node](https://github.com/tensorflow/tfjs/tree/master/tfjs-node) folder.\n\nAll history and contributions have been preserved in the monorepo.\n", "release_dates": []}, {"name": "tfjs-tsne", "description": null, "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## tSNE for TensorFlow.js\n\nThis library contains a improved tSNE implementation that runs in the browser.\n\n\n## Installation & Usage\n\nYou can use tfjs-tsne via a script tag or via NPM\n\n### Script tag\n\nTo use tfjs-tsne via script tag you need to load tfjs first. The following tags\ncan be put into the head section of your html page to load the library.\n\n```html\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.14.1\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tsne\"></script>\n```\n\nThis library will create a `tsne` variable on the global scope.\nYou can then do the following\n\n```js\n// Create some data\nconst data = tf.randomUniform([2000,10]);\n\n// Get a tsne optimizer\nconst tsneOpt = tsne.tsne(data);\n\n// Compute a T-SNE embedding, returns a promise.\n// Runs for 1000 iterations by default.\ntsneOpt.compute().then(() => {\n  // tsne.coordinate returns a *tensor* with x, y coordinates of\n  // the embedded data.\n  const coordinates = tsneOpt.coordinates();\n  coordinates.print();\n}) ;\n```\n\n### Via NPM\n\n```\nyarn add @tensorflow/tfjs-tsne\n```\nor\n```\nnpm install @tensorflow/tfjs-tsne\n```\n\nThen\n\n```js\nimport * as tsne from '@tensorflow/tfjs-tsne';\n\n// Create some data\nconst data = tf.randomUniform([2000,10]);\n\n// Initialize the tsne optimizer\nconst tsneOpt = tsne.tsne(data);\n\n// Compute a T-SNE embedding, returns a promise.\n// Runs for 1000 iterations by default.\ntsneOpt.compute().then(() => {\n  // tsne.coordinate returns a *tensor* with x, y coordinates of\n  // the embedded data.\n  const coordinates = tsneOpt.coordinates();\n  coordinates.print();\n}) ;\n```\n\n## API\n\n### tsne.tsne(data: tf.Tensor2d, config?: TSNEConfiguration)\n\nCreates and returns a TSNE optimizer.\n\n- `data` must be a Rank 2 tensor. Shape is [numPoints, dataPointDimensions]\n- `config` is an optional object with the following params (all are optional):\n  - perplexity: number \u2014 defaults to 18. Max value is defined by hardware limitations.\n  - verbose: boolean \u2014 defaults to false\n  - exaggeration: number \u2014 defaults to 4\n  - exaggerationIter: number \u2014 defaults to 300\n  - exaggerationDecayIter: number \u2014 defaults to 200\n  - momentum: number \u2014 defaults to 0.8\n\n### .compute(iterations: number): Promise<void>\n\nThe most direct way to get a tsne projection. Automatically runs the knn preprocessing\nand the tsne optimization. Returns a promise to indicate when it is done.\n\n- iterations the number of iterations to run the tsne optimization for. (The number of knn steps is automatically calculated).\n\n### .iterateKnn(iterations: number): Promise<void>\n\nWhen running tsne iteratively (see section below). This runs runs the knn preprocessing\nfor the specified number of iterations.\n\n### .iterate(iterations: number): Promise<void>\n\nWhen running tsne iteratively (see section below). This runs the tsne step for the specified number of iterations.\n\n### .coordinates(normalize: boolean): tf.Tensor\n\nGets the current x, y coordinates of the projected data as a tensor. By \ndefault the coordinates are normalized to the range 0-1.\n\n### .coordsArray(normalize: boolean): Promise<number[][]>\n\nGets the current x, y coordinates of the projected data as a JavaScript array.\nBy default the coordinates are normalized to the range 0-1. This function is\nasync and returns a promise.\n\n### Computing tSNE iteratively\n\nWhile the `.compute` method provides the most direct way to get an embedding. You can also compute the embedding iteratively and have more control over the process.\n\nThe first step is computing the KNN graph using iterateKNN.\n\nThen you can compute the tSNE iteratively and examine the result as it evolves.\n\nThe code below shows what that would look like\n\n```javascript\nconst data = tf.randomUniform([2000,10]);\nconst tsne = tf_tsne.tsne(data);\n\nasync function iterativeTsne() {\n  // Get the suggested number of iterations to perform.\n  const knnIterations = tsne.knnIterations();\n  // Do the KNN computation. This needs to complete before we run tsne\n  for(let i = 0; i < knnIterations; ++i){\n    await tsne.iterateKnn();\n    // You can update knn progress in your ui here.\n  }\n\n  const tsneIterations = 1000;\n  for(let i = 0; i < tsneIterations; ++i){\n    await tsne.iterate();\n    // Draw the embedding here...\n    const coordinates = tsne.coordinates();\n    coordinates.print();\n  }\n}\n\niterativeTsne();\n```\n\n### Example\n\nWe also have an [example](https://github.com/tensorflow/tfjs-examples/tree/master/tsne-mnist-canvas) of using this library to perform TSNE on the MNIST dataset [here](https://github.com/tensorflow/tfjs-examples/tree/master/tsne-mnist-canvas).\n\n### Limitations\n\nThis library requires WebGL 2 support and thus will not work on certain devices, mobile devices especially. Currently it best works on desktop devices.\n\nFrom our current experiments we suggest limiting the data size passed to this implementation\nto data with a shape of [10000,100], i.e. up to 10000 points with 100 dimensions each. You can do more but it might slow down.\n\nAbove a certain number of data points the computation of the similarities becomes a bottleneck, a problem that we plan to address in the future.\n\n\n### Implementation\nThis work makes use of [linear tSNE optimization](https://arxiv.org/abs/1805.10817) for the optimization of the embedding and an optimized brute force computation of the kNN graph in the GPU.\n\n### Reference\nReference to cite if you use this implementation in a research paper:\n\n```\n@article{TFjs:tSNE,\n  author = {Nicola Pezzotti and Alexander Mordvintsev and Thomas Hollt and Boudewijn P. F. Lelieveldt and Elmar Eisemann and Anna Vilanova},\n  title = {Linear tSNE Optimization for the Web},\n  year = {2018},\n  journal={arXiv preprint arXiv:1805.10817},\n}\n```\n", "release_dates": []}, {"name": "tfjs-vis", "description": "A set of utilities for in browser visualization with TensorFlow.js", "language": "TypeScript", "license": null, "readme": "# This repository has been archived in favor of tensorflow/tfjs.\n\nThis repo will remain around for some time to keep history but all future PRs should be sent to [tensorflow/tfjs](https://github.com/tensorflow/tfjs) inside the [tfjs-vis](https://github.com/tensorflow/tfjs/tree/master/tfjs-vis) folder.\n\nAll history and contributions have been preserved in the monorepo.\n", "release_dates": []}, {"name": "tfjs-website", "description": "WebGL-accelerated ML // linear algebra // automatic differentiation for JavaScript.", "language": "CSS", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# js.tensorflow.org\n\nThis repo is for the website for TensorFlow.js. The API docs are built using Hexo (a static site generator) that puts static assets in to the `public` folder.\nThe rest of the website is written as markdown and built internally within Google.\n\n## Development Setup\n\nYou need node.js, yarn, and git to use this repo effectively. Note that it clones the tfjs repo in order to build API docs.\n\nAfter checking out this repo the first time run\n\n```\nyarn prep\n```\n\nThis will download tfjs source code into a subfolder.\n\nTo start a dev server for the site. You should be able to make changes to the site and see them reflected in the dev server\n\nWhen you pull new commits from git you should run `yarn fetch-tfjs`.\n\n## Building and previewing the site\n\n### Fast build\n\n```sh\nyarn build-api && yarn build\n```\n\n```\nyarn serve\n```\n\n### Full build\n\nDoing a full build for final deployment can be done using (see below for more details)\n\n```\nyarn build-prod\n```\n\n## Making Changes\n\nThere are two broad classes of changes you might want to make, site content/design and API documentation changes.\n\n### Site Content/Design\n\nAll the site content except for the API docs are stored in the `docs` folder as markdown and yaml files. These files are served from tensorflow.org/js\n\nYou can submit a change by editing those files and can preview your changes in any markdown preview tool or using the GitHub pull review interface. These files are built using internal google tools so can't be previewed along with the site design outside of Google.\n\n### API Documentation\n\nUpdating the API docs is a bit more involved as they are built from the sources of tfjs-core, tfjs-layers, tfjs-node, tfjs-vis etc. The template for api docs is `api.hbs` and each version of the docs has a corresponding folder in `_data`. Files in `_data` are automatically generated and shouldn't be edited.\n\nTo edit the docs and see changes reflected in the site you can edit the repository that is located in `libs`. Note that this subfolder behaves like *regular a git repository* and has an origin pointing to the canonical repository for `tfjs`. Making an API doc change involves making a commit to the tfjs repo and to this one. You may need to add a new git remote in the submodule if you want to make a PR from a fork of tfjs-website repo. Make sure to do `git checkout master` or make a new branch for your changes.\n\nThere are two ways to regenerate the docs json.\n\nDuring local development (e.g. if you have changes in libs), run:\n\n```\nyarn [build-api|build-vis-api|build-node-api] // depending on which api docs you want to regenerate.\n```\n\nand\n\n```\nyarn build && yarn serve\n```\n\nThis will build a version of the api known as `local`. It will be accessible at `http://localhost:8080/api/local/`. This content is not checked in, nor is it linked to\nanywhere on the site, but it is suitable for testing changes to docs.\n\n\nOnce the your changes have been accepted into the repo in question (either tfjs-core or tfjs-layers), you can run `yarn prep` again in this repo (tfjs-website) to sync everything up (pull the most recent commits from master for each). You can then commit the **new submodule info** in the tfjs-website repo.\n\nOnce you are done you can do a full production build of the site using:\n\n```\nyarn build-prod\n```\n\nNote that this command will do a number of things that may **modify your working tree**. Its purpose is to build a new production build suitable for deployment to the site, as such it only builds docs that are in a released version of tfjs. The version it builds is driven off of the `@tensorflow/tfjs` dependency in package.json. **Build prod cannot build your local doc changes into the site**. To do this it will do a checkout of `libs/tfjs` that corresponds to the dependency listed for `@tensorflow/tfjs`. This will modify your working tree (in libs).\n\nIn both these cases starting the dev server and refreshing the page should allow you to see changes.\n\nIn addition to `http://localhost:8080/api/local/`, the build also provides `http://localhost:8080/api/latest/` which points to the last **production** version of the docs that have been built. `latest` will never point to `local`\n\n### Other API pages\n\nThese are the API pages generated by the project\n\n- http://localhost:8080/api/ : for tfjs (union package)\n- http://localhost:8080/api_vis/ : for tfjs-vis\n- http://localhost:8080/api_node/ : for tfjs-node\n- http://localhost:8080/api_react_native/ : for tfjs-react-native\n\n### Deployment\n\nDeployment instructions are available internally.\n", "release_dates": []}, {"name": "tfjs-wechat", "description": "WeChat Mini-program plugin for TensorFlow.js", "language": "TypeScript", "license": null, "readme": "# TensorFlow.js \u5fae\u4fe1\u5c0f\u7a0b\u5e8f\u63d2\u4ef6\n[TensorFlow.js](https://github.com/tensorflow/tfjs)\u662f\u8c37\u6b4c\u5f00\u53d1\u7684\u673a\u5668\u5b66\u4e60\u5f00\u6e90\u9879\u76ee\uff0c\u81f4\u529b\u4e8e\u4e3ajavascript\u63d0\u4f9b\u5177\u6709\u786c\u4ef6\u52a0\u901f\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u548c\u90e8\u7f72\u3002\nTensorFlow.js \u5fae\u4fe1\u5c0f\u7a0b\u5e8f\u63d2\u4ef6\u5c01\u88c5\u4e86TensorFlow.js\u5e93\uff0c\u7528\u4e8e\u63d0\u4f9b\u7ed9\u7b2c\u4e09\u65b9\u5c0f\u7a0b\u5e8f\u8c03\u7528\u3002\n\u4f8b\u5b50\u53ef\u4ee5\u770bTFJS Mobilenet [\u7269\u4f53\u8bc6\u522b\u5c0f\u7a0b\u5e8f](https://github.com/tensorflow/tfjs-wechat/tree/master/demo/mobilenet)\n## \u6dfb\u52a0\u63d2\u4ef6\n\u5728\u4f7f\u7528\u63d2\u4ef6\u524d\uff0c\u9996\u5148\u8981\u5728\u5c0f\u7a0b\u5e8f\u7ba1\u7406\u540e\u53f0\u7684\u201c\u8bbe\u7f6e-\u7b2c\u4e09\u65b9\u670d\u52a1-\u63d2\u4ef6\u7ba1\u7406\u201d\u4e2d\u6dfb\u52a0\u63d2\u4ef6\u3002\u5f00\u53d1\u8005\u53ef\u767b\u5f55\u5c0f\u7a0b\u5e8f\u7ba1\u7406\u540e\u53f0\uff0c\u901a\u8fc7 appid [wx6afed118d9e81df9] \u67e5\u627e\u63d2\u4ef6\u5e76\u6dfb\u52a0\u3002\u672c\u63d2\u4ef6\u65e0\u9700\u7533\u8bf7\uff0c\u6dfb\u52a0\u540e\u53ef\u76f4\u63a5\u4f7f\u7528\u3002\n\n### \u5f15\u5165\u63d2\u4ef6\u4ee3\u7801\u5305\n\u4f7f\u7528\u63d2\u4ef6\u524d\uff0c\u4f7f\u7528\u8005\u8981\u5728 app.json \u4e2d\u58f0\u660e\u9700\u8981\u4f7f\u7528\u7684\u63d2\u4ef6\uff0c\u4f8b\u5982\uff1a\n\n\u4ee3\u7801\u793a\u4f8b\uff1a\n```\n{\n  ...\n  \"plugins\": {\n    \"tfjsPlugin\": {\n      \"version\": \"0.0.6\",\n      \"provider\": \"wx6afed118d9e81df9\"\n    }\n  }\n  ...\n}\n```\n### \u5f15\u5165TensorFlow.js npm\nTensorFlow.js \u6700\u65b0\u7248\u672c\u662f\u4ee5npm\u5305\u7684\u5f62\u5f0f\u53d1\u5e03\uff0c\u5c0f\u7a0b\u5e8f\u9700\u8981\u4f7f\u7528npm\u6216\u8005yarn\u6765\u8f7d\u5165TensorFlow.js npm\u5305\u3002\u4e5f\u53ef\u4ee5\u624b\u52a8\u4fee\u6539 package.json \u6587\u4ef6\u6765\u52a0\u5165\u3002\n\nTensorFlow.js v2.0 \u6709\u4e00\u4e2a\u8054\u5408\u5305 - @tensorflow/tfjs\uff0c\u5305\u542b\u4e86\u516d\u4e2a\u5206npm\u5305\uff1a\n- tfjs-core: \u57fa\u7840\u5305\n- tfjs-converter: GraphModel \u5bfc\u5165\u548c\u6267\u884c\u5305\n- tfjs-layers: LayersModel \u521b\u5efa\uff0c\u5bfc\u5165\u548c\u6267\u884c\u5305\n- tfjs-backend-webgl: webgl \u540e\u7aef\n- tfjs-backend-cpu: cpu \u540e\u7aef\n- tfjs-data\uff1a\u6570\u636e\u6d41\n\n\n\u5bf9\u4e8e\u5c0f\u7a0b\u5e8f\u800c\u8a00\uff0c\u7531\u4e8e\u67092M\u7684app\u5927\u5c0f\u9650\u5236\uff0c\u4e0d\u5efa\u8bae\u76f4\u63a5\u4f7f\u7528\u8054\u5408\u5305\uff0c\u800c\u662f\u6309\u7167\u9700\u6c42\u52a0\u8f7d\u5206\u5305\u3002\n- \u5982\u679c\u5c0f\u7a0b\u5e8f\u53ea\u9700\u8981\u5bfc\u5165\u548c\u8fd0\u884cGraphModel\u6a21\u578b\u7684\u7684\u8bdd\uff0c\u5efa\u8bae\u81f3\u5c11\u52a0\u5165tfjs-core\uff0c tfjs-converter\uff0c tfjs-backend-webgl \u548ctfjs-backend-cpu\u5305\u3002\u8fd9\u6837\u53ef\u4ee5\u5c3d\u91cf\u51cf\u5c11\u5bfc\u5165\u5305\u7684\u5927\u5c0f\u3002\n- \u5982\u679c\u9700\u8981\u521b\u5efa,\u5bfc\u5165\u6216\u8bad\u7ec3LayersModel\u6a21\u578b\uff0c\u9700\u8981\u518d\u52a0\u5165 tfjs-layers\u5305\u3002\n\n\u4e0b\u9762\u7684\u4f8b\u5b50\u662f\u53ea\u7528\u5230tfjs-core\uff0c tfjs-converter\uff0ctfjs-backend-webgl \u548ctfjs-backend-cpu\u5305\u3002\u4ee3\u7801\u793a\u4f8b\uff1a\n```\n{\n  \"name\": \"yourProject\",\n  \"version\": \"0.0.1\",\n  \"main\": \"dist/index.js\",\n  \"license\": \"Apache-2.0\",\n  \"dependencies\": {\n    \"@tensorflow/tfjs-core\": \"3.5.0\"\uff0c\n    \"@tensorflow/tfjs-converter\": \"3.5.0\"\uff0c\n    \"@tensorflow/tfjs-backend-webgl\": \"3.5.0\"\n  }\n}\n```\n\n\u53c2\u8003\u5c0f\u7a0b\u5e8fnpm\u5de5\u5177[\u6587\u6863](https://developers.weixin.qq.com/miniprogram/dev/devtools/npm.html)\u5982\u4f55\u7f16\u8bd1npm\u5305\u5230\u5c0f\u7a0b\u5e8f\u4e2d\u3002\n\n__\u6ce8\u610f__\n\u8bf7\u4ece\u5fae\u4fe1\u5c0f\u7a0b\u5e8f[\u5f00\u53d1\u7248Nightly Build\u66f4\u65b0\u65e5\u5fd7](https://developers.weixin.qq.com/miniprogram/dev/devtools/nightly.html)\u4e0b\u8f7d\u6700\u65b0\u7684\u5fae\u4fe1\u5f00\u53d1\u8005\u5de5\u5177\uff0c\u4fdd\u8bc1\u7248\u672c\u53f7>=v1.02.1907022.\n\n### Polyfill fetch \u51fd\u6570\n\u5982\u679c\u9700\u8981\u4f7f\u7528tf.loadGraphModel\u6216tf.loadLayersModel API\u6765\u8f7d\u5165\u6a21\u578b\uff0c\u5c0f\u7a0b\u5e8f\u9700\u8981\u6309\u4ee5\u4e0b\u6d41\u7a0b\u586b\u5145fetch\u51fd\u6570\uff1a\n\n1. \u5982\u679c\u4f60\u4f7f\u7528npm, \u4f60\u53ef\u4ee5\u8f7d\u5165fetch-wechat npm \u5305\n\n```\n{\n  \"name\": \"yourProject\",\n  \"version\": \"0.0.1\",\n  \"main\": \"dist/index.js\",\n  \"license\": \"Apache-2.0\",\n  \"dependencies\": {\n    \"@tensorflow/tfjs-core\": \"3.5.0\"\uff0c\n    \"@tensorflow/tfjs-converter\": \"3.5.0\"\uff0c\n    \"@tensorflow/tfjs-backend-webgl\": \"3.5.0\"\n    \"fetch-wechat\": \"0.0.3\"\n  }\n}\n```\n\n2. \u4e5f\u53ef\u4ee5\u76f4\u63a5\u62f7\u8d1d\u4ee5\u4e0b\u6587\u4ef6\u5230\u4f60\u7684javascript\u6e90\u76ee\u5f55\uff1a\nhttps://cdn.jsdelivr.net/npm/fetch-wechat@0.0.3/dist/fetch_wechat.min.js\n\n\n### \u5728app.js\u7684onLaunch\u91cc\u8c03\u7528\u63d2\u4ef6configPlugin\u51fd\u6570\n\n```\nvar fetchWechat = require('fetch-wechat');\nvar tf = require('@tensorflow/tfjs-core');\nvar webgl = require('@tensorflow/tfjs-backend-webgl');\nvar plugin = requirePlugin('tfjsPlugin');\n//app.js\nApp({\n  onLaunch: function () {\n    plugin.configPlugin({\n      // polyfill fetch function\n      fetchFunc: fetchWechat.fetchFunc(),\n      // inject tfjs runtime\n      tf,\n      // inject webgl backend\n      webgl,\n      // provide webgl canvas\n      canvas: wx.createOffscreenCanvas()\n    });\n  }\n});\n```\n\n### \u652f\u6301\u6a21\u578blocalStorage\u7f13\u5b58\n\u91c7\u7528localStorage\u7f13\u5b58\u53ef\u4ee5\u51cf\u5c11\u6a21\u578b\u4e0b\u8f7d\u8017\u8d39\u5e26\u5bbd\u548c\u65f6\u95f4\u3002\u7531\u4e8e\u5fae\u4fe1\u5c0f\u7a0b\u5e8f\u5bf9\u4e8elocalStorage\u670910MB\u7684\u9650\u5236\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u9002\u7528\u4e8e\u5c0f\u4e8e10MB\u7684\u6a21\u578b\u3002\n\u6b65\u9aa4\u5982\u4e0b\uff1a\n1. \u5728app.js\u4e2d\u63d0\u4f9blocalStorageHandler\u51fd\u6570.\n\n```\nvar fetchWechat = require('fetch-wechat');\nvar tf = require('@tensorflow/tfjs-core');\nvar plugin = requirePlugin('tfjsPlugin');\n//app.js\nApp({\n  // expose localStorage handler\n  globalData: {localStorageIO: plugin.localStorageIO},\n  ...\n});\n```\n\n2. \u5728\u6a21\u578b\u52a0\u8f7d\u65f6\u52a0\u5165localStorageHandler\u903b\u8f91\u3002\n\n```\nconst LOCAL_STORAGE_KEY = 'mobilenet_model';\nexport class MobileNet {\n  private model: tfc.GraphModel;\n  constructor() { }\n\n  async load() {\n\n    const localStorageHandler = getApp().globalData.localStorageIO(LOCAL_STORAGE_KEY);\n    try {\n      this.model = await tfc.loadGraphModel(localStorageHandler);\n    } catch (e) {\n      this.model =\n        await tfc.loadGraphModel(MODEL_URL);\n      this.model.save(localStorageHandler);\n    }\n  }\n\n```\n\n### \u652f\u6301\u6a21\u578b\u4fdd\u5b58\u4e3a\u7528\u6237\u6587\u4ef6\n\u5fae\u4fe1\u4e5f\u652f\u6301\u4fdd\u5b58\u6a21\u578b\u4e3a\u6587\u4ef6\u3002\u540clocalStorage, \u5fae\u4fe1\u5c0f\u7a0b\u5e8f\u5bf9\u4e8e\u672c\u5730\u6587\u4ef6\u4e5f\u670910MB\u7684\u9650\u5236\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u9002\u7528\u4e8e\u5c0f\u4e8e10MB\u7684\u6a21\u578b\u3002\u7531\u4e8e\u6700\u7ec8\u6a21\u578b\u662f\u6309 binary \u4fdd\u5b58\uff0c\u8f83 localstorage \u4fdd\u5b58\u4e3a base64 string \u66f4\u4e3a\u8282\u7701\u7a7a\u95f4\u3002\n\n\u6b65\u9aa4\u5982\u4e0b\uff1a\n1. \u5728app.js\u4e2d\u63d0\u4f9b fileStorageHandler \u51fd\u6570.\n\n```js\nvar fetchWechat = require('fetch-wechat');\nvar tf = require('@tensorflow/tfjs-core');\nvar plugin = requirePlugin('tfjsPlugin');\n//app.js\nApp({\n  // expose fileStorage handler\n  globalData: {fileStorageIO: plugin.fileStorageIO},\n  ...\n});\n```\n\n2. \u5728\u6a21\u578b\u52a0\u8f7d\u65f6\u52a0\u5165 fileStorageHandler \u903b\u8f91\u3002\n\n```js\nconst FILE_STORAGE_PATH = 'mobilenet_model';\nexport class MobileNet {\n  private model: tfc.GraphModel;\n  constructor() { }\n\n  async load() {\n\n    const fileStorageHandler = getApp().globalData.fileStorageIO(\n        FILE_STORAGE_PATH, wx.getFileSystemManager());\n    try {\n      this.model = await tfc.loadGraphModel(fileStorageHandler);\n    } catch (e) {\n      this.model =\n        await tfc.loadGraphModel(MODEL_URL);\n      this.model.save(fileStorageHandler);\n    }\n  }\n}\n```\n\n### \u4f7f\u7528 WebAssembly backend\n\u5fae\u4fe1\u5c0f\u7a0b\u5e8f\u5728 Android \u624b\u673a\u4e0a\u63d0\u4f9b WebAssembly\u7684\u652f\u6301\u3002TensorFlow.js\u7684WASM backend\u975e\u5e38\u9002\u5408\u5728\u4e2d\u4f4e\u7aefAndroid\u624b\u673a\u4e0a\u4f7f\u7528\u3002\n\u4e2d\u4f4e\u7aef\u624b\u673a\u7684GPU\u5f80\u5f80\u76f8\u5bf9CPU\u8981\u5f31\u4e00\u4e9b\uff0c\u800cWASM backend\u662f\u8dd1\u5728CPU\u4e0a\u7684\uff0c\u8fd9\u5c31\u4e3a\u4e2d\u4f4e\u7aef\u624b\u673a\u63d0\u4f9b\u4e86\u53e6\u4e00\u4e2a\u52a0\u901f\u5e73\u53f0\u3002\u800c\u4e14WASM\u7684\u80fd\u8017\u4e00\u822c\u4f1a\u66f4\u4f4e\u3002\n\u4f7f\u7528WASM backend\u9700\u8981\u4fee\u6539package.json\u6587\u4ef6\uff1a\n\n```\n{\n  \"name\": \"yourProject\",\n  \"version\": \"0.0.1\",\n  \"main\": \"dist/index.js\",\n  \"license\": \"Apache-2.0\",\n  \"dependencies\": {\n    \"@tensorflow/tfjs-core\": \"2.0.0\"\uff0c\n    \"@tensorflow/tfjs-converter\": \"2.0.0\"\uff0c\n    \"@tensorflow/tfjs-backend-wasm\": \"2.0.0\",\n    ...\n  }\n}\n```\n\n\u7136\u540e\u5728app.js\u4e2d\u8bbe\u7f6e wasm backend, \u4f60\u53ef\u4ee5\u81ea\u884chost wasm file\u4ee5\u63d0\u9ad8\u4e0b\u8f7d\u901f\u5ea6, \u4e0b\u9762\u4f8b\u5b50\u4e2d\u7684 `wasmUrl`\u53ef\u4ee5\u66ff\u4ee3\u6210\u4f60host\u7684URL\u3002\n```\n    const info = wx.getSystemInfoSync();\n    const wasmUrl = 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm@2.0.0/wasm-out/tfjs-backend-wasm.wasm';\n    const usePlatformFetch = true;\n    console.log(info.platform);\n    if (info.platform == 'android') {\n      setWasmPath(wasmUrl, usePlatformFetch);\n      tf.setBackend('wasm').then(() => console.log('set wasm backend'));\n    }\n```\n\n__\u6ce8\u610f__\nWASM backend is broken due to bundle imcompatible with WeChat npm loader, will update here when it is fixed.\n\n\n__\u6ce8\u610f__\n\u7531\u4e8e\u6700\u65b0\u7248\u672c\u7684WeChat\u7684OffscreenCanvas\u4f1a\u968f\u9875\u9762\u8df3\u8f6c\u800c\u5931\u6548\uff0c\u5728app.js\u7684 onLaunch \u51fd\u6570\u4e2d\u8bbe\u7f6e tfjs \u4f1a\u5bfc\u81f4\u5c0f\u7a0b\u5e8f\u9000\u51fa\u6216\u9875\u9762\u8df3\u8f6c\u4e4b\u540e\u64cd\u4f5c\u51fa\u9519\u3002\u5efa\u8bae\u5728\u4f7f\u7528tfjs\u7684page\u7684onLoad\u4e2d\u8c03\u7528 configPlugin \u51fd\u6570\u3002\nWeChat\u768412\u6708\u7248\u672c\u4f1a\u4fee\u590d\u8fd9\u4e2a\u95ee\u9898\u3002\n\n```\nvar fetchWechat = require('fetch-wechat');\nvar tf = require('@tensorflow/tfjs-core');\nvar plugin = requirePlugin('tfjsPlugin');\n//index.js\nPage({\n  onLoad: function () {\n    plugin.configPlugin({\n      // polyfill fetch function\n      fetchFunc: fetchWechat.fetchFunc(),\n      // inject tfjs runtime\n      tf,\n      // provide webgl canvas\n      canvas: wx.createOffscreenCanvas(),\n      backendName: 'wechat-webgl-' + Date.now()\n    });\n    ...\n  }\n});\n```\n\n\u7ec4\u4ef6\u8bbe\u7f6e\u5b8c\u6bd5\u5c31\u53ef\u4ee5\u5f00\u59cb\u4f7f\u7528 TensorFlow.js\u5e93\u7684[API](https://js.tensorflow.org/api/latest/)\u4e86\u3002\n\n### \u4f7f\u7528 [tfjs-models](https://github.com/tensorflow/tfjs-models) \u6a21\u578b\u5e93\u6ce8\u610f\u4e8b\u9879\n\u6a21\u578b\u5e93\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u65b9\u4fbf\u5927\u5bb6\u5feb\u901f\u7684\u7ed9\u5c0f\u7a0b\u5e8f\u6ce8\u5165ML\u529f\u80fd\u3002\u6a21\u578b\u5206\u7c7b\u5305\u62ec\n- \u56fe\u50cf\u8bc6\u522b\n- \u8bed\u97f3\u8bc6\u522b\n- \u4eba\u4f53\u59ff\u6001\u8bc6\u522b\n- \u7269\u4f53\u8bc6\u522b\n- \u6587\u5b57\u5206\u7c7b\n\n\u7531\u4e8e\u8fd9\u4e9bAPI\u9ed8\u8ba4\u6a21\u578b\u6587\u4ef6\u90fd\u5b58\u50a8\u5728\u8c37\u6b4c\u4e91\u4e0a\uff0c\u76f4\u63a5\u4f7f\u7528\u4f1a\u5bfc\u81f4\u4e2d\u56fd\u7528\u6237\u65e0\u6cd5\u76f4\u63a5\u8bfb\u53d6\u3002\u5728\u5c0f\u7a0b\u5e8f\u5185\u4f7f\u7528\u6a21\u578bAPI\u65f6\u8981\u63d0\u4f9b modelUrl \u7684\u53c2\u6570\uff0c\u53ef\u4ee5\u6307\u5411\u6211\u4eec\u5728\u8c37\u6b4c\u4e2d\u56fd\u7684\u955c\u50cf\u670d\u52a1\u5668\u3002\n\u8c37\u6b4c\u4e91\u7684base url\u662f https://storage.googleapis.com\uff0c \u4e2d\u56fd\u955c\u50cf\u7684base url\u662fhttps://www.gstaticcnapps.cn\n\u6a21\u578b\u7684url path\u662f\u4e00\u81f4\u7684\uff0c\u6bd4\u5982\n- posenet\u6a21\u578b\u7684\u8c37\u6b4c\u4e91\u5730\u5740\u662f\uff1a\nhttps://storage.googleapis.com/tfjs-models/savedmodel/posenet/mobilenet/float/050/model-stride16.json\n- \u4e2d\u56fd\u955c\u50cf\u7684\u5730\u5740\u4e3a https://www.gstaticcnapps.cn/tfjs-models/savedmodel/posenet/mobilenet/float/050/model-stride16.json\n\n\u4ed6\u4eec\u7684 URL Path \u90fd\u662f /tfjs-models/savedmodel/posenet/mobilenet/float/050/model-stride16.json\n\n\u4e0b\u9762\u662f\u52a0\u8f7dposenet\u6a21\u578b\u7684\u4f8b\u5b50\uff1a\n\n```\nimport * as posenet from '@tensorflow-models/posenet';\n\nconst POSENET_URL =\n    'https://www.gstaticcnapps.cn/tfjs-models/savedmodel/posenet/mobilenet/float/050/model-stride16.json';\n\nconst model = await posenet.load({\n  architecture: 'MobileNetV1',\n  outputStride: 16,\n  inputResolution: 193,\n  multiplier: 0.5,\n  modelUrl: POSENET_URL\n});\n```\n\n## [tfjs-examples](https://github.com/tensorflow/tfjs-examples) tfjs\u4f8b\u5b50\u5e93\ntfjs API \u4f7f\u7528\u5b9e\u4f8b\u3002\n\n## \u7248\u672c\u9700\u6c42\n- \u5fae\u4fe1\u57fa\u7840\u5e93\u7248\u672c >= 2.7.3\n- \u5fae\u4fe1\u5f00\u53d1\u8005\u5de5\u5177 >= v1.02.1907022\n- tfjs-core >= 1.5.2\n- tfjs-converter >= 1.5.2 \u5982\u679c\u4f7f\u7528localStorage\u6a21\u578b\u7f13\u5b58\n\n__\u6ce8\u610f__\n\u5728\u5fae\u4fe1\u5f00\u53d1\u8005\u5de5\u5177 v1.02.19070300 \u4e2d\uff0c\u4f60\u9700\u8981\u5728\u901a\u7528\u8bbe\u7f6e\u4e2d\u6253\u5f00\u786c\u4ef6\u52a0\u901f\uff0c\u4ece\u800c\u5728TensorFlow.js\u4e2d\u542f\u7528WebGL\u52a0\u901f\u3002\n![setting](https://raw.githubusercontent.com/tensorflow/tfjs-wechat/master/doc/setting.png)\n## \u66f4\u65b0\u8bf4\u660e\n- 0.0.2 plugin\u4e0d\u518d\u6620\u5c04TensorFlow.js API\u5e93\uff0c\u7531\u5c0f\u7a0b\u5e8f\u7aef\u63d0\u4f9b\u3002\n- 0.0.3 \u4f7f\u7528offscreen canvas\uff0c\u5c0f\u7a0b\u5e8f\u65e0\u9700\u52a0\u5165plugin component\u3002\n- 0.0.5 \u4fee\u6539\u4f8b\u5b50\u7a0b\u5e8f\u4f7f\u7528tfjs\u5206\u5305\u6765\u964d\u4f4e\u5c0f\u7a0b\u5e8f\u5927\u5c0f\u3002\n- 0.0.6 \u652f\u6301 tfjs-core\u7248\u672c1.2.7\u3002\n- 0.0.7 \u5141\u8bb8\u7528\u6237\u8bbe\u7f6ewebgl backend name, \u8fd9\u53ef\u4ee5\u89e3\u51b3\u5c0f\u7a0b\u5e8foffscreen canvas\u4f1a\u5931\u6548\u7684\u95ee\u9898\u3002\n- 0.0.8 \u52a0\u5165localStorage\u652f\u6301\uff0c\u5141\u8bb8\u5c0f\u4e8e10M\u6a21\u578b\u5728localStorage\u5185\u7f13\u5b58\u3002\n- 0.0.9 \u52a0\u5165fileSystem\u652f\u6301\uff0c\u5141\u8bb8\u5c0f\u4e8e10M\u6a21\u578b\u5728local file system\u5185\u7f13\u5b58\u3002fixed missing kernel bug.\n- 0.1.0 \u652f\u6301 tfjs\u7248\u672c2.0.x\u3002\n- 0.2.0 \u652f\u6301 tfjs\u7248\u672c3.x\u3002\n", "release_dates": []}, {"name": "tflite-micro", "description": "Infrastructure to enable deployment of ML models to low-power resource-constrained embedded targets (including microcontrollers and digital signal processors).", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<!--ts-->\n   * [TensorFlow Lite for Microcontrollers](#tensorflow-lite-for-microcontrollers)\n   * [Build Status](#build-status)\n      * [Official Builds](#official-builds)\n      * [Community Supported TFLM Examples](#community-supported-tflm-examples)\n      * [Community Supported Kernels and Unit Tests](#community-supported-kernels-and-unit-tests)\n   * [Contributing](#contributing)\n   * [Getting Help](#getting-help)\n   * [Additional Documentation](#additional-documentation)\n   * [RFCs](#rfcs)\n\n<!-- Added by: advaitjain, at: Mon 04 Oct 2021 11:23:57 AM PDT -->\n\n<!--te-->\n\n# TensorFlow Lite for Microcontrollers\n\nTensorFlow Lite for Microcontrollers is a port of TensorFlow Lite designed to\nrun machine learning models on DSPs, microcontrollers and other devices with\nlimited memory.\n\nAdditional Links:\n * [Tensorflow github repository](https://github.com/tensorflow/tensorflow/)\n * [TFLM at tensorflow.org](https://www.tensorflow.org/lite/microcontrollers)\n\n# Build Status\n\n * [GitHub Status](https://www.githubstatus.com/)\n\n## Official Builds\n\nBuild Type       |    Status     |\n-----------      | --------------|\nCI (Linux)       | [![CI](https://github.com/tensorflow/tflite-micro/actions/workflows/run_ci.yml/badge.svg)](https://github.com/tensorflow/tflite-micro/actions/workflows/run_ci.yml) |\nCode Sync        | [![Sync from Upstream TF](https://github.com/tensorflow/tflite-micro/actions/workflows/sync.yml/badge.svg)](https://github.com/tensorflow/tflite-micro/actions/workflows/sync.yml) |\n\n\n## Community Supported TFLM Examples\nThis table captures platforms that TFLM has been ported to. Please see\n[New Platform Support](tensorflow/lite/micro/docs/new_platform_support.md) for\nadditional documentation.\n\nPlatform      |    Status     |\n-----------     | --------------|\nArduino         | [![Arduino](https://github.com/tensorflow/tflite-micro-arduino-examples/actions/workflows/ci.yml/badge.svg)](https://github.com/tensorflow/tflite-micro-arduino-examples/actions/workflows/ci.yml) [![Antmicro](https://github.com/antmicro/tensorflow-arduino-examples/actions/workflows/test_examples.yml/badge.svg)](https://github.com/antmicro/tensorflow-arduino-examples/actions/workflows/test_examples.yml) |\n[Coral Dev Board Micro](https://coral.ai/products/dev-board-micro) | [TFLM + EdgeTPU Examples for Coral Dev Board Micro](https://github.com/google-coral/coralmicro) |\nEspressif Systems Dev Boards  | [![ESP Dev Boards](https://github.com/espressif/tflite-micro-esp-examples/actions/workflows/ci.yml/badge.svg)](https://github.com/espressif/tflite-micro-esp-examples/actions/workflows/ci.yml) |\nRenesas Boards | [TFLM Examples for Renesas Boards](https://github.com/renesas/tflite-micro-renesas) |\nSilicon Labs Dev Kits        | [TFLM Examples for Silicon Labs Dev Kits](https://github.com/SiliconLabs/tflite-micro-efr32-examples)\nSparkfun Edge   | [![Sparkfun Edge](https://github.com/advaitjain/tflite-micro-sparkfun-edge-examples/actions/workflows/ci.yml/badge.svg?event=schedule)](https://github.com/advaitjain/tflite-micro-sparkfun-edge-examples/actions/workflows/ci.yml)\nTexas Instruments Dev Boards | [![Texas Instruments Dev Boards](https://github.com/TexasInstruments/tensorflow-lite-micro-examples/actions/workflows/ci.yml/badge.svg?event=status)](https://github.com/TexasInstruments/tensorflow-lite-micro-examples/actions/workflows/ci.yml)\n\n\n## Community Supported Kernels and Unit Tests\nThis is a list of targets that have optimized kernel implementations and/or run\nthe TFLM unit tests using software emulation or instruction set simulators.\n\nBuild Type      |    Status     |\n-----------     | --------------|\nCortex-M        | [![Cortex-M](https://github.com/tensorflow/tflite-micro/actions/workflows/cortex_m.yml/badge.svg)](https://github.com/tensorflow/tflite-micro/actions/workflows/cortex_m.yml) |\nHexagon         | [![Hexagon](https://github.com/tensorflow/tflite-micro/actions/workflows/run_hexagon.yml/badge.svg)](https://github.com/tensorflow/tflite-micro/actions/workflows/run_hexagon.yml) |\nRISC-V          | [![RISC-V](https://github.com/tensorflow/tflite-micro/actions/workflows/riscv.yml/badge.svg)](https://github.com/tensorflow/tflite-micro/actions/workflows/riscv.yml) |\nXtensa          | [![Xtensa](https://github.com/tensorflow/tflite-micro/actions/workflows/run_xtensa.yml/badge.svg)](https://github.com/tensorflow/tflite-micro/actions/workflows/run_xtensa.yml) |\nGenerate Integration Test          | [![Generate Integration Test](https://github.com/tensorflow/tflite-micro/actions/workflows/generate_integration_tests.yml/badge.svg)](https://github.com/tensorflow/tflite-micro/actions/workflows/generate_integration_tests.yml) |\n\n\n# Contributing\nSee our [contribution documentation](CONTRIBUTING.md).\n\n# Getting Help\n\nA [Github issue](https://github.com/tensorflow/tflite-micro/issues/new/choose)\nshould be the primary method of getting in touch with the TensorFlow Lite Micro\n(TFLM) team.\n\nThe following resources may also be useful:\n\n1.  SIG Micro [email group](https://groups.google.com/a/tensorflow.org/g/micro)\n    and\n    [monthly meetings](http://doc/1YHq9rmhrOUdcZnrEnVCWvd87s2wQbq4z17HbeRl-DBc).\n\n1.  SIG Micro [gitter chat room](https://gitter.im/tensorflow/sig-micro).\n\n1. For questions that are not specific to TFLM, please consult the broader TensorFlow project, e.g.:\n   * Create a topic on the [TensorFlow Discourse forum](https://discuss.tensorflow.org)\n   * Send an email to the [TensorFlow Lite mailing list](https://groups.google.com/a/tensorflow.org/g/tflite)\n   * Create a [TensorFlow issue](https://github.com/tensorflow/tensorflow/issues/new/choose)\n   * Create a [Model Optimization Toolkit](https://github.com/tensorflow/model-optimization) issue\n\n# Additional Documentation\n\n * [Continuous Integration](docs/continuous_integration.md)\n * [Benchmarks](tensorflow/lite/micro/benchmarks/README.md)\n * [Profiling](tensorflow/lite/micro/docs/profiling.md)\n * [Memory Management](tensorflow/lite/micro/docs/memory_management.md)\n * [Logging](tensorflow/lite/micro/docs/logging.md)\n * [Porting Reference Kernels from TfLite to TFLM](tensorflow/lite/micro/docs/porting_reference_ops.md)\n * [Optimized Kernel Implementations](tensorflow/lite/micro/docs/optimized_kernel_implementations.md)\n * [New Platform Support](tensorflow/lite/micro/docs/new_platform_support.md)\n * Platform/IP support\n   * [Arm IP support](tensorflow/lite/micro/docs/arm.md)\n * [Software Emulation with Renode](tensorflow/lite/micro/docs/renode.md)\n * [Software Emulation with QEMU](tensorflow/lite/micro/docs/qemu.md)\n * [Python Dev Guide](docs/python.md)\n * [Automatically Generated Files](docs/automatically_generated_files.md)\n * [Python Interpreter Guide](python/tflite_micro/README.md)\n\n# RFCs\n\n1. [Pre-allocated tensors](tensorflow/lite/micro/docs/rfc/001_preallocated_tensors.md)\n1. [TensorFlow Lite for Microcontrollers Port of 16x8 Quantized Operators](tensorflow/lite/micro/docs/rfc/002_16x8_quantization_port.md)\n", "release_dates": []}, {"name": "tflite-micro-arduino-examples", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Lite Micro Library for Arduino\n\nThis repository has the code (including examples) needed to use Tensorflow Lite Micro on an Arduino.\n\n## Table of contents\n<!--ts-->\n* [Table of contents](#table-of-contents)\n* [Build Status](#build-status)\n* [How to Install](#how-to-install)\n  * [GitHub](#github)\n  * [Checking your Installation](#checking-your-installation)\n* [Compatibility](#compatibility)\n* [License](#license)\n* [Contributing](#contributing)\n<!--te-->\n\n## Build Status\n\nBuild Type          |     Status    |\n---------------     | ------------- |\nArduino CLI on Linux  | [![Arduino](https://github.com/tensorflow/tflite-micro-arduino-examples/actions/workflows/ci.yml/badge.svg?event=schedule)](https://github.com/tensorflow/tflite-micro-arduino-examples/actions/workflows/ci.yml)\nSync from tflite-micro  | [![Sync from tflite-micro](https://github.com/tensorflow/tflite-micro-arduino-examples/actions/workflows/sync.yml/badge.svg)](https://github.com/tensorflow/tflite-micro-arduino-examples/actions/workflows/sync.yml)\n\n## How to Install\n\n### GitHub\n\nThe officially supported TensorFlow Lite Micro library for Arduino resides\nin the [tflite-micro-arduino-examples](https://github.com/tensorflow/tflite-micro-arduino-examples)\nGitHub repository.\nTo install the in-development version of this library, you can use the\nlatest version directly from the GitHub repository. This requires you clone the\nrepo into the folder that holds libraries for the Arduino IDE. The location for\nthis folder varies by operating system, but typically it's in\n`~/Arduino/libraries` on Linux, `~/Documents/Arduino/libraries/` on MacOS, and\n`My Documents\\Arduino\\Libraries` on Windows.\n\nOnce you're in that folder in the terminal, you can then grab the code using the\ngit command line tool:\n\n```\ngit clone https://github.com/tensorflow/tflite-micro-arduino-examples Arduino_TensorFlowLite\n```\n\nTo update your clone of the repository to the latest code, use the following terminal commands:\n```\ncd Arduino_TensorFlowLite\ngit pull\n```\n\n### Checking your Installation\n\nOnce the library has been installed, you should then start the Arduino IDE.\nYou will now see an `Arduino_TensorFlowLite`\nentry in the `File -> Examples` menu of the Arduino IDE. This submenu contains a list\nof sample projects you can try out.\n\n![Hello World](docs/hello_world_screenshot.png)\n\n## Compatibility\n\nThis library is designed for the `Arduino Nano 33 BLE Sense` board. The framework\ncode for running machine learning models should be compatible with most Arm Cortex\nM-based boards, such as the `Raspberry Pi Pico`, but the code to access peripherals\nlike microphones, cameras, and accelerometers is specific to the `Nano 33 BLE Sense`.\n\n## License\n\nThis code is made available under the Apache 2 license.\n\n## Contributing\n\nForks of this library are welcome and encouraged. If you have bug reports or\nfixes to contribute, the source of this code is at [https://github.com/tensorflow/tflite-micro](http://github.com/tensorflow/tflite-micro)\nand all issues and pull requests should be directed there.\n\nThe code here is created through an automatic project generation process\nand may differ from\nthat source of truth, since it's cross-platform and needs to be modified to\nwork within the Arduino IDE.\n", "release_dates": []}, {"name": "tflite-support", "description": "TFLite Support is a toolkit that helps users to develop ML and deploy TFLite models onto mobile / ioT devices.", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Lite Support\n\nTFLite Support is a toolkit that helps users to develop ML and deploy TFLite\nmodels onto mobile devices. It works cross-Platform and is supported on Java,\nC++ (WIP), and Swift (WIP). The TFLite Support project consists of the following\nmajor components:\n\n*   **TFLite Support Library**: a cross-platform library that helps to deploy\n    TFLite models onto mobile devices.\n*   **TFLite Model Metadata**: (metadata populator and metadata extractor\n    library): includes both human and machine readable information about what a\n    model does and how to use the model.\n*   **TFLite Support Codegen Tool**: an executable that generates model wrapper\n    automatically based on the Support Library and the metadata.\n*   **TFLite Support Task Library**: a flexible and ready-to-use library for\n    common machine learning model types, such as classification and detection,\n    client can also build their own native/Android/iOS inference API on Task\n    Library infra.\n\nTFLite Support library serves different tiers of deployment requirements from\neasy onboarding to fully customizable. There are three major use cases that\nTFLite Support targets at:\n\n*   **Provide ready-to-use APIs for users to interact with the model**. \\\n    This is achieved by the TFLite Support Codegen tool, where users can get the\n    model interface (contains ready-to-use APIs) simply by passing the model to\n    the codegen tool. The automatic codegen strategy is designed based on the\n    TFLite metadata.\n\n*   **Provide optimized model interface for popular ML tasks**. \\\n    The model interfaces provided by the TFLite Support Task Library are\n    specifically optimized compared to the codegen version in terms of both\n    usability and performance. Users can also swap their own custom models with\n    the default models in each task.\n\n*   **Provide the flexibility to customize model interface and build inference\n    pipelines**. \\\n    The TFLite Support Util Library contains varieties of util methods and data\n    structures to perform pre/post processing and data conversion. It is also\n    designed to match the behavior of TensorFlow modules, such as TF.Image and\n    TF.text, ensuring consistency from training to inferencing.\n\nSee the\n[documentation on tensorflow.org](https://www.tensorflow.org/lite/inference_with_metadata/overview)\nfor more instruction and examples.\n\n## Build Instructions\n\nWe use Bazel to build the project. When you're building the Java (Android)\nUtils, you need to set up following env variables correctly:\n\n*   `ANDROID_NDK_HOME`\n*   `ANDROID_SDK_HOME`\n*   `ANDROID_NDK_API_LEVEL`\n*   `ANDROID_SDK_API_LEVEL`\n*   `ANDROID_BUILD_TOOLS_VERSION`\n\n## How to contribute\n\nPlease issue a pull request and assign @lu-wang-g for a code review.\n\n## Contact us\n\nLet us know what you think about TFLite Support by creating a\n[new Github issue](https://github.com/tensorflow/tflite-support/issues/new), or\nemail us at tflite-support-team@google.com.\n", "release_dates": ["2023-07-17T22:29:56Z", "2022-11-02T22:32:28Z", "2022-08-18T16:58:48Z", "2022-06-01T19:58:25Z", "2022-05-10T02:39:28Z", "2021-12-01T04:05:21Z", "2021-10-08T06:51:52Z", "2021-08-04T06:44:28Z", "2021-05-19T06:26:47Z", "2021-01-11T06:25:01Z", "2020-12-24T05:16:58Z"]}, {"name": "tfrc", "description": null, "language": null, "license": null, "readme": "![TRC](assets/tpu-research-cloud.png)\n\n--------------------------------------------------------------------------------\n\nResearchers require enormous computational resources to train the machine\nlearning models that have delivered recent breakthroughs in\n[medical imaging](https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html),\n[neural machine translation](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html),\n[game playing](https://deepmind.com/research/alphago/), and many other domains.\nWe believe that significantly larger amounts of computation will make it\npossible for researchers to invent new types of ML models that will be even more\naccurate and useful.\n\nTo accelerate the pace of open machine-learning research, we have introduced the\n**TPU Research Cloud** (TRC), a cluster of 1,000\n[Cloud TPUs](https://cloud.google.com/tpu/) that are made available free of\ncharge to support a broad range of computationally-intensive research projects\nthat might not be possible otherwise.\n\nLearn more about the TPU Research Cloud at\n[sites.research.google/trc](https://sites.research.google/trc/).\n", "release_dates": []}, {"name": "tfx", "description": " TFX is an end-to-end platform for deploying production ML pipelines", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<!-- See: www.tensorflow.org/tfx/ -->\n\n# TFX\n\n[![Python](https://img.shields.io/badge/python%20-3.8%7C3.9-blue)](https://github.com/tensorflow/tfx)\n[![PyPI](https://badge.fury.io/py/tfx.svg)](https://badge.fury.io/py/tfx)\n[![TensorFlow](https://img.shields.io/badge/TensorFow-page-orange)](https://www.tensorflow.org/tfx)\n\n[TensorFlow Extended (TFX)](https://tensorflow.org/tfx) is a\nGoogle-production-scale machine learning platform based on TensorFlow. It\nprovides a configuration framework to express ML pipelines consisting of TFX\ncomponents. TFX pipelines can be orchestrated using\n[Apache Airflow](https://airflow.apache.org/) and\n[Kubeflow Pipelines](https://www.kubeflow.org/). Both the components themselves\nas well as the integrations with orchestration systems can be extended.\n\nTFX components interact with a\n[ML Metadata](https://github.com/google/ml-metadata) backend that keeps a record\nof component runs, input and output artifacts, and runtime configuration. This\nmetadata backend enables advanced functionality like experiment tracking or\nwarmstarting/resuming ML models from previous runs.\n\n![TFX Components](https://raw.githubusercontent.com/tensorflow/tfx/master/docs/guide/images/prog_fin.png)\n\n## Documentation\n\n### User Documentation\n\nPlease see the\n[TFX User Guide](https://github.com/tensorflow/tfx/blob/master/docs/guide/index.md).\n\n### Development References\n\n#### Roadmap\n\nThe TFX [Roadmap](https://github.com/tensorflow/tfx/blob/master/ROADMAP.md),\nwhich is updated quarterly.\n\n#### Release Details\n\nFor detailed previous and upcoming changes, please\n[check here](https://github.com/tensorflow/tfx/blob/master/RELEASE.md)\n\n#### Requests For Comment\n\nTFX is an open-source project and we strongly encourage active participation\nby the ML community in helping to shape TFX to meet or exceed their needs. An\nimportant component of that effort is the RFC process.  Please see the listing\nof [current and past TFX RFCs](RFCs.md). Please see the\n[TensorFlow Request for Comments (TF-RFC)](https://github.com/tensorflow/community/blob/master/governance/TF-RFCs.md)\nprocess page for information on how community members can contribute.\n\n## Examples\n\n*   [Chicago Taxi Example](https://github.com/tensorflow/tfx/tree/master/tfx/examples/chicago_taxi_pipeline)\n\n## Compatible versions\n\nThe following table describes how the `tfx` package versions are compatible with\nits major dependency PyPI packages. This is determined by our testing framework,\nbut other *untested* combinations may also work.\n\ntfx                                                                       | Python               | apache-beam[gcp] | ml-metadata | pyarrow | tensorflow        | tensorflow-data-validation | tensorflow-metadata | tensorflow-model-analysis | tensorflow-serving-api | tensorflow-transform | tfx-bsl\n------------------------------------------------------------------------- | -------------------- | ---------------- | ----------- | ------- | ----------------- | -------------------------- | ------------------- | ------------------------- | ---------------------- | -------------------- | -------\n[GitHub master](https://github.com/tensorflow/tfx/blob/master/RELEASE.md) | >=3.9,<3.11          | 2.47.0           | 1.14.0      | 10.0.0  | nightly (2.x)     | 1.14.0                     | 1.14.0              | 0.45.0                    | 2.9.0                  | 1.14.0               | 1.14.0\n[1.14.0](https://github.com/tensorflow/tfx/blob/v1.14.0/RELEASE.md)       | >=3.8,<3.11          | 2.47.0           | 1.14.0      | 10.0.0  | 2.13              | 1.14.0                     | 1.14.0              | 0.45.0                    | 2.9.0                  | 1.14.0               | 1.14.0\n[1.13.0](https://github.com/tensorflow/tfx/blob/v1.13.0/RELEASE.md)       | >=3.8,<3.10          | 2.40.0           | 1.13.1      | 6.0.0   | 2.12              | 1.13.0                     | 1.13.1              | 0.44.0                    | 2.9.0                  | 1.13.0               | 1.13.0\n[1.12.0](https://github.com/tensorflow/tfx/blob/v1.12.0/RELEASE.md)       | >=3.7,<3.10          | 2.40.0           | 1.12.0      | 6.0.0   | 2.11              | 1.12.0                     | 1.12.0              | 0.43.0                    | 2.9.0                  | 1.12.0               | 1.12.0\n[1.11.0](https://github.com/tensorflow/tfx/blob/v1.11.0/RELEASE.md)       | >=3.7,<3.10          | 2.40.0           | 1.11.0      | 6.0.0   | 1.15.5 / 2.10.0   | 1.11.0                     | 1.11.0              | 0.42.0                    | 2.9.0                  | 1.11.0               | 1.11.0\n[1.10.0](https://github.com/tensorflow/tfx/blob/v1.10.0/RELEASE.md)       | >=3.7,<3.10          | 2.40.0           | 1.10.0      | 6.0.0   | 1.15.5 / 2.9.0    | 1.10.0                     | 1.10.0              | 0.41.0                    | 2.9.0                  | 1.10.0               | 1.10.0\n[1.9.0](https://github.com/tensorflow/tfx/blob/v1.9.0/RELEASE.md)         | >=3.7,<3.10          | 2.38.0           | 1.9.0       | 5.0.0   | 1.15.5 / 2.9.0    | 1.9.0                      | 1.9.0               | 0.40.0                    | 2.9.0                  | 1.9.0                | 1.9.0\n[1.8.0](https://github.com/tensorflow/tfx/blob/v1.8.0/RELEASE.md)         | >=3.7,<3.10          | 2.38.0           | 1.8.0       | 5.0.0   | 1.15.5 / 2.8.0    | 1.8.0                      | 1.8.0               | 0.39.0                    | 2.8.0                  | 1.8.0                | 1.8.0\n[1.7.0](https://github.com/tensorflow/tfx/blob/v1.7.0/RELEASE.md)         | >=3.7,<3.9           | 2.36.0           | 1.7.0       | 5.0.0   | 1.15.5 / 2.8.0    | 1.7.0                      | 1.7.0               | 0.38.0                    | 2.8.0                  | 1.7.0                | 1.7.0\n[1.6.2](https://github.com/tensorflow/tfx/blob/v1.6.2/RELEASE.md)         | >=3.7,<3.9           | 2.35.0           | 1.6.0       | 5.0.0   | 1.15.5 / 2.8.0    | 1.6.0                      | 1.6.0               | 0.37.0                    | 2.7.0                  | 1.6.0                | 1.6.0\n[1.6.0](https://github.com/tensorflow/tfx/blob/v1.6.0/RELEASE.md)         | >=3.7,<3.9           | 2.35.0           | 1.6.0       | 5.0.0   | 1.15.5 / 2.7.0    | 1.6.0                      | 1.6.0               | 0.37.0                    | 2.7.0                  | 1.6.0                | 1.6.0\n[1.5.0](https://github.com/tensorflow/tfx/blob/v1.5.0/RELEASE.md)         | >=3.7,<3.9           | 2.34.0           | 1.5.0       | 5.0.0   | 1.15.2 / 2.7.0    | 1.5.0                      | 1.5.0               | 0.36.0                    | 2.7.0                  | 1.5.0                | 1.5.0\n[1.4.0](https://github.com/tensorflow/tfx/blob/v1.4.0/RELEASE.md)         | >=3.7,<3.9           | 2.33.0           | 1.4.0       | 5.0.0   | 1.15.0 / 2.6.0    | 1.4.0                      | 1.4.0               | 0.35.0                    | 2.6.0                  | 1.4.0                | 1.4.0\n[1.3.4](https://github.com/tensorflow/tfx/blob/v1.3.4/RELEASE.md)         | >=3.6,<3.9           | 2.32.0           | 1.3.0       | 2.0.0   | 1.15.0 / 2.6.0    | 1.3.0                      | 1.2.0               | 0.34.1                    | 2.6.0                  | 1.3.0                | 1.3.0\n[1.3.3](https://github.com/tensorflow/tfx/blob/v1.3.3/RELEASE.md)         | >=3.6,<3.9           | 2.32.0           | 1.3.0       | 2.0.0   | 1.15.0 / 2.6.0    | 1.3.0                      | 1.2.0               | 0.34.1                    | 2.6.0                  | 1.3.0                | 1.3.0\n[1.3.2](https://github.com/tensorflow/tfx/blob/v1.3.2/RELEASE.md)         | >=3.6,<3.9           | 2.32.0           | 1.3.0       | 2.0.0   | 1.15.0 / 2.6.0    | 1.3.0                      | 1.2.0               | 0.34.1                    | 2.6.0                  | 1.3.0                | 1.3.0\n[1.3.1](https://github.com/tensorflow/tfx/blob/v1.3.1/RELEASE.md)         | >=3.6,<3.9           | 2.32.0           | 1.3.0       | 2.0.0   | 1.15.0 / 2.6.0    | 1.3.0                      | 1.2.0               | 0.34.1                    | 2.6.0                  | 1.3.0                | 1.3.0\n[1.3.0](https://github.com/tensorflow/tfx/blob/v1.3.0/RELEASE.md)         | >=3.6,<3.9           | 2.32.0           | 1.3.0       | 2.0.0   | 1.15.0 / 2.6.0    | 1.3.0                      | 1.2.0               | 0.34.1                    | 2.6.0                  | 1.3.0                | 1.3.0\n[1.2.1](https://github.com/tensorflow/tfx/blob/v1.2.1/RELEASE.md)         | >=3.6,<3.9           | 2.31.0           | 1.2.0       | 2.0.0   | 1.15.0 / 2.5.0    | 1.2.0                      | 1.2.0               | 0.33.0                    | 2.5.1                  | 1.2.0                | 1.2.0\n[1.2.0](https://github.com/tensorflow/tfx/blob/v1.2.0/RELEASE.md)         | >=3.6,<3.9           | 2.31.0           | 1.2.0       | 2.0.0   | 1.15.0 / 2.5.0    | 1.2.0                      | 1.2.0               | 0.33.0                    | 2.5.1                  | 1.2.0                | 1.2.0\n[1.0.0](https://github.com/tensorflow/tfx/blob/v1.0.0/RELEASE.md)         | >=3.6,<3.9           | 2.29.0           | 1.0.0       | 2.0.0   | 1.15.0 / 2.5.0    | 1.0.0                      | 1.0.0               | 0.31.0                    | 2.5.1                  | 1.0.0                | 1.0.0\n[0.30.0](https://github.com/tensorflow/tfx/blob/v0.30.0/RELEASE.md)       | >=3.6,<3.9           | 2.28.0           | 0.30.0      | 2.0.0   | 1.15.0 / 2.4.0    | 0.30.0                     | 0.30.0              | 0.30.0                    | 2.4.0                  | 0.30.0               | 0.30.0\n[0.29.0](https://github.com/tensorflow/tfx/blob/v0.29.0/RELEASE.md)       | >=3.6,<3.9           | 2.28.0           | 0.29.0      | 2.0.0   | 1.15.0 / 2.4.0    | 0.29.0                     | 0.29.0              | 0.29.0                    | 2.4.0                  | 0.29.0               | 0.29.0\n[0.28.0](https://github.com/tensorflow/tfx/blob/v0.28.0/RELEASE.md)       | >=3.6,<3.9           | 2.28.0           | 0.28.0      | 2.0.0   | 1.15.0 / 2.4.0    | 0.28.0                     | 0.28.0              | 0.28.0                    | 2.4.0                  | 0.28.0               | 0.28.1\n[0.27.0](https://github.com/tensorflow/tfx/blob/v0.27.0/RELEASE.md)       | >=3.6,<3.9           | 2.27.0           | 0.27.0      | 2.0.0   | 1.15.0 / 2.4.0    | 0.27.0                     | 0.27.0              | 0.27.0                    | 2.4.0                  | 0.27.0               | 0.27.0\n[0.26.4](https://github.com/tensorflow/tfx/blob/v0.26.4/RELEASE.md)       | >=3.6,<3.9           | 2.28.0           | 0.26.0      | 0.17.0  | 1.15.0 / 2.3.0    | 0.26.1                     | 0.26.0              | 0.26.0                    | 2.3.0                  | 0.26.0               | 0.26.0\n[0.26.3](https://github.com/tensorflow/tfx/blob/v0.26.3/RELEASE.md)       | >=3.6,<3.9           | 2.25.0           | 0.26.0      | 0.17.0  | 1.15.0 / 2.3.0    | 0.26.0                     | 0.26.0              | 0.26.0                    | 2.3.0                  | 0.26.0               | 0.26.0\n[0.26.1](https://github.com/tensorflow/tfx/blob/v0.26.1/RELEASE.md)       | >=3.6,<3.9           | 2.25.0           | 0.26.0      | 0.17.0  | 1.15.0 / 2.3.0    | 0.26.0                     | 0.26.0              | 0.26.0                    | 2.3.0                  | 0.26.0               | 0.26.0\n[0.26.0](https://github.com/tensorflow/tfx/blob/v0.26.0/RELEASE.md)       | >=3.6,<3.9           | 2.25.0           | 0.26.0      | 0.17.0  | 1.15.0 / 2.3.0    | 0.26.0                     | 0.26.0              | 0.26.0                    | 2.3.0                  | 0.26.0               | 0.26.0\n[0.25.0](https://github.com/tensorflow/tfx/blob/v0.25.0/RELEASE.md)       | >=3.6,<3.9           | 2.25.0           | 0.24.0      | 0.17.0  | 1.15.0 / 2.3.0    | 0.25.0                     | 0.25.0              | 0.25.0                    | 2.3.0                  | 0.25.0               | 0.25.0\n[0.24.1](https://github.com/tensorflow/tfx/blob/v0.24.1/RELEASE.md)       | >=3.6,<3.9           | 2.24.0           | 0.24.0      | 0.17.0  | 1.15.0 / 2.3.0    | 0.24.1                     | 0.24.0              | 0.24.3                    | 2.3.0                  | 0.24.1               | 0.24.1\n[0.24.0](https://github.com/tensorflow/tfx/blob/v0.24.0/RELEASE.md)       | >=3.6,<3.9           | 2.24.0           | 0.24.0      | 0.17.0  | 1.15.0 / 2.3.0    | 0.24.1                     | 0.24.0              | 0.24.3                    | 2.3.0                  | 0.24.1               | 0.24.1\n[0.23.1](https://github.com/tensorflow/tfx/blob/v0.23.1/RELEASE.md)       | >=3.5,<4             | 2.24.0           | 0.23.0      | 0.17.0  | 1.15.0 / 2.3.0    | 0.23.1                     | 0.23.0              | 0.23.0                    | 2.3.0                  | 0.23.0               | 0.23.0\n[0.23.0](https://github.com/tensorflow/tfx/blob/v0.23.0/RELEASE.md)       | >=3.5,<4             | 2.23.0           | 0.23.0      | 0.17.0  | 1.15.0 / 2.3.0    | 0.23.0                     | 0.23.0              | 0.23.0                    | 2.3.0                  | 0.23.0               | 0.23.0\n[0.22.2](https://github.com/tensorflow/tfx/blob/v0.22.2/RELEASE.md)       | >=3.5,<4             | 2.21.0           | 0.22.1      | 0.16.0  | 1.15.0 / 2.2.0    | 0.22.2                     | 0.22.2              | 0.22.2                    | 2.2.0                  | 0.22.0               | 0.22.1\n[0.22.1](https://github.com/tensorflow/tfx/blob/v0.22.1/RELEASE.md)       | >=3.5,<4             | 2.21.0           | 0.22.1      | 0.16.0  | 1.15.0 / 2.2.0    | 0.22.2                     | 0.22.2              | 0.22.2                    | 2.2.0                  | 0.22.0               | 0.22.1\n[0.22.0](https://github.com/tensorflow/tfx/blob/v0.22.0/RELEASE.md)       | >=3.5,<4             | 2.21.0           | 0.22.0      | 0.16.0  | 1.15.0 / 2.2.0    | 0.22.0                     | 0.22.0              | 0.22.1                    | 2.2.0                  | 0.22.0               | 0.22.0\n[0.21.5](https://github.com/tensorflow/tfx/blob/v0.21.5/RELEASE.md)       | >=2.7,<3 or >=3.5,<4 | 2.17.0           | 0.21.2      | 0.15.0  | 1.15.0 / 2.1.0    | 0.21.5                     | 0.21.1              | 0.21.5                    | 2.1.0                  | 0.21.2               | 0.21.4\n[0.21.4](https://github.com/tensorflow/tfx/blob/v0.21.4/RELEASE.md)       | >=2.7,<3 or >=3.5,<4 | 2.17.0           | 0.21.2      | 0.15.0  | 1.15.0 / 2.1.0    | 0.21.5                     | 0.21.1              | 0.21.5                    | 2.1.0                  | 0.21.2               | 0.21.4\n[0.21.3](https://github.com/tensorflow/tfx/blob/v0.21.3/RELEASE.md)       | >=2.7,<3 or >=3.5,<4 | 2.17.0           | 0.21.2      | 0.15.0  | 1.15.0 / 2.1.0    | 0.21.5                     | 0.21.1              | 0.21.5                    | 2.1.0                  | 0.21.2               | 0.21.4\n[0.21.2](https://github.com/tensorflow/tfx/blob/v0.21.2/RELEASE.md)       | >=2.7,<3 or >=3.5,<4 | 2.17.0           | 0.21.2      | 0.15.0  | 1.15.0 / 2.1.0    | 0.21.5                     | 0.21.1              | 0.21.5                    | 2.1.0                  | 0.21.2               | 0.21.4\n[0.21.1](https://github.com/tensorflow/tfx/blob/0.21.1/RELEASE.md)        | >=2.7,<3 or >=3.5,<4 | 2.17.0           | 0.21.2      | 0.15.0  | 1.15.0 / 2.1.0    | 0.21.4                     | 0.21.1              | 0.21.4                    | 2.1.0                  | 0.21.2               | 0.21.3\n[0.21.0](https://github.com/tensorflow/tfx/blob/0.21.0/RELEASE.md)        | >=2.7,<3 or >=3.5,<4 | 2.17.0           | 0.21.0      | 0.15.0  | 1.15.0 / 2.1.0    | 0.21.0                     | 0.21.0              | 0.21.1                    | 2.1.0                  | 0.21.0               | 0.21.0\n[0.15.0](https://github.com/tensorflow/tfx/blob/0.15.0/RELEASE.md)        | >=2.7,<3 or >=3.5,<4 | 2.16.0           | 0.15.0      | 0.15.0  | 1.15.0            | 0.15.0                     | 0.15.0              | 0.15.2                    | 1.15.0                 | 0.15.0               | 0.15.1\n[0.14.0](https://github.com/tensorflow/tfx/blob/0.14.0/RELEASE.md)        | >=2.7,<3 or >=3.5,<4 | 2.14.0           | 0.14.0      | 0.14.0  | 1.14.0            | 0.14.1                     | 0.14.0              | 0.14.0                    | 1.14.0                 | 0.14.0               | n/a\n[0.13.0](https://github.com/tensorflow/tfx/blob/0.13.0/RELEASE.md)        | >=2.7,<3 or >=3.5,<4 | 2.12.0           | 0.13.2      | n/a     | 1.13.1            | 0.13.1                     | 0.13.0              | 0.13.2                    | 1.13.0                 | 0.13.0               | n/a\n[0.12.0](https://github.com/tensorflow/tfx/blob/0.12.0/RELEASE.md)        | >=2.7,<3             | 2.10.0           | 0.13.2      | n/a     | 1.12.0            | 0.12.0                     | 0.12.1              | 0.12.1                    | 1.12.0                 | 0.12.0               | n/a\n\n## Resources\n\n*   [TFX tutorials ](https://www.tensorflow.org/tfx/tutorials)\n*   [TensorFlow Extended (YouTube)](https://www.youtube.com/playlist?list=PLQY2H8rRoyvxR15n04JiW0ezF5HQRs_8F)\n*   [ MLOps Specialization ](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops)\n*   [ML Pipelines on Google Cloud](https://www.coursera.org/learn/ml-pipelines-google-cloud?specialization=preparing-for-google-cloud-machine-learning-engineer-professional-certificate)\n*   [Manage a production ML pipeline with TFX](https://www.youtube.com/watch?v=QQ13-Tkrbls)\n*   [How to build an ML pipeline with TFX](https://www.youtube.com/watch?v=17l3VR2MIeg)\n", "release_dates": ["2023-09-06T18:31:57Z", "2023-08-28T18:11:49Z", "2023-05-03T18:43:05Z", "2023-04-14T21:31:02Z", "2022-12-19T16:48:53Z", "2022-12-13T22:59:05Z", "2022-11-23T20:52:29Z", "2022-11-17T19:28:25Z", "2022-09-30T16:57:21Z", "2022-09-23T16:11:15Z", "2022-08-02T20:41:16Z", "2022-08-01T22:38:40Z", "2022-07-15T17:26:56Z", "2022-07-08T09:08:42Z", "2022-05-25T23:32:49Z", "2022-05-23T07:39:42Z", "2022-05-20T21:46:47Z", "2022-05-17T21:14:59Z", "2022-05-09T05:07:52Z", "2022-04-04T05:12:20Z", "2022-03-30T22:21:32Z", "2022-03-16T22:34:55Z", "2022-03-10T04:11:59Z", "2022-02-07T22:02:56Z", "2022-01-31T22:23:13Z", "2022-01-25T03:10:06Z", "2022-01-20T22:50:29Z", "2021-12-14T19:05:52Z", "2021-12-07T02:05:29Z", "2021-12-06T21:19:11Z"]}, {"name": "tfx-addons", "description": "Developers helping developers.  TFX-Addons is a collection of community projects to build new components, examples, libraries, and tools for TFX.  The projects are organized under the auspices of the special interest group, SIG TFX-Addons.  Join  the group at http://goo.gle/tfx-addons-group", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TFX Addons\n\n[![TFX Addons package CI](https://github.com/tensorflow/tfx-addons/actions/workflows/ci.yml/badge.svg)](https://github.com/tensorflow/tfx-addons/actions/workflows/ci.yml)\n[![TFX Addons CI for examples](https://github.com/tensorflow/tfx-addons/actions/workflows/ci_examples.yml/badge.svg)](https://github.com/tensorflow/tfx-addons/actions/workflows/ci_examples.yml)\n[![PyPI](https://badge.fury.io/py/tfx-addons.svg)](https://badge.fury.io/py/tfx-addons)\n\n\nSIG TFX-Addons is a community-led open source project. As such, the project depends on public contributions, bug fixes, and documentation. This project adheres to the [TensorFlow Code of Conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md). By participating, you are expected to uphold this code.\n\n## Maintainership\n\nThe maintainers of TensorFlow Addons can be found in the [CODEOWNERS](https://github.com/tensorflow/tfx-addons/blob/main/CODEOWNERS) file of the repo. If you would\nlike to maintain something, please feel free to submit a PR. We encourage multiple \nowners for all submodules.\n\n\n## Installation\n\nTFX Addons is available on PyPI for all OS. To install the latest version, \nrun the following:\n\n```\npip install tfx-addons\n```\n\nTo ensure you have a compatible version of dependencies for any given project, \nyou can specify the project name  as an extra requirement during install:\n\n```\npip install tfx-addons[feast_examplegen,schema_curation]\n``` \n\nTo use TFX Addons:\n\n```python\nfrom tfx import v1 as tfx\nimport tfx_addons as tfxa\n\n# Then you can easily load projects tfxa.{project_name}. Ex:\n\ntfxa.feast_examplegen.FeastExampleGen(...)\n\n```\n\n\n## TFX Addons projects\n\n* [tfxa.feast_examplegen](https://github.com/tensorflow/tfx-addons/tree/main/tfx_addons/feast_examplegen) \n* [tfxa.feature_selection](https://github.com/tensorflow/tfx-addons/tree/main/tfx_addons/feature_selection)\n* [tfxa.firebase_publisher](https://github.com/tensorflow/tfx-addons/tree/main/tfx_addons/firebase_publisher)\n* [tfxa.huggingface_pusher](https://github.com/tensorflow/tfx-addons/tree/main/tfx_addons/huggingface_pusher)\n* [tfxa.message_exit_handler](https://github.com/tensorflow/tfx-addons/tree/main/tfx_addons/message_exit_handler) \n* [tfxa.mlmd_client](https://github.com/tensorflow/tfx-addons/tree/main/tfx_addons/mlmd_client) \n* [tfxa.model_card_generator](https://github.com/tensorflow/tfx-addons/tree/main/tfx_addons/model_card_generator)\n* [tfxa.pandas_transform](https://github.com/tensorflow/tfx-addons/tree/main/tfx_addons/pandas_transform) \n* [tfxa.predictions_to_bigquery](https://github.com/tensorflow/tfx-addons/tree/main/tfx_addons/predictions_to_bigquery) \n* [tfxa.sampling](https://github.com/tensorflow/tfx-addons/tree/main/tfx_addons/sampling)\n* [tfxa.schema_curation](https://github.com/tensorflow/tfx-addons/tree/main/tfx_addons/schema_curation) \n* [tfxa.xgboost_evaluator](https://github.com/tensorflow/tfx-addons/tree/main/tfx_addons/xgboost_evaluator)\n \n\nCheck out [proposals](https://github.com/tensorflow/tfx-addons/tree/main/proposals) for a list of existing or upcoming projects proposals for TFX Addons.\n\n\n## Tutorials and examples\nSee [`examples/`](examples/)\nfor end-to-end examples of various addons.\n\n## Contributing\n\nTFX Addons is a community-led project. Please have a look at our contributing and development guides if you want to contribute to the project: [CONTRIBUTING.md](https://github.com/tensorflow/tfx-addons/blob/main/CONTRIBUTING.md)\n\n### Meeting cadence:\n\nWe meet bi-weekly on Wednesday. Check out our [Meeting notes](https://docs.google.com/document/d/1T0uZPoZhwNStuKkeCNsfE-kfc-PINISKIitYxkTK3Gw/edit?resourcekey=0-N9vT9Tn171wYplyYn4IPjQ) and join [sig-tfx-addons@tensorflow.com](https://groups.google.com/a/tensorflow.org/g/sig-tfx-addons) to get invited to the meeting.\n\n## Package releases\n\nCheck out [RELEASE.md](https://github.com/tensorflow/tfx-addons/blob/main/RELEASE.md) to learn how TFX Addons is released.\n\n## Resources\n\n- [sig-tfx-addons@tensorflow.org](https://groups.google.com/a/tensorflow.org/g/sig-tfx-addons) \u2013 Join our Google group\n- [tfx@tensorflow.org](https://groups.google.com/a/tensorflow.org/g/tfx) \u2013 General TFX mailing list\n- [TFX Addons Slack](https://tfxaddons.slack.com) -  join [here](https://join.slack.com/t/tfxaddons/shared_invite/zt-tu1981lj-npIhRSHF8gl9G0ldUovbcw)\n- [SIG Repository](http://github.com/tensorflow/tfx-addons) (this repo)\n- [SIG Charter](https://github.com/tensorflow/community/blob/master/sigs/tfx-addons/CHARTER.md)\n\n", "release_dates": ["2023-06-07T16:06:50Z", "2023-05-12T17:19:55Z", "2023-04-17T23:50:59Z", "2023-04-17T22:44:20Z", "2023-02-03T16:41:40Z", "2023-02-02T22:39:33Z", "2023-02-02T18:03:56Z", "2022-10-11T15:24:01Z", "2022-10-05T20:37:31Z", "2022-08-22T20:47:31Z", "2022-08-18T21:34:02Z", "2022-06-13T19:47:22Z", "2022-05-26T22:36:00Z", "2022-05-23T21:52:01Z", "2022-05-23T21:49:15Z", "2022-05-20T22:28:36Z", "2022-05-17T21:31:21Z", "2022-04-18T16:32:34Z", "2022-04-13T16:49:48Z", "2022-04-12T23:46:47Z", "2022-04-12T23:19:15Z", "2022-04-12T23:16:07Z"]}, {"name": "tfx-bsl", "description": "Common code for TFX", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# TFX Basic Shared Libraries\n\n[![Python](https://img.shields.io/badge/python%7C3.9%7C3.10%7C3.11-blue)](https://github.com/tensorflow/tfx-bsl)\n[![PyPI](https://badge.fury.io/py/tfx-bsl.svg)](https://badge.fury.io/py/tfx-bsl)\n\nTFX Basic Shared Libraries (`tfx_bsl`) contains libraries shared by many\n[TensorFlow eXtended (TFX)](https://www.tensorflow.org/tfx) components.\n\n__Only symbols exported by sub-modules under `tfx_bsl/public` are intended for\ndirect use by TFX users__, including by standalone TFX library (e.g. TFDV, TFMA,\nTFT) users, TFX pipeline authors and TFX component authors. Those APIs will\nbecome stable and follow semantic versioning once `tfx_bsl` goes beyond `1.0`.\n\nAPIs under other directories should be considered internal to TFX\n(and therefore there is no backward or forward compatibility guarantee for\nthem).\n\nEach minor version of a TFX library or TFX itself, if it needs to\ndepend on `tfx_bsl`, will depend on a specific minor version of it (e.g.\n`tensorflow_data_validation` 0.14.\\* will depend on, and only work with,\n`tfx_bsl` 0.14.\\*)\n\n## Installing from PyPI\n\n`tfx_bsl` is available as a [PyPI package](https://pypi.org/project/tfx-bsl/).\n\n```bash\npip install tfx-bsl\n```\n\n### Nightly Packages\n\nTFX-BSL also hosts nightly packages at https://pypi-nightly.tensorflow.org on\nGoogle Cloud. To install the latest nightly package, please use the following\ncommand:\n\n```bash\npip install --extra-index-url https://pypi-nightly.tensorflow.org/simple tfx-bsl\n```\n\nThis will install the nightly packages for the major dependencies of TFX-BSL\nsuch as TensorFlow Metadata (TFMD).\n\nHowever it is a dependency of many TFX components and usually as a user you\ndon't need to install it directly.\n\n## Build with Docker\n\nIf you want to build a TFX component from the master branch, past the latest\nrelease, you may also have to build the latest `tfx_bsl`, as that TFX component\nmight have depended on new features introduced past the latest `tfx_bsl`\nrelease.\n\nBuilding from Docker is the recommended way to build `tfx_bsl` under Linux,\nand is continuously tested at Google.\n\n### 1. Install Docker\n\nPlease first install [`docker`](https://docs.docker.com/install/) and\n[`docker-compose`](https://docs.docker.com/compose/install/) by following the\ndirections.\n\n### 2. Clone the `tfx_bsl` repository\n\n```shell\ngit clone https://github.com/tensorflow/tfx-bsl\ncd tfx-bsl\n```\n\nNote that these instructions will install the latest master branch of `tfx-bsl`.\nIf you want to install a specific branch (such as a release branch), pass\n`-b <branchname>` to the `git clone` command.\n\n### 3. Build the pip package\n\nThen, run the following at the project root:\n\n```bash\nsudo docker-compose build manylinux2010\nsudo docker-compose run -e PYTHON_VERSION=${PYTHON_VERSION} manylinux2010\n```\nwhere `PYTHON_VERSION` is one of `{39}`.\n\nA wheel will be produced under `dist/`.\n\n### 4. Install the pip package\n\n```shell\npip install dist/*.whl\n```\n\n## Build from source\n\n### 1. Prerequisites\n\n#### Install NumPy\n\nIf NumPy is not installed on your system, install it now by following [these\ndirections](https://www.scipy.org/scipylib/download.html).\n\n#### Install Bazel\n\nIf Bazel is not installed on your system, install it now by following [these\ndirections](https://bazel.build/versions/master/docs/install.html).\n\n\n### 2. Clone the `tfx_bsl` repository\n\n```shell\ngit clone https://github.com/tensorflow/tfx-bsl\ncd tfx-bsl\n```\n\nNote that these instructions will install the latest master branch of `tfx_bsl`\nIf you want to install a specific branch (such as a release branch),\npass `-b <branchname>` to the `git clone` command.\n\n### 3. Build the pip package\n\n`tfx_bsl` wheel is Python version dependent -- to build the pip package that\nworks for a specific Python version, use that Python binary to run:\n```shell\npython setup.py bdist_wheel\n```\n\nYou can find the generated `.whl` file in the `dist` subdirectory.\n\n### 4. Install the pip package\n\n```shell\npip install dist/*.whl\n```\n\n## Supported platforms\n\n`tfx_bsl` is tested on the following 64-bit operating systems:\n\n  * macOS 10.12.6 (Sierra) or later.\n  * Ubuntu 20.04 or later.\n\n## Compatible versions\n\nThe following table is the `tfx_bsl` package versions that are compatible with\neach other. This is determined by our testing framework, but other *untested*\ncombinations may also work.\n\ntfx-bsl                                                                         | apache-beam[gcp] | pyarrow  | tensorflow        | tensorflow-metadata | tensorflow-serving-api |\n------------------------------------------------------------------------------- | -----------------| ---------|-------------------|---------------------|------------------------|\n[GitHub master](https://github.com/tensorflow/tfx-bsl/blob/master/RELEASE.md)   | 2.47.0           | 10.0.0   | nightly (2.x)     | 1.14.0              | 2.13.0                 |\n[1.14.0](https://github.com/tensorflow/tfx-bsl/blob/v1.14.0/RELEASE.md)         | 2.47.0           | 10.0.0   | 2.13              | 1.14.0              | 2.13.0                 |\n[1.13.0](https://github.com/tensorflow/tfx-bsl/blob/v1.13.0/RELEASE.md)         | 2.40.0           | 6.0.0    | 2.12              | 1.13.1              | 2.9.0                  |\n[1.12.0](https://github.com/tensorflow/tfx-bsl/blob/v1.12.0/RELEASE.md)         | 2.40.0           | 6.0.0    | 2.11              | 1.12.0              | 2.9.0                  |\n[1.11.0](https://github.com/tensorflow/tfx-bsl/blob/v1.11.0/RELEASE.md)         | 2.40.0           | 6.0.0    | 1.15 / 2.10       | 1.11.0              | 2.9.0                  |\n[1.10.0](https://github.com/tensorflow/tfx-bsl/blob/v1.10.0/RELEASE.md)         | 2.40.0           | 6.0.0    | 1.15 / 2.9        | 1.10.0              | 2.9.0                  |\n[1.9.0](https://github.com/tensorflow/tfx-bsl/blob/v1.9.0/RELEASE.md)           | 2.38.0           | 5.0.0    | 1.15 / 2.9        | 1.9.0               | 2.9.0                  |\n[1.8.0](https://github.com/tensorflow/tfx-bsl/blob/v1.8.0/RELEASE.md)           | 2.38.0           | 5.0.0    | 1.15 / 2.8        | 1.8.0               | 2.8.0                  |\n[1.7.0](https://github.com/tensorflow/tfx-bsl/blob/v1.7.0/RELEASE.md)           | 2.36.0           | 5.0.0    | 1.15 / 2.8        | 1.7.0               | 2.8.0                  |\n[1.6.0](https://github.com/tensorflow/tfx-bsl/blob/v1.6.0/RELEASE.md)           | 2.35.0           | 5.0.0    | 1.15 / 2.7        | 1.6.0               | 2.7.0                  |\n[1.5.0](https://github.com/tensorflow/tfx-bsl/blob/v1.4.0/RELEASE.md)           | 2.34.0           | 5.0.0    | 1.15 / 2.7        | 1.5.0               | 2.7.0                  |\n[1.4.0](https://github.com/tensorflow/tfx-bsl/blob/v1.4.0/RELEASE.md)           | 2.31.0           | 5.0.0    | 1.15 / 2.6        | 1.4.0               | 2.6.0                  |\n[1.3.0](https://github.com/tensorflow/tfx-bsl/blob/v1.3.0/RELEASE.md)           | 2.31.0           | 2.0.0    | 1.15 / 2.6        | 1.2.0               | 2.6.0                  |\n[1.2.0](https://github.com/tensorflow/tfx-bsl/blob/v1.2.0/RELEASE.md)           | 2.31.0           | 2.0.0    | 1.15 / 2.5        | 1.2.0               | 2.5.1                  |\n[1.1.0](https://github.com/tensorflow/tfx-bsl/blob/v1.1.0/RELEASE.md)           | 2.29.0           | 2.0.0    | 1.15 / 2.5        | 1.1.0               | 2.5.1                  |\n[1.0.0](https://github.com/tensorflow/tfx-bsl/blob/v1.0.0/RELEASE.md)           | 2.29.0           | 2.0.0    | 1.15 / 2.5        | 1.0.0               | 2.5.1                  |\n[0.30.0](https://github.com/tensorflow/tfx-bsl/blob/v0.30.0/RELEASE.md)         | 2.28.0           | 2.0.0    | 1.15 / 2.4        | 0.30.0              | 2.4.0                  |\n[0.29.0](https://github.com/tensorflow/tfx-bsl/blob/v0.29.0/RELEASE.md)         | 2.28.0           | 2.0.0    | 1.15 / 2.4        | 0.29.0              | 2.4.0                  |\n[0.28.0](https://github.com/tensorflow/tfx-bsl/blob/v0.28.0/RELEASE.md)         | 2.28.0           | 2.0.0    | 1.15 / 2.4        | 0.28.0              | 2.4.0                  |\n[0.27.1](https://github.com/tensorflow/tfx-bsl/blob/v0.27.1/RELEASE.md)         | 2.27.0           | 2.0.0    | 1.15 / 2.4        | 0.27.0              | 2.4.0                  |\n[0.27.0](https://github.com/tensorflow/tfx-bsl/blob/v0.27.0/RELEASE.md)         | 2.27.0           | 2.0.0    | 1.15 / 2.4        | 0.27.0              | 2.4.0                  |\n[0.26.1](https://github.com/tensorflow/tfx-bsl/blob/v0.26.1/RELEASE.md)         | 2.25.0           | 0.17.0   | 1.15 / 2.3        | 0.27.0              | 2.3.0                  |\n[0.26.0](https://github.com/tensorflow/tfx-bsl/blob/v0.26.0/RELEASE.md)         | 2.25.0           | 0.17.0   | 1.15 / 2.3        | 0.27.0              | 2.3.0                  |\n", "release_dates": ["2023-08-11T20:09:11Z", "2023-04-13T17:54:49Z", "2022-12-07T22:11:17Z", "2022-11-14T19:17:43Z", "2022-08-26T17:49:07Z", "2022-08-26T17:00:11Z", "2022-06-28T05:48:53Z", "2022-05-12T18:40:06Z", "2022-03-01T23:24:09Z", "2022-01-20T18:45:59Z", "2021-12-01T18:49:33Z", "2021-10-27T19:44:44Z", "2021-08-25T16:43:57Z", "2021-07-28T18:01:10Z", "2021-07-16T18:53:37Z", "2021-06-22T00:55:47Z", "2021-05-24T16:13:00Z", "2021-04-21T16:42:29Z", "2021-03-24T18:09:33Z", "2021-02-23T17:25:42Z", "2021-02-22T23:58:12Z", "2021-01-28T22:26:50Z", "2021-01-27T19:25:08Z", "2020-12-17T00:28:02Z", "2020-12-11T19:21:59Z", "2020-11-03T23:04:27Z", "2020-09-24T18:51:14Z", "2020-09-09T21:08:30Z", "2020-08-12T19:32:15Z", "2020-06-24T20:59:48Z"]}, {"name": "toolchains", "description": "Bazel toolchain configurations used across TensorFlow ecosystem", "language": "Starlark", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# TensorFlow Toolchains (DEPRECATED)\n\n**The TensorFlow toolchains have moved to the main TensorFlow repository. See [tensorflow/tools/toolchains](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/toolchains).**\n", "release_dates": ["2022-02-08T19:42:42Z", "2022-02-08T19:22:14Z", "2022-02-08T17:32:30Z", "2022-02-04T19:28:23Z", "2022-02-01T18:07:34Z", "2022-01-26T17:02:50Z", "2022-01-26T00:36:12Z", "2022-01-24T20:56:35Z", "2022-01-22T16:53:39Z", "2022-01-21T23:24:14Z", "2022-01-12T18:38:43Z", "2022-01-07T22:46:32Z", "2022-01-07T17:51:16Z", "2022-01-05T17:58:41Z", "2021-12-02T16:01:54Z", "2021-11-18T21:30:28Z", "2021-11-16T17:04:51Z", "2021-10-15T02:09:37Z", "2021-10-04T19:15:44Z", "2021-08-24T14:48:55Z", "2021-08-19T20:43:55Z", "2021-08-16T23:59:57Z", "2021-08-16T23:21:13Z", "2021-08-16T17:22:41Z", "2021-07-07T18:48:04Z", "2021-06-29T22:39:43Z", "2021-06-29T19:49:53Z", "2021-06-23T16:47:41Z", "2021-06-23T00:12:59Z", "2021-06-22T18:43:03Z"]}, {"name": "tpu", "description": "Reference models and tools for Cloud TPUs.", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Cloud TPUs #\n\nThis repository is a collection of reference models and tools used with\n[Cloud TPUs](https://cloud.google.com/tpu/).\n\nThe fastest way to get started training a model on a Cloud TPU is by following\nthe tutorial. Click the button below to launch the tutorial using Google Cloud\nShell.\n\n[![Open in Cloud Shell](http://gstatic.com/cloudssh/images/open-btn.svg)](https://console.cloud.google.com/cloudshell/open?git_repo=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftpu&page=shell&tutorial=tools%2Fctpu%2Ftutorial.md)\n\n_Note:_ This repository is a public mirror, pull requests will not be accepted.\nPlease file an issue if you have a feature or bug request.\n\n## Running Models\n\nTo run models in the `models` subdirectory, you may need to add the top-level\n`/models` folder to the Python path with the command:\n\n```\nexport PYTHONPATH=\"$PYTHONPATH:/path/to/models\"\n```\n", "release_dates": []}, {"name": "transform", "description": "Input pipeline framework", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<!-- See: www.tensorflow.org/tfx/transform/ -->\n\n# TensorFlow Transform\n\n[![Python](https://img.shields.io/badge/python%7C3.9%7C3.10%7C3.11-blue)](https://github.com/tensorflow/transform)\n[![PyPI](https://badge.fury.io/py/tensorflow-transform.svg)](https://badge.fury.io/py/tensorflow-transform)\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/tfx/transform/api_docs/python/tft)\n\n*TensorFlow Transform* is a library for preprocessing data with TensorFlow.\n`tf.Transform` is useful for data that requires a full-pass, such as:\n\n* Normalize an input value by mean and standard deviation.\n* Convert strings to integers by generating a vocabulary over all input values.\n* Convert floats to integers by assigning them to buckets based on the observed\n  data distribution.\n\nTensorFlow has built-in support for manipulations on a single example or a batch\nof examples. `tf.Transform` extends these capabilities to support full-passes\nover the example data.\n\nThe output of `tf.Transform` is exported as a\n[TensorFlow graph](http://tensorflow.org/guide/graphs) to use for training and serving.\nUsing the same graph for both training and serving can prevent skew since the\nsame transformations are applied in both stages.\n\nFor an introduction to `tf.Transform`, see the `tf.Transform` section of the\nTFX Dev Summit talk on TFX\n([link](https://www.youtube.com/watch?v=vdG7uKQ2eKk&feature=youtu.be&t=199)).\n\n## Installation\n\nThe `tensorflow-transform`\n[PyPI package](https://pypi.org/project/tensorflow-transform/) is the\nrecommended way to install `tf.Transform`:\n\n```bash\npip install tensorflow-transform\n```\n\n### Build TFT from source\n\nTo build from source follow the following steps:\nCreate a virtual environment by running the commands\n\n```\npython3 -m venv <virtualenv_name>\nsource <virtualenv_name>/bin/activate\npip3 install setuptools wheel\ngit clone https://github.com/tensorflow/transform.git\ncd transform\npython3 setup.py bdist_wheel\n```\n\nThis will build the TFT wheel in the dist directory. To install the wheel from\ndist directory run the commands\n\n```\ncd dist\npip3 install tensorflow_transform-<version>-py3-none-any.whl\n```\n\n### Nightly Packages\n\nTFT also hosts nightly packages at https://pypi-nightly.tensorflow.org on\nGoogle Cloud. To install the latest nightly package, please use the following\ncommand:\n\n```bash\npip install --extra-index-url https://pypi-nightly.tensorflow.org/simple tensorflow-transform\n```\n\nThis will install the nightly packages for the major dependencies of TFT such\nas TensorFlow Metadata (TFMD), TFX Basic Shared Libraries (TFX-BSL).\n\n### Notable Dependencies\n\nTensorFlow is required.\n\n[Apache Beam](https://beam.apache.org/) is required; it's the way that efficient\ndistributed computation is supported. By default, Apache Beam runs in local\nmode but can also run in distributed mode using\n[Google Cloud Dataflow](https://cloud.google.com/dataflow/) and other Apache\nBeam\n[runners](https://beam.apache.org/documentation/runners/capability-matrix/).\n\n[Apache Arrow](https://arrow.apache.org/) is also required. TFT uses Arrow to\nrepresent data internally in order to make use of vectorized numpy functions.\n\n## Compatible versions\n\nThe following table is the `tf.Transform` package versions that are\ncompatible with each other. This is determined by our testing framework, but\nother *untested* combinations may also work.\n\ntensorflow-transform                                                            | apache-beam[gcp] | pyarrow | tensorflow        | tensorflow-metadata | tfx-bsl |\n------------------------------------------------------------------------------- | -----------------| --------|-------------------|---------------------|---------|\n[GitHub master](https://github.com/tensorflow/transform/blob/master/RELEASE.md) | 2.47.0           | 10.0.0  | nightly (2.x)     | 1.14.0              | 1.14.0  |\n[1.14.0](https://github.com/tensorflow/transform/blob/v1.14.0/RELEASE.md)       | 2.47.0           | 10.0.0  | 2.13              | 1.14.0              | 1.14.0  |\n[1.13.0](https://github.com/tensorflow/transform/blob/v1.13.0/RELEASE.md)       | 2.41.0           | 6.0.0   | 2.12              | 1.13.1              | 1.13.0  |\n[1.12.0](https://github.com/tensorflow/transform/blob/v1.12.0/RELEASE.md)       | 2.41.0           | 6.0.0   | 2.11              | 1.12.0              | 1.12.0  |\n[1.11.0](https://github.com/tensorflow/transform/blob/v1.11.0/RELEASE.md)       | 2.41.0           | 6.0.0   | 1.15.5 / 2.10     | 1.11.0              | 1.11.0  |\n[1.10.0](https://github.com/tensorflow/transform/blob/v1.10.0/RELEASE.md)       | 2.40.0           | 6.0.0   | 1.15.5 / 2.9      | 1.10.0              | 1.10.0  |\n[1.9.0](https://github.com/tensorflow/transform/blob/v1.9.0/RELEASE.md)         | 2.38.0           | 5.0.0   | 1.15.5 / 2.9      | 1.9.0               | 1.9.0   |\n[1.8.0](https://github.com/tensorflow/transform/blob/v1.8.0/RELEASE.md)         | 2.38.0           | 5.0.0   | 1.15.5 / 2.8      | 1.8.0               | 1.8.0   |\n[1.7.0](https://github.com/tensorflow/transform/blob/v1.7.0/RELEASE.md)         | 2.36.0           | 5.0.0   | 1.15.5 / 2.8      | 1.7.0               | 1.7.0   |\n[1.6.1](https://github.com/tensorflow/transform/blob/v1.6.1/RELEASE.md)         | 2.35.0           | 5.0.0   | 1.15.5 / 2.8      | 1.6.0               | 1.6.0   |\n[1.6.0](https://github.com/tensorflow/transform/blob/v1.6.0/RELEASE.md)         | 2.35.0           | 5.0.0   | 1.15.5 / 2.7      | 1.6.0               | 1.6.0   |\n[1.5.0](https://github.com/tensorflow/transform/blob/v1.5.0/RELEASE.md)         | 2.34.0           | 5.0.0   | 1.15.2 / 2.7      | 1.5.0               | 1.5.0   |\n[1.4.1](https://github.com/tensorflow/transform/blob/v1.4.1/RELEASE.md)         | 2.33.0           | 4.0.1   | 1.15.2 / 2.6      | 1.4.0               | 1.4.0   |\n[1.4.0](https://github.com/tensorflow/transform/blob/v1.4.0/RELEASE.md)         | 2.33.0           | 4.0.1   | 1.15.2 / 2.6      | 1.4.0               | 1.4.0   |\n[1.3.0](https://github.com/tensorflow/transform/blob/v1.3.0/RELEASE.md)         | 2.31.0           | 2.0.0   | 1.15.2 / 2.6      | 1.2.0               | 1.3.0   |\n[1.2.0](https://github.com/tensorflow/transform/blob/v1.2.0/RELEASE.md)         | 2.31.0           | 2.0.0   | 1.15.2 / 2.5      | 1.2.0               | 1.2.0   |\n[1.1.1](https://github.com/tensorflow/transform/blob/v1.1.1/RELEASE.md)         | 2.29.0           | 2.0.0   | 1.15.2 / 2.5      | 1.1.0               | 1.1.1   |\n[1.1.0](https://github.com/tensorflow/transform/blob/v1.1.0/RELEASE.md)         | 2.29.0           | 2.0.0   | 1.15.2 / 2.5      | 1.1.0               | 1.1.0   |\n[1.0.0](https://github.com/tensorflow/transform/blob/v1.0.0/RELEASE.md)         | 2.29.0           | 2.0.0   | 1.15 / 2.5        | 1.0.0               | 1.0.0   |\n[0.30.0](https://github.com/tensorflow/transform/blob/v0.30.0/RELEASE.md)       | 2.28.0           | 2.0.0   | 1.15 / 2.4        | 0.30.0              | 0.30.0  |\n[0.29.0](https://github.com/tensorflow/transform/blob/v0.29.0/RELEASE.md)       | 2.28.0           | 2.0.0   | 1.15 / 2.4        | 0.29.0              | 0.29.0  |\n[0.28.0](https://github.com/tensorflow/transform/blob/v0.28.0/RELEASE.md)       | 2.28.0           | 2.0.0   | 1.15 / 2.4        | 0.28.0              | 0.28.1  |\n[0.27.0](https://github.com/tensorflow/transform/blob/v0.27.0/RELEASE.md)       | 2.27.0           | 2.0.0   | 1.15 / 2.4        | 0.27.0              | 0.27.0  |\n[0.26.0](https://github.com/tensorflow/transform/blob/v0.26.0/RELEASE.md)       | 2.25.0           | 0.17.0  | 1.15 / 2.3        | 0.26.0              | 0.26.0  |\n[0.25.0](https://github.com/tensorflow/transform/blob/v0.25.0/RELEASE.md)       | 2.25.0           | 0.17.0  | 1.15 / 2.3        | 0.25.0              | 0.25.0  |\n[0.24.1](https://github.com/tensorflow/transform/blob/v0.24.1/RELEASE.md)       | 2.24.0           | 0.17.0  | 1.15 / 2.3        | 0.24.0              | 0.24.1  |\n[0.24.0](https://github.com/tensorflow/transform/blob/v0.24.0/RELEASE.md)       | 2.23.0           | 0.17.0  | 1.15 / 2.3        | 0.24.0              | 0.24.0  |\n[0.23.0](https://github.com/tensorflow/transform/blob/v0.23.0/RELEASE.md)       | 2.23.0           | 0.17.0  | 1.15 / 2.3        | 0.23.0              | 0.23.0  |\n[0.22.0](https://github.com/tensorflow/transform/blob/v0.22.0/RELEASE.md)       | 2.20.0           | 0.16.0  | 1.15 / 2.2        | 0.22.0              | 0.22.0  |\n[0.21.2](https://github.com/tensorflow/transform/blob/v0.21.2/RELEASE.md)       | 2.17.0           | 0.15.0  | 1.15 / 2.1        | 0.21.0              | 0.21.3  |\n[0.21.0](https://github.com/tensorflow/transform/blob/v0.21.0/RELEASE.md)       | 2.17.0           | 0.15.0  | 1.15 / 2.1        | 0.21.0              | 0.21.0  |\n[0.15.0](https://github.com/tensorflow/transform/blob/v0.15.0/RELEASE.md)       | 2.16.0           | 0.14.0  | 1.15 / 2.0        | 0.15.0              | 0.15.0  |\n[0.14.0](https://github.com/tensorflow/transform/blob/v0.14.0/RELEASE.md)       | 2.14.0           | 0.14.0  | 1.14              | 0.14.0              | n/a     |\n[0.13.0](https://github.com/tensorflow/transform/blob/v0.13.0/RELEASE.md)       | 2.11.0           | n/a     | 1.13              | 0.12.1              | n/a     |\n[0.12.0](https://github.com/tensorflow/transform/blob/v0.12.0/RELEASE.md)       | 2.10.0           | n/a     | 1.12              | 0.12.0              | n/a     |\n[0.11.0](https://github.com/tensorflow/transform/blob/v0.11.0/RELEASE.md)       | 2.8.0            | n/a     | 1.11              | 0.9.0               | n/a     |\n[0.9.0](https://github.com/tensorflow/transform/blob/v0.9.0/RELEASE.md)         | 2.6.0            | n/a     | 1.9               | 0.9.0               | n/a     |\n[0.8.0](https://github.com/tensorflow/transform/blob/v0.8.0/RELEASE.md)         | 2.5.0            | n/a     | 1.8               | n/a                 | n/a     |\n[0.6.0](https://github.com/tensorflow/transform/blob/v0.6.0/RELEASE.md)         | 2.4.0            | n/a     | 1.6               | n/a                 | n/a     |\n[0.5.0](https://github.com/tensorflow/transform/blob/v0.5.0/RELEASE.md)         | 2.3.0            | n/a     | 1.5               | n/a                 | n/a     |\n[0.4.0](https://github.com/tensorflow/transform/blob/v0.4.0/RELEASE.md)         | 2.2.0            | n/a     | 1.4               | n/a                 | n/a     |\n[0.3.1](https://github.com/tensorflow/transform/blob/v0.3.1/RELEASE.md)         | 2.1.1            | n/a     | 1.3               | n/a                 | n/a     |\n[0.3.0](https://github.com/tensorflow/transform/blob/v0.3.0/RELEASE.md)         | 2.1.1            | n/a     | 1.3               | n/a                 | n/a     |\n[0.1.10](https://github.com/tensorflow/transform/blob/v0.1.10/RELEASE.md)       | 2.0.0            | n/a     | 1.0               | n/a                 | n/a     |\n\n## Questions\n\nPlease direct any questions about working with `tf.Transform` to\n[Stack Overflow](https://stackoverflow.com) using the\n[tensorflow-transform](https://stackoverflow.com/questions/tagged/tensorflow-transform)\ntag.\n", "release_dates": ["2023-08-11T22:59:18Z", "2023-04-13T22:11:06Z", "2022-12-08T21:00:35Z", "2022-11-15T18:47:37Z", "2022-08-29T19:40:51Z", "2022-08-25T21:28:01Z", "2022-06-29T07:22:35Z", "2022-05-13T07:01:01Z", "2022-03-30T18:58:31Z", "2022-03-03T20:13:11Z", "2022-02-23T19:35:29Z", "2022-01-21T18:08:38Z", "2021-12-02T20:53:21Z", "2021-11-05T18:25:35Z", "2021-08-25T19:47:44Z", "2021-07-29T00:07:36Z", "2021-07-16T22:07:49Z", "2021-06-24T20:02:27Z", "2021-05-24T19:27:17Z", "2021-04-26T22:09:52Z", "2021-03-25T17:27:04Z", "2021-02-23T21:27:53Z", "2021-01-27T23:13:00Z", "2020-12-16T21:15:07Z", "2020-11-04T22:48:52Z", "2020-09-24T21:24:54Z", "2020-09-14T20:34:31Z", "2020-08-24T15:46:41Z", "2020-05-13T20:19:37Z", "2020-03-04T21:24:23Z"]}, {"name": "workshops", "description": "A few exercises for use at events.", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": []}]