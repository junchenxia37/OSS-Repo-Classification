[{"name": "ai-and-efficiency", "description": "Submissions for AI and Efficiency SOTA's", "language": null, "license": null, "readme": "# Algorithmic Efficiency SOTA Submissions\nWe found that in 2019 it took [44x less compute](https://openai.com/blog/ai-and-efficiency/) to train a neural net to AlexNet-level performance than in 2012.\n(Moore\u2019s Law would have only yielded an 11x change in cost over this period).\n\nGoing forward, we're going to use this git repository to help publicly track state of the art (SOTA) algorithmic efficiency.\nWe're beginning by tracking training efficiency SOTA's in image recognition and translation at two levels.\n\n#### AlexNet-level performance\n*79.1% top 5 accuracy on ImageNet*\n\n| Publication| Compute(tfs-s/days)| Reduction Factor| Analysis| Date |\n| ----------------------- | ------------- | ------------ | ----------------------- | ------------ |\n| [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)|3.1|1|AI and Efficiency| 6/1/2012|\n| [GoogLeNet](https://arxiv.org/abs/1409.4842)|0.71|4.3|[AI and Efficiency](https://openai.com/blog/ai-and-efficiency/)| 9/17/2014|\n| [MobileNet](https://arxiv.org/abs/1704.04861)|0.28|11|[AI and Efficiency](https://openai.com/blog/ai-and-efficiency/)| 4/17/2017|\n| [ShuffeNet](https://arxiv.org/abs/1707.01083)|0.15|21|[AI and Efficiency](https://openai.com/blog/ai-and-efficiency/)| 7/3/2017|\n| [ShuffleNet_v2](https://arxiv.org/abs/1807.11164)|0.12|25|[AI and Efficiency](https://openai.com/blog/ai-and-efficiency/)| 6/30/2018|\n| [EfficientNet](https://arxiv.org/abs/1905.11946)|0.069|44|[EfficientNet](https://arxiv.org/abs/1905.11946)| 5/28/2019|\n\n#### ResNet-50-level performance\n*92.9% top 5 accuracy on ImageNet*\n\n| Publication| Compute(tfs-s/days)| Reduction Factor| Analysis| Date |\n| ----------------------- | ------------- | ------------ | ----------------------- | ------------ |\n|[ResNet-50](https://arxiv.org/abs/1512.03385)|17|1|[AI and Efficiency](https://openai.com/blog/ai-and-efficiency/)| 1/10/2015|\n|[EfficientNet](https://arxiv.org/abs/1905.11946)|0.75|10|[EfficientNet](https://arxiv.org/abs/1905.11946)| 5/28/2019|\n\n#### Seq2Seq-level Performance\n*34.8 BLEU on WMT-14 EN-FR*\n\n| Publication| Compute(tfs-s/days)| Reduction Factor| Analysis| Date |\n| ----------------------- | ------------- | ------------ | ----------------------- | ------------ |\n[Seq2Seq (Ensemble)](https://arxiv.org/abs/1409.3215)|465|1|[AI and Compute](https://openai.com/blog/ai-and-compute/)| 1/10/2014\n[Transformer(Base)](https://arxiv.org/abs/1706.03762)|8|61|[Attention is all you need](https://arxiv.org/abs/1807.11164)| 1/12/2017\n\n#### GNMT-level performance\n*39.92 BLEU on WMT-14 EN-FR*\n\n| Publication| Compute(tfs-s/days)| Reduction Factor| Analysis| Date |\n| ----------------------- | ------------- | ------------ | ----------------------- | ------------ |\n[GNMT](https://arxiv.org/abs/1609.08144)|1620|1|[Attention is all you need](https://arxiv.org/abs/1807.11164)| 1/26/2016\n[Transformer (Big)](https://arxiv.org/abs/1706.03762)|181|9|[Attention is all you need](https://arxiv.org/abs/1807.11164)| 1/12/2017\n\n## In order to make an entry please submit a pull request in which you:\n1. Make the appropriate update to efficiency_sota.csv\n2. Make the appropriate update to the tables in this file, README.MD\n3. Add the relevant calculations/supporting information to the analysis folder. To get examples of calculations please see\n[AI and Compute](https://openai.com/blog/ai-and-compute/#appendixmethods) and Appendix A and B in [Measuring the Algorithmic Efficiency of Neural Networks](https://arxiv.org/abs/2005.04305).\n\nFAQ\n1. We're interested in tracking progress on additional benchmarks that have been of interest for many years and continue\nto be of interest. Please send thoughts or analysis on such benchmarks to *danny@openai.com.*\n2. ImageNet is the only training data source allowed for the vision benchmark. No human captioning, other images, or other data is allowed. Automated augmentation is ok.\n3. We currently place no restrictions on training data used for translation, but may split results by appropriate categories in the future.\n4. A tf-s/day equals a teraflop/s worth of compute run a day.\n\nTo cite this work please use the following bibtex entry.\n```\n@misc{hernandez2020efficiency\n    title = {Measuring the Algorithmic Efficiency of Neural Networks},\n    author = {Danny Hernandez, Tom B. Brown},\n    year = {2020},\n    eprint={2005.04305},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG},\n}\n```\n", "release_dates": []}, {"name": "assign-one-project-github-action", "description": "Automatically add an issue or pull request to specific GitHub Project when you create them.", "language": "Shell", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# GitHub Action for Assign to One Project\n\n[![Docker Cloud Automated build](https://img.shields.io/docker/cloud/automated/srggrs/assign-one-project-github-action)][docker]\n[![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/srggrs/assign-one-project-github-action)][docker]\n[![Docker Pulls](https://img.shields.io/docker/pulls/srggrs/assign-one-project-github-action)][docker]\n[![Docker Stars](https://img.shields.io/docker/stars/srggrs/assign-one-project-github-action)][docker]\n[![GitHub license](https://img.shields.io/github/license/srggrs/assign-one-project-github-action.svg)][license]\n\n[docker]: https://hub.docker.com/r/srggrs/assign-one-project-github-action\n[license]: https://github.com/srggrs/assign-one-project-github-action/blob/master/LICENSE\n\nAutomatically add an issue or pull request to specific [GitHub Project](https://help.github.com/articles/about-project-boards/) when you __create__ and/or __label__ them. By default, the issues are assigned to the `To do` column and the pull requests to the `In progress` one, so make sure you have those columns in your project dashboard. But the workflow allowed you to specify the column name as input, so you can assign the issues/PRs based on a set of conditions to a specific column of a specific project.\n\n## Acknowledgment & Motivations\n\nThis action has been modified from the original action from [masutaka](https://github.com/masutaka/github-actions-all-in-one-project). I needed to fix it as the original docker container would not build. Also I think the GitHub Action syntax changed a bit.\n\nI would like to thank @SunRunAway for adding the labelling functionality and custom column input.\n\n## Inputs\n\n### `project`\n\n**Required** The url of the project to be assigned to.\n\n### `column_name`\n\nThe column name of the project, defaults to `'To do'` for issues and `'In progress'` for pull requests.\n\n## Example usage\n\nExamples of action:\n\n### Repository project\n\n```yaml\nname: Auto Assign to Project(s)\n\non:\n  issues:\n    types: [opened, labeled]\n  pull_request:\n    types: [opened, labeled]\nenv:\n  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\njobs:\n  assign_one_project:\n    runs-on: ubuntu-latest\n    name: Assign to One Project\n    steps:\n    - name: Assign NEW issues and NEW pull requests to project 2\n      uses: srggrs/assign-one-project-github-action@1.2.0\n      if: github.event.action == 'opened'\n      with:\n        project: 'https://github.com/srggrs/assign-one-project-github-action/projects/2'\n\n    - name: Assign issues and pull requests with `bug` label to project 3\n      uses: srggrs/assign-one-project-github-action@1.2.0\n      if: |\n        contains(github.event.issue.labels.*.name, 'bug') ||\n        contains(github.event.pull_request.labels.*.name, 'bug')\n      with:\n        project: 'https://github.com/srggrs/assign-one-project-github-action/projects/3'\n        column_name: 'Labeled'\n```\n\n#### __Notes__\nBe careful of using the coditions above (opened and labeled issues/PRs) because in such workflow, if the issue/PR is opened and labeled at the same time, it will be assigned to __both__ projects!\n\n\nYou can use any combination of conditions. For example, to assign new issues or issues labeled with 'mylabel' to a project column, use:\n```yaml\n...\n\nif: |\n  github.event == 'issue' &&\n  (\n    github.event.action == 'opened' ||\n    contains(github.event.issue.labels.*.name, 'mylabel')\n  )\n...\n```\n\n### Organisation or User project\n\nGenerate a token from the Organisation settings or User Settings and add it as a secret in the repository secrets as `MY_GITHUB_TOKEN`\n\n```yaml\nname: Auto Assign to Project(s)\n\non:\n  issues:\n    types: [opened, labeled]\n  pull_request:\n    types: [opened, labeled]\nenv:\n  MY_GITHUB_TOKEN: ${{ secrets.MY_GITHUB_TOKEN }}\n\njobs:\n  assign_one_project:\n    runs-on: ubuntu-latest\n    name: Assign to One Project\n    steps:\n    - name: Assign NEW issues and NEW pull requests to project 2\n      uses: srggrs/assign-one-project-github-action@1.2.0\n      if: github.event.action == 'opened'\n      with:\n        project: 'https://github.com/srggrs/assign-one-project-github-action/projects/2'\n\n    - name: Assign issues and pull requests with `bug` label to project 3\n      uses: srggrs/assign-one-project-github-action@1.2.0\n      if: |\n        contains(github.event.issue.labels.*.name, 'bug') ||\n        contains(github.event.pull_request.labels.*.name, 'bug')\n      with:\n        project: 'https://github.com/srggrs/assign-one-project-github-action/projects/3'\n        column_name: 'Labeled'\n```\n", "release_dates": ["2020-07-13T22:07:47Z"]}, {"name": "atari-demo", "description": "Code for the blog post \"Learning Montezuma\u2019s Revenge from a Single Demonstration\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# atari-demo\nRecord demonstrations for atari.\n\nUse as `pythonw record_demo.py --game='MontezumaRevenge'`\n\nRequires pygame (`pip install pygame`)\n", "release_dates": []}, {"name": "atari-py", "description": "A packaged and slightly-modified version of https://github.com/bbitmaster/ale_python_interface", "language": "C++", "license": {"key": "gpl-2.0", "name": "GNU General Public License v2.0", "spdx_id": "GPL-2.0", "url": "https://api.github.com/licenses/gpl-2.0", "node_id": "MDc6TGljZW5zZTg="}, "readme": "**Status:** Deprecated (don't expect bug fixes or other updates)\n\nNotice: `atari-py` is fully deprecated and no future updates, bug fixes or releases will be made.\nPlease use the official [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment) Python package (`ale-py`) instead;\nit is fully backwards compatible with all `atari-py` code.\n\n\n# atari_py\n\n[![Build Status](https://travis-ci.org/openai/atari-py.svg?branch=master)](https://travis-ci.org/openai/atari-py)\n\nA packaged and slightly-modified version of [https://github.com/bbitmaster/ale_python_interface](https://github.com/bbitmaster/ale_python_interface).\n\n## Supported Systems\n\natari-py supports Linux and Mac OS X with Python 3.5, 3.6, and 3.7.  Binaries for those platforms are published to [PyPI](https://pypi.org/project/atari-py/)\n\nWe also have binaries for Windows, but compiling from source on Windows or using the binaries is not officially supported.\n\n## Installation\n\nTo install via pip, run:\n\n```pip install atari-py```\nThat *should* install a correct binary verison for your OS. If that does not work (or if you would like get the latest-latest\nversion, or you just want to tinker with the code yourself) see next paragraph. \n\n## ROMs\n\nIn order to import ROMS, you need to download `Roms.rar` from the [Atari 2600 VCS ROM Collection](http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html) and extract the `.rar` file.  Once you've done that, run:\n\n`python -m atari_py.import_roms <path to folder>`\n\nThis should print out the names of ROMs as it imports them.  The ROMs will be copied to your `atari_py` installation directory.\n\n## Installation from source\n\n  -  make sure you have `git`, `cmake`, `zlib1g`, and, on Linux, `zlib1g-dev` system packages installed \n  -  clone the repo\n  -  run `pip install -e .`\n\n## Included ROMs\n\nThe following non-commercial ROMs are included with atari-py for testing purposes:\n\n* Tetris26 by Colin Hughes\n", "release_dates": []}, {"name": "atari-reset", "description": "Code for the blog post \"Learning Montezuma\u2019s Revenge from a Single Demonstration\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Learn RL policies on Atari by resetting from a demonstration\n\nCodebase for learning to play Atari from demonstrations. Contrary to other work on learning from demonstrations we learn to maximize the score using pure RL, rather than trying to imitate the demo.\n\nAll learning is done through RL on the regular Atari environments, but we automatically build a curriculum for our agent by starting the rollouts from points in a demonstration provided by a human expert: We start by having each RL episode begin near the end of the demonstration. Once the agent is able to beat or at least tie the score of the demonstrator on the remaining part of the game, in at least 20\\% of the rollouts, we slowly move the starting point back in time. We keep doing this until the agent is playing from the start of the game, without using the demo at all, at which point we have an RL-trained agent beating or tying the human expert on the entire game.\n\n![](graphics/montezuma.png)\n\n*Impression of our agent learning to reach the first key in Montezuma\u2019s Revenge using RL and starting each episode from a demonstration state. When our agent starts playing the game, we place it right in front of the key, requiring it to only take a single jump to find success. After our agent has learned to do this consistently, we slowly move the starting point back in time. Our agent might then find itself halfway up the ladder that leads to the key. Once it learns to climb the ladder from there, we can have it start at the point where it needs to jump over the skull. After it learns to do that, we can have it start on the rope leading to the floor of the room, etc. Eventually, the agent starts in the original starting state of the game and is able to reach the key completely by itself.*\n\n# Replaying demo transitions\n\nWhen resetting to a state from the demonstration and when using recurrent policies, we need to make sure that the hidden state of the agent accurately reflects the recent game history: simply resetting the state to zero is not sufficient. At every episode we therefore recompute the hidden state from the last few transitions in the demonstration preceding the selected starting state.\n\n# PPO implementation\n\nOur PPO implementation is derived from the one in [OpenAI Baselines](https://github.com/openai/baselines). We use [generalized advantage estimation](https://arxiv.org/abs/1506.02438v5) with a lambda of 0.95 and gamma between 0.999 and 0.9999. For every minibatch we process during training we recompute the hidden state of our policy at the start of that minibatch, rather than just using the value we had computed using the previous set of parameters: effectively this comes down to using a larger minibatch in the time dimension, and throwing away the first part of the batch when calculating the value loss and policy loss.\n\n# How to use\n\nTraining is performed using the `train_atari.py` script. The code uses MPI (using [Horovod](https://github.com/uber/horovod)) for distributed training. We recommend running on at least 8 GPUs, preferably more: we used 128 for Montezuma. The default hyperparameter settings work well for Montezuma. When training stops making progress you should lower the learning rate and entropy coefficient to help the agent get unstuck.\n\n# Results\n\nSo far we have been able to train an agent to achieve a high score of 74,500 on Montezuma's Revenge from a single demonstration, better than any previously published result. The resulting policy is reasonably robust, achieving a score of 10,000 when evaluating with [sticky frames](https://arxiv.org/abs/1709.06009v2) and 8,400 with epsilon greedy noise where epsilon=0.01, also the best published so far.\n\n<p align=\"center\">\n  <a href=\"https://www.dropbox.com/s/3dc6i9d41svkgpz/monte_video.mp4?dl=1\" target=\"_blank\"><img src=\"graphics/video_screen.png\" \nalt=\"Our agent playing Montezuma\u2019s Revenge\" width=\"500\" border=\"10\" /></a>\n</p>\n\n*Our agent playing Montezuma\u2019s Revenge. The agent achieves a final score of 74,500 over approximately 12 minutes of play (video is double speed). Although much of the agent\u2019s game mirrors the demonstration, the agent surpasses the demonstration score of 71,500 by picking up more diamonds along the way. In addition, the agent learns to exploit a flaw in the emulator to make a key re-appear at minute 4:25 of the video, something not present in the demonstration.*\n\nThe trained model for Montezuma's Revenge can be downloaded [here](https://www.dropbox.com/s/nr4jmjcpsrrsgwf/003100?dl=1).\n\n# Remaining challenges\n\nThe algorithm is still fragile: some runs don't converge for Montezuma's Revenge, and the one that did converge required running at large scale, with a restart from a checkpoint halfway. We have not yet been able to match expert performance on Gravitar and Pitfall.\n\n# The demos\n\nThe repo includes demos for Montezuma's Revenge, PrivateEye, Pitfall, Gravitar, and Pong. These demonstrations were obtained by playing tool-assisted using [this code](https://github.com/openai/atari-demo).\n\n# Related work\n\nOur main insight is that we can make our RL problem easier to solve by decomposing it into a curriculum of subtasks requiring short action sequences; we construct this curriculum by starting each RL episode from a demonstration state. A variant of the same idea was used recently for [reverse curriculum generation for robotics](http://bair.berkeley.edu/blog/2017/12/20/reverse-curriculum/), where a curriculum was constructed by iteratively perturbing a set of starting states using random actions, and selecting the resulting states with the right level of difficulty.\n\nStarting episodes by resetting from demonstration states was [previously proposed](https://arxiv.org/abs/1607.05077v1), but without constructing a curriculum that gradually moves the starting state back from the end of the demonstration to the beginning. When combined with imitation learning, [several](https://arxiv.org/abs/1804.02717) [researchers](https://arxiv.org/abs/1709.10089) report benefit from this approach. For our use case we found such a curriculum to be vitally important for deriving benefit from the demonstration.\n\nRecently, DeepMind has shown an agent learning Montezuma's Revenge by imitation learning from a demonstration; [one approach](https://arxiv.org/abs/1805.11592v1) trains an agent to achieve the same states seen in a YouTube video of Montezuma's Revenge, and [another technique](https://arxiv.org/abs/1805.11593v1) combines a sophisticated version of Q-learning with maximizing the likelihood of actions taken in a demonstration. The advantage of these approaches is that they do not require as much control over the environment our technique does: they do not reset the environment to states other than the starting state of the game, and they do not presume access to the full game states encountered in the demonstration. Our method differs by directly optimizing what we care about, the game score, rather than making the agent imitate the demonstration; our method does not have the problem of overfitting to a sub-optimal demonstration and could offer benefits in multi-player games where we want to optimize performance against other opponents than just the one from the demonstration.\n", "release_dates": []}, {"name": "automated-interpretability", "description": null, "language": "Python", "license": null, "readme": "# Automated interpretability\n\n## Code and tools\n\nThis repository contains code and tools associated with the [Language models can explain neurons in\nlanguage models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html) paper, specifically:\n\n* Code for automatically generating, simulating, and scoring explanations of neuron behavior using\nthe methodology described in the paper. See the\n[neuron-explainer README](neuron-explainer/README.md) for more information.\n\nNote: if you run into errors of the form \"Error: Could not find any credentials that grant access to storage account: 'openaipublic' and container: 'neuron-explainer'\".\" you might be able to fix this by signing up for an azure account and specifying the credentials as described in the error message. \n\n* A tool for viewing neuron activations and explanations, accessible\n[here](https://openaipublic.blob.core.windows.net/neuron-explainer/neuron-viewer/index.html). See\nthe [neuron-viewer README](neuron-viewer/README.md) for more information.\n\n## Public datasets\n\nTogether with this code, we're also releasing public datasets of GPT-2 XL neurons and explanations.\nHere's an overview of those datasets.  \n\n* Neuron activations: `az://openaipublic/neuron-explainer/data/collated-activations/{layer_index}/{neuron_index}.json`\n    - Tokenized text sequences and their activations for the neuron. We\n    provide multiple sets of tokens and activations: top-activating ones, random\n    samples from several quantiles; and a completely random sample. We also provide\n    some basic statistics for the activations.\n    - Each file contains a JSON-formatted\n    [`NeuronRecord`](neuron-explainer/neuron_explainer/activations/activations.py#L89) dataclass.\n* Neuron explanations: `az://openaipublic/neuron-explainer/data/explanations/{layer_index}/{neuron_index}.jsonl`\n    - Scored model-generated explanations of the behavior of the neuron, including simulation results.\n    - Each file contains a JSON-formatted\n    [`NeuronSimulationResults`](neuron-explainer/neuron_explainer/explanations/explanations.py#L146)\n    dataclass.\n* Related neurons: `az://openaipublic/neuron-explainer/data/related-neurons/weight-based/{layer_index}/{neuron_index}.json`\n    - Lists of the upstream and downstream neurons with the most positive and negative connections (see below for definition).\n    - Each file contains a JSON-formatted dataclass whose definition is not included in this repo.\n* Tokens with high average activations:\n`az://openaipublic/neuron-explainer/data/related-tokens/activation-based/{layer_index}/{neuron_index}.json`\n    - Lists of tokens with the highest average activations for individual neurons, and their average activations.\n    - Each file contains a JSON-formatted [`TokenLookupTableSummaryOfNeuron`](neuron-explainer/neuron_explainer/activations/token_connections.py#L36)\n    dataclass.\n* Tokens with large inbound and outbound weights:\n`az://openaipublic/neuron-explainer/data/related-tokens/weight-based/{layer_index}/{neuron_index}.json`\n    - List of the most-positive and most-negative input and output tokens for individual neurons,\n    as well as the associated weight (see below for definition). \n    - Each file contains a JSON-formatted [`WeightBasedSummaryOfNeuron`](neuron-explainer/neuron_explainer/activations/token_connections.py#L17)\n    dataclass.\n\nUpdate (July 5, 2023):\nWe also released a set of explanations for GPT-2 Small. The methodology is slightly different from the methodology used for GPT-2 XL so the results aren't directly comparable.\n* Neuron activations: `az://openaipublic/neuron-explainer/gpt2_small_data/collated-activations/{layer_index}/{neuron_index}.json`\n* Neuron explanations: `az://openaipublic/neuron-explainer/gpt2_small_data/explanations/{layer_index}/{neuron_index}.jsonl`\n\nUpdate (August 30, 2023): We recently discovered a bug in how we performed inference on the GPT-2 series models used for the paper and for these datasets. Specifically, we used an optimized GELU implementation rather than the original GELU implementation associated with GPT-2. While the model\u2019s behavior is very similar across these two configurations, the post-MLP activation values we used to generate and simulate explanations differ from the correct values by the following amounts for GPT-2 small:\n\n- Median: 0.0090\n- 90th percentile: 0.0252\n- 99th percentile: 0.0839\n- 99.9th percentile: 0.1736\n\n### Definition of connection weights\n\nRefer to [GPT-2 model code](https://github.com/openai/gpt-2/blob/master/src/model.py) for\nunderstanding of model weight conventions.\n\n*Neuron-neuron*: For two neurons `(l1, n1)` and `(l2, n2)` with `l1 < l2`, the connection strength is defined as\n`h{l1}.mlp.c_proj.w[:, n1, :] @ diag(h{l2}.ln_2.g) @ h{l2}.mlp.c_fc.w[:, :, n2]`.\n\n*Neuron-token*: For token `t` and neuron `(l, n)`, the input weight is computed as\n`wte[t, :] @ diag(h{l}.ln_2.g) @ h{l}.mlp.c_fc.w[:, :, n]`\nand the output weight is computed as\n`h{l}.mlp.c_proj.w[:, n, :] @ diag(ln_f.g) @ wte[t, :]`.\n\n### Misc Lists of Interesting Neurons\nLists of neurons we thought were interesting according to different criteria, with some preliminary descriptions.\n* [Interesting Neurons (external)](https://docs.google.com/spreadsheets/d/1p7fYs31NU8sJoeKyUx4Mn2laGx8xXfHg_KcIvYiKPpg/edit#gid=0)\n* [Neurons that score high on random, possibly monosemantic? (external)](https://docs.google.com/spreadsheets/d/1TqKFcz-84jyIHLU7VRoTc8BoFBMpbgac-iNBnxVurQ8/edit?usp=sharing)\n* [Clusters of neurons well explained by activation explanation but not by tokens](https://docs.google.com/document/d/1lWhKowpKDdwTMALD_K541cdwgGoQx8DFUSuEe1U2AGE/edit?usp=sharing)\n* [Neurons sensitive to truncation](https://docs.google.com/document/d/1x89TWBvuHcyC2t01EDbJZJ5LQYHozlcS-VUmr5shf_A/edit?usp=sharing)\n", "release_dates": []}, {"name": "aws-fluent-plugin-kinesis", "description": "Fluentd output plugin that sends events to Amazon Kinesis Streams and Amazon Kinesis Firehose.", "language": "Ruby", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Fluent plugin for Amazon Kinesis\n\n[![Build Status](https://travis-ci.org/awslabs/aws-fluent-plugin-kinesis.svg?branch=master)](https://travis-ci.org/awslabs/aws-fluent-plugin-kinesis)\n\n[Fluentd][fluentd] output plugin\nthat sends events to [Amazon Kinesis Streams][streams] (via both API and [Kinesis Producer Library (KPL)][producer]) and [Amazon Kinesis Firehose][firehose] (via API). This gem includes three output plugins respectively:\n\n- `kinesis_streams`\n- `kinesis_producer`\n- `kinesis_firehose`\n\nAlso, there is a [documentation on Fluentd official site][fluentd-doc-kinesis].\n\n## Warning: `kinesis` is no longer supported\nAs of v1.0.0, `kinesis` plugin is no longer supported. Still you can use the plugin, but if you see the warn log below, please consider to use `kinesis_streams`.\n\n    [warn]: Deprecated warning: out_kinesis is no longer supported after v1.0.0. Please check out_kinesis_streams out.\n\nIf you still want to use `kinesis`, please see [the old README][old-readme].\n\n## Installation\nThis fluentd plugin is available as the `fluent-plugin-kinesis` gem from RubyGems.\n\n    gem install fluent-plugin-kinesis\n\nOr you can install this plugin for [td-agent][td-agent] as:\n\n    td-agent-gem install fluent-plugin-kinesis\n\nIf you would like to build by yourself and install, please see the section below. Your need [bundler][bundler] for this.\n\nIn case of using with Fluentd: Fluentd will be also installed via the process below.\n\n    git clone https://github.com/awslabs/aws-fluent-plugin-kinesis.git\n    cd aws-fluent-plugin-kinesis\n    bundle install\n    bundle exec rake build\n    bundle exec rake install\n\nAlso, you can use this plugin with td-agent: You have to install td-agent before installing this plugin.\n\n    git clone https://github.com/awslabs/aws-fluent-plugin-kinesis.git\n    cd aws-fluent-plugin-kinesis\n    bundle install\n    bundle exec rake build\n    fluent-gem install pkg/fluent-plugin-kinesis\n\nOr just download specify your Ruby library path. Below is the sample for specifying your library path via RUBYLIB.\n\n    git clone https://github.com/awslabs/aws-fluent-plugin-kinesis.git\n    cd aws-fluent-plugin-kinesis\n    bundle install\n    export RUBYLIB=$RUBYLIB:/path/to/aws-fluent-plugin-kinesis/lib\n\n## Dependencies\n * Ruby 2.0.0+\n * Fluentd 0.10.58+\n\n## Basic Usage\nHere are general procedures for using this plugin:\n\n 1. Install.\n 1. Edit configuration\n 1. Run Fluentd or td-agent\n\nYou can run this plugin with Fluentd as follows:\n\n 1. Install.\n 1. Edit configuration file and save it as 'fluentd.conf'.\n 1. Then, run `fluentd -c /path/to/fluentd.conf`\n\nTo run with td-agent, it would be as follows:\n\n 1. Install.\n 1. Edit configuration file provided by td-agent.\n 1. Then, run or restart td-agent.\n\n## Getting started\nAssume you use Amazon EC2 instances with Instance profile. If you want to use specific credentials, see [Credentials](#configuration-credentials).\n\n### kinesis_streams\n    <match your_tag>\n      @type kinesis_streams\n      region us-east-1\n      stream_name your_stream\n      partition_key key  # Otherwise, use random partition key\n    </match>\nFor more detail, see [Configuration: kinesis_streams](#configuration-kinesis_streams)\n\n### kinesis_producer\n    <match your_tag>\n      @type kinesis_producer\n      region us-east-1\n      stream_name your_stream\n      partition_key key  # Otherwise, use random partition key\n    </match>\nFor more detail, see [Configuration: kinesis_producer](#configuration-kinesis_producer)\n\n### kinesis_firehose\n    <match your_tag>\n      @type kinesis_firehose\n      region us-east-1\n      delivery_stream_name your_stream\n    </match>\nFor more detail, see [Configuration: kinesis_firehose](#configuration-kinesis_firehose)\n\n### For better throughput\nAdd configuration like below:\n\n      flush_interval 1\n      buffer_chunk_limit 1m\n      try_flush_interval 0.1\n      queued_chunk_flush_interval 0.01\n      num_threads 15\n      detach_process 5\n\nNote: Each value should be adjusted to your system by yourself.\n\n## Configuration: Credentials\nTo put records into Amazon Kinesis Streams or Firehose, you need to provide AWS security credentials.\n\nThe credential provider will be choosen by the steps below:\n\n- Use [**shared_credentials**](#shared_credentials) section if you set it\n- Use [**assume_role_credentials**](#assume_role_credentials) section if you set it\n- Otherwise, default provicder chain:\n    - [**aws_key_id**](#aws_key_id) and [**aws_sec_key**](#aws_sec_key)\n    - Environment variables (ex. `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, etc.)\n    - Default shared credentials (`default` in `~/.aws/credentials`)\n    - Instance profile (For Amazon EC2)\n\n### aws_key_id\nAWS access key id.\n\n### aws_sec_key\nAWS secret access key.\n\n### shared_credentials\nUse this config section to specify shared credential file path and profile name. If you want to use default profile (`default` in `~/.aws/credentials`), you don't have to specify here.\n\n#### profile_name\nProfile name of the credential file.\n\n#### path\nPath for the credential file.\n\n### assume_role_credentials\nUse this config section for cross account access.\n\n#### role_arn\nIAM Role to be assumed with [AssumeRole][assume_role].\n\n#### external_id\nA unique identifier that is used by third parties when [assuming roles][assmue_role] in their customers' accounts. Use this option with `role_arn` for third party cross account access. For detail, please see [How to Use an External ID When Granting Access to Your AWS Resources to a Third Party][external_id].\n\n## Configuraion: Format\nThis plugin use `Fluent::TextFormatter` to serialize record to string. For more detail, see [formatter.rb]. Also, this plugin includes `Fluent::SetTimeKeyMixin` and `Fluent::SetTagKeyMixin` to use **include_time_key** and **include_tagkey**.\n\n### formatter\nDefault `json`.\n\n### include_time_key\nDefalut `false`. If you want to include `time` field in your record, set `true`.\n\n### include_tag_key\nDefalut `false`. If you want to include `tag` field in your record, set `true`.\n\n### data_key\nIf your record contains a field whose string should be sent to Amazon Kinesis directly (without formatter), use this parameter to specify the field. In that case, other fileds than **data_key** are thrown away and never sent to Amazon Kinesis. Default `nil`, which means whole record will be formatted and sent.\n\n### log_truncate_max_size\nInteger, default 0. When emitting the log entry, the message will be truncated by this size to avoid infinite loop when the log is also sent to Kinesis. The value 0 (default) means no truncation.\n\n## Configuration: kinesis_streams\nHere are `kinesis_streams` specific configurations.\n\n### stream_name\nName of the stream to put data.\n\n### region\nAWS region of your stream. It should be in form like `us-east-1`, `us-west-2`. Refer to [Regions and Endpoints in AWS General Reference][region] for supported regions.\n\nDefault `nil`, which means try to find from environment variable `AWS_REGION`.\n\n### partition_key\nA key to extract partition key from JSON object. Default `nil`, which means partition key will be generated randomly.\n\n### retries_on_batch_request\nInteger, default is 3. The plugin will put multiple records to Amazon Kinesis Streams in batches using PutRecords. A set of records in a batch may fail for reasons documented in the Kinesis Service API Reference for PutRecords. Failed records will be retried **retries_on_batch_request** times. If a record fails all retries an error log will be emitted.\n\n### reset_backoff_if_success\nBoolean, default `true`. If enabled, when after retrying, the next retrying checks the number of succeeded records on the former batch request and reset exponential backoff if there is any success. Because batch request could be composed by requests across shards, simple exponential backoff for the batch request wouldn't work some cases.\n\n### batch_request_max_count\nInteger, default 500. The number of max count of making batch request from record chunk. It can't exceed the default value because it's API limit.\n\n### batch_request_max_size\nInteger, default 5 * 1024*1024. The number of max size of making batch request from record chunk. It can't exceed the default value because it's API limit.\n\n### http_proxy\nHTTP proxy for API calling. Default `nil`.\n\n### endpoint\nAPI endpoint URL, for testing. Defalut `nil`.\n\n### ssl_verify_peer\nBoolean. Disable if you want to verify ssl conncetion, for testing. Default `true`.\n\n### debug\nBoolean. Enable if you need to debug Amazon Kinesis Firehose API call. Default is `false`.\n\n## Configuration: kinesis_producer\nHere are `kinesis_producer` specific configurations.\n\n### stream_name\nName of the stream to put data.\n\n### region\nAWS region of your stream. It should be in form like `us-east-1`, `us-west-2`. Refer to [Regions and Endpoints in AWS General Reference][region] for supported regions.\n\nDefault `nil`, which means try to find from environment variable `AWS_REGION`. If both **region** and `AWS_REGION` are not defined, KPL will try to find region from Amazon EC2 metadata.\n\n### partition_key\nA key to extract partition key from JSON object. Default `nil`, which means partition key will be generated randomly.\n\n### debug\nBoolean. Enable if you need to debug Kinesis Producer Library metrics. Default is `false`.\n\n### kinesis_producer\nThis section is configuration for Kinesis Producer Library. Almost all of description comes from [deault_config.propertites of KPL Java Sample Application][default_config.properties].\n\n#### aggregation_enabled\nEnable aggregation. With aggregation, multiple user records are packed into a single KinesisRecord. If disabled, each user record is sent in its own KinesisRecord.\n\nIf your records are small, enabling aggregation will allow you to put many more records than you would otherwise be able to for a shard before getting throttled.\n\nDefault: `true`\n\n#### aggregation_max_count\nMaximum number of items to pack into an aggregated record.\n\nThere should be normally no need to adjust this. If you want to limit the time records spend buffering, look into record_max_buffered_time instead.\n\nDefault: 4294967295\nMinimum: 1\nMaximum (inclusive): 9223372036854775807\n\n#### aggregation_max_size\nMaximum number of bytes to pack into an aggregated Kinesis record.\n\nThere should be normally no need to adjust this. If you want to limit the time records spend buffering, look into [**record_max_buffered_time**](#record_max_buffered_time) instead.\n\nIf a record has more data by itself than this limit, it will bypass the aggregator. Note the backend enforces a limit of 50KB on record size. If you set this beyond 50KB, oversize records will be rejected at the backend.\n\nDefault: 51200\nMinimum: 64\nMaximum (inclusive): 1048576\n\n#### collection_max_count\nMaximum number of items to pack into an PutRecords request.\n\nThere should be normally no need to adjust this. If you want to limit the time records spend buffering, look into [**record_max_buffered_time**](#record_max_buffered_time) instead.\n\nDefault: 500\nMinimum: 1\nMaximum (inclusive): 500\n\n#### collection_max_size\nMaximum amount of data to send with a PutRecords request.\n\nThere should be normally no need to adjust this. If you want to limit the time records spend buffering, look into [**record_max_buffered_time**](#record_max_buffered_time) instead.\n\nRecords larger than the limit will still be sent, but will not be grouped with others.\n\nDefault: 5242880\nMinimum: 52224\nMaximum (inclusive): 9223372036854775807\n\n#### connect_timeout\nTimeout (milliseconds) for establishing TLS connections.\n\nDefault: 6000\nMinimum: 100\nMaximum (inclusive): 300000\n\n#### custom_endpoint\nUse a custom Kinesis and CloudWatch endpoint.\n\nMostly for testing use. Note this does not accept protocols or paths, only host names or ip addresses. There is no way to disable TLS. The KPL always connects with TLS.\n\nExpected pattern: `^([A-Za-z0-9-\\\\.]+)?$`\n\n#### fail_if_throttled\nIf `true`, throttled puts are not retried. The records that got throttled will be failed immediately upon receiving the throttling error. This is useful if you want to react immediately to any throttling without waiting for the KPL to retry. For example, you can use a different hash key to send the throttled record to a backup shard.\n\nIf `false`, the KPL will automatically retry throttled puts. The KPL performs backoff for shards that it has received throttling errors from, and will avoid flooding them with retries. Note that records may fail from expiration (see [**record_ttl**](#record_ttl)) if they get delayed for too long because of\nthrottling.\n\nDefault: `false`\n\n#### log_level\nMinimum level of logs. Messages below the specified level will not be logged. Logs for the native KPL daemon show up on stderr.\n\nDefault: `info`\nExpected pattern: `info|warning|error`\n\n#### max_connections\nMaximum number of connections to open to the backend. HTTP requests are sent in parallel over multiple connections.\n\nSetting this too high may impact latency and consume additional resources without increasing throughput.\n\nDefault: 4\nMinimum: 1\nMaximum (inclusive): 128\n\n#### metrics_granularity\nControls the granularity of metrics that are uploaded to CloudWatch. Greater granularity produces more metrics.\n\nWhen `shard` is selected, metrics are emitted with the stream name and shard id as dimensions. On top of this, the same metric is also emitted with only the stream name dimension, and lastly, without the stream name. This means for a particular metric, 2 streams with 2 shards (each) will produce 7 CloudWatch metrics, one for each shard, one for each stream, and one overall, all describing the same statistics, but at different levels of granularity.\n\nWhen `stream` is selected, per shard metrics are not uploaded; when `global` is selected, only the total aggregate for all streams and all shards are uploaded.\n\nConsider reducing the granularity if you're not interested in shard-level metrics, or if you have a large number of shards.\n\nIf you only have 1 stream, select `global`; the global data will be equivalent to that for the stream.\n\nRefer to the metrics documentation for details about each metric.\n\nDefault: `shard`\nExpected pattern: `global|stream|shard`\n\n#### metrics_level\nControls the number of metrics that are uploaded to CloudWatch.\n\n`none` disables all metrics.\n\n`summary` enables the following metrics: UserRecordsPut, KinesisRecordsPut, ErrorsByCode, AllErrors, BufferingTime.\n\n`detailed` enables all remaining metrics.\n\nRefer to the metrics documentation for details about each metric.\n\nDefault: `detailed`\nExpected pattern: `none|summary|detailed`\n\n#### metrics_namespace\nThe namespace to upload metrics under.\n\nIf you have multiple applications running the KPL under the same AWS account, you should use a different namespace for each application.\n\nIf you are also using the KCL, you may wish to use the application name you have configured for the KCL as the the namespace here. This way both your KPL and KCL metrics show up under the same namespace.\n\nDefault: `KinesisProducerLibrary`\nExpected pattern: `(?!AWS/).{1,255}`\n\n#### metrics_upload_delay\nDelay (in milliseconds) between each metrics upload.\n\nFor testing only. There is no benefit in setting this lower or higher in production.\n\nDefault: 60000\nMinimum: 1\nMaximum (inclusive): 60000\n\n#### min_connections\nMinimum number of connections to keep open to the backend.\n\nThere should be no need to increase this in general.\n\nDefault: 1\nMinimum: 1\nMaximum (inclusive): 16\n\n#### port\nServer port to connect to. Only useful with [**custom_endpoint**](#custom_endpoint).\n\nDefault: 443\nMinimum: 1\nMaximum (inclusive): 65535\n\n#### rate_limit\nLimits the maximum allowed put rate for a shard, as a percentage of the backend limits.\n\nThe rate limit prevents the producer from sending data too fast to a shard. Such a limit is useful for reducing bandwidth and CPU cycle wastage from sending requests that we know are going to fail from throttling.\n\nKinesis enforces limits on both the number of records and number of bytes per second. This setting applies to both.\n\nThe default value of 150% is chosen to allow a single producer instance to completely saturate the allowance for a shard. This is an aggressive setting. If you prefer to reduce throttling errors rather than completely saturate the shard, consider reducing this setting.\n\nDefault: 150\nMinimum: 1\nMaximum (inclusive): 9223372036854775807\n\n#### record_max_buffered_time\nMaximum amount of itme (milliseconds) a record may spend being buffered before it gets sent. Records may be sent sooner than this depending on the other buffering limits.\n\nThis setting provides coarse ordering among records - any two records will be reordered by no more than twice this amount (assuming no failures and retries and equal network latency).\n\nThe library makes a best effort to enforce this time, but cannot guarantee that it will be precisely met. In general, if the CPU is not overloaded, the library will meet this deadline to within 10ms.\n\nFailures and retries can additionally increase the amount of time records spend in the KPL. If your application cannot tolerate late records, use the [**record_ttl**](#record_ttl) setting to drop records that do not get transmitted in time.\n\nSetting this too low can negatively impact throughput.\n\nDefault: 100\nMaximum (inclusive): 9223372036854775807\n\n#### record_ttl\nSet a time-to-live on records (milliseconds). Records that do not get successfully put within the limit are failed.\n\nThis setting is useful if your application cannot or does not wish to tolerate late records. Records will still incur network latency after they leave the KPL, so take that into consideration when choosing a value for this setting.\n\nIf you do not wish to lose records and prefer to retry indefinitely, set record_ttl to a large value like INT_MAX. This has the potential to cause head-of-line blocking if network issues or throttling occur. You can respond to such situations by using the metrics reporting functions of the KPL. You may also set [**fail_if_throttled**](#fail_if_throttled) to true to prevent automatic retries in case of throttling.\n\nDefault: 30000\nMinimum: 100\nMaximum (inclusive): 9223372036854775807\n\n#### request_timeout\nThe maximum total time (milliseconds) elapsed between when we begin a HTTP request and receiving all of the response. If it goes over, the request will be timed-out.\n\nNote that a timed-out request may actually succeed at the backend. Retrying then leads to duplicates. Setting the timeout too low will therefore increase the probability of duplicates.\n\nDefault: 6000\nMinimum: 100\nMaximum (inclusive): 600000\n\n#### verify_certificate\nVerify the endpoint's certificate. Do not disable unless using [**custom_endpoint**](#custom_endpoint) for testing. Never disable this in production.\n\nDefault: `true`\n\n#### credentials_refresh_delay\nInterval milliseconds for refreshing credentials seding to KPL.\n\nDefalut 5000\n\n## Configuration: kinesis_firehose\nHere are `kinesis_firehose` specific configurations.\n\n### delivery_stream_name\nName of the delivery stream to put data.\n\n### region\nAWS region of your stream. It should be in form like `us-east-1`, `us-west-2`. Refer to [Regions and Endpoints in AWS General Reference][region] for supported regions.\n\nDefault `nil`, which means try to find from environment variable `AWS_REGION`.\n\n### append_new_line\nBoolean. Default `true`. If it is enabled, the plugin add new line character (`\\n`) to each serialized record.  \n\n### retries_on_batch_request\nInteger, default is 3. The plugin will put multiple records to Amazon Kinesis Firehose in batches using PutRecordBatch. A set of records in a batch may fail for reasons documented in the Kinesis Service API Reference for PutRecordBatch. Failed records will be retried **retries_on_batch_request** times. If a record fails all retries an error log will be emitted.\n\n### reset_backoff_if_success\nBoolean, default `true`. If enabled, when after retrying, the next retrying checks the number of succeeded records on the former batch request and reset exponential backoff if there is any success. Because batch request could be composed by requests across shards, simple exponential backoff for the batch request wouldn't work some cases.\n\n### batch_request_max_count\nInteger, default 500. The number of max count of making batch request from record chunk. It can't exceed the default value because it's API limit.\n\n### batch_request_max_size\nInteger, default 4 * 1024*1024. The number of max size of making batch request from record chunk. It can't exceed the default value because it's API limit.\n\n### http_proxy\nHTTP proxy for API calling. Default `nil`.\n\n### endpoint\nAPI endpoint URL, for testing. Defalut `nil`.\n\n### ssl_verify_peer\nBoolean. Disable if you want to verify ssl conncetion, for testing. Default `true`.\n\n### debug\nBoolean. Enable if you need to debug Amazon Kinesis Firehose API call. Default is `false`.\n\n## Configuration: Examples\n\nHere are some configuration examles.\nAssume that the JSON object below is coming to with tag 'your_tag'.\n\n    {\n      \"name\":\"foo\",\n      \"action\":\"bar\"\n    }\n\n### Improving throughput to Amazon Kinesis\nThe plugin can also be configured to execute in parallel. `detach_process` and `num_threads` configuration settings control parallelism.\n\nIn case of the configuration below, you will spawn 2 processes.\n\n    <match your_tag>\n    type kinesis_*\n\n    stream_name YOUR_STREAM_NAME\n    region us-east-1\n\n    detach_process 2\n    </match>\n\nYou can also specify a number of threads to put. The number of threads is bound to each individual processes. So in this case, you will spawn 1 process which has 50 threads.\n\n    <match your_tag>\n    type kinesis_*\n\n    stream_name YOUR_STREAM_NAME\n    region us-east-1\n\n    num_threads 50\n    </match>\n\nBoth options can be used together, in the configuration below, you will spawn 2 processes and 50 threads per each processes.\n\n    <match your_tag>\n    type kinesis_*\n\n    stream_name YOUR_STREAM_NAME\n    region us-east-1\n\n    detach_process 2\n    num_threads 50\n    </match>\n\n## Development\n\nTo launch `fluentd` process with this plugin for development, follow the steps below:\n\n    git clone https://github.com/awslabs/aws-fluent-plugin-kinesis.git\n    cd aws-fluent-plugin-kinesis\n    make # will install gems and download KPL jar file and extract binaries\n    bundle exec fluentd -c /path/to/fluent.conf\n\nIf you want to run benchmark, use `make benchmark`.\n\n## Contributing\n\nBug reports and pull requests are welcome on [GitHub][github].\n\n## Related Resources\n\n* [Amazon Kinesis Streams Developer Guide](http://docs.aws.amazon.com/kinesis/latest/dev/introduction.html)\n* [Amazon Kinesis Firehose Developer Guide](http://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html)\n\n[fluentd]: http://fluentd.org/\n[streams]: https://aws.amazon.com/kinesis/streams/\n[firehose]: https://aws.amazon.com/kinesis/firehose/\n[producer]: http://docs.aws.amazon.com/kinesis/latest/dev/developing-producers-with-kpl.html\n[td-agent]: https://github.com/treasure-data/td-agent\n[bundler]: http://bundler.io/\n[assume_role]: http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\n[external_id]: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html\n[region]: http://docs.aws.amazon.com/general/latest/gr/rande.html#ak_region\n[fluentd_buffer]: http://docs.fluentd.org/articles/buffer-plugin-overview\n[github]: https://github.com/awslabs/aws-fluent-plugin-kinesis\n[formatter.rb]: https://github.com/fluent/fluentd/blob/master/lib/fluent/formatter.rb\n[default_config.properties]: https://github.com/awslabs/amazon-kinesis-producer/blob/master/java/amazon-kinesis-producer-sample/default_config.properties\n[old-readme]: https://github.com/awslabs/aws-fluent-plugin-kinesis/blob/master/README-v0.4.md\n[fluentd-doc-kinesis]: http://docs.fluentd.org/articles/kinesis-stream\n", "release_dates": []}, {"name": "azure-cli", "description": "Azure Command-Line Interface", "language": null, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Microsoft Azure CLI\n\n[![Python](https://img.shields.io/pypi/pyversions/azure-cli.svg?maxAge=2592000)](https://pypi.python.org/pypi/azure-cli)\n[![Build Status](https://dev.azure.com/azure-sdk/public/_apis/build/status/cli/Azure.azure-cli?branchName=dev)](https://dev.azure.com/azure-sdk/public/_build/latest?definitionId=246&branchName=dev)\n[![Slack](https://img.shields.io/badge/Slack-azurecli.slack.com-blue.svg)](https://azurecli.slack.com)\n\nA great cloud needs great tools; we're excited to introduce *Azure CLI*, our next generation multi-platform command line experience for Azure.\n\nTake a test run now from [Azure Cloud Shell](https://portal.azure.com/#cloudshell)!\n\n## Installation\n\nPlease refer to the [install guide](https://docs.microsoft.com/cli/azure/install-azure-cli) for detailed install instructions.\n\nA list of common install issues and their resolutions are available at [install troubleshooting](https://github.com/Azure/azure-cli/blob/dev/doc/install_troubleshooting.md).\n\n### Developer installation (see below)\n\n- [Docker](#docker)\n- [Edge Builds](#edge-builds)\n- [Developer Setup](#developer-setup)\n\n## Usage\n\n```bash\n$ az [ group ] [ subgroup ] [ command ] {parameters}\n```\n\n### Get Started\n\nPlease refer to the [\"get started\" guide](https://docs.microsoft.com/cli/azure/get-started-with-az-cli2) for in-depth instructions.\n\nFor usage and help content, pass in the `-h` parameter, for example:\n\n```bash\n$ az storage -h\n$ az vm create -h\n```\n\n### Highlights\n\nHere are a few features and concepts that can help you get the most out of the Azure CLI.\n\n![Azure CLI Highlight Reel](doc/assets/AzBlogAnimation4.gif)\n\nThe following examples are showing using the `--output table` format, you can change your default using the `az configure` command.\n\n#### Tab completion\n\nWe support tab-completion for groups, commands, and some parameters\n\n```bash\n# looking up resource group and name\n$ az vm show -g [tab][tab]\nAccountingGroup   RGOne  WebPropertiesRG\n\n$ az vm show -g WebPropertiesRG -n [tab][tab]\nStoreVM  Bizlogic\n\n$ az vm show -g WebPropertiesRG -n Bizlogic\n```\n\n#### Query\n\nYou can use the `--query` parameter and the [JMESPath](http://jmespath.org/) query syntax to customize your output.\n\n```bash\n$ az vm list --query \"[?provisioningState=='Succeeded'].{ name: name, os: storageProfile.osDisk.osType }\"\nName                    Os\n----------------------  -------\nstorevm                 Linux\nbizlogic                Linux\ndemo32111vm             Windows\ndcos-master-39DB807E-0  Linux\n```\n\n#### Exit codes\n\nFor scripting purposes, we output certain exit codes for differing scenarios.\n\n|Exit Code   |Scenario   |\n|---|---|\n|0  |Command ran successfully.   |\n|1   |Generic error; server returned bad status code, CLI validation failed, etc.   |\n|2   |Parser error; check input to command line.   |\n|3   |Missing ARM resource; used for existence check from `show` commands.   |\n\n### Common scenarios and use Azure CLI effectively\n\nPlease check [Tips for using Azure CLI effectively](https://docs.microsoft.com/en-us/cli/azure/use-cli-effectively). It describes some common scenarios:\n\n- [Output formatting (json, table, or tsv)](https://docs.microsoft.com/en-us/cli/azure/use-cli-effectively#output-formatting-json-table-or-tsv)\n- [Pass values from one command to another](https://docs.microsoft.com/en-us/cli/azure/use-cli-effectively#pass-values-from-one-command-to-another)\n- [Async operations](https://docs.microsoft.com/en-us/cli/azure/use-cli-effectively#async-operations)\n- [Generic update arguments](https://docs.microsoft.com/en-us/cli/azure/use-cli-effectively#generic-update-arguments)\n- [Generic resource commands - `az resource`](https://docs.microsoft.com/en-us/cli/azure/use-cli-effectively#generic-resource-commands---az-resource)\n- [REST API command - `az rest`](https://docs.microsoft.com/en-us/cli/azure/use-cli-effectively#rest-api-command---az-rest)\n- [Quoting issues](https://docs.microsoft.com/en-us/cli/azure/use-cli-effectively#quoting-issues)\n- [Work behind a proxy](https://docs.microsoft.com/en-us/cli/azure/use-cli-effectively#work-behind-a-proxy)\n- [Concurrent builds](https://docs.microsoft.com/en-us/cli/azure/use-cli-effectively#concurrent-builds)\n\n### More samples and snippets\n\nFor more usage examples, take a look at our [GitHub samples repo](http://github.com/Azure/azure-cli-samples) or [https://docs.microsoft.com/cli/azure/overview](https://docs.microsoft.com/cli/azure/overview).\n\n### Write and run commands in Visual Studio Code\n\nWith the [Azure CLI Tools](https://marketplace.visualstudio.com/items?itemName=ms-vscode.azurecli) Visual Studio Code extension, you can create `.azcli` files and use these features:\n- IntelliSense for commands and their arguments.\n- Snippets for commands, inserting required arguments automatically.\n- Run the current command in the integrated terminal.\n- Run the current command and show its output in a side-by-side editor.\n- Show documentation on mouse hover.\n- Display current subscription and defaults in status bar.\n- To enable IntelliSense for other file types like `.ps1` or `.sh`, see [microsoft/vscode-azurecli#48](https://github.com/microsoft/vscode-azurecli/issues/48).\n\n![Azure CLI Tools in Action](https://github.com/microsoft/vscode-azurecli/blob/main/images/in_action.gif?raw=true)\n\n## Data Collection\n\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n### Telemetry Configuration\n\nTelemetry collection is on by default. To opt out, please run `az config set core.collect_telemetry=no` to turn it off.\n\n## Reporting issues and feedback\n\nIf you encounter any bugs with the tool please file an issue in the [Issues](https://github.com/Azure/azure-cli/issues) section of our GitHub repo.\n\nTo provide feedback from the command line, try the `az feedback` command.\n\n\\[Microsoft internal] You may contact the developer team via azpycli@microsoft.com.\n\n## Developer installation\n\n### Docker\n\nWe maintain a Docker image preconfigured with the Azure CLI.\nSee our [Docker tags](https://mcr.microsoft.com/v2/azure-cli/tags/list) for available versions.\n\n```bash\n$ docker run -u $(id -u):$(id -g) -v ${HOME}:/home/az -e HOME=/home/az --rm -it mcr.microsoft.com/azure-cli:<version>\n```\n\n### Edge builds\n\nIf you want to get the latest build from the `dev` branch, you can use our \"edge\" builds.\n\nYou can download the latest builds by following the links below:\n\n|      Package      | Link                                       |\n|:-----------------:|:-------------------------------------------|\n|        MSI        | https://aka.ms/InstallAzureCliWindowsEdge  |\n| Homebrew Formula  | https://aka.ms/InstallAzureCliHomebrewEdge |\n| Ubuntu Bionic Deb | https://aka.ms/InstallAzureCliBionicEdge   |\n| Ubuntu Focal Deb  | https://aka.ms/InstallAzureCliFocalEdge    |\n| Ubuntu Jammy Deb  | https://aka.ms/InstallAzureCliJammyEdge    |\n|      RPM el8      | https://aka.ms/InstallAzureCliRpmEl8Edge   |\n\nOn Windows, you need to uninstall the official version before installing the edge build. (See https://github.com/Azure/azure-cli/issues/25607#issuecomment-1452855212)\n\nYou can easily install the latest Homebrew edge build with the following command:\n\n```bash\n# You need to uninstall the stable version with `brew uninstall azure-cli` first\ncurl --location --silent --output azure-cli.rb https://aka.ms/InstallAzureCliHomebrewEdge\nbrew install --build-from-source azure-cli.rb\n```\n\nYou can install the edge build on Ubuntu Jammy with the following command:\n\n```bash\ncurl --location --silent --output azure-cli_jammy.deb https://aka.ms/InstallAzureCliJammyEdge && dpkg -i azure-cli_jammy.deb\n```\n\nAnd install the edge build with rpm package on RHEL 8 or CentOS Stream 8:\n\n```bash\ndnf install -y $(curl --location --silent --output /dev/null --write-out %{url_effective} https://aka.ms/InstallAzureCliRpmEl8Edge)\n```\n\nHere's an example of installing edge builds with pip3 in a virtual environment. The `--upgrade-strategy=eager` option will install the edge builds of dependencies as well. \n\n```bash\n$ python3 -m venv env\n$ . env/bin/activate\n$ pip3 install --pre azure-cli --extra-index-url https://azurecliprod.blob.core.windows.net/edge --upgrade-strategy=eager\n```\n\nTo upgrade your current edge build pass the `--upgrade` option. The `--no-cache-dir` option is also recommended since\nthe feed is frequently updated.\n\n```bash\n$ pip3 install --upgrade --pre azure-cli --extra-index-url https://azurecliprod.blob.core.windows.net/edge --no-cache-dir --upgrade-strategy=eager\n```\n\nThe edge build is generated for each PR merged to the `dev` branch as a part of the Azure DevOps Pipelines. \n\n### Get builds of arbitrary commit or PR\n\nIf you would like to get builds of arbitrary commit or PR, see:\n\n[Try new features before release](doc/try_new_features_before_release.md)\n\n## Developer setup\n\nIf you would like to setup a development environment and contribute to the CLI, see:\n\n[Configuring Your Machine](https://github.com/Azure/azure-cli/blob/dev/doc/configuring_your_machine.md)\n\n[Authoring Command Modules](https://github.com/Azure/azure-cli/tree/dev/doc/authoring_command_modules)\n\n[Code Generation](https://github.com/Azure/aaz-dev-tools)\n\n## Contribute code\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\nIf you would like to become an active contributor to this project please\nfollow the instructions provided in [Microsoft Open Source Guidelines](https://opensource.microsoft.com/collaborate).\n", "release_dates": []}, {"name": "baselines", "description": "OpenAI Baselines: high-quality implementations of reinforcement learning algorithms", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Maintenance (expect bug fixes and minor updates)\n\n<img src=\"data/logo.jpg\" width=25% align=\"right\" /> [![Build status](https://travis-ci.org/openai/baselines.svg?branch=master)](https://travis-ci.org/openai/baselines)\n\n# Baselines\n\nOpenAI Baselines is a set of high-quality implementations of reinforcement learning algorithms.\n\nThese algorithms will make it easier for the research community to replicate, refine, and identify new ideas, and will create good baselines to build research on top of. Our DQN implementation and its variants are roughly on par with the scores in published papers. We expect they will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. \n\n## Prerequisites \nBaselines requires python3 (>=3.5) with the development headers. You'll also need system packages CMake, OpenMPI and zlib. Those can be installed as follows\n### Ubuntu \n    \n```bash\nsudo apt-get update && sudo apt-get install cmake libopenmpi-dev python3-dev zlib1g-dev\n```\n    \n### Mac OS X\nInstallation of system packages on Mac requires [Homebrew](https://brew.sh). With Homebrew installed, run the following:\n```bash\nbrew install cmake openmpi\n```\n    \n## Virtual environment\nFrom the general python package sanity perspective, it is a good idea to use virtual environments (virtualenvs) to make sure packages from different projects do not interfere with each other. You can install virtualenv (which is itself a pip package) via\n```bash\npip install virtualenv\n```\nVirtualenvs are essentially folders that have copies of python executable and all python packages.\nTo create a virtualenv called venv with python3, one runs \n```bash\nvirtualenv /path/to/venv --python=python3\n```\nTo activate a virtualenv: \n```\n. /path/to/venv/bin/activate\n```\nMore thorough tutorial on virtualenvs and options can be found [here](https://virtualenv.pypa.io/en/stable/) \n\n\n## Tensorflow versions\nThe master branch supports Tensorflow from version 1.4 to 1.14. For Tensorflow 2.0 support, please use tf2 branch.\n\n## Installation\n- Clone the repo and cd into it:\n    ```bash\n    git clone https://github.com/openai/baselines.git\n    cd baselines\n    ```\n- If you don't have TensorFlow installed already, install your favourite flavor of TensorFlow. In most cases, you may use\n    ```bash \n    pip install tensorflow-gpu==1.14 # if you have a CUDA-compatible gpu and proper drivers\n    ```\n    or \n    ```bash\n    pip install tensorflow==1.14\n    ```\n    to install Tensorflow 1.14, which is the latest version of Tensorflow supported by the master branch. Refer to [TensorFlow installation guide](https://www.tensorflow.org/install/)\n    for more details. \n\n- Install baselines package\n    ```bash\n    pip install -e .\n    ```\n\n### MuJoCo\nSome of the baselines examples use [MuJoCo](http://www.mujoco.org) (multi-joint dynamics in contact) physics simulator, which is proprietary and requires binaries and a license (temporary 30-day license can be obtained from [www.mujoco.org](http://www.mujoco.org)). Instructions on setting up MuJoCo can be found [here](https://github.com/openai/mujoco-py)\n\n## Testing the installation\nAll unit tests in baselines can be run using pytest runner:\n```\npip install pytest\npytest\n```\n\n## Training models\nMost of the algorithms in baselines repo are used as follows:\n```bash\npython -m baselines.run --alg=<name of the algorithm> --env=<environment_id> [additional arguments]\n```\n### Example 1. PPO with MuJoCo Humanoid\nFor instance, to train a fully-connected network controlling MuJoCo humanoid using PPO2 for 20M timesteps\n```bash\npython -m baselines.run --alg=ppo2 --env=Humanoid-v2 --network=mlp --num_timesteps=2e7\n```\nNote that for mujoco environments fully-connected network is default, so we can omit `--network=mlp`\nThe hyperparameters for both network and the learning algorithm can be controlled via the command line, for instance:\n```bash\npython -m baselines.run --alg=ppo2 --env=Humanoid-v2 --network=mlp --num_timesteps=2e7 --ent_coef=0.1 --num_hidden=32 --num_layers=3 --value_network=copy\n```\nwill set entropy coefficient to 0.1, and construct fully connected network with 3 layers with 32 hidden units in each, and create a separate network for value function estimation (so that its parameters are not shared with the policy network, but the structure is the same)\n\nSee docstrings in [common/models.py](baselines/common/models.py) for description of network parameters for each type of model, and \ndocstring for [baselines/ppo2/ppo2.py/learn()](baselines/ppo2/ppo2.py#L152) for the description of the ppo2 hyperparameters. \n\n### Example 2. DQN on Atari \nDQN with Atari is at this point a classics of benchmarks. To run the baselines implementation of DQN on Atari Pong:\n```\npython -m baselines.run --alg=deepq --env=PongNoFrameskip-v4 --num_timesteps=1e6\n```\n\n## Saving, loading and visualizing models\n\n### Saving and loading the model\nThe algorithms serialization API is not properly unified yet; however, there is a simple method to save / restore trained models. \n`--save_path` and `--load_path` command-line option loads the tensorflow state from a given path before training, and saves it after the training, respectively. \nLet's imagine you'd like to train ppo2 on Atari Pong,  save the model and then later visualize what has it learnt.\n```bash\npython -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v4 --num_timesteps=2e7 --save_path=~/models/pong_20M_ppo2\n```\nThis should get to the mean reward per episode about 20. To load and visualize the model, we'll do the following - load the model, train it for 0 steps, and then visualize: \n```bash\npython -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v4 --num_timesteps=0 --load_path=~/models/pong_20M_ppo2 --play\n```\n\n*NOTE:* Mujoco environments require normalization to work properly, so we wrap them with VecNormalize wrapper. Currently, to ensure the models are saved with normalization (so that trained models can be restored and run without further training) the normalization coefficients are saved as tensorflow variables. This can decrease the performance somewhat, so if you require high-throughput steps with Mujoco and do not need saving/restoring the models, it may make sense to use numpy normalization instead. To do that, set 'use_tf=False` in [baselines/run.py](baselines/run.py#L116). \n\n### Logging and vizualizing learning curves and other training metrics\nBy default, all summary data, including progress, standard output, is saved to a unique directory in a temp folder, specified by a call to Python's [tempfile.gettempdir()](https://docs.python.org/3/library/tempfile.html#tempfile.gettempdir).\nThe directory can be changed with the `--log_path` command-line option.\n```bash\npython -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v4 --num_timesteps=2e7 --save_path=~/models/pong_20M_ppo2 --log_path=~/logs/Pong/\n```\n*NOTE:* Please be aware that the logger will overwrite files of the same name in an existing directory, thus it's recommended that folder names be given a unique timestamp to prevent overwritten logs.\n\nAnother way the temp directory can be changed is through the use of the `$OPENAI_LOGDIR` environment variable.\n\nFor examples on how to load and display the training data, see [here](docs/viz/viz.ipynb).\n\n## Subpackages\n\n- [A2C](baselines/a2c)\n- [ACER](baselines/acer)\n- [ACKTR](baselines/acktr)\n- [DDPG](baselines/ddpg)\n- [DQN](baselines/deepq)\n- [GAIL](baselines/gail)\n- [HER](baselines/her)\n- [PPO1](baselines/ppo1) (obsolete version, left here temporarily)\n- [PPO2](baselines/ppo2) \n- [TRPO](baselines/trpo_mpi)\n\n\n\n## Benchmarks\nResults of benchmarks on Mujoco (1M timesteps) and Atari (10M timesteps) are available \n[here for Mujoco](https://htmlpreview.github.com/?https://github.com/openai/baselines/blob/master/benchmarks_mujoco1M.htm) \nand\n[here for Atari](https://htmlpreview.github.com/?https://github.com/openai/baselines/blob/master/benchmarks_atari10M.htm) \nrespectively. Note that these results may be not on the latest version of the code, particular commit hash with which results were obtained is specified on the benchmarks page. \n\nTo cite this repository in publications:\n\n    @misc{baselines,\n      author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},\n      title = {OpenAI Baselines},\n      year = {2017},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      howpublished = {\\url{https://github.com/openai/baselines}},\n    }\n\n", "release_dates": []}, {"name": "baselines-results", "description": null, "language": "Jupyter Notebook", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# DQN results\n\nThis repository contains a [jupyter notebook with the results from running `openai/baselines`](https://github.com/openai/baselines-results/blob/master/dqn_results.ipynb)\n", "release_dates": []}, {"name": "bchess-personal", "description": "temporarily public for a bug report", "language": null, "license": null, "readme": "Hello world\n", "release_dates": []}, {"name": "blocksparse", "description": "Efficient GPU kernels for block-sparse matrix multiplication and convolution", "language": "Cuda", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Active (under active development, breaking changes may occur)\n\n# Blocksparse\n\nThe `blocksparse` package contains TensorFlow Ops and corresponding GPU kernels for block-sparse matrix multiplication.  Also included are related ops like edge bias, sparse weight norm and layer norm.\n\nTo learn more, see [the launch post on the OpenAI blog](https://blog.openai.com/block-sparse-gpu-kernels/).\n\n## Prerequisites\n\nFirst, you need at least one Nvidia GPU. For best performance, we recommend using a Pascal or Maxwell generation GPU -- this is the full list of features by GPU type:\n\n| GPU Family | BSMatMul-ASM | BSMatMul-CudaC | BSConv |\n|------------|------------------------|----------------|--------|\n| Kepler | - | X | - |\n| Maxwell | X (fastest) | X | X |\n| Pascal | X (fastest) | X | X |\n| Volta | - | X (fastest) | - |\n\nNote that BSMatMul-CudaC **only supports `feature_axis=0`**, while BSMatMul-ASM only supports `feature_axis=1`.\n\nAdditionally, you need:\n\n- A working Linux installation (we run Ubuntu 16.04) with the Nvidia drivers for your GPU.\n- CUDA 8 (in `/usr/local/cuda`)\n- Python 3.5 or newer, or 2.7 or newer\n- TensorFlow 1.4.0 or newer, [with GPU support](https://www.tensorflow.org/install/install_linux#install_tensorflow) (e.g. `pip install tensorflow-gpu`)\n- CUDA 9 and Volta will work if you update the build targets (-gencode=arch=compute_70,code=sm_70) and also build tenorflow from source.\n\n## Installation\n\n```\npip install blocksparse\n```\n\n## Usage\n\nThis example performs a block-sparse matrix multiplication:\n```\nfrom blocksparse.matmul import BlocksparseMatMul\nimport tensorflow as tf\nimport numpy as np\n\nhidden_size = 4096\nblock_size = 32\nminibatch_size = 64\n\n# Create a (random) sparsity pattern\nsparsity = np.random.randint(2, size=(hidden_size//block_size,hidden_size//block_size))\n\n# Initialize the sparse matrix multiplication object\nbsmm = BlocksparseMatMul(sparsity, block_size=block_size)\n\n# Input to graph\nx = tf.placeholder(tf.float32, shape=[None, hidden_size])\n\n# Initialize block-sparse weights\nw = tf.get_variable(\"w\", bsmm.w_shape, dtype=tf.float32)\n\n# Block-sparse matrix multiplication\ny = bsmm(x, w)\n\n# Run\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nresult = sess.run([y], feed_dict = {x: np.ones((minibatch_size,hidden_size), dtype='float32')})\nprint(result)\n```\n\nFor a more involved example using block-sparse ops to train a language model, see [`examples/`](./examples/).\n\n## Development\n\nIf you're interested in hacking on the ops and kernels, go ahead and build from source:\n\n    git clone git@github.com:openai/blocksparse.git\n    cd blocksparse\n\n    make compile\n    pip install dist/*.whl\n\n    # test it if you like\n    test/blocksparse_matmul_test.py\n    test/blocksparse_conv_test.py\n\nIf your CUDA is not in `/usr/local/cuda` or you have several versions, e.g. both `/usr/local/cuda-8.0` and `/usr/local/cuda-9.0`, set `CUDA_HOME` to the base path to use when compiling `make compile`.\n\n\n## API Documentation:\n\n\n### blocksparse.matmul\n\n    class BlocksparseMatMul(object)\n\n        def __init__(self, layout, block_size=32, feature_axis=1)\n        \"\"\"\n        layout: a 2d array of ones and zeros specifying the block layout\n        block_size: values 32, 16, 8 supported\n        feature_axis: when block_size is less than 32 memory access becomes far more efficient\n                      with a (C,N) activation layout\n        \"\"\"\n\n        # shape helpers for generating tensors (N=minibatch)\n        self.w_shape\n        def i_shape(self, N)\n        def o_shape(self, N)\n\n        # return the coordinates (c,k) in the layout that corresponds to a given block id\n        def block_coord(self, block)\n\n        # experimental ortho init\n        def ortho_init(self)\n\n        # in practice, identity_init + layernorm is all you need for initialization\n        # with gpu=True the init is performed by kernel on the device\n        def identity_init(self, gpu=False)\n\n        # To implement weight normalization.  In practice, layernorm works much better.\n        def l2_normalize(self, W, gain=None, epsilon=1e-6, dtype=np.float32)\n\n        def __call__(self, I, W, dw_dtype=tf.float32)\n        \"\"\"\n        Execute the op.  Note that the weight variable is independant from the bsmm object.\n        This allows multiple weights to be tied to the same bsmm layout.\n\n        dw_dtype: allows control over dw precision format.\n        \"\"\"\n\n\n    def group_param_grads(param_grad, group_size=8, cast32=True)\n    \"\"\"\n    param_grad: the tensorflow parameter gradient for a give bsmm weight variable (returned from tf.gradients)\n    group_size: desired group size, up to 8 supported\n\n    This causes the tf graph to be rewritten so that weight grad matmuls from different time steps\n    (and shared weights across time) are combined into a more efficient single matmul.\n    \"\"\"\n\n\n    class SparseProj(object):\n        def __init__(self, nhidden, nproj=None, proj_stride=None, block_size=32, gather_lut=None)\n        \"\"\"\n        Experimental class to support dense-to-sparse and sparse-to-dense projections.\n        Basically the same as the tensorflow ops but faster and support alternate precision formats.\n        They assume a unique 1 to 1 mapping so atomics need not be used on backward ops.\n        \"\"\"\n\n        def gather(self, x)\n        def scatter(self, x)\n        def scatter_add(self, x, y)\n        def scatter_mul(self, x, y)\n\n\n\n### blocksparse.conv\n\n    class BlocksparseConv(object):\n        def __init__(self, BCK, TRS, DHW, MPQ=None, strides=(1,1,1), dilates=(1,1,1), padding=\"SAME\", edge_bias=False)\n        \"\"\"\n        BCK: (                                             # block(B)/input(C)/output(K) feature dims\n                 ( (c0, c1, c2, ...), (k0, k1, k2, ...) ), # block 0 c,k are indeces into C,K dims\n                 ( (c0, c1, c2, ...), (k0, k1, k2, ...) ), # block 1\n                 ( (c0, c1, c2, ...), (k0, k1, k2, ...) ), # block 2 ...\n             )\n        TRS: (T,R,S) or (R,S) or (S,)         - filter spatial size dims\n        DHW: (D,H,W) or (H,W) or (W,)         - input image spatial size dims\n        MPQ: (M,P,Q) or (P,Q) or (Q,) or None - output image spatial size dims (used for ambiguous dims in strided transpose conv)\n        strides: (1,1,1) or (1,1) or (1,)\n        dilates: (1,1,1) or (1,1) or (1,)\n        padding: (1,1,1) or (1,1) or (1,) or \"SAME\" or \"VALID\"\n        edge_bias: True/False\n        \"\"\"\n\n        # shape helpers for setting up variables or test tensors\n        def edge_bias_shape(self)\n        def f_shape(self, block=None)\n        def i_shape(self, N)\n        def o_shape(self, N)\n\n        # execute op passing in param variables and input\n        def __call__(self, F, I, edge_bias=None):\n\n        # for implementing weight norm\n        def l2_normalize(self, F, gain=None, epsilon=1e-6, dtype=np.float32):\n\n    class BlocksparseDeconv(BlocksparseConv)\n        def __init__(self, BCK, TRS, DHW, MPQ=None, strides=(1,1,1), dilates=(1,1,1), padding=\"SAME\", edge_bias=False)\n        \"\"\"\n        Deconvolution.  Same params as above.\n        \"\"\"\n\n    def cwise_linear(x, a=None, b=None)\n    \"\"\"\n    In the NCHW tensor format, tensorflow is extremely slow at implementing simple broadcasting ops on the middle C dim.\n    This lets you do:\n        y = a*x + b\n        y = a*x\n        y = x + b\n\n    Where a and b are of shape (1,C,1,1)\n    This is useful for ops like weight norm.\n\n### blocksparse.ew\n\n    # same as tf ops but generally more efficient and allow custom precision formats\n    def        add(x, y, name=None)\n    def   multiply(x, y, name=None)\n    def   subtract(x, y, name=None)\n    def     divide(x, y, name=None)\n    def    maximum(x, y, name=None)\n    def    minimum(x, y, name=None)\n\n    def   negative(x,    name=None)\n    def reciprocal(x,    name=None)\n    def     square(x,    name=None)\n    def       sqrt(x,    name=None)\n    def        exp(x,    name=None)\n    def        log(x,    name=None)\n    def    sigmoid(x,    name=None)\n    def       tanh(x,    name=None)\n    def       relu(x,    name=None)\n    def       elu (x, alpha=1.0, name=None)\n\n    # here args can be the 4 independant gate tensors or\n    # a single merged gate tensor (which gets split in 4 internally)\n    def fused_lstm_gates(c, *args, name=None)\n\n    def split4(x)\n    def concat4(x0, x1, x2, x3)\n\n    # A custom cast op to help explore novel precision formats\n    def float_cast(x, dtype, dx_dtype=None)\n\n    # a much faster (and non-deterministic) dropout op\n    # also supports novel precision formats\n    def dropout(x, keep_prob=0.8, mask=None)\n\n    # an op to be used in tf.gradients when adding together multiple contributions of a gradient.\n    # note that only 8 inputs are supported as you'd never want a single op to consume all possible inputs\n    # before it starts executing in the graph (and hence reducing the memory footprint)\n    def add_n8(xs, name=None)\n\n\n\n### blocksparse.norms\n\n    def layer_norm(x, g, b, axis=1, epsilon=1e-6, relu=False)\n    \"\"\"\n    Very fast layernorm to support both bsmm feature_axis activation layouts.\n    Also inlcludes optional integrated relu (applied to end)\n    \"\"\"\n\n    # basic batch norm ops for the NCHW layout\n    def batch_norm(x, g, b, epsilon=1e-6)\n    def batch_norm_inference(x, g, b, m, v, epsilon=1e-6)\n\n\n", "release_dates": []}, {"name": "box2d-py", "description": null, "language": "C++", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "**Status:** Maintenance (expect bug fixes and minor updates)\n\nThis is a repackaged version of [pybox2d](https://github.com/pybox2d/pybox2d). For all information, see that repo. For licensing information, see `LICENSE`.\n", "release_dates": []}, {"name": "bugbounty-gpt", "description": "A helpful gpt-based triage tool for BugCrowd bugbounty programs.", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# BugCrowd GPT Classifier\n\nThis project is a system designed to manage, classify, and process BugCrowd submissions using OpenAI, and a Postgres Database for data storage. It provides an automatic handling mechanism for submissions and can be run directly or within a Docker container.\n\n## Table of Contents\n\n- [BugCrowd GPT Clasifier](#bugcrowd-gpt-clasifier)\n  - [Table of Contents](#table-of-contents)\n  - [Prerequisites](#prerequisites)\n  - [Getting Started](#getting-started)\n    - [Configuration](#configuration)\n      - [`config.yaml` File](#configyaml-file)\n        - [API Settings](#api-settings)\n        - [User Settings](#user-settings)\n        - [Categories](#categories)\n        - [OpenAI Prompt](#openai-prompt)\n      - [Environment Variables](#environment-variables)\n      - [Docker Compose](#docker-compose)\n  - [Environment Variables for Docker Compose](#environment-variables-for-docker-compose)\n\n## Prerequisites\n\nBefore starting, make sure you have the following installed:\n\n- Docker\n- Docker Compose\n\n## Getting Started\nAs it stands, there is a Dockerfile within the root directory of this repository that, upon being built, will download all depenedencies and will package everything up for usage. After configuring the application and setting all sensitive environment variables, you can run this Dockerfile and start classifying and triaging results immediately.\n\nTo actually test your configuration, it is reccomended that you take utilize docker-compose to quickly spin-up and clear-out an ephermal database. Connect locally to the database with `psql` and inspect classifications.\n\nIf Docker isn't your cup of tea, you can also install the requirements locally and run the classifier/responder via `python -m bugbounty_gpt`\n\n### Configuration\n\nConfiguration of the application involves setting up the `config.yaml` file as well as certain environment variables. Below are detailed descriptions of each configuration option. The current `config.yaml` is a working example that you can base your changes off of.\n\n#### `config.yaml` File\n\n##### API Settings\n\n- `base_url`: URL of the Bugcrowd API, e.g. `\"https://api.bugcrowd.com\"`.\n- `openai_model`: Chat model used for the OpenAI integration, e.g. `\"gpt-4\"`.\n\n##### User Settings\n\n- `user_id`: Identifier for the user interacting with the application. This represents the Bugcrowd ID of the user assigned to the report, typically a Bugcrowd employee. This user is responsible for validating that the actual report is indeed correctly triaged. It can be left empty as `\"\"` if not needed for a particular configuration.\n\n- `filter_program`: Identifier for the specific Bugcrowd program that you're targeting. This configuration is used to filter API responses so that only the submissions from the program specified here are fetched. For example, it could be set to `\"openai-test-sandbox\"` to ensure that only submissions related to that particular program are retrieved and processed.\n\n\n##### Categories\n\n- `valid`: A list of all valid categories available for classifying reports. These categories encompass the entire range of possible classifications within the system. For example, they may include categories like \"Functional Bugs or Glitches,\" \"Customer Support Issues,\" \"Security Report,\" etc. Every report submitted must be classified into one of these valid categories.\n\n- `default`: Specifies the default category to be assigned when a submission fails to be classified into any of the `valid` categories. The value should be one of the categories listed under `valid`. Typically, this should be a category that doesn't have an automated response attached to it, so manual handling can be applied as needed. For example, it could be set to `\"Security Report\"` if this is a category that requires further manual review.\n\n- `response`: Defines a subset of the `valid` categories that the system will close and respond to. It consists of a list of objects, each containing two keys: `name` and `response`. The `name` must match one of the valid category names, and `response` provides a corresponding template response for that category. These template responses will be used to automatically reply to submissions that are classified into these specific categories. For example, a response to \"Functional Bugs or Glitches\" might provide information on how to submit the report through standard support channels since it falls outside the scope of a security-focused bug bounty program.\n\n##### OpenAI Prompt\n\n- `openai_prompt`: A multi-line string that specifies the instructions, categories, and guidelines for classifying reports. The example in `config.yaml` is a great place to start building the prompt for your specific program.\n\n#### Environment Variables\n\n- `BUGCROWD_API_KEY`: API key for Bugcrowd, should be stored as an environment variable.\n- `OPENAI_API_KEY`: API key for OpenAI, should be stored as an environment variable.\n- `SQLALCHEMY_URL`: Connection URL for the database, utilizing an asynchronous driver such as `asyncpg`. This URL should follow the format `postgresql+asyncpg://<username>:<password>@<host>:<port>/<database>`, where you replace the placeholders with your specific PostgreSQL database connection details.\n\n#### Docker Compose\n\nThe `docker-compose.yaml` file defines the containers for the application and database, including build arguments and volumes for data persistence. Specifically, it facilitates the process of testing changes to the configuration, especially those related to valid categories, which are translated into an Enum class and ingested as a type into the database.\n\nWhen modifying category types, the associated Enum type needs to be reset \u2014 a change that alembic doesn't effectively detect. To quickly test such configuration changes, the `docker-compose` setup enables the use of an ephemeral database model, where the entire schema, including Enums and tables, can be discarded and rebuilt. The `ephemeral_db` argument in the Dockerfile controls whether to use this temporary database or point to a long-living production database.\n\nBy running `docker-compose up --build` and `docker-compose down -v`, you can repeatedly test changes to your config in a contained environment, ensuring that everything is functioning as expected without affecting the actual production setup.\n\nIn a production environment, manage alembic scripts manually and build the Dockerfile without the `ephemeral_db` argument, pointing to a long-living production database instead. This approach guarantees that the Docker Compose setup accurately reflects the structure and behavior of the production system while allowing for flexible and rapid testing during development.\n\n1. **Configuration**: Make sure to properly set up the configuration in the `config.yaml` file, as well as required environment variables, before building and running the application. Detailed instructions are provided in the Configuration section below.\n\n2. **Build the Docker Containers**: Navigate to the project directory and run the following command:\n\n    ```bash\n    docker-compose build\n    ```\n\n3. **Run the Containers**: Once the build is complete, start the containers with:\n\n    ```bash\n    docker-compose up\n    ```\n\n4. **Stop and Remove Containers**: To stop and remove everything, use:\n\n    ```bash\n    docker-compose down -v\n    ```\n\n   The `-v` option will also remove the volume used to persist the database data.\n\n## Environment Variables for Docker Compose\n\nWhen using Docker Compose to build and run containers for the purpose of testing configuration changes, you'll need to provide certain environment variables. These variables are specifically for the Docker Compose setup and can be set in the `docker-compose.yml` file:\n\n- `DB_USER`: Database user, used for connecting to the ephemeral database in Docker Compose.\n- `DB_PASSWORD`: Database password, used for connecting to the ephemeral database in Docker Compose.\n- `DB_NAME`: Database name, used for specifying the database within the ephemeral database environment in Docker Compose.\n- `BUGCROWD_API_KEY`: BugCrowd API key.\n- `OPENAI_API_KEY`: OpenAI API key.\n- ... and any other environment variables required for your specific testing configuration.\n\nThese environment variables are essential for defining the database connection and other parameters within the Docker Compose setup, enabling you to repeatedly test changes to your configuration without affecting general production workloads. They facilitate the process of rebuilding the schema, including Enums and tables, in a contained environment, and should be adjusted according to the needs of your specific test setup.\n", "release_dates": []}, {"name": "ceph-chef", "description": "Chef cookbooks for managing a Ceph cluster", "language": "Ruby", "license": null, "readme": "# Ceph-Chef Cookbook\n\n[![Join the chat at https://gitter.im/ceph/ceph-chef](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/ceph/ceph-chef?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![License](https://img.shields.io/badge/license-Apache_2-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0)\n\n## DESCRIPTION\n\nInstalls and configures Ceph, a distributed network storage and filesystem designed to provide excellent performance, reliability, and scalability. Supports *Hammer* and higher releases (nothing below Hammer is supported in this repo).\n\n>Once *Hammer* support has stopped in Ceph then it will be removed from this cookbook as an option.\n\nThe current version is focused on installing and configuring Ceph for Ubuntu, CentOS and RHEL.\n\nFor documentation on how to use this cookbook, refer to the **[USAGE](#USAGE)** section.\n\n>Ceph-Chef works with __ALL__ of the latest versions of Ceph and includes `Ceph-Mgr` recipe.\n\n### Recommendation\n\nThere are many Enterprises that use the cookbook to install and manage Ceph. Below are a few reference Chef Wrapper Repos and projects that use Ceph-Chef.\n\n>**[https://github.com/bloomberg/chef-bcs](https://github.com/bloomberg/chef-bcs)**. That Chef App (repo) uses this repo for Bloomberg's large clusters. The **chef-bcs repo** is an S3 Object Store Cluster used in multiple data centers.\n\n>**[https://github.com/cepheus-io/cepheus](https://github.com/cepheus-io/cepheus)** or http://cepheus.io. This is a powerful and flexible example of use Ceph-Chef. Everything is data driven so you can simply change the data and use it to build your own Ceph cluster or as a reference. It also provides a full management stack for Ceph.\n\nNote: The documentation is a WIP along with a few other features. This repo is actively managed.  \n\nFor help, use **[Gitter chat](https://gitter.im/ceph/ceph-chef)**, **[mailing-list](mailto:ceph-users-join@lists.ceph.com)** or **[issues](https://github.com/ceph/ceph-chef/issues)** here in this repo.\n\n### NOTE: Users of ceph-cookbook\nThe original ceph-cookbook will remain and may continue to be updated (see that repo for specifics). We tried to use some of the interesting features of ceph-cookbook but we added a lot of enhancements and simplifications. Simply replacing ceph-cookbook with ceph-chef will not work without a few modifications. Also, ceph-chef only works with Chef 12.8+ and Hammer and higher. Nothing below the Hammer release of Ceph is supported in this repo. In addition, only **civitweb** is used going forward (not Apache).\n\nNOTE: The current LWRP are using the style prior to Chef version 12.5. There will be a new release shortly that will support the now recommended way of handling custom resources. To make that change easier we will be using a helper cookbook called Poise. Using Poise makes creating custom resources and common services very simple.\n\n### Chef\n\n\\>= 12.8+\n\n### Platform\n\nTested as working:\n\n* Ubuntu Trusty (16.04) [Still verifying updates work]\n* CentOS (7.3)\n* RHEL (7.3)\n\n### Cookbooks\n\nThe ceph cookbook requires the following cookbooks from Chef:\n\nhttps://supermarket.chef.io/\n\n* [apt](https://supermarket.chef.io/cookbooks/apt)\n* [apache2](https://supermarket.chef.io/cookbooks/apache2)\n* [yum](https://supermarket.chef.io/cookbooks/yum)\n* [ntp](https://supermarket.chef.io/cookbooks/ntp)\n* [poise](https://supermarket.chef.io/cookbooks/poise)\n* [chef-sugar](https://supermarket.chef.io/cookbooks/chef-sugar)\n\n## USAGE\n\nCeph cluster design and Ceph support are beyond the scope of this README, please turn to the:\n\npublic wiki, mailing lists, visit our IRC channel, or contact Red Hat:\n\nhttp://ceph.com/docs/master\nhttp://ceph.com/resources/mailing-list-irc/\n\nThis cookbook can be used to implement a chosen cluster design. Most of the configuration is retrieved from node attributes, which can be set by an environment json file or by a wrapper cookbook that updates the attributes directly. A basic cluster configuration will need most of the following attributes:\n\n* `node['ceph']['config']['fsid']` - the cluster UUID\n* `node['ceph']['config]'['global']['public network']` - a CIDR specification of the public network\n* `node['ceph']['config]'['global']['cluster network']` - a CIDR specification of a separate cluster replication network\n* `node['ceph']['config]'['global']['rgw dns name']` -  the main domain of the radosgw daemon\n\nMost notably, the configuration does **NOT** need to set the `mon initial members`, because the cookbook does a node search based on TAGS or ENVIRONMENTS to find other mons in the same environment. However, you can add them to `node['ceph']['config']['global']['mon initial members'] = <whatever mon ip list you want>`\n\nThe other set of attributes that this recipe needs is `node['ceph']['osd']['devices']`, which is an array of OSD definitions, similar to the following:\n\n* {'device' => '/dev/sdb'} - Use a full disk for the OSD, with a small partition for the journal\n* {'type' => 'directory', 'device' => '/src/node/sdb1/ceph'} - Use a directory, and have a small file for the journal\n* {'device' => '/dev/sde', 'dmcrypt' => true} - Store the data encrypted by passing --dmcrypt to `ceph-disk-prepare`\n* {'device' => '/dev/sdc', 'journal' => '/dev/sdd2'} - use a full disk for the OSD with a custom partition for the journal on another device such as an SSD or NMVe\n\n### Ceph Admin Commands\n\nAn example of finding a mon socket in a generic like environment.\n`ceph-conf --name mon.$(hostname -s) --show-config-value admin_socket`\n\n### Trouble Shooting\n\nPools - After creating it appears that some of the PGs are stuck 'creating+peering'. This can be caused by a number of things. Most likely an OSD is not blocking the creation. Do something like:\nceph pg ls-by-pool <whatever the pool name> | grep creating\n\nSomething like: `ceph pg ls-by-pool .rgw | grep creating`\n\nIt should show you the PG number as the first column. Use that to query it to see if something is blocking it.\nceph pg <pg num> query\n\nSomething like: `ceph pg 175.f7 query`\n\nThis will return a lot of json data. You can grep or look for 'blocking'. If found then restart the given OSD. You can find the host for the OSD with: `ceph osd find <OSD number>`\n\nOnce you're on the host simply restart the specific OSD with: `sudo service ceph restart osd.<whatever the number>`\n\n\n### Using a Policy Wrapper Cookbook\n\nTo automate setting several of these node attributes, it is recommended to use a policy wrapper cookbook. This allows the ability to use Chef Server cookbook versions along with environment version restrictions to roll out configuration changes in an ordered fashion.\n\nIt also can help with automating some settings. For example, a wrapper cookbook could peek at the list of hard drives that `ohai` has found and populate node['ceph']['osd_devices'] accordingly, instead of manually typing them all in:\n\n```ruby\nnode.override['ceph']['osd_devices'] = node['block_device'].each.reject{ |name, data| name !~ /^sd[b-z]/}.sort.map { |name, data| {'journal' => \"/dev/#{name}\"} }\n```\n\nFor best results, the wrapper cookbook's recipe should be placed before the Ceph cookbook in the node's runlist. This will ensure that any attributes are in place before the Ceph cookbook runs and consumes those attributes.\n\n### Ceph Monitor\n\nCeph monitor nodes should use the ceph-mon role.\n\nIncludes:\n\n* ceph-chef::default\n\n### Ceph Metadata Server\n\nCeph metadata server nodes should use the ceph-mds role. NB: Only required for Ceph-FS\n\nIncludes:\n\n* ceph-chef::default\n\n### Ceph OSD\n\nCeph OSD nodes should use the ceph-osd role\n\nIncludes:\n\n* ceph-chef::default\n\n### Ceph RADOS Gateway (RGW)\n\nCeph RGW nodes should use the ceph-radosgw role\n\n## ATTRIBUTES\n\n### General\n\n* `node['ceph']['search_environment']` - a custom Chef environment to search when looking for mon nodes. The cookbook defaults to searching the current environment\n* `node['ceph']['branch']` - selects whether to install the stable, testing, or dev version of Ceph\n* `node['ceph']['version']` - install a version of Ceph that is different than the cookbook default. If this is changed in a wrapper cookbook, some repository urls may also need to be replaced, and they are found in attributes/repo.rb. If the branch attribute is set to dev, this selects the gitbuilder branch to install\n* `node['ceph']['extras_repo']` - whether to install the ceph extras repo. The tgt recipe requires this\n\n* `node['ceph']['config']['fsid']` - the cluster UUID\n* `node['ceph']['config']['global']['public network']` - a CIDR specification of the public network\n* `node['ceph']['config']['global']['cluster network']` - a CIDR specification of a separate cluster replication network\n* `node['ceph']['config']['config-sections']` - add to this hash to add extra config sections to the ceph.conf\n\n* `node['ceph']['user_pools']` - an array of pool definitions, with attributes `name`, `pg_num` and `create_options` (optional), that are automatically created when a monitor is deployed\n\n### Ceph MON\n\n* `node['ceph']['config']['mon']` - a hash of settings to save in ceph.conf in the [mon] section, such as `'mon osd nearfull ratio' => '0.75'`\n\n### Ceph OSD\n\n* `node['ceph']['osd_devices']` - an array of OSD definitions for the current node\n* `node['ceph']['config']['osd']` - a hash of settings to save in ceph.conf in the [osd] section, such as `'osd max backfills' => 2`\n* `node['ceph']['config']['osd']['osd crush location']` - this attribute can be set on a per-node basis to maintain Crush map locations\n\n### Ceph MDS\n\n* `node['ceph']['config']['mds']` - a hash of settings to save in ceph.conf in the [mds] section, such as `'mds cache size' => '100000'`\n* `node['ceph']['cephfs_mount']` - where the cephfs recipe should mount CephFS\n* `node['ceph']['cephfs_use_fuse']` - whether the cephfs recipe should use the fuse cephfs client. It will default to heuristics based on the kernel version\n\n### Ceph RADOS Gateway (RGW)\n### Note: Only supports the newer 'civetweb' version of RGW (not Apache)\n\n* `node['ceph']['radosgw']['port']` - Port of the rgw. Defaults to 80\n* `node['ceph']['radosgw']['rgw_dns_name']` -  the main domain of the radosgw daemon, to calculate the bucket name from a subdomain\n\n## Resources/Providers\n\n### ceph\\_client\n\nThe ceph\\_client LWRP provides an easy way to construct a Ceph client key. These keys are needed by anything that needs to talk to the Ceph cluster, including RGW, CephFS, and RBD access.\n\n#### Actions\n\n- :add - creates a client key with the given parameters\n\n#### Parameters\n\n- :name - name attribute. The name of the client key to create. This is used to provide a default for the other parameters\n- :caps - A hash of capabilities that should be granted to the client key. Defaults to `{ 'mon' => 'allow r', 'osd' => 'allow r' }`\n- :as\\_keyring - Whether the key should be saved in a keyring format or a simple secret key. Defaults to true, meaning it is saved as a keyring\n- :keyname - The key name to register in Ceph. Defaults to `client.#{name}.#{hostname}`\n- :filename - Where to save the key. Defaults to `/etc/ceph/ceph.client.#{name}.#{hostname}.keyring` if `as_keyring` and `/etc/ceph/ceph.client.#{name}.#{hostname}.secret` if not `as_keyring`\n- :owner - Which owner should own the saved key file. Defaults to root\n- :group - Which group should own the saved key file. Defaults to root\n- :mode - What file mode should be applied. Defaults to '00640'\n\n### ceph\\_cephfs\n\nThe ceph\\_cephfs LWRP provides an easy way to mount CephFS. It will automatically create a Ceph client key for the machine and mount CephFS to the specified location. If the kernel client is used, instead of the fuse client, a pre-existing subdirectory of CephFS can be mounted instead of the root.\n\n#### Actions\n\n- :mount - mount CephFS\n- :umount - unmount CephFS\n- :remount - remount CephFS\n- :enable - adds an fstab entry to mount CephFS\n- :disable - removes an fstab entry to mount CephFS\n\n#### Parameters\n\n- :directory - name attribute. Where to mount CephFS in the local filesystem\n- :use\\_fuse - whether to use ceph-fuse or the kernel client to mount the filesystem. ceph-fuse is updated more often, but the kernel client allows for subdirectory mounting. Defaults to true\n- :cephfs\\_subdir - which CephFS subdirectory to mount. Defaults to '/'. An exception will be thrown if this option is set to anything other than '/' if use\\_fuse is also true\n\n### ceph\\_pool\n\nThe ceph\\_pool LWRP provides an easy way to create and delete Ceph pools.\n\nIt assumes that connectivity to the cluster is setup and that admin credentials are available from default locations, e.g. /etc/ceph/ceph.client.admin.keyring.\n\n#### Actions\n\n- :add - creates a pool with the given number of placement groups\n- :delete - deletes an existing pool\n\n#### Parameters\n\n- :name - the name of the pool to create or delete\n- :pg_num - number of placement groups, when creating a new pool\n- :create_options - arguments for pool creation (optional)\n- :force - force the deletion of an exiting pool along with any data that is stored in it\n\n## DEVELOPING\n\n### Style Guide\n\nThis cookbook requires a style guide for all contributions. Travis will automatically verify that every Pull Request follows the style guide.\n\n1. Install [ChefDK](http://downloads.chef.io/chef-dk/)\n2. Activate ChefDK's copy of ruby: `eval \"$(chef shell-init bash)\"`\n3. `bundle install`\n4. `bundle exec rake style`\n\n### Testing\n\nThis cookbook uses Test Kitchen to verify functionality. A Pull Request can't be merged if it causes any of the test configurations to fail.\n\n1. Install [ChefDK](http://downloads.chef.io/chef-dk/)\n2. Activate ChefDK's copy of ruby: `eval \"$(chef shell-init bash)\"`\n3. `bundle install`\n4. `bundle exec kitchen test aio-debian-74`\n5. `bundle exec kitchen test aio-ubuntu-1204`\n6. `bundle exec kitchen test aio-ubuntu-1404`\n\n## AUTHORS\n* Author: Hans Chris Jones <chris.jones@lambdastack.io>\nNOTE: No longer with Bloomberg - * Author: Hans Chris Jones <cjones303@bloomberg.net>\nActively maintained by Hans Chris Jones, others from Bloomberg, Red Hat and other firms\n\n## CONTRIBUTIONS\nWe welcome ALL contributions:\n1. Fork\n2. Create a feature branch\n3. Make changes\n4. Test\n5. Make Pull Request\n\nYour changes will be tested and if all goes well it will be merged - Thanks!!\n", "release_dates": []}, {"name": "chatgpt-retrieval-plugin", "description": "The ChatGPT Retrieval Plugin lets you easily find personal or work documents by asking questions in natural language.", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# ChatGPT Retrieval Plugin\n\nBuild Custom GPTs with a Retrieval Plugin backend to give ChatGPT access to personal documents.\n![Example Custom GPT Screenshot](/assets/example.png)\n\n## Introduction\n\nThe ChatGPT Retrieval Plugin repository provides a flexible solution for semantic search and retrieval of personal or organizational documents using natural language queries. It is a standalone retrieval backend, and can be used with [ChatGPT custom GPTs](https://chat.openai.com/gpts/discovery), [function calling](https://platform.openai.com/docs/guides/function-calling) with the [chat completions](https://platform.openai.com/docs/guides/text-generation) or [assistants APIs](https://platform.openai.com/docs/assistants/overview), or with the [ChatGPT plugins model (deprecated)](https://chat.openai.com/?model=gpt-4-plugins). ChatGPT and the Assistants API both natively support retrieval from uploaded files, so you should use the Retrieval Plugin as a backend only if you want more granular control of your retrieval system (e.g. document text chunk length, embedding model / size, etc.).\n\nThe repository is organized into several directories:\n\n| Directory                       | Description                                                                                                                |\n| ------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |\n| [`datastore`](/datastore)       | Contains the core logic for storing and querying document embeddings using various vector database providers.              |\n| [`docs`](/docs)                 | Includes documentation for setting up and using each vector database provider, webhooks, and removing unused dependencies. |\n| [`examples`](/examples)         | Provides example configurations, authentication methods, and provider-specific examples.                                   |\n| [`local_server`](/local_server) | Contains an implementation of the Retrieval Plugin configured for localhost testing.                                       |\n| [`models`](/models)             | Contains the data models used by the plugin, such as document and metadata models.                                         |\n| [`scripts`](/scripts)           | Offers scripts for processing and uploading documents from different data sources.                                         |\n| [`server`](/server)             | Houses the main FastAPI server implementation.                                                                             |\n| [`services`](/services)         | Contains utility services for tasks like chunking, metadata extraction, and PII detection.                                 |\n| [`tests`](/tests)               | Includes integration tests for various vector database providers.                                                          |\n| [`.well-known`](/.well-known)   | Stores the plugin manifest file and OpenAPI schema, which define the plugin configuration and API specification.           |\n\nThis README provides detailed information on how to set up, develop, and deploy the ChatGPT Retrieval Plugin (stand-alone retrieval backend).\n\n## Table of Contents\n\n- [Quickstart](#quickstart)\n- [About](#about)\n  - [Retrieval Plugin](#retrieval-plugin)\n  - [Retrieval Plugin with custom GPTs](#retrieval-plugin-with-custom-gpts)\n  - [Retrieval Plugin with function calling](#retrieval-plugin-with-function-calling)\n  - [Retrieval Plugin with the plugins model (deprecated)](#chatgpt-plugins-model)\n  - [API Endpoints](#api-endpoints)\n  - [Memory Feature](#memory-feature)\n  - [Security](#security)\n  - [Choosing an Embeddings Model](#choosing-an-embeddings-model)\n- [Development](#development)\n  - [Setup](#setup)\n    - [General Environment Variables](#general-environment-variables)\n  - [Choosing a Vector Database](#choosing-a-vector-database)\n    - [Pinecone](#pinecone)\n    - [Elasticsearch](#elasticsearch)\n    - [Weaviate](#weaviate)\n    - [Zilliz](#zilliz)\n    - [Milvus](#milvus)\n    - [Qdrant](#qdrant)\n    - [Redis](#redis)\n    - [Llama Index](#llamaindex)\n    - [Chroma](#chroma)\n    - [Azure Cognitive Search](#azure-cognitive-search)\n    - [Azure CosmosDB Mongo vCore](#azure-cosmosdb-mongo-vcore)\n    - [Supabase](#supabase)\n    - [Postgres](#postgres)\n    - [AnalyticDB](#analyticdb)\n  - [Running the API Locally](#running-the-api-locally)\n  - [Personalization](#personalization)\n  - [Authentication Methods](#authentication-methods)\n- [Deployment](#deployment)\n- [Webhooks](#webhooks)\n- [Scripts](#scripts)\n- [Limitations](#limitations)\n- [Contributors](#contributors)\n- [Future Directions](#future-directions)\n\n## Quickstart\n\nFollow these steps to quickly set up and run the ChatGPT Retrieval Plugin:\n\n1. Install Python 3.10, if not already installed.\n2. Clone the repository: `git clone https://github.com/openai/chatgpt-retrieval-plugin.git`\n3. Navigate to the cloned repository directory: `cd /path/to/chatgpt-retrieval-plugin`\n4. Install poetry: `pip install poetry`\n5. Create a new virtual environment with Python 3.10: `poetry env use python3.10`\n6. Activate the virtual environment: `poetry shell`\n7. Install app dependencies: `poetry install`\n8. Create a [bearer token](#general-environment-variables)\n9. Set the required environment variables:\n\n   ```\n   export DATASTORE=<your_datastore>\n   export BEARER_TOKEN=<your_bearer_token>\n   export OPENAI_API_KEY=<your_openai_api_key>\n   export EMBEDDING_DIMENSION=256 # edit this value based on the dimension of the embeddings you want to use\n   export EMBEDDING_MODEL=text-embedding-3-large # edit this based on your model preference, e.g. text-embedding-3-small, text-embedding-ada-002\n\n   # Optional environment variables used when running Azure OpenAI\n   export OPENAI_API_BASE=https://<AzureOpenAIName>.openai.azure.com/\n   export OPENAI_API_TYPE=azure\n   export OPENAI_EMBEDDINGMODEL_DEPLOYMENTID=<Name of embedding model deployment>\n   export OPENAI_METADATA_EXTRACTIONMODEL_DEPLOYMENTID=<Name of deployment of model for metatdata>\n   export OPENAI_COMPLETIONMODEL_DEPLOYMENTID=<Name of general model deployment used for completion>\n   export OPENAI_EMBEDDING_BATCH_SIZE=<Batch size of embedding, for AzureOAI, this value need to be set as 1>\n\n   # Add the environment variables for your chosen vector DB.\n   # Some of these are optional; read the provider's setup docs in /docs/providers for more information.\n\n   # Pinecone\n   export PINECONE_API_KEY=<your_pinecone_api_key>\n   export PINECONE_ENVIRONMENT=<your_pinecone_environment>\n   export PINECONE_INDEX=<your_pinecone_index>\n\n   # Weaviate\n   export WEAVIATE_URL=<your_weaviate_instance_url>\n   export WEAVIATE_API_KEY=<your_api_key_for_WCS>\n   export WEAVIATE_CLASS=<your_optional_weaviate_class>\n\n   # Zilliz\n   export ZILLIZ_COLLECTION=<your_zilliz_collection>\n   export ZILLIZ_URI=<your_zilliz_uri>\n   export ZILLIZ_USER=<your_zilliz_username>\n   export ZILLIZ_PASSWORD=<your_zilliz_password>\n\n   # Milvus\n   export MILVUS_COLLECTION=<your_milvus_collection>\n   export MILVUS_HOST=<your_milvus_host>\n   export MILVUS_PORT=<your_milvus_port>\n   export MILVUS_USER=<your_milvus_username>\n   export MILVUS_PASSWORD=<your_milvus_password>\n\n   # Qdrant\n   export QDRANT_URL=<your_qdrant_url>\n   export QDRANT_PORT=<your_qdrant_port>\n   export QDRANT_GRPC_PORT=<your_qdrant_grpc_port>\n   export QDRANT_API_KEY=<your_qdrant_api_key>\n   export QDRANT_COLLECTION=<your_qdrant_collection>\n\n   # AnalyticDB\n   export PG_HOST=<your_analyticdb_host>\n   export PG_PORT=<your_analyticdb_port>\n   export PG_USER=<your_analyticdb_username>\n   export PG_PASSWORD=<your_analyticdb_password>\n   export PG_DATABASE=<your_analyticdb_database>\n   export PG_COLLECTION=<your_analyticdb_collection>\n\n\n   # Redis\n   export REDIS_HOST=<your_redis_host>\n   export REDIS_PORT=<your_redis_port>\n   export REDIS_PASSWORD=<your_redis_password>\n   export REDIS_INDEX_NAME=<your_redis_index_name>\n   export REDIS_DOC_PREFIX=<your_redis_doc_prefix>\n   export REDIS_DISTANCE_METRIC=<your_redis_distance_metric>\n   export REDIS_INDEX_TYPE=<your_redis_index_type>\n\n   # Llama\n   export LLAMA_INDEX_TYPE=<gpt_vector_index_type>\n   export LLAMA_INDEX_JSON_PATH=<path_to_saved_index_json_file>\n   export LLAMA_QUERY_KWARGS_JSON_PATH=<path_to_saved_query_kwargs_json_file>\n   export LLAMA_RESPONSE_MODE=<response_mode_for_query>\n\n   # Chroma\n   export CHROMA_COLLECTION=<your_chroma_collection>\n   export CHROMA_IN_MEMORY=<true_or_false>\n   export CHROMA_PERSISTENCE_DIR=<your_chroma_persistence_directory>\n   export CHROMA_HOST=<your_chroma_host>\n   export CHROMA_PORT=<your_chroma_port>\n\n   # Azure Cognitive Search\n   export AZURESEARCH_SERVICE=<your_search_service_name>\n   export AZURESEARCH_INDEX=<your_search_index_name>\n   export AZURESEARCH_API_KEY=<your_api_key> (optional, uses key-free managed identity if not set)\n\n   # Azure CosmosDB Mongo vCore\n   export AZCOSMOS_API = <your azure cosmos db api, for now it only supports mongo>\n   export AZCOSMOS_CONNSTR = <your azure cosmos db mongo vcore connection string>\n   export AZCOSMOS_DATABASE_NAME = <your mongo database name>\n   export AZCOSMOS_CONTAINER_NAME = <your mongo container name>\n\n   # Supabase\n   export SUPABASE_URL=<supabase_project_url>\n   export SUPABASE_ANON_KEY=<supabase_project_api_anon_key>\n\n   # Postgres\n   export PG_HOST=<postgres_host>\n   export PG_PORT=<postgres_port>\n   export PG_USER=<postgres_user>\n   export PG_PASSWORD=<postgres_password>\n   export PG_DB=<postgres_database>\n\n   # Elasticsearch\n   export ELASTICSEARCH_URL=<elasticsearch_host_and_port> (either specify host or cloud_id)\n   export ELASTICSEARCH_CLOUD_ID=<elasticsearch_cloud_id>\n\n   export ELASTICSEARCH_USERNAME=<elasticsearch_username>\n   export ELASTICSEARCH_PASSWORD=<elasticsearch_password>\n   export ELASTICSEARCH_API_KEY=<elasticsearch_api_key>\n\n   export ELASTICSEARCH_INDEX=<elasticsearch_index_name>\n   export ELASTICSEARCH_REPLICAS=<elasticsearch_replicas>\n   export ELASTICSEARCH_SHARDS=<elasticsearch_shards>\n   ```\n\n10. Run the API locally: `poetry run start`\n11. Access the API documentation at `http://0.0.0.0:8000/docs` and test the API endpoints (make sure to add your bearer token).\n\n## About\n\n### Retrieval Plugin\n\nThis is a standalone retrieval backend that can be used with [ChatGPT custom GPTs](https://chat.openai.com/gpts/discovery), [function calling](https://platform.openai.com/docs/guides/function-calling) with the [chat completions](https://platform.openai.com/docs/guides/text-generation) or [assistants APIs](https://platform.openai.com/docs/assistants/overview), or with the [ChatGPT plugins model (deprecated)](https://chat.openai.com/?model=gpt-4-plugins).\n\nIt enables a model to carry out semantic search and retrieval of personal or organizational documents, and write answers informed by relevent retrieved context (sometimes referred to as \"Retrieval-Augmented Generation\" or \"RAG\"). It allows users to obtain the most relevant document snippets from their data sources, such as files, notes, or emails, by asking questions or expressing needs in natural language. Enterprises can make their internal documents available to their employees through ChatGPT using this plugin.\n\nThe plugin uses OpenAI's embeddings model (`text-embedding-3-large` 256 dimension embeddings by default) to generate embeddings of document chunks, and then stores and queries them using a vector database on the backend. As an open-source and self-hosted solution, developers can deploy their own Retrieval Plugin and register it with ChatGPT. The Retrieval Plugin supports several vector database providers, allowing developers to choose their preferred one from a list.\n\nA FastAPI server exposes the plugin's endpoints for upserting, querying, and deleting documents. Users can refine their search results by using metadata filters by source, date, author, or other criteria. The plugin can be hosted on any cloud platform that supports Docker containers, such as Fly.io, Heroku, Render, or Azure Container Apps. To keep the vector database updated with the latest documents, the plugin can process and store documents from various data sources continuously, using incoming webhooks to the upsert and delete endpoints. Tools like [Zapier](https://zapier.com) or [Make](https://www.make.com) can help configure the webhooks based on events or schedules.\n\n### Retrieval Plugin with Custom GPTs\n\nTo create a custom GPT that can use your Retrieval Plugin for semantic search and retrieval of your documents, and even store new information back to the database, you first need to have deployed a Retrieval Plugin. For detailed instructions on how to do this, please refer to the [Deployment section](#deployment). Once you have your app URL (e.g., `https://your-app-url.com`), take the following steps:\n\n1. Navigate to the create GPT page at `https://chat.openai.com/gpts/editor`.\n2. Follow the standard creation flow to set up your GPT.\n3. Navigate to the \"Configure\" tab. Here, you can manually fill in fields such as name, description, and instructions, or use the smart creator for assistance.\n4. Under the \"Actions\" section, click on \"Create new action\".\n5. Choose an authentication method. The Retrieval Plugin supports None, API key (Basic or Bearer) and OAuth. For more information on these methods, refer to the [Authentication Methods Section](#authentication-methods).\n6. Import the OpenAPI schema. You can either:\n   - Import directly from the OpenAPI schema hosted in your app at `https://your-app-url.com/.well-known/openapi.yaml`.\n   - Copy and paste the contents of [this file](/.well-known/openapi.yaml) into the Schema input area if you only want to expose the query endpoint to the GPT. Remember to change the URL under the `-servers` section of the OpenAPI schema you paste in.\n7. Optionally, you might want to add a fetch endpoint. This would involve editing the [`/server/main.py`](/server/main.py) file to add an endpoint and implement this for your chosen vector database. If you make this change, please consider contributing it back to the project by opening a pull request! Adding the fetch endpoint to the OpenAPI schema would allow the model to fetch more content from a document by ID if some text is cut off in the retrieved result. It might also be useful to pass in a string with the text from the retrieved result and an option to return a fixed length of context before and after the retrieved result.\n8. If you want the GPT to be able to save information back to the vector database, you can give it access to the Retrieval Plugin's `/upsert` endpoint. To do this, copy the contents of [this file](/examples/memory/openapi.yaml) into the schema area. This allows the GPT to store new information it generates or learns during the conversation. More details on this feature can be found at [Memory Feature](#memory-feature) and [in the docs here](/examples/memory).\n\nRemember: ChatGPT and custom GPTs natively support retrieval from uploaded files, so you should use the Retrieval Plugin as a backend only if you want more granular control of your retrieval system (e.g. self-hosting, embedding chunk length, embedding model / size, etc.).\n\n### Retrieval Plugin with Function Calling\n\nThe Retrieval Plugin can be integrated with function calling in both the [Chat Completions API](https://platform.openai.com/docs/guides/function-calling) and the [Assistants API](https://platform.openai.com/docs/assistants/overview). This allows the model to decide when to use your functions (query, fetch, upsert) based on the conversation context.\n\n#### Function Calling with Chat Completions\n\nIn a call to the chat completions API, you can describe functions and have the model generate a JSON object containing arguments to call one or many functions. The latest models (gpt-3.5-turbo-0125 and gpt-4-turbo-preview) have been trained to detect when a function should be called and to respond with JSON that adheres to the function signature.\n\nYou can define the functions for the Retrieval Plugin endpoints and pass them in as tools when you use the Chat Completions API with one of the latest models. The model will then intelligently call the functions. You can use function calling to write queries to your APIs, call the endpoint on the backend, and return the response as a tool message to the model to continue the conversation. The function definitions/schemas and an example can be found [here](/examples/function-calling/).\n\n#### Function Calling with Assistants API\n\nYou can use the same function definitions with the OpenAI [Assistants API](https://platform.openai.com/docs/assistants/overview), specifically the [function calling in tool use](https://platform.openai.com/docs/assistants/tools/function-calling). The Assistants API allows you to build AI assistants within your own applications, leveraging models, tools, and knowledge to respond to user queries. The function definitions/schemas and an example can be found [here](/examples/function-calling/). The Assistants API natively supports retrieval from uploaded files, so you should use the Retrieval Plugin with function calling only if you want more granular control of your retrieval system (e.g. embedding chunk length, embedding model / size, etc.).\n\nParallel function calling is supported for both the Chat Completions API and the Assistants API. This means you can perform multiple tasks, such as querying something and saving something back to the vector database, in the same message.\n\nRead more about function calling with the Retrieval Plugin [here](/examples/function-calling/).\n\n### ChatGPT Plugins Model\n\n(deprecated) We recommend using custom actions with GPTs to make use of the Retrieval Plugin through ChatGPT. Instrucitons for using retrieval with the deprecated plugins model can be found [here](/docs/deprecated/plugins.md).\n\n### API Endpoints\n\nThe Retrieval Plugin is built using FastAPI, a web framework for building APIs with Python. FastAPI allows for easy development, validation, and documentation of API endpoints. Find the FastAPI documentation [here](https://fastapi.tiangolo.com/).\n\nOne of the benefits of using FastAPI is the automatic generation of interactive API documentation with Swagger UI. When the API is running locally, Swagger UI at `<local_host_url i.e. http://0.0.0.0:8000>/docs` can be used to interact with the API endpoints, test their functionality, and view the expected request and response models.\n\nThe plugin exposes the following endpoints for upserting, querying, and deleting documents from the vector database. All requests and responses are in JSON format, and require a valid bearer token as an authorization header.\n\n- `/upsert`: This endpoint allows uploading one or more documents and storing their text and metadata in the vector database. The documents are split into chunks of around 200 tokens, each with a unique ID. The endpoint expects a list of documents in the request body, each with a `text` field, and optional `id` and `metadata` fields. The `metadata` field can contain the following optional subfields: `source`, `source_id`, `url`, `created_at`, and `author`. The endpoint returns a list of the IDs of the inserted documents (an ID is generated if not initially provided).\n\n- `/upsert-file`: This endpoint allows uploading a single file (PDF, TXT, DOCX, PPTX, or MD) and storing its text and metadata in the vector database. The file is converted to plain text and split into chunks of around 200 tokens, each with a unique ID. The endpoint returns a list containing the generated id of the inserted file.\n\n- `/query`: This endpoint allows querying the vector database using one or more natural language queries and optional metadata filters. The endpoint expects a list of queries in the request body, each with a `query` and optional `filter` and `top_k` fields. The `filter` field should contain a subset of the following subfields: `source`, `source_id`, `document_id`, `url`, `created_at`, and `author`. The `top_k` field specifies how many results to return for a given query, and the default value is 3. The endpoint returns a list of objects that each contain a list of the most relevant document chunks for the given query, along with their text, metadata and similarity scores.\n\n- `/delete`: This endpoint allows deleting one or more documents from the vector database using their IDs, a metadata filter, or a delete_all flag. The endpoint expects at least one of the following parameters in the request body: `ids`, `filter`, or `delete_all`. The `ids` parameter should be a list of document IDs to delete; all document chunks for the document with these IDS will be deleted. The `filter` parameter should contain a subset of the following subfields: `source`, `source_id`, `document_id`, `url`, `created_at`, and `author`. The `delete_all` parameter should be a boolean indicating whether to delete all documents from the vector database. The endpoint returns a boolean indicating whether the deletion was successful.\n\nThe detailed specifications and examples of the request and response models can be found by running the app locally and navigating to http://0.0.0.0:8000/openapi.json, or in the OpenAPI schema [here](/.well-known/openapi.yaml). Note that the OpenAPI schema only contains the `/query` endpoint, because that is the only function that ChatGPT needs to access. This way, ChatGPT can use the plugin only to retrieve relevant documents based on natural language queries or needs. However, if developers want to also give ChatGPT the ability to remember things for later, they can use the `/upsert` endpoint to save snippets from the conversation to the vector database. An example of a manifest and OpenAPI schema that gives ChatGPT access to the `/upsert` endpoint can be found [here](/examples/memory).\n\nTo include custom metadata fields, edit the `DocumentMetadata` and `DocumentMetadataFilter` data models [here](/models/models.py), and update the OpenAPI schema [here](/.well-known/openapi.yaml). You can update this easily by running the app locally, copying the JSON found at http://0.0.0.0:8000/sub/openapi.json, and converting it to YAML format with [Swagger Editor](https://editor.swagger.io/). Alternatively, you can replace the `openapi.yaml` file with an `openapi.json` file.\n\n### Memory Feature\n\nA notable feature of the Retrieval Plugin is its capacity to provide ChatGPT with memory. By using the plugin's upsert endpoint, ChatGPT can save snippets from the conversation to the vector database for later reference (only when prompted to do so by the user). This functionality contributes to a more context-aware chat experience by allowing ChatGPT to remember and retrieve information from previous conversations. Learn how to configure the Retrieval Plugin with memory [here](/examples/memory).\n\n### Security\n\nThe Retrieval Plugin allows ChatGPT to search a vector database of content, and then add the best results into the ChatGPT session. This means it doesn\u2019t have any external effects, and the main risk consideration is data authorization and privacy. Developers should only add content into their Retrieval Plugin that they have authorization for and that they are fine with appearing in users\u2019 ChatGPT sessions. You can choose from a number of different authentication methods to secure the plugin (more information [here](#authentication-methods)).\n\n### Choosing an Embeddings Model\n\nThe ChatGPT Retrieval Plugin uses OpenAI's embeddings models to generate embeddings of document chunks. The default model for the Retrieval Plugin is `text-embedding-3-large` with 256 dimensions. OpenAI offers two latest embeddings models, `text-embedding-3-small` and `text-embedding-3-large`, as well as an older model, `text-embedding-ada-002`.\n\nThe new models support shortening embeddings without significant loss of retrieval accuracy, allowing you to balance retrieval accuracy, cost, and speed.\n\nHere's a comparison of the models:\n\n| Model                  | Embedding Size | Average MTEB Score | Cost per 1k Tokens |\n| ---------------------- | -------------- | ------------------ | ------------------ |\n| text-embedding-3-large | 3072           | 64.6%              | $0.00013           |\n| text-embedding-3-large | 1024           | 64.1%              | $0.00013           |\n| text-embedding-3-large | 256            | 62.0%              | $0.00013           |\n| text-embedding-3-small | 1536           | 62.3%              | $0.00002           |\n| text-embedding-3-small | 512            | 61.6%              | $0.00002           |\n| text-embedding-ada-002 | 1536           | 61.0%              | $0.0001            |\n\nWhen choosing a model, consider:\n\n1. **Retrieval Accuracy vs Cost**: `text-embedding-3-large` offers the highest accuracy but at a higher cost. `text-embedding-3-small` is more cost-effective with competitive accuracy. The older `text-embedding-ada-002` model has the lowest accuracy.\n\n2. **Embedding Size**: Larger embeddings provide better accuracy but consume more storage and could be slower to query. You can adjust the size of the embeddings to balance these factors.\n\nFor example, if your vector database supports up to 1024 dimensions, you can use `text-embedding-3-large` and set the dimensions API parameter to 1024. This shortens the embedding from 3072 dimensions, trading off some accuracy for lower storage and query costs.\n\nTo change your chosen embeddings model and size, edit the following environment variables:\n\n```\nEMBEDDING_DIMENSION=256 # edit this value based on the dimension of the embeddings you want to use\nEMBEDDING_MODEL=\"text-embedding-3-large\" # edit this value based on the model you want to use e.g. text-embedding-3-small, text-embedding-ada-002\n```\n\n## Development\n\n### Setup\n\nThis app uses Python 3.10, and [poetry](https://python-poetry.org/) for dependency management.\n\nInstall Python 3.10 on your machine if it isn't already installed. It can be downloaded from the official [Python website](https://www.python.org/downloads/) or with a package manager like `brew` or `apt`, depending on your system.\n\nClone the repository from GitHub:\n\n```\ngit clone https://github.com/openai/chatgpt-retrieval-plugin.git\n```\n\nNavigate to the cloned repository directory:\n\n```\ncd /path/to/chatgpt-retrieval-plugin\n```\n\nInstall poetry:\n\n```\npip install poetry\n```\n\nCreate a new virtual environment that uses Python 3.10:\n\n```\npoetry env use python3.10\npoetry shell\n```\n\nInstall app dependencies using poetry:\n\n```\npoetry install\n```\n\n**Note:** If adding dependencies in the `pyproject.toml`, make sure to run `poetry lock` and `poetry install`.\n\n#### General Environment Variables\n\nThe API requires the following environment variables to work:\n\n| Name             | Required | Description                                                                                                                                                                                                                                                   |\n| ---------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `DATASTORE`      | Yes      | This specifies the vector database provider you want to use to store and query embeddings. You can choose from `elasticsearch`, `chroma`, `pinecone`, `weaviate`, `zilliz`, `milvus`, `qdrant`, `redis`, `azuresearch`, `supabase`, `postgres`, `analyticdb`. |\n| `BEARER_TOKEN`   | Yes      | This is a secret token that you need to authenticate your requests to the API. You can generate one using any tool or method you prefer, such as [jwt.io](https://jwt.io/).                                                                                   |\n| `OPENAI_API_KEY` | Yes      | This is your OpenAI API key that you need to generate embeddings using the one of the OpenAI embeddings model. You can get an API key by creating an account on [OpenAI](https://openai.com/).                                                                |\n\n### Using the plugin with Azure OpenAI\n\nThe Azure Open AI uses URLs that are specific to your resource and references models not by model name but by the deployment id. As a result, you need to set additional environment variables for this case.\n\nIn addition to the `OPENAI_API_BASE` (your specific URL) and `OPENAI_API_TYPE` (azure), you should also set `OPENAI_EMBEDDINGMODEL_DEPLOYMENTID` which specifies the model to use for getting embeddings on upsert and query. For this, we recommend deploying `text-embedding-ada-002` model and using the deployment name here.\n\nIf you wish to use the data preparation scripts, you will also need to set `OPENAI_METADATA_EXTRACTIONMODEL_DEPLOYMENTID`, used for metadata extraction and\n`OPENAI_COMPLETIONMODEL_DEPLOYMENTID`, used for PII handling.\n\n### Choosing a Vector Database\n\nThe plugin supports several vector database providers, each with different features, performance, and pricing. Depending on which one you choose, you will need to use a different Dockerfile and set different environment variables. The following sections provide brief introductions to each vector database provider.\n\nFor more detailed instructions on setting up and using each vector database provider, please refer to the respective documentation in the `/docs/providers/<datastore_name>/setup.md` file ([folders here](/docs/providers)).\n\n#### Pinecone\n\n[Pinecone](https://www.pinecone.io) is a managed vector database designed for speed, scale, and rapid deployment to production. It supports hybrid search and is currently the only datastore to natively support SPLADE sparse vectors. For detailed setup instructions, refer to [`/docs/providers/pinecone/setup.md`](/docs/providers/pinecone/setup.md).\n\n#### Weaviate\n\n[Weaviate](https://weaviate.io/) is an open-source vector search engine built to scale seamlessly into billions of data objects. It supports hybrid search out-of-the-box, making it suitable for users who require efficient keyword searches. Weaviate can be self-hosted or managed, offering flexibility in deployment. For detailed setup instructions, refer to [`/docs/providers/weaviate/setup.md`](/docs/providers/weaviate/setup.md).\n\n#### Zilliz\n\n[Zilliz](https://zilliz.com) is a managed cloud-native vector database designed for billion-scale data. It offers a wide range of features, including multiple indexing algorithms, distance metrics, scalar filtering, time travel searches, rollback with snapshots, full RBAC, 99.9% uptime, separated storage and compute, and multi-language SDKs. For detailed setup instructions, refer to [`/docs/providers/zilliz/setup.md`](/docs/providers/zilliz/setup.md).\n\n#### Milvus\n\n[Milvus](https://milvus.io/) is an open-source, cloud-native vector database that scales to billions of vectors. It is the open-source version of Zilliz and shares many of its features, such as various indexing algorithms, distance metrics, scalar filtering, time travel searches, rollback with snapshots, multi-language SDKs, storage and compute separation, and cloud scalability. For detailed setup instructions, refer to [`/docs/providers/milvus/setup.md`](/docs/providers/milvus/setup.md).\n\n#### Qdrant\n\n[Qdrant](https://qdrant.tech/) is a vector database capable of storing documents and vector embeddings. It offers both self-hosted and managed [Qdrant Cloud](https://cloud.qdrant.io/) deployment options, providing flexibility for users with different requirements. For detailed setup instructions, refer to [`/docs/providers/qdrant/setup.md`](/docs/providers/qdrant/setup.md).\n\n#### Redis\n\n[Redis](https://redis.com/solutions/use-cases/vector-database/) is a real-time data platform suitable for a variety of use cases, including everyday applications and AI/ML workloads. It can be used as a low-latency vector engine by creating a Redis database with the [Redis Stack docker container](/examples/docker/redis/docker-compose.yml). For a hosted/managed solution, [Redis Cloud](https://app.redislabs.com/#/) is available. For detailed setup instructions, refer to [`/docs/providers/redis/setup.md`](/docs/providers/redis/setup.md).\n\n#### LlamaIndex\n\n[LlamaIndex](https://github.com/jerryjliu/llama_index) is a central interface to connect your LLM's with external data.\nIt provides a suite of in-memory indices over your unstructured and structured data for use with ChatGPT.\nUnlike standard vector databases, LlamaIndex supports a wide range of indexing strategies (e.g. tree, keyword table, knowledge graph) optimized for different use-cases.\nIt is light-weight, easy-to-use, and requires no additional deployment.\nAll you need to do is specifying a few environment variables (optionally point to an existing saved Index json file).\nNote that metadata filters in queries are not yet supported.\nFor detailed setup instructions, refer to [`/docs/providers/llama/setup.md`](/docs/providers/llama/setup.md).\n\n#### Chroma\n\n[Chroma](https://trychroma.com) is an AI-native open-source embedding database designed to make getting started as easy as possible. Chroma runs in-memory, or in a client-server setup. It supports metadata and keyword filtering out of the box. For detailed instructions, refer to [`/docs/providers/chroma/setup.md`](/docs/providers/chroma/setup.md).\n\n#### Azure Cognitive Search\n\n[Azure Cognitive Search](https://azure.microsoft.com/products/search/) is a complete retrieval cloud service that supports vector search, text search, and hybrid (vectors + text combined to yield the best of the two approaches). It also offers an [optional L2 re-ranking step](https://learn.microsoft.com/azure/search/semantic-search-overview) to further improve results quality. For detailed setup instructions, refer to [`/docs/providers/azuresearch/setup.md`](/docs/providers/azuresearch/setup.md)\n\n#### Azure CosmosDB Mongo vCore\n\n[Azure CosmosDB Mongo vCore](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/) supports vector search on embeddings, and it could be used to seamlessly integrate your AI-based applications with your data stored in the Azure CosmosDB. For detailed instructions, refer to [`/docs/providers/azurecosmosdb/setup.md`](/docs/providers/azurecosmosdb/setup.md)\n\n#### Supabase\n\n[Supabase](https://supabase.com/blog/openai-embeddings-postgres-vector) offers an easy and efficient way to store vectors via [pgvector](https://github.com/pgvector/pgvector) extension for Postgres Database. [You can use Supabase CLI](https://github.com/supabase/cli) to set up a whole Supabase stack locally or in the cloud or you can also use docker-compose, k8s and other options available. For a hosted/managed solution, try [Supabase.com](https://supabase.com/) and unlock the full power of Postgres with built-in authentication, storage, auto APIs, and Realtime features. For detailed setup instructions, refer to [`/docs/providers/supabase/setup.md`](/docs/providers/supabase/setup.md).\n\n#### Postgres\n\n[Postgres](https://www.postgresql.org) offers an easy and efficient way to store vectors via [pgvector](https://github.com/pgvector/pgvector) extension. To use pgvector, you will need to set up a PostgreSQL database with the pgvector extension enabled. For example, you can [use docker](https://www.docker.com/blog/how-to-use-the-postgres-docker-official-image/) to run locally. For a hosted/managed solution, you can use any of the cloud vendors which support [pgvector](https://github.com/pgvector/pgvector#hosted-postgres). For detailed setup instructions, refer to [`/docs/providers/postgres/setup.md`](/docs/providers/postgres/setup.md).\n\n#### AnalyticDB\n\n[AnalyticDB](https://www.alibabacloud.com/help/en/analyticdb-for-postgresql/latest/product-introduction-overview) is a distributed cloud-native vector database designed for storing documents and vector embeddings. It is fully compatible with PostgreSQL syntax and managed by Alibaba Cloud. AnalyticDB offers a powerful vector compute engine, processing billions of data vectors and providing features such as indexing algorithms, structured and unstructured data capabilities, real-time updates, distance metrics, scalar filtering, and time travel searches. For detailed setup instructions, refer to [`/docs/providers/analyticdb/setup.md`](/docs/providers/analyticdb/setup.md).\n\n#### Elasticsearch\n\n[Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html) currently supports storing vectors through the `dense_vector` field type and uses them to calculate document scores. Elasticsearch 8.0 builds on this functionality to support fast, approximate nearest neighbor search (ANN). This represents a much more scalable approach, allowing vector search to run efficiently on large datasets. For detailed setup instructions, refer to [`/docs/providers/elasticsearch/setup.md`](/docs/providers/elasticsearch/setup.md).\n\n### Running the API locally\n\nTo run the API locally, you first need to set the requisite environment variables with the `export` command:\n\n```\nexport DATASTORE=<your_datastore>\nexport BEARER_TOKEN=<your_bearer_token>\nexport OPENAI_API_KEY=<your_openai_api_key>\n<Add the environment variables for your chosen vector DB here>\n```\n\nStart the API with:\n\n```\npoetry run start\n```\n\nAppend `docs` to the URL shown in the terminal and open it in a browser to access the API documentation and try out the endpoints (i.e. http://0.0.0.0:8000/docs). Make sure to enter your bearer token and test the API endpoints.\n\n**Note:** If you add new dependencies to the pyproject.toml file, you need to run `poetry lock` and `poetry install` to update the lock file and install the new dependencies.\n\n### Personalization\n\nYou can personalize the Retrieval Plugin for your own use case by doing the following:\n\n- **Replace the logo**: Replace the image in [logo.png](/.well-known/logo.png) with your own logo.\n\n- **Edit the data models**: Edit the `DocumentMetadata` and `DocumentMetadataFilter` data models in [models.py](/models/models.py) to add custom metadata fields. Update the OpenAPI schema in [openapi.yaml](/.well-known/openapi.yaml) accordingly. To update the OpenAPI schema more easily, you can run the app locally, then navigate to `http://0.0.0.0:8000/sub/openapi.json` and copy the contents of the webpage. Then go to [Swagger Editor](https://editor.swagger.io/) and paste in the JSON to convert it to a YAML format. You could also replace the [openapi.yaml](/.well-known/openapi.yaml) file with an openapi.json file in the [.well-known](/.well-known) folder.\n\n- **Change the plugin name, description, and usage instructions**: Update the plugin name, user-facing description, and usage instructions for the model. You can either edit the descriptions in the [main.py](/server/main.py) file or update the [openapi.yaml](/.well-known/openapi.yaml) file. Follow the same instructions as in the previous step to update the OpenAPI schema.\n\n- **Enable ChatGPT to save information from conversations**: See the instructions in the [memory example folder](/examples/memory).\n\n### Authentication Methods\n\nYou can choose from four options for authenticating requests to your plugin:\n\n1. **No Authentication**: Anyone can add your plugin and use its API without any credentials. This option is suitable if you are only exposing documents that are not sensitive or already public. It provides no security for your data. If using this method, copy the contents of this [main.py](/examples/authentication-methods/no-auth/main.py) into the [actual main.py file](/server/main.py). Example manifest [here](/examples/authentication-methods/no-auth/ai-plugin.json).\n\n2. **HTTP Bearer**: You can use a secret token as a header to authorize requests to your plugin. There are two variants of this option:\n\n   - **User Level** (default for this implementation): Each user who adds your plugin to ChatGPT must provide the bearer token when adding the plugin. You can generate and distribute these tokens using any tool or method you prefer, such as [jwt.io](https://jwt.io/). This method provides better security as each user has to enter the shared access token. If you require a unique access token for each user, you will need to implement this yourself in the [main.py](/server/main.py) file. Example manifest [here](/examples/authentication-methods/user-http/ai-plugin.json).\n\n   - **Service Level**: Anyone can add your plugin and use its API without credentials, but you must add a bearer token when registering the plugin. When you install your plugin, you need to add your bearer token, and will then receive a token from ChatGPT that you must include in your hosted manifest file. Your token will be used by ChatGPT to authorize requests to your plugin on behalf of all users who add it. This method is more convenient for users, but it may be less secure as all users share the same token and do not need to add a token to install the plugin. Example manifest [here](/examples/authentication-methods/service-http/ai-plugin.json).\n\n3. **OAuth**: Users must go through an OAuth flow to add your plugin. You can use an OAuth provider to authenticate users who add your plugin and grant them access to your API. This method offers the highest level of security and control, as users authenticate through a trusted third-party provider. However, you will need to implement the OAuth flow yourself in the [main.py](/server/main.py) file and provide the necessary parameters in your manifest file. Example manifest [here](/examples/authentication-methods/oauth/ai-plugin.json).\n\nConsider the benefits and drawbacks of each authentication method before choosing the one that best suits your use case and security requirements. If you choose to use a method different to the default (User Level HTTP), make sure to update the manifest file [here](/.well-known/ai-plugin.json).\n\n## Deployment\n\nYou can deploy your app to different cloud providers, depending on your preferences and requirements. However, regardless of the provider you choose, you will need to update two files in your app: [openapi.yaml](/.well-known/openapi.yaml) and [ai-plugin.json](/.well-known/ai-plugin.json). As outlined above, these files define the API specification and the AI plugin configuration for your app, respectively. You need to change the url field in both files to match the address of your deployed app.\n\nRender has a 1-click deploy option that automatically updates the url field in both files:\n\n[<img src=\"https://render.com/images/deploy-to-render-button.svg\" alt=\"Deploy to Render\" />](https://render.com/deploy?repo=https://github.com/render-examples/chatgpt-retrieval-plugin/tree/main)\n\nBefore deploying your app, you might want to remove unused dependencies from your [pyproject.toml](/pyproject.toml) file to reduce the size of your app and improve its performance. Depending on the vector database provider you choose, you can remove the packages that are not needed for your specific provider. Refer to the respective documentation in the [`/docs/deployment/removing-unused-dependencies.md`](/docs/deployment/removing-unused-dependencies.md) file for information on removing unused dependencies for each provider.\n\nInstructions:\n\n- [Deploying to Fly.io](/docs/deployment/flyio.md)\n- [Deploying to Heroku](/docs/deployment/heroku.md)\n- [Deploying to Render](/docs/deployment/render.md)\n- [Other Deployment Options](/docs/deployment/other-options.md) (Azure Container Apps, Google Cloud Run, AWS Elastic Container Service, etc.)\n\nOnce you have deployed your app, consider uploading an initial batch of documents using one of [these scripts](/scripts) or by calling the `/upsert` endpoint.\n\n## Webhooks\n\nTo keep the documents stored in the vector database up-to-date, consider using tools like [Zapier](https://zapier.com) or [Make](https://www.make.com) to configure incoming webhooks to your plugin's API based on events or schedules. For example, this could allow you to sync new information as you update your notes or receive emails. You can also use a [Zapier Transfer](https://zapier.com/blog/zapier-transfer-guide/) to batch process a collection of existing documents and upload them to the vector database.\n\nIf you need to pass custom fields from these tools to your plugin, you might want to create an additional Retrieval Plugin API endpoint that calls the datastore's upsert function, such as `upsert-email`. This custom endpoint can be designed to accept specific fields from the webhook and process them accordingly.\n\nTo set up an incoming webhook, follow these general steps:\n\n- Choose a webhook tool like Zapier or Make and create an account.\n- Set up a new webhook or transfer in the tool, and configure it to trigger based on events or schedules.\n- Specify the target URL for the webhook, which should be the API endpoint of your Retrieval Plugin (e.g. `https://your-plugin-url.com/upsert`).\n- Configure the webhook payload to include the necessary data fields and format them according to your Retrieval Plugin's API requirements.\n- Test the webhook to ensure it's working correctly and sending data to your Retrieval Plugin as expected.\n\nAfter setting up the webhook, you may want to run a backfill to ensure that any previously missed data is included in the vector database.\n\nRemember that if you want to use incoming webhooks to continuously sync data, you should consider running a backfill after setting these up to avoid missing any data.\n\nIn addition to using tools like Zapier and Make, you can also build your own custom integrations to sync data with your Retrieval Plugin. This allows you to have more control over the data flow and tailor the integration to your specific needs and requirements.\n\n## Scripts\n\nThe `scripts` folder contains scripts to batch upsert or process text documents from different data sources, such as a zip file, JSON file, or JSONL file. These scripts use the plugin's upsert utility functions to upload the documents and their metadata to the vector database, after converting them to plain text and splitting them into chunks. Each script folder has a README file that explains how to use it and what parameters it requires. You can also optionally screen the documents for personally identifiable information (PII) using a language model and skip them if detected, with the [`services.pii_detection`](/services/pii_detection.py) module. This can be helpful if you want to avoid uploading sensitive or private documents to the vector database unintentionally. Additionally, you can optionally extract metadata from the document text using a language model, with the [`services.extract_metadata`](/services/extract_metadata.py) module. This can be useful if you want to enrich the document metadata. **Note:** if using incoming webhooks to continuously sync data, consider running a backfill after setting these up to avoid missing any data.\n\nThe scripts are:\n\n- [`process_json`](scripts/process_json/): This script processes a file dump of documents in a JSON format and stores them in the vector database with some metadata. The format of the JSON file should be a list of JSON objects, where each object represents a document. The JSON object should have a `text` field and optionally other fields to populate the metadata. You can provide custom metadata as a JSON string and flags to screen for PII and extract metadata.\n- [`process_jsonl`](scripts/process_jsonl/): This script processes a file dump of documents in a JSONL format and stores them in the vector database with some metadata. The format of the JSONL file should be a newline-delimited JSON file, where each line is a valid JSON object representing a document. The JSON object should have a `text` field and optionally other fields to populate the metadata. You can provide custom metadata as a JSON string and flags to screen for PII and extract metadata.\n- [`process_zip`](scripts/process_zip/): This script processes a file dump of documents in a zip file and stores them in the vector database with some metadata. The format of the zip file should be a flat zip file folder of docx, pdf, txt, md, pptx or csv files. You can provide custom metadata as a JSON string and flags to screen for PII and extract metadata.\n\n## Pull Request (PR) Checklist\n\nIf you'd like to contribute, please follow the checklist below when submitting a PR. This will help us review and merge your changes faster! Thank you for contributing!\n\n1. **Type of PR**: Indicate the type of PR by adding a label in square brackets at the beginning of the title, such as `[Bugfix]`, `[Feature]`, `[Enhancement]`, `[Refactor]`, or `[Documentation]`.\n\n2. **Short Description**: Provide a brief, informative description of the PR that explains the changes made.\n\n3. **Issue(s) Linked**: Mention any related issue(s) by using the keyword `Fixes` or `Closes` followed by the respective issue number(s) (e.g., Fixes #123, Closes #456).\n\n4. **Branch**: Ensure that you have created a new branch for the changes, and it is based on the latest version of the `main` branch.\n\n5. **Code Changes**: Make sure the code changes are minimal, focused, and relevant to the issue or feature being addressed.\n\n6. **Commit Messages**: Write clear and concise commit messages that explain the purpose of each commit.\n\n7. **Tests**: Include unit tests and/or integration tests for any new code or changes to existing code. Make sure all tests pass before submitting the PR.\n\n8. **Documentation**: Update relevant documentation (e.g., README, inline comments, or external documentation) to reflect any changes made.\n\n9. **Review Requested**: Request a review from at least one other contributor or maintainer of the repository.\n\n10. **Video Submission** (For Complex/Large PRs): If your PR introduces significant changes, complexities, or a large number of lines of code, submit a brief video walkthrough along with the PR. The video should explain the purpose of the changes, the logic behind them, and how they address the issue or add the proposed feature. This will help reviewers to better understand your contribution and expedite the review process.\n\n## Pull Request Naming Convention\n\nUse the following naming convention for your PR branches:\n\n```\n<type>/<short-description>-<issue-number>\n```\n\n- `<type>`: The type of PR, such as `bugfix`, `feature`, `enhancement`, `refactor`, or `docs`. Multiple types are ok and should appear as <type>, <type2>\n- `<short-description>`: A brief description of the changes made, using hyphens to separate words.\n- `<issue-number>`: The issue number associated with the changes made (if applicable).\n\nExample:\n\n```\nfeature/advanced-chunking-strategy-123\n```\n\n## Limitations\n\nWhile the ChatGPT Retrieval Plugin is designed to provide a flexible solution for semantic search and retrieval, it does have some limitations:\n\n- **Keyword search limitations**: The embeddings generated by the chosen OpenAI embeddings model may not always be effective at capturing exact keyword matches. As a result, the plugin might not return the most relevant results for queries that rely heavily on specific keywords. Some vector databases, like Elasticsearch, Pinecone, Weaviate and Azure Cognitive Search, use hybrid search and might perform better for keyword searches.\n- **Sensitive data handling**: The plugin does not automatically detect or filter sensitive data. It is the responsibility of the developers to ensure that they have the necessary authorization to include content in the Retrieval Plugin and that the content complies with data privacy requirements.\n- **Scalability**: The performance of the plugin may vary depending on the chosen vector database provider and the size of the dataset. Some providers may offer better scalability and performance than others.\n- **Metadata extraction**: The optional metadata extraction feature relies on a language model to extract information from the document text. This process may not always be accurate, and the quality of the extracted metadata may vary depending on the document content and structure.\n- **PII detection**: The optional PII detection feature is not foolproof and may not catch all instances of personally identifiable information. Use this feature with caution and verify its effectiveness for your specific use case.\n\n## Future Directions\n\nThe ChatGPT Retrieval Plugin provides a flexible solution for semantic search and retrieval, but there is always potential for further development. We encourage users to contribute to the project by submitting pull requests for new features or enhancements. Notable contributions may be acknowledged with OpenAI credits.\n\nSome ideas for future directions include:\n\n- **More vector database providers**: If you are interested in integrating another vector database provider with the ChatGPT Retrieval Plugin, feel free to submit an implementation.\n- **Additional scripts**: Expanding the range of scripts available for processing and uploading documents from various data sources would make the plugin even more versatile.\n- **User Interface**: Developing a user interface for managing documents and interacting with the plugin could improve the user experience.\n- **Hybrid search / TF-IDF option**: Enhancing the [datastore's upsert function](/datastore/datastore.py#L18) with an option to use hybrid search or TF-IDF indexing could improve the plugin's performance for keyword-based queries.\n- **Advanced chunking strategies and embeddings calculations**: Implementing more sophisticated chunking strategies and embeddings calculations, such as embedding document titles and summaries, performing weighted averaging of document chunks and summaries, or calculating the average embedding for a document, could lead to better search results.\n- **Custom metadata**: Allowing users to add custom metadata to document chunks, such as titles or other relevant information, might improve the retrieved results in some use cases.\n- **Additional optional services**: Integrating more optional services, such as summarizing documents or pre-processing documents before embedding them, could enhance the plugin's functionality and quality of retrieved results. These services could be implemented using language models and integrated directly into the plugin, rather than just being available in the scripts.\n\nWe welcome contributions from the community to help improve the ChatGPT Retrieval Plugin and expand its capabilities. If you have an idea or feature you'd like to contribute, please submit a pull request to the repository.\n\n## Contributors\n\nWe would like to extend our gratitude to the following contributors for their code / documentation contributions, and support in integrating various vector database providers with the ChatGPT Retrieval Plugin:\n\n- [Pinecone](https://www.pinecone.io/)\n  - [acatav](https://github.com/acatav)\n  - [gkogan](https://github.com/gkogan)\n  - [jamescalam](https://github.com/jamescalam)\n- [Weaviate](https://www.semi.technology/)\n  - [byronvoorbach](https://github.com/byronvoorbach)\n  - [hsm207](https://github.com/hsm207)\n  - [sebawita](https://github.com/sebawita)\n- [Zilliz](https://zilliz.com/)\n  - [filip-halt](https://github.com/filip-halt)\n- [Milvus](https://milvus.io/)\n  - [filip-halt](https://github.com/filip-halt)\n- [Qdrant](https://qdrant.tech/)\n  - [kacperlukawski](https://github.com/kacperlukawski)\n- [Redis](https://redis.io/)\n  - [spartee](https://github.com/spartee)\n  - [tylerhutcherson](https://github.com/tylerhutcherson)\n- [LlamaIndex](https://github.com/jerryjliu/llama_index)\n  - [jerryjliu](https://github.com/jerryjliu)\n  - [Disiok](https://github.com/Disiok)\n- [Supabase](https://supabase.com/)\n  - [egor-romanov](https://github.com/egor-romanov)\n- [Postgres](https://www.postgresql.org/)\n  - [egor-romanov](https://github.com/egor-romanov)\n  - [mmmaia](https://github.com/mmmaia)\n- [Elasticsearch](https://www.elastic.co/)\n  - [joemcelroy](https://github.com/joemcelroy)\n", "release_dates": []}, {"name": "chef-cookbook-hostname", "description": "Chef cookbook to set node's hostname and FQDN.", "language": null, "license": null, "readme": "# hostname cookbook\n\n[![Build Status](https://travis-ci.org/3ofcoins/chef-cookbook-hostname.svg?branch=master)](https://travis-ci.org/3ofcoins/chef-cookbook-hostname)\n[![Chef cookbook](https://img.shields.io/cookbook/v/hostname.svg)](https://supermarket.chef.io/cookbooks/hostname)\n## Description\n\nSets hostname and FQDN of the node. The latest code is hosted at\nhttps://github.com/3ofcoins/chef-cookbook-hostname\n\n### Important\n\nSetting hostname on FQDN is not (and won't be) supported. Unfortunately, using dots in the hostname can cause\n[inconsistent results for any system that consumes DNS](http://serverfault.com/questions/229331/can-i-have-dots-in-a-hostname)\nand [is not allowed by RFC952](http://tools.ietf.org/html/rfc952). If a user\nneeds additional info in their shell prompt, they can change PS1 in etc/profile\nto include the FQDN together with any information they find useful (such as\nthe customer, the environment, etc).\n\n## Attributes\n\n- `node['set_fqdn']` - FQDN to set.\n\nThe asterisk character will be replaced with `node.name`. This way,\nyou can add this to base role:\n\n```ruby\ndefault_attributes :set_fqdn => '*.project-domain.com'\n```\n\nand have node set its FQDN and hostname based on its chef node name\n(which is provided on `chef-client` first run's command line).\n\n- `node['hostname_cookbook']['hostsfile_ip']` -- IP used in\n  `/etc/hosts` to correctly set FQDN (default: `127.0.1.1`)\n- `node['hostname_cookbook']['hostsfile_aliases']` -- list of aliases used in\n  `/etc/hosts` for the ip set above (default: [`hostname`])\n- `node['hostname_cookbook']['hostsfile_include_hostname_in_aliases']` -- whether to include the hostname\n  at the end of the aliases list above (default: true)\n\n\n## Recipes\n\n* `hostname::default` -- will set node's FQDN to value of `set_fqdn` attribute,\nand hostname to its host part (up to first dot).\n* `hostname::vmware` -- sets hostname automatically using vmtoolsd.\nYou do not need to set `node[\"set_fqdn\"]`.\n\n## Author\n\nAuthor: Maciej Pasternacki maciej@3ofcoins.net\n", "release_dates": []}, {"name": "chef-logdna_agent", "description": "Installs the LogDNA host agent. Currently only supports apt systems.", "language": "Ruby", "license": null, "readme": "logdna_agent Cookbook\n=====================\nInstalls the LogDNA host agent. Currently only supports apt systems.\n\n\nAttributes\n----------\n\n#### logdna_agent::default\n<table>\n  <tr>\n    <th>Key</th>\n    <th>Type</th>\n    <th>Description</th>\n    <th>Default</th>\n  </tr>\n  <tr>\n    <td><tt>['logdna_agent']['api_key']</tt></td>\n    <td>String</td>\n    <td>Your API key as provided by LogDNA.</td>\n    <td><tt>''</tt></td>\n  </tr>\n  <tr>\n    <td><tt>['logdna_agent']['log_directories']</tt></td>\n    <td>Array[String]</td>\n    <td>Directories to follow (recursively) in addition to <var>/var/log</var>.</td>\n    <td><tt>[]</tt></td>\n  </tr>\n  <tr>\n    <td><tt>['logdna_agent']['tags']</tt></td>\n    <td>String</td>\n    <td>(Optional) Comma-separated list of tags to apply to the host.</td>\n    <td><tt>''</tt></td>\n  </tr>\n</table>\n\nUsage\n-----\n#### logdna_agent::default\nJust include `logdna_agent` in your node's `run_list` and set `api_key` and (optionally)`log_directories`:\n\n```json\n{\n  \"name\":\"my_node\",\n  \"run_list\": [\n    \"recipe[logdna_agent]\"\n  ]\n}\n```\n\n```ruby\nnode.set['logdna_agent']['api_key'] = 'XXXXXXXXXXXXXXXXXXXXXXXXX'\nnode.set['logdna_agent']['log_directories'] = ['/go/bin/']\n```\n", "release_dates": []}, {"name": "CLIP", "description": "CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image", "language": "Jupyter Notebook", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# CLIP\n\n[[Blog]](https://openai.com/blog/clip/) [[Paper]](https://arxiv.org/abs/2103.00020) [[Model Card]](model-card.md) [[Colab]](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb)\n\nCLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. We found CLIP matches the performance of the original ResNet50 on ImageNet \u201czero-shot\u201d without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision.\n\n\n\n## Approach\n\n![CLIP](CLIP.png)\n\n\n\n## Usage\n\nFirst, [install PyTorch 1.7.1](https://pytorch.org/get-started/locally/) (or later) and torchvision, as well as small additional dependencies, and then install this repo as a Python package. On a CUDA GPU machine, the following will do the trick:\n\n```bash\n$ conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n$ pip install ftfy regex tqdm\n$ pip install git+https://github.com/openai/CLIP.git\n```\n\nReplace `cudatoolkit=11.0` above with the appropriate CUDA version on your machine or `cpuonly` when installing on a machine without a GPU.\n\n```python\nimport torch\nimport clip\nfrom PIL import Image\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\nimage = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\ntext = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n\nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    \n    logits_per_image, logits_per_text = model(image, text)\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\nprint(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]\n```\n\n\n## API\n\nThe CLIP module `clip` provides the following methods:\n\n#### `clip.available_models()`\n\nReturns the names of the available CLIP models.\n\n#### `clip.load(name, device=..., jit=False)`\n\nReturns the model and the TorchVision transform needed by the model, specified by the model name returned by `clip.available_models()`. It will download the model as necessary. The `name` argument can also be a path to a local checkpoint.\n\nThe device to run the model can be optionally specified, and the default is to use the first CUDA device if there is any, otherwise the CPU. When `jit` is `False`, a non-JIT version of the model will be loaded.\n\n#### `clip.tokenize(text: Union[str, List[str]], context_length=77)`\n\nReturns a LongTensor containing tokenized sequences of given text input(s). This can be used as the input to the model\n\n---\n\nThe model returned by `clip.load()` supports the following methods:\n\n#### `model.encode_image(image: Tensor)`\n\nGiven a batch of images, returns the image features encoded by the vision portion of the CLIP model.\n\n#### `model.encode_text(text: Tensor)`\n\nGiven a batch of text tokens, returns the text features encoded by the language portion of the CLIP model.\n\n#### `model(image: Tensor, text: Tensor)`\n\nGiven a batch of images and a batch of text tokens, returns two Tensors, containing the logit scores corresponding to each image and text input. The values are cosine similarities between the corresponding image and text features, times 100.\n\n\n\n## More Examples\n\n### Zero-Shot Prediction\n\nThe code below performs zero-shot prediction using CLIP, as shown in Appendix B in the paper. This example takes an image from the [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), and predicts the most likely labels among the 100 textual labels from the dataset.\n\n```python\nimport os\nimport clip\nimport torch\nfrom torchvision.datasets import CIFAR100\n\n# Load the model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load('ViT-B/32', device)\n\n# Download the dataset\ncifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n\n# Prepare the inputs\nimage, class_id = cifar100[3637]\nimage_input = preprocess(image).unsqueeze(0).to(device)\ntext_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n\n# Calculate features\nwith torch.no_grad():\n    image_features = model.encode_image(image_input)\n    text_features = model.encode_text(text_inputs)\n\n# Pick the top 5 most similar labels for the image\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features /= text_features.norm(dim=-1, keepdim=True)\nsimilarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\nvalues, indices = similarity[0].topk(5)\n\n# Print the result\nprint(\"\\nTop predictions:\\n\")\nfor value, index in zip(values, indices):\n    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")\n```\n\nThe output will look like the following (the exact numbers may be slightly different depending on the compute device):\n\n```\nTop predictions:\n\n           snake: 65.31%\n          turtle: 12.29%\n    sweet_pepper: 3.83%\n          lizard: 1.88%\n       crocodile: 1.75%\n```\n\nNote that this example uses the `encode_image()` and `encode_text()` methods that return the encoded features of given inputs.\n\n\n### Linear-probe evaluation\n\nThe example below uses [scikit-learn](https://scikit-learn.org/) to perform logistic regression on image features.\n\n```python\nimport os\nimport clip\nimport torch\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR100\nfrom tqdm import tqdm\n\n# Load the model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load('ViT-B/32', device)\n\n# Load the dataset\nroot = os.path.expanduser(\"~/.cache\")\ntrain = CIFAR100(root, download=True, train=True, transform=preprocess)\ntest = CIFAR100(root, download=True, train=False, transform=preprocess)\n\n\ndef get_features(dataset):\n    all_features = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n            features = model.encode_image(images.to(device))\n\n            all_features.append(features)\n            all_labels.append(labels)\n\n    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n\n# Calculate the image features\ntrain_features, train_labels = get_features(train)\ntest_features, test_labels = get_features(test)\n\n# Perform logistic regression\nclassifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\nclassifier.fit(train_features, train_labels)\n\n# Evaluate using the logistic regression classifier\npredictions = classifier.predict(test_features)\naccuracy = np.mean((test_labels == predictions).astype(float)) * 100.\nprint(f\"Accuracy = {accuracy:.3f}\")\n```\n\nNote that the `C` value should be determined via a hyperparameter sweep using a validation split.\n\n\n## See Also\n\n* [OpenCLIP](https://github.com/mlfoundations/open_clip): includes larger and independently trained CLIP models up to ViT-G/14\n* [Hugging Face implementation of CLIP](https://huggingface.co/docs/transformers/model_doc/clip): for easier integration with the HF ecosystem\n", "release_dates": []}, {"name": "CLIP-featurevis", "description": "code for reproducing some of the diagrams in the paper \"Multimodal Neurons in Artificial Neural Networks\"", "language": "Python", "license": null, "readme": null, "release_dates": []}, {"name": "code-align-evals-data", "description": null, "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# code-align-evals-data\n\n# Repo structure\n\n## Datasets\n\n **human_eval**: Contains original 158 human-written tasks for evaluating capabilities performance. Each file contains a docstring describing a function, a human-written solution to that task, and human-written code to test whether an implementation is correct\n\n**bad_solutions**: Buggy solutions to the first 30 human_eval tasks. These can be put in the context of an eval task and we can measure the decrease in performance\n\n**alignment**: Eval tasks testing alignment, of four different types:\n\n- **bad_contexts**: Human eval tasks with buggy vs non-buggy solutions in the contex\n\n- **find_bug**: The same human_eval tasks, with an example implementation in the docstring, that contains a single bug. The model is asked to print which line contains the bug. The tests check that the model has output the correct line. \n\n- **instruction_following**: Tasks that ask the model to follow instructions to write particular code, along with tests to check the instructions have been followed\n\n- **invalid_use**: Tasks testing invalid use. These are examples of tasks where the desired behaviour is for the model to refuse to generate code. The tests check    that the model has not output any code.   \n\n**robustness**: Eval tasks that test robustness more than alignment\n\n", "release_dates": []}, {"name": "coinrun", "description": "Code for the paper \"Quantifying Transfer in Reinforcement Learning\"", "language": "C++", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Quantifying Generalization in Reinforcement Learning\n\n#### [[Blog Post]](https://blog.openai.com/quantifying-generalization-in-reinforcement-learning/) [[Paper]](https://drive.google.com/file/d/1U1-uufB_ZzQ1HG67BhW9bB8mTJ6JtS19/view)\n\nThis is code for the environments used in the paper [Quantifying Generalization in Reinforcement Learning](https://drive.google.com/file/d/1U1-uufB_ZzQ1HG67BhW9bB8mTJ6JtS19/view) along with an example training script.\n\nAuthors: Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, John Schulman\n\n![CoinRun](coinrun.png?raw=true \"CoinRun\")\n\n## Install\n\nYou should install the package in development mode so you can easily change the files.  You may also want to create a virtualenv before installing since it depends on a specific version of OpenAI baselines.\n\nThis environment has been used on Mac OS X and Ubuntu 16.04 with Python 3.6.\n\n```\n# Linux\napt-get install mpich build-essential qt5-default pkg-config\n# Mac\nbrew install qt open-mpi pkg-config\n\ngit clone https://github.com/openai/coinrun.git\ncd coinrun\npip install tensorflow==1.12.0  # or tensorflow-gpu\npip install -r requirements.txt\npip install -e .\n```\n\nNote that this does not compile the environment, the environment will be compiled when the `coinrun` package is imported.\n\n## Try it out\n\nTry the environment out with the keyboard:\n\n```\npython -m coinrun.interactive\n```\n\nIf this fails, you may be missing a dependency or may need to fix `coinrun/Makefile` for your machine.\n\nTrain an agent using PPO:\n\n```\npython -m coinrun.train_agent --run-id myrun --save-interval 1\n```\n\nAfter each parameter update, this will save a copy of the agent to `./saved_models/`. Results are logged to `/tmp/tensorflow` by default.\n\nRun parallel training using MPI:\n\n```\nmpiexec -np 8 python -m coinrun.train_agent --run-id myrun\n```\n\nTrain an agent on a fixed set of N levels:\n\n```\npython -m coinrun.train_agent --run-id myrun --num-levels N\n```\n\nTrain an agent on the same 500 levels used in the paper:\n\n```\npython -m coinrun.train_agent --run-id myrun --num-levels 500\n```\n\nTrain an agent on a different set of 500 levels:\n\n```\npython -m coinrun.train_agent --run-id myrun --num-levels 500 --set-seed 13\n```\n\nContinue training an agent from a checkpoint:\n\n```\npython -m coinrun.train_agent --run-id newrun --restore-id myrun\n```\n\nEvaluate an agent's final training performance across N parallel environments. Evaluate K levels on each environment (NxK total levels). Default N=20 is reasonable. Evaluation levels will be drawn from the same set as those seen during training.\n\n```\npython -m coinrun.enjoy --train-eval --restore-id myrun -num-eval N -rep K\n```\n\nEvaluate an agent's final test performance on PxNxK distinct levels. All evaluation levels are uniformly sampled from the set of all high difficulty levels. Although we don't explicitly enforce that the test set avoid training levels, the probability of collisions is negligible.\n\n```\nmpiexec -np P python -m coinrun.enjoy --test-eval --restore-id myrun -num-eval N -rep K\n```\n\nRun simultaneous training and testing using MPI. Half the workers will train and half will test.\n\n```\nmpiexec -np 8 python -m coinrun.train_agent --run-id myrun --test\n```\n\nView training options:\n\n```\npython -m coinrun.train_agent --help\n```\n\nWatch a trained agent play a level:\n\n```\npython -m coinrun.enjoy --restore-id myrun --hres\n```\n\nTrain an agent to play RandomMazes:\n\n```\npython train_agent.py --run-id random_mazes --game-type maze --use-lstm 1\n```\n\nTrain an agent to play CoinRun-Platforms, using a larger number of environments to stabilize learning:\n\n```\npython train_agent.py --run-id coinrun_plat --game-type platform --num-envs 96 --use-lstm 1\n```\n\nThere's an example random agent script in [`coinrun/random_agent.py`](coinrun/random_agent.py) which you can run like so:\n\n```\npython -m coinrun.random_agent\n```\n\n## Docker\n\nThere's also a `Dockerfile` to create a CoinRun docker image:\n\n```\ndocker build --tag coinrun .\ndocker run --rm coinrun python -um coinrun.random_agent\n```\n\n## Colab\n\nThere's also a [Colab notebook](https://colab.research.google.com/drive/1e2Eyl8HANzcqPheVBMbdwi3wqDv41kZt) showing how to setup CoinRun.\n\nSee [LICENSES](ASSET_LICENSES.md) for asset license information.\n", "release_dates": []}, {"name": "compose-richtext", "description": "(OpenAI Fork) A collection of Compose libraries for advanced text formatting and alternative display types.", "language": "Kotlin", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# compose-richtext\n\n[![Maven Central](https://img.shields.io/maven-central/v/com.halilibo.compose-richtext/richtext-ui.svg?label=Maven%20Central)](https://search.maven.org/search?q=g:%22com.halilibo.compose-richtext%22)\n[![GitHub license](https://img.shields.io/badge/license-Apache%20License%202.0-blue.svg?style=flat)](https://www.apache.org/licenses/LICENSE-2.0)\n\n> **Warning**\n> compose-richtext library and all its modules are very experimental and undermaintained. The roadmap is unclear at the moment. Thanks for your patience. Fork option is available as always.\n\nA collection of Compose libraries for working with rich text formatting and documents.\n\n`richtext-ui`, `richtext-commonmark`, and `richtext-material-ui` are Kotlin Multiplatform Compose Libraries.\n\nThis repo is currently very experimental and really just proofs-of-concept: there are no tests and some things\nmight be broken or very non-performant.\n\n----\n\n**Documentation is available at [halilibo.com/compose-richtext](https://halilibo.com/compose-richtext).**\n\n----\n\n```kotlin\n@Composable fun App() {\n  val printController = rememberPrintableController()\n\n  Printable(printController) {\n    RichText(Modifier.background(color = Color.White)) {\n      Heading(0, \"Title\")\n      Text(\"Summary paragraph.\")\n\n      HorizontalRule()\n\n      BlockQuote {\n        Text(\"A wise person once said\u2026\")\n      }\n    }\n  }\n\n  Button(onClick = { printController.print(\"README\") }) {\n    Text(\"PRINT ME\")\n  }\n}\n```\n\n## License\n```\nCopyright 2022 Halil Ozercan\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n", "release_dates": []}, {"name": "consistencydecoder", "description": "Consistency Distilled Diff VAE", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Consistency Decoder\n\n[[DALL\u00b7E 3]](https://openai.com/dall-e-3)\n[[Improving Image Generation with Better Captions]](https://cdn.openai.com/papers/dall-e-3.pdf)\n[[Consistency Models]](https://arxiv.org/abs/2303.01469)\n\nImproved decoding for stable diffusion vaes.\n\n## Installation\n\n```\n$ pip install git+https://github.com/openai/consistencydecoder.git\n```\n\n## Usage\n\n```python\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom consistencydecoder import ConsistencyDecoder, save_image, load_image\n\n# encode with stable diffusion vae\npipe = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, device=\"cuda:0\"\n)\npipe.vae.cuda()\ndecoder_consistency = ConsistencyDecoder(device=\"cuda:0\") # Model size: 2.49 GB\n\nimage = load_image(\"assets/gt1.png\", size=(256, 256), center_crop=True)\nlatent = pipe.vae.encode(image.half().cuda()).latent_dist.mean\n\n# decode with gan\nsample_gan = pipe.vae.decode(latent).sample.detach()\nsave_image(sample_gan, \"gan.png\")\n\n# decode with vae\nsample_consistency = decoder_consistency(latent)\nsave_image(sample_consistency, \"con.png\")\n```\n\n## Examples\n Original Image | GAN Decoder | Consistency Decoder |\n:---:|:---:|:---:|\n![Original Image](assets/gt1.png) | ![GAN Image](assets/gan1.png) | ![VAE Image](assets/con1.png) |\n![Original Image](assets/gt2.png) | ![GAN Image](assets/gan2.png) | ![VAE Image](assets/con2.png) |\n![Original Image](assets/gt3.png) | ![GAN Image](assets/gan3.png) | ![VAE Image](assets/con3.png) |\n", "release_dates": []}, {"name": "consistency_models", "description": "Official repo for consistency models.", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Consistency Models\n\nThis repository contains the codebase for [Consistency Models](https://arxiv.org/abs/2303.01469), implemented using PyTorch for conducting large-scale experiments on ImageNet-64, LSUN Bedroom-256, and LSUN Cat-256. We have based our repository on [openai/guided-diffusion](https://github.com/openai/guided-diffusion), which was initially released under the MIT license. Our modifications have enabled support for consistency distillation, consistency training, as well as several sampling and editing algorithms discussed in the paper.\n\nThe repository for CIFAR-10 experiments is in JAX and can be found at [openai/consistency_models_cifar10](https://github.com/openai/consistency_models_cifar10).\n\n# Pre-trained models\n\nWe have released checkpoints for the main models in the paper. Before using these models, please review the corresponding [model card](model-card.md) to understand the intended use and limitations of these models.\n\nHere are the download links for each model checkpoint:\n\n * EDM on ImageNet-64: [edm_imagenet64_ema.pt](https://openaipublic.blob.core.windows.net/consistency/edm_imagenet64_ema.pt)\n * CD on ImageNet-64 with l2 metric: [cd_imagenet64_l2.pt](https://openaipublic.blob.core.windows.net/consistency/cd_imagenet64_l2.pt)\n * CD on ImageNet-64 with LPIPS metric: [cd_imagenet64_lpips.pt](https://openaipublic.blob.core.windows.net/consistency/cd_imagenet64_lpips.pt)\n * CT on ImageNet-64: [ct_imagenet64.pt](https://openaipublic.blob.core.windows.net/consistency/ct_imagenet64.pt)\n * EDM on LSUN Bedroom-256: [edm_bedroom256_ema.pt](https://openaipublic.blob.core.windows.net/consistency/edm_bedroom256_ema.pt)\n * CD on LSUN Bedroom-256 with l2 metric: [cd_bedroom256_l2.pt](https://openaipublic.blob.core.windows.net/consistency/cd_bedroom256_l2.pt)\n * CD on LSUN Bedroom-256 with LPIPS metric: [cd_bedroom256_lpips.pt](https://openaipublic.blob.core.windows.net/consistency/cd_bedroom256_lpips.pt)\n * CT on LSUN Bedroom-256: [ct_bedroom256.pt](https://openaipublic.blob.core.windows.net/consistency/ct_bedroom256.pt)\n * EDM on LSUN Cat-256: [edm_cat256_ema.pt](https://openaipublic.blob.core.windows.net/consistency/edm_cat256_ema.pt)\n * CD on LSUN Cat-256 with l2 metric: [cd_cat256_l2.pt](https://openaipublic.blob.core.windows.net/consistency/cd_cat256_l2.pt)\n * CD on LSUN Cat-256 with LPIPS metric: [cd_cat256_lpips.pt](https://openaipublic.blob.core.windows.net/consistency/cd_cat256_lpips.pt)\n * CT on LSUN Cat-256: [ct_cat256.pt](https://openaipublic.blob.core.windows.net/consistency/ct_cat256.pt)\n\n# Dependencies\n\nTo install all packages in this codebase along with their dependencies, run\n```sh\npip install -e .\n```\n\nTo install with Docker, run the following commands:\n```sh\ncd docker && make build && make run\n```\n\n# Model training and sampling\n\nWe provide examples of EDM training, consistency distillation, consistency training, single-step generation, and multistep generation in [scripts/launch.sh](scripts/launch.sh).\n\n# Evaluations\n\nTo compare different generative models, we use FID, Precision, Recall, and Inception Score. These metrics can all be calculated using batches of samples stored in `.npz` (numpy) files. One can evaluate samples with [cm/evaluations/evaluator.py](evaluations/evaluator.py) in the same way as described in [openai/guided-diffusion](https://github.com/openai/guided-diffusion), with reference dataset batches provided therein.\n\n## Use in \ud83e\udde8 diffusers\n\nConsistency models are supported in [\ud83e\udde8 diffusers](https://github.com/huggingface/diffusers) via the [`ConsistencyModelPipeline` class](https://huggingface.co/docs/diffusers/main/en/api/pipelines/consistency_models). Below we provide an example:\n\n```python\nimport torch\n\nfrom diffusers import ConsistencyModelPipeline\n\ndevice = \"cuda\"\n# Load the cd_imagenet64_l2 checkpoint.\nmodel_id_or_path = \"openai/diffusers-cd_imagenet64_l2\"\npipe = ConsistencyModelPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\n# Onestep Sampling\nimage = pipe(num_inference_steps=1).images[0]\nimage.save(\"consistency_model_onestep_sample.png\")\n\n# Onestep sampling, class-conditional image generation\n# ImageNet-64 class label 145 corresponds to king penguins\n\nclass_id = 145\nclass_id = torch.tensor(class_id, dtype=torch.long)\n\nimage = pipe(num_inference_steps=1, class_labels=class_id).images[0]\nimage.save(\"consistency_model_onestep_sample_penguin.png\")\n\n# Multistep sampling, class-conditional image generation\n# Timesteps can be explicitly specified; the particular timesteps below are from the original Github repo.\n# https://github.com/openai/consistency_models/blob/main/scripts/launch.sh#L77\nimage = pipe(timesteps=[22, 0], class_labels=class_id).images[0]\nimage.save(\"consistency_model_multistep_sample_penguin.png\")\n```\nYou can further speed up the inference process by using `torch.compile()` on `pipe.unet` (only supported from PyTorch 2.0). For more details, please check out the [official documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/consistency_models). This support was contributed to \ud83e\udde8 diffusers by [dg845](https://github.com/dg845) and [ayushtues](https://github.com/ayushtues).\n\n# Citation\n\nIf you find this method and/or code useful, please consider citing\n\n```bibtex\n@article{song2023consistency,\n  title={Consistency Models},\n  author={Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},\n  journal={arXiv preprint arXiv:2303.01469},\n  year={2023},\n}\n```\n", "release_dates": []}, {"name": "consistency_models_cifar10", "description": "Consistency models trained on CIFAR-10, in JAX.", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Consistency Models (CIFAR-10)\n\nThis is the codebase for [Consistency Models](https://arxiv.org/abs/2303.01469), implemented using JAX for our experiments on CIFAR-10. We have based our repository on [yang-song/score_sde](https://github.com/yang-song/score_sde), which was released under the Apache-2.0 license. We have modified the code to streamline diffusion model training, with additional implementations for consistency distillation, consistency training, and various sampling & editing algorithms included in the paper.\n\nFor code and checkpoints for experiments on ImageNet-64, LSUN Bedroom-256, and LSUN Cat-256, check [openai/consistency_models](https://github.com/openai/consistency_models).\n\n# Pre-trained models\n\nWe have released checkpoints for the main models in the paper. Before using these models, please review the corresponding [model card](model-card.md) to understand the intended use and limitations of these models.\n\nHere are the download links for each model checkpoint:\n\n * EDM on CIFAR-10: [edm_cifar10_ema](https://openaipublic.blob.core.windows.net/consistency/jcm_checkpoints/edm_cifar10_ema)\n * CD on CIFAR-10 with l1 metric: [cd-l1](https://openaipublic.blob.core.windows.net/consistency/jcm_checkpoints/cd-l1/checkpoints/checkpoint_80)\n * CD on CIFAR-10 with l2 metric: [cd-l2](https://openaipublic.blob.core.windows.net/consistency/jcm_checkpoints/cd-l2/checkpoints/checkpoint_80)\n * CD on CIFAR-10 with LPIPS metric: [cd-lpips](https://openaipublic.blob.core.windows.net/consistency/jcm_checkpoints/cd-lpips/checkpoints/checkpoint_80)\n * CT on CIFAR-10 with adaptive schedules and LPIPS metric: [ct-lpips](https://openaipublic.blob.core.windows.net/consistency/jcm_checkpoints/ct-lpips/checkpoints/checkpoint_74)\n * Continuous-time CD on CIFAR-10 with l2 metric: [cifar10-continuous-cd-l2](https://openaipublic.blob.core.windows.net/consistency/jcm_checkpoints/cifar10-continuous-cd-l2/checkpoints/checkpoint_40)\n * Continuous-time CD on CIFAR-10 with l2 metric and stopgrad: [cifar10-continuous-cd-l2-stopgrad](https://openaipublic.blob.core.windows.net/consistency/jcm_checkpoints/cifar10-continuous-cd-l2-stopgrad/checkpoints/checkpoint_40)\n * Continuous-time CD on CIFAR-10 with LPIPS metric and stopgrad: [cifar10-continuous-cd-lpips-stopgrad](https://openaipublic.blob.core.windows.net/consistency/jcm_checkpoints/cifar10-continuous-cd-lpips-stopgrad/checkpoints/checkpoint_40)\n * Continuous-time CT on CIFAR-10 with l2 metric: [continuous-ct-l2](https://openaipublic.blob.core.windows.net/consistency/jcm_checkpoints/continuous-ct-l2/checkpoints/checkpoint_80)\n * Continuous-time CT on CIFAR-10 with LPIPS metric: [continuous-ct-lpips](https://openaipublic.blob.core.windows.net/consistency/jcm_checkpoints/continuous-ct-lpips/checkpoints/checkpoint_40)\n\n# Dependencies\n\nTo install all packages in this codebase along with their dependencies, run\n```sh\npip install -e .\n```\nThen manually install jax by running\n```sh\npip install https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.7+cuda11.cudnn82-cp39-cp39-manylinux2014_x86_64.whl\n```\nThe code assumes Python 3.9, JAX 0.4.10, CUDA 11 and cuDNN 8.2. For different Python, CUDA and cuDNN versions, you need to modify the above command accordingly.\n\n# Model training and sampling\n\nWe provide examples of EDM training, consistency distillation, consistency training, single-step generation, and model evaluation in [launch.sh](launch.sh).\n\n# Zero-shot editing\nWe provide examples for multistep generation and zero-shot image editing in [editing_multistep_sampling.ipynb](editing_multistep_sampling.ipynb).\n\n# Citation\n\nIf you find this method and/or code useful, please consider citing\n\n```bibtex\n@article{song2023consistency,\n  title={Consistency Models},\n  author={Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},\n  journal={arXiv preprint arXiv:2303.01469},\n  year={2023},\n}\n```\n\nThis repo is built upon previous work [score_sde](https://github.com/yang-song/score_sde). Please consider citing\n\n```bibtex\n@inproceedings{\n  song2021scorebased,\n  title={Score-Based Generative Modeling through Stochastic Differential Equations},\n  author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},\n  booktitle={International Conference on Learning Representations},\n  year={2021},\n  url={https://openreview.net/forum?id=PxTIG12RRHS}\n}\n```\n", "release_dates": []}, {"name": "consul-helm", "description": "Helm chart to install Consul and other associated components.", "language": null, "license": {"key": "mpl-2.0", "name": "Mozilla Public License 2.0", "spdx_id": "MPL-2.0", "url": "https://api.github.com/licenses/mpl-2.0", "node_id": "MDc6TGljZW5zZTE0"}, "readme": "# Consul Helm Chart\n\nThis repository contains the official HashiCorp Helm chart for installing\nand configuring Consul on Kubernetes. This chart supports multiple use\ncases of Consul on Kubernetes depending on the values provided.\n\nFor full documentation on this Helm chart along with all the ways you can\nuse Consul with Kubernetes, please see the\n[Consul and Kubernetes documentation](https://www.consul.io/docs/platform/k8s/index.html).\n\n## Prerequisites\n\nTo use the charts here, [Helm](https://helm.sh/) must be installed in your\nKubernetes cluster. Setting up Kubernetes and Helm and is outside the scope\nof this README. Please refer to the Kubernetes and Helm documentation.\n\nThe versions required are:\n\n  * **Helm 2.10+** - This is the earliest version of Helm tested. It is possible\n    it works with earlier versions but this chart is untested for those versions.\n  * **Kubernetes 1.9+** - This is the earliest version of Kubernetes tested.\n    It is possible that this chart works with earlier versions but it is\n    untested.\n\n## Usage\n\nFor now, we do not host a chart repository. To use the charts, you must\ndownload this repository and unpack it into a directory. Either\n[download a tagged release](https://github.com/hashicorp/consul-helm/releases) or\nuse `git checkout` to a tagged release.\nAssuming this repository was unpacked into the directory `consul-helm`, the chart can\nthen be installed directly:\n\n    helm install ./consul-helm\n\nPlease see the many options supported in the `values.yaml`\nfile. These are also fully documented directly on the\n[Consul website](https://www.consul.io/docs/platform/k8s/helm.html).\n", "release_dates": []}, {"name": "DALL-E", "description": "PyTorch package for the discrete VAE used for DALL\u00b7E.", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Overview\n\n[[Blog]](https://openai.com/blog/dall-e/) [[Paper]](https://arxiv.org/abs/2102.12092) [[Model Card]](model_card.md) [[Usage]](notebooks/usage.ipynb)\n\nThis is the official PyTorch package for the discrete VAE used for DALL\u00b7E. The transformer used to generate the images from the text is not part of this code release.\n\n# Installation\n\nBefore running [the example notebook](notebooks/usage.ipynb), you will need to install the package using\n\n\tpip install DALL-E\n", "release_dates": []}, {"name": "dalle-2-preview", "description": null, "language": null, "license": null, "readme": null, "release_dates": []}, {"name": "dalle3-eval-samples", "description": "Text-to-image samples collected for the evaluation of DALL-E 3 in the whitepaper.", "language": null, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# DALL-E 3 Evaluation Samples\n\nThis repository contains text-to-image samples collected for the evaluations of DALL-E 3 in the whitepaper. We provide samples not only from DALL-E 3, but from the competitors we compare against in the paper. \n\nThe intent of this repository is to enable researchers in the text-to-image space to reproduce our results and foster forward progress of the text-to-image field as a whole. The samples from this repository are *not* meant to be demonstrations of the DALL-E 3 system.\n\n## Structure\n\nThere are six directories in this repository:\n\n### coco\n\nContains ~32,000 samples from each model derived from ~8,000 captions from the MSCOCO 2014 evaluation set. These samples are intended to be used for CLIP score calculation.\n\n### drawbench\n\nContains 4 samples for each prompt from the [drawbench dataset](https://imagen.research.google/) for each model. In the paper, we evaluate these samples using GPT-4 with Vision and using human raters.\n\n### drawbench_upsampled\n\nContains 4 samples for each prompt in our upsampled drawbench dataset, which was derived using the caption upsampling methodology described in the paper. We evaluate these samples using GPT-4 with Vision.\n\n### prompts\n\nContains the prompts used to generate all of the samples in the other directories. Prompt files are simple text files. The order of the prompts in these files corresponds with the order of the respective image samples.\n\n### t2i_compbench\n\nContains 4 samples for each prompt in the [T2I CompBench evaluation](https://github.com/Karine-Huang/T2I-CompBench). We use the scripts provided with that evaluation to measure the performance of the models in our comparison.\n", "release_dates": []}, {"name": "dallify-discord-bot", "description": "Example code for using OpenAI\u2019s NodeJS SDK with discord.js SDK to create a Discord Bot that uses Slash Commands.", "language": "TypeScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Please read!\n\n\n**For any problems running this specific bot:** [Discord Project Post](https://discord.com/channels/974519864045756446/1039968564699992106)\n\n**For general OpenAI API problems or questions:** [Discord API Discussions](https://discord.com/channels/974519864045756446/1037561178286739466)\n\n**For bugs in the template code:** create an Issue\n\n**For feature requests:** this repo is not accepting feature requests, you can discuss potential features in [Discord Project Post](https://discord.com/channels/974519864045756446/1039968564699992106)\n\n**For PRs:** only bug fix PRs wil be accepted. If you are implementing a new feature, please fork this repo.\n\nThank you!\n\n---\nExample code for running a Discord Bot that uses OpenAI's DALL-E api to generate AI images.\n\nThis bot uses OpenAI's NodeJS SDK, and v14 of discord.js, and is written in Typescript.\n\n\n# Features include:\n\n- draw command to generate images (1 to 9) using the generations endpoint\n- save button to send images to user's DMs\n- reroll button to rerun that generation\n- expand button to zoom out of the image by using the edits (inpaint) endpoint\n\n# Discord Bot setup:\n\nhttps://discordjs.guide/preparations/setting-up-a-bot-application.html\n\nYour bot needs the following bot permissions:\n\n- Send Messages\n- Use Slash Commands\n- Attach Files\n\nUse the invite link in `src/Bot.ts`, which includes the above permissions.\n\n# Secrets setup:\n\n1. Go to `src/utils/constants.ts` and follow the comments to create `src/utils/config.json`\n2. Copy your bot's client id and token into `config.json`\n3. Copy your server's id into `config.json` (https://support.discord.com/hc/en-us/articles/206346498-Where-can-I-find-my-User-Server-Message-ID-)\n4. Copy your OpenAI API key into `config.json` (https://platform.openai.com/api-keys)\n5. Copy your OpenAI Organization ID into `config.json` (https://platform.openai.com/account/organization)\n![img.png](organization_id_location.png)\n# Node server setup:\n\n1. install brew if you don't have it (https://docs.brew.sh/Installation)\n2. `brew install npm` if you don't have it\n3. `npm install` in repo root\n4. `npm run start` in repo root\n\n<img width=\"406\" alt=\"Screen Shot 2022-11-04 at 8 35 39 PM\" src=\"https://user-images.githubusercontent.com/1757898/200099159-d7c01e4d-8f27-4f02-ab76-62e229115edb.png\">\n", "release_dates": []}, {"name": "deeptype", "description": " Code for the paper \"DeepType: Multilingual Entity Linking by Neural Type System Evolution\"", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\nDeepType: Multilingual Entity Linking through Neural Type System Evolution\n--------------------------------------------------------------------------\n\nThis repository contains code necessary for designing, evolving type systems, and training neural type systems. To read more about this technique and our results [see this blog post](https://blog.openai.com/discovering-types-for-entity-disambiguation/) or [read the paper](https://arxiv.org/abs/1802.01021).\n\nAuthors: Jonathan Raiman & Olivier Raiman\n\nOur latest approach to learning symbolic structures from data allows us to discover a set of task specific constraints on a neural network in the form of a type system, to guide its understanding of documents, and obtain state of the art accuracy at [recognizing entities in natural language](https://en.wikipedia.org/wiki/Entity_linking). Recognizing entities in documents can be quite challenging since there are often millions of possible answers. However, when using a type system to constrain the options to only those that semantically \"type check,\" we shrink the answer set and make the problem dramatically easier to solve. Our new results suggest that learning types is a very strong signal for understanding natural language: if types were given to us by an oracle, we find that it is possible to obtain accuracies of 98.6-99% on two benchmark tasks [CoNLL (YAGO)](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/aida/) and the [TAC KBP 2010 challenge](https://pdfs.semanticscholar.org/b7fb/11ef06b0dcdc89ef0a5507c6c9ccea4206d8.pdf).\n\n### Data collection\n\nGet wikiarticle -> wikidata mapping (all languages) + Get anchor tags, redirections, category links, statistics (per language). To store all wikidata ids, their key properties (`instance of`, `part of`, etc..), and\na mapping from all wikipedia article names to a wikidata id do as follows,\nalong with wikipedia anchor tags and links, in three languages: English (en), French (fr), and Spanish (es):\n\n```\nexport DATA_DIR=data/\n./extraction/full_preprocess.sh ${DATA_DIR} en fr es\n```\n\n### Create a type system manually and check oracle accuracy:\n\nTo build a graph projection using a set of rules inside `type_classifier.py`\n(or any Python file containing a `classify` method), and a set of nodes\nthat should not be traversed in `blacklist.json`:\n\n```\nexport LANGUAGE=fr\nexport DATA_DIR=data/\npython3 extraction/project_graph.py ${DATA_DIR}wikidata/ extraction/classifiers/type_classifier.py\n```\n\nTo save a graph projection as a numpy array along with a list of classes to a\ndirectory stored in `CLASSIFICATION_DIR`:\n\n```\nexport LANGUAGE=fr\nexport DATA_DIR=data/\nexport CLASSIFICATION_DIR=data/type_classification\npython3 extraction/project_graph.py ${DATA_DIR}wikidata/ extraction/classifiers/type_classifier.py  --export_classification ${CLASSIFICATION_DIR}\n```\n\nTo use the saved graph projection on wikipedia data to test out how discriminative this\nclassification is (Oracle performance) (edit the config file to make changes to the classification used):\n\n```\nexport DATA_DIR=data/\npython3 extraction/evaluate_type_system.py extraction/configs/en_disambiguator_config_export_small.json --relative_to ${DATA_DIR}\n```\n\n### Obtain learnability scores for types\n\n```bash\nexport DATA_DIR=data/\npython3 extraction/produce_wikidata_tsv.py extraction/configs/en_disambiguator_config_export_small.json --relative_to ${DATA_DIR} sample_data.tsv\npython3 learning/evaluate_learnability.py sample_data.tsv --out report.json --wikidata ${DATA_DIR}wikidata/\n```\n\nSee `learning/LearnabilityStudy.ipynb` for a visual analysis of the AUC scores.\n\n### Evolve a type system\n\n```bash\npython3 extraction/evolve_type_system.py extraction/configs/en_disambiguator_config_export_small.json --relative_to ${DATA_DIR}  --method cem  --penalty 0.00007\n```\nMethod can be `cem`, `greedy`, `beam`, or `ga`, and penalty is the soft constraint on the size of the type system (lambda in the paper).\n\n#### Convert a type system solution into a trainable type classifier\n\nThe output of `evolve_type_system.py` is a set of types (root + relation) that can be used to build a type system. To create a config file that can be used to train an LSTM use the jupyter notebook `extraction/TypeSystemToNeuralTypeSystem.ipynb`.\n\n### Train a type classifier using a type system\n\nFor each language create a training file:\n\n```\nexport LANGUAGE=en\npython3 extraction/produce_wikidata_tsv.py extraction/configs/${LANGUAGE}_disambiguator_config_export.json /Volumes/Samsung_T3/tahiti/2017-12/${LANGUAGE}_train.tsv  --relative_to /Volumes/Samsung_T3/tahiti/2017-12/\n```\n\nThen create an H5 file from each language containing the mapping from tokens to their entity ids in Wikidata:\n\n```\nexport LANGUAGE=en\npython3 extraction/produce_windowed_h5_tsv.py  /Volumes/Samsung_T3/tahiti/2017-12/${LANGUAGE}_train.tsv /Volumes/Samsung_T3/tahiti/2017-12/${LANGUAGE}_train.h5 /Volumes/Samsung_T3/tahiti/2017-12/${LANGUAGE}_dev.h5 --window_size 10  --validation_start 1000000 --total_size 200500000\n```\n\nCreate a training config with all languages, `my_config.json`. Paths to the datasets is relative to config file (e.g. you can place it in the same directory as the dataset h5 files):\n[Note: set `wikidata_path` to where you extracted wikidata information, and `classification_path` to where you exported the classifications with `project_graph.py`]. See learning/configs for a pre written config covering English, French, Spanish, German, and Portuguese.\n\n```\n{\n    \"datasets\": [\n        {\n            \"type\": \"train\",\n            \"path\": \"en_train.h5\",\n            \"x\": 0,\n            \"ignore\": \"other\",\n            \"y\": [\n                {\n                    \"column\": 1,\n                    \"objective\": \"type\",\n                    \"classification\": \"type_classification\"\n                },...\n            ],\n            \"ignore\": \"other\",\n            \"comment\": \"#//#\"\n        },\n        {\n            \"type\": \"dev\",\n            \"path\": \"en_dev.h5\",\n            \"x\": 0,\n            \"ignore\": \"other\",\n            \"y\": [\n                {\n                    \"column\": 1,\n                    \"objective\": \"type\",\n                    \"classification\": \"type_classification\"\n                },...\n            ],\n            \"ignore\": \"other\",\n            \"comment\": \"#//#\"\n        }, ...\n    ],\n    \"features\": [\n        {\n            \"type\": \"word\",\n            \"dimension\": 200,\n            \"max_vocab\": 1000000\n        },...\n    ],\n    \"objectives\": [\n        {\n            \"name\": \"type\",\n            \"type\": \"softmax\",\n            \"vocab\": \"type_classes.txt\"\n        }, ...\n    ],\n    \"wikidata_path\": \"wikidata\",\n    \"classification_path\": \"classifications\"\n}\n```\n\nLaunch training on a single gpu:\n\n```\nCUDA_VISIBLE_DEVICES=0 python3 learning/train_type.py my_config.json --cudnn --fused --hidden_sizes 200 200 --batch_size 256 --max_epochs 10000  --name TypeClassifier --weight_noise 1e-6  --save_dir my_great_model  --anneal_rate 0.9999\n```\n\nSeveral key parameters:\n\n- `name`: main scope for model variables, avoids name clashing when multiple classifiers are loaded\n- `batch_size`: how many examples are used for training simultaneously, can cause out of memory issues\n- `max_epochs`: length of training before auto-stopping. In practice this number should be larger than needed.\n- `fused`: glue all output layers into one, and do a single matrix multiply (recommended).\n- `hidden_sizes`: how many stacks of LSTMs are used, and their sizes (here 2, each with 200 dimensions).\n- `cudnn`: use faster CuDNN kernels for training\n- `anneal_rate`: shrink the learning rate by this amount every 33000 training steps\n- `weight_noise`: sprinkle Gaussian noise with this standard deviation on the weights of the LSTM (regularizer, recommended).\n\n\n#### To test that training works:\n\nYou can test that training works as expected using the dummy training set containing a Part of Speech CRF objective and cat vs dogs log likelihood objective is contained under learning/test:\n\n```bash\npython3 learning/train_type.py learning/test/config.json\n```\n\n### Installation\n\n#### Mac OSX\n\n```\npip3 install -r requirements.txt\npip3 install wikidata_linker_utils_src/\n```\n\n#### Fedora 25\n\n```\nsudo dnf install redhat-rpm-config\nsudo dnf install gcc-c++\nsudo pip3 install marisa-trie==0.7.2\nsudo pip3 install -r requirements.txt\npip3 install wikidata_linker_utils_src/\n```\n", "release_dates": []}, {"name": "democratic-inputs", "description": null, "language": "HTML", "license": null, "readme": "# OpenAI Democratic Input Grant Repository\n\nThe [Democratic Inputs to AI grant program](https://openai.com/blog/democratic-inputs-to-ai) funded 10 teams to develop and test their ideas for processes to help govern AI. We summarized some of their findings in our recap blog post, and here we present a repository where they have shared their code, alongside links to their reports and contact information.\n\nIf you would like to see the teams describe their ambitions during the September 2023 OpenAI Demo Day, please watch the [recording here](https://vimeo.com/875039398/c777de0595).\n\n---\n\n## Table of Contents\n\n- [Case Law for AI Policy](#case-law-for-ai-policy)\n- [Collective Dialogues for Democratic Policy Development](#collective-dialogues-for-democratic-policy-development)\n- [Deliberation at Scale: Socially democratic inputs to AI](#deliberation-at-scale-socially-democratic-inputs-to-ai)\n- [Democratic Fine-Tuning](#democratic-fine-tuning)\n- [Aligned: An Alignment Platform](#aligned-an-alignment-platform)\n- [Generative Social Choice](#generative-social-choice)\n- [Inclusive.AI: Engaging Underserved Populations in Democratic Decision-Making on AI](#inclusiveai-engaging-underserved-populations-in-democratic-decision-making-on-ai)\n- [Making AI Transparent and Accountable by Rappler](#making-ai-transparent-and-accountable-by-rappler)\n- [Ubuntu-AI: A Platform for Equitable and Inclusive Model Training](#ubuntu-ai-a-platform-for-equitable-and-inclusive-model-training)\n- [vTaiwan and Chatham House: Bridging the Recursive Public](#vtaiwan-and-chatham-house-bridging-the-recursive-public)\n\n---\n\n## [Case Law for AI Policy](https://github.com/openai/democratic-inputs/tree/main/projects/case_law_for_ai_policy)\n\n**Description**: Creating a robust case repository around AI interaction scenarios that can be used to make case-law-inspired judgments through a process that democratically engages experts, laypeople, and key stakeholders.\n\n**LINKS**: [Report](https://social.cs.washington.edu/case-law-ai-policy/); [Contact](mailto:sfl-case-law@cs.washington.edu)\n\n\n**Team members**:\n- Quan Ze (Jim) Chen\n- Kevin Feng\n- Inyoung Cheong\n- Amy X. Zhang\n- King Xia\n\n---\n\n## [Collective Dialogues for Democratic Policy Development](https://github.com/openai/democratic-inputs/tree/main/projects/collective_dialogues_for_democratic_input)\n\n**Description**: Developing policies that reflect informed public will using collective dialogues to efficiently scale democratic deliberation and find areas of consensus.\n\n**LINKS**: [Report](https://arxiv.org/pdf/2311.02242.pdf); [Contact](mailto:andrew@remesh.org)\n\n**Team members**:\n- Andrew Konya\n- Lisa Schirch\n- Colin Irwin\n\n\n---\n\n## [Deliberation at Scale: Socially democratic inputs to AI](https://github.com/openai/democratic-inputs/tree/main/projects/deliberation_at_scale)\n\n**Description**: Enabling democratic deliberation in small group conversations conducted via AI-facilitated video calls.\n\n**LINKS**: [Report](https://findcommonground.online/top-level-pages/final-report-democratic-inputs-to-ai); [Contact](https://findcommonground.online/top-level-pages/contact-us)\n\n**Team members**:\n- Jorim Theuns\n- Evelien Nieuwenburg\n- Pepijn Verburg\n- Lei Nelissen\n- Brett Hennig\n- Rich Rippin\n- Ran Haase\n- Aldo de Moor\n- CeesJan Mol\n- Naomi Esther\n- Rolf Kleef\n- Bram Delisse\n\n\n---\n\n## [Democratic Fine-Tuning](https://github.com/openai/democratic-inputs/tree/main/projects/democratic_fine_tuning)\n\n**Description**: Eliciting values from participants in a chat dialogue in order to create a moral graph of values that can be used to fine-tune models.\n\n**LINKS**: [Report](https://meaningalignment.substack.com/p/the-first-moral-graph); [Website](https://www.meaningalignment.org/); [Contact](mailto:hello@meaningalignment.org)\n\n**Team members**:\n- Joe Edelman\n- Oliver Klingefjord\n- Ivan Vendrov\n\n\n---\n\n## [Aligned: An Alignment Platform](https://github.com/openai/democratic-inputs/tree/main/projects/Aligned-Platform-EnergizeAI)\n\n**Description**: Developing guidelines for aligning AI models with live, large-scale participation and a 'community notes' algorithm.\n\n**LINKS**: [Report](https://energize.ai/openai/); [Contact](mailto:energize.ai)\n\n**Team members**:\n- Ethan Shaotran\n- Ido Pesok\n- Sam Jones\n\n\n---\n\n## [Generative Social Choice](https://github.com/openai/democratic-inputs/tree/main/projects/generative_social_choice)\n\n**Description**: Distilling a large number of free-text opinions into a concise slate that guarantees fair representation using mathematical arguments from social choice theory.\n\n**LINKS**: [Report](http://tinyurl.com/gensocopenaireport); [Contact](mailto:manuel.wuthrich@pm.me)\n\n**Team members**:\n- Sara Fish\n- Paul G\u00f6lz\n- Ariel Procaccia\n- Gili Rusak\n- Itai Shapira\n- Manuel W\u00fcthrich\n\n\n---\n\n## [Inclusive.AI: Engaging Underserved Populations in Democratic Decision-Making on AI](https://github.com/openai/democratic-inputs/tree/main/projects/inclusive_ai_a_chatgpt_plugin_and_dao_to_engage_marginalized_groups_in_ai)\n\n**Description**: Facilitating decision-making processes related to AI using a platform with decentralized governance (e.g., DAO) mechanisms that empower underserved groups.\n\n**LINKS**: [Report](https://socialcomputing.web.illinois.edu/images/Report-InclusiveAI.pdf); [Website](https://socialcomputing.web.illinois.edu/inclusiveai.html)\n\n**Team members**:\n- Yang Wang\n- Yun Huang\n- Tanusree Sharma\n- Dawn Song\n- Sunny Liu\n- Jeff Hancock\n\n\n---\n\n## [Making AI Transparent and Accountable by Rappler](https://github.com/openai/democratic-inputs/tree/main/projects/making_ai_transparent_and_accountable)\n\n**Description**: Enabling discussion and understanding of participants' views on complex, polarizing topics via linked offline and online processes.\n\n**LINKS**: [Report](https://www.rappler.com/technology/features/generative-ai-use-enriching-democratic-consultations/); [Contact](mailto:openai-experiments@rappler.com)\n\n**Team members**:\n- Gemma B. Mendoza\n- Gilian Uy\n- Don Kevin Hapal\n- Ogoy San Juan\n- Maria Ressa\n\n\n---\n\n## [Ubuntu-AI: A Platform for Equitable and Inclusive Model Training](https://github.com/openai/democratic-inputs/tree/main/projects/Ubuntu_AI_a_platform_for_equitable_and_inclusive_model_training)\n\n**Description**: Returning value to those who help create it while facilitating LLM development and ensuring more inclusive knowledge of African creative work.\n\n**LINKS**: [Report](https://www.researchgate.net/publication/374870830_Interim_Report_for_Ubuntu-AI_A_Bottom-up_Approach_to_More_Democratic_and_Equitable_Training_and_Outcomes_for_Machine_Learning); [Website](https://generativejustice.org/)\n\n**Team members**:\n- Ron Eglash\n- Joshua Mounsey\n- Micheal Nayebare\n- Rehema Baguma\n- Ussen Kimanuka\n\n\n---\n\n## [vTaiwan and Chatham House: Bridging the Recursive Public](https://github.com/openai/democratic-inputs/tree/main/projects/vtaiwan_and_chatham_house_bridging_the_recursive_public)\n**Description**: Using an adapted vTaiwan methodology to create a recursive, connected participatory process for AI.\n\n**LINKS**: [Report](https://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf); [Website](https://www.recursivepublic.com/); [Contact](mailto:akrasodomski@chathamhouse.org)\n\n**Team members**:\n- Alex Krasodomski-Jones\n- Carl Miller\n- Flynn Devine\n- Jia-Wei (Peter) Cui\n- Shu Yang Lin\n\n\n", "release_dates": []}, {"name": "distribution_augmentation", "description": "Code for the paper, \"Distribution Augmentation for Generative Modeling\", ICML 2020.", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "NOTE: REFERENCES TO 'gs://nmt-models' have been removed as we've migrated from GCP to Azure. The code provided may not work. Pull requests are welcome.\n\n# Distribution Augmentation for Generative Modeling\n\nThis is the code repository for `Distribution Augmentation for Generative Modeling`, presented at ICML 2020.  \n\nPaper link: https://proceedings.icml.cc/static/paper_files/icml/2020/6095-Paper.pdf\n\nUnconditional samples from our autoregressive CIFAR-10 model. (150m params, t=0.94)\n![Samples from our model](https://github.com/openai/distribution_augmentation/blob/master/samples.png?raw=true)\n\n\n# Setup\nThis code is tested on Python 3.7.3, Ubuntu 16.04, Anaconda 4.7.11, Tensorflow version 1.13.1, and CUDA 10. It requires V100 GPUs.\n\nIt requires installing `blocksparse` from source:\n\n```bash\ngit clone git@github.com:openai/blocksparse.git\ncd blocksparse\ngit checkout 89074c5ccf78e3a88b4aa2aefc9e208d4773dcbc  # master at time of release\nmake compile\npip install dist/*.whl\n```\n\n\n# Running experiments\nHyperparameters for experiments live in `hyperparams.py`. They can be selected in a training run using the `--hps [name]` flag. For instance:\n\n1 gpu runs can be run as so:\n`CUDA_VISIBLE_DEVICES=0 python train.py --hps c10_small`\n\n8 gpu runs can be run as so:\n`mpiexec -n 8 python train.py --hps c10_150m_baseline`\n\n64 gpu runs depend on the specifics of your network and system. We use the `-f` flag with a hostfile, but see options in `mpiexec` for whatever works for you.\n\nIf you run imagenet64 or imagenet32, you need to download the datasets. See the corresponding class in `datasets.py` to see how to do that. We dumped copies of the datasets in Azure buckets for convenience.\n\nThe specific code for experiments we reported in the paper live in the `experiments/` folder.\n\n# Reloading, evaluating, and sampling from saved models\n\nWe also stored some model weights in storage buckets, which can be loaded with this codebase to generate samples, run evaluations, or to run a custom training procedure.\n\nFor instance, to generate samples from a trained 15m parameter model, you can run the following:\n\n```\nwget https://openaipublic.blob.core.windows.net/distribution-augmentation-assets/models/c10-15m-baseline.npz\npython train.py --desc samples-c10-15m-baseline --hps c10_15m --restore_path c10-15m-baseline.npz --sample_and_exit --samples_to_generate 4 --sample_batch 4\n```\n\nThis will dump samples in the model directory.\n\nOther examples of loading from saved models can be found in `eval.sh` or `sample.sh`, where we include saved versions of various models reported in the paper.\n\n# Citation\nIf you find this work useful in your research, consider citing us with the following bibtex entry:\n```\n@incollection{icml2020_6095,\n abstract = {We present distribution augmentation (DistAug), a simple and powerful method of regularizing generative models. Core to our approach is applying augmentation functions to data and then conditioning the generative model on the specific function used. Unlike typical data augmentation, distribution augmentation allows usage of functions which modify the target density, enabling aggressive augmentations more commonly seen in supervised and self-supervised learning. We demonstrate this is a more effective regularizer than standard methods, and use it to train a 150M parameter autoregressive model on CIFAR-10 to 2.56 bits per dim (relative to the state-of-the-art 2.80). Samples from this model attain FID 12.75 and IS 8.40, outperforming the majority of GANs. We further demonstrate the technique is broadly applicable across model architectures, objectives, and problem domains.},\n author = {Jun, Heewoo and Child, Rewon and Chen, Mark and Schulman, John and Ramesh, Aditya and Radford, Alec and Sutskever, Ilya},\n booktitle = {Proceedings of Machine Learning and Systems 2020},\n pages = {10563--10576},\n title = {Distribution Augmentation for Generative Modeling},\n year = {2020}\n}\n```\n", "release_dates": []}, {"name": "doom-py", "description": "ViZDoom Python wrapper", "language": "C++", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# doom_py\n\n[![Build Status](https://travis-ci.org/openai/doom-py.svg?branch=master)](https://travis-ci.org/openai/doom-py)\n\nDEPRECATED, please use the official [ViZDoom python bindings](http://vizdoom.cs.put.edu.pl/) instead.\n\nPython wrappers for [ViZDoom](http://vizdoom.cs.put.edu.pl/).\nContains a modified version of the [bleeding-edge](https://github.com/Marqt/ViZDoom/tree/54a1091830aa08b3afc8e811dbb4f8947bb20bce)\nViZDoom source code.\n\n###Requirements\n\n* CMake 3.0+\n* Make\n* GCC 4.6+\n* Boost Libraries\n* SDL 2.0.2\n* Python with Numpy\n\nTo install dependencies on OS X via Brew, type\n\n```brew install cmake boost boost-python sdl2 wget```\n\nTo run with python3 you may need to run\n\n```brew install boost-python --with-python3```\n\nTo install dependencies on Ubuntu, type\n\n```apt-get install -y python-numpy cmake zlib1g-dev libjpeg-dev libboost-all-dev gcc libsdl2-dev wget unzip```\n\n\n### Installation\n\n```\npython setup.py build\npip install -e .\n```\n", "release_dates": []}, {"name": "ebm_code_release", "description": "Code for Implicit Generation and Generalization with Energy Based Models", "language": "Python", "license": null, "readme": "# Implicit Generation and Generalization in Energy Based Models\n\nCode for [Implicit Generation and Generalization in Energy Based Models](https://arxiv.org/pdf/1903.08689.pdf). Blog post can be found [here](https://openai.com/blog/energy-based-models/) and website with pretrained models can be found [here](https://sites.google.com/view/igebm/home).\n\n## Requirements\n\nTo install the prerequisites for the project run \n```\npip install -r requirements.txt\nmkdir sandbox_cachedir\n```\n\nDownload all [pretrained models](https://sites.google.com/view/igebm/home) and unzip into the folder cachedir.\n\n## Download Datasets\n\nFor MNIST and CIFAR-10 datasets, the code will directly download the data.\n\nFor ImageNet 128x128 dataset, download the TFRecords of the Imagenet dataset by running the following command\n\n```\nfor i in $(seq -f \"%05g\" 0 1023)\ndo\n  wget https://[deprecated]/data/imagenet/train-$i-of-01024\ndone\n\nfor i in $(seq -f \"%05g\" 0 127)\ndo\n  wget https://[deprecated]/data/imagenet/validation-$i-of-00128\ndone\n\nwget https://[deprecated]/data/imagenet/index.json\n```\n\nFor Imagenet 32x32 dataset, download the Imagenet 32x32 dataset and unzip by running the following command\n\n```\nwget https://[deprecated]/data/imagenet32/Imagenet32_train.zip\nwget https://[deprecated]/data/imagenet32/Imagenet32_val.zip\n```\n\nFor dSprites dataset, download the dataset by running\n\n```\nwget https://github.com/deepmind/dsprites-dataset/blob/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz?raw=true\n```\n\n## Training\n\nTo train on different datasets:\n\nFor CIFAR-10 Unconditional\n\n```\npython train.py --exp=cifar10_uncond --dataset=cifar10 --num_steps=60 --batch_size=128 --step_lr=10.0 --proj_norm=0.01 --zero_kl --replay_batch --large_model\n```\n\nFor CIFAR-10 Conditional\n\n```\npython train.py --exp=cifar10_cond --dataset=cifar10 --num_steps=60 --batch_size=128 --step_lr=10.0 --proj_norm=0.01 --zero_kl --replay_batch --cclass\n```\n\nFor ImageNet 32x32 Conditional\n\n```\npython train.py --exp=imagenet_cond --num_steps=60  --wider_model --batch_size=32 step_lr=10.0 --proj_norm=0.01 --replay_batch --cclass --zero_kl --dataset=imagenet --imagenet_path=<imagenet32x32 path>\n```\n\nFor ImageNet 128x128 Conditional\n\n```\npython train.py --exp=imagenet_cond --num_steps=50 --batch_size=16 step_lr=100.0 --replay_batch --swish_act --cclass --zero_kl --dataset=imagenetfull --imagenet_datadir=<full imagenet path>\n```\n\nAll code supports horovod execution, so model training can be increased substantially by using multiple different workers by running each command.\n```\nmpiexec -n <worker_num>  <command>\n```\n\n## Demo\n\nThe imagenet_demo.py file contains code to experiments with EBMs on conditional ImageNet 128x128. To generate a gif on sampling, you can run the command:\n\n```\npython imagenet_demo.py --exp=imagenet128_cond --resume_iter=2238000 --swish_act\n```\n\nThe ebm_sandbox.py file contains several different tasks that can be used to evaluate EBMs, which are defined by different settings of task flag in the file. For example, to visualize cross class mappings in CIFAR-10, you can run:\n\n```\npython ebm_sandbox.py --task=crossclass --num_steps=40 --exp=cifar10_cond --resume_iter=74700\n```\n\n\n## Generalization\n\nTo test generalization to out of distribution classification for SVHN (with similar commands for other datasets)\n```\npython ebm_sandbox.py --task=mixenergy --num_steps=40 --exp=cifar10_large_model_uncond --resume_iter=121200 --large_model --svhnmix --cclass=False\n```\n\nTo test classification on CIFAR-10 using a conditional model under either L2 or Li perturbations\n```\npython ebm_sandbox.py --task=label --exp=cifar10_wider_model_cond --resume_iter=21600 --lnorm=-1 --pgd=<number of pgd steps> --num_steps=10 --lival=<li bound value> --wider_model\n```\n\n\n## Concept Combination\n\nTo train EBMs on conditional dSprites dataset, you can train each model seperately on each conditioned latent in cond_pos, cond_rot, cond_shape, cond_scale, with an example command given below.\n\n```\npython train.py --dataset=dsprites --exp=dsprites_cond_pos --zero_kl --num_steps=20 --step_lr=500.0 --swish_act  --cond_pos --replay_batch -cclass\n```\n\nOnce models are trained, they can be sampled from jointly by running\n\n```\npython ebm_combine.py --task=conceptcombine --exp_size=<exp_size> --exp_shape=<exp_shape> --exp_pos=<exp_pos> --exp_rot=<exp_rot> --resume_size=<resume_size> --resume_shape=<resume_shape> --resume_rot=<resume_rot> --resume_pos=<resume_pos>\n```\n\n\n\n", "release_dates": []}, {"name": "EPG", "description": "Code for the paper \"Evolved Policy Gradients\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Evolved Policy Gradients (EPG)\n\nThe paper is located at https://arxiv.org/abs/1802.04821. A demonstration video can be found at https://youtu.be/-Z-ieH6w0LA.\n\n> Houthooft, R., Chen, R. Y., Isola, P., Stadie, B. C., Wolski, F., Ho, J., Abbeel, P. (2018). Evolved Policy\nGradients. arXiv preprint arXiv:1802.04821.\n\n### Installation\n\nInstall Anaconda:\n```\ncurl -o /tmp/miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\nbash /tmp/miniconda.sh\nconda create -n epg python=3.6.1\nsource activate epg\n```\n\nInstall necessary OSX packages for MPI:\n```\nbrew install open-mpi\n```\n\nInstall necessary Python packages:\n```\npip install mpi4py==3.0.0 scipy \\\npandas tqdm joblib cloudpickle == 0.5.2 \\\nprogressbar2 opencv-python flask >= 0.11.1 matplotlib pytest cython \\\nchainer pathos mujoco_py 'gym[all]'\n```\n\n\n### Running\nFirst go to the EPG code folder:\n```\ncd <path_to_EPG_folder>\n```\nThen launch the entry script:\n```\nPYTHONPATH=. python epg/launch_local.py\n```\nExperiment data is saved in `<home_dir>/EPG_experiments/<month>-<day>/<experiment_name>`.\n\n### Testing\n\nFirst, set `theta_load_path = '<path_to_theta.npy>/theta.npy'` in `launch_local.py` according to the `theta.npy` obtained after running the `launch_local.py` script. This file should be located in `/<home_dir>/EPG_experiments/<month>-<day>/<experiment_name>/thetas/`.\n\nThen run:\n```\nPYTHONPATH=. python epg/launch_local.py --test true\n```\n\n### Visualizing experiment data\n\nAssuming the experiment data is saved in `<home_dir>/EPG_experiments/<month>-<day>/<experiment_name>`, run:\n```\nPYTHONPATH=. python epg/viskit/frontend.py <home_dir>/EPG_experiments/<month>-<day>/<experiment_name>\n```\nThen go to `http://0.0.0.0:5000` in your browser.\n\nViskit sourced from\n\n> Duan, Y., Chen, X., Houthooft, R., Schulman, J., Abbeel, P. \"Benchmarking Deep Reinforcement Learning for Continuous Control\". Proceedings of the 33rd International Conference on Machine Learning (ICML), 2016.\n\n### BibTeX entry\n\n```\n@article{Houthooft18Evolved,\nauthor = {Houthooft, Rein and Chen, Richard Y. and Isola, Phillip and Stadie, Bradly C. and Wolski, Filip and Ho, Jonathan and Abbeel, Pieter},\ntitle = {Evolved Policy Gradients},\njournal={arXiv preprint arXiv:1802.04821},\nyear = {2018}}\n```", "release_dates": []}, {"name": "etcd", "description": "Development repository for the etcd cookbook", "language": "Ruby", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Etcd Cookbook\n\n[![Cookbook Version](https://img.shields.io/cookbook/v/etcd.svg)](https://supermarket.chef.io/cookbooks/etcd)\n[![CI State](https://github.com/sous-chefs/etcd/workflows/ci/badge.svg)](https://github.com/sous-chefs/etcd/actions?query=workflow%3Aci)\n[![OpenCollective](https://opencollective.com/sous-chefs/backers/badge.svg)](#backers)\n[![OpenCollective](https://opencollective.com/sous-chefs/sponsors/badge.svg)](#sponsors)\n[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n\nThe Etcd Cookbook is a library cookbook that provides custom resources for use in recipes.\n\n## Scope\n\nThis cookbook is concerned with the [Etcd](https://coreos.com/etcd/) distributed key/value store as distributed by CoreOS, Inc.\n\n## Maintainers\n\nThis cookbook is maintained by the Sous Chefs. The Sous Chefs are a community of Chef cookbook maintainers working together to maintain important cookbooks. If you\u2019d like to know more please visit [sous-chefs.org](https://sous-chefs.org/) or come chat with us on the Chef Community Slack in [#sous-chefs](https://chefcommunity.slack.com/messages/C2V7B88SF).\n\n## Requirements\n\n- Chef Infra Client 12.15+\n- Network accessible web server hosting the etcd binary.\n\n## Platform Support\n\nThe following platforms have been tested with Test Kitchen. It will most likely work on other platforms as well\n\n```\n|---------------+--------+\n|               | 3.2.15 |\n|---------------+--------+\n| amazonlinux 2 |   X    |\n|---------------+--------+\n| centos-7      |   X    |\n|---------------+--------+\n| debian-9      |   X    |\n|---------------+--------+\n| debian-10     |   X    |\n|---------------+--------+\n| fedora        |   X    |\n|---------------+--------+\n| ubuntu-16.04  |   X    |\n|---------------+--------+\n| ubuntu-18.04  |   X    |\n|---------------+--------+\n| ubuntu-20.04  |   X    |\n|---------------+--------+\n| opensuse-leap |   X    |\n|---------------+--------+\n```\n\n## Cookbook Dependencies\n\n- [docker](https://supermarket.chef.io/cookbooks/docker)\n\n## Usage\n\n- Add `depends 'etcd'` to your cookbook's metadata.rb\n- Use the resources shipped in cookbook in a recipe, the same way you'd use core Chef resources (file, template, directory, package, etc).\n\n```ruby\netcd_service 'etcd0' do\n  advertise_client_urls 'http://127.0.0.1:2379'\n  listen_client_urls 'http://0.0.0.0:2379'\n  initial_advertise_peer_urls 'http://127.0.0.1:2380'\n  listen_peer_urls 'http://0.0.0.0:2380'\n  initial_cluster_token 'etcd-cluster-1'\n  initial_cluster 'etcd0=http://127.0.0.1:2380,etcd1=http://127.0.0.1:3380,etcd2=http://127.0.0.1:4380'\n  initial_cluster_state 'new'\n  action :start\n  ignore_failure true # required for the first cluster build\nend\n\netcd_service 'etcd1' do\n  advertise_client_urls 'http://127.0.0.1:3379'\n  listen_client_urls 'http://0.0.0.0:3379'\n  initial_advertise_peer_urls 'http://127.0.0.1:3380'\n  listen_peer_urls 'http://0.0.0.0:3380'\n  initial_cluster_token 'etcd-cluster-1'\n  initial_cluster 'etcd0=http://127.0.0.1:2380,etcd1=http://127.0.0.1:3380,etcd2=http://127.0.0.1:4380'\n  initial_cluster_state 'new'\n  action :start\n  ignore_failure true\nend\n\netcd_service 'etcd2' do\n  advertise_client_urls 'http://127.0.0.1:4379'\n  listen_client_urls 'http://0.0.0.0:4379'\n  initial_advertise_peer_urls 'http://127.0.0.1:4380'\n  listen_peer_urls 'http://0.0.0.0:4380'\n  initial_cluster_token 'etcd-cluster-1'\n  initial_cluster 'etcd0=http://127.0.0.1:2380,etcd1=http://127.0.0.1:3380,etcd2=http://127.0.0.1:4380'\n  initial_cluster_state 'new'\n  action :start\n  ignore_failure true\nend\n```\n\nBy default reosource creates `etcd-NODE_NAME` unit file name. Sometimes it's not comfortable.\n\nIf you don't run multi etcd service in node, you can change this action to default service name.\n\n```\netcd_service 'etcd' do\n  action :start\n  default_service_name true\n...\nend\n```\n\n## Test Cookbooks as Examples\n\nThe cookbooks ran under test-kitchen make excellent usage examples.\n\nThe test recipes are found at:\n\n```ruby\ntest/cookbooks/etcd_test/\n```\n\n## Resources Overview\n\n- `etcd_service`: composite resource that uses etcd_installation and etcd_service_manager\n- `etcd_installation`: automatically selects an installation method\n- `etcd_service_manager`: automatically selects a service manager\n- `etcd_key`: manages keys in etcd\n- `etcd_installation_binary`: copies a pre-compiled etcd binary onto disk\n- `etcd_installation_docker`: pulls a docker image to the DOCKER_HOST\n- `etcd_service_manager_systemd`: manage etcd daemon with systemd unit files\n- `etcd_service_manager_docker`: starts a docker process on the DOCKER_HOST\n\n## Resources Details\n\n### etcd_installation\n\nThe `etcd_installation` resource auto-selects one of the below resources with the provider resolution system. Defaults to binary installation.\n\n#### Example\n\n```ruby\netcd_installation 'default' do\n  action :create\nend\n```\n\n### etcd_installation_binary\n\nThe `etcd_installation_binary` resource copies the precompiled Go binary onto the disk.\n\n#### Example\n\n```ruby\netcd_installation_binary 'default' do\n  version '3.2.6'\n  source 'https://my.computers.biz/dist/etcd'\n  checksum '90aff7364caa43932fd46974825af20e0ecb70fe7e01981e2d3a496106f147e7'\n  action :create\nend\n```\n\n### etcd_installation_docker\n\nThe `etcd_installation_docker` resource uses the `docker_image` resource to pull an image to the DOCKER_HOST.\n\n#### Properties\n\n- `repo` - The image name to pull. Defaults to 'quay.io/coreos/etcd'\n- `tag` - The image tag to pull.\n- `version` - String used to calculate tag string when tag is omitted. Defaults to '2.3.7'\n\n### etcd_service_manager\n\nThe `etcd_service_manager` resource auto-selects one of the below resources with the provider resolution system. The `etcd_service` family all share a common set of properties, which are listed under the `etcd_service` composite resource.\n\n#### Warning\n\netcd startup behavior is a bit quirky. etcd loops indefinitely on startup until quorum can be established. Due to this the first nodes service start will fail unless all nodes come up at the same time. Due to this there is an ignore_failure property for the systemd service managers which allows you to continue on in the chef run if the service fails to start. systemd will automatically keep restarting the service until all nodes are up and the cluster is healthy. For sys-v init you're on your own.\n\n#### Example\n\n```ruby\netcd_service_manager 'default' do\n  action :start\nend\n```\n\n#### properties\n\n- ignore_failure - Ignore failures starting the etcd service. Before quorum is established nodes will loop indefinitely and never successfully start. This can help ensure all instances are up when init systems can handle restart on failure. Default: false\n\n### etcd_service_manager_systemd\n\n#### Example\n\n```ruby\netcd_service_manager_systemd 'default' do\n  action :start\nend\n```\n\n#### properties\n\n- service_timeout - The time in seconds before the service start fails. Default: 120\n- ignore_failure - Ignore failures starting the etcd service. Before quorum is established nodes will loop indefinitely and never successfully start. This can help ensure all instances are up when init systems can handle restart on failure. Default: false\n\n### etcd_service_manager_docker\n\n#### Example\n\n```ruby\netcd_service_manager_docker 'default' do\n  action :start\nend\n```\n\n#### properties\n\n- repo - defaults to 'quay.io/coreos/etcd'\n- tag - default calculated from version\n- version - defaults to '3.2.15',\n- container_name - defaults to resource name\n- port - defaults to ['2379/tcp4:2379', '2380/tcp4:2380']\n- host_data_path - Path to store data locally on the host, which will be mounted into the container\n\n### etcd_service\n\nThe `etcd_service`: resource is a composite resource that uses `etcd_installation` and `etcd_service_manager` resources to install and manage the etcd service.\n\n- The `:create` action uses an `etcd_installation`\n- The `:delete` action uses an`etcd_installation`\n- The `:start` action uses an `etcd_service_manager`\n- The `:stop` action uses an `etcd_service_manager`\n\nThe service management strategy for the host platform is dynamically chosen based on platform, but can be overridden.\n\n#### Properties\n\nThe `etcd_service` resource property list corresponds to the options found in\n\n[Etcd Configuration Flags documentation](https://coreos.com/etcd/docs/3.2.15/op-guide/configuration.html)\n\n##### Member properties\n\n- `source`\n- `node_name`\n- `data_dir`\n- `wal_dir`\n- `snapshot_count` snapshot to disk.\n- `heartbeat_interval`\n- `election_timeout`\n- `listen_peer_urls`\n- `listen_client_urls`\n- `max_snapshots`\n- `max_wals`\n- `cors`\n- `quota_backend_bytes`\n\n##### Clustering properties\n\n- `initial`\n- `initial_advertise_peer_urls`\n- `initial_cluster`\n- `initial_cluster_state`\n- `initial_cluster_token`\n- `advertise_client_urls`\n- `discovery`\n- `discovery_srv`\n- `discovery_fallback`\n- `discovery_proxy`\n- `strict_reconfig_check`\n- `auto_compaction_retention`\n- `enable_v2`\n\n##### Proxy properties\n\n- `proxy`\n- `proxy_failure_wait`\n- `proxy_refresh_interval`\n- `proxy_dial_timeout`\n- `proxy_write_timeout`\n- `proxy_read_timeout`\n\n##### Security properties\n\n- `cert_file`\n- `key_file`\n- `client_cert_auth`\n- `trusted_ca_file`\n- `auto_tls`\n- `peer_cert_file`\n- `peer_key_file`\n- `peer_client_cert_auth`\n- `peer_trusted_ca_file`\n- `peer_cert_allowed_cn`\n- `peer_auto_tls`\n- `etcdctl_client_cert_file`\n- `etcdctl_client_key_file`\n- `experimental_peer_skip_client_san_verification`\n\n##### Logging properties\n\n- `debug`\n- `log_package_levels`\n\n##### Profiling properties\n\n- `enable_pprof`\n- `metrics`\n- `listen-metrics-urls`\n\n##### Auth Flpropertiesags\n\n- `auth_token`\n\n##### Unsafe properties\n\n- `force_new_cluster`\n\n##### Misc properties\n\n- `http_proxy`\n- `https_proxy`\n- `no_proxy`\n- `auto_restart`\n\n### etcd_key\n\nThe `etcd_key` resource sets, watches and deletes keys in etcd.\n\n#### Actions\n\n- The `:set` action sets a key\n- The `:delete` action deletes a key\n- The `:watch` action waits for a key to update\n\n##### Properties\n\n- `key` - The key name\n- `value` - The desired value\n- `ttl` - The ttl for the key (optional)\n- `host` - The hostname of the etcd server, defaults to `127.0.0.1`\n- `port` - The port that etcd is listening on, defaults to `2379`\n\n#### Examples\n\n```ruby\netcd_key \"/test\" do\n  value \"a_test_value\"\n  action :set\nend\n```\n\n## Contributors\n\nThis project exists thanks to all the people who [contribute.](https://opencollective.com/sous-chefs/contributors.svg?width=890&button=false)\n\n### Additional Contributors\n\n- [Jesse Nelson](https://github.com/spheromak)\n- [Soulou](https://github.com/Soulou)\n- [Aaron O'Mullan](https://github.com/AaronO)\n- [Anthony Scalisi](https://github.com/scalp42)\n- [Robert Coleman](https://github.com/rjocoleman)\n- [James Gregory](https://github.com/jagregory)\n- [Sean OMeara](https://github.com/someara)\n\n### Backers\n\nThank you to all our backers!\n\n![https://opencollective.com/sous-chefs#backers](https://opencollective.com/sous-chefs/backers.svg?width=600&avatarHeight=40)\n\n### Sponsors\n\nSupport this project by becoming a sponsor. Your logo will show up here with a link to your website.\n\n![https://opencollective.com/sous-chefs/sponsor/0/website](https://opencollective.com/sous-chefs/sponsor/0/avatar.svg?avatarHeight=100)\n![https://opencollective.com/sous-chefs/sponsor/1/website](https://opencollective.com/sous-chefs/sponsor/1/avatar.svg?avatarHeight=100)\n![https://opencollective.com/sous-chefs/sponsor/2/website](https://opencollective.com/sous-chefs/sponsor/2/avatar.svg?avatarHeight=100)\n![https://opencollective.com/sous-chefs/sponsor/3/website](https://opencollective.com/sous-chefs/sponsor/3/avatar.svg?avatarHeight=100)\n![https://opencollective.com/sous-chefs/sponsor/4/website](https://opencollective.com/sous-chefs/sponsor/4/avatar.svg?avatarHeight=100)\n![https://opencollective.com/sous-chefs/sponsor/5/website](https://opencollective.com/sous-chefs/sponsor/5/avatar.svg?avatarHeight=100)\n![https://opencollective.com/sous-chefs/sponsor/6/website](https://opencollective.com/sous-chefs/sponsor/6/avatar.svg?avatarHeight=100)\n![https://opencollective.com/sous-chefs/sponsor/7/website](https://opencollective.com/sous-chefs/sponsor/7/avatar.svg?avatarHeight=100)\n![https://opencollective.com/sous-chefs/sponsor/8/website](https://opencollective.com/sous-chefs/sponsor/8/avatar.svg?avatarHeight=100)\n![https://opencollective.com/sous-chefs/sponsor/9/website](https://opencollective.com/sous-chefs/sponsor/9/avatar.svg?avatarHeight=100)\n", "release_dates": []}, {"name": "evals", "description": "Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# OpenAI Evals\n\nEvals provide a framework for evaluating large language models (LLMs) or systems built using LLMs. We offer an existing registry of evals to test different dimensions of OpenAI models and the ability to write your own custom evals for use cases you care about. You can also use your data to build private evals which represent the common LLMs patterns in your workflow without exposing any of that data publicly.\n\nIf you are building with LLMs, creating high quality evals is one of the most impactful things you can do. Without evals, it can be very difficult and time intensive to understand how different model versions might affect your use case. In the words of [OpenAI's President Greg Brockman](https://twitter.com/gdb/status/1733553161884127435):\n\n<img width=\"596\" alt=\"https://x.com/gdb/status/1733553161884127435?s=20\" src=\"https://github.com/openai/evals/assets/35577566/ce7840ff-43a8-4d88-bb2f-6b207410333b\">\n\n## Setup\n\nTo run evals, you will need to set up and specify your [OpenAI API key](https://platform.openai.com/account/api-keys). After you obtain an API key, specify it using the [`OPENAI_API_KEY` environment variable](https://platform.openai.com/docs/quickstart/step-2-setup-your-api-key). Please be aware of the [costs](https://openai.com/pricing) associated with using the API when running evals. You can also run and create evals using [Weights & Biases](https://wandb.ai/wandb_fc/openai-evals/reports/OpenAI-Evals-Demo-Using-W-B-Prompts-to-Run-Evaluations--Vmlldzo0MTI4ODA3).\n\n**Minimum Required Version: Python 3.9**\n\n### Downloading evals\n\nOur evals registry is stored using [Git-LFS](https://git-lfs.com/). Once you have downloaded and installed LFS, you can fetch the evals (from within your local copy of the evals repo) with:\n```sh\ncd evals\ngit lfs fetch --all\ngit lfs pull\n```\n\nThis will populate all the pointer files under `evals/registry/data`.\n\nYou may just want to fetch data for a select eval. You can achieve this via:\n```sh\ngit lfs fetch --include=evals/registry/data/${your eval}\ngit lfs pull\n```\n\n### Making evals\n\nIf you are going to be creating evals, we suggest cloning this repo directly from GitHub and installing the requirements using the following command:\n\n```sh\npip install -e .\n```\n\nUsing `-e`, changes you make to your eval will be reflected immediately without having to reinstall.\n\nOptionally, you can install the formatters for pre-committing with:\n\n```sh\npip install -e .[formatters]\n```\n\nThen run `pre-commit install` to install pre-commit into your git hooks. pre-commit will now run on every commit.\n\nIf you want to manually run all pre-commit hooks on a repository, run `pre-commit run --all-files`. To run individual hooks use `pre-commit run <hook_id>`.\n\n## Running evals\n\nIf you don't want to contribute new evals, but simply want to run them locally, you can install the evals package via pip:\n\n```sh\npip install evals\n```\n\nYou can find the full instructions to run existing evals in [`run-evals.md`](docs/run-evals.md) and our existing eval templates in [`eval-templates.md`](docs/eval-templates.md). For more advanced use cases like prompt chains or tool-using agents, you can use our [Completion Function Protocol](docs/completion-fns.md).\n\nWe provide the option for you to log your eval results to a Snowflake database, if you have one or wish to set one up. For this option, you will further have to specify the `SNOWFLAKE_ACCOUNT`, `SNOWFLAKE_DATABASE`, `SNOWFLAKE_USERNAME`, and `SNOWFLAKE_PASSWORD` environment variables.\n\n## Writing evals\n\nWe suggest getting starting by: \n\n- Walking through the process for building an eval: [`build-eval.md`](docs/build-eval.md)\n- Exploring an example of implementing custom eval logic: [`custom-eval.md`](docs/custom-eval.md)\n- Writing your own completion functions: [`completion-fns.md`](docs/completion-fns.md)\n\nPlease note that we are currently not accepting evals with custom code! While we ask you to not submit such evals at the moment, you can still submit model-graded evals with custom model-graded YAML files.\n\nIf you think you have an interesting eval, please open a pull request with your contribution. OpenAI staff actively review these evals when considering improvements to upcoming models.\n\n## FAQ\n\nDo you have any examples of how to build an eval from start to finish?\n\n- Yes! These are in the `examples` folder. We recommend that you also read through [`build-eval.md`](docs/build-eval.md) in order to gain a deeper understanding of what is happening in these examples.\n\nDo you have any examples of evals implemented in multiple different ways?\n\n- Yes! In particular, see `evals/registry/evals/coqa.yaml`. We have implemented small subsets of the [CoQA](https://stanfordnlp.github.io/coqa/) dataset for various eval templates to help illustrate the differences.\n\nWhen I run an eval, it sometimes hangs at the very end (after the final report). What's going on?\n\n- This is a known issue, but you should be able to interrupt it safely and the eval should finish immediately after.\n\nThere's a lot of code, and I just want to spin up a quick eval. Help? OR,\n\nI am a world-class prompt engineer. I choose not to code. How can I contribute my wisdom?\n\n- If you follow an existing [eval template](docs/eval-templates.md) to build a basic or model-graded eval, you don't need to write any evaluation code at all! Just provide your data in JSON format and specify your eval parameters in YAML. [build-eval.md](docs/build-eval.md) walks you through these steps, and you can supplement these instructions with the Jupyter notebooks in the `examples` folder to help you get started quickly. Keep in mind, though, that a good eval will inevitably require careful thought and rigorous experimentation!\n\n## Disclaimer\n\nBy contributing to evals, you are agreeing to make your evaluation logic and data under the same MIT license as this repository. You must have adequate rights to upload any data used in an eval. OpenAI reserves the right to use this data in future service improvements to our product. Contributions to OpenAI evals will be subject to our usual Usage Policies: https://platform.openai.com/docs/usage-policies.\n", "release_dates": []}, {"name": "evolution-strategies-starter", "description": "Code for the paper \"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Distributed evolution\n\nThis is a distributed implementation of the algorithm described in [Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://arxiv.org/abs/1703.03864) (Tim Salimans, Jonathan Ho, Xi Chen, Ilya Sutskever).\n\nThe implementation here uses a master-worker architecture: at each iteration, the master broadcasts parameters to the workers, and the workers send returns back to the master. The humanoid scaling experiment in the paper was generated with an implementation similar to this one.\n\nThe code here runs on EC2, so you need an AWS account. It's resilient to worker termination, so it's safe to run the workers on spot instances.\n\n## Instructions\n\n### Build AMI\nThe humanoid experiment depends on Mujoco. Provide your own Mujoco license and binary in `scripts/dependency.sh`.\n\nInstall [Packer](https://www.packer.io/), and then build images by running (you can optionally configure `scripts/packer.json` to choose build instance or AWS regions)\n```\ncd scripts && packer build packer.json\n```\n\nPacker should return you a list of AMI ids, which you should place in `AMI_MAP` in `scripts/launch.py`.\n\n### Launching\nUse `scripts/launch.py` along with an experiment JSON file. An example JSON file is provided in the `configurations` directory. You must fill in all command-line arguments to `scripts/launch.py`.\n", "release_dates": []}, {"name": "fetch_robots", "description": "Bringup for fetch & freight", "language": "Python", "license": null, "readme": "# fetch_robots\n\nBringup files for Fetch and Freight.", "release_dates": []}, {"name": "finetune-transformer-lm", "description": "Code and model for the paper \"Improving Language Understanding by Generative Pre-Training\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# finetune-transformer-lm\nCode and model for the paper \"Improving Language Understanding by Generative Pre-Training\"\n\nCurrently this code implements the ROCStories Cloze Test result reported in the paper by running:\n`python train.py --dataset rocstories --desc rocstories --submit --analysis --data_dir [path to data here]`\n\nNote: The code is currently non-deterministic due to various GPU ops. The median accuracy of 10 runs with this codebase (using default hyperparameters) is 85.8% - slightly lower than the reported single run of 86.5% from the paper. \n\nThe ROCStories dataset can be downloaded from the associated [website](http://cs.rochester.edu/nlp/rocstories/).\n", "release_dates": []}, {"name": "fluent-plugin-kubernetes_metadata_filter", "description": "Enrich your fluentd events with Kubernetes metadata", "language": "Ruby", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# fluent-plugin-kubernetes_metadata_filter, a plugin for [Fluentd](http://fluentd.org)\n[![Circle CI](https://circleci.com/gh/fabric8io/fluent-plugin-kubernetes_metadata_filter.svg?style=svg)](https://circleci.com/gh/fabric8io/fluent-plugin-kubernetes_metadata_filter)\n[![Code Climate](https://codeclimate.com/github/fabric8io/fluent-plugin-kubernetes_metadata_filter/badges/gpa.svg)](https://codeclimate.com/github/fabric8io/fluent-plugin-kubernetes_metadata_filter)\n[![Test Coverage](https://codeclimate.com/github/fabric8io/fluent-plugin-kubernetes_metadata_filter/badges/coverage.svg)](https://codeclimate.com/github/fabric8io/fluent-plugin-kubernetes_metadata_filter)\n\nThe Kubernetes metadata plugin filter enriches container log records with pod and namespace metadata.\n\nThis plugin derives basic metadata about the container that emitted a given log record using the source of the log record. Records from journald provide metadata about the\ncontainer environment as named fields. Records from JSON files encode metadata about the container in the file name.  The initial metadata derived from the source is used\nto lookup additional metadata about the container's associated pod and namespace (e.g. UUIDs, labels, annotations) when the kubernetes_url is configured.  If the plugin cannot\nauthoritatively determine the namespace of the container emitting a log record, it will use an 'orphan' namespace ID in the metadata. This behaviors supports multi-tenant systems\nthat rely on the authenticity of the namespace for proper log isolation.\n\n## Requirements\n\n| fluent-plugin-kubernetes_metadata_filter  | fluentd | ruby |\n|-------------------|---------|------|\n| >= 2.0.0 | >= v0.14.20 | >= 2.1 |\n|  < 2.0.0 | >= v0.12.0 | >= 1.9 |\n\nNOTE: For v0.12 version, you should use 1.x.y version. Please send patch into v0.12 branch if you encountered 1.x version's bug.\n\nNOTE: This documentation is for fluent-plugin-kubernetes_metadata_filter-plugin-elasticsearch 2.x or later. For 1.x documentation, please see [v0.12 branch](https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter/tree/v0.12).\n\n## Installation\n\n    gem install fluent-plugin-kubernetes_metadata_filter\n\n## Configuration\n\nConfiguration options for fluent.conf are:\n\n* `kubernetes_url` - URL to the API server. Set this to retrieve further kubernetes metadata for logs from kubernetes API server. If not specified, environment variables `KUBERNETES_SERVICE_HOST` and `KUBERNETES_SERVICE_PORT` will be used if both are present which is typically true when running fluentd in a pod.\n* `apiVersion` - API version to use (default: `v1`)\n* `ca_file` - path to CA file for Kubernetes server certificate validation\n* `verify_ssl` - validate SSL certificates (default: `true`)\n* `client_cert` - path to a client cert file to authenticate to the API server\n* `client_key` - path to a client key file to authenticate to the API server\n* `bearer_token_file` - path to a file containing the bearer token to use for authentication\n* `tag_to_kubernetes_name_regexp` - the regular expression used to extract kubernetes metadata (pod name, container name, namespace) from the current fluentd tag.\nThis must used named capture groups for `container_name`, `pod_name` & `namespace` (default: `\\.(?<pod_name>[^\\._]+)_(?<namespace>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\\.log$</pod>)`)\n* `cache_size` - size of the cache of Kubernetes metadata to reduce requests to the API server (default: `1000`)\n* `cache_ttl` - TTL in seconds of each cached element. Set to negative value to disable TTL eviction (default: `3600` - 1 hour)\n* `watch` - set up a watch on pods on the API server for updates to metadata (default: `true`)\n* `de_dot` - replace dots in labels and annotations with configured `de_dot_separator`, required for ElasticSearch 2.x compatibility (default: `true`)\n* `de_dot_separator` - separator to use if `de_dot` is enabled (default: `_`)\n* *DEPRECATED* `use_journal` - If false, messages are expected to be formatted and tagged as if read by the fluentd in\\_tail plugin with wildcard filename.  If true, messages are expected to be formatted as if read from the systemd journal.  The `MESSAGE` field has the full message.  The `CONTAINER_NAME` field has the encoded k8s metadata (see below).  The `CONTAINER_ID_FULL` field has the full container uuid.  This requires docker to use the `--log-driver=journald` log driver.  If unset (the default), the plugin will use the `CONTAINER_NAME` and `CONTAINER_ID_FULL` fields\nif available, otherwise, will use the tag in the `tag_to_kubernetes_name_regexp` format.\n* `container_name_to_kubernetes_regexp` - The regular expression used to extract the k8s metadata encoded in the journal `CONTAINER_NAME` field (default: `'^(?<name_prefix>[^_]+)_(?<container_name>[^\\._]+)(\\.(?<container_hash>[^_]+))?_(?<pod_name>[^_]+)_(?<namespace>[^_]+)_[^_]+_[^_]+$'`\n  * This corresponds to the definition [in the source](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/dockertools/docker.go#L317)\n* `annotation_match` - Array of regular expressions matching annotation field names. Matched annotations are added to a log record.\n* `allow_orphans` - Modify the namespace and namespace id to the values of `orphaned_namespace_name` and `orphaned_namespace_id`\nwhen true (default: `true`)\n* `orphaned_namespace_name` - The namespace to associate with records where the namespace can not be determined (default: `.orphaned`)\n* `orphaned_namespace_id` - The namespace id to associate with records where the namespace can not be determined (default: `orphaned`)\n* `lookup_from_k8s_field` - If the field `kubernetes` is present, lookup the metadata from the given subfields such as `kubernetes.namespace_name`, `kubernetes.pod_name`, etc.  This allows you to avoid having to pass in metadata to lookup in an explicitly formatted tag name or in an explicitly formatted `CONTAINER_NAME` value.  For example, set `kubernetes.namespace_name`, `kubernetes.pod_name`, `kubernetes.container_name`, and `docker.id` in the record, and the filter will fill in the rest. (default: `true`)\n* `ssl_partial_chain` - if `ca_file` is for an intermediate CA, or otherwise we do not have the root CA and want\n  to trust the intermediate CA certs we do have, set this to `true` - this corresponds to\n  the `openssl s_client -partial_chain` flag and `X509_V_FLAG_PARTIAL_CHAIN` (default: `false`)\n* `skip_labels` - Skip all label fields from the metadata.\n* `skip_container_metadata` - Skip some of the container data of the metadata. The metadata will not contain the container_image and container_image_id fields.\n* `skip_master_url` - Skip the master_url field from the metadata.\n* `skip_namespace_metadata` - Skip the namespace_id field from the metadata. The fetch_namespace_metadata function will be skipped. The plugin will be faster and cpu consumption will be less.\n* `watch_retry_interval` - The time interval in seconds for retry backoffs when watch connections fail. (default: `10`)\n\n**NOTE:** As of the release 2.1.x of this plugin, it no longer supports parsing the source message into JSON and attaching it to the\npayload.  The following configuration options are removed:\n\n* `merge_json_log`\n* `preserve_json_log`\n\nOne way of preserving JSON logs can be through the [parser plugin](https://docs.fluentd.org/filter/parser)\n\n**NOTE** As of this release, the use of `use_journal` is **DEPRECATED**.  If this setting is not present, the plugin will\nattempt to figure out the source of the metadata fields from the following:\n- If `lookup_from_k8s_field true` (the default) and the following fields are present in the record:\n`docker.container_id`, `kubernetes.namespace_name`, `kubernetes.pod_name`, `kubernetes.container_name`,\nthen the plugin will use those values as the source to use to lookup the metadata\n- If `use_journal true`, or `use_journal` is unset, and the fields `CONTAINER_NAME` and `CONTAINER_ID_FULL` are present in the record,\nthen the plugin will parse those values using `container_name_to_kubernetes_regexp` and use those as the source to lookup the metadata\n- Otherwise, if the tag matches `tag_to_kubernetes_name_regexp`, the plugin will parse the tag and use those values to\nlookup the metdata\n\nReading from the JSON formatted log files with `in_tail` and wildcard filenames while respecting the CRI-o log format with the same config you need the fluent-plugin \"multi-format-parser\":\n\n```\nfluent-gem install fluent-plugin-multi-format-parser\n```\n\nThe config block could look like this:\n```\n<source>\n  @type tail\n  path /var/log/containers/*.log\n  pos_file fluentd-docker.pos\n  read_from_head true\n  tag kubernetes.*\n  <parse>\n    @type multi_format\n    <pattern>\n      format json\n      time_key time\n      time_type string\n      time_format \"%Y-%m-%dT%H:%M:%S.%NZ\"\n      keep_time_key false\n    </pattern>\n    <pattern>\n      format regexp\n      expression /^(?<time>.+) (?<stream>stdout|stderr)( (?<logtag>.))? (?<log>.*)$/\n      time_format '%Y-%m-%dT%H:%M:%S.%N%:z'\n      keep_time_key false\n    </pattern>\n  </parse>\n</source>\n\n<filter kubernetes.var.log.containers.**.log>\n  @type kubernetes_metadata\n</filter>\n\n<match **>\n  @type stdout\n</match>\n```\n\nReading from the systemd journal (requires the fluentd `fluent-plugin-systemd` and `systemd-journal` plugins, and requires docker to use the `--log-driver=journald` log driver):\n```\n<source>\n  @type systemd\n  path /run/log/journal\n  pos_file journal.pos\n  tag journal\n  read_from_head true\n</source>\n\n# probably want to use something like fluent-plugin-rewrite-tag-filter to\n# retag entries from k8s\n<match journal>\n  @type rewrite_tag_filter\n  rewriterule1 CONTAINER_NAME ^k8s_ kubernetes.journal.container\n  ...\n</match>\n\n<filter kubernetes.**>\n  @type kubernetes_metadata\n  use_journal true\n</filter>\n\n<match **>\n  @type stdout\n</match>\n```\n## Log content as JSON\nIn former versions this plugin parsed the value of the key log as JSON. In the current version this feature was removed, to avoid duplicate features in the fluentd plugin ecosystem. It can parsed with the parser plugin like this:\n```\n<filter kubernetes.**>\n  @type parser\n  key_name log\n  <parse>\n    @type json\n    json_parser json\n  </parse>\n  replace_invalid_sequence true\n  reserve_data true # this preserves unparsable log lines\n  emit_invalid_record_to_error false # In case of unparsable log lines keep the error log clean\n  reserve_time # the time was already parsed in the source, we don't want to overwrite it with current time.\n</filter>\n```\n\n## Environment variables for Kubernetes\n\nIf the name of the Kubernetes node the plugin is running on is set as\nan environment variable with the name `K8S_NODE_NAME`, it will reduce cache\nmisses and needless calls to the Kubernetes API.\n\nIn the Kubernetes container definition, this is easily accomplished by:\n\n```yaml\nenv:\n- name: K8S_NODE_NAME\n  valueFrom:\n    fieldRef:\n      fieldPath: spec.nodeName\n```\n\n## Example input/output\n\nKubernetes creates symlinks to Docker log files in `/var/log/containers/*.log`. Docker logs in JSON format.\n\nAssuming following inputs are coming from a log file named `/var/log/containers/fabric8-console-controller-98rqc_default_fabric8-console-container-df14e0d5ae4c07284fa636d739c8fc2e6b52bc344658de7d3f08c36a2e804115.log`:\n\n```\n{\n  \"log\": \"2015/05/05 19:54:41 \\n\",\n  \"stream\": \"stderr\",\n  \"time\": \"2015-05-05T19:54:41.240447294Z\"\n}\n```\n\nThen output becomes as belows\n```\n{\n  \"log\": \"2015/05/05 19:54:41 \\n\",\n  \"stream\": \"stderr\",\n  \"docker\": {\n    \"id\": \"df14e0d5ae4c07284fa636d739c8fc2e6b52bc344658de7d3f08c36a2e804115\",\n  }\n  \"kubernetes\": {\n    \"host\": \"jimmi-redhat.localnet\",\n    \"pod_name\":\"fabric8-console-controller-98rqc\",\n    \"pod_id\": \"c76927af-f563-11e4-b32d-54ee7527188d\",\n    \"pod_ip\": \"172.17.0.8\",\n    \"container_name\": \"fabric8-console-container\",\n    \"namespace_name\": \"default\",\n    \"namespace_id\": \"23437884-8e08-4d95-850b-e94378c9b2fd\",\n    \"namespace_annotations\": {\n      \"fabric8.io/git-commit\": \"5e1116f63df0bac2a80bdae2ebdc563577bbdf3c\"\n    },\n    \"namespace_labels\": {\n      \"product_version\": \"v1.0.0\"\n    },\n    \"labels\": {\n      \"component\": \"fabric8Console\"\n    }\n  }\n}\n```\n\nIf using journal input, from docker configured with `--log-driver=journald`, the input looks like the `journalctl -o export` format:\n```\n# The stream identification is encoded into the PRIORITY field as an\n# integer: 6, or github.com/coreos/go-systemd/journal.Info, marks stdout,\n# while 3, or github.com/coreos/go-systemd/journal.Err, marks stderr.\nPRIORITY=6\nCONTAINER_ID=b6cbb6e73c0a\nCONTAINER_ID_FULL=b6cbb6e73c0ad63ab820e4baa97cdc77cec729930e38a714826764ac0491341a\nCONTAINER_NAME=k8s_registry.a49f5318_docker-registry-1-hhoj0_default_ae3a9bdc-1f66-11e6-80a2-fa163e2fff3a_799e4035\nMESSAGE=172.17.0.1 - - [21/May/2016:16:52:05 +0000] \"GET /healthz HTTP/1.1\" 200 0 \"\" \"Go-http-client/1.1\"\n```\n\n## Contributing\n\n1. Fork it\n2. Create your feature branch (`git checkout -b my-new-feature`)\n3. Commit your changes (`git commit -am 'Add some feature'`)\n4. Test it (`GEM_HOME=vendor bundle install; GEM_HOME=vendor bundle exec rake test`)\n5. Push to the branch (`git push origin my-new-feature`)\n6. Create new Pull Request\n\n## Copyright\n  Copyright (c) 2015 jimmidyson\n", "release_dates": []}, {"name": "following-instructions-human-feedback", "description": null, "language": null, "license": null, "readme": "# InstructGPT: Training Language Models to Follow Instructions with Human Feedback\n\n[Paper link][LINK_TO_PAPER]\n\n> Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI-API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback (RLHF). We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.\n\n\n## Contents\n- [model-card.md](model-card.md) - InstructGPT model card\n- [automatic-eval-samples](automatic-eval-samples/) - Samples from our models (both GPT-3 and InstructGPT) on public NLP benchmarks.\n- [API distribution labeling instructions](https://docs.google.com/document/d/1MJCqDNjzD04UbcnVZ-LmeXJ04-TKEICDAepXyMCBUb8/edit#) - Google doc of instructions given to contractors for final evaluations on our API prompt distribution.\n- [Toxicity labeling instructions](https://docs.google.com/document/d/1d3n6AqNrd-SJEKm_etEo3rUwXxKG4evCbzfWExvcGxg/edit?usp=sharing) - Google doc of instructions given to contractors for labeling toxic outputs on the RealToxicityPrompts dataset\n\n[LINK_TO_PAPER]: https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf\n\n", "release_dates": []}, {"name": "gen", "description": "Common generator scripts for all client libraries", "language": "Shell", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# gen\nCommon generator scripts for all client libraries\n\n# Badges\n\n[![Client Support Level](https://img.shields.io/badge/Kubernetes%20client-Bronze-blue.svg?style=plastic&colorB=cd7f32&colorA=306CE8)](https://github.com/kubernetes-client)\n\n[![Client Support Level](https://img.shields.io/badge/Kubernetes%20client-Silver-blue.svg?style=plastic&colorB=C0C0C0&colorA=306CE8)](https://github.com/kubernetes-client)\n\n[![Client Support Level](https://img.shields.io/badge/Kubernetes%20client-Gold-blue.svg?style=plastic&colorB=FFD700&colorA=306CE8)](https://github.com/kubernetes-client)\n\n# Generating a client\nTo generate a client, first make sure the client generator exists. For any language other than\ngo, check `openapi/` folder for a script with `${CLIENT_LANGUAGE}.sh` and run this command:\n\n```bash\n${CLIENT_LANGUAGE}.sh OUTPUT_DIR SETTING_FILE\n```\n\n`SETTING_FILE` is a bash script exporting required setting to generate a client. These\nare normally:\n\n- `KUBERNETES_BRANCH`: The kubernetes branch to get OpenAPI spec from. e.g. \"master\"\n- `CLIENT_VERSION`: Client version string. e.g. \"1.0.0b1\"\n- `PACKAGE_NAME`: Package name for the generated client. e.g. \"kubernetes\"\n\nExample settings file for python-client:\n\nexport KUBERNETES_BRANCH=\"master\"\nexport CLIENT_VERSION=\"8.0.0b1\"\nexport PACKAGE_NAME=\"client\"\n\nNote: For generating the client for any language, the PACKAGE_NAME should be \"client\".\n      You can use the latest version for the CLIENT_VERSION. It's displayed here for\n      the python-client ( https://github.com/kubernetes-client/python ), and similarly\n      for other language clients. \n\nRecommended structure is to generate client in a folder called `kubernetes` at the root of\nthe client repo and put all settings in a file named `settings` at the root of the repo.\nIf you followed these recommendations, you can simply run autoupdate script anywhere inside\nthe client repo:\n\n```bash\ncd ${CLIENT_ROOT}/...\n${GEN_REPO_ROOT}/openapi/autoupdate.sh\n```\n\n## Contributing\n\nPlease see [CONTRIBUTING.md](CONTRIBUTING.md) for instructions on how to contribute.\n", "release_dates": []}, {"name": "generating-reviews-discovering-sentiment", "description": "Code for \"Learning to Generate Reviews and Discovering Sentiment\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Generating Reviews and Discovering Sentiment\n\nCode for [Learning to Generate Reviews and Discovering Sentiment](https://arxiv.org/abs/1704.01444) (Alec Radford, Rafal Jozefowicz, Ilya Sutskever).\n\nRight now the code supports using the language model as a feature extractor.\n\n```\nfrom encoder import Model\n\nmodel = Model()\ntext = ['demo!']\ntext_features = model.transform(text)\n```\n\nA demo of using the features for sentiment classification as reported in the paper for the binary version of the Stanford Sentiment Treebank (SST) is included as `sst_binary_demo.py`. Additionally this demo visualizes the distribution of the sentiment unit like Figure 3 in the paper.\n\n![Sentiment Unit Visualization](/data/sst_binary_sentiment_unit_vis.png)\n\nAdditionally there is a [PyTorch port](https://github.com/guillitte/pytorch-sentiment-neuron) made by @guillitte which demonstrates how to train a model from scratch.\n\nThis repo also contains the parameters of the multiplicative LSTM model with 4,096 units we trained on the Amazon product review dataset introduced in McAuley et al. (2015) [1]. The dataset in de-duplicated form contains over 82 million product reviews from May 1996 to July 2014 amounting to over 38 billion training bytes. Training took one month across four NVIDIA Pascal GPUs, with our model processing 12,500 characters per second.\n\n[1] McAuley, Julian, Pandey, Rahul, and Leskovec, Jure. Inferring networks of substitutable and complementary products. In *Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, pp. 785\u2013794. ACM, 2015.\n", "release_dates": []}, {"name": "glide-text2im", "description": "GLIDE: a diffusion-based text-conditional image synthesis model", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# GLIDE\n\nThis is the official codebase for running the small, filtered-data GLIDE model from [GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](https://arxiv.org/abs/2112.10741).\n\nFor details on the pre-trained models in this repository, see the [Model Card](model-card.md).\n\n# Usage\n\nTo install this package, clone this repository and then run:\n\n```\npip install -e .\n```\n\nFor detailed usage examples, see the [notebooks](notebooks) directory.\n\n * The [text2im](notebooks/text2im.ipynb) [![][colab]][colab-text2im] notebook shows how to use GLIDE (filtered) with classifier-free guidance to produce images conditioned on text prompts. \n * The [inpaint](notebooks/inpaint.ipynb) [![][colab]][colab-inpaint] notebook shows how to use GLIDE (filtered) to fill in a masked region of an image, conditioned on a text prompt. \n * The [clip_guided](notebooks/clip_guided.ipynb) [![][colab]][colab-guided] notebook shows how to use GLIDE (filtered) + a filtered noise-aware CLIP model to produce images conditioned on text prompts. \n\n[colab]: <https://colab.research.google.com/assets/colab-badge.svg>\n[colab-text2im]: <https://colab.research.google.com/github/openai/glide-text2im/blob/main/notebooks/text2im.ipynb>\n[colab-inpaint]: <https://colab.research.google.com/github/openai/glide-text2im/blob/main/notebooks/inpaint.ipynb>\n[colab-guided]: <https://colab.research.google.com/github/openai/glide-text2im/blob/main/notebooks/clip_guided.ipynb>\n", "release_dates": []}, {"name": "glow", "description": "Code for reproducing results in \"Glow: Generative Flow with Invertible 1x1 Convolutions\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Glow\n\nCode for reproducing results in [\"Glow: Generative Flow with Invertible 1x1 Convolutions\"](https://d4mucfpksywv.cloudfront.net/research-covers/glow/paper/glow.pdf)\n\nTo use pretrained CelebA-HQ model, make your own manipulation vectors and run our interactive demo, check `demo` folder.\n\n## Requirements\n\n - Tensorflow (tested with v1.8.0)\n - Horovod (tested with v0.13.8) and (Open)MPI\n\nRun\n```\npip install -r requirements.txt\n```\n\nTo setup (Open)MPI, check instructions on Horovod github [page](https://github.com/uber/horovod).\n\n## Download datasets\nFor small scale experiments, use MNIST/CIFAR-10 (directly downloaded by `train.py` using keras)\n\nFor larger scale experiments, the datasets used are in the Google Cloud locations `https://openaipublic.azureedge.net/glow-demo/data/{dataset_name}-tfr.tar`. The dataset_names are below, we mention the exact preprocessing / downsampling method for a correct comparison of likelihood.\n\nQuantitative results\n- `imagenet-oord` - 20GB. Unconditional ImageNet 32x32 and 64x64, as described in PixelRNN/RealNVP papers (we downloaded [this](http://image-net.org/small/download.php) processed version).\n- `lsun_realnvp` - 140GB. LSUN 96x96. Random 64x64 crops taken at processing time, as described in RealNVP.\n\nQualitative results\n- `celeba` - 4GB. CelebA-HQ 256x256 dataset, as described in Progressive growing of GAN's. For 1024x1024 version (120GB), use `celeba-full-tfr.tar` while downloading.\n- `imagenet` - 20GB. ImageNet 32x32 and 64x64 with class labels. Centre cropped, area downsampled.\n- `lsun` - 700GB. LSUN 256x256. Centre cropped, area downsampled.\n\nTo download and extract celeb for example, run\n```\nwget https://openaipublic.azureedge.net/glow-demo/data/celeba-tfr.tar\ntar -xvf celeb-tfr.tar\n```\nChange `hps.data_dir` in train.py file to point to the above folder (or use the `--data_dir` flag when you run train.py)\n\nFor `lsun`, since download can be quite big, you can instead follow the instructions in `data_loaders/generate_tfr/lsun.py` to generate the tfr file directly from LSUN images. `church_outdoor` will be the smallest category.\n\n## Simple Train with 1 GPU\n\nRun wtih small depth to test\n```\nCUDA_VISIBLE_DEVICES=0 python train.py --depth 1\n```\n\n## Train with multiple GPUs using MPI and Horovod\n\nRun default training script with 8 GPUs:\n```\nmpiexec -n 8 python train.py\n```\n\n##### Ablation experiments\n\n```\nmpiexec -n 8 python train.py --problem cifar10 --image_size 32 --n_level 3 --depth 32 --flow_permutation [0/1/2] --flow_coupling [0/1] --seed [0/1/2] --learntop --lr 0.001\n```\n\nPretrained models, logs and samples\n```\nwget https://openaipublic.azureedge.net/glow-demo/logs/abl-[reverse/shuffle/1x1]-[add/aff].tar\n```\n\n##### CIFAR-10 Quantitative result\n\n```\nmpiexec -n 8 python train.py --problem cifar10 --image_size 32 --n_level 3 --depth 32 --flow_permutation 2 --flow_coupling 1 --seed 0 --learntop --lr 0.001 --n_bits_x 8\n```\n\n##### ImageNet 32x32 Quantitative result\n\n```\nmpiexec -n 8 python train.py --problem imagenet-oord --image_size 32 --n_level 3 --depth 48 --flow_permutation 2 --flow_coupling 1 --seed 0 --learntop --lr 0.001 --n_bits_x 8\n```\n\n##### ImageNet 64x64 Quantitative result\n```\nmpiexec -n 8 python train.py --problem imagenet-oord --image_size 64 --n_level 4 --depth 48 --flow_permutation 2 --flow_coupling 1 --seed 0 --learntop --lr 0.001 --n_bits_x 8\n```\n\n##### LSUN 64x64 Quantitative result\n```\nmpiexec -n 8 python train.py --problem lsun_realnvp --category [bedroom/church_outdoor/tower] --image_size 64 --n_level 3 --depth 48 --flow_permutation 2 --flow_coupling 1 --seed 0 --learntop --lr 0.001 --n_bits_x 8\n```\n\nPretrained models, logs and samples\n```\nwget https://openaipublic.azureedge.net/glow-demo/logs/lsun-rnvp-[bdr/crh/twr].tar\n```\n\n##### CelebA-HQ 256x256 Qualitative result\n\n```\nmpiexec -n 40 python train.py --problem celeba --image_size 256 --n_level 6 --depth 32 --flow_permutation 2 --flow_coupling 0 --seed 0 --learntop --lr 0.001 --n_bits_x 5\n```\n\n##### LSUN 96x96 and 128x128 Qualitative result\n```\nmpiexec -n 40 python train.py --problem lsun --category [bedroom/church_outdoor/tower] --image_size [96/128] --n_level 5 --depth 64 --flow_permutation 2 --flow_coupling 0 --seed 0 --learntop --lr 0.001 --n_bits_x 5\n```\n\nLogs and samples\n```\nwget https://openaipublic.azureedge.net/glow-demo/logs/lsun-bdr-[96/128].tar\n```\n\n##### Conditional CIFAR-10 Qualitative result\n```\nmpiexec -n 8 python train.py --problem cifar10 --image_size 32 --n_level 3 --depth 32 --flow_permutation 2 --flow_coupling 0 --seed 0 --learntop --lr 0.001 --n_bits_x 5 --ycond --weight_y=0.01\n```\n\n##### Conditional ImageNet 32x32 Qualitative result\n```\nmpiexec -n 8 python train.py --problem imagenet --image_size 32 --n_level 3 --depth 48 --flow_permutation 2 --flow_coupling 0 --seed 0 --learntop --lr 0.001 --n_bits_x 5 --ycond --weight_y=0.01\n```\n", "release_dates": []}, {"name": "go-alias", "description": "Service for quickly aliasing and redirecting to long URLs", "language": "JavaScript", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\nUse this service to quickly alias and redirect to long URLs\n\n## Setup\n### Server setup\nRecommended to use Heroku to host the service if you don't care about security. Should take about 5 minutes to stand up. Otherwise you can put it on a Kubernetes cluster.\n1. Create a Firebase project\n1. Export Firebase creds on your [Firebase service account](https://console.developers.google.com/iam-admin/serviceaccounts/) as a json file\n1. Create a new Heroku app and connect it to this git repo or your fork of it\n1. Make json file's creds available as environment variables in your deployment environment (eg. through Heroku dashboard)\n    * `FIREBASE_PROJECT_ID`\n    * `FIREBASE_CLIENT_EMAIL`\n    * `FIREBASE_DATABASE_NAME`\n    * `FIREBASE_PRIVATE_KEY` which can have literal `\\n` instead of actual newlines\n1. Deploy!\n\n### Client setup\nAdd this site to your <a href=\"chrome://settings/searchEngines\">Chrome search engines</a> by scrolling down until you see three empty boxes. Enter values <b>go</b>, <b>go</b>, and <b>go.corp.mycompany.com/%s</b>, respectively.\n\n### Advanced setup\nTo get the most benefit from the service, you should setup a DNS entry\non your local network, `go.corp.mycompany.com`. Make sure that\n`corp.mycompany.com` is in the search domains for each user on the\nnetwork. This is usually easily accomplished by configuring your DHCP\nserver. Now, simply typing `go` into your browser should take you to\nthe service, where you can register shortcuts. Those\nshortcuts will also be available by typing `go/shortcut`.\n\nOn a mac, you can add the domain to your DNS search paths\n```\nnetworksetup -setsearchdomains Wi-Fi corp.example.com\n```\nHowever, this will require all bare names to hang instead of failing instantly, so you may wish to stick with the Chrome-only solution.\n\n## Use\nWhen you type in the Chrome URL bar <b>go&lt;TAB&gt;</b> anything you type afterwards will bring you to that alias here.\nYou can then paste your link in the box and the next time you visit this alias, it will bring you straight to the site you pasted before.\n\nYou can override any alias by visiting eg., #{path}/myalias/mynewsite.com\n\n## Develop\nExport your firebase credentials as appropriate environment variables. You'll need:\n* `FIREBASE_PROJECT_ID`\n* `FIREBASE_CLIENT_EMAIL`\n* `FIREBASE_DATABASE_NAME`\n* `FIREBASE_PRIVATE_KEY` which can have literal `\\n` instead of actual newlines\n\n```\nnpm install\nnpm install -g devtool\nnpm run dev\n```\n\nVisit http://localhost:3000/myalias\n\n## Test in docker\n```\nmake run\n```\n\n## Deploy\n```\nkubectl apply -f kube/api-deployment.yaml\nkubectl apply -f kube/api-svc.yaml\n```\n", "release_dates": []}, {"name": "go-retryablehttp", "description": "Retryable HTTP client in Go", "language": "Go", "license": {"key": "mpl-2.0", "name": "Mozilla Public License 2.0", "spdx_id": "MPL-2.0", "url": "https://api.github.com/licenses/mpl-2.0", "node_id": "MDc6TGljZW5zZTE0"}, "readme": "go-retryablehttp\n================\n\n[![Build Status](http://img.shields.io/travis/hashicorp/go-retryablehttp.svg?style=flat-square)][travis]\n[![Go Documentation](http://img.shields.io/badge/go-documentation-blue.svg?style=flat-square)][godocs]\n\n[travis]: http://travis-ci.org/hashicorp/go-retryablehttp\n[godocs]: http://godoc.org/github.com/hashicorp/go-retryablehttp\n\nThe `retryablehttp` package provides a familiar HTTP client interface with\nautomatic retries and exponential backoff. It is a thin wrapper over the\nstandard `net/http` client library and exposes nearly the same public API. This\nmakes `retryablehttp` very easy to drop into existing programs.\n\n`retryablehttp` performs automatic retries under certain conditions. Mainly, if\nan error is returned by the client (connection errors, etc.), or if a 500-range\nresponse code is received (except 501), then a retry is invoked after a wait\nperiod.  Otherwise, the response is returned and left to the caller to\ninterpret.\n\nThe main difference from `net/http` is that requests which take a request body\n(POST/PUT et. al) can have the body provided in a number of ways (some more or\nless efficient) that allow \"rewinding\" the request body if the initial request\nfails so that the full request can be attempted again. See the\n[godoc](http://godoc.org/github.com/hashicorp/go-retryablehttp) for more\ndetails.\n\nVersion 0.6.0 and before are compatible with Go prior to 1.12. From 0.6.1 onward, Go 1.12+ is required.\nFrom 0.6.7 onward, Go 1.13+ is required.\n\nExample Use\n===========\n\nUsing this library should look almost identical to what you would do with\n`net/http`. The most simple example of a GET request is shown below:\n\n```go\nresp, err := retryablehttp.Get(\"/foo\")\nif err != nil {\n    panic(err)\n}\n```\n\nThe returned response object is an `*http.Response`, the same thing you would\nusually get from `net/http`. Had the request failed one or more times, the above\ncall would block and retry with exponential backoff.\n\n## Getting a stdlib `*http.Client` with retries\n\nIt's possible to convert a `*retryablehttp.Client` directly to a `*http.Client`.\nThis makes use of retryablehttp broadly applicable with minimal effort. Simply\nconfigure a `*retryablehttp.Client` as you wish, and then call `StandardClient()`:\n\n```go\nretryClient := retryablehttp.NewClient()\nretryClient.RetryMax = 10\n\nstandardClient := retryClient.StandardClient() // *http.Client\n```\n\nFor more usage and examples see the\n[godoc](http://godoc.org/github.com/hashicorp/go-retryablehttp).\n", "release_dates": []}, {"name": "go-vncdriver", "description": "Fast VNC driver", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# go-vncdriver\n\nA fast VNC driver.\n\n## Installation on Ubuntu:\n\nIf you have Ubuntu 14, get the latest Go compiler:\n```sh\n# Needed for Ubuntu 14, not Ubuntu 16\n$ sudo add-apt-repository ppa:ubuntu-lxc/lxd-stable  # for newer golang\n$ sudo apt-get update\n```\nThen\n```sh\n$ sudo apt-get install -y python-dev make golang libjpeg-turbo8-dev\n```\nAnd if you want OpenGL rendering support (you probably do, unless you're running on a headless server):\n```sh\n$ sudo apt-get install libx11-dev libxcursor-dev libxrandr-dev libxinerama-dev libxi-dev \\\n  libxxf86vm-dev libgl1-mesa-dev mesa-common-dev\n```\nNOTE: If you're using a Python named something other than `python`, such as `python3`, replace both `python` and `pip` below with the commands for the corresponding Python\n```sh\n$ git clone https://github.com/openai/go-vncdriver.git\n$ cd go-vncdriver\n$ python build.py\n$ pip install -e .\n```\n\n## Installation on OSX:\n\n```\n$ brew install libjpeg-turbo golang\n```\n\n(On OSX newer than El Capitan, you may need to\n[install golang](https://golang.org/doc/install) from their site, and\nthen just install `brew install libjpeg-turbo`.)\n\nThen\n```\n$ git clone https://github.com/openai/go-vncdriver.git\n$ cd go-vncdriver\n$ python build.py\n$ pip install -e .\n```\n\n## OpenGL viewer\n\nThe OpenGL renderer is optional. If you get an error of the form:\n\n```\ngo_vncdriver was installed without OpenGL support. See https://github.com/openai/go-vncdriver for details on how debug.\n```\n\nThat means that your `go-vncdriver` was built without OpenGL\nsupport. (The installer will first try to install with OpenGL, but\nwill fall back to installing without it.)\n\nDo the installation steps above, including the extra dependencies to add OpenGL rendering.\n\nIf you get errors like below, the dependencies aren't installed properly:\n```\nfatal error: X11/Xcursor/Xcursor.h: No such file or directory\nfatal error: X11/extensions/Xrandr.h: No such file or directory\nfatal error: X11/extensions/XInput.h: No such file or directory\nfatal error: GL/gl.h: No such file or directory\n```\n\n## Python versions\n\n`go_vncdriver` has been tested on Python 2.7 and 3.5.\n", "release_dates": ["2016-12-03T02:08:40Z"]}, {"name": "go-vncdriver-feedstock", "description": "A conda-smithy repository for go-vncdriver.", "language": "Shell", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\nAbout go-vncdriver\n==================\n\nHome: https://github.com/openai/go-vncdriver\n\nPackage license: \n\nFeedstock license: BSD 3-Clause\n\nSummary: fast VNC viewer\n\n\n\nCurrent build status\n====================\n\nLinux: [![Circle CI](https://circleci.com/gh/openai/go-vncdriver-feedstock.svg?style=shield)](https://circleci.com/gh/openai/go-vncdriver-feedstock)\nOSX: [![TravisCI](https://travis-ci.org/openai/go-vncdriver-feedstock.svg?branch=master)](https://travis-ci.org/openai/go-vncdriver-feedstock)\nWindows: [![AppVeyor](https://ci.appveyor.com/api/projects/status/github/openai/go-vncdriver-feedstock?svg=True)](https://ci.appveyor.com/project/openai/go-vncdriver-feedstock/branch/master)\n\nCurrent release info\n====================\nVersion: [![Anaconda-Server Badge](https://anaconda.org/jeremyschlatter/go-vncdriver/badges/version.svg)](https://anaconda.org/jeremyschlatter/go-vncdriver)\nDownloads: [![Anaconda-Server Badge](https://anaconda.org/jeremyschlatter/go-vncdriver/badges/downloads.svg)](https://anaconda.org/jeremyschlatter/go-vncdriver)\n\nInstalling go-vncdriver\n=======================\n\nInstalling `go-vncdriver` from the `jeremyschlatter` channel can be achieved by adding `jeremyschlatter` to your channels with:\n\n```\nconda config --add channels jeremyschlatter\n```\n\nOnce the `jeremyschlatter` channel has been enabled, `go-vncdriver` can be installed with:\n\n```\nconda install go-vncdriver\n```\n\nIt is possible to list all of the versions of `go-vncdriver` available on your platform with:\n\n```\nconda search go-vncdriver --channel jeremyschlatter\n```\n\n\n\n\nUpdating go-vncdriver-feedstock\n===============================\n\nIf you would like to improve the go-vncdriver recipe or build a new\npackage version, please fork this repository and submit a PR. Upon submission,\nyour changes will be run on the appropriate platforms to give the reviewer an\nopportunity to confirm that the changes result in a successful build. Once\nmerged, the recipe will be re-built and uploaded automatically to the\n`jeremyschlatter` channel, whereupon the built conda packages will be available for\neverybody to install and use from the `jeremyschlatter` channel.\nNote that all branches in the conda-forge/go-vncdriver-feedstock are\nimmediately built and any created packages are uploaded, so PRs should be based\non branches in forks and branches in the main repository should only be used to\nbuild distinct package versions.\n\nIn order to produce a uniquely identifiable distribution:\n * If the version of a package **is not** being increased, please add or increase\n   the [``build/number``](http://conda.pydata.org/docs/building/meta-yaml.html#build-number-and-string).\n * If the version of a package **is** being increased, please remember to return\n   the [``build/number``](http://conda.pydata.org/docs/building/meta-yaml.html#build-number-and-string)\n   back to 0.\n", "release_dates": []}, {"name": "gpt-2", "description": "Code for the paper \"Language Models are Unsupervised Multitask Learners\"", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# gpt-2\n\nCode and models from the paper [\"Language Models are Unsupervised Multitask Learners\"](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).\n\nYou can read about GPT-2 and its staged release in our [original blog post](https://openai.com/research/better-language-models/), [6 month follow-up post](https://openai.com/blog/gpt-2-6-month-follow-up/), and [final post](https://www.openai.com/blog/gpt-2-1-5b-release/).\n\nWe have also [released a dataset](https://github.com/openai/gpt-2-output-dataset) for researchers to study their behaviors.\n\n<sup>*</sup> *Note that our original parameter counts were wrong due to an error (in our previous blog posts and paper).  Thus you may have seen small referred to as 117M and medium referred to as 345M.*\n\n## Usage\n\nThis repository is meant to be a starting point for researchers and engineers to experiment with GPT-2.\n\nFor basic information, see our [model card](./model_card.md).\n\n### Some caveats\n\n- GPT-2 models' robustness and worst case behaviors are not well-understood.  As with any machine-learned model, carefully evaluate GPT-2 for your use case, especially if used without fine-tuning or in safety-critical applications where reliability is important.\n- The dataset our GPT-2 models were trained on contains many texts with [biases](https://twitter.com/TomerUllman/status/1101485289720242177) and factual inaccuracies, and thus GPT-2 models are likely to be biased and inaccurate as well.\n- To avoid having samples mistaken as human-written, we recommend clearly labeling samples as synthetic before wide dissemination.  Our models are often incoherent or inaccurate in subtle ways, which takes more than a quick read for a human to notice.\n\n### Work with us\n\nPlease [let us know](mailto:languagequestions@openai.com) if you\u2019re doing interesting research with or working on applications of GPT-2!  We\u2019re especially interested in hearing from and potentially working with those who are studying\n- Potential malicious use cases and defenses against them (e.g. the detectability of synthetic text)\n- The extent of problematic content (e.g. bias) being baked into the models and effective mitigations\n\n## Development\n\nSee [DEVELOPERS.md](./DEVELOPERS.md)\n\n## Contributors\n\nSee [CONTRIBUTORS.md](./CONTRIBUTORS.md)\n\n## Citation\n\nPlease use the following bibtex entry:\n```\n@article{radford2019language,\n  title={Language Models are Unsupervised Multitask Learners},\n  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n  year={2019}\n}\n```\n\n## Future work\n\nWe may release code for evaluating the models on various benchmarks.\n\nWe are still considering release of the larger models.\n\n## License\n\n[Modified MIT](./LICENSE)\n", "release_dates": []}, {"name": "gpt-2-output-dataset", "description": "Dataset of GPT-2 outputs for research in detection, biases, and more", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# gpt-2-output-dataset\n\nThis dataset contains:\n- 250K documents from the WebText test set\n- For each GPT-2 model (trained on the WebText training set), 250K random samples (temperature 1, no truncation) and 250K samples generated with Top-K 40 truncation\n\nWe look forward to the research produced using this data!\n\n### Download\n\nFor each model, we have a training split of 250K generated examples, as well as validation and test splits of 5K examples.\n\nAll data is located in Google Cloud Storage, under the directory `gs://gpt-2/output-dataset/v1`.  (NOTE: everything has been migrated to Azure `https://openaipublic.blob.core.windows.net/gpt-2/output-dataset/v1/`)\n\nThere, you will find files:\n\n- `webtext.${split}.jsonl`\n- `small-117M.${split}.jsonl`\n- `small-117M-k40.${split}.jsonl`\n- `medium-345M.${split}.jsonl`\n- `medium-345M-k40.${split}.jsonl`\n- `large-762M.${split}.jsonl`\n- `large-762M-k40.${split}.jsonl`\n- `xl-1542M.${split}.jsonl`\n- `xl-1542M-k40.${split}.jsonl`\n\nwhere split is one of `train`, `test`, and `valid`.\n\nWe've provided a script to download all of them, in `download_dataset.py`.\n\n#### Finetuned model samples\n\nAdditionally, we encourage research on detection of finetuned models.  We have released data under `gs://gpt-2/output-dataset/v1-amazonfinetune/` with samples from a GPT-2 full model finetuned to output Amazon reviews.\n\n### Detectability baselines\n\nWe're interested in seeing research in detectability of GPT-2 model family generations.\n\nWe provide some [initial analysis](detection.md) of two baselines, as well as [code](./baseline.py) for the better baseline.\n\nOverall, we are able to achieve accuracies in the mid-90s for Top-K 40 generations, and mid-70s to high-80s (depending on model size) for random generations.  We also find some evidence that adversaries can evade detection via finetuning from released models.\n\n### Data removal requests\n\nIf you believe your work is included in WebText and would like us to remove it, please let us know at webtextdata@openai.com.\n", "release_dates": []}, {"name": "gpt-3", "description": "GPT-3: Language Models are Few-Shot Learners", "language": null, "license": null, "readme": "# GPT-3: Language Models are Few-Shot Learners\n\n[arXiv link](https://arxiv.org/abs/2005.14165)\n> Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions \u2013 something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans.  We discuss broader societal impacts of this finding and of GPT-3 in general.\n\n## Contents\n- [175b_samples.jsonl](175b_samples.jsonl) - Unconditional, unfiltered 2048 token samples from GPT-3 with p=.85, t=1.&#12288;\n**CONTENT WARNING:** GPT-3 was trained on arbitrary data from the web, so may contain offensive content and language.\n- [data](data) - Synthetic datasets for word scramble and arithmetic tasks described in the paper.\n- [dataset_statistics](dataset_statistics) - Statistics for all languages included in the training dataset mix.\n- [overlap_frequency.md](overlap_frequency.md) - Samples of 13-gram overlaps between our training data and benchmarks, selected by frequency in the training set.\n- [model-card.md](model-card.md) - GPT-3 Model Card.\n\n## How to cite\n```\n@article{brown2020language,\n    title={Language Models are Few-Shot Learners},\n    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},\n    year={2020},\n    eprint={2005.14165},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n", "release_dates": []}, {"name": "GPT-3-Encoder", "description": "Javascript BPE Encoder Decoder for GPT-2 / GPT-3", "language": null, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "GPT-3-Encoder\nJavascript BPE Encoder Decoder for GPT-2 / GPT-3\n\n## About\nGPT-2 and GPT-3 use byte pair encoding to turn text into a series of integers to feed into the model. This is a javascript implementation of OpenAI's original python encoder/decoder which can be found [here](https://github.com/openai/gpt-2)\n\n## Install with npm\n\n`npm install gpt-3-encoder`\n\n## Usage\n\nCompatible with Node >= 12\n\n```\nconst {encode, decode} = require('gpt-3-encoder')\n\nconst str = 'This is an example sentence to try encoding out on!'\nconst encoded = encode(str)\nconsole.log('Encoded this string looks like: ', encoded)\n\nconsole.log('We can look at each token and what it represents')\nfor(let token of encoded){\n  console.log({token, string: decode([token])})\n}\n\nconst decoded = decode(encoded)\nconsole.log('We can decode it back into:\\n', decoded)\n\n```\n\n\n", "release_dates": []}, {"name": "gpt-discord-bot", "description": "Example Discord bot written in Python that uses the completions API to have conversations with the `text-davinci-003` model, and the moderations API to filter the messages.", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Please read!\n\n\n**For any problems running this specific bot:** [Discord Project Post](https://discord.com/channels/974519864045756446/1055336272543092757)\n\n**For general OpenAI API problems or questions:** [Discord API Discussions](https://discord.com/channels/974519864045756446/1037561178286739466)\n\n**For bugs in the template code:** create an Issue\n\n**For feature requests:** this repo is not accepting feature requests, you can discuss potential features in [Discord Project Post](https://discord.com/channels/974519864045756446/1055336272543092757)\n\n**For PRs:** only bug fix PRs wil be accepted. If you are implementing a new feature, please fork this repo.\n\nThank you!\n\n---\n# GPT Discord Bot\n\nExample Discord bot written in Python that uses the [chat completions API](https://platform.openai.com/docs/api-reference/chat/create) to have conversations with the `gpt-3.5-turbo` model, and the [moderations API](https://beta.openai.com/docs/api-reference/moderations) to filter the messages.\n\nThis bot uses the [OpenAI Python Library](https://github.com/openai/openai-python) and [discord.py](https://discordpy.readthedocs.io/).\n\n\n# Features\n\n- `/chat` starts a public thread, with a `message` argument which is the first user message passed to the bot. You can optionally also adjust the `temperature` and `max_tokens` parameters.\n- The model will generate a reply for every user message in any threads started with `/chat`\n- The entire thread will be passed to the model for each request, so the model will remember previous messages in the thread\n- when the context limit is reached, or a max message count is reached in the thread, bot will close the thread\n- you can customize the bot instructions by modifying `config.yaml`\n- you can change the model, the default value is `gpt-3.5-turbo`\n\n# Setup\n\n1. Copy `.env.example` to `.env` and start filling in the values as detailed below\n1. Go to https://beta.openai.com/account/api-keys, create a new API key, and fill in `OPENAI_API_KEY`\n1. Create your own Discord application at https://discord.com/developers/applications\n1. Go to the Bot tab and click \"Add Bot\"\n    - Click \"Reset Token\" and fill in `DISCORD_BOT_TOKEN`\n    - Disable \"Public Bot\" unless you want your bot to be visible to everyone\n    - Enable \"Message Content Intent\" under \"Privileged Gateway Intents\"\n1. Go to the OAuth2 tab, copy your \"Client ID\", and fill in `DISCORD_CLIENT_ID`\n1. Copy the ID the server you want to allow your bot to be used in by right clicking the server icon and clicking \"Copy ID\". Fill in `ALLOWED_SERVER_IDS`. If you want to allow multiple servers, separate the IDs by \",\" like `server_id_1,server_id_2`\n1. Install dependencies and run the bot\n    ```\n    pip install -r requirements.txt\n    python -m src.main\n    ```\n    You should see an invite URL in the console. Copy and paste it into your browser to add the bot to your server.\n    Note: make sure you are using Python 3.9+ (check with python --version)\n\n# Optional configuration\n\n1. If you want moderation messages, create and copy the channel id for each server that you want the moderation messages to send to in `SERVER_TO_MODERATION_CHANNEL`. This should be of the format: `server_id:channel_id,server_id_2:channel_id_2`\n1. If you want to change the personality of the bot, go to `src/config.yaml` and edit the instructions\n1. If you want to change the moderation settings for which messages get flagged or blocked, edit the values in `src/constants.py`. A higher value means less chance of it triggering, with 1.0 being no moderation at all for that category.\n\n# FAQ\n\n> Why isn't my bot responding to commands?\n\nEnsure that the channels your bots have access to allow the bot to have these permissions.\n- Send Messages\n- Send Messages in Threads\n- Create Public Threads\n- Manage Messages (only for moderation to delete blocked messages)\n- Manage Threads\n- Read Message History\n- Use Application Commands\n", "release_dates": []}, {"name": "grade-school-math", "description": null, "language": "Python", "license": null, "readme": "**Status**: Archive (code is provided as-is, no updates expected)\n\n# Grade School Math\n\n#### [[Blog Post]](https://openai.com/blog/grade-school-math/) [[Paper]](https://arxiv.org/abs/2110.14168)\n\nState-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we're releasing GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution.\n\n<p align=\"center\">\n    <img src=\"grade_school_math/img/example_problems.png\" height=\"300\"/>\n</p>\n\n## Dataset Details\n\nGSM8K consists of 8.5K high quality grade school math problems created by human problem writers. We segmented these into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ - / \\*) to reach the final answer. A bright middle school student should be able to solve every problem.\n\nThe raw data files can be found in:\n\n- `grade_school_math/data/train.jsonl`\n- `grade_school_math/data/test.jsonl`\n\nEach line of those files corresponds to a single grade school math problem, saved as a json dictionary (with a \"question\" key and an \"answer\" key). The answer is formatted such that it uses calculation annotations and so that the final numeric solution is the final line of the solution, preceded by `####`.\n\n### Calculation Annotations\n\nOur models frequently fail to accurately perform calculations. Although larger models make fewer arithmetic mistakes than smaller models, this remains a common source of errors. To mitigate this issue, we train our models to use a calculator by injecting calculation annotations into the training set. At training time, we simply finetune on this language data as is. At test time, a calculator will override sampling when the model chooses to use these annotations. An example implementation of the calculator sampling can be found in `calculator.py`.\n\nIf you would like to remove the calculator annotations, simply remove any string that starts with `<<` and ends with `>>`.\n\n### Solution Extracting\n\nTo extract the final numeric solution for a particular question, simply parse the completion to extract the numeric value immediately following the `####` token. Some example python code to do so is shown in `dataset.py:is_correct`.\n\n### Socratic Dataset\n\nDuring our research, we also investigated a modified solution format that injects automatically generated \"Socratic subquestions\" before each step. Although we ultimately did not use this format for any experiments in the paper, we make this data available to anyone who is interested.\n\nWe show an example below, with the socratic subquestions in bold:\n\n<pre>\nA carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\n<b>How much did the booth make selling cotton candy each day? **</b> The booth made $50 x 3 = $<<50*3=150>>150 selling cotton candy each day.\n<b>How much did the booth make in a day? **</b> In a day, the booth made a total of $150 + $50 = $<<150+50=200>>200.\n<b>How much did the booth make in 5 days? **</b> In 5 days, they made a total of $200 x 5 = $<<200*5=1000>>1000.\n<b>How much did the booth have to pay? **</b> The booth has to pay a total of $30 + $75 = $<<30+75=105>>105.\n<b>How much did the booth earn after paying the rent and the cost of ingredients? **</b> Thus, the booth earned $1000 - $105 = $<<1000-105=895>>895.\n</pre>\n\nWe generated each Socratic subquestion by conditioning on each ground truth (contractor-provided) step in a solution, using a model specifically finetuned for this task (on around 800 examples). To construct the full Socratic dataset, each step in the solution was prefixed by the model-generated Socratic subquestion. Steps were otherwise left untouched.\n\nThese data files can be found in:\n\n- `grade_school_math/data/train_socratic.jsonl`\n- `grade_school_math/data/test_socratic.jsonl`\n\n## View Model Solutions\n\nFor each test question, we provide solutions generated from 6B finetuning, 6B verification, 175B finetuning and 175B verification. This data can be found in:\n\n- `grade_school_math/data/example_model_solutions.jsonl`\n\nTo view these results problem-by-problem, run:\n\n```bash\npython view_model_solutions.py\n```\n\nNote: These model-generated samples used a slightly older version of the calculator. Previous implementation bugs led to calculator failures in roughly 1% of model samples. Those issues have been fixed in the codebase, but since the samples have not been regenerated, occasional calculation errors are present.\n\n## Citation\n\nPlease use the below BibTeX entry to cite this dataset:\n\n```\n@article{cobbe2021gsm8k,\n  title={Training Verifiers to Solve Math Word Problems},\n  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},\n  journal={arXiv preprint arXiv:2110.14168},\n  year={2021}\n}\n```\n\n# Usage\n\nWe present a basic example of training a GPT2 sized model and using the calculator in the sampling process. We include this code for illustrative purposes only. This pipeline was not used for any experiments in the paper.\n\n**Training a Model**\n\n```bash\npython train.py\n```\n\n**Sampling from the Model**\n\n```bash\npython sample.py\n```\n\nThe core calculator sampling logic can be found in `calculator.py:sample`. Note that this code is inefficient as implemented. Specifically, the function does not support batches, and does not cache activations from previous tokens.\n", "release_dates": []}, {"name": "grok", "description": null, "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# OpenAI Grok Curve Experiments\n\n```bash\npip install -e .\n./scripts/train.py\n```", "release_dates": []}, {"name": "guided-diffusion", "description": null, "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# guided-diffusion\n\nThis is the codebase for [Diffusion Models Beat GANS on Image Synthesis](http://arxiv.org/abs/2105.05233).\n\nThis repository is based on [openai/improved-diffusion](https://github.com/openai/improved-diffusion), with modifications for classifier conditioning and architecture improvements.\n\n# Download pre-trained models\n\nWe have released checkpoints for the main models in the paper. Before using these models, please review the corresponding [model card](model-card.md) to understand the intended use and limitations of these models.\n\nHere are the download links for each model checkpoint:\n\n * 64x64 classifier: [64x64_classifier.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/64x64_classifier.pt)\n * 64x64 diffusion: [64x64_diffusion.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/64x64_diffusion.pt)\n * 128x128 classifier: [128x128_classifier.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/128x128_classifier.pt)\n * 128x128 diffusion: [128x128_diffusion.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/128x128_diffusion.pt)\n * 256x256 classifier: [256x256_classifier.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_classifier.pt)\n * 256x256 diffusion: [256x256_diffusion.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion.pt)\n * 256x256 diffusion (not class conditional): [256x256_diffusion_uncond.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt)\n * 512x512 classifier: [512x512_classifier.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/512x512_classifier.pt)\n * 512x512 diffusion: [512x512_diffusion.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/512x512_diffusion.pt)\n * 64x64 -&gt; 256x256 upsampler: [64_256_upsampler.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/64_256_upsampler.pt)\n * 128x128 -&gt; 512x512 upsampler: [128_512_upsampler.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/128_512_upsampler.pt)\n * LSUN bedroom: [lsun_bedroom.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/lsun_bedroom.pt)\n * LSUN cat: [lsun_cat.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/lsun_cat.pt)\n * LSUN horse: [lsun_horse.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/lsun_horse.pt)\n * LSUN horse (no dropout): [lsun_horse_nodropout.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/lsun_horse_nodropout.pt)\n\n# Sampling from pre-trained models\n\nTo sample from these models, you can use the `classifier_sample.py`, `image_sample.py`, and `super_res_sample.py` scripts.\nHere, we provide flags for sampling from all of these models.\nWe assume that you have downloaded the relevant model checkpoints into a folder called `models/`.\n\nFor these examples, we will generate 100 samples with batch size 4. Feel free to change these values.\n\n```\nSAMPLE_FLAGS=\"--batch_size 4 --num_samples 100 --timestep_respacing 250\"\n```\n\n## Classifier guidance\n\nNote for these sampling runs that you can set `--classifier_scale 0` to sample from the base diffusion model.\nYou may also use the `image_sample.py` script instead of `classifier_sample.py` in that case.\n\n * 64x64 model:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --dropout 0.1 --image_size 64 --learn_sigma True --noise_schedule cosine --num_channels 192 --num_head_channels 64 --num_res_blocks 3 --resblock_updown True --use_new_attention_order True --use_fp16 True --use_scale_shift_norm True\"\npython classifier_sample.py $MODEL_FLAGS --classifier_scale 1.0 --classifier_path models/64x64_classifier.pt --classifier_depth 4 --model_path models/64x64_diffusion.pt $SAMPLE_FLAGS\n```\n\n * 128x128 model:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --image_size 128 --learn_sigma True --noise_schedule linear --num_channels 256 --num_heads 4 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython classifier_sample.py $MODEL_FLAGS --classifier_scale 0.5 --classifier_path models/128x128_classifier.pt --model_path models/128x128_diffusion.pt $SAMPLE_FLAGS\n```\n\n * 256x256 model:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython classifier_sample.py $MODEL_FLAGS --classifier_scale 1.0 --classifier_path models/256x256_classifier.pt --model_path models/256x256_diffusion.pt $SAMPLE_FLAGS\n```\n\n * 256x256 model (unconditional):\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython classifier_sample.py $MODEL_FLAGS --classifier_scale 10.0 --classifier_path models/256x256_classifier.pt --model_path models/256x256_diffusion_uncond.pt $SAMPLE_FLAGS\n```\n\n * 512x512 model:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --image_size 512 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 False --use_scale_shift_norm True\"\npython classifier_sample.py $MODEL_FLAGS --classifier_scale 4.0 --classifier_path models/512x512_classifier.pt --model_path models/512x512_diffusion.pt $SAMPLE_FLAGS\n```\n\n## Upsampling\n\nFor these runs, we assume you have some base samples in a file `64_samples.npz` or `128_samples.npz` for the two respective models.\n\n * 64 -&gt; 256:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --large_size 256  --small_size 64 --learn_sigma True --noise_schedule linear --num_channels 192 --num_heads 4 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython super_res_sample.py $MODEL_FLAGS --model_path models/64_256_upsampler.pt --base_samples 64_samples.npz $SAMPLE_FLAGS\n```\n\n * 128 -&gt; 512:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16 --class_cond True --diffusion_steps 1000 --large_size 512 --small_size 128 --learn_sigma True --noise_schedule linear --num_channels 192 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython super_res_sample.py $MODEL_FLAGS --model_path models/128_512_upsampler.pt $SAMPLE_FLAGS --base_samples 128_samples.npz\n```\n\n## LSUN models\n\nThese models are class-unconditional and correspond to a single LSUN class. Here, we show how to sample from `lsun_bedroom.pt`, but the other two LSUN checkpoints should work as well:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --dropout 0.1 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython image_sample.py $MODEL_FLAGS --model_path models/lsun_bedroom.pt $SAMPLE_FLAGS\n```\n\nYou can sample from `lsun_horse_nodropout.pt` by changing the dropout flag:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --dropout 0.0 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython image_sample.py $MODEL_FLAGS --model_path models/lsun_horse_nodropout.pt $SAMPLE_FLAGS\n```\n\nNote that for these models, the best samples result from using 1000 timesteps:\n\n```\nSAMPLE_FLAGS=\"--batch_size 4 --num_samples 100 --timestep_respacing 1000\"\n```\n\n# Results\n\nThis table summarizes our ImageNet results for pure guided diffusion models:\n\n| Dataset          | FID  | Precision | Recall |\n|------------------|------|-----------|--------|\n| ImageNet 64x64   | 2.07 | 0.74      | 0.63   |\n| ImageNet 128x128 | 2.97 | 0.78      | 0.59   |\n| ImageNet 256x256 | 4.59 | 0.82      | 0.52   |\n| ImageNet 512x512 | 7.72 | 0.87      | 0.42   |\n\nThis table shows the best results for high resolutions when using upsampling and guidance together:\n\n| Dataset          | FID  | Precision | Recall |\n|------------------|------|-----------|--------|\n| ImageNet 256x256 | 3.94 | 0.83      | 0.53   |\n| ImageNet 512x512 | 3.85 | 0.84      | 0.53   |\n\nFinally, here are the unguided results on individual LSUN classes:\n\n| Dataset      | FID  | Precision | Recall |\n|--------------|------|-----------|--------|\n| LSUN Bedroom | 1.90 | 0.66      | 0.51   |\n| LSUN Cat     | 5.57 | 0.63      | 0.52   |\n| LSUN Horse   | 2.57 | 0.71      | 0.55   |\n\n# Training models\n\nTraining diffusion models is described in the [parent repository](https://github.com/openai/improved-diffusion). Training a classifier is similar. We assume you have put training hyperparameters into a `TRAIN_FLAGS` variable, and classifier hyperparameters into a `CLASSIFIER_FLAGS` variable. Then you can run:\n\n```\nmpiexec -n N python scripts/classifier_train.py --data_dir path/to/imagenet $TRAIN_FLAGS $CLASSIFIER_FLAGS\n```\n\nMake sure to divide the batch size in `TRAIN_FLAGS` by the number of MPI processes you are using.\n\nHere are flags for training the 128x128 classifier. You can modify these for training classifiers at other resolutions:\n\n```sh\nTRAIN_FLAGS=\"--iterations 300000 --anneal_lr True --batch_size 256 --lr 3e-4 --save_interval 10000 --weight_decay 0.05\"\nCLASSIFIER_FLAGS=\"--image_size 128 --classifier_attention_resolutions 32,16,8 --classifier_depth 2 --classifier_width 128 --classifier_pool attention --classifier_resblock_updown True --classifier_use_scale_shift_norm True\"\n```\n\nFor sampling from a 128x128 classifier-guided model, 25 step DDIM:\n\n```sh\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --image_size 128 --learn_sigma True --num_channels 256 --num_heads 4 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\nCLASSIFIER_FLAGS=\"--image_size 128 --classifier_attention_resolutions 32,16,8 --classifier_depth 2 --classifier_width 128 --classifier_pool attention --classifier_resblock_updown True --classifier_use_scale_shift_norm True --classifier_scale 1.0 --classifier_use_fp16 True\"\nSAMPLE_FLAGS=\"--batch_size 4 --num_samples 50000 --timestep_respacing ddim25 --use_ddim True\"\nmpiexec -n N python scripts/classifier_sample.py \\\n    --model_path /path/to/model.pt \\\n    --classifier_path path/to/classifier.pt \\\n    $MODEL_FLAGS $CLASSIFIER_FLAGS $SAMPLE_FLAGS\n```\n\nTo sample for 250 timesteps without DDIM, replace `--timestep_respacing ddim25` to `--timestep_respacing 250`, and replace `--use_ddim True` with `--use_ddim False`.\n", "release_dates": []}, {"name": "gym", "description": "A toolkit for developing and comparing reinforcement learning algorithms.", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://pre-commit.com/) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n## Important Notice\n\n### The team that has been maintaining Gym since 2021 has moved all future development to [Gymnasium](https://github.com/Farama-Foundation/Gymnasium), a drop in replacement for Gym (import gymnasium as gym), and Gym will not be receiving any future updates. Please switch over to Gymnasium as soon as you're able to do so. If you'd like to read more about the story behind this switch, please check out [this blog post](https://farama.org/Announcing-The-Farama-Foundation).\n\n## Gym\n\nGym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym's API has become the field standard for doing this.\n\nGym documentation website is at [https://www.gymlibrary.dev/](https://www.gymlibrary.dev/), and you can propose fixes and changes to it [here](https://github.com/Farama-Foundation/gym-docs).\n\nGym also has a discord server for development purposes that you can join here: https://discord.gg/nHg2JRN489\n\n## Installation\n\nTo install the base Gym library, use `pip install gym`.\n\nThis does not include dependencies for all families of environments (there's a massive number, and some can be problematic to install on certain systems). You can install these dependencies for one family like `pip install gym[atari]` or use `pip install gym[all]` to install all dependencies.\n\nWe support Python 3.7, 3.8, 3.9 and 3.10 on Linux and macOS. We will accept PRs related to Windows, but do not officially support it.\n\n## API\n\nThe Gym API's API models environments as simple Python `env` classes. Creating environment instances and interacting with them is very simple- here's an example using the \"CartPole-v1\" environment:\n\n```python\nimport gym\nenv = gym.make(\"CartPole-v1\")\nobservation, info = env.reset(seed=42)\n\nfor _ in range(1000):\n    action = env.action_space.sample()\n    observation, reward, terminated, truncated, info = env.step(action)\n\n    if terminated or truncated:\n        observation, info = env.reset()\nenv.close()\n```\n\n## Notable Related Libraries\n\nPlease note that this is an incomplete list, and just includes libraries that the maintainers most commonly point newcommers to when asked for recommendations.\n\n* [CleanRL](https://github.com/vwxyzjn/cleanrl) is a learning library based on the Gym API. It is designed to cater to newer people in the field and provides very good reference implementations.\n* [Tianshou](https://github.com/thu-ml/tianshou) is a learning library that's geared towards very experienced users and is design to allow for ease in complex algorithm modifications.\n* [RLlib](https://docs.ray.io/en/latest/rllib/index.html) is a learning library that allows for distributed training and inferencing and supports an extraordinarily large number of features throughout the reinforcement learning space.\n* [PettingZoo](https://github.com/Farama-Foundation/PettingZoo) is like Gym, but for environments with multiple agents.\n\n## Environment Versioning\n\nGym keeps strict versioning for reproducibility reasons. All environments end in a suffix like \"\\_v0\".  When changes are made to environments that might impact learning results, the number is increased by one to prevent potential confusion.\n\n## MuJoCo Environments\n\nThe latest \"\\_v4\" and future versions of the MuJoCo environments will no longer depend on `mujoco-py`. Instead `mujoco` will be the required dependency for future gym MuJoCo environment versions. Old gym MuJoCo environment versions that depend on `mujoco-py` will still be kept but unmaintained.\nTo install the dependencies for the latest gym MuJoCo environments use `pip install gym[mujoco]`. Dependencies for old MuJoCo environments can still be installed by `pip install gym[mujoco_py]`. \n\n## Citation\n\nA whitepaper from when Gym just came out is available https://arxiv.org/pdf/1606.01540, and can be cited with the following bibtex entry:\n\n```\n@misc{1606.01540,\n  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},\n  Title = {OpenAI Gym},\n  Year = {2016},\n  Eprint = {arXiv:1606.01540},\n}\n```\n\n## Release Notes\n\nThere used to be release notes for all the new Gym versions here. New release notes are being moved to [releases page](https://github.com/openai/gym/releases) on GitHub, like most other libraries do. Old notes can be viewed [here](https://github.com/openai/gym/blob/31be35ecd460f670f0c4b653a14c9996b7facc6c/README.rst).\n", "release_dates": ["2022-10-04T16:39:12Z", "2022-09-16T20:40:27Z", "2022-09-06T18:23:48Z", "2022-08-18T17:41:14Z", "2022-07-26T22:30:34Z", "2022-07-13T20:01:47Z", "2022-06-07T13:51:45Z", "2022-05-25T16:44:14Z", "2022-03-11T17:25:29Z", "2022-03-04T20:51:37Z", "2022-02-17T18:59:59Z", "2021-10-02T00:37:17Z", "2021-09-14T13:19:07Z", "2021-08-13T04:23:03Z", "2019-05-29T00:54:38Z", "2018-02-01T19:05:03Z", "2018-01-26T21:18:22Z", "2017-03-05T21:57:57Z", "2017-02-01T03:43:30Z"]}, {"name": "gym-http-api", "description": "API to access OpenAI Gym from other languages via HTTP", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n<img align=\"left\" src=\"http://i.imgur.com/568Luwb.png\">gym-http-api\n============\n\nThis project provides a local REST API to the [gym](https://github.com/openai/gym) open-source library, allowing development in languages other than python.\n\nA python client is included, to demonstrate how to interact with the server.\n\nContributions of clients in other languages are welcomed!\n\nInstallation\n============\n\nTo download the code and install the requirements, you can run the following shell commands:\n\n    git clone https://github.com/openai/gym-http-api\n    cd gym-http-api\n    pip install -r requirements.txt\n\n\nGetting started\n============\n\nThis code is intended to be run locally by a single user. The server runs in python. You can implement your own HTTP clients using any language; a demo client written in python is provided to demonstrate the idea.\n\nTo start the server from the command line, run this:\n\n    python gym_http_server.py\n\nIn a separate terminal, you can then try running the example python agent and see what happens:\n\n    python example_agent.py\n\nThe example lua agent behaves very similarly:\n\n    cd binding-lua\n    lua example_agent.lua\n\nYou can also write code like this to create your own client, and test it out by creating a new environment. For example, in python:\n\n    remote_base = 'http://127.0.0.1:5000'\n    client = Client(remote_base)\n\n    env_id = 'CartPole-v0'\n    instance_id = client.env_create(env_id)\n    client.env_step(instance_id, 0)\n\n\nTesting\n============\n\nThis repository contains integration tests, using the python client implementation to send requests to the local server. They can be run using the `nose2` framework. From a shell (such as bash) you can run nose2 directly:\n\n    cd gym-http-api\n    nose2\n\n\nAPI specification\n============\n\n  * POST `/v1/envs/`\n      * Create an instance of the specified environment\n      * param: `env_id` -- gym environment ID string, such as 'CartPole-v0'\n      * returns: `instance_id` -- a short identifier (such as '3c657dbc')\n\t    for the created environment instance. The instance_id is\n        used in future API calls to identify the environment to be\n        manipulated\n\n  * GET `/v1/envs/`\n      * List all environments running on the server\n\t  * returns: `envs` -- dict mapping `instance_id` to `env_id`\n\t    (e.g. `{'3c657dbc': 'CartPole-v0'}`) for every env on the server\n\n  * POST `/v1/envs/<instance_id>/reset/`\n      * Reset the state of the environment and return an initial\n        observation.\n      * param: `instance_id` -- a short identifier (such as '3c657dbc')\n        for the environment instance\n      * returns: `observation` -- the initial observation of the space\n\n  * POST `/v1/envs/<instance_id>/step/`\n      *  Step though an environment using an action.\n      * param: `instance_id` -- a short identifier (such as '3c657dbc')\n        for the environment instance\n\t  * param: `action` -- an action to take in the environment\n      * returns: `observation` -- agent's observation of the current\n        environment\n      * returns: `reward` -- amount of reward returned after previous action\n      * returns: `done` -- whether the episode has ended\n      * returns: `info` -- a dict containing auxiliary diagnostic information\n\n  * GET `/v1/envs/<instance_id>/action_space/`\n      * Get information (name and dimensions/bounds) of the env's\n        `action_space`\n      * param: `instance_id` -- a short identifier (such as '3c657dbc')\n        for the environment instance\n      * returns: `info` -- a dict containing 'name' (such as 'Discrete'), and\n    additional dimensional info (such as 'n') which varies from\n    space to space\n\n  * GET `/v1/envs/<instance_id>/observation_space/`\n      * Get information (name and dimensions/bounds) of the env's\n        `observation_space`\n      * param: `instance_id` -- a short identifier (such as '3c657dbc')\n        for the environment instance\n      * returns: `info` -- a dict containing 'name' (such as 'Discrete'), and\n    additional dimensional info (such as 'n') which varies from\n    space to space\n\n  * POST `/v1/envs/<instance_id>/monitor/start/`\n      * Start monitoring\n      * param: `instance_id` -- a short identifier (such as '3c657dbc')\n        for the environment instance\n      * param: `force` (default=False) -- Clear out existing training\n        data from this directory (by deleting every file\n        prefixed with \"openaigym.\")\n      * param: `resume` (default=False) -- Retain the training data\n        already in this directory, which will be merged with\n        our new data\n      * (NOTE: the `video_callable` parameter from the native\n    `env.monitor.start` function is NOT implemented)\n\n  * POST `/v1/envs/<instance_id>/monitor/close/`\n      * Flush all monitor data to disk\n      * param: `instance_id` -- a short identifier (such as '3c657dbc')\n        for the environment instance\n\n  * POST `/v1/upload/`\n      * Flush all monitor data to disk\n      * param: `training_dir` -- A directory containing the results of a\n        training run.\n      * param: `api_key` -- Your OpenAI API key\n      * param: `algorithm_id` (default=None) -- An arbitrary string\n        indicating the paricular version of the algorithm\n        (including choices of parameters) you are running.\n\n  * POST `/v1/shutdown/`\n      * Request a server shutdown\n      * Currently used by the integration tests to repeatedly create and destroy fresh copies of the server running in a separate thread\n\nContributors\n============\n\nSee the [contributors page] (https://github.com/openai/gym-http-api/graphs/contributors)\n", "release_dates": []}, {"name": "gym-recording", "description": "Add-on package to gym, to record sequences of actions, observations, and rewards", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# gym-recording\n\nA Python package to capture the sequences of actions and observations on a [Gym](https://github.com/openai/gym) environment\nby wrapping it in a `TraceRecordingWrapper`, like this:\n\n```Python\nimport gym\nfrom gym_recording.wrappers import TraceRecordingWrapper\n\ndef main():\n    env = gym.make('CartPole-v0')\n    env = TraceRecordingWrapper(env)\n    # ... exercise the environment\n```\n\nIt will save recorded traces in a directory, which it will print with `logging`.\nYou can get the directory name from your code as `env.directory`.\n\nLater you can play back the recording with code like the following, which runs a callback for every episode.\n\n```Python\nfrom gym_recording import playback\n\ndef handle_ep(observations, actions, rewards):\n  # ... learn a model\n\nplayback.scan_recorded_traces(directory, handle_ep)\n```\n\nYou can also use the storage_s3 module to upload and download traces from S3, so you it can run across machines.\n\n```Python\nfrom gym_recording import playback, storage_s3\n\ndef main():\n    env = gym.make('CartPole-v0')\n    env = TraceRecordingWrapper(env)\n    # ... exercise the environment\n\n    s3url = storage_s3.upload_recording(env.directory, env.spec.id, 'openai-example-traces')\n    # ... store s3url in a database\n\n    # ... Switch to another machine\n\n    # ... load s3url from a database\n\n    def handle_ep(observations, actions, rewards):\n      # ... learn a model\n    playback.scan_recorded_traces(storage_s3.download_recording(s3url), handle_ep)\n\n```\n", "release_dates": []}, {"name": "gym-soccer", "description": null, "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# gym-soccer\n\nThe [Soccer environment](https://github.com/LARG/HFO) is a multiagent\ndomain featuring continuous state and action spaces. Currently,\nseveral tasks are supported:\n\n## Soccer\n\nThe soccer task initializes a single offensive agent on the field and rewards +1 for scoring a goal and 0 otherwise. In order to score a goal, the agent will need to know how to approach the ball and kick towards the goal. The sparse nature of the goal reward makes this task very difficult to accomplish.\n\n## SoccerEmptyGoal\n\nThe SoccerEmptyGoal task features a more informative reward signal than the Soccer task. As before, the objective is to score a goal. However, SoccerEmtpyGoal rewards the agent for approaching the ball and moving the ball towards the goal. These frequent rewards make the task much more accessible.\n\n## SoccerAgainstKeeper\n\nThe objective of the SoccerAgainstKeeper task is to score against a goal keeper. The agent is rewarded for moving the ball towards the goal and for scoring a goal. The goal keeper uses a hand-coded policy developed by the Helios RoboCup team. The difficulty in this task is learning how to shoot around the goal keeper.\n\n# Installation\n\n```bash\ncd gym-soccer\npip install -e .\n```\n", "release_dates": []}, {"name": "gym-wikinav", "description": "Wikipedia navigation environment for OpenAI Gym", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n", "release_dates": []}, {"name": "gym3", "description": "Vectorized interface for reinforcement learning environments", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Maintenance (expect bug fixes and minor updates)\n\n# gym3\n\n`gym3` provides a unified interface for reinforcement learning environments that improves upon the `gym` interface and includes vectorization, which is invaluable for performance.  `gym3` is just the interface and associated tools, and includes no environments beyond some simple testing environments.\n\n`gym3` is used internally inside OpenAI and is released here primarily for use by OpenAI environments.  External users should likely use [`gym`](https://github.com/openai/gym).\n\nSupported platforms:\n\n- Windows\n- macOS\n- Linux\n\nSupported Pythons:\n\n- `>=3.6`\n\nInstallation:\n\n`pip install gym3`\n\n## Overview\n\n`gym3.Env` is similar to combining multiple `gym.Env` environments into a single environment, with automatic reset when episodes are complete.\n\nA `gym3` random agent looks like this (run `pip install --upgrade procgen` to get the environment):\n\n```py\nimport gym3\nfrom procgen import ProcgenGym3Env\nenv = ProcgenGym3Env(num=2, env_name=\"coinrun\")\nstep = 0\nwhile True:\n    env.act(gym3.types_np.sample(env.ac_space, bshape=(env.num,)))\n    rew, obs, first = env.observe()\n    print(f\"step {step} reward {rew} first {first}\")\n    step += 1\n```\n\nTo visualize what the agent is doing:\n\n```py\nimport gym3\nfrom procgen import ProcgenGym3Env\nenv = ProcgenGym3Env(num=2, env_name=\"coinrun\", render_mode=\"rgb_array\")\nenv = gym3.ViewerWrapper(env, info_key=\"rgb\")\nstep = 0\nwhile True:\n    env.act(gym3.types_np.sample(env.ac_space, bshape=(env.num,)))\n    rew, obs, first = env.observe()\n    print(f\"step {step} reward {rew} first {first}\")\n    step += 1\n```\n\nA command line example is included under `scripts`:\n\n```\npython -m gym3.scripts.random_agent --fn_path procgen:ProcgenGym3Env --env_name coinrun --render_mode rgb_array\n```\n\nThe observations and actions can be either arrays, or \"trees\" of arrays, where a tree is a (potentially nested) dictionary with string keys.  `gym3` includes a handy function, `gym3.types.multimap` for mapping functions over trees, as well as a number of utilities in `gym3.types_np` that produce trees numpy arrays from space objects, such as `types_np.sample()` seen above.\n\nCompatibility with existing `gym` environments is provided as well:\n\n```py\nimport gym3\nenv = gym3.vectorize_gym(num=2, render_mode=\"human\", env_kwargs={\"id\": \"CartPole-v0\"})\nstep = 0\nwhile True:\n    env.act(gym3.types_np.sample(env.ac_space, bshape=(env.num,)))\n    rew, obs, first = env.observe()\n    print(f\"step {step} reward {rew} first {first}\")\n    step += 1\n```\n\n## Documentation\n\n* [API Reference](docs/api.md)\n* [`gym3` for `gym` users](docs/gym3-for-gym-users.md)\n* [Design Choices](docs/design.md)\n\n## Example Environments\n\n* [Example Gridworld](https://github.com/christopher-hesse/example-gridworld) - Simple gridworld where you observe the state directly\n* [Computer Tennis](https://github.com/christopher-hesse/computer-tennis) - Clone of the game \"Pong\" where you observe pixels. Renders with Cairo or OpenGL headless (no X server required)\n* [`libenv_fixedseq.c`](gym3/libenv_fixedseq.c) - Example environment using the `libenv` C API.  For a full C++ environment that uses this interface, see [Procgen Benchmark](https://github.com/openai/procgen)\n\n## Changelog\n\nSee [CHANGES](CHANGES.md) for changes present in each release.\n\n## Contributing\n\nSee [CONTRIBUTING](CONTRIBUTING.md) for information on contributing.", "release_dates": []}, {"name": "human-eval", "description": "Code for the paper \"Evaluating Large Language Models Trained on Code\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# HumanEval: Hand-Written Evaluation Set \n\nThis is an evaluation harness for the HumanEval problem solving dataset\ndescribed in the paper \"[Evaluating Large Language Models Trained on\nCode](https://arxiv.org/abs/2107.03374)\".\n\n## Installation\n\nMake sure to use python 3.7 or later:\n```\n$ conda create -n codex python=3.7\n$ conda activate codex\n```\n\nCheck out and install this repository:\n```\n$ git clone https://github.com/openai/human-eval\n$ pip install -e human-eval\n```\n\n## Usage\n\n**This program exists to run untrusted model-generated code. Users are strongly\nencouraged not to do so outside of a robust security sandbox. The [execution\ncall](https://github.com/openai/human-eval/blob/master/human_eval/execution.py#L48-L58)\nin `execution.py` is deliberately commented out to ensure users read this\ndisclaimer before running code in a potentially unsafe manner. See the comment in\n`execution.py` for more information and instructions.**\n\nAfter following the above instructions to enable execution, generate samples\nand save them in the following JSON Lines (jsonl) format, where each sample is\nformatted into a single line like so:\n```\n{\"task_id\": \"Corresponding HumanEval task ID\", \"completion\": \"Completion only without the prompt\"}\n```\nWe provide `example_problem.jsonl` and `example_solutions.jsonl` under `data`\nto illustrate the format and help with debugging.\n\nHere is nearly functional example code (you just have to provide\n`generate_one_completion` to make it work) that saves generated completions to\n`samples.jsonl`.\n```\nfrom human_eval.data import write_jsonl, read_problems\n\nproblems = read_problems()\n\nnum_samples_per_task = 200\nsamples = [\n    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"prompt\"]))\n    for task_id in problems\n    for _ in range(num_samples_per_task)\n]\nwrite_jsonl(\"samples.jsonl\", samples)\n```\n\nTo evaluate the samples, run\n```\n$ evaluate_functional_correctness samples.jsonl\nReading samples...\n32800it [00:01, 23787.50it/s]\nRunning test suites...\n100%|...| 32800/32800 [16:11<00:00, 33.76it/s]\nWriting results to samples.jsonl_results.jsonl...\n100%|...| 32800/32800 [00:00<00:00, 42876.84it/s]\n{'pass@1': ..., 'pass@10': ..., 'pass@100': ...}\n```\nThis script provides more fine-grained information in a new file ending in\n`<input_path>_results.jsonl`. Each row now contains whether the completion\n`passed` along with the execution `result` which is one of \"passed\", \"timed\nout\", or \"failed\".\n\nAs a quick sanity-check, the example samples should yield 0.5 pass@1.\n```\n$ evaluate_functional_correctness data/example_samples.jsonl --problem_file=data/example_problem.jsonl\nReading samples...\n6it [00:00, 3397.11it/s]\nRunning example suites...\n100%|...| 6/6 [00:03<00:00,  1.96it/s]\nWriting results to data/example_samples.jsonl_results.jsonl...\n100%|...| 6/6 [00:00<00:00, 6148.50it/s]\n{'pass@1': 0.4999999999999999}\n```\n\nBecause there is no unbiased way of estimating pass@k when there are fewer\nsamples than k, the script does not evaluate pass@k for these cases. To\nevaluate with other k values, pass `--k=<comma-separated-values-here>`. For\nother options, see\n```\n$ evaluate_functional_correctness --help\n```\nHowever, we recommend that you use the default values for the rest.\n\n## Known Issues\n\nWhile evaluation uses very little memory, you might see the following error\nmessage when the system is running out of RAM. Since this may cause some\ncorrect programs to fail, we recommend that you free some memory and try again.\n```\nmalloc: can't allocate region\n```\n\n## Citation\n\nPlease cite using the following bibtex entry:\n\n```\n@article{chen2021codex,\n  title={Evaluating Large Language Models Trained on Code},\n  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},\n  year={2021},\n  eprint={2107.03374},\n  archivePrefix={arXiv},\n  primaryClass={cs.LG}\n}\n```\n", "release_dates": []}, {"name": "human-eval-infilling", "description": "Code for the paper \"Efficient Training of Language Models to Fill in the Middle\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# HumanEval Infilling Benchmarks\n\nThis is an evaluation harness for the HumanEval infilling benchmarks described in the [FIM paper](https://arxiv.org/abs/2207.14255).\n\n## Installation\n\nMake sure to use python 3.7 or later:\n```\n$ conda create -n codex python=3.7\n$ conda activate codex\n```\n\nCheck out and install this repository:\n```\n$ git clone https://github.com/openai/human-eval-infilling\n$ pip install -e human-eval-infilling\n```\n\n## Usage\n\n**This program exists to run untrusted model-generated code. Users are strongly\nencouraged not to do so outside of a robust security sandbox. The [execution\ncall](https://github.com/openai/human-eval-infilling/blob/master/human_eval_infilling/execution.py#L74-L84)\nin `execution.py` is deliberately commented out to ensure users read this\ndisclaimer before running code in a potentially unsafe manner. See the comment in\n`execution.py` for more information and instructions.**\n\nAfter following the above instructions to enable execution, generate samples\nand save them in the following JSON Lines (jsonl) format, where each sample is\nformatted into a single line like so:\n```\n{\"task_id\": \"Corresponding task ID from the desired benchmark\", \"completion\": \"Completion only without the prompt\"}\n```\nEnsure that the `task_id` used matches the `task_id` from the desired benchmark. See below and the paper for information on the benchmarks available.\n\nWe provide `example_problem.jsonl` and `example_solutions.jsonl` under `data`\nto illustrate the format and help with debugging.\n\nHere is nearly functional example code (you just have to provide\n`generate_one_completion` to make it work) that saves generated completions for\nthe single-line infilling benchmark to `samples.jsonl`.\n```\nfrom human_eval_infilling.data import write_jsonl, read_problems\n\nproblems = read_problems(benchmark_name=\"single-line\")\n\nnum_samples_per_task = 100\nsamples = [\n    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"prompt\"], problems[task_id][\"suffix\"]))\n    for task_id in problems\n    for _ in range(num_samples_per_task)\n]\nwrite_jsonl(\"samples.jsonl\", samples)\n```\n\nTo evaluate the samples, run\n```\n$ evaluate_infilling_functional_correctness samples.jsonl --benchmark_name=single-line\nReading samples...\n103300it [00:01, 23787.50it/s]\nRunning test suites...\n100%|...| 103300/103300 [16:11<00:00, 33.76it/s]\nWriting results to samples.jsonl_results.jsonl...\n100%|...| 103300/103300 [00:00<00:00, 42876.84it/s]\n{'pass@1': ..., 'pass@10': ..., 'pass@100': ...}\n```\nThis script provides more fine-grained information in a new file ending in\n`<input_path>_results.jsonl`. Each row now contains whether the completion\n`passed` along with the execution `result` which is one of \"passed\", \"timed\nout\", or \"failed\".\n\nAs a quick sanity-check, the example samples should yield 30% pass@1.\n```\n$ evaluate_infilling_functional_correctness data/example_samples.jsonl --benchmark_name=test\nReading samples...\n10it [00:00, 3365.94it/s]\n100%|...| 10/10 [00:03<00:00,  2.76it/s]\nWriting results to data/example_samples.jsonl_results.jsonl...\n100%|...| 10/10 [00:00<00:00, 1309.08it/s]\n{'pass@1': 0.30000000000000004, 'pass@10': 1.0}\n```\n\nThere are 4 available benchmarks: single-line, multi-line, random-span, random-span-light. The first two are introduced in the [InCoder paper](https://arxiv.org/abs/2204.05999) and the latter two are introduced in the [FIM paper](https://arxiv.org/abs/2207.14255). All benchmarks are used extensively in the FIM paper. There is also a dummy benchmark for testing.\n\nBecause there is no unbiased way of estimating pass@k when there are fewer\nsamples than k, the script does not evaluate pass@k for these cases. To\nevaluate with other k values, pass `--k=<comma-separated-values-here>`. For\nother options, see\n```\n$ evaluate_infilling_functional_correctness --help\n```\nHowever, we recommend that you use the default values for the rest.\n\n## Known Issues\n\nWhile evaluation uses very little memory, you might see the following error\nmessage when the system is running out of RAM. Since this may cause some\ncorrect programs to fail, we recommend that you free some memory and try again.\n```\nmalloc: can't allocate region\n```\n", "release_dates": []}, {"name": "iaf", "description": "Code for reproducing key results in the paper \"Improving Variational Inference with Inverse Autoregressive Flow\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Improve Variational Inference with Inverse Autoregressive Flow\n\nCode for reproducing key results in the paper [Improving Variational Inference with Inverse Autoregressive Flow](http://arxiv.org/abs/1606.04934) by Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.\n\n## Prerequisites\n\n1. Make sure that recent versions installed of:\n    - Python (version 2.7 or higher)\n    - Numpy (e.g. `pip install numpy`)\n    - Theano (e.g. `pip install Theano`)\n\n2. Set `floatX = float32` in the `[global]` section of Theano config (usually `~/.theanorc`). Alternatively you could prepend `THEANO_FLAGS=floatX=float32 ` to the python commands below. \n\n3. Clone this repository, e.g.:\n```sh\ngit clone https://github.com/openai/iaf.git\n```\n\n4. Download the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) (get the *Python* version) and create an environment variable `CIFAR10_PATH` that points to the subdirectory with CIFAR-10 data. For example:\n```sh\nexport CIFAR10_PATH=\"$HOME/cifar-10\"\n```\n\n## Syntax of train.py\n\nExample:\n```sh\npython train.py with problem=cifar10 n_z=32 n_h=64 depths=[2,2,2] margs.depth_ar=1 margs.posterior=down_iaf2_NL margs.kl_min=0.25\n```\n\n`problem` is the problem (dataset) to train on. I only tested `cifar10` for this release.\n\n`n_z` is the number of stochastic featuremaps in each layer.\n\n`n_h` is the number of deterministic featuremaps used throughout the model.\n\n`depths` is an array of integers that denotes the depths of the *levels* in the model. Each level is a sequence of layers. Each subsequent level operates over spatially smaller featuremaps. In case of CIFAR-10, the first level operates over 16x16 featuremaps, the second over 8x8 featuremaps, etc.\n\nSome possible choices for `margs.posterior` are:\n- `up_diag`: bottom-up factorized Gaussian\n- `up_iaf1_nl`: bottom-up IAF, mean-only perturbation\n- `up_iaf2_nl`: bottom-up IAF\n- `down_diag`: top-down factorized Gaussian\n- `down_iaf1_nl`: top-down IAF, mean-only perturbation\n- `down_iaf2_nl`: top-down IAF\n\n`margs.depth_ar` is the number of hidden layers within IAF, and can be any non-negative integer.\n\n`margs.kl_min`: the minimum information constraint. Should be a non-negative float (where 0 is no constraint).\n\n## Results of Table 3\n\n(3.28 bits/dim)\n\n```sh\npython train.py with problem=cifar10 n_h=160 depths=[10,10] margs.depth_ar=2 margs.posterior=down_iaf2_nl margs.prior=diag margs.kl_min=0.25\n```\n\nMore instructions will follow.\n\n\n## Multi-GPU TensorFlow implementation\n\n### Prerequisites\n\nMake sure that recent versions installed of:\n- Python (version 2.7 or higher)\n- TensorFlow\n- tqdm\n   \n`CIFAR10_PATH` environment variable should point to the dataset location.\n\n### Syntax of tf_train.py\n\nTraining script:\n```sh\npython tf_train.py --logdir <logdir> --hpconfig depth=1,num_blocks=20,kl_min=0.1,learning_rate=0.002,batch_size=32 --num_gpus 8 --mode train\n```\n\nIt will run the training procedure on a given number of GPUs. Model checkpoints will be stored in `<logdir>/train` directory along with TensorBoard summaries that are useful for monitoring and debugging issues.\n\nEvaluation script:\n```sh\npython tf_train.py --logdir <logdir> --hpconfig depth=1,num_blocks=20,kl_min=0.1,learning_rate=0.002,batch_size=32 --num_gpus 1 --mode eval_test\n```\n\nIt will run the evaluation on the test set using a single GPU and will produce TensorBoard summary with the results and generated samples.\n\nTo start TensorBoard:\n```sh\ntensorboard --logdir <logdir>\n```\n\nFor the description of hyper-parameters, take a look at `get_default_hparams` function in `tf_train.py`.\n\n\n### Loading from the checkpoint\n\nThe best IAF model trained on CIFAR-10 reached 3.15 bits/dim when evaluated with a single sample. With 10,000 samples, the estimation of log likelihood is 3.111 bits/dim.\nThe checkpoint is available at [link](https://drive.google.com/file/d/0B-pv8mYT4p0OOXFfWElyeUs0bUk/view?usp=sharing).\nSteps to use it:\n- download the file\n- create directory `<logdir>/train/` and copy the checkpoint there\n- run the following command:\n```sh\npython tf_train.py --logdir <logdir> --hpconfig depth=1,num_blocks=20,kl_min=0.1,learning_rate=0.002,batch_size=32 --num_gpus 1 --mode eval_test\n```\n\nThe script will run the evaluation on the test set and generate samples stored in TensorFlow events file that can be accessed using TensorBoard.", "release_dates": []}, {"name": "image-gpt", "description": null, "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# image-gpt\n\nCode and models from the paper [\"Generative Pretraining from Pixels\"](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf).\n\nSupported Platforms:\n\n- Ubuntu 16.04\n\n## Install\n\nYou can get miniconda from https://docs.conda.io/en/latest/miniconda.html, or install the dependencies shown below manually.\n\n```\nconda create --name image-gpt python=3.7.3\nconda activate image-gpt\n\nconda install numpy=1.16.3\nconda install tensorflow-gpu=1.13.1\n\nconda install imageio=2.8.0\nconda install requests=2.21.0\nconda install tqdm=4.46.0\n```\n\n## Usage\n\nThis repository is meant to be a starting point for researchers and engineers to experiment with image GPT (iGPT). Our code forks GPT-2 to highlight that it can be easily applied across domains. The diff from `gpt-2/src/model.py` to `image-gpt/src/model.py` includes a new activation function, renaming of several variables, and the introduction of a start-of-sequence token, none of which change the model architecture.\n\n### Downloading Pre-trained Models\n\nTo download a model checkpoint, run `download.py`. The `--model` argument should be one of \"s\", \"m\", or \"l\", and the `--ckpt` argument should be one of \"131000\", \"262000\", \"524000\", or \"1000000\".\n\n```\npython download.py --model s --ckpt 1000000\n```\n\nThis command downloads the iGPT-S checkpoint at 1M training iterations. The default download directory is set to `/root/downloads/`, and can be changed using the `--download_dir` argument.\n\n### Downloading Datasets\n\nTo download datasets, run `download.py` with the `--dataset` argument set to \"imagenet\" or \"cifar10\".\n\n```\npython download.py --model s --ckpt 1000000 --dataset imagenet\n```\n\nThis command additionally downloads 32x32 ImageNet encoded with the 9-bit color palette described in the paper. The datasets we provide are center-cropped images intended for evaluation; random cropped images are required to faithfully replicate training.\n\n### Downloading Color Clusters\n\nTo download the color cluster file defining our 9-bit color palette, run `download.py` with the `--clusters` flag set.\n\n```\npython download.py --model s --ckpt 1000000 --dataset imagenet --clusters\n```\n\nThis command additionally downloads the color cluster file. `src/run.py:sample` shows how to decode from 9-bit color to RGB and `src/utils.py:color_quantize` shows how to go the other way around.\n\n### Sampling\n\nOnce the desired checkpoint and color cluster file are downloaded, we can run the script in sampling mode. The following commands sample from iGPT-S, iGPT-M, and iGPT-L respectively:\n\n```\npython src/run.py --sample --n_embd 512  --n_head 8  --n_layer 24\npython src/run.py --sample --n_embd 1024 --n_head 8  --n_layer 36\npython src/run.py --sample --n_embd 1536 --n_head 16 --n_layer 48\n```\n\nIf your data is not in `/root/downloads/`, set `--ckpt_path` and `--color_cluster_path` manually. To run on fewer than 8 GPUs, use a command of the following form:\n\n```\nCUDA_VISIBLE_DEVICES=0,1 python src/run.py --sample --n_embd 512  --n_head 8  --n_layer 24 --n_gpu 2\n```\n\n### Evaluating\n\nOnce the desired checkpoint and evaluation dataset are downloaded, we can run the script in evaluation mode. The following commands evaluate iGPT-S, iGPT-M, and iGPT-L on ImageNet respectively:\n\n```\npython src/run.py --eval --n_embd 512  --n_head 8  --n_layer 24\npython src/run.py --eval --n_embd 1024 --n_head 8  --n_layer 36\npython src/run.py --eval --n_embd 1536 --n_head 16 --n_layer 48\n```\n\nIf your data is not in `/root/downloads/`, set `--ckpt_path` and `--data_path` manually. You should see that the test generative losses are 2.0895, 2.0614, and 2.0466, matching Figure 3 in the paper.\n\n### Citation\n\nPlease use the following bibtex entry:\n```\n@article{chen2020generative,\n  title={Generative Pretraining from Pixels},\n  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},\n  year={2020}\n}\n```\n\n## License\n\n[Modified MIT](./LICENSE)\n", "release_dates": []}, {"name": "imitation", "description": " Code for the paper \"Generative Adversarial Imitation Learning\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": null, "release_dates": []}, {"name": "improved-diffusion", "description": "Release for Improved Denoising Diffusion Probabilistic Models", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# improved-diffusion\n\nThis is the codebase for [Improved Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2102.09672).\n\n# Usage\n\nThis section of the README walks through how to train and sample from a model.\n\n## Installation\n\nClone this repository and navigate to it in your terminal. Then run:\n\n```\npip install -e .\n```\n\nThis should install the `improved_diffusion` python package that the scripts depend on.\n\n## Preparing Data\n\nThe training code reads images from a directory of image files. In the [datasets](datasets) folder, we have provided instructions/scripts for preparing these directories for ImageNet, LSUN bedrooms, and CIFAR-10.\n\nFor creating your own dataset, simply dump all of your images into a directory with \".jpg\", \".jpeg\", or \".png\" extensions. If you wish to train a class-conditional model, name the files like \"mylabel1_XXX.jpg\", \"mylabel2_YYY.jpg\", etc., so that the data loader knows that \"mylabel1\" and \"mylabel2\" are the labels. Subdirectories will automatically be enumerated as well, so the images can be organized into a recursive structure (although the directory names will be ignored, and the underscore prefixes are used as names).\n\nThe images will automatically be scaled and center-cropped by the data-loading pipeline. Simply pass `--data_dir path/to/images` to the training script, and it will take care of the rest.\n\n## Training\n\nTo train your model, you should first decide some hyperparameters. We will split up our hyperparameters into three groups: model architecture, diffusion process, and training flags. Here are some reasonable defaults for a baseline:\n\n```\nMODEL_FLAGS=\"--image_size 64 --num_channels 128 --num_res_blocks 3\"\nDIFFUSION_FLAGS=\"--diffusion_steps 4000 --noise_schedule linear\"\nTRAIN_FLAGS=\"--lr 1e-4 --batch_size 128\"\n```\n\nHere are some changes we experiment with, and how to set them in the flags:\n\n * **Learned sigmas:** add `--learn_sigma True` to `MODEL_FLAGS`\n * **Cosine schedule:** change `--noise_schedule linear` to `--noise_schedule cosine`\n * **Importance-sampled VLB:** add `--use_kl True` to `DIFFUSION_FLAGS` and add `--schedule_sampler loss-second-moment` to  `TRAIN_FLAGS`.\n * **Class-conditional:** add `--class_cond True` to `MODEL_FLAGS`.\n\nOnce you have setup your hyper-parameters, you can run an experiment like so:\n\n```\npython scripts/image_train.py --data_dir path/to/images $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS\n```\n\nYou may also want to train in a distributed manner. In this case, run the same command with `mpiexec`:\n\n```\nmpiexec -n $NUM_GPUS python scripts/image_train.py --data_dir path/to/images $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS\n```\n\nWhen training in a distributed manner, you must manually divide the `--batch_size` argument by the number of ranks. In lieu of distributed training, you may use `--microbatch 16` (or `--microbatch 1` in extreme memory-limited cases) to reduce memory usage.\n\nThe logs and saved models will be written to a logging directory determined by the `OPENAI_LOGDIR` environment variable. If it is not set, then a temporary directory will be created in `/tmp`.\n\n## Sampling\n\nThe above training script saves checkpoints to `.pt` files in the logging directory. These checkpoints will have names like `ema_0.9999_200000.pt` and `model200000.pt`. You will likely want to sample from the EMA models, since those produce much better samples.\n\nOnce you have a path to your model, you can generate a large batch of samples like so:\n\n```\npython scripts/image_sample.py --model_path /path/to/model.pt $MODEL_FLAGS $DIFFUSION_FLAGS\n```\n\nAgain, this will save results to a logging directory. Samples are saved as a large `npz` file, where `arr_0` in the file is a large batch of samples.\n\nJust like for training, you can run `image_sample.py` through MPI to use multiple GPUs and machines.\n\nYou can change the number of sampling steps using the `--timestep_respacing` argument. For example, `--timestep_respacing 250` uses 250 steps to sample. Passing `--timestep_respacing ddim250` is similar, but uses the uniform stride from the [DDIM paper](https://arxiv.org/abs/2010.02502) rather than our stride.\n\nTo sample using [DDIM](https://arxiv.org/abs/2010.02502), pass `--use_ddim True`.\n\n## Models and Hyperparameters\n\nThis section includes model checkpoints and run flags for the main models in the paper.\n\nNote that the batch sizes are specified for single-GPU training, even though most of these runs will not naturally fit on a single GPU. To address this, either set `--microbatch` to a small value (e.g. 4) to train on one GPU, or run with MPI and divide `--batch_size` by the number of GPUs.\n\nUnconditional ImageNet-64 with our `L_hybrid` objective and cosine noise schedule [[checkpoint](https://openaipublic.blob.core.windows.net/diffusion/march-2021/imagenet64_uncond_100M_1500K.pt)]:\n\n```bash\nMODEL_FLAGS=\"--image_size 64 --num_channels 128 --num_res_blocks 3 --learn_sigma True\"\nDIFFUSION_FLAGS=\"--diffusion_steps 4000 --noise_schedule cosine\"\nTRAIN_FLAGS=\"--lr 1e-4 --batch_size 128\"\n```\n\nUnconditional CIFAR-10 with our `L_hybrid` objective and cosine noise schedule [[checkpoint](https://openaipublic.blob.core.windows.net/diffusion/march-2021/cifar10_uncond_50M_500K.pt)]:\n\n```bash\nMODEL_FLAGS=\"--image_size 32 --num_channels 128 --num_res_blocks 3 --learn_sigma True --dropout 0.3\"\nDIFFUSION_FLAGS=\"--diffusion_steps 4000 --noise_schedule cosine\"\nTRAIN_FLAGS=\"--lr 1e-4 --batch_size 128\"\n```\n\nClass-conditional ImageNet-64 model (270M parameters, trained for 250K iterations) [[checkpoint](https://openaipublic.blob.core.windows.net/diffusion/march-2021/imagenet64_cond_270M_250K.pt)]:\n\n```bash\nMODEL_FLAGS=\"--image_size 64 --num_channels 192 --num_res_blocks 3 --learn_sigma True --class_cond True\"\nDIFFUSION_FLAGS=\"--diffusion_steps 4000 --noise_schedule cosine --rescale_learned_sigmas False --rescale_timesteps False\"\nTRAIN_FLAGS=\"--lr 3e-4 --batch_size 2048\"\n```\n\nUpsampling 256x256 model (280M parameters, trained for 500K iterations) [[checkpoint](https://openaipublic.blob.core.windows.net/diffusion/march-2021/upsample_cond_500K.pt)]:\n\n```bash\nMODEL_FLAGS=\"--num_channels 192 --num_res_blocks 2 --learn_sigma True --class_cond True\"\nDIFFUSION_FLAGS=\"--diffusion_steps 4000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False\"\nTRAIN_FLAGS=\"--lr 3e-4 --batch_size 256\"\n```\n\nLSUN bedroom model (lr=1e-4) [[checkpoint](https://openaipublic.blob.core.windows.net/diffusion/march-2021/lsun_uncond_100M_1200K_bs128.pt)]:\n\n```bash\nMODEL_FLAGS=\"--image_size 256 --num_channels 128 --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16\"\nDIFFUSION_FLAGS=\"--diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False\"\nTRAIN_FLAGS=\"--lr 1e-4 --batch_size 128\"\n```\n\nLSUN bedroom model (lr=2e-5) [[checkpoint](https://openaipublic.blob.core.windows.net/diffusion/march-2021/lsun_uncond_100M_2400K_bs64.pt)]:\n\n```bash\nMODEL_FLAGS=\"--image_size 256 --num_channels 128 --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16\"\nDIFFUSION_FLAGS=\"--diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --use_scale_shift_norm False\"\nTRAIN_FLAGS=\"--lr 2e-5 --batch_size 128\"\n```\n\nUnconditional ImageNet-64 with the `L_vlb` objective and cosine noise schedule [[checkpoint](https://openaipublic.blob.core.windows.net/diffusion/march-2021/imagenet64_uncond_vlb_100M_1500K.pt)]:\n\n```bash\nMODEL_FLAGS=\"--image_size 64 --num_channels 128 --num_res_blocks 3 --learn_sigma True\"\nDIFFUSION_FLAGS=\"--diffusion_steps 4000 --noise_schedule cosine --use_kl True\"\nTRAIN_FLAGS=\"--lr 1e-4 --batch_size 128 --schedule_sampler loss-second-moment\"\n```\n\nUnconditional CIFAR-10 with the `L_vlb` objective and cosine noise schedule [[checkpoint](https://openaipublic.blob.core.windows.net/diffusion/march-2021/cifar10_uncond_vlb_50M_500K.pt)]:\n\n```bash\nMODEL_FLAGS=\"--image_size 32 --num_channels 128 --num_res_blocks 3 --learn_sigma True --dropout 0.3\"\nDIFFUSION_FLAGS=\"--diffusion_steps 4000 --noise_schedule cosine --use_kl True\"\nTRAIN_FLAGS=\"--lr 1e-4 --batch_size 128 --schedule_sampler loss-second-moment\"\n```\n", "release_dates": []}, {"name": "improved-gan", "description": "Code for the paper \"Improved Techniques for Training GANs\"", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# improved-gan\ncode for the paper \"Improved Techniques for Training GANs\"\n\nMNIST, SVHN, CIFAR10 experiments in the mnist_svhn_cifar10 folder\n\nimagenet experiments in the imagenet folder\n\n", "release_dates": []}, {"name": "InfoGAN", "description": "Code for reproducing key results in the paper \"InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\"", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# InfoGAN\n\nCode for reproducing key results in the paper [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657) by Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel.\n\n## Dependencies\n\nThis project currently requires the dev version of TensorFlow available on Github: https://github.com/tensorflow/tensorflow. As of the release, the latest commit is [79174a](https://github.com/tensorflow/tensorflow/commit/79174afa30046ecdc437b531812f2cb41a32695e).\n\nIn addition, please `pip install` the following packages:\n- `prettytensor`\n- `progressbar`\n- `python-dateutil`\n\n## Running in Docker\n\n```bash\n$ git clone git@github.com:openai/InfoGAN.git\n$ docker run -v $(pwd)/InfoGAN:/InfoGAN -w /InfoGAN -it -p 8888:8888 gcr.io/tensorflow/tensorflow:r0.9rc0-devel\nroot@X:/InfoGAN# pip install -r requirements.txt\nroot@X:/InfoGAN# python launchers/run_mnist_exp.py\n```\n\n## Running Experiment\n\nWe provide the source code to run the MNIST example:\n\n```bash\nPYTHONPATH='.' python launchers/run_mnist_exp.py\n```\n\nYou can launch TensorBoard to view the generated images:\n\n```bash\ntensorboard --logdir logs/mnist\n```\n", "release_dates": []}, {"name": "interactive-textbook-demo", "description": "Interactive Textbook Demo", "language": "TypeScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "https://github.com/openai/interactive-textbook-demo/assets/5464875/02abe187-2357-49f1-a9bb-cb8e800f1fb6\n\n# Interactive textbook demo\n\nThis repository contains the code for an interactive textbook demo that showcases how OpenAI's technologies can be used to make it more accessible to people with visual disabilities or language and learning barriers. \n\n[Hosted demo](https://interactive-textbook-demo.vercel.app/) (email kaichen@openai.com for access).\n\n\n## Development\n\nCreate `.env.local` file in the project root and copy the contents of `.env.example` into it. You can create an OpenAI API key here: [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)\n\nInstall dependencies:\n```\nnpm install\n```\n\nRun development environment:\n```\nnpm run dev\n```\n\n## Using the `trufflehog` Pre-Commit Hook\nThis repository includes a pre-commit hook that uses the `trufflehog` tool to scan your code for secrets before each commit. This helps prevent secrets, such as API keys and passwords, from being accidentally committed to the repository.\n\n### Prerequisites\nInstall `pre-commit` by running:\n```bash\npip3 install pre-commit\n```\nBefore you can use the `trufflehog` pre-commit hook, you need to have the `trufflehog` tool installed. You can install it using the following command:\n```bash\nbrew install trufflehog\n```\nOnce you have both tools installed, you can run `pre-commit install` to install the pre-commit hooks in your repository:\n\n### Using the Pre-Commit Hook\nOnce you have the `trufflehog` tool installed and have added the patterns you want to search for (OAI keys added by default), you can use the pre-commit hook to automatically scan your code before each commit. To use the pre-commit hook, simply run the `git commit` command as you normally would. \n\nThe `trufflehog` tool will automatically scan your code for secrets and reject the commit if any are found. If any secrets are found, you will be prompted to remove them before trying.\n", "release_dates": []}, {"name": "jukebox", "description": "Code for the paper \"Jukebox: A Generative Model for Music\"", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Jukebox\nCode for \"Jukebox: A Generative Model for Music\"\n\n[Paper](https://arxiv.org/abs/2005.00341) \n[Blog](https://openai.com/blog/jukebox) \n[Explorer](http://jukebox.openai.com/) \n[Colab](https://colab.research.google.com/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb) \n\n# Install\nInstall the conda package manager from https://docs.conda.io/en/latest/miniconda.html    \n    \n``` \n# Required: Sampling\nconda create --name jukebox python=3.7.5\nconda activate jukebox\nconda install mpi4py=3.0.3 # if this fails, try: pip install mpi4py==3.0.3\nconda install pytorch=1.4 torchvision=0.5 cudatoolkit=10.0 -c pytorch\ngit clone https://github.com/openai/jukebox.git\ncd jukebox\npip install -r requirements.txt\npip install -e .\n\n# Required: Training\nconda install av=7.0.01 -c conda-forge \npip install ./tensorboardX\n \n# Optional: Apex for faster training with fused_adam\nconda install pytorch=1.1 torchvision=0.3 cudatoolkit=10.0 -c pytorch\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n```\n\n# Sampling\n## Sampling from scratch\nTo sample normally, run the following command. Model can be `5b`, `5b_lyrics`, `1b_lyrics`\n``` \npython jukebox/sample.py --model=5b_lyrics --name=sample_5b --levels=3 --sample_length_in_seconds=20 \\\n--total_sample_length_in_seconds=180 --sr=44100 --n_samples=6 --hop_fraction=0.5,0.5,0.125\n```\n``` \npython jukebox/sample.py --model=1b_lyrics --name=sample_1b --levels=3 --sample_length_in_seconds=20 \\\n--total_sample_length_in_seconds=180 --sr=44100 --n_samples=16 --hop_fraction=0.5,0.5,0.125\n```\nThe above generates the first `sample_length_in_seconds` seconds of audio from a song of total length `total_sample_length_in_seconds`.\nTo use multiple GPU's, launch the above scripts as `mpiexec -n {ngpus} python jukebox/sample.py ...` so they use `{ngpus}`\n\nThe samples decoded from each level are stored in `{name}/level_{level}`. \nYou can also view the samples as an html with the aligned lyrics under `{name}/level_{level}/index.html`. \nRun `python -m http.server` and open the html through the server to see the lyrics animate as the song plays.  \nA summary of all sampling data including zs, x, labels and sampling_kwargs is stored in `{name}/level_{level}/data.pth.tar`.\n\nThe hps are for a V100 GPU with 16 GB GPU memory. The `1b_lyrics`, `5b`, and `5b_lyrics` top-level priors take up \n3.8 GB, 10.3 GB, and 11.5 GB, respectively. The peak memory usage to store transformer key, value cache is about 400 MB \nfor `1b_lyrics` and 1 GB for `5b_lyrics` per sample. If you are having trouble with CUDA OOM issues, try `1b_lyrics` or \ndecrease `max_batch_size` in sample.py, and `--n_samples` in the script call.\n\nOn a V100, it takes about 3 hrs to fully sample 20 seconds of music. Since this is a long time, it is recommended to use `n_samples > 1` so you can generate as many samples as possible in parallel. The 1B lyrics and upsamplers can process 16 samples at a time, while 5B can fit only up to 3. Since the vast majority of time is spent on upsampling, we recommend using a multiple of 3 less than 16 like `--n_samples 15` for `5b_lyrics`. This will make the top-level generate samples in groups of three while upsampling is done in one pass.\n\nTo continue sampling from already generated codes for a longer duration, you can run\n```\npython jukebox/sample.py --model=5b_lyrics --name=sample_5b --levels=3 --mode=continue \\\n--codes_file=sample_5b/level_0/data.pth.tar --sample_length_in_seconds=40 --total_sample_length_in_seconds=180 \\\n--sr=44100 --n_samples=6 --hop_fraction=0.5,0.5,0.125\n```\nHere, we take the 20 seconds samples saved from the first sampling run at `sample_5b/level_0/data.pth.tar` and continue by adding 20 more seconds. \n\nYou could also continue directly from the level 2 saved outputs, just pass `--codes_file=sample_5b/level_2/data.pth.tar`.\n Note this will upsample the full 40 seconds song at the end.\n\nIf you stopped sampling at only the first level and want to upsample the saved codes, you can run\n```\npython jukebox/sample.py --model=5b_lyrics --name=sample_5b --levels=3 --mode=upsample \\\n--codes_file=sample_5b/level_2/data.pth.tar --sample_length_in_seconds=20 --total_sample_length_in_seconds=180 \\\n--sr=44100 --n_samples=6 --hop_fraction=0.5,0.5,0.125\n```\nHere, we take the 20 seconds samples saved from the first sampling run at `sample_5b/level_2/data.pth.tar` and upsample the lower two levels.\n\n## Prompt with your own music\nIf you want to prompt the model with your own creative piece or any other music, first save them as wave files and run\n```\npython jukebox/sample.py --model=5b_lyrics --name=sample_5b_prompted --levels=3 --mode=primed \\\n--audio_file=path/to/recording.wav,awesome-mix.wav,fav-song.wav,etc.wav --prompt_length_in_seconds=12 \\\n--sample_length_in_seconds=20 --total_sample_length_in_seconds=180 --sr=44100 --n_samples=6 --hop_fraction=0.5,0.5,0.125\n```\nThis will load the four files, tile them to fill up to `n_samples` batch size, and prime the model with the first `prompt_length_in_seconds` seconds.\n\n# Training\n## VQVAE\nTo train a small vqvae, run\n```\nmpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae --name=small_vqvae --sample_length=262144 --bs=4 \\\n--audio_files_dir={audio_files_dir} --labels=False --train --aug_shift --aug_blend\n```\nHere, `{audio_files_dir}` is the directory in which you can put the audio files for your dataset, and `{ngpus}` is number of GPU's you want to use to train. \nThe above trains a two-level VQ-VAE with `downs_t = (5,3)`, and `strides_t = (2, 2)` meaning we downsample the audio by `2**5 = 32` to get the first level of codes, and `2**8 = 256` to get the second level codes.  \nCheckpoints are stored in the `logs` folder. You can monitor the training by running Tensorboard\n```\ntensorboard --logdir logs\n```\n    \n## Prior\n### Train prior or upsamplers\nOnce the VQ-VAE is trained, we can restore it from its saved checkpoint and train priors on the learnt codes. \nTo train the top-level prior, we can run\n\n```\nmpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae,small_prior,all_fp16,cpu_ema --name=small_prior \\\n--sample_length=2097152 --bs=4 --audio_files_dir={audio_files_dir} --labels=False --train --test --aug_shift --aug_blend \\\n--restore_vqvae=logs/small_vqvae/checkpoint_latest.pth.tar --prior --levels=2 --level=1 --weight_decay=0.01 --save_iters=1000\n```\n\nTo train the upsampler, we can run\n```\nmpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae,small_upsampler,all_fp16,cpu_ema --name=small_upsampler \\\n--sample_length=262144 --bs=4 --audio_files_dir={audio_files_dir} --labels=False --train --test --aug_shift --aug_blend \\\n--restore_vqvae=logs/small_vqvae/checkpoint_latest.pth.tar --prior --levels=2 --level=0 --weight_decay=0.01 --save_iters=1000\n```\nWe pass `sample_length = n_ctx * downsample_of_level` so that after downsampling the tokens match the n_ctx of the prior hps. \nHere, `n_ctx = 8192` and `downsamples = (32, 256)`, giving `sample_lengths = (8192 * 32, 8192 * 256) = (65536, 2097152)` respectively for the bottom and top level. \n\n### Learning rate annealing\nTo get the best sample quality anneal the learning rate to 0 near the end of training. To do so, continue training from the latest \ncheckpoint and run with\n```\n--restore_prior=\"path/to/checkpoint\" --lr_use_linear_decay --lr_start_linear_decay={already_trained_steps} --lr_decay={decay_steps_as_needed}\n```\n\n### Reuse pre-trained VQ-VAE and train top-level prior on new dataset from scratch.\n#### Train without labels\nOur pre-trained VQ-VAE can produce compressed codes for a wide variety of genres of music, and the pre-trained upsamplers \ncan upsample them back to audio that sound very similar to the original audio.\nTo re-use these for a new dataset of your choice, you can retrain just the top-level  \n\nTo train top-level on a new dataset, run\n```\nmpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_prior \\\n--sample_length=1048576 --bs=4 --aug_shift --aug_blend --audio_files_dir={audio_files_dir} \\\n--labels=False --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000\n```\nTraining the `small_prior` with a batch size of 2, 4, and 8 requires 6.7 GB, 9.3 GB, and 15.8 GB of GPU memory, respectively. A few days to a week of training typically yields reasonable samples when the dataset is homogeneous (e.g. all piano pieces, songs of the same style, etc).\n\nNear the end of training, follow [this](#learning-rate-annealing) to anneal the learning rate to 0\n\n#### Sample from new model\nYou can then run sample.py with the top-level of our models replaced by your new model. To do so,\n- Add an entry `my_model=(\"vqvae\", \"upsampler_level_0\", \"upsampler_level_1\", \"small_prior\")` in `MODELS` in `make_models.py`. \n- Update the `small_prior` dictionary in `hparams.py` to include `restore_prior='path/to/checkpoint'`. If you\nyou changed any hps directly in the command line script (eg:`heads`), make sure to update them in the dictionary too so \nthat `make_models` restores our checkpoint correctly.\n- Run sample.py as outlined in the sampling section, but now with `--model=my_model` \n\nFor example, let's say we trained `small_vqvae`, `small_prior`, and `small_upsampler` under `/path/to/jukebox/logs`. In `make_models.py`, we are going to declare a tuple of the new models as `my_model`.\n```\nMODELS = {\n    '5b': (\"vqvae\", \"upsampler_level_0\", \"upsampler_level_1\", \"prior_5b\"),\n    '5b_lyrics': (\"vqvae\", \"upsampler_level_0\", \"upsampler_level_1\", \"prior_5b_lyrics\"),\n    '1b_lyrics': (\"vqvae\", \"upsampler_level_0\", \"upsampler_level_1\", \"prior_1b_lyrics\"),\n    'my_model': (\"my_small_vqvae\", \"my_small_upsampler\", \"my_small_prior\"),\n}\n```\n\nNext, in `hparams.py`, we add them to the registry with the corresponding `restore_`paths and any other command line options used during training. Another important note is that for top-level priors with lyric conditioning, we have to locate a self-attention layer that shows alignment between the lyric and music tokens. Look for layers where `prior.prior.transformer._attn_mods[layer].attn_func` is either 6 or 7. If your model is starting to sing along lyrics, it means some layer, head pair has learned alignment. Congrats!\n```\nmy_small_vqvae = Hyperparams(\n    restore_vqvae='/path/to/jukebox/logs/small_vqvae/checkpoint_some_step.pth.tar',\n)\nmy_small_vqvae.update(small_vqvae)\nHPARAMS_REGISTRY[\"my_small_vqvae\"] = my_small_vqvae\n\nmy_small_prior = Hyperparams(\n    restore_prior='/path/to/jukebox/logs/small_prior/checkpoint_latest.pth.tar',\n    level=1,\n    labels=False,\n    # TODO For the two lines below, if `--labels` was used and the model is\n    # trained with lyrics, find and enter the layer, head pair that has learned\n    # alignment.\n    alignment_layer=47,\n    alignment_head=0,\n)\nmy_small_prior.update(small_prior)\nHPARAMS_REGISTRY[\"my_small_prior\"] = my_small_prior\n\nmy_small_upsampler = Hyperparams(\n    restore_prior='/path/to/jukebox/logs/small_upsampler/checkpoint_latest.pth.tar',\n    level=0,\n    labels=False,\n)\nmy_small_upsampler.update(small_upsampler)\nHPARAMS_REGISTRY[\"my_small_upsampler\"] = my_small_upsampler\n```\n\n#### Train with labels \nTo train with you own metadata for your audio files, implement `get_metadata` in `data/files_dataset.py` to return the \n`artist`, `genre` and `lyrics` for a given audio file. For now, you can pass `''` for lyrics to not use any lyrics.\n\nFor training with labels, we'll use `small_labelled_prior` in `hparams.py`, and we set `labels=True,labels_v3=True`. \nWe use 2 kinds of labels information:\n- Artist/Genre: \n  - For each file, we return an artist_id and a list of genre_ids. The reason we have a list and not a single genre_id \n  is that in v2, we split genres like `blues_rock` into a bag of words `[blues, rock]`, and we pass atmost \n  `max_bow_genre_size` of those, in `v3` we consider it as a single word and just set `max_bow_genre_size=1`.\n  - Update the `v3_artist_ids` and `v3_genre_ids` to use ids from your new dataset. \n  - In `small_labelled_prior`, set the hps `y_bins = (number_of_genres, number_of_artists)` and `max_bow_genre_size=1`. \n- Timing: \n  - For each chunk of audio, we return the `total_length` of the song, the `offset` the current audio chunk is at and \n  the `sample_length` of the audio chunk. We have three timing embeddings: total_length, our current position, and our \n  current position as a fraction of the total length, and we divide the range of these values into `t_bins` discrete bins. \n  - In `small_labelled_prior`, set the hps `min_duration` and `max_duration` to be the shortest/longest duration of audio \n  files you want for your dataset, and `t_bins` for how many bins you want to discretize timing information into. Note \n  `min_duration * sr` needs to be at least `sample_length` to have an audio chunk in it.\n\nAfter these modifications, to train a top-level with labels, run\n```\nmpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_labelled_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_prior_labels \\\n--sample_length=1048576 --bs=4 --aug_shift --aug_blend --audio_files_dir={audio_files_dir} \\\n--labels=True --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000\n```\n\nFor sampling, follow same instructions as [above](#sample-from-new-model) but use `small_labelled_prior` instead of `small_prior`.  \n\n#### Train with lyrics\nTo train in addition with lyrics, update `get_metadata` in `data/files_dataset.py` to return `lyrics` too.\nFor training with lyrics, we'll use `small_single_enc_dec_prior` in `hparams.py`. \n- Lyrics: \n  - For each file, we linearly align the lyric characters to the audio, find the position in lyric that corresponds to \n  the midpoint of our audio chunk, and pass a window of `n_tokens` lyric characters centred around that. \n  - In `small_single_enc_dec_prior`, set the hps `use_tokens=True` and `n_tokens` to be the number of lyric characters \n  to use for an audio chunk. Set it according to the `sample_length` you're training on so that its large enough that \n  the lyrics for an audio chunk are almost always found inside a window of that size.\n  - If you use a non-English vocabulary, update `text_processor.py` with your new vocab and set\n  `n_vocab = number of characters in vocabulary` accordingly in `small_single_enc_dec_prior`. In v2, we had a `n_vocab=80` \n  and in v3 we missed `+` and so `n_vocab=79` of characters. \n\nAfter these modifications, to train a top-level with labels and lyrics, run\n```\nmpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_single_enc_dec_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_single_enc_dec_prior_labels \\\n--sample_length=786432 --bs=4 --aug_shift --aug_blend --audio_files_dir={audio_files_dir} \\\n--labels=True --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000\n```\nTo simplify hps choices, here we used a `single_enc_dec` model like the `1b_lyrics` model that combines both encoder and \ndecoder of the transformer into a single model. We do so by merging the lyric vocab and vq-vae vocab into a single \nlarger vocab, and flattening the lyric tokens and the vq-vae codes into a single sequence of length `n_ctx + n_tokens`. \nThis uses `attn_order=12` which includes `prime_attention` layers with keys/values from lyrics and queries from audio. \nIf you instead want to use a model with the usual encoder-decoder style transformer, use `small_sep_enc_dec_prior`.\n\nFor sampling, follow same instructions as [above](#sample-from-new-model) but use `small_single_enc_dec_prior` instead of \n`small_prior`. To also get the alignment between lyrics and samples in the saved html, you'll need to set `alignment_layer` \nand `alignment_head` in `small_single_enc_dec_prior`. To find which layer/head is best to use, run a forward pass on a training example,\nsave the attention weight tensors for all prime_attention layers, and pick the (layer, head) which has the best linear alignment \npattern between the lyrics keys and music queries. \n\n### Fine-tune pre-trained top-level prior to new style(s)\nPreviously, we showed how to train a small top-level prior from scratch. Assuming you have a GPU with at least 15 GB of memory and support for fp16, you could fine-tune from our pre-trained 1B top-level prior. Here are the steps:\n\n- Support `--labels=True` by implementing `get_metadata` in `jukebox/data/files_dataset.py` for your dataset.\n- Add new entries in `jukebox/data/ids`. We recommend replacing existing mappings (e.g. rename `\"unknown\"`, etc with styles of your choice). This uses the pre-trained style vectors as initialization and could potentially save some compute.\n\nAfter these modifications, run \n```\nmpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,prior_1b_lyrics,all_fp16,cpu_ema --name=finetuned \\\n--sample_length=1048576 --bs=1 --aug_shift --aug_blend --audio_files_dir={audio_files_dir} \\\n--labels=True --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000\n```\nTo get the best sample quality, it is recommended to anneal the learning rate in the end. Training the 5B top-level requires GPipe which is not supported in this release.\n\n# Citation\n\nPlease cite using the following bibtex entry:\n\n```\n@article{dhariwal2020jukebox,\n  title={Jukebox: A Generative Model for Music},\n  author={Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},\n  journal={arXiv preprint arXiv:2005.00341},\n  year={2020}\n}\n```\n\n# License \n[Noncommercial Use License](./LICENSE) \n\nIt covers both released code and weights. \n\n", "release_dates": []}, {"name": "junit-annotate-buildkite-plugin", "description": "\ud83d\udcc8 Summarise your test failures as a build annotation", "language": "Ruby", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# JUnit Annotate Buildkite Plugin\n\nA [Buildkite plugin](https://buildkite.com/docs/agent/v3/plugins) that parses junit.xml artifacts (generated across any number of parallel steps) and creates a [build annotation](https://buildkite.com/docs/agent/v3/cli-annotate) listing the individual tests that failed.\n\n## Example\n\nThe following pipeline will run `test.sh` jobs in parallel, and then process all the resulting JUnit XML files to create a summary build annotation.\n\n```yml\nsteps:\n  - command: test.sh\n    parallelism: 50\n    artifact_paths: tmp/junit-*.xml\n  - wait: ~\n    continue_on_failure: true\n  - plugins:\n      junit-annotate#v0.0.1:\n        artifacts: tmp/junit-*.xml\n```\n\n## Configuration\n\n### `artifacts` (required)\n\nThe artifact glob path to find the JUnit XML files.\n\nExample: `tmp/junit-*.xml`\n\n## License\n\nMIT (see [LICENSE](LICENSE))", "release_dates": []}, {"name": "kubernetes", "description": "Production-Grade Container Scheduling and Management", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Kubernetes (K8s)\n\n[![GoPkg Widget]][GoPkg] [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/569/badge)](https://bestpractices.coreinfrastructure.org/projects/569)\n\n<img src=\"https://github.com/kubernetes/kubernetes/raw/master/logo/logo.png\" width=\"100\">\n\n----\n\nKubernetes, also known as K8s, is an open source system for managing [containerized applications]\nacross multiple hosts. It provides basic mechanisms for deployment, maintenance,\nand scaling of applications.\n\nKubernetes builds upon a decade and a half of experience at Google running\nproduction workloads at scale using a system called [Borg],\ncombined with best-of-breed ideas and practices from the community.\n\nKubernetes is hosted by the Cloud Native Computing Foundation ([CNCF]).\nIf your company wants to help shape the evolution of\ntechnologies that are container-packaged, dynamically scheduled,\nand microservices-oriented, consider joining the CNCF.\nFor details about who's involved and how Kubernetes plays a role,\nread the CNCF [announcement].\n\n----\n\n## To start using K8s\n\nSee our documentation on [kubernetes.io].\n\nTry our [interactive tutorial].\n\nTake a free course on [Scalable Microservices with Kubernetes].\n\nTo use Kubernetes code as a library in other applications, see the [list of published components](https://git.k8s.io/kubernetes/staging/README.md).\nUse of the `k8s.io/kubernetes` module or `k8s.io/kubernetes/...` packages as libraries is not supported.\n\n## To start developing K8s\n\nThe [community repository] hosts all information about\nbuilding Kubernetes from source, how to contribute code\nand documentation, who to contact about what, etc.\n\nIf you want to build Kubernetes right away there are two options:\n\n##### You have a working [Go environment].\n\n```\nmkdir -p $GOPATH/src/k8s.io\ncd $GOPATH/src/k8s.io\ngit clone https://github.com/kubernetes/kubernetes\ncd kubernetes\nmake\n```\n\n##### You have a working [Docker environment].\n\n```\ngit clone https://github.com/kubernetes/kubernetes\ncd kubernetes\nmake quick-release\n```\n\nFor the full story, head over to the [developer's documentation].\n\n## Support\n\nIf you need support, start with the [troubleshooting guide],\nand work your way through the process that we've outlined.\n\nThat said, if you have questions, reach out to us\n[one way or another][communication].\n\n[announcement]: https://cncf.io/news/announcement/2015/07/new-cloud-native-computing-foundation-drive-alignment-among-container\n[Borg]: https://research.google.com/pubs/pub43438.html\n[CNCF]: https://www.cncf.io/about\n[communication]: https://git.k8s.io/community/communication\n[community repository]: https://git.k8s.io/community\n[containerized applications]: https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/\n[developer's documentation]: https://git.k8s.io/community/contributors/devel#readme\n[Docker environment]: https://docs.docker.com/engine\n[Go environment]: https://golang.org/doc/install\n[GoPkg]: https://pkg.go.dev/k8s.io/kubernetes\n[GoPkg Widget]: https://pkg.go.dev/badge/k8s.io/kubernetes.svg\n[interactive tutorial]: https://kubernetes.io/docs/tutorials/kubernetes-basics\n[kubernetes.io]: https://kubernetes.io\n[Scalable Microservices with Kubernetes]: https://www.udacity.com/course/scalable-microservices-with-kubernetes--ud615\n[troubleshooting guide]: https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/", "release_dates": []}, {"name": "kubernetes-ec2-autoscaler", "description": "A batch-optimized scaling manager for Kubernetes", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archived (no changes will be merged)\n\n# kubernetes-ec2-autoscaler\n\nkubernetes-ec2-autoscaler was a node-level autoscaler for [Kubernetes](http://kubernetes.io/)\non AWS EC2 designed for batch jobs.\n\nWe recommend looking at the [Kubernetes Cluster Autoscaler](https://github.com/kubernetes/autoscaler).\n", "release_dates": []}, {"name": "large-scale-curiosity", "description": "Code for the paper \"Large-Scale Study of Curiosity-Driven Learning\"", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n## Large-Scale Study of Curiosity-Driven Learning ##\n#### [[Project Website]](https://pathak22.github.io/large-scale-curiosity/) [[Demo Video]](https://youtu.be/l1FqtAHfJLI)\n\n[Yuri Burda*](https://sites.google.com/site/yburda/), [Harri Edwards*](https://github.com/harri-edwards/), [Deepak Pathak*](https://people.eecs.berkeley.edu/~pathak/), <br/>[Amos Storkey](http://homepages.inf.ed.ac.uk/amos/), [Trevor Darrell](https://people.eecs.berkeley.edu/~trevor/), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros/)<br/>\n(&#42; alphabetical ordering, equal contribution)\n\nUniversity of California, Berkeley<br/>\nOpenAI<br/>\nUniversity of Edinburgh\n\n<a href=\"https://pathak22.github.io/large-scale-curiosity/\">\n<img src=\"https://pathak22.github.io/large-scale-curiosity/resources/teaser.jpg\" width=\"500\">\n</img></a>\n\nThis is a TensorFlow based implementation for our [paper on large-scale study of curiosity-driven learning](https://pathak22.github.io/large-scale-curiosity/) across\n54 environments. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper, We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments. We further investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). If you find this work useful in your research, please cite:\n\n    @inproceedings{largeScaleCuriosity2018,\n        Author = {Burda, Yuri and Edwards, Harri and\n                  Pathak, Deepak and Storkey, Amos and\n                  Darrell, Trevor and Efros, Alexei A.},\n        Title = {Large-Scale Study of Curiosity-Driven Learning},\n        Booktitle = {arXiv:1808.04355},\n        Year = {2018}\n    }\n\n### Installation and Usage\nThe following command should train a pure exploration agent on Breakout with default experiment parameters.\n```bash\npython run.py\n```\nTo use more than one gpu/machine, use MPI (e.g. `mpiexec -n 8 python run.py` should use 1024 parallel environments to collect experience instead of the default 128 on an 8 gpu machine).\n\n### Data for plots in paper\n\n[Data for Figure-2](https://www.dropbox.com/s/ufr7o8g9omb9zpl/experiments.tar.gz): contains raw game score data along with the plotting code to generate Figure-2 in the paper.\n\n### Other helpful pointers\n- [Paper](https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf)\n- [Project Website](https://pathak22.github.io/large-scale-curiosity/)\n- [Demo Video](https://youtu.be/l1FqtAHfJLI)\n", "release_dates": []}, {"name": "lean-gym", "description": null, "language": "Lean", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# lean-gym\n\nThis repository lets you interact with Lean through a REPL.\nSee [Formal Mathematics Statement Curriculum Learning](https://arxiv.org/abs/2202.01344)\nfor a presentation of `lean-gym`.\n\n## Setup\n\n```bash\n# Download pre-built binaries and build the project (targeting mathlib).\nbash ./scripts/setup.sh\n```\n\n## Usage\n\n```bash\nlean --run src/repl.lean\n```\n\nStarts a fresh REPL. Once started, the REPL accepts the following commands:\n\n- `init_search`: takes a declaration name as well as a list of open namespaces\nto initialize a search at the given declaration opening the provided namespaces,\nand returning the initial tactic state (along with a fresh `search_id` and\n`tactic_state_id`).\n- `run_tac`: takes a `search_id`, a `tactic_state_id` and a tactic to apply at\nthe tactic state denoted by the provided ids.\n- `clear_search`: takes a `search_id` to clear all state related to a search.\n\nThe commands can be interleaved freely enabling the parallelization of multiple\nproof searches against the same REPL.\n\n```\n$ lean --run src/repl.lean\n\n[\"init_search\", [\"int.prime.dvd_mul\", \"\"]]\n{\"error\":null,\"proof_steps\":[],\"search_id\":\"0\",\"tactic_state\":\"\u22a2 \u2200 {m n : \u2124} {p : \u2115}, nat.prime p \u2192 \u2191p \u2223 m * n \u2192 p \u2223 m.nat_abs \u2228 p \u2223 n.nat_abs\",\"tactic_state_id\":\"0\"}\n\n[\"run_tac\",[\"0\",\"0\",\"intros\"]]\n{\"error\":null,\"proof_steps\":[],\"search_id\":\"0\",\"tactic_state\":\"m n : \u2124,\\np : \u2115,\\nhp : nat.prime p,\\nh : \u2191p \u2223 m * n\\n\u22a2 p \u2223 m.nat_abs \u2228 p \u2223 n.nat_abs\",\"tactic_state_id\":\"1\"}\n\n[\"run_tac\",[\"0\",\"1\",\"apply (nat.prime.dvd_mul hp).mp\"]]\n{\"error\":null,\"proof_steps\":[],\"search_id\":\"0\",\"tactic_state\":\"m n : \u2124,\\np : \u2115,\\nhp : nat.prime p,\\nh : \u2191p \u2223 m * n\\n\u22a2 p \u2223 m.nat_abs * n.nat_abs\",\"tactic_state_id\":\"2\"}\n\n[\"run_tac\",[\"0\",\"2\",\"rw \u2190 int.nat_abs_mul\"]]\n{\"error\":null,\"proof_steps\":[],\"search_id\":\"0\",\"tactic_state\":\"m n : \u2124,\\np : \u2115,\\nhp : nat.prime p,\\nh : \u2191p \u2223 m * n\\n\u22a2 p \u2223 (m * n).nat_abs\",\"tactic_state_id\":\"3\"}\n\n[\"run_tac\",[\"0\",\"3\",\"exact int.coe_nat_dvd_left.mp h\"]]\n{\"error\":null,\"proof_steps\":[],\"search_id\":\"0\",\"tactic_state\":\"no goals\",\"tactic_state_id\":\"4\"}\n```\n\n## Declaration names\n\nDeclaration names and open namespaces are available in the `data/` directory to be used with the\n`init_search` command.\n\n## Notes\n\nThe REPL is subject to crashes in rare cases. Empirically such crash happens no\nmore than ~0.01% of the time.\n\nWhen a tactic state is reached with no left goals, some custom logic is run to check that the\nresulting proof's type matches the top level goal type and does not rely on `sorry`. We also check\nfor the presence of `undefined` in the proof term. As an example, the following MiniF2F proofs will\nsafely fail with error `proof_validation_failed`.\n\n```\n[\"init_search\", [\"mathd_algebra_35\", \"\"]]\n[\"run_tac\", [\"0\", \"0\", \"intros\"]]\n[\"run_tac\", [\"0\", \"1\", \"sorry\"]]\n```\n\n```\n[\"init_search\", [\"induction_divisibility_3divnto3m2n\", \"\"]]\n[\"run_tac\", [\"0\", \"0\", \"intros\"]]\n[\"run_tac\", [\"0\", \"1\", \"rw [add_comm]\"]]\n[\"run_tac\", [\"0\", \"2\", \"have h3 : 1 * (n + 1) \u2264 (n + 1)\"]]\n[\"run_tac\", [\"0\", \"3\", \"rw one_mul\"]]\n[\"run_tac\", [\"0\", \"4\", \"apply dvd_trans\"]]\n[\"run_tac\", [\"0\", \"5\", \"swap\"]]\n[\"run_tac\", [\"0\", \"6\", \"simp []\"]]\n```\n\n```\n[\"init_search\", [\"mathd_numbertheory_13\", \"\"]]\n[\"run_tac\", [\"0\", \"0\", \"intros u v hu hv hsum\"]]\n[\"run_tac\", [\"0\", \"1\", \"intro h\"]]\n[\"run_tac\", [\"0\", \"2\", \"contrapose h\"]]\n[\"run_tac\", [\"0\", \"3\", \"intro hn\"]]\n[\"run_tac\", [\"0\", \"4\", \"exact not_lt_of_lt hn undefined\"]]\n```\n", "release_dates": []}, {"name": "LHOPT", "description": "Learned Hyperparameter Optimizers", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# LHOPT\n\nCode for inner optimizer (CIAO) and an example training config for \"A Generalizable Approach to Learning Optimizers\".\n", "release_dates": []}, {"name": "lm-human-preferences", "description": "Code for the paper Fine-Tuning Language Models from Human Preferences", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n**Status:** All references to `gs://lm-human-preferences/` were updated to `https://openaipublic.blob.core.windows.net/lm-human-preferences`, as we migrated from GCP to Azure. The code provided as is may no longer work. Pull requests welcome\n\n# lm-human-preferences\n\nThis repository contains code for the paper [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593).  See also our [blog post](https://openai.com/blog/fine-tuning-gpt-2/).\n\nWe provide code for:\n- Training reward models from human labels\n- Fine-tuning language models using those reward models\n\nIt does not contain code for generating labels.  However, we have released human labels collected for our experiments, at `gs://lm-human-preferences/labels`.\nFor those interested, the question and label schemas are simple and documented in [`label_types.py`](./lm_human_preferences/label_types.py).\n\nThe code has only been tested using the smallest GPT-2 model (124M parameters).\n\n## Instructions\n\nThis code has only been tested using Python 3.7.3.  Training has been tested on GCE machines with 8 V100s, running Ubuntu 16.04, but development also works on Mac OS X.\n\n### Installation\n\n- Install [pipenv](https://github.com/pypa/pipenv#installation).\n\n- Install [tensorflow](https://www.tensorflow.org/install/gpu):  Install CUDA 10.0 and cuDNN 7.6.2, then `pipenv install tensorflow-gpu==1.13.1`.  The code may technically run with tensorflow on CPU but will be very slow.\n\n- Install [`gsutil`](https://cloud.google.com/storage/docs/gsutil_install)\n\n- Clone this repo.  Then:\n  ```\n  pipenv install\n  ```\n\n- (Recommended) Install [`horovod`](https://github.com/horovod/horovod#install) to speed up the code, or otherwise substitute some fast implementation in the `mpi_allreduce_sum` function of [`core.py`](./lm_human_preferences/utils/core.py).  Make sure to use pipenv for the install, e.g. `pipenv install horovod==0.18.1`.\n\n### Running\n\nThe following examples assume we are aiming to train a model to continue text in a physically descriptive way.\nYou can read [`launch.py`](./launch.py) to see how the `descriptiveness` experiments and others are defined.\n\nNote that we provide pre-trained models, so you can skip directly to RL fine-tuning or even to sampling from a trained policy, if desired.\n\n#### Training a reward model\n\nTo train a reward model, use a command such as\n```\nexperiment=descriptiveness\nreward_experiment_name=testdesc-$(date +%y%m%d%H%M)\npipenv run ./launch.py train_reward $experiment $reward_experiment_name\n```\n\nThis will save outputs (and tensorboard event files) to the directory `/tmp/save/train_reward/$reward_experiment_name`.  The directory can be changed via the `--save_dir` flag.\n\n#### Finetuning a language model\n\nOnce you have trained a reward model, you can finetune against it.\n\nFirst, set\n```\ntrained_reward_model=/tmp/save/train_reward/$reward_experiment_name\n```\nor if using our pretrained model,\n```\ntrained_reward_model=gs://lm-human-preferences/runs/descriptiveness/reward_model\n```\n\nThen,\n```\nexperiment=descriptiveness\npolicy_experiment_name=testdesc-$(date +%y%m%d%H%M)\npipenv run ./launch.py train_policy $experiment $policy_experiment_name --rewards.trained_model $trained_reward_model --rewards.train_new_model 'off'\n```\n\nThis will save outputs (and tensorboard event files) to the directory `/tmp/save/train_policy/$policy_experiment_name`.  The directory can be changed via the `--save_dir` flag.\n\n#### Both steps at once\n\nYou can run a single command to train a reward model and then finetune against it\n```\nexperiment=descriptiveness\nexperiment_name=testdesc-$(date +%y%m%d%H%M)\npipenv run ./launch.py train_policy $experiment $experiment_name\n```\n\nIn this case, outputs are in the directory `/tmp/save/train_policy/$policy_experiment_name`, and the reward model is saved to a subdirectory `reward_model`.  The directory can be changed via the `--save_dir` flag.\n\n#### Sampling from a trained policy\n\nSpecify the policy to load:\n```\nsave_dir=/tmp/save/train_policy/$policy_experiment_name\n```\nor if using our pretrained model,\n```\nsave_dir=gs://lm-human-preferences/runs/descriptiveness\n```\n\nThen run:\n```\npipenv run ./sample.py sample --save_dir $save_dir --savescope policy\n```\n\nNote that this script can run on less than 8 GPUs.  You can pass the flag `--mpi 1`, for exapmle, if you only have one GPU.\n\n## LICENSE\n\n[MIT](./LICENSE)\n\n## Citation\n\nPlease cite the paper with the following bibtex entry:\n```\n@article{ziegler2019finetuning,\n  title={Fine-Tuning Language Models from Human Preferences},\n  author={Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},\n  journal={arXiv preprint arXiv:1909.08593},\n  url={https://arxiv.org/abs/1909.08593},\n  year={2019}\n}\n```\n", "release_dates": []}, {"name": "lucid", "description": "A collection of infrastructure and tools for research in neural network interpretability.", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<img src=\"https://storage.googleapis.com/lucid-static/common/stickers/channels-visualizations.jpg\" width=\"782\"></img>\n\n# Lucid\n\n<!--*DeepDream, but sane. Home of cats, dreams, and interpretable neural networks.*-->\n\n[![PyPI project status](https://img.shields.io/pypi/status/Lucid.svg)]()\n[![Travis build status](https://img.shields.io/travis/tensorflow/lucid.svg)](https://travis-ci.org/tensorflow/lucid)\n[![Code coverage](https://img.shields.io/coveralls/github/tensorflow/lucid.svg)](https://coveralls.io/github/tensorflow/lucid)\n[![Supported Python version](https://img.shields.io/pypi/pyversions/Lucid.svg)]()\n[![PyPI release version](https://img.shields.io/pypi/v/Lucid.svg)](https://pypi.org/project/Lucid/)\n\n\nLucid is a collection of infrastructure and tools for research in neural\nnetwork interpretability.\n\n* [\ud83d\udcd3\u2000**Notebooks**](#notebooks) -- Get started without any setup!\n* [\ud83d\udcda\u2000**Reading**](#recomended-reading) -- Learn more about visualizing neural nets.\n* [\ud83d\udcac\u2000**Community**](#community) -- Want to get involved? Please reach out!\n* [\ud83d\udd27\u2000**Additional Information**](#additional-information) -- Licensing, code style, etc.\n* [\ud83d\udd2c\u2000**Start Doing Research!**](https://github.com/tensorflow/lucid/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3Aresearch) -- Want to get involved? We're trying to research openly!\n\n<br>\n\n# Notebooks\n\nStart visualizing neural networks ***with no setup***. The following notebooks\nrun right from your browser, thanks to [Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb). It's a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud.\n\nYou can run the notebooks on your local machine, too. Clone the repository and find them in the `notebooks` subfolder. You will need to run a local instance of the [Jupyter notebook environment](http://jupyter.org/install.html) to execute them.\n\n## Tutorial Notebooks\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/tutorial.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/common/stickers/colab-tutorial.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/modelzoo.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/common/stickers/colab-modelzoo.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<!--If you want to study techniques for visualizing and understanding neural networks, it's important to be able to try your experiments on multiple models. As of lucid v0.3, we provide a consistent API for interacting with 27 different vision models.-->\n\n## Feature Visualization Notebooks\n*Notebooks corresponding to the [Feature Visualization](https://distill.pub/2017/feature-visualization/) article*\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/feature-visualization/stickers/colab-neuron-negative.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/neuron_diversity.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/feature-visualization/stickers/colab-neuron-diversity.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/neuron_interaction.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/feature-visualization/stickers/colab-neuron-interaction.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/regularization.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/feature-visualization/stickers/colab-regularization.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n## Building Blocks Notebooks\n*Notebooks corresponding to the [Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/) article*\n\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/SemanticDictionary.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/building-blocks/stickers/colab-semantic-dict.png\" width=\"500\" alt=\"\"></img>\n</a>\n<br>\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/ActivationGrid.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/building-blocks/stickers/colab-grid.png\" width=\"500\" alt=\"\"></img>\n</a>\n<br>\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/AttrSpatial.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/building-blocks/stickers/colab-spatial-attr.png\" width=\"500\" alt=\"\"></img>\n</a>\n<br>\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/AttrChannel.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/building-blocks/stickers/colab-channel-attr.png\" width=\"500\" alt=\"\"></img>\n</a>\n<br>\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/NeuronGroups.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/building-blocks/stickers/colab-neuron-groups.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n\n## Differentiable Image Parameterizations Notebooks\n*Notebooks corresponding to the [Differentiable Image Parameterizations](https://distill.pub/2018/differentiable-parameterizations/) article*\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/aligned_interpolation.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/differentiable-parameterizations/stickers/colab-interpolate.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/style_transfer_2d.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/differentiable-parameterizations/stickers/colab-style-beyond-vgg.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/xy2rgb.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/differentiable-parameterizations/stickers/colab-xy2rgb.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/transparency.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/differentiable-parameterizations/stickers/colab-transparent.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/texture_synth_3d.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/differentiable-parameterizations/stickers/colab-3d-texture.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/style_transfer_3d.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/differentiable-parameterizations/stickers/colab-3d-style.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<br>\n\n## Miscellaneous Notebooks\n\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/misc/feature_inversion_caricatures.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/misc/stickers/colab-feature-inversion.ipynb.png\" width=\"500\" alt=\"\"></img>\n</a>\n<br>\n<a href=\"https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/misc/neuron_interaction_grids.ipynb\">\n<img src=\"https://storage.googleapis.com/lucid-static/misc/stickers/colab-interaction-grid.png\" width=\"500\" alt=\"\"></img>\n</a>\n\n<br> \n\n# Recomended Reading\n\n* [Feature Visualization](https://distill.pub/2017/feature-visualization/)\n* [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/)\n* [Using Arti\ufb01cial Intelligence to Augment Human Intelligence](https://distill.pub/2017/aia/)\n* [Visualizing Representations: Deep Learning and Human Beings](http://colah.github.io/posts/2015-01-Visualizing-Representations/)\n* [Differentiable Image Parameterizations](https://distill.pub/2018/differentiable-parameterizations/)\n\n## Related Talks\n* [Lessons from a year of Distill ML Research](https://www.youtube.com/watch?v=jlZsgUZaIyY) (Shan Carter, OpenVisConf)\n* [Machine Learning for Visualization](https://www.youtube.com/watch?v=6n-kCYn0zxU) (Ian Johnson, OpenVisConf)\n\n# Community\n\nWe're in `#proj-lucid` on the Distill slack ([join link](https://join.slack.com/t/distillpub/shared_invite/enQtMzg1NzU3MzEzMTg3LWJkNmQ4Y2JlNjJkNDlhYTU2ZmQxMGFkM2NiMTI2NGVjNzJkOTdjNTFiOGZmNDBjNTEzZGUwM2U0Mzg4NDAyN2E)).\n\nWe'd love to see more people doing research in this space!\n\n<br>\n\n# Additional Information\n\n## License and Disclaimer\n\nYou may use this software under the Apache 2.0 License. See [LICENSE](LICENSE).\n\nThis project is research code. It is not an official Google product.\n\n## Special consideration for TensorFlow dependency\n\nLucid requires `tensorflow`, but does not explicitly depend on it in `setup.py`. Due to the way [tensorflow is packaged](https://github.com/tensorflow/tensorflow/issues/7166) and some deficiencies in how pip handles dependencies, specifying either the GPU or the non-GPU version of tensorflow will conflict with the version of tensorflow your already may have installed.\n\nIf you don't want to add your own dependency on tensorflow, you can specify which tensorflow version you want lucid to install by selecting from `extras_require` like so: `lucid[tf]` or `lucid[tf_gpu]`.\n\n**In actual practice, we recommend you use your already installed version of tensorflow.**\n", "release_dates": []}, {"name": "lustre", "description": null, "language": null, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "Many resources for using, configuring, and troubleshooting Lustre are at:\n    http://wiki.lustre.org/\n\nThe low-volume list for announcements of new releases is at:\n    http://lists.lustre.org/listinfo.cgi/lustre-announce-lustre.org\nThe generic Lustre discussion mailing list is available at:\n    http://lists.lustre.org/listinfo.cgi/lustre-discuss-lustre.org\nThe Lustre developer mailing list is at:\n    http://lists.lustre.org/pipermail/lustre-devel-lustre.org\n\nInstructions for building, configuring and running Lustre:\n    http://wiki.lustre.org/Compiling_Lustre\nand\n    https://wiki.whamcloud.com/display/PUB/Getting+started+with+Lustre.\n\nInstructions for contributing patches for Lustre:\n    http://wiki.lustre.org/Submitting_Changes\nand\n    http://wiki.lustre.org/Using_Gerrit\n\nThe Lustre Coding Style Guidelines can be found at:\n    http://wiki.lustre.org/Lustre_Coding_Style_Guidelines\n\nThe Lustre Test Script Style Guidelines can be found at:\n    http://wiki.lustre.org/Lustre_Script_Coding_Style\n\nIn order to improve the quality of patches submitted to the Lustre tree,\nit is possible to automatically check every patch and commit against the\nLustre Coding Guidelines.  To do this, run in the top-level lustre/ tree:\n\n    cd .git/hooks\n    ln -s ../../contrib/git-hooks/{prepare-,}commit-msg ./\n    cd ../..\n", "release_dates": []}, {"name": "maddpg", "description": "Code for the MADDPG algorithm from the paper \"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n\nThis is the code for implementing the MADDPG algorithm presented in the paper:\n[Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf).\nIt is configured to be run in conjunction with environments from the\n[Multi-Agent Particle Environments (MPE)](https://github.com/openai/multiagent-particle-envs).\nNote: this codebase has been restructured since the original paper, and the results may\nvary from those reported in the paper.\n\n**Update:** the original implementation for policy ensemble and policy estimation can be found [here](https://www.dropbox.com/s/jlc6dtxo580lpl2/maddpg_ensemble_and_approx_code.zip?dl=0). The code is provided as-is. \n\n## Installation\n\n- To install, `cd` into the root directory and type `pip install -e .`\n\n- Known dependencies: Python (3.5.4), OpenAI gym (0.10.5), tensorflow (1.8.0), numpy (1.14.5)\n\n## Case study: Multi-Agent Particle Environments\n\nWe demonstrate here how the code can be used in conjunction with the\n[Multi-Agent Particle Environments (MPE)](https://github.com/openai/multiagent-particle-envs).\n\n- Download and install the MPE code [here](https://github.com/openai/multiagent-particle-envs)\nby following the `README`.\n\n- Ensure that `multiagent-particle-envs` has been added to your `PYTHONPATH` (e.g. in `~/.bashrc` or `~/.bash_profile`).\n\n- To run the code, `cd` into the `experiments` directory and run `train.py`:\n\n``python train.py --scenario simple``\n\n- You can replace `simple` with any environment in the MPE you'd like to run.\n\n## Command-line options\n\n### Environment options\n\n- `--scenario`: defines which environment in the MPE is to be used (default: `\"simple\"`)\n\n- `--max-episode-len` maximum length of each episode for the environment (default: `25`)\n\n- `--num-episodes` total number of training episodes (default: `60000`)\n\n- `--num-adversaries`: number of adversaries in the environment (default: `0`)\n\n- `--good-policy`: algorithm used for the 'good' (non adversary) policies in the environment\n(default: `\"maddpg\"`; options: {`\"maddpg\"`, `\"ddpg\"`})\n\n- `--adv-policy`: algorithm used for the adversary policies in the environment\n(default: `\"maddpg\"`; options: {`\"maddpg\"`, `\"ddpg\"`})\n\n### Core training parameters\n\n- `--lr`: learning rate (default: `1e-2`)\n\n- `--gamma`: discount factor (default: `0.95`)\n\n- `--batch-size`: batch size (default: `1024`)\n\n- `--num-units`: number of units in the MLP (default: `64`)\n\n### Checkpointing\n\n- `--exp-name`: name of the experiment, used as the file name to save all results (default: `None`)\n\n- `--save-dir`: directory where intermediate training results and model will be saved (default: `\"/tmp/policy/\"`)\n\n- `--save-rate`: model is saved every time this number of episodes has been completed (default: `1000`)\n\n- `--load-dir`: directory where training state and model are loaded from (default: `\"\"`)\n\n### Evaluation\n\n- `--restore`: restores previous training state stored in `load-dir` (or in `save-dir` if no `load-dir`\nhas been provided), and continues training (default: `False`)\n\n- `--display`: displays to the screen the trained policy stored in `load-dir` (or in `save-dir` if no `load-dir`\nhas been provided), but does not continue training (default: `False`)\n\n- `--benchmark`: runs benchmarking evaluations on saved policy, saves results to `benchmark-dir` folder (default: `False`)\n\n- `--benchmark-iters`: number of iterations to run benchmarking for (default: `100000`)\n\n- `--benchmark-dir`: directory where benchmarking data is saved (default: `\"./benchmark_files/\"`)\n\n- `--plots-dir`: directory where training curves are saved (default: `\"./learning_curves/\"`)\n\n## Code structure\n\n- `./experiments/train.py`: contains code for training MADDPG on the MPE\n\n- `./maddpg/trainer/maddpg.py`: core code for the MADDPG algorithm\n\n- `./maddpg/trainer/replay_buffer.py`: replay buffer code for MADDPG\n\n- `./maddpg/common/distributions.py`: useful distributions used in `maddpg.py`\n\n- `./maddpg/common/tf_util.py`: useful tensorflow functions used in `maddpg.py`\n\n\n\n## Paper citation\n\nIf you used this code for your experiments or found it helpful, consider citing the following paper:\n\n<pre>\n@article{lowe2017multi,\n  title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},\n  author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},\n  journal={Neural Information Processing Systems (NIPS)},\n  year={2017}\n}\n</pre>\n", "release_dates": []}, {"name": "miniF2F", "description": "Formal to Formal Mathematics Benchmark", "language": "Objective-C++", "license": null, "readme": "# MiniF2F\n\nMiniF2F is a formal mathematics benchmark (translated across multiple formal systems) consisting of\nexercise statements from olympiads (AMC, AIME, IMO) as well as high-school and undergraduate maths\nclasses.\n\nThe goal of the project is to provide a shared benchmark to evaluate and directly compare automated\ntheorem proving systems based on the formal systems targeted, initially **Lean**, and **Metamath**\n(targeting also **Hol Light** and\n**Isabelle**).\n\nThe benchmark (released under permissive licenses (MIT for Metamath, Apache for Lean)) is a work in\nprogress and contributions are welcome and encouraged through pull requests.\n\n## Citation\n\nThe benchmark is described in detail in the following [pre-print](https://arxiv.org/abs/2109.00110):\n```\n@article{zheng2021minif2f,\n  title={MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics},\n  author={Zheng, Kunhao and Han, Jesse Michael and Polu, Stanislas},\n  journal={arXiv preprint arXiv:2109.00110},\n  year={2021}\n}\n```\n\n## Statistics\n\n|           | Test | Valid |\n|:---------:|:----:|:-----:|\n|   Lean    |  244 |  244  |\n| Metamath  |  244 |  244  |\n| Isabelle  |  244 |  244  |\n| Hol Light |  165 |  165  |\n\n## Structure\n\nEach problem is represented by a unique name and a file for each of the formal systems we target.\nEach file consists at minima in the problem statement and optionally one or more example proofs\nassociated with it. The benchmark is divided in two splits:\n\n- `valid`: validation set that can be used while designing automated theorem proving systems\n  (early-stopping, reinforcement learning, data-augmentation, curriculum design, ...).\n- `test`: held-out test set reserved for final evaluation.\n\nNaming conventions are still a work in progress. Olympiads problems are generally named after their\ncompetition year and problem number (eg. `imo-1990-p3` or `aime-1983-p2`). Problems coming from a\nparticular dataset (eg the [MATH](https://arxiv.org/abs/2103.03874) dataset) are named to ease their\nretrieval (eg. `mathd-algebra-125`). Other problems are prefixed by a category hint and a unique\nname in the style of Metamath naming conventions (eg. `induction-11div10tonmn1ton`).\n\nEach exercise file complies to the following system-specific conventions.\n\n### Lean\n\nTo install the project make sure you have [elan](https://github.com/leanprover/elan) installed,\nthen in the directory where you want the project installed run:\n\n```\ngit clone https://github.com/openai/miniF2F\ncd miniF2F\nleanpkg configure\nleanpkg build\n```\n\nSince having one file per statement causes slowness in Lean parsing stage, all Lean statements are\nexceptionally aggregated in two files (`valid.lean` and `test.lean`). These files contain a list of\nthe problem statements defined as `theorem`s. Optionally, proofs for these statements are provided\nas well as potential lemmas to support the ground-truth proof.\n\nNo `theorem` should appear that do not correspond to a problem statement; use `lemma` instead.\n\nPlease use `lean/scripts/lint_style.py` to check all the statements pass the linter. You can also\nmake use of `lean/scripts/simple_formatter.sh` to enforce a few basic formatting rules.\n\nThe `lean` folder is released under the Apache License (so that it is aligned with Lean's mathlib\nlicense).\n\n### Metamath\n\nEach file contains the problem statement with the same name as the problem unique name. The\nstatement is commented (using Metamath convention) if provided without proof.\n\nThe `metamath` folder is released under the MIT License.\n\n### HOL Light\n\nEach file contains the problem statement defined as a HOL Light term\nwhose name must match the file name.\n\nThe `hollight` folder is released under the FreeBSD License.\n\n### Isabelle\n\nEach file contains the problem statement defined as a theorem\nwhose name must match the file name, optionally with a proof for it as well as the necessary\nimports.\n\nThe `isabelle` folder is released under the Apache License.\n\n## Code of Conduct and Contributions\n\nMiniF2F is meant to serve as a shared and useful resource for the machine learning community working\non formal mathematics. \n\nThere is no obligation tied with the use and reporting of a result based on miniF2F. But if you're\nusing it and discovering new proofs (manually or automatically) please contribute them back to the\nbenchmark.\n\nAll contributions, such as new statements for later versions, addition of missing statements for\nexisting versions, bug fixes, additional proofs are all welcome.\n\n## Versioning\n\nA version of miniF2F is defined by a frozen set of statements. The goal for each version is to get\nfull coverage on all formal systems for that version even if that might not be the case when the\nversion is frozen.\n\nWhen reporting a result based on miniF2F please always specify the version you used. The current\nversion is `v1`, frozen as of August 2021, including 244 statements (fully translated to Lean and\nMetamath but still WIP in other formal systems).\n\nEach version will live in its own branch to allow later additions of translated statements or fixes\nto existing statements as needed. The `main` branch remains reserved for active development and\nshould not be used when reporting results.\n\n### Active version\n\n- Version: `v1`\n- Freeze date: August 2021\n- Branch: [v1](https://github.com/openai/miniF2F/tree/v1)\n", "release_dates": []}, {"name": "mitmproxy", "description": "An interactive TLS-capable intercepting HTTP proxy for penetration testers and software developers", "language": "JavaScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": null, "release_dates": []}, {"name": "ml-agents", "description": "Unity Machine Learning Agents Toolkit", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<img src=\"docs/images/unity-wide.png\" align=\"middle\" width=\"3000\"/>\n\n<img src=\"docs/images/image-banner.png\" align=\"middle\" width=\"3000\"/>\n\n# Unity ML-Agents Toolkit (Beta)\n[![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)](docs/Readme.md)\n[![license badge](https://img.shields.io/badge/license-Apache--2.0-green.svg)](LICENSE)\n\n**The Unity Machine Learning Agents Toolkit** (ML-Agents) is an open-source\nUnity plugin that enables games and simulations to serve as environments for\ntraining intelligent agents. Agents can be trained using reinforcement learning,\nimitation learning, neuroevolution, or other machine learning methods through a\nsimple-to-use Python API. We also provide implementations (based on TensorFlow)\nof state-of-the-art algorithms to enable game developers and hobbyists to easily\ntrain intelligent agents for 2D, 3D and VR/AR games. These trained agents can be\nused for multiple purposes, including controlling NPC behavior (in a variety of\nsettings such as multi-agent and adversarial), automated testing of game builds\nand evaluating different game design decisions pre-release. The ML-Agents\ntoolkit is mutually beneficial for both game developers and AI researchers as it\nprovides a central platform where advances in AI can be evaluated on Unity\u2019s\nrich environments and then made accessible to the wider research and game\ndeveloper communities.\n\n## Features\n\n* Unity environment control from Python\n* 10+ sample Unity environments\n* Two deep reinforcement learning algorithms, [Proximal Policy Optimization](docs/Training-PPO.md) (PPO) and [Soft Actor-Critic](docs/Training-SAC.md) (SAC)\n* Support for multiple environment configurations and training scenarios\n* Train memory-enhanced agents using deep reinforcement learning\n* Easily definable Curriculum Learning and Generalization scenarios\n* Broadcasting of agent behavior for supervised learning\n* Built-in support for Imitation Learning\n* Flexible agent control with On Demand Decision Making\n* Visualizing network outputs within the environment\n* Simplified set-up with Docker\n* Wrap learning environments as a gym\n* Utilizes the Unity Inference Engine\n* Train using concurrent Unity environment instances\n\n## Documentation\n\n* For more information, in addition to installation and usage instructions, see\n  our [documentation home](docs/Readme.md).\n* If you are a researcher interested in a discussion of Unity as an AI platform, see a pre-print of our [reference paper on Unity and the ML-Agents Toolkit](https://arxiv.org/abs/1809.02627). Also, see below for instructions on citing this paper.\n* If you have used an earlier version of the ML-Agents toolkit, we strongly\n  recommend our [guide on migrating from earlier versions](docs/Migrating.md).\n\n## Additional Resources\n\nWe have published a series of blog posts that are relevant for ML-Agents:\n\n* Overviewing reinforcement learning concepts\n  ([multi-armed bandit](https://blogs.unity3d.com/2017/06/26/unity-ai-themed-blog-entries/)\n  and\n  [Q-learning](https://blogs.unity3d.com/2017/08/22/unity-ai-reinforcement-learning-with-q-learning/))\n* [Using Machine Learning Agents in a real game: a beginner\u2019s guide](https://blogs.unity3d.com/2017/12/11/using-machine-learning-agents-in-a-real-game-a-beginners-guide/)\n* [Post](https://blogs.unity3d.com/2018/02/28/introducing-the-winners-of-the-first-ml-agents-challenge/)\n  announcing the winners of our\n  [first ML-Agents Challenge](https://connect.unity.com/challenges/ml-agents-1)\n* [Post](https://blogs.unity3d.com/2018/01/23/designing-safer-cities-through-simulations/)\n  overviewing how Unity can be leveraged as a simulator to design safer cities.\n\nIn addition to our own documentation, here are some additional, relevant articles:\n\n* [Unity AI - Unity 3D Artificial Intelligence](https://www.youtube.com/watch?v=bqsfkGbBU6k)\n* [A Game Developer Learns Machine Learning](https://mikecann.co.uk/machine-learning/a-game-developer-learns-machine-learning-intent/)\n* [Explore Unity Technologies ML-Agents Exclusively on Intel Architecture](https://software.intel.com/en-us/articles/explore-unity-technologies-ml-agents-exclusively-on-intel-architecture)\n\n## Community and Feedback\n\nThe ML-Agents toolkit is an open-source project and we encourage and welcome\ncontributions. If you wish to contribute, be sure to review our\n[contribution guidelines](CONTRIBUTING.md) and\n[code of conduct](CODE_OF_CONDUCT.md).\n\nIf you run into any problems using the ML-Agents toolkit,\n[submit an issue](https://github.com/Unity-Technologies/ml-agents/issues) and\nmake sure to include as much detail as possible.\n\nYour opinion matters a great deal to us. Only by hearing your thoughts on the Unity ML-Agents Toolkit can we continue to improve and grow. Please take a few minutes to [let us know about it](https://github.com/Unity-Technologies/ml-agents/issues/1454).\n\n\nFor any other questions or feedback, connect directly with the ML-Agents\nteam at ml-agents@unity3d.com.\n\n## Translations\n\nTo make the Unity ML-Agents toolkit accessible to the global research and\nUnity developer communities, we're attempting to create and maintain\ntranslations of our documentation. We've started with translating a subset\nof the documentation to one language (Chinese), but we hope to continue\ntranslating more pages and to other languages. Consequently,\nwe welcome any enhancements and improvements from the community.\n\n* [Chinese](docs/localized/zh-CN/)\n* [Korean](docs/localized/KR/)\n\n## License\n\n[Apache License 2.0](LICENSE)\n\n## Citation\n\nIf you use Unity or the ML-Agents Toolkit to conduct research, we ask that you cite the following paper as a reference:\n\nJuliani, A., Berges, V., Vckay, E., Gao, Y., Henry, H., Mattar, M., Lange, D. (2018). Unity: A General Platform for Intelligent Agents. *arXiv preprint arXiv:1809.02627.* https://github.com/Unity-Technologies/ml-agents.\n", "release_dates": []}, {"name": "mlsh", "description": " Code for the paper \"Meta-Learning Shared Hierarchies\"", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Meta-Learning Shared Hierarchies\n\nCode for [Meta-Learning Shared Hierarchies](https://s3-us-west-2.amazonaws.com/openai-assets/MLSH/mlsh_paper.pdf).\n\n\n##### Installation\n\n```\nAdd to your .bash_profile (replace ... with path to directory):\nexport PYTHONPATH=$PYTHONPATH:/.../mlsh/gym;\nexport PYTHONPATH=$PYTHONPATH:/.../mlsh/rl-algs;\n\nInstall MovementBandits environments:\ncd test_envs\npip install -e .\n```\n\n##### Running Experiments\n```\npython main.py --task AntBandits-v1 --num_subs 2 --macro_duration 1000 --num_rollouts 2000 --warmup_time 20 --train_time 30 --replay False AntAgent\n\n```\nOnce you've trained your agent, view it by running:\n```\npython main.py [...] --replay True --continue_iter [your iteration] AntAgent\n```\nThe MLSH script works on any Gym environment that implements the randomizeCorrect() function. See the envs/ folder for examples of such environments.\n\nTo run on multiple cores:\n```\nmpirun -np 12 python main.py ...\n```\n", "release_dates": []}, {"name": "moderation-api-release", "description": null, "language": null, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "## Evaluation dataset for the paper \"A Holistic Approach to Undesired Content Detection\"\n\nThe evaluation dataset `data/samples-1680.jsonl.gz` is the test set used in [the following paper](https://arxiv.org/abs/2208.03274):\n\n```\n@article{openai2022moderation,\n  title={A Holistic Approach to Undesired Content Detection},\n  author={Todor Markov and Chong Zhang and Sandhini Agarwal and Tyna Eloundou and Teddy Lee and Steven Adler and Angela Jiang and Lilian Weng},\n  journal={arXiv preprint arXiv:2208.03274},\n  year={2022}\n}\n```\n\nEach line contains information about one sample in a JSON object and each sample is labeled according to our taxonomy. The category label is a binary flag, but if it does not include in the JSON, it means we do not know the label.\n\n| Category | Label | Definition |\n| -------- | ----- | ---------- |\n| sexual   | `S`   | Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness). |\n| hate     | `H`   | Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. |\n| violence | `V`   | Content that promotes or glorifies violence or celebrates the suffering or humiliation of others. |\n| harassment       | `HR`   | Content that may be used to torment or annoy individuals in real life, or make harassment more likely to occur. |\n| self-harm        | `SH`   | Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders. |\n| sexual/minors    | `S3`   | Sexual content that includes an individual who is under 18 years old. |\n| hate/threatening | `H2`   | Hateful content that also includes violence or serious harm towards the targeted group. |\n| violence/graphic | `V2`   | Violent content that depicts death, violence, or serious physical injury in extreme graphic detail. |\n\n", "release_dates": []}, {"name": "monorepo-diff-buildkite-plugin", "description": "Run separate pipelines for each folder in your monorepo", "language": null, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# monorepo-diff-buildkite-plugin\n\nThis plugin will assist you in triggering pipelines by watching folders in your `monorepo`.\n\n## Example\n\n### Simple\n\n```yml\nsteps:\n  - label: \"Triggering pipelines\"\n    plugins:\n      - chronotc/monorepo-diff#v1.1.1:\n          diff: \"git diff --name-only HEAD~1\"\n          watch:\n            - path: \"foo-service/\"\n              config:\n                trigger: \"deploy-foo-service\"\n```\n\n### Detailed\n\n```yml\nsteps:\n  - label: \"Triggering pipelines\"\n    plugins:\n      - chronotc/monorepo-diff#v1.1.1:\n          diff: \"git diff --name-only $(head -n 1 last_successful_build)\"\n          watch:\n            - path: \"foo-service/\"\n              config:\n                trigger: \"deploy-foo-service\"\n                build:\n                  message: \"Deploying foo service\"\n                  env:\n                    - HELLO=123\n                    - AWS_REGION\n            - path:\n                - \"ops/terraform/\"\n                - \"ops/templates/terraform/\"\n              config:\n                trigger: \"provision-terraform-resources\"\n                async: true\n          wait: true\n          hooks:\n            - command: \"echo $(git rev-parse HEAD) > last_successful_build\"\n```\n\n## Configuration\n\n### `diff` (optional)\n\nThis will run the script provided to determine the folder changes.\nDepending on your use case, you may want to determine the point where the branch occurs\nhttps://stackoverflow.com/questions/1527234/finding-a-branch-point-with-git and perform a diff against the branch point.\n\n#### Sample output:\n```\nREADME.md\nlib/trigger.bash\ntests/trigger.bats\n```\n\nDefault: `git diff --name-only HEAD~1`\n\n#### Examples:\n\n`diff: ./diff-against-last-successful-build.sh`\n\n```bash\n#!/bin/bash\n\nset -ueo pipefail\n\nLAST_SUCCESSFUL_BUILD_COMMIT=\"$(aws s3 cp \"${S3_LAST_SUCCESSFUL_BUILD_COMMIT_PATH}\" - | head -n 1)\"\ngit diff --name-only \"$LAST_SUCCESSFUL_BUILD_COMMIT\"\n```\n\n`diff: ./diff-against-last-built-tag.sh`\n\n```bash\n#!/bin/bash\n\nset -ueo pipefail\n\nLATEST_BUILT_TAG=$(git describe --tags --match foo-service-* --abbrev=0)\ngit diff --name-only \"$LATEST_TAG\"\n```\n\n### `watch`\n\nDeclare a list of\n\n```yaml\n- path: app/cms/\n  config: # Required [trigger step configuration]\n    trigger: cms-deploy # Required [trigger pipeline slug]\n- path:\n    - services/email\n    - assets/images/email\n  config:\n    trigger: email-deploy\n```\n\n#### `path`\n\nIf the `path` specified here in the appears in the `diff` output, a `trigger` step will be added to the dynamically generated pipeline.yml\n\nA list of paths can be provided to trigger the desired pipeline. Changes in any of the paths will initiate the pipeline provided in trigger.\n\n#### `config`\n\nThe configuration for the `trigger` step https://buildkite.com/docs/pipelines/trigger-step\n\nBy default, it will pass the following values to the `build` attributes unless an alternative values are provided\n\n```yaml\n- path: app/cms/\n  config:\n    trigger: cms-deploy\n    build:\n      commit: $BUILDKITE_COMMIT\n      branch: $BUILDKITE_BRANCH\n```\n\n### `wait` (optional)\n\nBy setting `wait` to `true`, the build will wait until the triggered pipeline builds are successful before proceeding\n\n### `hooks` (optional)\n\nCurrently supports a list of `commands` you wish to execute after the `watched` pipelines have been triggered\n\n```yaml\nhooks:\n  - command: upload unit tests reports\n  - command: echo success\n\n```\n\n## Environment\n\n### `DEBUG` (optional)\n\nBy turning `DEBUG` on, the generated pipeline will be displayed prior to upload\n\n```yaml\nsteps:\n  - label: \"Triggering pipelines\"\n    env:\n      DEBUG: true\n    plugins:\n      - chronotc/monorepo-diff:\n          diff: \"git diff --name-only HEAD~1\"\n          watch:\n            - path: \"foo-service/\"\n              config:\n                trigger: \"deploy-foo-service\"\n```\n\n## References\n\nhttps://stackoverflow.com/questions/1527234/finding-a-branch-point-with-git\n\n## Contribute\n\n### To run tests\n\nEnsure that all tests are in the `./tests`\n\n`docker-compose run --rm tests`\n\n### To run lint\n\n`docker-compose run --rm lint`\n", "release_dates": []}, {"name": "mujoco-py", "description": "MuJoCo is a physics engine for detailed, efficient rigid body simulations with contacts. mujoco-py allows using MuJoCo from Python 3.", "language": "Cython", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "**Status**: Deprecated\n\n### `mujoco-py` does not support versions of MuJoCo after 2.1.0.\n\n### New users should depend on the [official MuJoCo Python bindings](https://github.com/deepmind/mujoco/blob/main/python/README.md).\n\n# mujoco-py [![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://openai.github.io/mujoco-py/build/html/index.html) [![Build Status](https://travis-ci.org/openai/mujoco-py.svg?branch=master)](https://travis-ci.org/openai/mujoco-py)\n\n[MuJoCo](http://mujoco.org/) is a physics engine for detailed, efficient rigid body simulations with contacts.\n`mujoco-py` allows using MuJoCo from Python 3.\n\nThis library has been updated to be compatible with MuJoCo version 2.1 released on 2021-10-18.\n\n\n## Synopsis\n\n### Requirements\n\nThe following platforms are currently supported:\n\n- Linux with Python 3.6+. See [the `Dockerfile`](Dockerfile) for the canonical list of system dependencies.\n- OS X with Python 3.6+.\n\nThe following platforms are DEPRECATED and unsupported:\n\n- Windows support has been DEPRECATED and removed in [2.0.2.0](https://github.com/openai/mujoco-py/releases/tag/v2.0.2.0a1). One known good past version is [1.50.1.68](https://github.com/openai/mujoco-py/blob/9ea9bb000d6b8551b99f9aa440862e0c7f7b4191/README.md#requirements).\n- Python 2 has been DEPRECATED and removed in [1.50.1.0](https://github.com/openai/mujoco-py/releases/tag/1.50.1.0). Python 2 users can stay on the [`0.5` branch](https://github.com/openai/mujoco-py/tree/0.5). The latest release there is [`0.5.7`](https://github.com/openai/mujoco-py/releases/tag/0.5.7) which can be installed with `pip install mujoco-py==0.5.7`.\n\n### Install MuJoCo\n\n1. Download the MuJoCo version 2.1 binaries for\n   [Linux](https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz) or\n   [OSX](https://mujoco.org/download/mujoco210-macos-x86_64.tar.gz).\n1. Extract the downloaded `mujoco210` directory into `~/.mujoco/mujoco210`.\n\nIf you want to specify a nonstandard location for the package,\nuse the env variable `MUJOCO_PY_MUJOCO_PATH`.\n\n### Install and use `mujoco-py`\nTo include `mujoco-py` in your own package, add it to your requirements like so:\n```\nmujoco-py<2.2,>=2.1\n```\nTo play with `mujoco-py` interactively, follow these steps:\n```\n$ pip3 install -U 'mujoco-py<2.2,>=2.1'\n$ python3\nimport mujoco_py\nimport os\nmj_path = mujoco_py.utils.discover_mujoco()\nxml_path = os.path.join(mj_path, 'model', 'humanoid.xml')\nmodel = mujoco_py.load_model_from_path(xml_path)\nsim = mujoco_py.MjSim(model)\n\nprint(sim.data.qpos)\n# [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\nsim.step()\nprint(sim.data.qpos)\n# [-2.09531783e-19  2.72130735e-05  6.14480786e-22 -3.45474715e-06\n#   7.42993721e-06 -1.40711141e-04 -3.04253586e-04 -2.07559344e-04\n#   8.50646247e-05 -3.45474715e-06  7.42993721e-06 -1.40711141e-04\n#  -3.04253586e-04 -2.07559344e-04 -8.50646247e-05  1.11317030e-04\n#  -7.03465386e-05 -2.22862221e-05 -1.11317030e-04  7.03465386e-05\n#  -2.22862221e-05]\n```\n\nSee the [full documentation](https://openai.github.io/mujoco-py/build/html/index.html) for advanced usage.\n\n## Troubleshooting\n\n### You're on MacOS and you see `clang: error: unsupported option '-fopenmp'`\n\nIf this happend during installation or just running `python -c \"import mujoco_py\"` then the issue seems to be related to [this](https://github.com/velocyto-team/velocyto.R/issues/2#issuecomment-341165967) and the TL;DR is that for macOS the default compiler Apple clang LLVM does not support openmp. So you can try to install another clang/llvm installation. For example (requires [brew](https://brew.sh/)):\n\n```bash\nbrew install llvm\nbrew install boost\nbrew install hdf5\n\n# Add this to your .bashrc/.zshrc:\nexport PATH=\"/usr/local/opt/llvm/bin:$PATH\"\n\nexport CC=\"/usr/local/opt/llvm/bin/clang\"\nexport CXX=\"/usr/local/opt/llvm/bin/clang++\"\nexport CXX11=\"/usr/local/opt/llvm/bin/clang++\"\nexport CXX14=\"/usr/local/opt/llvm/bin/clang++\"\nexport CXX17=\"/usr/local/opt/llvm/bin/clang++\"\nexport CXX1X=\"/usr/local/opt/llvm/bin/clang++\"\n\nexport LDFLAGS=\"-L/usr/local/opt/llvm/lib\"\nexport CPPFLAGS=\"-I/usr/local/opt/llvm/include\"\n```\n\n**Note:** Don't forget to source your `.bashrc/.zshrc` after editing it and try to install `mujoco-py` again:\n\n```bash\n# Make sure your python environment is activated\npip install -U 'mujoco-py<2.2,>=2.1'\n```\n\n### Missing GLFW\n\nA common error when installing is:\n\n    raise ImportError(\"Failed to load GLFW3 shared library.\")\n\nWhich happens when the `glfw` python package fails to find a GLFW dynamic library.\n\nMuJoCo ships with its own copy of this library, which can be used during installation.\n\nAdd the path to the mujoco bin directory to your dynamic loader:\n\n    LD_LIBRARY_PATH=$HOME/.mujoco/mujoco210/bin pip install mujoco-py\n\nThis is particularly useful on Ubuntu 14.04, which does not have a GLFW package.\n\n\n### Ubuntu installtion troubleshooting\n\nBecause `mujoco_py` has compiled native code that needs to be linked to a supplied MuJoCo binary, it's installation\non linux can be more challenging than pure Python source packages.\n\nTo install mujoco-py on Ubuntu, make sure you have the following libraries installed:\n\n    sudo apt install libosmesa6-dev libgl1-mesa-glx libglfw3\n\nIf you installed above libraries and you still see an error that `-lGL` cannot be found, most likely you need\nto create the symbolic link directly:\n\n    sudo ln -s /usr/lib/x86_64-linux-gnu/libGL.so.1 /usr/lib/x86_64-linux-gnu/libGL.so\n\n\n## Usage Examples\n\nA number of examples demonstrating some advanced features of `mujoco-py` can be found in [`examples/`](/./examples/). These include:\n- [`body_interaction.py`](./examples/body_interaction.py): shows interactions between colliding bodies\n- [`disco_fetch.py`](./examples/disco_fetch.py): shows how `TextureModder` can be used to randomize object textures\n- [`internal_functions.py`](./examples/internal_functions.py): shows how to call raw mujoco functions like `mjv_room2model`\n- [`markers_demo.py`](./examples/markers_demo.py): shows how to add visualization-only geoms to the viewer\n- [`serialize_model.py`](./examples/serialize_model.py): shows how to save and restore a model\n- [`setting_state.py`](./examples/setting_state.py):  shows how to reset the simulation to a given state\n- [`tosser.py`](./examples/tosser.py): shows a simple actuated object sorting robot application\n\nSee the [full documentation](https://openai.github.io/mujoco-py/build/html/index.html) for advanced usage.\n\n## Development\n\nTo run the provided unit and integrations tests:\n\n```\nmake test\n```\n\nTo test GPU-backed rendering, run:\n\n```\nmake test_gpu\n```\n\nThis is somewhat dependent on internal OpenAI infrastructure at the moment, but it should run if you change the `Makefile` parameters for your own setup.\n\n## Changelog\n\n- 03/08/2018: We removed MjSimPool, because most of benefit one can get with multiple processes having single simulation.\n\n## Credits\n\n`mujoco-py` is maintained by the OpenAI Robotics team. Contributors include:\n\n- Alex Ray\n- Bob McGrew\n- Jonas Schneider\n- Jonathan Ho\n- Peter Welinder\n- Wojciech Zaremba\n- Jerry Tworek\n", "release_dates": ["2021-11-29T21:01:08Z", "2019-08-12T23:44:24Z", "2019-04-16T00:53:21Z", "2019-04-15T20:14:49Z", "2019-03-22T20:41:47Z", "2019-03-19T00:28:52Z", "2017-06-28T06:05:12Z", "2017-06-28T05:47:05Z"]}, {"name": "mujoco-worldgen", "description": "Automatic object XML generation for Mujoco", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Worldgen: Randomized MuJoCo environments\n\nWorldgen allows users to generate complex, heavily randomized environments environments. Examples of such environments can be found in the `examples` folder.\n\nActions in `action_space` are all actuators of objects added during world building. Not all objects will have actuators, but some do (e.g. `ObjFromXML('particle')` and `ObjFromXML('particle_hinge')`). You can examine the meaning of a given action by looking at the xml file of the object in `assets/xmls/.../main.xml`\n\nObservation spaces are inferred based on the outputs of the `get_obs` function in an `Env` object (`Env` class is defined in `mujoco_worldgen/env.py`.\n\n## Installation\n\nThis repository requires the MuJoCo physics engine (the repository has been tested with MuJoCo 1.50). To install MuJoCo, follow the instructions in the [mujoco-py](https://github.com/openai/mujoco-py/tree/1.50.1.0) repository.\n\n```\n    pip install -r requirements.txt\n    pip install -e .\n```\n\nThis repository has been used on Mac OS X and Ubuntu 16.04 with Python 3.6\n\n## Walkthrough\n\n### Initial steps\n\nLet\u2019s analyze an example of one such generation.  First we create the `WorldParams`:\n\n```\n    world_params = WorldParams(size=(5, 5, 3.5))\n```\n\nThis defines the global properties of our generated world.\nMetric units used in worldgen are meters, kilograms, and angles are given in radians: `[-pi, pi]`. \nThe following are all optional paramaters with defaults:\n\n-  `size`: The size of the space available for placing objects. (Default: `(10., 10., 2.5)`)\n-  `num_substeps`: The number of physics substeps to perform within every call to `step()`. (Default: `1`)\n-  `randomize_light`: Whether to randomize the lighting conditions. (Default: `False`)\n-  `randomize_material`: Whether to randomize the materials that are applied to objects. (Default: `False`)\n-  `placement_margin`: The minimum distance between placed objects. (Default: `0.0`)\n-  `show_outer_bounds`: Whether to visualize the outer bounds. (Default: `False`)\n\nThen, we create a builder. The `WorldBuilder` constructor takes a `WorldParams` object and `seed`, which is used for randomization:\n\n```\n\tbuilder = WorldBuilder(world_params, seed)\n```\n\n### Placing objects\n\nHere's an example of adding geometries to our world:\n\n```\n    # Create a floor\n    floor = Floor()\n\n    # Load geometries from XML, and add to floor\n    robot = ObjFromXML(\"particle\")\n    floor.append(robot)\n    sphere = ObjFromXML(\"sphere\")\n    floor.append(sphere)\n\n    # Create a primitive geometry, and add to floor\n    box = Geom('box')\n    floor.append(box)\n\n    # Add the root floor to the builder\n    builder.append(floor)\n```\n\nThe `append()` function allows to specify a placement indicator (Usually \u201ctop\u201d or \u201cinside\u201d).\nPlacements are spaces we\u2019re able to place objects. \nPlacements are always world-aligned rectangular prisms, which objects can be oriented within.  All objects within a placement align along the bottom (and are positioned with X,Y coordinates).\nIf there are multiple placements for a given name (e.g. \u201cinside\\_0\u201d, \u201cinside\\_1\u201d, \u2026) then we choose one at random.\nThe default placement name is \u201ctop\u201d. \n\nTo customize the placement position:\n\n```\n    obj.append(child_obj, placement_name=\"top\", placement_xy=None)\n```\n\n`placement_xy` is None if you want the world generation algorithm to randomly place it.\nOtherwise it is an X, Y pair where both are within `[0.0, 1.0]`.\nThe object will be placed within the bounds, scaled by its size.\n*Note:* this is because the size of the placement (e.g. table) might not\nbe known until generation time. To add some more objects:\n\n```\n    obj.append(child_obj, 'top', (0.5, 1))  # placed in the center of the back\n    obj.append(child_obj, 'top', (0.5, 0.5))  # placed in the center\n    obj.append(child_obj, 'top', (0, 0))  # placed in the lower left corner\n```\n\n### Geoms\n\nThere are several kinds of primitive geoms: \u201cbox\u201d, \u201csphere\u201d, \u201ccylinder\u201d.\nWe can specify the size of a geom by providing a second parameter: `Geom(\"box\", (0.1, 0.2, 0.3))` which would result in box of size 10cm x 20cm x 30cm.\nFor a cube: `Geom(\"box\", 0.25)` results in \u201cbox\u201d of size 25cm x 25cm x 25cm.\nMoreover, we can provide a range to sample a random size box: `Geom(\"box\", (0.1, 0.2, 0.3), (1.1, 1.2, 1.3))`.\nSize of this box would be random between 10cm x 20cm x 30cm and 1.1m x 1.2m x 1.3m.\n\n### Sites\nYou can create sites - static markers on an object - by calling `obj.mark()`. The current position of all created sites is stored in the `sim` object of the worldgen environment. For more information on sites, check out the `mark` function in the `Obj` class defined in `mujoco_worldgen/objs/obj.py`\n\nExample usage:\n```\n# Create a floor and a box\nfloor = Floor()\nbox = Geom('box')\n\n# Mark sites on floor and box\nfloor.mark(mark_name='floor_site', relative_xyz=(0.2, 0.2, 0))\nbox.mark(mark_name='box_site', relative_xyz=(0, 0, 0.5))\n\n# Add box to floor, add the floor to the builder, and create a sim with the builder\nfloor.append(box)\nbuilder.append(floor)\nsim = builder.get_sim()\n\n# Get the current position of the floor site and the box site\nfloor_site_pos = sim.data.site_xpos[sim.model.site_name2id('floor_site')]\nbox_site_pos = sim.data.site_xpos[sim.model.site_name2id('box_site')]\n```\n\n### Environments\n\nYou can create new environments with Worldgen by subclassing the `Env` class and defining the following methods:\n\n- `_get_sim`\n- `_get_obs`\n- `_get_reward` : You can also use a reward wrapper instead of defining this.\n\nYou can see two examples in the `examples` folder - the `simple_particle` environment and the `particle_gather` environment.\nYou can test out environments using the `/bin/examine` script, providing either a python script defining the `make_env` or a jsonnet file.\n\n```\n    ./bin/examine.py examples/simple_particle.py\n```\n\n```\n    ./bin/examine.py examples/example_env_examine.jsonnet\n```\n\n## Acknowledgements\n\nCredits to Alex Ray for writing the original version of Worldgen that OpenAI was using internally - the majority of the code in this repository has been copied over from the original Worldgen with minor changes.\n", "release_dates": []}, {"name": "multi-agent-emergence-environments", "description": "Environment generation code for the paper \"Emergent Tool Use From Multi-Agent Autocurricula\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Multiagent emergence environments\nEnvironment generation code for [Emergent Tool Use From Multi-Agent Autocurricula](https://arxiv.org/abs/1909.07528) ([blog](https://openai.com/blog/emergent-tool-use/))\n\n### Installation\nThis repository depends on the [mujoco-worldgen](https://github.com/openai/mujoco-worldgen) package. You will need to clone the mujoco-worldgen repository and install it and its dependencies:\n```\npip install -r mujoco-worldgen/requirements.txt\npip install -e mujoco-worldgen/\npip install -e multi-agent-emergence-environments/\n```\n\nThis repository has been tested only on Mac OS X and Ubuntu 16.04 with Python 3.6\n\n### Use\n\nEnvironment construction works in the following way: You start from the `Base` environment (defined in `mae_envs/envs/base.py`) and then you add environment modules (e.g. `Boxes`, `Ramps`, `RandomWalls`, etc.) and then wrappers on top. You can see examples in the `mae_envs/envs` folder.\n\nIf you want to construct a new environment, we highly recommend using the above paradigm in order to minimize code duplication. If you need new objects or game dynamics that don't already exist in this codebase, add them in via a new `EnvModule` class or a `gym.Wrapper` class rather than subclassing `Base` (or mujoco-worldgen's `Env` class). In general, `EnvModules` should be used for adding objects or sites to the environment, or otherwise modifying the mujoco simulator; wrappers should be used for everything else (e.g. adding rewards, additional observations, or implementing game mechanics like Lock and Grab).\n\nThe environments defined in this repository are: \\\n*Hide and seek* - `mae_envs/envs/hide_and_seek.py` - The Hide and Seek environment described in the paper. This encompasses the *random rooms*, *quadrant* and *food* versions of the game (you can switch between them by changing the arguments given to the `make_env` function in the file) \\\n*Box locking* - `mae_envs/envs/box_locking.py` - Encompasses the *Lock and Return* and *Sequential Lock* transfer tasks described in the paper. \\\n*Blueprint Construction* - `mae_envs/envs/blueprint_construction.py` \\\n*Shelter Construction* - `mae_envs/envs/shelter_construction.py`\n\nYou can test out environments by using the `bin/examine` script. Example usage: `bin/examine.py base`. \\\nYou can also use `bin/examine` to play a saved policy on an environment. There are several environment jsonnets and policies in the `examples` folder. Example usage:\n\n```bin/examine.py examples/hide_and_seek_quadrant.jsonnet examples/hide_and_seek_quadrant.npz``` \n\nNote that to be able to play saved policies, you will need to install a few additional packages. You can do this via\n\n`pip install -r multi-agent-emergence-environments/requirements_ma_policy.txt`\n", "release_dates": []}, {"name": "multiagent-competition", "description": " Code for the paper \"Emergent Complexity via Multi-agent Competition\"", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Competitive Multi-Agent Environments\n\nThis repository contains the environments for the paper [Emergent Complexity via Multi-agent Competition](https://arxiv.org/abs/1710.03748)\n\n## Dependencies\nUse `pip install -r requirements.txt` to install dependencies. If you haven't used MuJoCo before, please refer to the [installation guide](https://github.com/openai/mujoco-py).\nThe code has been tested with the following dependencies:\n* Python version 3.6\n* [OpenAI GYM](https://github.com/openai/gym) version 0.9.1 with MuJoCo 1.31 support (use [mujoco-py version 0.5.7](https://github.com/openai/mujoco-py/tree/0.5))\n* [Tensorflow](https://www.tensorflow.org/versions/r1.1/install/) version 1.1.0\n* [Numpy](https://scipy.org/install.html) version 1.12.1\n\n## Installing Package\nAfter installing all dependencies, make sure gym works with support for MuJoCo environments.\nNext install `gym-compete` package as:\n```bash\ncd gym-compete\npip install -e .\n```\nCheck install is successful by coming out of the directory and trying `import gym_compete` in python console. Some users might require a `sudo pip install`.\n\n## Trying the environments\nAgent policies are provided for the various environments in folder `agent-zoo`. To see a demo of all the environments do:\n```bash\nbash demo_tasks.sh all\n```\nTo instead try a single environment use:\n```bash\nbash demo_tasks.sh <task>\n```\nwhere `<task>` is one of: `run-to-goal-humans`, `run-to-goal-ants`, `you-shall-not-pass`, `sumo-ants`, `sumo-humans` and `kick-and-defend`\n", "release_dates": []}, {"name": "multiagent-particle-envs", "description": "Code for a multi-agent particle environment used in the paper \"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Maintained Fork\n\nThe maintained version of these environments, which includenumerous fixes, comprehensive documentation, support for installation via pip, and support for current versions of Python are available in PettingZoo (https://github.com/Farama-Foundation/PettingZoo , https://pettingzoo.farama.org/environments/mpe/)\n\n# Multi-Agent Particle Environment\n\nA simple multi-agent particle world with a continuous observation and discrete action space, along with some basic simulated physics.\nUsed in the paper [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf).\n\n## Getting started:\n\n- To install, `cd` into the root directory and type `pip install -e .`\n\n- To interactively view moving to landmark scenario (see others in ./scenarios/):\n`bin/interactive.py --scenario simple.py`\n\n- Known dependencies: Python (3.5.4), OpenAI gym (0.10.5), numpy (1.14.5), pyglet (1.5.27)\n\n- To use the environments, look at the code for importing them in `make_env.py`.\n\n## Code structure\n\n- `make_env.py`: contains code for importing a multiagent environment as an OpenAI Gym-like object.\n\n- `./multiagent/environment.py`: contains code for environment simulation (interaction physics, `_step()` function, etc.)\n\n- `./multiagent/core.py`: contains classes for various objects (Entities, Landmarks, Agents, etc.) that are used throughout the code.\n\n- `./multiagent/rendering.py`: used for displaying agent behaviors on the screen.\n\n- `./multiagent/policy.py`: contains code for interactive policy based on keyboard input.\n\n- `./multiagent/scenario.py`: contains base scenario object that is extended for all scenarios.\n\n- `./multiagent/scenarios/`: folder where various scenarios/ environments are stored. scenario code consists of several functions:\n    1) `make_world()`: creates all of the entities that inhabit the world (landmarks, agents, etc.), assigns their capabilities (whether they can communicate, or move, or both).\n     called once at the beginning of each training session\n    2) `reset_world()`: resets the world by assigning properties (position, color, etc.) to all entities in the world\n    called before every episode (including after make_world() before the first episode)\n    3) `reward()`: defines the reward function for a given agent\n    4) `observation()`: defines the observation space of a given agent\n    5) (optional) `benchmark_data()`: provides diagnostic data for policies trained on the environment (e.g. evaluation metrics)\n\n### Creating new environments\n\nYou can create new scenarios by implementing the first 4 functions above (`make_world()`, `reset_world()`, `reward()`, and `observation()`).\n\n## List of environments\n\n\n| Env name in code (name in paper) |  Communication? | Competitive? | Notes |\n| --- | --- | --- | --- |\n| `simple.py` | N | N | Single agent sees landmark position, rewarded based on how close it gets to landmark. Not a multiagent environment -- used for debugging policies. |\n| `simple_adversary.py` (Physical deception) | N | Y | 1 adversary (red), N good agents (green), N landmarks (usually N=2). All agents observe position of landmarks and other agents. One landmark is the \u2018target landmark\u2019 (colored green). Good agents rewarded based on how close one of them is to the target landmark, but negatively rewarded if the adversary is close to target landmark. Adversary is rewarded based on how close it is to the target, but it doesn\u2019t know which landmark is the target landmark. So good agents have to learn to \u2018split up\u2019 and cover all landmarks to deceive the adversary. |\n| `simple_crypto.py` (Covert communication) | Y | Y | Two good agents (alice and bob), one adversary (eve). Alice must sent a private message to bob over a public channel. Alice and bob are rewarded based on how well bob reconstructs the message, but negatively rewarded if eve can reconstruct the message. Alice and bob have a private key (randomly generated at beginning of each episode), which they must learn to use to encrypt the message. |\n| `simple_push.py` (Keep-away) | N |Y  | 1 agent, 1 adversary, 1 landmark. Agent is rewarded based on distance to landmark. Adversary is rewarded if it is close to the landmark, and if the agent is far from the landmark. So the adversary learns to push agent away from the landmark. |\n| `simple_reference.py` | Y | N | 2 agents, 3 landmarks of different colors. Each agent wants to get to their target landmark, which is known only by other agent. Reward is collective. So agents have to learn to communicate the goal of the other agent, and navigate to their landmark. This is the same as the simple_speaker_listener scenario where both agents are simultaneous speakers and listeners. |\n| `simple_speaker_listener.py` (Cooperative communication) | Y | N | Same as simple_reference, except one agent is the \u2018speaker\u2019 (gray) that does not move (observes goal of other agent), and other agent is the listener (cannot speak, but must navigate to correct landmark).|\n| `simple_spread.py` (Cooperative navigation) | N | N | N agents, N landmarks. Agents are rewarded based on how far any agent is from each landmark. Agents are penalized if they collide with other agents. So, agents have to learn to cover all the landmarks while avoiding collisions. |\n| `simple_tag.py` (Predator-prey) | N | Y | Predator-prey environment. Good agents (green) are faster and want to avoid being hit by adversaries (red). Adversaries are slower and want to hit good agents. Obstacles (large black circles) block the way. |\n| `simple_world_comm.py` | Y | Y | Environment seen in the video accompanying the paper. Same as simple_tag, except (1) there is food (small blue balls) that the good agents are rewarded for being near, (2) we now have \u2018forests\u2019 that hide agents inside from being seen from outside; (3) there is a \u2018leader adversary\u201d that can see the agents at all times, and can communicate with the other adversaries to help coordinate the chase. |\n\n## Paper citation\n\nIf you used this environment for your experiments or found it helpful, consider citing the following papers:\n\nEnvironments in this repo:\n<pre>\n@article{lowe2017multi,\n  title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},\n  author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},\n  journal={Neural Information Processing Systems (NIPS)},\n  year={2017}\n}\n</pre>\n\nOriginal particle world environment:\n<pre>\n@article{mordatch2017emergence,\n  title={Emergence of Grounded Compositional Language in Multi-Agent Populations},\n  author={Mordatch, Igor and Abbeel, Pieter},\n  journal={arXiv preprint arXiv:1703.04908},\n  year={2017}\n}\n</pre>\n", "release_dates": []}, {"name": "neural-gpu", "description": "Code for the Neural GPU model originally described in \"Neural GPUs Learn Algorithms\"", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\nCode for the Neural GPU model originally described in\n[[http://arxiv.org/abs/1511.08228]].\n\n\nRunning experiments\n===================\n\nRunning one instance\n--------------------\n\nThe following would use 256 filters to train on binary multiplication,\nthen 4-ary, then decimal:\n```\npython neural_gpu_trainer.py --nmaps=256 --task=bmul,qmul,mul --progressive_curriculum=5\n```\n\nMy typical invocation is something like\n\n```\n  CUDA_VISIBLE_DEVICES=0 python neural_gpu_trainer.py --random_seed=0 --max_steps=200000 --forward_max=201 --nmaps=256 --task=bmul,qmul,mul --time_till_eval=4 --progressive_curriculum=5 --train_dir=../logs/August-12-curriculum/forward_max=201-nmaps=256-task=bmul,qmul,mul-progressive_curriculum=5-random_seed=0\n```\n\nThe tests on decimal carry were done using invocations like the following:\n```\n  CUDA_VISIBLE_DEVICES=0 neural_gpu_trainer.py --train_dir=../logs/run1 --random_seed=1 --max_steps=100000 --forward_max=201 --nmaps=128 --task=add --time_till_eval=4 --time_till_ckpt=1\n```\n\nYou can find a list of options, and their default values, in `neuralgpu/trainer.py`.\n\nExamining results\n=================\n\nLoading and examining a model\n-----------------------------\n\n`examples/examples_for_loading_model.py` gives a simple instance of loading a\nmodel and running it on an instance.\n\nPlotting results\n----------------\n\nSomething like `python plots/get_pretty_score.py cachedlogs/*/*task=bmul,qmul,mul-*` works.  There are a lot of options to make it prettier (renaming stuff, removing some runs, changing titles, reordering, etc.).  For example, one of my plots was made with\n\n```\npython get_pretty_score.py cachedlogs/A*/*256*[=,]mul-* --titles '256 filters|' --title 'Decimal multiplication is easier with curriculum' --task mul --remove_strings='|-progressive_curriculum=5' --exclude='layer|progressive' --order '4,2,1,3' --global-legend=1\n```\n\nRequirements\n============\n\n* TensorFlow (see tensorflow.org for how to install)\n* Matplotlib for Python (sudo apt-get install python-matplotlib)\n* joblib\n\nCredits\n=======\n\nOriginal code by Lukasz Kaiser (lukaszkaiser).  Modified by Eric Price\n(ecprice)\n", "release_dates": []}, {"name": "neural-mmo", "description": "Code for the paper \"Neural MMO: A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "[ags]: resource/docs/ags.png?raw=true\n[earth]: resource/docs/earth_thumbnail.png\n[fire]: resource/docs/fire_thumbnail.png\n[water]: resource/docs/water_thumbnail.png\n[air]: resource/docs/air_thumbnail.png\n[env]: resource/docs/env.jpg\n[demo]: v1.2.png\n\n**Status:** Archive (code is provided as-is, no updates expected)\n\n# ![][ags] Neural MMO: A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents\n\n## (9/19) This is an archive of the inital release of the [Neural MMO project](https://openai.com/blog/neural-mmo/). Active development continuing at [jsuarez5341/neural-mmo](https://github.com/jsuarez5341/neural-mmo). At the time of archiving, the project is on version 1.2 and has made significant progress since v1.0 -- here is a screenshot:\n\n![][demo]\n\n# ![][ags] Neural MMO v1.0 Release\n\nThis environment is the first neural MMO; it attempts to create agents that scale to real world complexity. Simulating evolution on Earth is computationally infeasible, but we can construct a reasonable and efficient facsimile. We consider MMORPGs (Massive Multiplayer Online Role Playing Games) the best proxy for the real world among human games: they are complete macrocosms featuring thousands of agents per persistent world, diverse skilling systems, global economies, and ad-hoc high stakes single and team based conflict.\n\n![][env]\n\n## ![][ags] Quickstart\n\n```\n# Recommended Setup:\nmkdir projekt\ncd projekt\n\ngit clone https://github.com/jsuarez5341/neural-mmo-client\ncd neural-mmo-client\nbash setup.sh\ncd ..\n\ngit clone https://github.com/openai/neural-mmo\ncd neural-mmo\nbash scripts/setup/setup.sh\npython setup.py\n```\n\nThis will download both the OpenAI environment and the independent client, which is required for rendering. Some setups may require you fix the symlink to the client (e.g. `ln -s ../../neural-mmo-client/ embyr` from `forge/embyr`). The setup file will only install _external_ dependencies -- we assume you have already setup **Anaconda with Python 3.6+**. The environment is framework independently, but our experiment code does depend on PyTorch -- set this up separately.\n\nRun the following, then navigate to http://localhost:8080/forge/embyr/ in Firefox or Chrome to pull up the renderer. Click to start and give it a few seconds to load assets and stabilize fps. For troubleshooting, see the [Client Repo](https://github.com/jsuarez5341/neural-mmo-client).\n```\npython Forge.py --render #Run the environment with rendering on\n```\n\n## ![][ags] Overview\n\nThe project is divided into four modules:\n\n| Engineering   | Research      |\n| ------------- |-------------|\n| ![][earth] Blade: Env | ![][water] Trinity: API   |\n| ![][fire] Embyr: Render  | ![][air] Ethyr: Neural |\n\nThe objective is to create agents that scale to the complexity and robustness of the real world. This is a variant phrasing of \"artificial life.\" A key perspective of the project is decoupling this statement into subproblems that are concrete, feasible, and directly composable to solve the whole problem. We split the objective into \"agents that scale to their environment\" and \"environments that scale to the real world.\" These are large respective research and engineering problems, but unlike the original objective, they are specific enough to attempt individually. For a more thorough overview of the project approach and objective, see this [Two Pager](https://docs.google.com/document/d/1_76rYTPtPysSh2_cFFz3Mfso-9VL3_tF5ziaIZ8qmS8/edit?usp=sharing).\n\n![][water]![][air] Research: Agents that scale to env complexity\n\n![][earth]![][fire] Engineering: Env that scales to real world complexity\n\n### ![][water] Trinity\nTrinity is the native API for researchers (the naming is simply flavor -- see \"Namesake\" below). It consists of three base classes, Pantheon, God, and Sword, which you can override to execute code at the Cluster, Server, and Agent levels, respectively.\n\n```python\nfrom forge.trinity import smith, Trinity, Pantheon, God, Sword\ntrinity = Trinity(Pantheon, God, Sword)\nenvs = smith.Native(config, args, trinity)\nenvs.run()\n```\nThat's it -- all communications are handled internally. The provided Pantheon class includes sample gradient aggregation, optimization, and model saving code. The provided God class is mainly a stub to enable future work incorporating population level training algorithms. The provided Sword class contains our simple fully connected model. The full API is defined in forge/trinity/trinity.py.\n\nThe VecEnv/Gym computation model not well suited to this setting and performs 10-1000x more interprocess communication than is required. The native API is simpler, more efficient, and requires much less code. However, we do provide a canonical Gym API minimally modified to support a variable number of agents. Forge.py contains a complete forward pass example for a single population. Basic usage is:\n\n```python\nfrom forge.trinity import smith\nenvs = smith.VecEnv(config, args, self.step)\n\n#The environment is persistent: call reset only upon initialization\nobs = envs.reset()\n\n#Observations contain entity and stimulus\n#for each agent in each environment.\nactions = your_algorithm_here(obs)\n\n#The environment is persistent: \"dones\" is always None\n#If an observation is missing, that agent has died\nobs, rewards, dones, infos = envs.step(actions)\n```\n\nYou can try these both out with:\n```\n# Run Options:\npython Forge.py --nRealm 2 --api native #Run 2 environments with native API\npython Forge.py --nRealm 2 --api vecenv #Run 2 environments with vecenv API\n```\n\n### ![][air] Ethyr\nEthyr is the \"contrib\" for this project. It contains useful research tools for interacting with the project. I've seeded it with the helper classes for our experiments, including a model save/load manager, a rollout objects, and a basic optimizer. If you would like to contribute code (in any framework, not just PyTorch), please submit a pull request.\n\n### ![][earth] Blade\nBlade is the core environment, including game state and control flow. Researchers should not need to touch this, outside perhaps importing core configurations and enums.\n### ![][fire] Embyr\nEmbyr is the independent THREE.js web client. This is downloaded from a separate repository in the setup above and symlinked to the OpenAI repository. You may have to fix the symbolic link if you get forge/embyr import errors. Again, researchers should not have to edit this. In order to run it, run Forge.py with --render enabled, then navigate to localhost:8080/forge/embyr in Firefox. It will take a couple seconds to initialize and load assets. You will need to refresh the page whenever you reboot the server (Forge.py). Chrome and Safari might work, but we do not currently offer official support.\n\n## ![][ags] Failure Modes\nEvaluation can be somewhat difficult in our setting but is not a major blocker. For smaller experiments, we find population size and resource utilization to be reasonable metrics of success. For larger experiments with sufficient domain randomization, Tournaments (as described in the accompanying paper) allow for cross validation of approaches.\n\nWe are currently aware of three failure cases for the project:\n  * Computational infeasibility\n  * \"Agents that scale to their environment\" is too hard\n  * \"Environments that scale to the real world\" is too hard\n\nThe first failure case is a serious risk, but is shared among all areas of the field. This project is not uniquely compute intensive -- in fact, it is one of few environments where it is straightforward to train reasonable policies on a single CPU. If scale is the main issue here, it is likely shared among most if not all other approaches.\n\nThe second problem is probably most familiar to researchers as exploration. Given a cold start, how can agents bootstrap both to better policies and to better exploration strategies? This is a hard problem, but it is unlikely to kill the project because:\n  * This is independently an important problem that many researchers are already working on already\n  * The environment of this project is designed collaboratively to assist agents early on in learning, rather than adversarially as a hard benchmark\n  * [Recent](https://blog.openai.com/openai-five/) [projects](https://blog.openai.com/learning-dexterity/) have demonstrated success at scale.\n\nThe third problem probably appears most likely to many researchers, but least likely to anyone who has spent a significant amount of time in MMOs. Here is a map of the NYC subway:\n\n![QuestMap](resource/docs/quests.png)\n[Source](https://www.reddit.com/user/Gamez_X)\n\nActually, it's a quest map of Runescape, a particular MMO that our environment is loosely based upon. Each quest is a puzzle in itself, takes anywhere from several minutes to several hours to complete, is part of an interconnected web of prerequisites of other quests, and provides different incentives for completion ranging from equipment to unlockable content to experience in a tightly connected set of skills:\n\n![Skills](resource/docs/skills.png)\n![Equipment](resource/docs/equipment.png)\n[Source](https://www.jagex.com/en-GB/)\n\nIn a massive open world:\n\n![GameMap](resource/docs/map.png)\n[Source](https://www.jagex.com/en-GB/)\n\nThe most complex class of games considered to date is MOBAs (Massive Online Battle Arenas, e.g. Dota, Quake CTF), which are round based, take on order of an hour, and are mechanically intensive. Achieving 99 in all skills and acquiring the best gear in Runescape takes, at minimum, several thousand hours. In a tournament setting where attacking other players is allowed everywhere, moment-to-moment gameplay is less important than balancing the risks and rewards of any potential strategy--especially in the presence of hundreds of other players attempting to do the same. There is almost certainly still a complexity gap from MMOs to the real world, but we believe it is much smaller than that in environments currently available.\n\nWhile our environment is nowhere near the level of complexity of a real MMO yet, it does contain key properties of persistence, population scale, and open-endedness. As agents begin to reach the ceiling of the current environment, we plan on continuing development to raise the ceiling.\n\n## ![][ags] File Structure\n![][water] **/forge/trinity** ~350 lines\n* **/forge/trinity/ann.py** - Defines architectures\n* **/forge/trinity/god.py** - Defines server level code (e.g. entity tagging)\n* **/forge/trinity/pantheon.py** - Defines cluster level code (e.g. gradient averaging)\n* **/forge/trinity/sword.py** - Defines core level code (e.g. running networks, collecting rollouts, computing gradients)\n* **/forge/trinity/trinity.py** - Wraps a pantheon, god, and sword\n* **/forge/trinity/smith.py** - Defines the Native and VecEnv / Gym APIs\n\n![][air] **/forge/ethyr** ~250 lines\n* **/forge/ethyr/rollouts.py** - Collects and merges rollouts\n* **/forge/ethyr/stim.py** - Produces a stimulus from local game state\n* **/forge/ethyr/torch** - pytorch specific neural utilities\n  * **/forge/ethyr/torch/loss.py** - Defines policy/value loss and advantage\n  * **/forge/ethyr/torch/optim.py** - Defines optimization and gradient computation\n  * **/forge/ethyr/torch/param.py** - Network surgery useful for serialization\n  * **/forge/ethyr/torch/stim.py** - Wraps the generic stimulus library with pytorch tensors\n  * **/forge/ethyr/torch/utils.py** - Generic pytorch tools\n\n![][earth] **/forge/blade** ~2k lines, of which >1k are for future expansion. Only italicized files are relevant.\n  * **/forge/blade/action** - Defines what entities can do, what occurs when they do it, and provides structure for reasoning over actions.\n    * **/forge/blade/action/action.py** - Class stubs for each action\n    * **/forge/blade/action/tree.py** - Tree structure for assembling actions (e.g. action -> sub-action -> args)\n    * **_/forge/blade/action/v2.py_** - Actions that entities can select, instantiate, and .call() to directly modify world state\n  * **/forge/blade/core** \u2014 Contains ~500 lines of state and game loop code.\n    * **_/forge/blade/core/config.py_** - Defines specifications for each test environment, including entity statistics, the map, and spawn locations.\n    * **_/forge/blade/core/tile.py_** - Defines an individual game tile\n    * **_/forge/blade/core/map.py_** - Defines a map of game tiles\n    * **_/forge/blade/core/env.py_** - Wraps the full game state\n    * **_/forge/blade/core/realm.py_** - Defines the game loop updating the environment and agents.\n  * **/forge/blade/entity** - Defines agents, both neural and scripted\n    * **_/forge/blade/entity/player.py_** \u2014 Defines \"player\" state\n    * **/forge/blade/entity/npc/**\n      * **/forge/blade/entity/npc/npc.py** \u2014 This defines client state (e.g. wraps the neural net making decisions)\n      * **/forge/blade/entity/npc/mobs.py** - Defines scripted npcs\n  * **/forge/blade/hook** - Defines startup scripts that preassemble references\n  * **/forge/blade/item** - Defines all items in the game, including equipment\n  * **/forge/blade/lib** - Defines generic utilities that either do not have clean python implementations or require specific changes for the project\n  * **/forge/blade/systems** - Defines game content\n\n![][fire] **/forge/embyr** See [Client Repo](https://github.com/jsuarez5341/Godsword-Client)\n\n## ![][ags] Namesake\nIn formal publications, we refer to our project as simply a \"Neural MMO.\" Internally and informally, we call it \"Projekt: Godsword.\" The name comes from two sources: CD Projekt Red, my personal favorite game dev studio, and OldSchool Runescape, which contains an iconic set of weapons called god swords. The latter is a particularly good model for AI environments; the former is more of a soft flavor inspiration.\n\n## ![][ags] Authorship\n I, [Joseph Suarez](https://github.com/jsuarez5341), am the author of the core code base. Yilun Du assisted with running experiments and particularly in setting up tournaments. Phillip Isola and Igor Mordatch have been invaluable collaborators and advisers throughout the project. The client was developed independently as a collaboration between myself and Clare Zhu.\n\n## ![][ags] License & Assets\nThe OpenAI repository is open sourced under the MIT License. There is a smaller original code base and game kernel that I (Joseph Suarez) retain ownership of, along with associated ideas. I created these before my employment -- the initial commit here represents the latest pre-employment timestep.\n\nSome assets used in this project belong to [Jagex](https://www.jagex.com/en-GB/)\n, the creators of Runescape, such as\n\n![Alt text](resource/docs/ags.png?raw=true \"Title\")\n![Alt text](resource/docs/earth_thumbnail.png?raw=true \"Title\")\n![Alt text](resource/docs/water_thumbnail.png?raw=true \"Title\")\n![Alt text](resource/docs/fire_thumbnail.png?raw=true \"Title\")\n![Alt text](resource/docs/air_thumbnail.png?raw=true \"Title\")\n\nWe currently use them for flavor as an homage to the game that inspired the project. We believe these fall under fair use as a not-for-profit project for the advancement of artificial intelligence research -- however, we are more than happy to remove them upon request. We do own the 2D and 3D files for agents.\n\n![Alt text](resource/docs/neuralRED.png?raw=true \"Title\")\n![Alt text](resource/docs/neuralBLUE.png?raw=true \"Title\")\n![Alt text](resource/docs/neuralGREEN.png?raw=true \"Title\")\n![Alt text](resource/docs/neuralFUCHSIA.png?raw=true \"Title\")\n![Alt text](resource/docs/neuralORANGE.png?raw=true \"Title\")\n![Alt text](resource/docs/neuralMINT.png?raw=true \"Title\")\n![Alt text](resource/docs/neuralPURPLE.png?raw=true \"Title\")\n![Alt text](resource/docs/neuralSPRING.png?raw=true \"Title\")\n![Alt text](resource/docs/neuralYELLOW.png?raw=true \"Title\")\n![Alt text](resource/docs/neuralCYAN.png?raw=true \"Title\")\n![Alt text](resource/docs/neuralMAGENTA.png?raw=true \"Title\")\n![Alt text](resource/docs/neuralSKY.png?raw=true \"Title\")\n", "release_dates": []}, {"name": "oauth2_proxy", "description": "A reverse proxy that provides authentication with Google, Github or other provider", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# oauth2_proxy\n\nA reverse proxy and static file server that provides authentication using Providers (Google, GitHub, and others)\nto validate accounts by email, domain or group.\n\n**Note:** This repository was forked from [bitly/OAuth2_Proxy](https://github.com/bitly/oauth2_proxy) on 27/11/2018.\nVersions v3.0.0 and up are from this fork and will have diverged from any changes in the original fork.\nA list of changes can be seen in the [CHANGELOG](CHANGELOG.md).\n\n[![Build Status](https://secure.travis-ci.org/pusher/oauth2_proxy.svg?branch=master)](http://travis-ci.org/pusher/oauth2_proxy)\n\n![Sign In Page](https://cloud.githubusercontent.com/assets/45028/4970624/7feb7dd8-6886-11e4-93e0-c9904af44ea8.png)\n\n## Architecture\n\n![OAuth2 Proxy Architecture](https://cloud.githubusercontent.com/assets/45028/8027702/bd040b7a-0d6a-11e5-85b9-f8d953d04f39.png)\n\n## Installation\n\n1.  Choose how to deploy:\n\n    a. Download [Prebuilt Binary](https://github.com/pusher/oauth2_proxy/releases) (current release is `v3.2.0`)\n\n    b. Build with `$ go get github.com/pusher/oauth2_proxy` which will put the binary in `$GOROOT/bin`\n\n    c. Using the prebuilt docker image [quay.io/pusher/oauth2_proxy](https://quay.io/pusher/oauth2_proxy) (AMD64, ARMv6 and ARM64 tags available)\n\nPrebuilt binaries can be validated by extracting the file and verifying it against the `sha256sum.txt` checksum file provided for each release starting with version `v3.0.0`.\n\n```\nsha256sum -c sha256sum.txt 2>&1 | grep OK\noauth2_proxy-3.2.0.linux-amd64: OK\n```\n\n2.  Select a Provider and Register an OAuth Application with a Provider\n3.  Configure OAuth2 Proxy using config file, command line options, or environment variables\n4.  Configure SSL or Deploy behind a SSL endpoint (example provided for Nginx)\n\n## OAuth Provider Configuration\n\nYou will need to register an OAuth application with a Provider (Google, GitHub or another provider), and configure it with Redirect URI(s) for the domain you intend to run `oauth2_proxy` on.\n\nValid providers are :\n\n- [Google](#google-auth-provider) _default_\n- [Azure](#azure-auth-provider)\n- [Facebook](#facebook-auth-provider)\n- [GitHub](#github-auth-provider)\n- [GitLab](#gitlab-auth-provider)\n- [LinkedIn](#linkedin-auth-provider)\n- [login.gov](#logingov-provider)\n\nThe provider can be selected using the `provider` configuration value.\n\n### Google Auth Provider\n\nFor Google, the registration steps are:\n\n1.  Create a new project: https://console.developers.google.com/project\n2.  Choose the new project from the top right project dropdown (only if another project is selected)\n3.  In the project Dashboard center pane, choose **\"API Manager\"**\n4.  In the left Nav pane, choose **\"Credentials\"**\n5.  In the center pane, choose **\"OAuth consent screen\"** tab. Fill in **\"Product name shown to users\"** and hit save.\n6.  In the center pane, choose **\"Credentials\"** tab.\n    - Open the **\"New credentials\"** drop down\n    - Choose **\"OAuth client ID\"**\n    - Choose **\"Web application\"**\n    - Application name is freeform, choose something appropriate\n    - Authorized JavaScript origins is your domain ex: `https://internal.yourcompany.com`\n    - Authorized redirect URIs is the location of oauth2/callback ex: `https://internal.yourcompany.com/oauth2/callback`\n    - Choose **\"Create\"**\n7.  Take note of the **Client ID** and **Client Secret**\n\nIt's recommended to refresh sessions on a short interval (1h) with `cookie-refresh` setting which validates that the account is still authorized.\n\n#### Restrict auth to specific Google groups on your domain. (optional)\n\n1.  Create a service account: https://developers.google.com/identity/protocols/OAuth2ServiceAccount and make sure to download the json file.\n2.  Make note of the Client ID for a future step.\n3.  Under \"APIs & Auth\", choose APIs.\n4.  Click on Admin SDK and then Enable API.\n5.  Follow the steps on https://developers.google.com/admin-sdk/directory/v1/guides/delegation#delegate_domain-wide_authority_to_your_service_account and give the client id from step 2 the following oauth scopes:\n\n```\nhttps://www.googleapis.com/auth/admin.directory.group.readonly\nhttps://www.googleapis.com/auth/admin.directory.user.readonly\n```\n\n6.  Follow the steps on https://support.google.com/a/answer/60757 to enable Admin API access.\n7.  Create or choose an existing administrative email address on the Gmail domain to assign to the `google-admin-email` flag. This email will be impersonated by this client to make calls to the Admin SDK. See the note on the link from step 5 for the reason why.\n8.  Create or choose an existing email group and set that email to the `google-group` flag. You can pass multiple instances of this flag with different groups\n    and the user will be checked against all the provided groups.\n9.  Lock down the permissions on the json file downloaded from step 1 so only oauth2_proxy is able to read the file and set the path to the file in the `google-service-account-json` flag.\n10. Restart oauth2_proxy.\n\nNote: The user is checked against the group members list on initial authentication and every time the token is refreshed ( about once an hour ).\n\n### Azure Auth Provider\n\n1. Add an application: go to [https://portal.azure.com](https://portal.azure.com), choose **\"Azure Active Directory\"** in the left menu, select **\"App registrations\"** and then click on **\"New app registration\"**.\n2. Pick a name and choose **\"Webapp / API\"** as application type. Use `https://internal.yourcompany.com` as Sign-on URL. Click **\"Create\"**.\n3. On the **\"Settings\"** / **\"Properties\"** page of the app, pick a logo and select **\"Multi-tenanted\"** if you want to allow users from multiple organizations to access your app. Note down the application ID. Click **\"Save\"**.\n4. On the **\"Settings\"** / **\"Required Permissions\"** page of the app, click on **\"Windows Azure Active Directory\"** and then on **\"Access the directory as the signed in user\"**. Hit **\"Save\"** and then then on **\"Grant permissions\"** (you might need another admin to do this).\n5. On the **\"Settings\"** / **\"Reply URLs\"** page of the app, add `https://internal.yourcompanycom/oauth2/callback` for each host that you want to protect by the oauth2 proxy. Click **\"Save\"**.\n6. On the **\"Settings\"** / **\"Keys\"** page of the app, add a new key and note down the value after hitting **\"Save\"**.\n7. Configure the proxy with\n\n```\n   --provider=azure\n   --client-id=<application ID from step 3>\n   --client-secret=<value from step 6>\n```\n\n### Facebook Auth Provider\n\n1.  Create a new FB App from <https://developers.facebook.com/>\n2.  Under FB Login, set your Valid OAuth redirect URIs to `https://internal.yourcompany.com/oauth2/callback`\n\n### GitHub Auth Provider\n\n1.  Create a new project: https://github.com/settings/developers\n2.  Under `Authorization callback URL` enter the correct url ie `https://internal.yourcompany.com/oauth2/callback`\n\nThe GitHub auth provider supports two additional parameters to restrict authentication to Organization or Team level access. Restricting by org and team is normally accompanied with `--email-domain=*`\n\n    -github-org=\"\": restrict logins to members of this organisation\n    -github-team=\"\": restrict logins to members of any of these teams (slug), separated by a comma\n\nIf you are using GitHub enterprise, make sure you set the following to the appropriate url:\n\n    -login-url=\"http(s)://<enterprise github host>/login/oauth/authorize\"\n    -redeem-url=\"http(s)://<enterprise github host>/login/oauth/access_token\"\n    -validate-url=\"http(s)://<enterprise github host>/api/v3\"\n\n### GitLab Auth Provider\n\nWhether you are using GitLab.com or self-hosting GitLab, follow [these steps to add an application](http://doc.gitlab.com/ce/integration/oauth_provider.html)\n\nIf you are using self-hosted GitLab, make sure you set the following to the appropriate URL:\n\n    -login-url=\"<your gitlab url>/oauth/authorize\"\n    -redeem-url=\"<your gitlab url>/oauth/token\"\n    -validate-url=\"<your gitlab url>/api/v4/user\"\n\n### LinkedIn Auth Provider\n\nFor LinkedIn, the registration steps are:\n\n1.  Create a new project: https://www.linkedin.com/secure/developer\n2.  In the OAuth User Agreement section:\n    - In default scope, select r_basicprofile and r_emailaddress.\n    - In \"OAuth 2.0 Redirect URLs\", enter `https://internal.yourcompany.com/oauth2/callback`\n3.  Fill in the remaining required fields and Save.\n4.  Take note of the **Consumer Key / API Key** and **Consumer Secret / Secret Key**\n\n### Microsoft Azure AD Provider\n\nFor adding an application to the Microsoft Azure AD follow [these steps to add an application](https://azure.microsoft.com/en-us/documentation/articles/active-directory-integrating-applications/).\n\nTake note of your `TenantId` if applicable for your situation. The `TenantId` can be used to override the default `common` authorization server with a tenant specific server.\n\n### OpenID Connect Provider\n\nOpenID Connect is a spec for OAUTH 2.0 + identity that is implemented by many major providers and several open source projects. This provider was originally built against CoreOS Dex and we will use it as an example.\n\n1.  Launch a Dex instance using the [getting started guide](https://github.com/coreos/dex/blob/master/Documentation/getting-started.md).\n2.  Setup oauth2_proxy with the correct provider and using the default ports and callbacks.\n3.  Login with the fixture use in the dex guide and run the oauth2_proxy with the following args:\n\n    -provider oidc\n    -client-id oauth2_proxy\n    -client-secret proxy\n    -redirect-url http://127.0.0.1:4180/oauth2/callback\n    -oidc-issuer-url http://127.0.0.1:5556\n    -cookie-secure=false\n    -email-domain example.com\n\n### login.gov Provider\n\nlogin.gov is an OIDC provider for the US Government.\nIf you are a US Government agency, you can contact the login.gov team through the contact information\nthat you can find on https://login.gov/developers/ and work with them to understand how to get login.gov\naccounts for integration/test and production access.\n\nA developer guide is available here: https://developers.login.gov/, though this proxy handles everything\nbut the data you need to create to register your application in the login.gov dashboard.\n\nAs a demo, we will assume that you are running your application that you want to secure locally on\nhttp://localhost:3000/, that you will be starting your proxy up on http://localhost:4180/, and that\nyou have an agency integration account for testing.\n\nFirst, register your application in the dashboard.  The important bits are:\n  * Identity protocol:  make this `Openid connect`\n  * Issuer:  do what they say for OpenID Connect.  We will refer to this string as `${LOGINGOV_ISSUER}`.\n  * Public key:  This is a self-signed certificate in .pem format generated from a 2048 bit RSA private key.\n    A quick way to do this is `openssl req -x509 -newkey rsa:2048 -keyout key.pem -out cert.pem -days 3650 -nodes -subj '/C=US/ST=Washington/L=DC/O=GSA/OU=18F/CN=localhost'`,\n    The contents of the `key.pem` shall be referred to as `${OAUTH2_PROXY_JWT_KEY}`.\n  * Return to App URL:  Make this be `http://localhost:4180/`\n  * Redirect URIs:  Make this be `http://localhost:4180/oauth2/callback`.\n  * Attribute Bundle:  Make sure that email is selected.\n\nNow start the proxy up with the following options:\n```\n./oauth2_proxy -provider login.gov \\\n  -client-id=${LOGINGOV_ISSUER} \\\n  -redirect-url=http://localhost:4180/oauth2/callback \\\n  -oidc-issuer-url=https://idp.int.identitysandbox.gov/ \\\n  -cookie-secure=false \\\n  -email-domain=gsa.gov \\\n  -upstream=http://localhost:3000/ \\\n  -cookie-secret=somerandomstring12341234567890AB \\\n  -cookie-domain=localhost \\\n  -skip-provider-button=true \\\n  -pubjwk-url=https://idp.int.identitysandbox.gov/api/openid_connect/certs \\\n  -profile-url=https://idp.int.identitysandbox.gov/api/openid_connect/userinfo \\\n  -jwt-key=\"${OAUTH2_PROXY_JWT_KEY}\"\n```\nYou can also set all these options with environment variables, for use in cloud/docker environments.\nOne tricky thing that you may encounter is that some cloud environments will pass in environment\nvariables in a docker env-file, which does not allow multiline variables like a PEM file.\nIf you encounter this, then you can create a `jwt_signing_key.pem` file in the top level\ndirectory of the repo which contains the key in PEM format and then do your docker build.\nThe docker build process will copy that file into your image which you can then access by\nsetting the `OAUTH2_PROXY_JWT_KEY_FILE=/etc/ssl/private/jwt_signing_key.pem`\nenvironment variable, or by setting `-jwt-key-file=/etc/ssl/private/jwt_signing_key.pem` on the commandline.\n\nOnce it is running, you should be able to go to `http://localhost:4180/` in your browser,\nget authenticated by the login.gov integration server, and then get proxied on to your\napplication running on `http://localhost:3000/`.  In a real deployment, you would secure\nyour application with a firewall or something so that it was only accessible from the\nproxy, and you would use real hostnames everywhere.\n\n#### Skip OIDC discovery\n\nSome providers do not support OIDC discovery via their issuer URL, so oauth2_proxy cannot simply grab the authorization, token and jwks URI endpoints from the provider's metadata.\n\nIn this case, you can set the `-skip-oidc-discovery` option, and supply those required endpoints manually:\n\n```\n    -provider oidc\n    -client-id oauth2_proxy\n    -client-secret proxy\n    -redirect-url http://127.0.0.1:4180/oauth2/callback\n    -oidc-issuer-url http://127.0.0.1:5556\n    -skip-oidc-discovery\n    -login-url http://127.0.0.1:5556/authorize\n    -redeem-url http://127.0.0.1:5556/token\n    -oidc-jwks-url http://127.0.0.1:5556/keys\n    -cookie-secure=false\n    -email-domain example.com\n```\n\n## Email Authentication\n\nTo authorize by email domain use `--email-domain=yourcompany.com`. To authorize individual email addresses use `--authenticated-emails-file=/path/to/file` with one email per line. To authorize all email addresses use `--email-domain=*`.\n\n## Configuration\n\n`oauth2_proxy` can be configured via [config file](#config-file), [command line options](#command-line-options) or [environment variables](#environment-variables).\n\nTo generate a strong cookie secret use `python -c 'import os,base64; print base64.urlsafe_b64encode(os.urandom(16))'`\n\n### Config File\n\nAn example [oauth2_proxy.cfg](contrib/oauth2_proxy.cfg.example) config file is in the contrib directory. It can be used by specifying `-config=/etc/oauth2_proxy.cfg`\n\n### Command Line Options\n\n```\nUsage of oauth2_proxy:\n  -acr-values string:  optional, used by login.gov (default \"http://idmanagement.gov/ns/assurance/loa/1\")\n  -approval-prompt string: OAuth approval_prompt (default \"force\")\n  -authenticated-emails-file string: authenticate against emails via file (one per line)\n  -azure-tenant string: go to a tenant-specific or common (tenant-independent) endpoint. (default \"common\")\n  -basic-auth-password string: the password to set when passing the HTTP Basic Auth header\n  -client-id string: the OAuth Client ID: ie: \"123456.apps.googleusercontent.com\"\n  -client-secret string: the OAuth Client Secret\n  -config string: path to config file\n  -cookie-domain string: an optional cookie domain to force cookies to (ie: .yourcompany.com)\n  -cookie-expire duration: expire timeframe for cookie (default 168h0m0s)\n  -cookie-httponly: set HttpOnly cookie flag (default true)\n  -cookie-name string: the name of the cookie that the oauth_proxy creates (default \"_oauth2_proxy\")\n  -cookie-path string: an optional cookie path to force cookies to (ie: /poc/)* (default \"/\")\n  -cookie-refresh duration: refresh the cookie after this duration; 0 to disable\n  -cookie-secret string: the seed string for secure cookies (optionally base64 encoded)\n  -cookie-secure: set secure (HTTPS) cookie flag (default true)\n  -custom-templates-dir string: path to custom html templates\n  -display-htpasswd-form: display username / password login form if an htpasswd file is provided (default true)\n  -email-domain value: authenticate emails with the specified domain (may be given multiple times). Use * to authenticate any email\n  -flush-interval: period between flushing response buffers when streaming responses (default \"1s\")\n  -footer string: custom footer string. Use \"-\" to disable default footer.\n  -gcp-healthchecks: will enable /liveness_check, /readiness_check, and / (with the proper user-agent) endpoints that will make it work well with GCP App Engine and GKE Ingresses (default false)\n  -github-org string: restrict logins to members of this organisation\n  -github-team string: restrict logins to members of any of these teams (slug), separated by a comma\n  -google-admin-email string: the google admin to impersonate for api calls\n  -google-group value: restrict logins to members of this google group (may be given multiple times).\n  -google-service-account-json string: the path to the service account json credentials\n  -htpasswd-file string: additionally authenticate against a htpasswd file. Entries must be created with \"htpasswd -s\" for SHA encryption\n  -http-address string: [http://]<addr>:<port> or unix://<path> to listen on for HTTP clients (default \"127.0.0.1:4180\")\n  -https-address string: <addr>:<port> to listen on for HTTPS clients (default \":443\")\n  -jwt-key string: private key in PEM format used to sign JWT, so that you can say something like -jwt-key=\"${OAUTH2_PROXY_JWT_KEY}\": required by login.gov\n  -jwt-key-file string: path to the private key file in PEM format used to sign the JWT so that you can say something like -jwt-key-file=/etc/ssl/private/jwt_signing_key.pem: required by login.gov\n  -login-url string: Authentication endpoint\n  -oidc-issuer-url: the OpenID Connect issuer URL. ie: \"https://accounts.google.com\"\n  -oidc-jwks-url string: OIDC JWKS URI for token verification; required if OIDC discovery is disabled\n  -pass-access-token: pass OAuth access_token to upstream via X-Forwarded-Access-Token header\n  -pass-authorization-header: pass OIDC IDToken to upstream via Authorization Bearer header\n  -pass-basic-auth: pass HTTP Basic Auth, X-Forwarded-User and X-Forwarded-Email information to upstream (default true)\n  -pass-host-header: pass the request Host Header to upstream (default true)\n  -pass-user-headers: pass X-Forwarded-User and X-Forwarded-Email information to upstream (default true)\n  -profile-url string: Profile access endpoint\n  -provider string: OAuth provider (default \"google\")\n  -proxy-prefix string: the url root path that this proxy should be nested under (e.g. /<oauth2>/sign_in) (default \"/oauth2\")\n  -proxy-websockets: enables WebSocket proxying (default true)\n  -pubjwk-url string: JWK pubkey access endpoint: required by login.gov\n  -redeem-url string: Token redemption endpoint\n  -redirect-url string: the OAuth Redirect URL. ie: \"https://internalapp.yourcompany.com/oauth2/callback\"\n  -request-logging: Log requests to stdout (default true)\n  -request-logging-format: Template for request log lines (see \"Logging Format\" paragraph below)\n  -resource string: The resource that is protected (Azure AD only)\n  -scope string: OAuth scope specification\n  -set-xauthrequest: set X-Auth-Request-User and X-Auth-Request-Email response headers (useful in Nginx auth_request mode)\n  -set-authorization-header: set Authorization Bearer response header (useful in Nginx auth_request mode)\n  -signature-key string: GAP-Signature request signature key (algorithm:secretkey)\n  -skip-auth-preflight: will skip authentication for OPTIONS requests\n  -skip-auth-regex value: bypass authentication for requests path's that match (may be given multiple times)\n  -skip-oidc-discovery: bypass OIDC endpoint discovery. login-url, redeem-url and oidc-jwks-url must be configured in this case\n  -skip-provider-button: will skip sign-in-page to directly reach the next step: oauth/start\n  -ssl-insecure-skip-verify: skip validation of certificates presented when using HTTPS\n  -tls-cert string: path to certificate file\n  -tls-key string: path to private key file\n  -upstream value: the http url(s) of the upstream endpoint or file:// paths for static files. Routing is based on the path\n  -validate-url string: Access token validation endpoint\n  -version: print version string\n  -whitelist-domain: allowed domains for redirection after authentication. Prefix domain with a . to allow subdomains (eg .example.com)\n```\n\nNote, when using the `whitelist-domain` option, any domain prefixed with a `.` will allow any subdomain of the specified domain as a valid redirect URL.\n\nSee below for provider specific options\n\n### Upstreams Configuration\n\n`oauth2_proxy` supports having multiple upstreams, and has the option to pass requests on to HTTP(S) servers or serve static files from the file system. HTTP and HTTPS upstreams are configured by providing a URL such as `http://127.0.0.1:8080/` for the upstream parameter, that will forward all authenticated requests to be forwarded to the upstream server. If you instead provide `http://127.0.0.1:8080/some/path/` then it will only be requests that start with `/some/path/` which are forwarded to the upstream.\n\nStatic file paths are configured as a file:// URL. `file:///var/www/static/` will serve the files from that directory at `http://[oauth2_proxy url]/var/www/static/`, which may not be what you want. You can provide the path to where the files should be available by adding a fragment to the configured URL. The value of the fragment will then be used to specify which path the files are available at. `file:///var/www/static/#/static/` will ie. make `/var/www/static/` available at `http://[oauth2_proxy url]/static/`.\n\nMultiple upstreams can either be configured by supplying a comma separated list to the `-upstream` parameter, supplying the parameter multiple times or provinding a list in the [config file](#config-file). When multiple upstreams are used routing to them will be based on the path they are set up with.\n\n### Environment variables\n\nThe following environment variables can be used in place of the corresponding command-line arguments:\n\n- `OAUTH2_PROXY_CLIENT_ID`\n- `OAUTH2_PROXY_CLIENT_SECRET`\n- `OAUTH2_PROXY_COOKIE_NAME`\n- `OAUTH2_PROXY_COOKIE_SECRET`\n- `OAUTH2_PROXY_COOKIE_DOMAIN`\n- `OAUTH2_PROXY_COOKIE_PATH`\n- `OAUTH2_PROXY_COOKIE_EXPIRE`\n- `OAUTH2_PROXY_COOKIE_REFRESH`\n- `OAUTH2_PROXY_SIGNATURE_KEY`\n\n## SSL Configuration\n\nThere are two recommended configurations.\n\n1.  Configure SSL Termination with OAuth2 Proxy by providing a `--tls-cert=/path/to/cert.pem` and `--tls-key=/path/to/cert.key`.\n\nThe command line to run `oauth2_proxy` in this configuration would look like this:\n\n```bash\n./oauth2_proxy \\\n   --email-domain=\"yourcompany.com\"  \\\n   --upstream=http://127.0.0.1:8080/ \\\n   --tls-cert=/path/to/cert.pem \\\n   --tls-key=/path/to/cert.key \\\n   --cookie-secret=... \\\n   --cookie-secure=true \\\n   --provider=... \\\n   --client-id=... \\\n   --client-secret=...\n```\n\n2.  Configure SSL Termination with [Nginx](http://nginx.org/) (example config below), Amazon ELB, Google Cloud Platform Load Balancing, or ....\n\nBecause `oauth2_proxy` listens on `127.0.0.1:4180` by default, to listen on all interfaces (needed when using an\nexternal load balancer like Amazon ELB or Google Platform Load Balancing) use `--http-address=\"0.0.0.0:4180\"` or\n`--http-address=\"http://:4180\"`.\n\nNginx will listen on port `443` and handle SSL connections while proxying to `oauth2_proxy` on port `4180`.\n`oauth2_proxy` will then authenticate requests for an upstream application. The external endpoint for this example\nwould be `https://internal.yourcompany.com/`.\n\nAn example Nginx config follows. Note the use of `Strict-Transport-Security` header to pin requests to SSL\nvia [HSTS](http://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security):\n\n```\nserver {\n    listen 443 default ssl;\n    server_name internal.yourcompany.com;\n    ssl_certificate /path/to/cert.pem;\n    ssl_certificate_key /path/to/cert.key;\n    add_header Strict-Transport-Security max-age=2592000;\n\n    location / {\n        proxy_pass http://127.0.0.1:4180;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Scheme $scheme;\n        proxy_connect_timeout 1;\n        proxy_send_timeout 30;\n        proxy_read_timeout 30;\n    }\n}\n```\n\nThe command line to run `oauth2_proxy` in this configuration would look like this:\n\n```bash\n./oauth2_proxy \\\n   --email-domain=\"yourcompany.com\"  \\\n   --upstream=http://127.0.0.1:8080/ \\\n   --cookie-secret=... \\\n   --cookie-secure=true \\\n   --provider=... \\\n   --client-id=... \\\n   --client-secret=...\n```\n\n## Endpoint Documentation\n\nOAuth2 Proxy responds directly to the following endpoints. All other endpoints will be proxied upstream when authenticated. The `/oauth2` prefix can be changed with the `--proxy-prefix` config variable.\n\n- /robots.txt - returns a 200 OK response that disallows all User-agents from all paths; see [robotstxt.org](http://www.robotstxt.org/) for more info\n- /ping - returns a 200 OK response, which is intended for use with health checks\n- /oauth2/sign_in - the login page, which also doubles as a sign out page (it clears cookies)\n- /oauth2/start - a URL that will redirect to start the OAuth cycle\n- /oauth2/callback - the URL used at the end of the OAuth cycle. The oauth app will be configured with this as the callback url.\n- /oauth2/auth - only returns a 202 Accepted response or a 401 Unauthorized response; for use with the [Nginx `auth_request` directive](#nginx-auth-request)\n\n## Request signatures\n\nIf `signature_key` is defined, proxied requests will be signed with the\n`GAP-Signature` header, which is a [Hash-based Message Authentication Code\n(HMAC)](https://en.wikipedia.org/wiki/Hash-based_message_authentication_code)\nof selected request information and the request body [see `SIGNATURE_HEADERS`\nin `oauthproxy.go`](./oauthproxy.go).\n\n`signature_key` must be of the form `algorithm:secretkey`, (ie: `signature_key = \"sha1:secret0\"`)\n\nFor more information about HMAC request signature validation, read the\nfollowing:\n\n- [Amazon Web Services: Signing and Authenticating REST\n  Requests](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html)\n- [rc3.org: Using HMAC to authenticate Web service\n  requests](http://rc3.org/2011/12/02/using-hmac-to-authenticate-web-service-requests/)\n\n## Logging Format\n\nBy default, OAuth2 Proxy logs requests to stdout in a format similar to Apache Combined Log.\n\n```\n<REMOTE_ADDRESS> - <user@domain.com> [19/Mar/2015:17:20:19 -0400] <HOST_HEADER> GET <UPSTREAM_HOST> \"/path/\" HTTP/1.1 \"<USER_AGENT>\" <RESPONSE_CODE> <RESPONSE_BYTES> <REQUEST_DURATION>\n```\n\nIf you require a different format than that, you can configure it with the `-request-logging-format` flag.\nThe default format is configured as follows:\n\n```\n{{.Client}} - {{.Username}} [{{.Timestamp}}] {{.Host}} {{.RequestMethod}} {{.Upstream}} {{.RequestURI}} {{.Protocol}} {{.UserAgent}} {{.StatusCode}} {{.ResponseSize}} {{.RequestDuration}}\n```\n\n[See `logMessageData` in `logging_handler.go`](./logging_handler.go) for all available variables.\n\n## Adding a new Provider\n\nFollow the examples in the [`providers` package](providers/) to define a new\n`Provider` instance. Add a new `case` to\n[`providers.New()`](providers/providers.go) to allow `oauth2_proxy` to use the\nnew `Provider`.\n\n## <a name=\"nginx-auth-request\"></a>Configuring for use with the Nginx `auth_request` directive\n\nThe [Nginx `auth_request` directive](http://nginx.org/en/docs/http/ngx_http_auth_request_module.html) allows Nginx to authenticate requests via the oauth2_proxy's `/auth` endpoint, which only returns a 202 Accepted response or a 401 Unauthorized response without proxying the request through. For example:\n\n```nginx\nserver {\n  listen 443 ssl;\n  server_name ...;\n  include ssl/ssl.conf;\n\n  location /oauth2/ {\n    proxy_pass       http://127.0.0.1:4180;\n    proxy_set_header Host                    $host;\n    proxy_set_header X-Real-IP               $remote_addr;\n    proxy_set_header X-Scheme                $scheme;\n    proxy_set_header X-Auth-Request-Redirect $request_uri;\n  }\n  location = /oauth2/auth {\n    proxy_pass       http://127.0.0.1:4180;\n    proxy_set_header Host             $host;\n    proxy_set_header X-Real-IP        $remote_addr;\n    proxy_set_header X-Scheme         $scheme;\n    # nginx auth_request includes headers but not body\n    proxy_set_header Content-Length   \"\";\n    proxy_pass_request_body           off;\n  }\n\n  location / {\n    auth_request /oauth2/auth;\n    error_page 401 = /oauth2/sign_in;\n\n    # pass information via X-User and X-Email headers to backend,\n    # requires running with --set-xauthrequest flag\n    auth_request_set $user   $upstream_http_x_auth_request_user;\n    auth_request_set $email  $upstream_http_x_auth_request_email;\n    proxy_set_header X-User  $user;\n    proxy_set_header X-Email $email;\n\n    # if you enabled --pass-access-token, this will pass the token to the backend\n    auth_request_set $token  $upstream_http_x_auth_request_access_token;\n    proxy_set_header X-Access-Token $token;\n\n    # if you enabled --cookie-refresh, this is needed for it to work with auth_request\n    auth_request_set $auth_cookie $upstream_http_set_cookie;\n    add_header Set-Cookie $auth_cookie;\n\n    # When using the --set-authorization-header flag, some provider's cookies can exceed the 4kb\n    # limit and so the OAuth2 Proxy splits these into multiple parts.\n    # Nginx normally only copies the first `Set-Cookie` header from the auth_request to the response,\n    # so if your cookies are larger than 4kb, you will need to extract additional cookies manually.\n    auth_request_set $auth_cookie_name_upstream_1 $upstream_cookie_auth_cookie_name_1;\n\n    # Extract the Cookie attributes from the first Set-Cookie header and append them\n    # to the second part ($upstream_cookie_* variables only contain the raw cookie content)\n    if ($auth_cookie ~* \"(; .*)\") {\n        set $auth_cookie_name_0 $auth_cookie;\n        set $auth_cookie_name_1 \"auth_cookie_name_1=$auth_cookie_name_upstream_1$1\";\n    }\n\n    # Send both Set-Cookie headers now if there was a second part\n    if ($auth_cookie_name_upstream_1) {\n        add_header Set-Cookie $auth_cookie_name_0;\n        add_header Set-Cookie $auth_cookie_name_1;\n    }\n\n    proxy_pass http://backend/;\n    # or \"root /path/to/site;\" or \"fastcgi_pass ...\" etc\n  }\n}\n```\n\nIf you use ingress-nginx in Kubernetes (which includes the Lua module), you also can use the following configuration snippet for your Ingress:\n\n```yaml\nnginx.ingress.kubernetes.io/auth-response-headers: Authorization\nnginx.ingress.kubernetes.io/auth-signin: https://$host/oauth2/start?rd=$request_uri\nnginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth\nnginx.ingress.kubernetes.io/configuration-snippet: |\n  auth_request_set $name_upstream_1 $upstream_cookie_name_1;\n\n  access_by_lua_block {\n    if ngx.var.name_upstream_1 ~= \"\" then\n      ngx.header[\"Set-Cookie\"] = \"name_1=\" .. ngx.var.name_upstream_1 .. ngx.var.auth_cookie:match(\"(; .*)\")\n    end\n  }\n```\n\n## Contributing\n\nPlease see our [Contributing](CONTRIBUTING.md) guidelines.\n", "release_dates": []}, {"name": "openai-cookbook", "description": "Examples and guides for using the OpenAI API", "language": "MDX", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "<a href=\"https://cookbook.openai.com\" target=\"_blank\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"/images/openai-cookbook-white.png\" style=\"max-width: 100%; width: 400px; margin-bottom: 20px\">\n    <img alt=\"OpenAI Cookbook Logo\" src=\"/images/openai-cookbook.png\" width=\"400px\">\n  </picture>\n</a>\n\n<h3></h3>\n \n> \u2728 Navigate at [cookbook.openai.com](https://cookbook.openai.com)\n\nExample code and guides for accomplishing common tasks with the [OpenAI API](https://platform.openai.com/docs/introduction). To run these examples, you'll need an OpenAI account and associated API key ([create a free account here](https://beta.openai.com/signup)).\n\nMost code examples are written in Python, though the concepts can be applied in any language.\n\nFor other useful tools, guides and courses, check out these [related resources from around the web](https://cookbook.openai.com/related_resources).\n\n## Contributing\n\nThe OpenAI Cookbook is a community-driven resource. Whether you're submitting an idea, fixing a typo, adding a new guide, or improving an existing one, your contributions are greatly appreciated!\n\nBefore contributing, read through the existing issues and pull requests to see if someone else is already working on something similar. That way you can avoid duplicating efforts.\n\nIf there are examples or guides you'd like to see, feel free to suggest them on the [issues page](https://github.com/openai/openai-cookbook/issues).\n\nIf you'd like to contribute new content, make sure to read through our [contribution guidelines](/CONTRIBUTING.md). We welcome high-quality submissions of new examples and guides, as long as they meet our criteria and fit within the scope of the cookbook.\n\nThe contents of this repo are automatically rendered into [cookbook.openai.com](https://cookbook.openai.com) based on [registry.yaml](/registry.yaml).\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=468576060&machine=basicLinux32gb&location=EastUs)\n", "release_dates": []}, {"name": "openai-deno-build", "description": "Deno build of the official Typescript library for the OpenAI API.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# OpenAI Node API Library - Deno build\n\nThis is a build produced from https://github.com/openai/openai-node \u2013 please go\nthere to read the source and docs, file issues, etc.\n\nUsage:\n\n```ts\nimport OpenAI from \"https://deno.land/x/openai@v4.28.4/mod.ts\";\n\nconst client = new OpenAI();\n```\n\nNote that in most Deno environments, you can also do this:\n\n```ts\nimport OpenAI from \"npm:openai\";\n```\n", "release_dates": []}, {"name": "openai-gemm", "description": "Open single and half precision gemm implementations ", "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# openai-gemm\nOpen single and half precision gemm implementations.  The main speedups over cublas are with small minibatch and in fp16 data formats.\n\n## Quick Install\n\nThe demonstration code currently depends on [Nervana neon](https://github.com/NervanaSystems/neon):\n\n    git clone git@github.com:NervanaSystems/neon.git\n    cd neon\n    make\n    . .venv/bin/activate\n\nClone and run this repo:\n\n    git clone git@github.com:openai/openai-gemm.git\n\n    Run the benchmark:\n    ./benchmark.py\n\n    Run the unit test:\n    ./test.py\n\n\n### DeepBench on Pascal TITAN X\n( https://github.com/baidu-research/DeepBench )\n\n\n|     M|     N|     K| Op|OpenAI_32|cuBLAS_32|ratio_32|OpenAI_16|cuBLAS_16|ratio_16|\n|------|------|------|---|---------|---------|--------|---------|---------|--------|\n|    16|  1760|  1760| NN|     2557|     2195|     1.2|     3507|      346|    10.1|\n|    32|  1760|  1760| NN|     5010|     1128|     4.4|     6814|      526|    13.0|\n|    64|  1760|  1760| NN|     6486|     4112|     1.6|     8235|     2801|     2.9|\n|   128|  1760|  1760| NN|     7068|     6931|     1.0|     9400|     5307|     1.8|\n|  7000|  1760|  1760| NN|     9968|     9584|     1.0|    10515|     9807|     1.1|\n|    16|  2048|  2048| NN|     2569|     1516|     1.7|     3619|      242|    15.0|\n|    32|  2048|  2048| NN|     5034|     1356|     3.7|     6576|      606|    10.8|\n|    64|  2048|  2048| NN|     6636|     2815|     2.4|     8285|     3241|     2.6|\n|   128|  2048|  2048| NN|     7316|     6373|     1.1|     9066|     5334|     1.7|\n|  7000|  2048|  2048| NN|    10081|     9900|     1.0|    11275|     9948|     1.1|\n|    16|  2560|  2560| NN|     2718|     1312|     2.1|     4312|      251|    17.2|\n|    32|  2560|  2560| NN|     5370|     1660|     3.2|     7525|      749|    10.0|\n|    64|  2560|  2560| NN|     7331|     2687|     2.7|     8436|      951|     8.9|\n|   128|  2560|  2560| NN|     8007|     5238|     1.5|     9277|     6123|     1.5|\n|  7000|  2560|  2560| NN|    10282|    10131|     1.0|    11027|     9974|     1.1|\n|    16|  4096|  4096| NN|     2695|     1110|     2.4|     4442|      266|    16.7|\n|    32|  4096|  4096| NN|     5266|     2264|     2.3|     7723|      758|    10.2|\n|    64|  4096|  4096| NN|     6942|     3922|     1.8|     8904|     1055|     8.4|\n|   128|  4096|  4096| NN|     8127|     5686|     1.4|     9711|     5681|     1.7|\n|  7000|  4096|  4096| NN|    10462|    10082|     1.0|    11152|     9991|     1.1|\n|    16|  1760|  1760| NT|     1719|     1095|     1.6|     2692|      290|     9.3|\n|    32|  1760|  1760| NT|     3316|     1312|     2.5|     5068|      447|    11.3|\n|    64|  1760|  1760| NT|     5247|     1955|     2.7|     7621|     1797|     4.2|\n|   128|  1760|  1760| NT|     6720|     3393|     2.0|     8886|     3342|     2.7|\n|  7000|  1760|  1760| NT|     9341|     8513|     1.1|    10085|     9635|     1.0|\n|    16|  2048|  2048| NT|     2442|     1231|     2.0|     3641|      299|    12.2|\n|    32|  2048|  2048| NT|     4801|     1251|     3.8|     5849|      468|    12.5|\n|    64|  2048|  2048| NT|     6317|     1967|     3.2|     7825|     3128|     2.5|\n|   128|  2048|  2048| NT|     7176|     5041|     1.4|     8616|     4843|     1.8|\n|  7000|  2048|  2048| NT|     9975|     9173|     1.1|    10741|     9560|     1.1|\n|    16|  2560|  2560| NT|     1834|     1208|     1.5|     3154|      297|    10.6|\n|    32|  2560|  2560| NT|     3610|     1436|     2.5|     5418|      584|     9.3|\n|    64|  2560|  2560| NT|     6083|     2815|     2.2|     8331|     1042|     8.0|\n|   128|  2560|  2560| NT|     7702|     3246|     2.4|     8857|     5259|     1.7|\n|  7000|  2560|  2560| NT|     9257|     7829|     1.2|    10659|     9548|     1.1|\n|    16|  4096|  4096| NT|     2546|     1297|     2.0|     4164|      309|    13.5|\n|    32|  4096|  4096| NT|     4992|     2290|     2.2|     8156|      775|    10.5|\n|    64|  4096|  4096| NT|     6746|     4157|     1.6|     8429|     1381|     6.1|\n|   128|  4096|  4096| NT|     7843|     5425|     1.4|     9298|     5527|     1.7|\n|  7000|  4096|  4096| NT|     9925|     6879|     1.4|    10630|     9784|     1.1|\n|  7133|  1760|  1760| TN|     9752|    10186|     1.0|    10517|     8912|     1.2|\n|  7133|  2048|  2048| TN|    10485|    10319|     1.0|    10674|     9608|     1.1|\n|  7133|  2560|  2560| TN|    10743|    11057|     1.0|    11195|    10059|     1.1|\n|  7133|  4096|  4096| TN|    10384|    10290|     1.0|    10980|    10558|     1.0|\n|  9124|  5124|  1760| NN|     9920|     9480|     1.0|    10580|     9743|     1.1|\n|  9124|  5124|  2048| NN|    10008|     9415|     1.1|    10602|     9796|     1.1|\n|  9124|  5124|  2560| NN|     9925|     9426|     1.1|    10586|     9850|     1.1|\n|  9124|  5124|  4096| NN|     9982|     9489|     1.1|    10580|     9472|     1.1|\n|  9124|  5124|  1760| NT|     9093|     3497|     2.6|     9302|     8692|     1.1|\n|  9124|  5124|  2048| NT|     9506|     6512|     1.5|     9506|     8883|     1.1|\n|  9124|  5124|  2560| NT|     8704|     3364|     2.6|     9855|     7733|     1.3|\n|  9124|  5124|  4096| NT|     9733|     6109|     1.6|    10278|     8760|     1.2|\n|  8457|    35|  1760| NN|     3343|     1020|     3.3|     3841|      736|     5.2|\n|  8457|    35|  2048| NN|     3419|     1996|     1.7|     4782|      803|     6.0|\n|  8457|    35|  2560| NN|     3415|     1072|     3.2|     3868|      789|     4.9|\n|  8457|    35|  4096| NN|     3743|     2009|     1.9|     4741|      804|     5.9|\n|  8457|    35|  1760| NT|     3574|     1970|     1.8|     4176|     1243|     3.4|\n|  8457|    35|  2048| NT|     4564|     3069|     1.5|     4818|     1255|     3.8|\n|  8457|    35|  2560| NT|     3598|     2062|     1.7|     3597|     1135|     3.2|\n|  8457|    35|  4096| NT|     4311|     2990|     1.4|     4927|     1303|     3.8|\n|    16|  7680|  2560| NN|     2683|      718|     3.7|     4449|      289|    15.4|\n|    32|  7680|  2560| NN|     5304|     3660|     1.4|     7837|      979|     8.0|\n|    64|  7680|  2560| NN|     7311|     4979|     1.5|     9310|     1274|     7.3|\n|   128|  7680|  2560| NN|     7931|     6109|     1.3|     9390|     6591|     1.4|\n|    16|  7680|  2560| NT|     1885|     1191|     1.6|     3401|      290|    11.7|\n|    32|  7680|  2560| NT|     3731|     1808|     2.1|     6373|     1004|     6.3|\n|    64|  7680|  2560| NT|     6274|     3509|     1.8|     8809|     1655|     5.3|\n|   128|  7680|  2560| NT|     7957|     2988|     2.7|     9246|     4695|     2.0|\n|    16|  3072|  1024| NN|     2277|     1295|     1.8|     3373|      282|    12.0|\n|    32|  3072|  1024| NN|     4494|     1798|     2.5|     6011|      807|     7.4|\n|    64|  3072|  1024| NN|     6272|     3046|     2.1|     6790|      917|     7.4|\n|   128|  3072|  1024| NN|     7364|     5436|     1.4|     7768|     5749|     1.4|\n|    16|  3072|  1024| NT|     2285|     1077|     2.1|     3439|      244|    14.1|\n|    32|  3072|  1024| NT|     4597|     1540|     3.0|     5645|      677|     8.3|\n|    64|  3072|  1024| NT|     6392|     2969|     2.2|     7555|     1204|     6.3|\n|   128|  3072|  1024| NT|     7460|     5058|     1.5|     8586|     5535|     1.6|\n|  7435|  3072|  1024| TN|     9829|     8804|     1.1|    10123|     9365|     1.1|\n|  5481|  7680|  2560| TN|     9448|     9309|     1.0|     9466|     9394|     1.0|\n\n\n### DeepBench on DGX1 (P100)\nNote that the OpenAI kernels do not yet implement fp16x2 instructions.  Even still it seems the current cublas hgemm implentation is only good for large dimensions.  There are also accuracy considerations when accumulating large reductions in fp16.\n\n|     M|     N|     K| Op|OpenAI_32|cuBLAS_32|ratio_32|OpenAI_16|cuBLAS_16|ratio_16|\n|------|------|------|---|---------|---------|--------|---------|---------|--------|\n|    16|  1760|  1760| NN|     2595|     2048|     1.3|     2935|      463|     6.3|\n|    32|  1760|  1760| NN|     4963|      864|     5.7|     5766|      895|     6.4|\n|    64|  1760|  1760| NN|     7565|     3909|     1.9|     7760|     1711|     4.5|\n|   128|  1760|  1760| NN|     8140|     6053|     1.3|     8422|     4089|     2.1|\n|  7000|  1760|  1760| NN|     9653|     8722|     1.1|     9617|    16143|     0.6|\n|    16|  2048|  2048| NN|     2255|     1746|     1.3|     3211|      546|     5.9|\n|    32|  2048|  2048| NN|     4467|     1012|     4.4|     4533|     1019|     4.4|\n|    64|  2048|  2048| NN|     6618|     4198|     1.6|     6591|     2018|     3.3|\n|   128|  2048|  2048| NN|     8059|     5921|     1.4|     7936|     4667|     1.7|\n|  7000|  2048|  2048| NN|     9761|     9346|     1.0|     9910|    18715|     0.5|\n|    16|  2560|  2560| NN|     2883|     2108|     1.4|     4210|      685|     6.1|\n|    32|  2560|  2560| NN|     5701|     1279|     4.5|     5820|     1297|     4.5|\n|    64|  2560|  2560| NN|     8100|     6054|     1.3|     8099|     2558|     3.2|\n|   128|  2560|  2560| NN|     8308|     6799|     1.2|     8790|     5901|     1.5|\n|  7000|  2560|  2560| NN|     9740|     9538|     1.0|     9845|    18499|     0.5|\n|    16|  4096|  4096| NN|     3449|     1342|     2.6|     4299|     1069|     4.0|\n|    32|  4096|  4096| NN|     6863|     2045|     3.4|     6907|     2103|     3.3|\n|    64|  4096|  4096| NN|     8404|     4059|     2.1|     8248|     4183|     2.0|\n|   128|  4096|  4096| NN|     8224|     8039|     1.0|     8853|     8669|     1.0|\n|  7000|  4096|  4096| NN|     9818|     9519|     1.0|    10011|    18588|     0.5|\n|    16|  1760|  1760| NT|     2579|     1324|     1.9|     2763|      428|     6.4|\n|    32|  1760|  1760| NT|     5089|      878|     5.8|     5382|      857|     6.3|\n|    64|  1760|  1760| NT|     7501|     3017|     2.5|     7695|     1695|     4.5|\n|   128|  1760|  1760| NT|     8043|     5494|     1.5|     8192|     3426|     2.4|\n|  7000|  1760|  1760| NT|     9477|     7571|     1.3|     9355|    16113|     0.6|\n|    16|  2048|  2048| NT|     2267|     1276|     1.8|     3171|      504|     6.3|\n|    32|  2048|  2048| NT|     4484|     1026|     4.4|     4489|     1009|     4.4|\n|    64|  2048|  2048| NT|     6567|     3986|     1.6|     6551|     2018|     3.2|\n|   128|  2048|  2048| NT|     8019|     5825|     1.4|     7968|     4496|     1.8|\n|  7000|  2048|  2048| NT|     9625|     9373|     1.0|     9713|    17878|     0.5|\n|    16|  2560|  2560| NT|     2870|     1460|     2.0|     4256|      638|     6.7|\n|    32|  2560|  2560| NT|     5614|     1299|     4.3|     5705|     1271|     4.5|\n|    64|  2560|  2560| NT|     8014|     4402|     1.8|     8085|     2521|     3.2|\n|   128|  2560|  2560| NT|     8219|     5640|     1.5|     8240|     5137|     1.6|\n|  7000|  2560|  2560| NT|     9534|     9091|     1.0|     9735|    18025|     0.5|\n|    16|  4096|  4096| NT|     3366|     1547|     2.2|     4354|     1047|     4.2|\n|    32|  4096|  4096| NT|     6714|     2055|     3.3|     6859|     2093|     3.3|\n|    64|  4096|  4096| NT|     8297|     3445|     2.4|     8289|     4178|     2.0|\n|   128|  4096|  4096| NT|     8335|     7450|     1.1|     7911|     7973|     1.0|\n|  7000|  4096|  4096| NT|     9578|     9214|     1.0|     9877|    18073|     0.5|\n|  7133|  1760|  1760| TN|     9704|     9267|     1.0|     9506|    15605|     0.6|\n|  7133|  2048|  2048| TN|     9747|     9836|     1.0|    10012|    19110|     0.5|\n|  7133|  2560|  2560| TN|     9742|     9748|     1.0|     9805|    19107|     0.5|\n|  7133|  4096|  4096| TN|     9807|     9733|     1.0|    10122|    19559|     0.5|\n|  9124|  5124|  1760| NN|     9326|     9076|     1.0|     9631|    17496|     0.6|\n|  9124|  5124|  2048| NN|     9414|     9054|     1.0|     9602|    17523|     0.5|\n|  9124|  5124|  2560| NN|     9353|     9041|     1.0|     9698|    17380|     0.6|\n|  9124|  5124|  4096| NN|     9370|     9051|     1.0|     9689|    17617|     0.5|\n|  9124|  5124|  1760| NT|     9124|     8746|     1.0|     9524|    16777|     0.6|\n|  9124|  5124|  2048| NT|     9294|     8817|     1.1|     9641|    16935|     0.6|\n|  9124|  5124|  2560| NT|     9221|     8499|     1.1|     9637|    16820|     0.6|\n|  9124|  5124|  4096| NT|     9270|     8961|     1.0|     9568|    17080|     0.6|\n|  8457|    35|  1760| NN|     3301|     2233|     1.5|     4505|     3154|     1.4|\n|  8457|    35|  2048| NN|     3265|     3066|     1.1|     4501|     3335|     1.3|\n|  8457|    35|  2560| NN|     3127|     2300|     1.4|     4516|     3135|     1.4|\n|  8457|    35|  4096| NN|     3257|     3272|     1.0|     4729|     3485|     1.4|\n|  8457|    35|  1760| NT|     4563|     3142|     1.5|     4612|     2998|     1.5|\n|  8457|    35|  2048| NT|     4554|     3202|     1.4|     4601|     3109|     1.5|\n|  8457|    35|  2560| NT|     4567|     3144|     1.5|     4654|     3039|     1.5|\n|  8457|    35|  4096| NT|     4353|     3415|     1.3|     4457|     3257|     1.4|\n|    16|  7680|  2560| NN|     3668|     1200|     3.1|     5020|     1236|     4.1|\n|    32|  7680|  2560| NN|     7245|     3385|     2.1|     7519|     2465|     3.1|\n|    64|  7680|  2560| NN|     8440|     5210|     1.6|     8349|     4910|     1.7|\n|   128|  7680|  2560| NN|     8765|     4872|     1.8|     9131|    11349|     0.8|\n|    16|  7680|  2560| NT|     3229|     1515|     2.1|     5032|     1157|     4.3|\n|    32|  7680|  2560| NT|     6640|     2721|     2.4|     6810|     2307|     3.0|\n|    64|  7680|  2560| NT|     8282|     5113|     1.6|     8362|     4494|     1.9|\n|   128|  7680|  2560| NT|     8763|     4646|     1.9|     8617|     9159|     0.9|\n|    16|  3072|  1024| NN|     2929|     1717|     1.7|     3335|      750|     4.4|\n|    32|  3072|  1024| NN|     5801|     1399|     4.1|     6116|     1420|     4.3|\n|    64|  3072|  1024| NN|     6958|     4340|     1.6|     6923|     2814|     2.5|\n|   128|  3072|  1024| NN|     8047|     6492|     1.2|     7769|     6302|     1.2|\n|    16|  3072|  1024| NT|     2990|     1068|     2.8|     3384|      705|     4.8|\n|    32|  3072|  1024| NT|     5834|     1429|     4.1|     6021|     1411|     4.3|\n|    64|  3072|  1024| NT|     6921|     3500|     2.0|     6893|     2819|     2.4|\n|   128|  3072|  1024| NT|     7918|     6034|     1.3|     7876|     5760|     1.4|\n|  7435|  3072|  1024| TN|     9367|     9391|     1.0|     9559|    17234|     0.6|\n|  5481|  7680|  2560| TN|     9672|     9520|     1.0|     9967|    18832|     0.5|\n", "release_dates": []}, {"name": "openai-node", "description": "The official Node.js / Typescript library for the OpenAI API", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# OpenAI Node API Library\n\n[![NPM version](https://img.shields.io/npm/v/openai.svg)](https://npmjs.org/package/openai)\n\nThis library provides convenient access to the OpenAI REST API from TypeScript or JavaScript.\n\nIt is generated from our [OpenAPI specification](https://github.com/openai/openai-openapi) with [Stainless](https://stainlessapi.com/).\n\nTo learn how to use the OpenAI API, check out our [API Reference](https://platform.openai.com/docs/api-reference) and [Documentation](https://platform.openai.com/docs).\n\n## Installation\n\n```sh\nnpm install --save openai\n# or\nyarn add openai\n```\n\nYou can import in Deno via:\n\n<!-- x-release-please-start-version -->\n\n```ts\nimport OpenAI from 'https://deno.land/x/openai@v4.28.4/mod.ts';\n```\n\n<!-- x-release-please-end -->\n\n## Usage\n\nThe full API of this library can be found in [api.md file](api.md) along with many [code examples](https://github.com/openai/openai-node/tree/master/examples). The code below shows how to get started using the chat completions API.\n\n<!-- prettier-ignore -->\n```js\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted\n});\n\nasync function main() {\n  const chatCompletion = await openai.chat.completions.create({\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    model: 'gpt-3.5-turbo',\n  });\n}\n\nmain();\n```\n\n## Streaming Responses\n\nWe provide support for streaming responses using Server Sent Events (SSE).\n\n```ts\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const stream = await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    stream: true,\n  });\n  for await (const chunk of stream) {\n    process.stdout.write(chunk.choices[0]?.delta?.content || '');\n  }\n}\n\nmain();\n```\n\nIf you need to cancel a stream, you can `break` from the loop\nor call `stream.controller.abort()`.\n\n### Request & Response types\n\nThis library includes TypeScript definitions for all request params and response fields. You may import and use them like so:\n\n<!-- prettier-ignore -->\n```ts\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted\n});\n\nasync function main() {\n  const params: OpenAI.Chat.ChatCompletionCreateParams = {\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    model: 'gpt-3.5-turbo',\n  };\n  const chatCompletion: OpenAI.Chat.ChatCompletion = await openai.chat.completions.create(params);\n}\n\nmain();\n```\n\nDocumentation for each method, request param, and response field are available in docstrings and will appear on hover in most modern editors.\n\n> [!IMPORTANT]\n> Previous versions of this SDK used a `Configuration` class. See the [v3 to v4 migration guide](https://github.com/openai/openai-node/discussions/217).\n\n### Streaming responses\n\nThis library provides several conveniences for streaming chat completions, for example:\n\n```ts\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const stream = await openai.beta.chat.completions.stream({\n    model: 'gpt-4',\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    stream: true,\n  });\n\n  stream.on('content', (delta, snapshot) => {\n    process.stdout.write(delta);\n  });\n\n  // or, equivalently:\n  for await (const chunk of stream) {\n    process.stdout.write(chunk.choices[0]?.delta?.content || '');\n  }\n\n  const chatCompletion = await stream.finalChatCompletion();\n  console.log(chatCompletion); // {id: \"\u2026\", choices: [\u2026], \u2026}\n}\n\nmain();\n```\n\nStreaming with `openai.beta.chat.completions.stream({\u2026})` exposes\n[various helpers for your convenience](helpers.md#events) including event handlers and promises.\n\nAlternatively, you can use `openai.chat.completions.create({ stream: true, \u2026 })`\nwhich only returns an async iterable of the chunks in the stream and thus uses less memory\n(it does not build up a final chat completion object for you).\n\nIf you need to cancel a stream, you can `break` from a `for await` loop or call `stream.abort()`.\n\n### Automated function calls\n\nWe provide the `openai.beta.chat.completions.runTools({\u2026})`\nconvenience helper for using function tool calls with the `/chat/completions` endpoint\nwhich automatically call the JavaScript functions you provide\nand sends their results back to the `/chat/completions` endpoint,\nlooping as long as the model requests tool calls.\n\nIf you pass a `parse` function, it will automatically parse the `arguments` for you\nand returns any parsing errors to the model to attempt auto-recovery.\nOtherwise, the args will be passed to the function you provide as a string.\n\nIf you pass `tool_choice: {function: {name: \u2026}}` instead of `auto`,\nit returns immediately after calling that function (and only loops to auto-recover parsing errors).\n\n```ts\nimport OpenAI from 'openai';\n\nconst client = new OpenAI();\n\nasync function main() {\n  const runner = client.beta.chat.completions\n    .runTools({\n      model: 'gpt-3.5-turbo',\n      messages: [{ role: 'user', content: 'How is the weather this week?' }],\n      tools: [\n        {\n          type: 'function',\n          function: {\n            function: getCurrentLocation,\n            parameters: { type: 'object', properties: {} },\n          },\n        },\n        {\n          type: 'function',\n          function: {\n            function: getWeather,\n            parse: JSON.parse, // or use a validation library like zod for typesafe parsing.\n            parameters: {\n              type: 'object',\n              properties: {\n                location: { type: 'string' },\n              },\n            },\n          },\n        },\n      ],\n    })\n    .on('message', (message) => console.log(message));\n\n  const finalContent = await runner.finalContent();\n  console.log();\n  console.log('Final content:', finalContent);\n}\n\nasync function getCurrentLocation() {\n  return 'Boston'; // Simulate lookup\n}\n\nasync function getWeather(args: { location: string }) {\n  const { location } = args;\n  // \u2026 do lookup \u2026\n  return { temperature, precipitation };\n}\n\nmain();\n\n// {role: \"user\",      content: \"How's the weather this week?\"}\n// {role: \"assistant\", tool_calls: [{type: \"function\", function: {name: \"getCurrentLocation\", arguments: \"{}\"}, id: \"123\"}\n// {role: \"tool\",      name: \"getCurrentLocation\", content: \"Boston\", tool_call_id: \"123\"}\n// {role: \"assistant\", tool_calls: [{type: \"function\", function: {name: \"getWeather\", arguments: '{\"location\": \"Boston\"}'}, id: \"1234\"}]}\n// {role: \"tool\",      name: \"getWeather\", content: '{\"temperature\": \"50degF\", \"preciptation\": \"high\"}', tool_call_id: \"1234\"}\n// {role: \"assistant\", content: \"It's looking cold and rainy - you might want to wear a jacket!\"}\n//\n// Final content: \"It's looking cold and rainy - you might want to wear a jacket!\"\n```\n\nLike with `.stream()`, we provide a variety of [helpers and events](helpers.md#events).\n\nNote that `runFunctions` was previously available as well, but has been deprecated in favor of `runTools`.\n\nRead more about various examples such as with integrating with [zod](helpers.md#integrate-with-zod),\n[next.js](helpers.md#integrate-wtih-next-js), and [proxying a stream to the browser](helpers.md#proxy-streaming-to-a-browser).\n\n## File Uploads\n\nRequest parameters that correspond to file uploads can be passed in many different forms:\n\n- `File` (or an object with the same structure)\n- a `fetch` `Response` (or an object with the same structure)\n- an `fs.ReadStream`\n- the return value of our `toFile` helper\n\n```ts\nimport fs from 'fs';\nimport fetch from 'node-fetch';\nimport OpenAI, { toFile } from 'openai';\n\nconst openai = new OpenAI();\n\n// If you have access to Node `fs` we recommend using `fs.createReadStream()`:\nawait openai.files.create({ file: fs.createReadStream('input.jsonl'), purpose: 'fine-tune' });\n\n// Or if you have the web `File` API you can pass a `File` instance:\nawait openai.files.create({ file: new File(['my bytes'], 'input.jsonl'), purpose: 'fine-tune' });\n\n// You can also pass a `fetch` `Response`:\nawait openai.files.create({ file: await fetch('https://somesite/input.jsonl'), purpose: 'fine-tune' });\n\n// Finally, if none of the above are convenient, you can use our `toFile` helper:\nawait openai.files.create({\n  file: await toFile(Buffer.from('my bytes'), 'input.jsonl'),\n  purpose: 'fine-tune',\n});\nawait openai.files.create({\n  file: await toFile(new Uint8Array([0, 1, 2]), 'input.jsonl'),\n  purpose: 'fine-tune',\n});\n```\n\n## Handling errors\n\nWhen the library is unable to connect to the API,\nor if the API returns a non-success status code (i.e., 4xx or 5xx response),\na subclass of `APIError` will be thrown:\n\n<!-- prettier-ignore -->\n```ts\nasync function main() {\n  const job = await openai.fineTuning.jobs\n    .create({ model: 'gpt-3.5-turbo', training_file: 'file-abc123' })\n    .catch((err) => {\n      if (err instanceof OpenAI.APIError) {\n        console.log(err.status); // 400\n        console.log(err.name); // BadRequestError\n        console.log(err.headers); // {server: 'nginx', ...}\n      } else {\n        throw err;\n      }\n    });\n}\n\nmain();\n```\n\nError codes are as followed:\n\n| Status Code | Error Type                 |\n| ----------- | -------------------------- |\n| 400         | `BadRequestError`          |\n| 401         | `AuthenticationError`      |\n| 403         | `PermissionDeniedError`    |\n| 404         | `NotFoundError`            |\n| 422         | `UnprocessableEntityError` |\n| 429         | `RateLimitError`           |\n| >=500       | `InternalServerError`      |\n| N/A         | `APIConnectionError`       |\n\n### Azure OpenAI\n\nAn example of using this library with Azure OpenAI can be found [here](https://github.com/openai/openai-node/blob/master/examples/azure.ts).\n\nPlease note there are subtle differences in API shape & behavior between the Azure OpenAI API and the OpenAI API,\nso using this library with Azure OpenAI may result in incorrect types, which can lead to bugs.\n\nSee [`@azure/openai`](https://www.npmjs.com/package/@azure/openai) for an Azure-specific SDK provided by Microsoft.\n\n### Retries\n\nCertain errors will be automatically retried 2 times by default, with a short exponential backoff.\nConnection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict,\n429 Rate Limit, and >=500 Internal errors will all be retried by default.\n\nYou can use the `maxRetries` option to configure or disable this:\n\n<!-- prettier-ignore -->\n```js\n// Configure the default for all requests:\nconst openai = new OpenAI({\n  maxRetries: 0, // default is 2\n});\n\n// Or, configure per-request:\nawait openai.chat.completions.create({ messages: [{ role: 'user', content: 'How can I get the name of the current day in Node.js?' }], model: 'gpt-3.5-turbo' }, {\n  maxRetries: 5,\n});\n```\n\n### Timeouts\n\nRequests time out after 10 minutes by default. You can configure this with a `timeout` option:\n\n<!-- prettier-ignore -->\n```ts\n// Configure the default for all requests:\nconst openai = new OpenAI({\n  timeout: 20 * 1000, // 20 seconds (default is 10 minutes)\n});\n\n// Override per-request:\nawait openai.chat.completions.create({ messages: [{ role: 'user', content: 'How can I list all files in a directory using Python?' }], model: 'gpt-3.5-turbo' }, {\n  timeout: 5 * 1000,\n});\n```\n\nOn timeout, an `APIConnectionTimeoutError` is thrown.\n\nNote that requests which time out will be [retried twice by default](#retries).\n\n## Auto-pagination\n\nList methods in the OpenAI API are paginated.\nYou can use `for await \u2026 of` syntax to iterate through items across all pages:\n\n```ts\nasync function fetchAllFineTuningJobs(params) {\n  const allFineTuningJobs = [];\n  // Automatically fetches more pages as needed.\n  for await (const fineTuningJob of openai.fineTuning.jobs.list({ limit: 20 })) {\n    allFineTuningJobs.push(fineTuningJob);\n  }\n  return allFineTuningJobs;\n}\n```\n\nAlternatively, you can make request a single page at a time:\n\n```ts\nlet page = await openai.fineTuning.jobs.list({ limit: 20 });\nfor (const fineTuningJob of page.data) {\n  console.log(fineTuningJob);\n}\n\n// Convenience methods are provided for manually paginating:\nwhile (page.hasNextPage()) {\n  page = page.getNextPage();\n  // ...\n}\n```\n\n## Advanced Usage\n\n### Accessing raw Response data (e.g., headers)\n\nThe \"raw\" `Response` returned by `fetch()` can be accessed through the `.asResponse()` method on the `APIPromise` type that all methods return.\n\nYou can also use the `.withResponse()` method to get the raw `Response` along with the parsed data.\n\n<!-- prettier-ignore -->\n```ts\nconst openai = new OpenAI();\n\nconst response = await openai.chat.completions\n  .create({ messages: [{ role: 'user', content: 'Say this is a test' }], model: 'gpt-3.5-turbo' })\n  .asResponse();\nconsole.log(response.headers.get('X-My-Header'));\nconsole.log(response.statusText); // access the underlying Response object\n\nconst { data: chatCompletion, response: raw } = await openai.chat.completions\n  .create({ messages: [{ role: 'user', content: 'Say this is a test' }], model: 'gpt-3.5-turbo' })\n  .withResponse();\nconsole.log(raw.headers.get('X-My-Header'));\nconsole.log(chatCompletion);\n```\n\n## Customizing the fetch client\n\nBy default, this library uses `node-fetch` in Node, and expects a global `fetch` function in other environments.\n\nIf you would prefer to use a global, web-standards-compliant `fetch` function even in a Node environment,\n(for example, if you are running Node with `--experimental-fetch` or using NextJS which polyfills with `undici`),\nadd the following import before your first import `from \"OpenAI\"`:\n\n```ts\n// Tell TypeScript and the package to use the global web fetch instead of node-fetch.\n// Note, despite the name, this does not add any polyfills, but expects them to be provided if needed.\nimport 'openai/shims/web';\nimport OpenAI from 'openai';\n```\n\nTo do the inverse, add `import \"openai/shims/node\"` (which does import polyfills).\nThis can also be useful if you are getting the wrong TypeScript types for `Response` -\u00a0more details [here](https://github.com/openai/openai-node/tree/master/src/_shims#readme).\n\nYou may also provide a custom `fetch` function when instantiating the client,\nwhich can be used to inspect or alter the `Request` or `Response` before/after each request:\n\n```ts\nimport { fetch } from 'undici'; // as one example\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  fetch: async (url: RequestInfo, init?: RequestInfo): Promise<Response> => {\n    console.log('About to make a request', url, init);\n    const response = await fetch(url, init);\n    console.log('Got response', response);\n    return response;\n  },\n});\n```\n\nNote that if given a `DEBUG=true` environment variable, this library will log all requests and responses automatically.\nThis is intended for debugging purposes only and may change in the future without notice.\n\n## Configuring an HTTP(S) Agent (e.g., for proxies)\n\nBy default, this library uses a stable agent for all http/https requests to reuse TCP connections, eliminating many TCP & TLS handshakes and shaving around 100ms off most requests.\n\nIf you would like to disable or customize this behavior, for example to use the API behind a proxy, you can pass an `httpAgent` which is used for all requests (be they http or https), for example:\n\n<!-- prettier-ignore -->\n```ts\nimport http from 'http';\nimport HttpsProxyAgent from 'https-proxy-agent';\n\n// Configure the default for all requests:\nconst openai = new OpenAI({\n  httpAgent: new HttpsProxyAgent(process.env.PROXY_URL),\n});\n\n// Override per-request:\nawait openai.models.list({\n  baseURL: 'http://localhost:8080/test-api',\n  httpAgent: new http.Agent({ keepAlive: false }),\n})\n```\n\n## Semantic Versioning\n\nThis package generally follows [SemVer](https://semver.org/spec/v2.0.0.html) conventions, though certain backwards-incompatible changes may be released as minor versions:\n\n1. Changes that only affect static types, without breaking runtime behavior.\n2. Changes to library internals which are technically public but not intended or documented for external use. _(Please open a GitHub issue to let us know if you are relying on such internals)_.\n3. Changes that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an [issue](https://www.github.com/openai/openai-node/issues) with questions, bugs, or suggestions.\n\n## Requirements\n\nTypeScript >= 4.5 is supported.\n\nThe following runtimes are supported:\n\n- Node.js 18 LTS or later ([non-EOL](https://endoflife.date/nodejs)) versions.\n- Deno v1.28.0 or higher, using `import OpenAI from \"npm:openai\"`.\n- Bun 1.0 or later.\n- Cloudflare Workers.\n- Vercel Edge Runtime.\n- Jest 28 or greater with the `\"node\"` environment (`\"jsdom\"` is not supported at this time).\n- Nitro v2.6 or greater.\n\nNote that React Native is not supported at this time.\n\nIf you are interested in other runtime environments, please open or upvote an issue on GitHub.\n", "release_dates": ["2024-02-28T23:20:41Z", "2024-02-20T19:28:21Z", "2024-02-20T02:51:02Z", "2024-02-19T18:52:30Z", "2024-02-13T05:17:26Z", "2024-02-12T23:45:49Z", "2024-02-09T00:15:07Z", "2024-02-05T06:07:43Z", "2024-01-25T20:00:24Z", "2024-01-21T08:51:42Z", "2024-01-13T06:06:55Z", "2024-01-12T20:07:24Z", "2024-01-12T16:07:21Z", "2024-01-12T00:25:15Z", "2024-01-10T18:53:49Z", "2024-01-09T01:43:27Z", "2023-12-22T10:59:20Z", "2023-12-19T23:37:47Z", "2023-12-17T01:56:50Z", "2023-12-16T06:07:16Z", "2023-12-15T05:15:57Z", "2023-12-13T00:04:34Z", "2023-11-27T22:06:39Z", "2023-11-22T17:38:33Z", "2023-11-20T22:21:27Z", "2023-11-15T19:32:12Z", "2023-11-14T19:43:21Z", "2023-11-13T21:06:38Z", "2023-11-10T19:19:02Z", "2023-11-10T02:13:54Z"]}, {"name": "openai-openapi", "description": "OpenAPI specification for the OpenAI API", "language": null, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# OpenAPI spec for the OpenAI API\n\nThis repository contains an [OpenAPI](https://www.openapis.org/) specification for the [OpenAI API](https://platform.openai.com/docs/api-reference).\n", "release_dates": ["2023-06-19T21:44:06Z", "2023-06-13T17:14:47Z"]}, {"name": "openai-python", "description": "The official Python library for the OpenAI API", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# OpenAI Python API library\n\n[![PyPI version](https://img.shields.io/pypi/v/openai.svg)](https://pypi.org/project/openai/)\n\nThe OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+\napplication. The library includes type definitions for all request params and response fields,\nand offers both synchronous and asynchronous clients powered by [httpx](https://github.com/encode/httpx).\n\nIt is generated from our [OpenAPI specification](https://github.com/openai/openai-openapi) with [Stainless](https://stainlessapi.com/).\n\n## Documentation\n\nThe REST API documentation can be found [on platform.openai.com](https://platform.openai.com/docs). The full API of this library can be found in [api.md](api.md).\n\n## Installation\n\n> [!IMPORTANT]\n> The SDK was rewritten in v1, which was released November 6th 2023. See the [v1 migration guide](https://github.com/openai/openai-python/discussions/742), which includes scripts to automatically update your code.\n\n```sh\npip install openai\n```\n\n## Usage\n\nThe full API of this library can be found in [api.md](api.md).\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n```\n\nWhile you can provide an `api_key` keyword argument,\nwe recommend using [python-dotenv](https://pypi.org/project/python-dotenv/)\nto add `OPENAI_API_KEY=\"My API Key\"` to your `.env` file\nso that your API Key is not stored in source control.\n\n## Async usage\n\nSimply import `AsyncOpenAI` instead of `OpenAI` and use `await` with each API call:\n\n```python\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main() -> None:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n```\n\nFunctionality between the synchronous and asynchronous clients is otherwise identical.\n\n## Streaming Responses\n\nWe provide support for streaming responses using Server Side Events (SSE).\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n```\n\nThe async client uses the exact same interface.\n\n```python\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n        stream=True,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n```\n\n## Module-level client\n\n> [!IMPORTANT]\n> We highly recommend instantiating client instances instead of relying on the global client.\n\nWe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\n```py\nimport openai\n\n# optional; defaults to `os.environ['OPENAI_API_KEY']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `OpenAI` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n```\n\nThe API is the exact same as the standard client instance based API.\n\nThis is intended to be used within REPLs or notebooks for faster iteration, **not** in application code.\n\nWe recommend that you always instantiate a client (e.g., with `client = OpenAI()`) in application code because:\n\n- It can be difficult to reason about where client options are configured\n- It's not possible to change certain client options without potentially causing race conditions\n- It's harder to mock for testing purposes\n- It's not possible to control cleanup of network connections\n\n## Using types\n\nNested request parameters are [TypedDicts](https://docs.python.org/3/library/typing.html#typing.TypedDict). Responses are [Pydantic models](https://docs.pydantic.dev), which provide helper methods for things like:\n\n- Serializing back into JSON, `model.model_dump_json(indent=2, exclude_unset=True)`\n- Converting to a dictionary, `model.model_dump(exclude_unset=True)`\n\nTyped requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set `python.analysis.typeCheckingMode` to `basic`.\n\n## Pagination\n\nList methods in the OpenAI API are paginated.\n\nThis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\n```python\nimport openai\n\nclient = OpenAI()\n\nall_jobs = []\n# Automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # Do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n```\n\nOr, asynchronously:\n\n```python\nimport asyncio\nimport openai\n\nclient = AsyncOpenAI()\n\n\nasync def main() -> None:\n    all_jobs = []\n    # Iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n```\n\nAlternatively, you can use the `.has_next_page()`, `.next_page_info()`, or `.get_next_page()` methods for more granular control working with pages:\n\n```python\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# Remove `await` for non-async usage.\n```\n\nOr just work directly with the returned data:\n\n```python\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# Remove `await` for non-async usage.\n```\n\n## Nested params\n\nNested parameters are dictionaries, typed using `TypedDict`, for example:\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n```\n\n## File Uploads\n\nRequest parameters that correspond to file uploads can be passed as `bytes`, a [`PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) instance or a tuple of `(filename, contents, media type)`.\n\n```python\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclient.files.create(\n    file=Path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n```\n\nThe async client uses the exact same interface. If you pass a [`PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) instance, the file contents will be read asynchronously automatically.\n\n## Handling errors\n\nWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of `openai.APIConnectionError` is raised.\n\nWhen the API returns a non-success status code (that is, 4xx or 5xx\nresponse), a subclass of `openai.APIStatusError` is raised, containing `status_code` and `response` properties.\n\nAll errors inherit from `openai.APIError`.\n\n```python\nimport openai\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ntry:\n    client.fine_tuning.jobs.create(\n        model=\"gpt-3.5-turbo\",\n        training_file=\"file-abc123\",\n    )\nexcept openai.APIConnectionError as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying Exception, likely raised within httpx.\nexcept openai.RateLimitError as e:\n    print(\"A 429 status code was received; we should back off a bit.\")\nexcept openai.APIStatusError as e:\n    print(\"Another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n```\n\nError codes are as followed:\n\n| Status Code | Error Type                 |\n| ----------- | -------------------------- |\n| 400         | `BadRequestError`          |\n| 401         | `AuthenticationError`      |\n| 403         | `PermissionDeniedError`    |\n| 404         | `NotFoundError`            |\n| 422         | `UnprocessableEntityError` |\n| 429         | `RateLimitError`           |\n| >=500       | `InternalServerError`      |\n| N/A         | `APIConnectionError`       |\n\n### Retries\n\nCertain errors are automatically retried 2 times by default, with a short exponential backoff.\nConnection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict,\n429 Rate Limit, and >=500 Internal errors are all retried by default.\n\nYou can use the `max_retries` option to configure or disable retry settings:\n\n```python\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # default is 2\n    max_retries=0,\n)\n\n# Or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I get the name of the current day in Node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n```\n\n### Timeouts\n\nBy default requests time out after 10 minutes. You can configure this with a `timeout` option,\nwhich accepts a float or an [`httpx.Timeout`](https://www.python-httpx.org/advanced/#fine-tuning-the-configuration) object:\n\n```python\nfrom openai import OpenAI\n\n# Configure the default for all requests:\nclient = OpenAI(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# More granular control:\nclient = OpenAI(\n    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# Override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How can I list all files in a directory using Python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n```\n\nOn timeout, an `APITimeoutError` is thrown.\n\nNote that requests that time out are [retried twice by default](#retries).\n\n## Advanced\n\n### Logging\n\nWe use the standard library [`logging`](https://docs.python.org/3/library/logging.html) module.\n\nYou can enable logging by setting the environment variable `OPENAI_LOG` to `debug`.\n\n```shell\n$ export OPENAI_LOG=debug\n```\n\n### How to tell whether `None` means `null` or missing\n\nIn an API response, a field may be explicitly `null`, or missing entirely; in either case, its value is `None` in this library. You can differentiate the two cases with `.model_fields_set`:\n\n```py\nif response.my_field is None:\n  if 'my_field' not in response.model_fields_set:\n    print('Got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('Got json like {\"my_field\": null}.')\n```\n\n### Accessing raw response data (e.g. headers)\n\nThe \"raw\" Response object can be accessed by prefixing `.with_raw_response.` to any HTTP method call, e.g.,\n\n```py\nfrom openai import OpenAI\n\nclient = OpenAI()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('X-My-Header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n```\n\nThese methods return an [`LegacyAPIResponse`](https://github.com/openai/openai-python/tree/main/src/openai/_legacy_response.py) object. This is a legacy class as we're changing it slightly in the next major version.\n\nFor the sync client this will mostly be the same with the exception\nof `content` & `text` will be methods instead of properties. In the\nasync client, all methods will be async.\n\nA migration script will be provided & the migration in general should\nbe smooth.\n\n#### `.with_streaming_response`\n\nThe above interface eagerly reads the full response body when you make the request, which may not always be what you want.\n\nTo stream the response body, use `.with_streaming_response` instead, which requires a context manager and only reads the response body once you call `.read()`, `.text()`, `.json()`, `.iter_bytes()`, `.iter_text()`, `.iter_lines()` or `.parse()`. In the async client, these are async methods.\n\nAs such, `.with_streaming_response` methods return a different [`APIResponse`](https://github.com/openai/openai-python/tree/main/src/openai/_response.py) object, and the async client returns an [`AsyncAPIResponse`](https://github.com/openai/openai-python/tree/main/src/openai/_response.py) object.\n\n```python\nwith client.chat.completions.with_streaming_response.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n) as response:\n    print(response.headers.get(\"X-My-Header\"))\n\n    for line in response.iter_lines():\n        print(line)\n```\n\nThe context manager is required so that the response will reliably be closed.\n\n### Configuring the HTTP client\n\nYou can directly override the [httpx client](https://www.python-httpx.org/api/#client) to customize it for your use case, including:\n\n- Support for proxies\n- Custom transports\n- Additional [advanced](https://www.python-httpx.org/advanced/#client-instances) functionality\n\n```python\nimport httpx\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # Or use the `OPENAI_BASE_URL` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.Client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\n```\n\n### Managing HTTP resources\n\nBy default the library closes underlying HTTP connections whenever the client is [garbage collected](https://docs.python.org/3/reference/datamodel.html#object.__del__). You can manually close the client using the `.close()` method if desired, or with a context manager that closes when exiting.\n\n## Microsoft Azure OpenAI\n\nTo use this library with [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview), use the `AzureOpenAI`\nclass instead of the `OpenAI` class.\n\n> [!IMPORTANT]\n> The Azure API shape differs from the core API shape which means that the static types for responses / params\n> won't always be correct.\n\n```py\nfrom openai import AzureOpenAI\n\n# gets the API Key from environment variable AZURE_OPENAI_API_KEY\nclient = AzureOpenAI(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How do I output all files in a directory using Python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n```\n\nIn addition to the options provided in the base `OpenAI` client, the following options are provided:\n\n- `azure_endpoint` (or the `AZURE_OPENAI_ENDPOINT` environment variable)\n- `azure_deployment`\n- `api_version` (or the `OPENAI_API_VERSION` environment variable)\n- `azure_ad_token` (or the `AZURE_OPENAI_AD_TOKEN` environment variable)\n- `azure_ad_token_provider`\n\nAn example of using the client with Azure Active Directory can be found [here](https://github.com/openai/openai-python/blob/main/examples/azure_ad.py).\n\n## Versioning\n\nThis package generally follows [SemVer](https://semver.org/spec/v2.0.0.html) conventions, though certain backwards-incompatible changes may be released as minor versions:\n\n1. Changes that only affect static types, without breaking runtime behavior.\n2. Changes to library internals which are technically public but not intended or documented for external use. _(Please open a GitHub issue to let us know if you are relying on such internals)_.\n3. Changes that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an [issue](https://www.github.com/openai/openai-python/issues) with questions, bugs, or suggestions.\n\n## Requirements\n\nPython 3.7 or higher.\n", "release_dates": ["2024-02-28T23:22:00Z", "2024-02-20T19:26:46Z", "2024-02-20T17:23:23Z", "2024-02-20T02:51:29Z", "2024-02-09T00:14:26Z", "2024-02-04T19:12:08Z", "2024-02-03T05:06:51Z", "2024-01-25T20:00:35Z", "2024-01-21T08:49:38Z", "2024-01-16T21:16:23Z", "2024-01-12T16:07:32Z", "2024-01-10T19:06:45Z", "2024-01-09T01:43:00Z", "2023-12-22T11:06:33Z", "2023-12-19T23:37:38Z", "2023-12-17T01:57:28Z", "2023-12-15T05:16:40Z", "2023-12-13T01:31:07Z", "2023-12-09T02:21:00Z", "2023-12-01T18:47:58Z", "2023-11-29T01:31:09Z", "2023-11-22T17:45:51Z", "2023-11-21T08:29:40Z", "2023-11-17T23:36:00Z", "2023-11-16T20:00:42Z", "2023-11-16T17:31:05Z", "2023-11-15T18:12:25Z", "2023-11-13T18:25:43Z", "2023-11-10T18:52:30Z", "2023-11-09T21:48:21Z"]}, {"name": "openai-quickstart-node", "description": "Node.js example app from the OpenAI API quickstart tutorial", "language": "JavaScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# OpenAI API Quickstart - Node.js example app\n\nThis is an example chat app intended to get you started with your first OpenAI API project. It uses the [Chat Completions API](https://platform.openai.com/docs/api-reference/chat) to create a simple general purpose chat app with streaming.\n\n## Basic request\n\nTo send your first API request with the [OpenAI Node SDK](https://github.com/openai/openai-node), make sure you have the right [dependencies installed](https://platform.openai.com/docs/quickstart?context=node) and then run the following code:\n\n```python\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [{ role: \"system\", content: \"You are a helpful assistant.\" }],\n    model: \"gpt-3.5-turbo\",\n  });\n\n  console.log(completion.choices[0]);\n}\n\nmain();\n```\n\nThis quickstart app builds on top of the example code above, with streaming and a UI to visualize messages.\n\n## Setup\n\n1. If you don\u2019t have Node.js installed, install it from [nodejs.org](https://nodejs.org/en/) (Node.js version >= 16.0.0 required)\n\n2. Clone this repository\n\n3. Navigate into the project directory\n\n   ```bash\n   $ cd openai-quickstart-node\n   ```\n\n4. Install the requirements\n\n   ```bash\n   $ npm install\n   ```\n\n5. Make a copy of the example environment variables file\n\n   On Linux systems: \n   ```bash\n   $ cp .env.example .env\n   ```\n   On Windows:\n   ```powershell\n   $ copy .env.example .env\n   ```\n6. Add your [API key](https://platform.openai.com/account/api-keys) to the newly created `.env` file\n\n7. Run the app\n\n   ```bash\n   $ npm run dev\n   ```\n\nYou should now be able to access the app at [http://localhost:3000](http://localhost:3000)! For the full context behind this example app, check out the [tutorial](https://platform.openai.com/docs/quickstart).\n", "release_dates": []}, {"name": "openai-quickstart-python", "description": "Python example app from the OpenAI API quickstart tutorial", "language": null, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# OpenAI API Quickstart - Python\n\nThis repository hosts multiple quickstart apps for different OpenAI API endpoints (chat, assistants, etc). Check out the `examples` folder to try out different examples and get started using the OpenAI API.\n\n## Basic request\n\nTo send your first API request with the [OpenAI Python SDK](https://github.com/openai/openai-python), make sure you have the right [dependencies installed](https://platform.openai.com/docs/quickstart?context=python) and then run the following code:\n\n```python\nfrom openai import OpenAI\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n```\n\n## Setup\n\n1. If you don\u2019t have Python installed, install it [from Python.org](https://www.python.org/downloads/).\n\n2. [Clone](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) this repository.\n\n3. Navigate into the project directory:\n\n   ```bash\n   $ cd openai-quickstart-python\n   ```\n\n4. Create a new virtual environment:\n\n   - macOS:\n\n     ```bash\n     $ python -m venv venv\n     $ . venv/bin/activate\n     ```\n\n   - Windows:\n     ```cmd\n     > python -m venv venv\n     > .\\venv\\Scripts\\activate\n     ```\n\n5. Install the requirements:\n\n   ```bash\n   $ pip install -r requirements.txt\n   ```\n\n6. Make a copy of the example environment variables file:\n\n   ```bash\n   $ cp .env.example .env\n   ```\n\n7. Add your [API key](https://platform.openai.com/api-keys) to the newly created `.env` file.\n\n8. Run the app:\n\nThis step depends on the app itself. If the code uses flask (like the chat-basic example), you can run:\n\n```bash\n$ flask run\n```\n\nYou should now be able to access the app from your browser at the following URL: [http://localhost:5000](http://localhost:5000)!\n\nIf the code is just a simple Python script, you can run it with:\n\n```bash\n$ python my_file.py\n```\n", "release_dates": []}, {"name": "orrb", "description": "Code for the paper \"OpenAI Remote Rendering Backend\"", "language": "C#", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# ORRB\nOpenAI Remote Rendering Backend\n\n![OpenAI Remote Rendering Backend](/docs/orrb.png?raw=true \"Orrb\")\n\nORRB is a high performance rendering tool that can be used as a source of synthetic image data in training of ML models.\n\n## Paper\nMore technical details are available in [ORRB Technical Report](https://arxiv.org/abs/1906.11633). Please cite this as:\n```\n@inproceedings{orrb2019,\n  author = {{Maciek Chociej}, {Peter Welinder}, {Lilian Weng}},\n  title = {ORRB: OpenAI Remote Rendering Backend},\n  year = {2019},\n  month = june,\n  booktitle = {eprint arXiv},\n  eprint = {arXiv:1906.11633},\n  url = {https://arxiv.org/abs/1906.11633}\n}\n```\n\n## License\nThis project is released under the MIT License. Please review the [License file](LICENSE.md) for more details.\n\n# Installation\n\nThis code runs on Mac OS X and Linux with Python 3.6.5.  The Linux version requires a running X11 server with OpenGL support.\n\nFirst, install [`pyenv`](https://github.com/pyenv/pyenv#installation).\n\nThen prepare a clean python environment & activate it:\n\n```\npyenv virtualenv 3.6.5 orrb\npyenv activate orrb\n```\n\nClone the repository:\n\n```\ngit clone https://github.com/openai/orrb.git\ncd orrb\n```\n\nInstall prerequisites:\n\n```\npip install -r requirements.txt\n```\n\nInstall pre-built binaries:\n\n```\ncd bin\n./install-prebuilt.sh\ncd ..\n```\n\nYou should see something similar to:\n\n```\n  Execute or add this to your .bashrc/.bashprofile.\n  export ORRB_BINARIES_DIR=\"/Users/mac/code/orrb/bin\"\n```\n\nAdd the required environment variable:\n\n```\nexport ORRB_BINARIES_DIR=\"/Users/mac/code/orrb/bin\"\n```\n\nFinally install the python package (in the root orrb directory):\n\n```\npip install -e .\n```\n\n# Demos\n\n## Interactive\n\nThis demo opens the renderer in interactive mode. You can cycle through the available cameras and manually tweak and run the configured randomizers.\n\n```\npython bin/demo.py interactive\n```\n\n![OpenAI Remote Rendering Backend](/docs/interactive.png?raw=true \"Orrb\")\n\n## Batch\n\nThis demo will render batches of images and output them in a simple OpenCV window.\n\n```\npython bin/demo.py batch\n```\n\n## Batch (with config updates)\n\nThis demo will render batches of images updating the renderer config once in a while.\n\n```\npython bin/demo.py batch-update\n```\n\n## Gym\n\nThis demo will open the gym environment in interactive renderer mode. The `asset-basedir` needs to be passed so that the renderer knows where to look for the `.stl` files (unfortunately the MuJoCo XML loses that information).\n\n```\nOPENAI_GYM_PATH=$(python -c \"import gym; print(gym.__path__[0])\")\npython bin/demo.py gymenv FetchSlide-v1 --asset-basedir=$OPENAI_GYM_PATH/envs/robotics/assets/fetch/\n```\n\n## Benchmark\n\nThis script spawns multiple render servers and executes batch render requests as fast as possible.\n\nFor example, in order to run on GPUs 0 through 3 with 2 workers per gpu (8 render servers in total) execute:\n\n```\npython bin/benchmark.py --num-gpus=4 --workers-per-gpu=2\n```\n\nThis demo supports MPI and will distribute the workload / render servers between MPI processes, e.g.:\n\n```\nmpirun -n 8 bin/benchmark.py --num-gpus=8 --workers-per-gpu=4\n```\n\nwill spawn 8 processes that talk to 32 render servers running across 8 GPUs.\n\n## Keras\n\nWe have provided a sample application that uses ORRB and Keras to train a simple vision predictor. It takes the example environment (with the hand and the cube), and a sample batch of states. The training set is constructed\nby augmenting this example batch with: randomized block rotation, perturbed block position and perturbed robot joint positions. The unaugmented sample batch (with rendering randomizer seed constant at 0) is used as the test set.\nWe use a simple convolutional network with the Adam optimizer to predict the screen position of the block. To run the demo execute:\n\n```\npython bin/demo_keras.py\n```\n\n# Linux\n\nIn order to run this on Linux, you need a X11 display. In order to run on different GPUs configure a screen per gpu (internally Orrb uses the `:ORRB_DISPLAY.GPU_NUM` notation to assign different GPUs to different render servers).\nUse the `ORRB_DISPLAY` environment variable to specify which display to use (default is `:0`). We use the NVidia virtual frame buffers and None display device mode to run headless in datacenters, see [Remote Visualization on Tesla GPUs](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/remote-viz-tesla-gpus.pdf) for more details.\n\n# Building the Unity binaries\n\nThe `unity` directory contains the Standalone Renderer project. In `Build` directory you will find a script that builds the binaries:\n\n```\ncd unity/Builds\n./build.sh 20181009\n```\n\nThe parameter is the version id, that can be used to distinguish between builds in the client python code. Note that you cannot run the Unity Editor while building.\n\nThe script requires that the `UNITY_CMD` environment variable is set, you will need to point it to where Unity is installed, e.g.:\n\n```\nexport UNITY_CMD=\"/Applications/Unity/Unity.app/Contents/MacOS/Unity\"\n```\n\n# Useful Environment Variables\n\n- `ORRB_DETACHED=1`: A flag that makes the job wait for you to start the server manually (in commandline or from the Unity Editor).\n- `ORRB_MINIMAL=<int>`: A flag that defines the maximum number of local renderers running at the same time.\n- `ORRB_BINARIES_DIR=<path>` : A flag that determines where the renderer binaries are located.\n- `ORRB_DISPLAY=<int>` : A flag that defines which X11 display should be used for the render servers.\n- `UNITY_CMD=<path>` : Used by the build script, points to the Unity Editor binary.\n\n# External code\n\n- GRPC - [grpc.io](https://grpc.io/)\n- Protobuf - [developers.google.com/protocol-buffers](https://developers.google.com/protocol-buffers/)\n- Stl Importer / Exporter - [git](https://github.com/karl-/pb_Stl)\n- MIConvexHull - [git](https://designengrlab.github.io/MIConvexHull/)\n- FBX Exporter - [git](https://github.com/KellanHiggins/UnityFBXExporter)\n- PostProcessingV2 - [git](https://github.com/Unity-Technologies/PostProcessing)\n", "release_dates": []}, {"name": "ot-gan", "description": " Code for the paper \"Improving GANs Using Optimal Transport\"", "language": "Jupyter Notebook", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Improving GANs Using Optimal Transport\n\nCode for the paper [Improving GANs Using Optimal Transport](https://arxiv.org/abs/1803.05573)\n", "release_dates": []}, {"name": "pachi-py", "description": "Python wrappers for Pachi. Contains a modified version of the bleeding-edge Pachi source code.", "language": "C", "license": {"key": "gpl-2.0", "name": "GNU General Public License v2.0", "spdx_id": "GPL-2.0", "url": "https://api.github.com/licenses/gpl-2.0", "node_id": "MDc6TGljZW5zZTg="}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# pachi_py\n\n[![Build Status](https://travis-ci.org/openai/pachi-py.svg?branch=master)](https://travis-ci.org/openai/pachi-py)\n\nPython wrappers for [Pachi](http://pachi.or.cz/).\nContains a modified version of the [bleeding-edge](https://github.com/pasky/pachi/tree/d87f6899b38cb89a9eee7acaa44dd14d3496e772)\nPachi source code.\n", "release_dates": []}, {"name": "phasic-policy-gradient", "description": "Code for the paper \"Phasic Policy Gradient\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Phasic Policy Gradient\n\n#### [[Paper]](https://arxiv.org/abs/2009.04416)\n\nThis is code for training agents using [Phasic Policy Gradient](https://arxiv.org/abs/2009.04416) [(citation)](#citation).\n\nSupported platforms:\n\n- macOS 10.14 (Mojave)\n- Ubuntu 16.04\n\nSupported Pythons:\n\n- 3.7 64-bit\n\n## Install\n\nYou can get miniconda from https://docs.conda.io/en/latest/miniconda.html if you don't have it, or install the dependencies from [`environment.yml`](environment.yml) manually.\n\n```\ngit clone https://github.com/openai/phasic-policy-gradient.git\nconda env update --name phasic-policy-gradient --file phasic-policy-gradient/environment.yml\nconda activate phasic-policy-gradient\npip install -e phasic-policy-gradient\n```\n\n## Reproduce and Visualize Results\n\nPPG with default hyperparameters (results/ppg-runN):\n\n```\nmpiexec -np 4 python -m phasic_policy_gradient.train\npython -m phasic_policy_gradient.graph --experiment_name ppg\n```\n\nPPO baseline (results/ppo-runN):\n\n```\nmpiexec -np 4 python -m phasic_policy_gradient.train --n_epoch_pi 3 --n_epoch_vf 3 --n_aux_epochs 0 --arch shared\npython -m phasic_policy_gradient.graph --experiment_name ppo\n```\n\nPPG, varying E_pi (results/e-pi-N):\n\n```\nmpiexec -np 4 python -m phasic_policy_gradient.train --n_epoch_pi N\npython -m phasic_policy_gradient.graph --experiment_name e_pi\n```\n\nPPG, varying E_aux (results/e-aux-N):\n\n```\nmpiexec -np 4 python -m phasic_policy_gradient.train --n_aux_epochs N\npython -m phasic_policy_gradient.graph --experiment_name e_aux\n```\n\nPPG, varying N_pi (results/n-pi-N):\n\n```\nmpiexec -np 4 python -m phasic_policy_gradient.train --n_pi N\npython -m phasic_policy_gradient.graph --experiment_name n_pi\n```\n\nPPG, using L_KL instead of L_clip (results/ppgkl-runN):\n\n```\nmpiexec -np 4 python -m phasic_policy_gradient.train --clip_param 0 --kl_penalty 1\npython -m phasic_policy_gradient.graph --experiment_name ppgkl\n```\n\nPPG, single network variant (results/ppgsingle-runN):\n\n```\nmpiexec -np 4 python -m phasic_policy_gradient.train --arch detach\npython -m phasic_policy_gradient.graph --experiment_name ppg_single_network\n```\n\nPass `--normalize_and_reduce` to compute and visualize the mean normalized return with `phasic_policy_gradient.graph`.\n\n# Citation\n\nPlease cite using the following bibtex entry:\n\n```\n@article{cobbe2020ppg,\n  title={Phasic Policy Gradient},\n  author={Cobbe, Karl and Hilton, Jacob and Klimov, Oleg and Schulman, John},\n  journal={arXiv preprint arXiv:2009.04416},\n  year={2020}\n}\n```\n", "release_dates": []}, {"name": "pixel-cnn", "description": "Code for the paper \"PixelCNN++: A PixelCNN Implementation with Discretized Logistic Mixture Likelihood and Other Modifications\"", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n\n# pixel-cnn++\n\nThis is a Python3 / [Tensorflow](https://www.tensorflow.org/) implementation \nof [PixelCNN++](https://openreview.net/pdf?id=BJrFC6ceg), as described in the following \npaper:\n\n**PixelCNN++: A PixelCNN Implementation with Discretized Logistic Mixture Likelihood and Other Modifications**, by\nTim Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma, and Yaroslav Bulatov.\n\nOur work builds on PixelCNNs that were originally proposed in [van der Oord et al.](https://arxiv.org/abs/1606.05328) \nin June 2016. PixelCNNs are a class of powerful generative models with tractable \nlikelihood that are also easy to sample from. The core convolutional neural network\ncomputes a probability distribution over a value of one pixel conditioned on the values\nof pixels to the left and above it. Below are example samples from a model\ntrained on CIFAR-10 that achieves **2.92 bits per dimension** (compared to 3.03 of \nthe PixelCNN in van der Oord et al.):\n\nSamples from the model (**left**) and samples from a model that is conditioned\non the CIFAR-10 class labels (**right**):\n\n![Improved PixelCNN papers](data/pixelcnn_samples.png)\n\nThis code supports multi-GPU training of our improved PixelCNN on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\nand [Small ImageNet](http://image-net.org/small/download.php), but is easy to adapt\nfor additional datasets. Training on a machine with 8 Maxwell TITAN X GPUs achieves\n3.0 bits per dimension in about 10 hours and it takes approximately 5 days to converge to 2.92.\n\n## Setup\n\nTo run this code you need the following:\n\n- a machine with multiple GPUs\n- Python3\n- Numpy, TensorFlow and imageio packages:\n```\npip install numpy tensorflow-gpu imageio\n```\n\n## Training the model\n\nUse the `train.py` script to train the model. To train the default model on \nCIFAR-10 simply use:\n\n```\npython3 train.py\n```\n\nYou might want to at least change the `--data_dir` and `--save_dir` which\npoint to paths on your system to download the data to (if not available), and\nwhere to save the checkpoints.\n\n**I want to train on fewer GPUs**. To train on fewer GPUs we recommend using `CUDA_VISIBLE_DEVICES` \nto narrow the visibility of GPUs to only a few and then run the script. Don't forget to modulate\nthe flag `--nr_gpu` accordingly.\n\n**I want to train on my own dataset**. Have a look at the `DataLoader` classes\nin the `data/` folder. You have to write an analogous data iterator object for\nyour own dataset and the code should work well from there.\n\n## Pretrained model checkpoint\n\nYou can download our pretrained (TensorFlow) model that achieves 2.92 bpd on CIFAR-10 [here](http://alpha.openai.com/pxpp.zip) (656MB).\n\n## Citation\n\nIf you find this code useful please cite us in your work:\n\n```\n@inproceedings{Salimans2017PixeCNN,\n  title={PixelCNN++: A PixelCNN Implementation with Discretized Logistic Mixture Likelihood and Other Modifications},\n  author={Tim Salimans and Andrej Karpathy and Xi Chen and Diederik P. Kingma},\n  booktitle={ICLR},\n  year={2017}\n}\n```\n# pixel-cnn-rotations\n", "release_dates": []}, {"name": "plugins-quickstart", "description": "Get a ChatGPT plugin up and running in under 5 minutes!", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# ChatGPT plugins quickstart\n\nGet a TODO list ChatGPT plugin up and running in under 5 minutes using Python. This plugin is designed to work in conjunction with the [ChatGPT plugins documentation](https://platform.openai.com/docs/plugins). Plugins have been superseded by GPTs, learn more about [creating a GPT with actions](https://platform.openai.com/docs/actions/introduction).\n\n## Setup locally\n\nTo install the required packages for this plugin, run the following command:\n\n```bash\npip install -r requirements.txt\n```\n\nTo run the plugin, enter the following command:\n\n```bash\npython main.py\n```\n\nOnce the local server is running:\n\n1. Navigate to https://chat.openai.com. \n2. In the Model drop down, select \"Plugins\" (note, if you don't see it there, you don't have access yet).\n3. Select \"Plugin store\"\n4. Select \"Develop your own plugin\"\n5. Enter in `localhost:5003` since this is the URL the server is running on locally, then select \"Find manifest file\".\n\nThe plugin should now be installed and enabled! You can start with a question like \"What is on my todo list\" and then try adding something to it as well! \n\n## Getting help\n\nIf you run into issues or have questions building a plugin, please join our [Developer community forum](https://community.openai.com/c/chat-plugins/20).\n", "release_dates": []}, {"name": "point-e", "description": "Point cloud diffusion for 3D model synthesis", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Point\u00b7E\n\n![Animation of four 3D point clouds rotating](point_e/examples/paper_banner.gif)\n\nThis is the official code and model release for [Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751).\n\n# Usage\n\nInstall with `pip install -e .`.\n\nTo get started with examples, see the following notebooks:\n\n * [image2pointcloud.ipynb](point_e/examples/image2pointcloud.ipynb) - sample a point cloud, conditioned on some example synthetic view images.\n * [text2pointcloud.ipynb](point_e/examples/text2pointcloud.ipynb) - use our small, worse quality pure text-to-3D model to produce 3D point clouds directly from text descriptions. This model's capabilities are limited, but it does understand some simple categories and colors.\n * [pointcloud2mesh.ipynb](point_e/examples/pointcloud2mesh.ipynb) - try our SDF regression model for producing meshes from point clouds.\n\nFor our P-FID and P-IS evaluation scripts, see:\n\n * [evaluate_pfid.py](point_e/evals/scripts/evaluate_pfid.py)\n * [evaluate_pis.py](point_e/evals/scripts/evaluate_pis.py)\n\nFor our Blender rendering code, see [blender_script.py](point_e/evals/scripts/blender_script.py)\n\n# Samples\n\nYou can download the seed images and point clouds corresponding to the paper banner images [here](https://openaipublic.azureedge.net/main/point-e/banner_pcs.zip).\n\nYou can download the seed images used for COCO CLIP R-Precision evaluations [here](https://openaipublic.azureedge.net/main/point-e/coco_images.zip).\n", "release_dates": []}, {"name": "post--example", "description": "Example Distill article repository\u2014clone, rename, start writing!", "language": "TeX", "license": {"key": "cc-by-4.0", "name": "Creative Commons Attribution 4.0 International", "spdx_id": "CC-BY-4.0", "url": "https://api.github.com/licenses/cc-by-4.0", "node_id": "MDc6TGljZW5zZTI1"}, "readme": "# Distill post--example\n\nDistill articles depend on [distillpub/template](https://github.com/distillpub/template) for styling and some functionality such as footnotes, citations, and math rendering. We built template as a standalone library of styles and [webcomponents](https://www.webcomponents.org/) to allow you to use any web development workflow you'd like. But what if you don't have strong opinions about that and just want a starter kit that works out of the box? This is such a starter kit.\n\nThis is using `webpack` for bundling, `svelte` & `svelte-loader` to build interactive components/diagrams, and `ejs` to inline SVGs\u2014the same technology choices we used when building ambitious articles such as [Building Blocks of Interpretability](https://distill.pub/2018/building-blocks).\n\n## Get started\n\nFork and rename, or simply copy this repository.\n\n### First time setup\n\n- [ ] Clone your fork or copy of this repository: `git clone \u2026`\n- [ ] [Install `npm` (Node Packet Manager)](https://docs.npmjs.com/getting-started/installing-node#installing-npm-from-the-nodejs-site). If you're on a Mac and have `brew`: `brew install node` will do the job.\n- [ ] `cd post--example` into the root directory of this project.\n- [ ] `npm install` to install dependencies.\n- [ ] Execute `npm run dev` to run a development server that autoreloads when you make changes to the article (\"hot-reloading\"). The console output will link you to a hot-reloading preview of the article.\n- [ ] Your article text is in `src/index.ejs`.\n\nComponents and diagrams are stored in `src`. The `.html` files are [svelte](https://svelte.technology/guide) components, the `.js` files are compilation endpoints that are also defined in `webpack.config.js`. These compiled endpoints are then consumed by hand authored `.ejs` files in `src`, such as `index.ejs` which contains your content. For most articles you can simply use the provided `index.ejs` for your text, `index.js` to instantiate diagrams, and `.html` svelte files to create them.\n\n\n## Feedback\n\nThis is a new template, and we don't yet have a lot of documentation on it. Please bear with us while we work to improve it!\n\nPlease [join our Distill Slack workspace](https://join.slack.com/t/distillpub/shared_invite/enQtMzg1NzU3MzEzMTg3LWJkNmQ4Y2JlNjJkNDlhYTU2ZmQxMGFkM2NiMTI2NGVjNzJkOTdjNTFiOGZmNDBjNTEzZGUwM2U0Mzg4NDAyN2E) if you have any questions. [Open an issue](https://github.com/distillpub/post--example/issues) if you'd like to see something improved!\n", "release_dates": []}, {"name": "ppo-ewma", "description": "Code for the paper \"Batch size invariance for policy optimization\"", "language": "Jupyter Notebook", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# PPO-EWMA\n\n#### [ [Paper] ](https://arxiv.org/abs/2110.00641)\n\nThis is code for training agents using PPO-EWMA and PPG-EWMA, introduced in the paper [Batch size-invariance for policy optimization](https://arxiv.org/abs/2110.00641) ([citation](#citation)). It is based on the code for [Phasic Policy Gradient](https://github.com/openai/phasic-policy-gradient).\n\n## Installation\n\nSupported platforms: MacOS and Ubuntu, Python 3.7\n\nInstallation using [Miniconda](https://docs.conda.io/en/latest/miniconda.html):\n\n```\ngit clone https://github.com/openai/ppo-ewma.git\nconda env update --name ppo-ewma --file ppo-ewma/environment.yml\nconda activate ppo-ewma\npip install -e ppo-ewma\n```\n\nAlternatively, install the dependencies from [`environment.yml`](environment.yml) manually.\n\n## Visualize results\n\nResults are stored in blob storage at `https://openaipublic.blob.core.windows.net/rl-batch-size-invariance/`, and can be visualized as in the paper using [this Colab notebook](https://colab.research.google.com/github/openai/ppo-ewma/blob/master/notebooks/rl_batch_size_invariance.ipynb).\n\n## Citation\n\nPlease cite using the following BibTeX entry:\n```\n@article{hilton2021batch,\n  title={Batch size-invariance for policy optimization},\n  author={Hilton, Jacob and Cobbe, Karl and Schulman, John},\n  journal={arXiv preprint arXiv:2110.00641},\n  year={2021}\n}\n```\n", "release_dates": []}, {"name": "prm800k", "description": "800,000 step-level correctness labels on LLM solutions to MATH problems", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# PRM800K: A Process Supervision Dataset\n\n#### [[Blog Post]](https://openai.com/research/improving-mathematical-reasoning-with-process-supervision) [[Paper]](https://arxiv.org/abs/2305.20050)\n\nThis repository accompanies the paper [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050) and presents the PRM800K dataset introduced there. PRM800K is a process supervision dataset containing 800,000 step-level correctness labels for model-generated solutions to problems from the [MATH](https://github.com/hendrycks/math) dataset. More information on PRM800K and the project can be found in the paper.\n\nWe are releasing the raw labels as well as the instructions we gave labelers during phase 1 and phase 2 of the project. Example labels can be seen in the image below.\n\n<p align=\"center\">\n    <img src=\"prm800k/img/interface.png\" height=\"300\"/>\n</p>\n\n\n## Data\n\nThe `data/` folder contains our labels formatted as newline-delimited lists of `json` data. The data has been uploaded with [Git LFS](https://git-lfs.com/), which you'll need to install in order to properly clone the repository.\n\nEach line represents 1 full solution sample and can contain many step-level labels. Here is one annotated line:\n\n\n```javascript\n{\n  // UUID representing a particular labeler.\n  \"labeler\": \"340d89bc-f5b7-45e9-b272-909ba68ee363\",\n\n  // The timestamp this trajectory was submitted.\n  \"timestamp\": \"2023-01-22T04:34:27.052924\",\n\n  // In phase 2, we split our data collection into generations, using our best\n  // PRM so far to pick which solutions to score in the next generation.\n  // In phase 1, this value should always be null.\n  \"generation\": 9,\n\n  // In each generation, we reserve some solutions for quality control. We serve\n  // these solutions to every labeler, and check that they agree with our\n  // gold labels.\n  \"is_quality_control_question\": false,\n\n  // generation -1 was reserved for a set of 30 questions we served every\n  // labeler in order to screen for base task performance.\n  \"is_initial_screening_question\": false,\n\n  // Metadata about the question this solution is a response to.\n  \"question\": {\n    // Text of the MATH problem being solved.\n    \"problem\": \"What is the greatest common factor of $20 !$ and $200,\\\\!000$?  (Reminder: If $n$ is a positive integer, then $n!$ stands for the product $1\\\\cdot 2\\\\cdot 3\\\\cdot \\\\cdots \\\\cdot (n-1)\\\\cdot n$.)\",\n    // Ground truth solution from the MATH dataset.\n    \"ground_truth_solution\": \"The prime factorization of $200,000$ is $2^6 \\\\cdot 5^5$. Then count the number of factors of $2$ and $5$ in $20!$. Since there are $10$ even numbers, there are more than $6$ factors of $2$. There are $4$ factors of $5$. So the greatest common factor is $2^6 \\\\cdot 5^4=\\\\boxed{40,\\\\!000}$.\",\n    // Ground truth answer.\n    \"ground_truth_answer\": \"40,\\\\!000\",\n\n    // The full steps of the model-generated solution. This is only set for\n    // phase 2 where we pre-generated all solutions that we labeled.\n    \"pre_generated_steps\": [\n      \"I want to find the largest positive integer that divides both $20 !$ and $200,\\\\!000$ evenly.\",\n      \"One way to do this is to factor both numbers into prime factors and look for the common ones.\",\n      \"I know that $200,\\\\!000 = 2^5\\\\cdot 10^4 = 2^9\\\\cdot 5^4$.\",\n      \"To find the prime factorization of $20 !$, I can use the fact that it is the product of all the positive integers from $1$ to $20$.\",\n      \"For each prime number $p$ between $1$ and $20$, I can count how many multiples of $p$ are in that range.\",\n      \"For example, there are $10$ multiples of $2$ between $1$ and $20$, namely $2, 4, 6, \\\\dots, 20$.\",\n      \"But there are also $5$ multiples of $4$, which is $2^2$, and $2$ multiples of $8$, which is $2^3$, and $1$ multiple of $16$, which is $2^4$.\",\n      \"So, the total power of $2$ in $20 !$ is $10 + 5 + 2 + 1 = 18$.\",\n      \"Similarly, there are $4$ multiples of $5$, namely $5, 10, 15, 20$, so the power of $5$ in $20 !$ is $4$.\",\n      \"There are $6$ multiples of $3$, namely $3, 6, 9, \\\\dots, 18$, but there are also $2$ multiples of $9$, which is $3^2$, so the power of $3$ in $20 !$ is $6 + 2 = 8$.\",\n      \"There are $2$ multiples of $7$, namely $7$ and $14$, so the power of $7$ in $20 !$ is $2$.\",\n      \"There are $1$ multiple of each of the other prime numbers $11, 13, 17$, and $19$, so the powers of those primes in $20 !$ are $1$ each.\",\n      \"Therefore, the prime factorization of $20 !$ is $2^{18}\\\\cdot 3^8\\\\cdot 5^4\\\\cdot 7^2\\\\cdot 11\\\\cdot 13\\\\cdot 17\\\\cdot 19$.\",\n      \"To find the greatest common factor of $20 !$ and $200,\\\\!000$, I need to take the lowest power of each common prime factor.\",\n      \"The only common prime factors are $2$ and $5$, and the lowest powers are $9$ and $4$, respectively.\",\n      \"So, the greatest common factor is $2^9\\\\cdot 5^4 = 512\\\\cdot 625 = 320,\\\\!000$.\\n\\n# Answer\\n\\n320,000\"\n    ],\n    // The answer given as the end of the pre-generated solution. We can see\n    // this solution is incorrect.\n    \"pre_generated_answer\": \"320,000\",\n    // The score given by our PRM to this solution. This one isn't rated very\n    // highly!\n    \"pre_generated_verifier_score\": 0.010779580529581414\n  },\n\n  // The human data we collected for this solution, containing correctness\n  // labels for each step of the solution.\n  \"label\": {\n    \"steps\": [\n      // Each object here represents labels for one step of the solution.\n      {\n        // Each step will contain one or more completions. These are candidate\n        // steps the model output at this step of the trajectory. In phase 1,\n        // we frequently collect labels on alternative steps, while in phase 2\n        // we only collect labels on alternative steps after the first mistake,\n        // so most completions lists are singletons.\n        \"completions\": [\n          {\n            // Text of the step.\n            \"text\": \"I want to find the largest positive integer that divides both $20 !$ and $200,\\\\!000$ evenly.\",\n            // The rating the labeler gave to this step. Can be -1, 0, or +1.\n            // This is a 0 because it isn't incorrect, but it does not make\n            // any progress.\n            \"rating\": 0,\n            // The labeler can flag steps that they don't know how to label.\n            // This is rarely used.\n            \"flagged\": null\n          }\n        ],\n        // In phase 1, if all completions were rated -1, we allowed labelers to\n        // write their own +1 step. This is null for all steps in phase 2.\n        \"human_completion\": null,\n        // The index of the completion \"chosen\" at this step, or null if the\n        // human_completion was used. You can reconstruct the solution\n        // trajectory like:\n        // [\n        //     step[\"human_completion\"] if step[\"chosen_completion\"] is None\n        //     else step[\"completions\"][step[\"chosen_completion\"]][\"text\"]\n        //     for step in labeled_solution[\"label\"][\"steps\"]\n        // ]\n        \"chosen_completion\": 0\n      },\n      {\n        \"completions\": [\n          {\n            \"text\": \"One way to do this is to factor both numbers into prime factors and look for the common ones.\",\n            \"rating\": 0,\n            \"flagged\": null\n          }\n        ],\n        \"human_completion\": null,\n        \"chosen_completion\": 0\n      },\n      {\n        // Some steps contain multiple alternative completions, and each one\n        // gets a rating.\n        \"completions\": [\n          {\n            \"text\": \"I know that $200,\\\\!000 = 2^5\\\\cdot 10^4 = 2^9\\\\cdot 5^4$.\",\n            \"rating\": -1,\n            \"flagged\": null\n          },\n          {\n            \"text\": \"To factor $20 !$, I can use the fact that every factorial is a multiple of every number less than or equal to it.\",\n            \"rating\": 0,\n            \"flagged\": false\n          },\n          {\n            \"text\": \"I can use a factor tree to find the prime factors of $200,\\\\!000$: $200,\\\\!000 = 2^5\\\\cdot 10^4 = 2^5\\\\cdot 2^4\\\\cdot 5^4 = 2^9\\\\cdot 5^4$.\",\n            \"rating\": -1,\n            \"flagged\": false\n          },\n          {\n            \"text\": \"I can use a factor tree to find the prime factors of $200,\\\\!000$.\",\n            \"rating\": 0,\n            \"flagged\": false\n          },\n          {\n            \"text\": \"To factor $20 !$, I can use the fact that any factorial is divisible by all the primes less than or equal to the input.\",\n            \"rating\": 0,\n            \"flagged\": false\n          }\n        ],\n        \"human_completion\": null,\n        \"chosen_completion\": null\n      }\n    ],\n    // Total time in milliseconds spent on labeling this solution.\n    \"total_time\": 278270,\n    // Final result of labeling this solution. Will be one of:\n    //   - \"found_error\": In phase 2 we stop labeling a solution after the\n    //                    first error is found.\n    //   - \"solution\": We reached a step that concluded in the correct answer\n    //                 to the problem.\n    //   - \"bad_problem\": The labeler reported the problem as broken.\n    //   - \"give_up\": The labeler was stuck (the problem was taking too long,\n    //                or the instructions were unclear) and moved onto the\n    //                next problem.\n    \"finish_reason\": \"found_error\"\n  }\n}\n```\n\n\n## Instructions\n\nThe `instructions/` folder contains the instructions documents we gave to\nlabelers during each phase of the project.\n\n\n## Answer Grading\n\nThe `grading/` folder contains the python grading logic we used for determining if a model-outputted answer correctly matched\nthe ground truth answer in Hendrycks' MATH dataset. We build off of Hendrycks' math normalization logic in `math_normalize.py`\nand use sympy to check for equality of expressions in `grader.py`. We recommend using `grader.grade_answer(model_answer, gt_answer)`\nwhere both answers are strings to determine if a solution is correct or not.\n\nAnswer grading is difficult in general. This grading logic is designed to be conservative and will sometimes reject correct\nanswers, though it does so less frequently than the normalization logic from MATH. Our logic might sometimes admit incorrect\nanswers, though we've put effort into minimizing this.\n\n\n## MATH Splits\n\nAs explained in Let's Verify Step by Step, we use a nonstandard MATH train/test split.\n\n> In order to avoid the risk of over-fitting on the 7,500 MATH training problems, we expanded the training set to include 4,500 MATH test split problems. We therefore evaluate our models only on the remaining 500 held-out problems. We selected these 500 test problems uniformly at random, and we believe they are representative of the test set as a whole.\n\nThe `math_splits/` folder contains our selected splits in the `train.jsonl` and `test.jsonl` files. You'll need [Git LFS](https://git-lfs.com/) to properly clone these files.\n\n\n## Scored Samples\n\nWe release all large-scale model samples used to evaluate the large-scale ORM and PRM, corresponding to Figure 3 in the paper. Each test problem has to 1860 scored samples. Solutions that failed to reach an answer within 1024 tokens were discarded, resulting in less than 1860 samples on some problems. We account for this in the best-of-N evaluation logic. \n\nEvaluate the PRM:\n\n```bash\npython eval/eval.py --method prm\n```\n\nEvaluate the ORM:\n\n```bash\npython eval/eval.py --method orm\n```\n\n\n## Citation\n\nPlease use the below BibTeX entry to cite this dataset:\n\n```\n@article{lightman2023lets,\n      title={Let's Verify Step by Step}, \n      author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},\n      journal={arXiv preprint arXiv:2305.20050},\n      year={2023}\n}\n```\n", "release_dates": []}, {"name": "procgen", "description": "Procgen Benchmark: Procedurally-Generated Game-Like Gym-Environments", "language": "C++", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Maintenance (expect bug fixes and minor updates)\n\n# Procgen Benchmark\n\n#### [[Blog Post]](https://openai.com/blog/procgen-benchmark/) [[Paper]](https://arxiv.org/abs/1912.01588)\n\n16 simple-to-use procedurally-generated [gym](https://github.com/openai/gym) environments which provide a direct measure of how quickly a reinforcement learning agent learns generalizable skills.  The environments run at high speed (thousands of steps per second) on a single core.\n\nWe ran a competition in 2020 which used these environments to measure sample efficiency and generalization in RL. You can learn more [here](https://www.aicrowd.com/challenges/neurips-2020-procgen-competition).\n\n<img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/procgen.gif\">\n\nThese environments are associated with the paper [Leveraging Procedural Generation to Benchmark Reinforcement Learning](https://cdn.openai.com/procgen.pdf) [(citation)](#citation).  The code for running some experiments from the paper is in the [train-procgen](https://github.com/openai/train-procgen) repo.  For those familiar with the original [CoinRun environment](https://github.com/openai/coinrun), be sure to read the updated CoinRun description below as there have been subtle changes to the environment.\n\nCompared to [Gym Retro](https://github.com/openai/retro), these environments are:\n\n* Faster: Gym Retro environments are already fast, but Procgen environments can run >4x faster.\n* Randomized: Gym Retro environments are always the same, so you can memorize a sequence of actions that will get the highest reward.  Procgen environments are randomized so this is not possible.\n* Customizable: If you install from source, you can perform experiments where you change the environments, or build your own environments.  The environment-specific code for each environment is often less than 300 lines.  This is almost impossible with Gym Retro.\n\nSupported platforms:\n\n- Windows 10\n- macOS 10.14 (Mojave), 10.15 (Catalina)\n- Linux (manylinux2010)\n\nSupported Pythons:\n\n- 3.7 64-bit\n- 3.8 64-bit\n- 3.9 64-bit\n- 3.10 64-bit\n\nSupported CPUs:\n\n- Must have at least AVX\n\n## Installation\n\nFirst make sure you have a supported version of python:\n\n```\n# run these commands to check for the correct python version\npython -c \"import sys; assert (3,7,0) <= sys.version_info <= (3,10,0), 'python is incorrect version'; print('ok')\"\npython -c \"import platform; assert platform.architecture()[0] == '64bit', 'python is not 64-bit'; print('ok')\"\n```\n\nTo install the wheel:\n\n```\npip install procgen\n```\n\nIf you get an error like `\"Could not find a version that satisfies the requirement procgen\"`, please upgrade pip: `pip install --upgrade pip`.\n\nTo try an environment out interactively:\n\n```\npython -m procgen.interactive --env-name coinrun\n```\n\nThe keys are: left/right/up/down + q, w, e, a, s, d for the different (environment-dependent) actions.  Your score is displayed as \"episode_return\" in the lower left.  At the end of an episode, you can see your final \"episode_return\" as well as \"prev_level_complete\" which will be `1` if you successfully completed the level.\n\nTo create an instance of the [gym](https://github.com/openai/gym) environment:\n\n```\nimport gym\nenv = gym.make(\"procgen:procgen-coinrun-v0\")\n```\n\nTo create an instance of the [gym3](https://github.com/openai/gym3) (vectorized) environment:\n\n```\nfrom procgen import ProcgenGym3Env\nenv = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n```\n\n### Docker\n\nA [`Dockerfile`](docker/Dockerfile) is included to demonstrate a minimal Docker-based setup that works for running random agent.\n\n```\ndocker build docker --tag procgen\ndocker run --rm -it procgen python3 -m procgen.examples.random_agent_gym\n```\n\nThere is a second `Dockerfile` to demonstrate installing from source:\n\n```\ndocker build . --tag procgen --file docker/Dockerfile.dev\ndocker run --rm -it procgen python -c \"from procgen import ProcgenGym3Env; env = ProcgenGym3Env(num=1, env_name='coinrun'); print(env.observe())\"\n```\n\n## Environments\n\nThe observation space is a box space with the RGB pixels the agent sees in a numpy array of shape (64, 64, 3).  The expected step rate for a human player is 15 Hz.\n\nThe action space is `Discrete(15)` for which button combo to press.  The button combos are defined in [`env.py`](procgen/env.py).\n\nIf you are using the vectorized environment, the observation space is a dictionary space where the pixels are under the key \"rgb\".\n\nHere are the 16 environments:\n\n| Image | Name | Description |\n| --- | --- | --- |\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/bigfish.png\" width=\"200px\"> | `bigfish` | The player starts as a small fish and becomes bigger by eating other fish. The player may only eat fish smaller than itself, as determined solely by width. If the player comes in contact with a larger fish, the player is eaten and the episode ends. The player receives a small reward for eating a smaller fish and a large reward for becoming bigger than all other fish, at which point the episode ends. \n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/bossfight.png\" width=\"200px\"> | `bossfight` | The player controls a small starship and must destroy a much bigger boss starship. The boss randomly selects from a set of possible attacks when engaging the player. The player must dodge the incoming projectiles or be destroyed. The player can also use randomly scattered meteors for cover. After a set timeout, the boss becomes vulnerable and its shields go down. At this point, the players projectile attacks will damage the boss. Once the boss receives a certain amount of damage, the player receives a reward, and the boss re-raises its shields. If the player damages the boss several times in this way, the boss is destroyed, the player receives a large reward, and the episode ends.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/caveflyer.png\" width=\"200px\"> | `caveflyer` | The player must navigate a network of caves to reach the exit. Player movement mimics the Atari game \u201cAsteroids\u201d: the ship can rotate and travel forward or backward along the current axis. The majority of the reward comes from successfully reaching the end of the level, though additional reward can be collected by destroying target objects along the way with the ship's lasers. There are stationary and moving lethal obstacles throughout the level.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/chaser.png\" width=\"200px\"> | `chaser` | Inspired by the Atari game \u201cMsPacman\u201d. Maze layouts are generated using Kruskal\u2019s algorithm, and then walls are removed until no dead-ends remain in the maze. The player must collect all the green orbs. 3 large stars spawn that will make enemies vulnerable for a short time when collected. A collision with an enemy that isn\u2019t vulnerable results in the player\u2019s death. When a vulnerable enemy is eaten, an egg spawns somewhere on the map that will hatch into a new enemy after a short time, keeping the total number of enemies constant. The player receives a small reward for collecting each orb and a large reward for completing the level.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/climber.png\" width=\"200px\"> | `climber` | A simple platformer. The player must climb a sequence of platforms, collecting stars along the way. A small reward is given for collecting a star, and a larger reward is given for collecting all stars in a level. If all stars are collected, the episode ends. There are lethal flying monsters scattered throughout the level.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/coinrun.png\" width=\"200px\"> | `coinrun` | A simple platformer. The goal is to collect the coin at the far right of the level, and the player spawns on the far left. The agent must dodge stationary saw obstacles, enemies that pace back and forth, and chasms that lead to death. Note that while the previously released version of CoinRun painted velocity information directly onto observations, the current version does not. This makes the environment significantly more difficult.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/dodgeball.png\" width=\"200px\"> | `dodgeball` | Loosely inspired by the Atari game \u201cBerzerk\u201d. The player spawns in a room with a random configuration of walls and enemies. Touching a wall loses the game and ends the episode. The player moves relatively slowly and can navigate throughout the room. There are enemies which also move slowly and which will occasionally throw balls at the player. The player can also throw balls, but only in the direction they are facing. If all enemies are hit, the player can move to the unlocked platform and earn a significant level completion bonus.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/fruitbot.png\" width=\"200px\"> | `fruitbot` | A scrolling game where the player controls a robot that must navigate between gaps in walls and collect fruit along the way. The player receives a positive reward for collecting a piece of fruit, and a larger negative reward for mistakenly collecting a non-fruit object. Half of the spawned objects are fruit (positive reward) and half are non-fruit (negative reward). The player receives a large reward if they reach the end of the level. Occasionally the player must use a key to unlock gates which block the way.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/heist.png\" width=\"200px\"> | `heist` | The player must steal the gem hidden behind a network of locks. Each lock comes in one of three colors, and the necessary keys to open these locks are scattered throughout the level. The level layout takes the form of a maze, again generated by Kruskal's algorithm. Once the player collects a key of a certain color, the player may open the lock of that color. All keys in the player's possession are shown in the top right corner of the screen.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/jumper.png\" width=\"200px\"> | `jumper` | A platformer with an open world layout. The player, a bunny, must navigate through the world to find the carrot. It might be necessary to ascend or descend the level to do so. The player is capable of \u201cdouble jumping\u201d, allowing it to navigate tricky layouts and reach high platforms. There are spike obstacles which will destroy the player on contact. The screen includes a compass which displays direction and distance to the carrot. The only reward in the game comes from collect the carrot, at which point the episode ends. Due to a bug that permits the player to spawn on top of critical objects (an obstacle or the goal), ~7% of levels will terminate after a single action, the vast majority of which will have 0 reward.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/leaper.png\" width=\"200px\"> | `leaper` | Inspired by the classic game \u201cFrogger\u201d. The player must cross several lanes to reach the finish line and earn a reward. The first group of lanes contains cars which must be avoided. The second group of lanes contains logs on a river. The player must hop from log to log to cross the river. If the player falls in the river, the episode ends.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/maze.png\" width=\"200px\"> | `maze` | The player, a mouse, must navigate a maze to find the sole piece of cheese and earn a reward. Mazes are generated by Kruskal's algorithm and range in size from 3x3 to 25x25. The maze dimensions are uniformly sampled over this range. The player may move up, down, left or right to navigate the maze.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/miner.png\" width=\"200px\"> | `miner` | Inspired by the classic game \u201cBoulderDash\u201d. The player, a robot, can dig through dirt to move throughout the world. The world has gravity, and dirt supports boulders and diamonds. Boulders and diamonds will fall through free space and roll off each other. If a boulder or a diamond falls on the player, the game is over. The goal is to collect all the diamonds in the level and then proceed through the exit. The player receives a small reward for collecting a diamond and a larger reward for completing the level.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/ninja.png\" width=\"200px\"> | `ninja` | A simple platformer. The player, a ninja, must jump across narrow ledges while avoiding bomb obstacles. The player can toss throwing stars at several angles in order to clear bombs, if necessary. The player's jump can be charged over several timesteps to increase its effect. The player receives a reward for collecting the mushroom at the end of the level, at which point the episode terminates.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/plunder.png\" width=\"200px\"> | `plunder` | The player must destroy enemy pirate ships by firing cannonballs from its own ship at the bottom of the screen. An on-screen timer slowly counts down. If this timer runs out, the episode ends. Whenever the player fires, the timer skips forward a few steps, encouraging the player to conserve ammunition. The player must take care to avoid hitting friendly ships. The player receives a positive reward for hitting an enemy ship and a large timer penalty for hitting a friendly ship. A target in the bottom left corner identifies the color of the enemy ships to target.\n| <img src=\"https://raw.githubusercontent.com/openai/procgen/master/screenshots/starpilot.png\" width=\"200px\"> | `starpilot` | A simple side scrolling shooter game. Relatively challenging for humans to play since all enemies fire projectiles that directly target the player. An inability to dodge quickly leads to the player's demise. There are fast and slow enemies, stationary turrets with high health, clouds which obscure player vision, and impassable meteors.\n\n## Known Issues\n\n* `bigfish` - It is possible for the player to occasionally become trapped along the borders of the environment.\n* `caveflyer` - In ~0.5% of levels, the player spawns next to an enemy and will die in a single step regardless of which action is taken.\n* `jumper` - In ~7% of levels, the player will spawn on top of an enemy or the goal, resulting in the episode terminating after a single step regardless of which action is taken.\n* `miner` - There is a low probability of unsolvable level configurations, with either a diamond or the exit being unreachable.\n\nRather than patch these issues, we plan to keep the environments in their originally released form, in order to ease the reproducibility of results that are already published.\n\n## Environment Options\n\n* `env_name` - Name of environment, or comma-separate list of environment names to instantiate as each env in the VecEnv.\n* `num_levels=0` - The number of unique levels that can be generated. Set to 0 to use unlimited levels.\n* `start_level=0` - The lowest seed that will be used to generated levels. 'start_level' and 'num_levels' fully specify the set of possible levels.\n* `paint_vel_info=False` - Paint player velocity info in the top left corner. Only supported by certain games.\n* `use_generated_assets=False` - Use randomly generated assets in place of human designed assets.\n* `debug=False` - Set to `True` to use the debug build if building from source.\n* `debug_mode=0` - A useful flag that's passed through to procgen envs. Use however you want during debugging.\n* `center_agent=True` - Determines whether observations are centered on the agent or display the full level. Override at your own risk.\n* `use_sequential_levels=False` - When you reach the end of a level, the episode is ended and a new level is selected.  If `use_sequential_levels` is set to `True`, reaching the end of a level does not end the episode, and the seed for the new level is derived from the current level seed.  If you combine this with `start_level=<some seed>` and `num_levels=1`, you can have a single linear series of levels similar to a gym-retro or ALE game.\n* `distribution_mode=\"hard\"` - What variant of the levels to use, the options are `\"easy\", \"hard\", \"extreme\", \"memory\", \"exploration\"`.  All games support `\"easy\"` and `\"hard\"`, while other options are game-specific.  The default is `\"hard\"`.  Switching to `\"easy\"` will reduce the number of timesteps required to solve each game and is useful for testing or when working with limited compute resources.\n* `use_backgrounds=True` - Normally games use human designed backgrounds, if this flag is set to `False`, games will use pure black backgrounds.\n* `restrict_themes=False` - Some games select assets from multiple themes, if this flag is set to `True`, those games will only use a single theme.\n* `use_monochrome_assets=False` - If set to `True`, games will use monochromatic rectangles instead of human designed assets. best used with `restrict_themes=True`.\n\nHere's how to set the options:\n\n```\nimport gym\nenv = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n```\n\nSince the gym environment is adapted from a gym3 environment, early calls to `reset()` are disallowed and the `render()` method does not do anything.  To render the environment, pass `render_mode=\"human\"` to the constructor, which will send `render_mode=\"rgb_array\"` to the environment constructor and wrap it in a `gym3.ViewerWrapper`.  If you just want the frames instead of the window, pass `render_mode=\"rgb_array\"`.\n\nFor the gym3 vectorized environment:\n\n```\nfrom procgen import ProcgenGym3Env\nenv = ProcgenGym3Env(num=1, env_name=\"coinrun\", start_level=0, num_levels=1)\n```\n\nTo render with the gym3 environment, pass `render_mode=\"rgb_array\"`.  If you wish to view the output, use a `gym3.ViewerWrapper`.\n\n## Saving and loading the environment state\n\nIf you are using the gym3 interface, you can save and load the environment state:\n\n```\nfrom procgen import ProcgenGym3Env\nenv = ProcgenGym3Env(num=1, env_name=\"coinrun\", start_level=0, num_levels=1)\nstates = env.callmethod(\"get_state\")\nenv.callmethod(\"set_state\", states)\n```\n\nThis returns a list of byte strings representing the state of each game in the vectorized environment.\n\n## Notes\n\n* You should depend on a specific version of this library (using `==`) for your experiments to ensure they are reproducible.  You can get the current installed version with `pip show procgen`.\n* This library does not require or make use of GPUs.\n* While the library should be thread safe, each individual environment instance should only be used from a single thread.  The library is not fork safe unless you set `num_threads=0`.  Even if you do that, `Qt` is not guaranteed to be fork safe, so you should probably create the environment after forking or not use fork at all.\n\n# Install from Source\n\nIf you want to change the environments or create new ones, you should build from source.  You can get miniconda from https://docs.conda.io/en/latest/miniconda.html if you don't have it, or install the dependencies from [`environment.yml`](environment.yml) manually.  On Windows you will also need \"Visual Studio 16 2019\" installed.\n\n```\ngit clone git@github.com:openai/procgen.git\ncd procgen\nconda env update --name procgen --file environment.yml\nconda activate procgen\npip install -e .\n# this should say \"building procgen...done\"\npython -c \"from procgen import ProcgenGym3Env; ProcgenGym3Env(num=1, env_name='coinrun')\"\n# this should create a window where you can play the coinrun environment\npython -m procgen.interactive\n```\n\nThe environment code is in C++ and is compiled into a shared library exposing the [`gym3.libenv`](https://github.com/openai/gym3/blob/master/gym3/libenv.h) C interface that is then loaded by python.  The C++ code uses [Qt](https://www.qt.io/) for drawing.\n\n# Create a new environment\n\nOnce you have installed from source, you can customize an existing environment or make a new environment of your own.  If you want to create a fast C++ 2D environment, you can fork this repo and do the following:\n\n* Copy [`src/games/bigfish.cpp`](procgen/src/games/bigfish.cpp) to `src/games/<name>.cpp`\n* Replace `BigFish` with `<name>` and `\"bigfish\"` with `\"<name>\"` in your cpp file\n* Add `src/games/<name>.cpp` to [`CMakeLists.txt`](procgen/CMakeLists.txt)\n* Run `python -m procgen.interactive --env-name <name>` to test it out\n\nThis repo includes a travis configuration that will compile your environment and build python wheels for easy installation.  In order to have this build more quickly by caching the Qt compilation, you will want to configure a GCS bucket in [common.py](https://github.com/openai/procgen/blob/master/procgen-build/procgen_build/common.py#L5) and [setup service account credentials](https://github.com/openai/procgen/blob/master/procgen-build/procgen_build/build_package.py#L41).\n\n# Add information to the info dictionary\n\nTo export game information from the C++ game code to Python, you can define a new `info_type`.  `info_type`s appear in the `info` dict returned by the gym environment, or in `get_info()` from the gym3 environment.\n\nTo define a new one, add the following code to the `VecGame` constructor here: [vecgame.cpp](https://github.com/openai/procgen/blob/master/procgen/src/vecgame.cpp#L290)\n\n```\n{\n    struct libenv_tensortype s;\n    strcpy(s.name, \"heist_key_count\");\n    s.scalar_type = LIBENV_SCALAR_TYPE_DISCRETE;\n    s.dtype = LIBENV_DTYPE_INT32;\n    s.ndim = 0,\n    s.low.int32 = 0;\n    s.high.int32 = INT32_MAX;\n    info_types.push_back(s);\n}\n```\n\nThis lets the Python code know to expect a single integer and expose it in the `info` dict.\n\nAfter adding that, you can add the following code to [heist.cpp](https://github.com/openai/procgen/blob/master/procgen/src/games/heist.cpp#L93):\n\n```\nvoid observe() override {\n    Game::observe();\n    int32_t key_count = 0;\n    for (const auto& has_key : has_keys) {\n        if (has_key) {\n            key_count++;\n        }\n    }\n    *(int32_t *)(info_bufs[info_name_to_offset.at(\"heist_key_count\")]) = key_count;\n}\n```\n\nThis populates the `heist_key_count` info value each time the environment is observed.\n\nIf you run the interactive script (making sure that you installed from source), the new keys should appear in the bottom left hand corner:\n\n`python -m procgen.interactive --env-name heist`\n\n# Changelog\n\nSee [CHANGES](CHANGES.md) for changes present in each release.\n\n# Contributing\n\nSee [CONTRIBUTING](CONTRIBUTING.md) for information on contributing.\n\n# Assets\n\nSee [ASSET_LICENSES](ASSET_LICENSES.md) for asset license information.\n\n# Citation\n\nPlease cite using the following bibtex entry:\n\n```\n@article{cobbe2019procgen,\n  title={Leveraging Procedural Generation to Benchmark Reinforcement Learning},\n  author={Cobbe, Karl and Hesse, Christopher and Hilton, Jacob and Schulman, John},\n  journal={arXiv preprint arXiv:1912.01588},\n  year={2019}\n}\n```\n", "release_dates": ["2022-01-24T17:31:28Z", "2022-01-15T04:25:20Z", "2022-01-15T02:51:43Z", "2020-07-10T22:00:19Z", "2020-06-15T18:36:58Z", "2020-06-03T16:46:18Z", "2020-06-03T15:50:04Z", "2020-06-03T15:38:11Z", "2020-01-10T23:29:29Z", "2019-12-29T08:07:44Z", "2019-12-03T19:14:42Z", "2019-12-03T02:11:57Z"]}, {"name": "prometheus", "description": "The Prometheus monitoring system and time series database.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Prometheus\n\n[![CircleCI](https://circleci.com/gh/prometheus/prometheus/tree/master.svg?style=shield)][circleci]\n[![Docker Repository on Quay](https://quay.io/repository/prometheus/prometheus/status)][quay]\n[![Docker Pulls](https://img.shields.io/docker/pulls/prom/prometheus.svg?maxAge=604800)][hub]\n[![Go Report Card](https://goreportcard.com/badge/github.com/prometheus/prometheus)](https://goreportcard.com/report/github.com/prometheus/prometheus)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/486/badge)](https://bestpractices.coreinfrastructure.org/projects/486)\n[![fuzzit](https://app.fuzzit.dev/badge?org_id=prometheus&branch=master)](https://fuzzit.dev)\n\nVisit [prometheus.io](https://prometheus.io) for the full documentation,\nexamples and guides.\n\nPrometheus, a [Cloud Native Computing Foundation](https://cncf.io/) project, is a systems and service monitoring system. It collects metrics\nfrom configured targets at given intervals, evaluates rule expressions,\ndisplays the results, and can trigger alerts if some condition is observed\nto be true.\n\nPrometheus's main distinguishing features as compared to other monitoring systems are:\n\n- a **multi-dimensional** data model (timeseries defined by metric name and set of key/value dimensions)\n- a **flexible query language** to leverage this dimensionality\n- no dependency on distributed storage; **single server nodes are autonomous**\n- timeseries collection happens via a **pull model** over HTTP\n- **pushing timeseries** is supported via an intermediary gateway\n- targets are discovered via **service discovery** or **static configuration**\n- multiple modes of **graphing and dashboarding support**\n- support for hierarchical and horizontal **federation**\n\n## Architecture overview\n\n![](https://cdn.jsdelivr.net/gh/prometheus/prometheus@c34257d069c630685da35bcef084632ffd5d6209/documentation/images/architecture.svg)\n\n## Install\n\nThere are various ways of installing Prometheus.\n\n### Precompiled binaries\n\nPrecompiled binaries for released versions are available in the\n[*download* section](https://prometheus.io/download/)\non [prometheus.io](https://prometheus.io). Using the latest production release binary\nis the recommended way of installing Prometheus.\nSee the [Installing](https://prometheus.io/docs/introduction/install/)\nchapter in the documentation for all the details.\n\nDebian packages [are available](https://packages.debian.org/sid/net/prometheus).\n\n### Docker images\n\nDocker images are available on [Quay.io](https://quay.io/repository/prometheus/prometheus) or [Docker Hub](https://hub.docker.com/r/prom/prometheus/).\n\nYou can launch a Prometheus container for trying it out with\n\n    $ docker run --name prometheus -d -p 127.0.0.1:9090:9090 prom/prometheus\n\nPrometheus will now be reachable at http://localhost:9090/.\n\n### Building from source\n\nTo build Prometheus from the source code yourself you need to have a working\nGo environment with [version 1.13 or greater installed](https://golang.org/doc/install).\nYou will also need to have [Node.js](https://nodejs.org/) and [Yarn](https://yarnpkg.com/)\ninstalled in order to build the frontend assets.\n\nYou can directly use the `go` tool to download and install the `prometheus`\nand `promtool` binaries into your `GOPATH`:\n\n    $ go get github.com/prometheus/prometheus/cmd/...\n    $ prometheus --config.file=your_config.yml\n\n*However*, when using `go get` to build Prometheus, Prometheus will expect to be able to\nread its web assets from local filesystem directories under `web/ui/static` and\n`web/ui/templates`. In order for these assets to be found, you will have to run Prometheus\nfrom the root of the cloned repository. Note also that these directories do not include the\nnew experimental React UI unless it has been built explicitly using `make assets` or `make build`.\n\nAn example of the above configuration file can be found [here.](https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus.yml)\n\nYou can also clone the repository yourself and build using `make build`, which will compile in\nthe web assets so that Prometheus can be run from anywhere:\n\n    $ mkdir -p $GOPATH/src/github.com/prometheus\n    $ cd $GOPATH/src/github.com/prometheus\n    $ git clone https://github.com/prometheus/prometheus.git\n    $ cd prometheus\n    $ make build\n    $ ./prometheus --config.file=your_config.yml\n\nThe Makefile provides several targets:\n\n  * *build*: build the `prometheus` and `promtool` binaries (includes building and compiling in web assets)\n  * *test*: run the tests\n  * *test-short*: run the short tests\n  * *format*: format the source code\n  * *vet*: check the source code for common errors\n  * *docker*: build a docker container for the current `HEAD`\n\n## React UI Development\n\nFor more information on building, running, and developing on the new React-based UI, see the React app's [README.md](https://github.com/prometheus/prometheus/blob/master/web/ui/react-app/README.md).\n\n## More information\n\n  * The source code is periodically indexed: [Prometheus Core](https://godoc.org/github.com/prometheus/prometheus).\n  * You will find a CircleCI configuration in `.circleci/config.yml`.\n  * See the [Community page](https://prometheus.io/community) for how to reach the Prometheus developers and users on various communication channels.\n\n## Contributing\n\nRefer to [CONTRIBUTING.md](https://github.com/prometheus/prometheus/blob/master/CONTRIBUTING.md)\n\n## License\n\nApache License 2.0, see [LICENSE](https://github.com/prometheus/prometheus/blob/master/LICENSE).\n\n\n[hub]: https://hub.docker.com/r/prom/prometheus/\n[circleci]: https://circleci.com/gh/prometheus/prometheus\n[quay]: https://quay.io/repository/prometheus/prometheus\n", "release_dates": []}, {"name": "psaw", "description": "Python Pushshift.io API Wrapper (for comment/submission search)", "language": "Python", "license": {"key": "bsd-2-clause", "name": "BSD 2-Clause \"Simplified\" License", "spdx_id": "BSD-2-Clause", "url": "https://api.github.com/licenses/bsd-2-clause", "node_id": "MDc6TGljZW5zZTQ="}, "readme": null, "release_dates": []}, {"name": "pyconfigatron", "description": "a python3 compatible pyconfigatron", "language": "Python", "license": null, "readme": "# pyconfigatron\n\nA Python port of two Ruby projects:\n[configatron](https://github.com/markbates/configatron) and\n[chalk-config](https://github.com/stripe/chalk-config).\n", "release_dates": []}, {"name": "pytorch", "description": "Tensors and Dynamic neural networks in Python with strong GPU acceleration", "language": "C++", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "![PyTorch Logo](https://github.com/pytorch/pytorch/blob/master/docs/source/_static/img/pytorch-logo-dark.png)\n\n--------------------------------------------------------------------------------\n\nPyTorch is a Python package that provides two high-level features:\n- Tensor computation (like NumPy) with strong GPU acceleration\n- Deep neural networks built on a tape-based autograd system\n\nYou can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.\n\n<!-- toc -->\n\n- [More About PyTorch](#more-about-pytorch)\n  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)\n  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)\n  - [Python First](#python-first)\n  - [Imperative Experiences](#imperative-experiences)\n  - [Fast and Lean](#fast-and-lean)\n  - [Extensions Without Pain](#extensions-without-pain)\n- [Installation](#installation)\n  - [Binaries](#binaries)\n    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)\n  - [From Source](#from-source)\n    - [Install Dependencies](#install-dependencies)\n    - [Get the PyTorch Source](#get-the-pytorch-source)\n    - [Install PyTorch](#install-pytorch)\n      - [Adjust Build Options (Optional)](#adjust-build-options-optional)\n  - [Docker Image](#docker-image)\n    - [Using pre-built images](#using-pre-built-images)\n    - [Building the image yourself](#building-the-image-yourself)\n  - [Building the Documentation](#building-the-documentation)\n  - [Previous Versions](#previous-versions)\n- [Getting Started](#getting-started)\n- [Resources](#resources)\n- [Communication](#communication)\n- [Releases and Contributing](#releases-and-contributing)\n- [The Team](#the-team)\n- [License](#license)\n\n<!-- tocstop -->\n\n| System | 3.6 | 3.7 | 3.8 |\n| :---: | :---: | :---: | :--: |\n| Linux CPU | [![Build Status](https://ci.pytorch.org/jenkins/job/pytorch-master/badge/icon)](https://ci.pytorch.org/jenkins/job/pytorch-master/) | [![Build Status](https://ci.pytorch.org/jenkins/job/pytorch-master/badge/icon)](https://ci.pytorch.org/jenkins/job/pytorch-master/) | <center>\u2014</center> |\n| Linux GPU | [![Build Status](https://ci.pytorch.org/jenkins/job/pytorch-master/badge/icon)](https://ci.pytorch.org/jenkins/job/pytorch-master/) | [![Build Status](https://ci.pytorch.org/jenkins/job/pytorch-master/badge/icon)](https://ci.pytorch.org/jenkins/job/pytorch-master/) | <center>\u2014</center> |\n| Windows CPU / GPU | <center>\u2014</center> | [![Build Status](https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-win-ws2016-cuda9-cudnn7-py3-trigger/badge/icon)](https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-win-ws2016-cuda9-cudnn7-py3-trigger/) |  <center>\u2014</center> |\n| Linux (ppc64le) CPU | <center>\u2014</center> | [![Build Status](https://powerci.osuosl.org/job/pytorch-master-nightly-py3-linux-ppc64le/badge/icon)](https://powerci.osuosl.org/job/pytorch-master-nightly-py3-linux-ppc64le/) | <center>\u2014</center> |\n| Linux (ppc64le) GPU | <center>\u2014</center> | [![Build Status](https://powerci.osuosl.org/job/pytorch-master-nightly-py3-linux-ppc64le-gpu/badge/icon)](https://powerci.osuosl.org/job/pytorch-master-nightly-py3-linux-ppc64le-gpu/) | <center>\u2014</center> |\n| Linux (aarch64) CPU | [![Build Status](http://openlabtesting.org:15000/badge?project=pytorch%2Fpytorch&job_name=pytorch-arm64-build-daily-master-py36)](https://status.openlabtesting.org/builds/builds?project=pytorch%2Fpytorch&job_name=pytorch-arm64-build-daily-master-py36) | [![Build Status](http://openlabtesting.org:15000/badge?project=pytorch%2Fpytorch&job_name=pytorch-arm64-build-daily-master-py37)](https://status.openlabtesting.org/builds/builds?project=pytorch%2Fpytorch&job_name=pytorch-arm64-build-daily-master-py37) | [![Build Status](http://openlabtesting.org:15000/badge?project=pytorch%2Fpytorch&job_name=pytorch-arm64-build-daily-master-py38)](https://status.openlabtesting.org/builds/builds?project=pytorch%2Fpytorch&job_name=pytorch-arm64-build-daily-master-py38) |\n\nSee also the [ci.pytorch.org HUD](https://hud.pytorch.org/build2/pytorch-master).\n\n\n## More About PyTorch\n\nAt a granular level, PyTorch is a library that consists of the following components:\n\n| Component | Description |\n| ---- | --- |\n| [**torch**](https://pytorch.org/docs/stable/torch.html) | a Tensor library like NumPy, with strong GPU support |\n| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | a neural networks library deeply integrated with autograd designed for maximum flexibility |\n| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n\nUsually, PyTorch is used either as:\n\n- A replacement for NumPy to use the power of GPUs.\n- A deep learning research platform that provides maximum flexibility and speed.\n\nElaborating Further:\n\n### A GPU-Ready Tensor Library\n\nIf you use NumPy, then you have used Tensors (a.k.a. ndarray).\n\n![Tensor illustration](./docs/source/_static/img/tensor_illustration.png)\n\nPyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the\ncomputation by a huge amount.\n\nWe provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\nsuch as slicing, indexing, math operations, linear algebra, reductions.\nAnd they are fast!\n\n### Dynamic Neural Networks: Tape-Based Autograd\n\nPyTorch has a unique way of building neural networks: using and replaying a tape recorder.\n\nMost frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.\nOne has to build a neural network and reuse the same structure again and again.\nChanging the way the network behaves means that one has to start from scratch.\n\nWith PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to\nchange the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes\nfrom several research papers on this topic, as well as current and past work such as\n[torch-autograd](https://github.com/twitter/torch-autograd),\n[autograd](https://github.com/HIPS/autograd),\n[Chainer](https://chainer.org), etc.\n\nWhile this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.\nYou get the best of speed and flexibility for your crazy research.\n\n![Dynamic graph](https://github.com/pytorch/pytorch/blob/master/docs/source/_static/img/dynamic_graph.gif)\n\n### Python First\n\nPyTorch is not a Python binding into a monolithic C++ framework.\nIt is built to be deeply integrated into Python.\nYou can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.\nYou can write your new neural network layers in Python itself, using your favorite libraries\nand use packages such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).\nOur goal is to not reinvent the wheel where appropriate.\n\n### Imperative Experiences\n\nPyTorch is designed to be intuitive, linear in thought, and easy to use.\nWhen you execute a line of code, it gets executed. There isn't an asynchronous view of the world.\nWhen you drop into a debugger or receive error messages and stack traces, understanding them is straightforward.\nThe stack trace points to exactly where your code was defined.\nWe hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.\n\n### Fast and Lean\n\nPyTorch has minimal framework overhead. We integrate acceleration libraries\nsuch as [Intel MKL](https://software.intel.com/mkl) and NVIDIA ([cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://developer.nvidia.com/nccl)) to maximize speed.\nAt the core, its CPU and GPU Tensor and neural network backends\nare mature and have been tested for years.\n\nHence, PyTorch is quite fast \u2013 whether you run small or large neural networks.\n\nThe memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.\nWe've written custom memory allocators for the GPU to make sure that\nyour deep learning models are maximally memory efficient.\nThis enables you to train bigger deep learning models than before.\n\n### Extensions Without Pain\n\nWriting new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward\nand with minimal abstractions.\n\nYou can write new neural network layers in Python using the torch API\n[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).\n\nIf you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.\nNo wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp).\n\n\n## Installation\n\n### Binaries\nCommands to install from binaries via Conda or pip wheels are on our website:\n[https://pytorch.org](https://pytorch.org)\n\n\n#### NVIDIA Jetson Platforms\n\nPython wheels for NVIDIA's Jetson Nano, Jetson TX2, and Jetson AGX Xavier are available via the following URLs:\n\n- Stable binaries:\n  - Python 3.6: https://nvidia.box.com/v/torch-stable-cp36-jetson-jp42\n- Rolling weekly binaries:\n  - Python 3.6: https://nvidia.box.com/v/torch-weekly-cp36-jetson-jp42\n\nThey require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv) maintains them\n\n\n### From Source\n\nIf you are installing from source, you will need Python 3.6.2 or later and a C++14 compiler. Also, we highly recommend installing an [Anaconda](https://www.anaconda.com/distribution/#download-section) environment.\nYou will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.\n\nOnce you have [Anaconda](https://www.anaconda.com/distribution/#download-section) installed, here are the instructions.\n\nIf you want to compile with CUDA support, install\n- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads) 10.2 or above\n- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v7 or above\n- [Compiler](https://gist.github.com/ax3l/9489132) compatible with CUDA\nNote: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/pdf/cuDNN-Support-Matrix.pdf) for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardwares\n\nIf you want to disable CUDA support, export environment variable `USE_CUDA=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\nIf you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)\n\nIf you want to compile with ROCm support, install\n- [AMD ROCm](https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html) 4.0 and above installation\n- ROCm is currently supported only for Linux system.\n\nIf you want to disable ROCm support, export environment variable `USE_ROCM=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\n#### Install Dependencies\n\nCommon\n```bash\nconda install astunparse numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses\n```\n\nOn Linux\n```bash\n# CUDA only: Add LAPACK support for the GPU if needed\nconda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo\n```\n\nOn MacOS\n```bash\n# Add these packages if torch.distributed is needed\nconda install pkg-config libuv\n```\n\nOn Windows\n```bash\n# Add these packages if torch.distributed is needed.\n# Distributed package support on Windows is a prototype feature and is subject to changes.\nconda install -c conda-forge libuv=1.39\n```\n\n#### Get the PyTorch Source\n```bash\ngit clone --recursive https://github.com/pytorch/pytorch\ncd pytorch\n# if you are updating an existing checkout\ngit submodule sync\ngit submodule update --init --recursive --jobs 0\n```\n\n#### Install PyTorch\nOn Linux\n```bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\npython setup.py install\n```\n\nNote that if you are compiling for ROCm, you must run this command first:\n```bash\npython tools/amd_build/build_amd.py\n```\n\nNote that if you are using [Anaconda](https://www.anaconda.com/distribution/#download-section), you may experience an error caused by the linker:\n\n```plaintext\nbuild/temp.linux-x86_64-3.7/torch/csrc/stub.o: file not recognized: file format not recognized\ncollect2: error: ld returned 1 exit status\nerror: command 'g++' failed with exit status 1\n```\n\nThis is caused by `ld` from Conda environment shadowing the system `ld`. You should use a newer version of Python that fixes this issue. The recommended Python version is 3.6.10+, 3.7.6+ and 3.8.1+.\n\nOn macOS\n```bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install\n```\n\nCUDA is not supported on macOS.\n\n\nOn Windows\n\nChoose Correct Visual Studio Version.\n\nSometimes there are regressions in new versions of Visual Studio, so\nit's best to use the same Visual Studio Version [16.8.5](https://github.com/pytorch/pytorch/blob/master/.circleci/scripts/vs_install.ps1) as Pytorch CI's.\nYou can use Visual Studio Enterprise, Professional or Community though PyTorch CI uses Visual Studio BuildTools.\n\nIf you want to build legacy python code, please refer to [Building on legacy code and CUDA](https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#building-on-legacy-code-and-cuda)\n\nBuild with CPU\n\nIt's fairly easy to build with CPU.\n\nNote on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you'll need to manually download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH` and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/master/docs/source/notes/windows.rst#building-from-source) is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.\n\nBuild with CUDA\n\n[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) is needed to build Pytorch with CUDA.\nNVTX is a part of CUDA distributive, where it is called \"Nsight Compute\". To install it onto already installed CUDA run CUDA installation once again and check the corresponding checkbox.\nMake sure that CUDA with Nsight Compute is installed after Visual Studio.\n\nCurrently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019.\n<br/> If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.\n\nAdditional libraries such as\n[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a MKLDNN or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache) are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/master/.jenkins/pytorch/win-test-helpers/installation-helpers) to install them.\n\nYou can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/master/.jenkins/pytorch/win-test-helpers/build_pytorch.bat) script for some other environment variables configurations\n\n\n```cmd\ncmd\n\n:: Set the environment variables after you have downloaded and upzipped the mkl package,\n:: else CMake would throw error as `Could NOT find OpenMP`.\nset CMAKE_INCLUDE_PATH={Your directory}\\mkl\\include\nset LIB={Your directory}\\mkl\\lib;%LIB%\n\n:: Read the content in the previous section carefully before you proceed.\n:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.\n:: \"Visual Studio 2019 Developer Command Prompt\" will be run automatically.\n:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.\nset CMAKE_GENERATOR_TOOLSET_VERSION=14.27\nset DISTUTILS_USE_SDK=1\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,17^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n\n:: [Optional] If you want to override the CUDA host compiler\nset CUDAHOSTCXX=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64\\cl.exe\n\npython setup.py install\n\n```\n\n##### Adjust Build Options (Optional)\n\nYou can adjust the configuration of cmake variables optionally (without building first), by doing\nthe following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done\nwith such a step.\n\nOn Linux\n```bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\npython setup.py build --cmake-only\nccmake build  # or cmake-gui build\n```\n\nOn macOS\n```bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only\nccmake build  # or cmake-gui build\n```\n\n### Docker Image\n\n#### Using pre-built images\n\nYou can also pull a pre-built docker image from Docker Hub and run with docker v19.03+\n\n```bash\ndocker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest\n```\n\nPlease note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.\nfor multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you\nshould increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.\n\n#### Building the image yourself\n\n**NOTE:** Must be built with a docker version > 18.06\n\nThe `Dockerfile` is supplied to build images with CUDA 11.1 support and cuDNN v8.\nYou can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it\nunset to use the default.\n```bash\nmake -f docker.Makefile\n# images are tagged as docker.io/${your_docker_username}/pytorch\n```\n\n### Building the Documentation\n\nTo build documentation in various formats, you will need [Sphinx](http://www.sphinx-doc.org) and the\nreadthedocs theme.\n\n```bash\ncd docs/\npip install -r requirements.txt\n```\nYou can then build the documentation by running ``make <format>`` from the\n``docs/`` folder. Run ``make`` to get a list of all available output formats.\n\nIf you get a katex error run ```npm install katex```.  If it persists, try\n```npm install -g katex```\n\n### Previous Versions\n\nInstallation instructions and binaries for previous PyTorch versions may be found\non [Our Website](https://pytorch.org/previous-versions).\n\n\n## Getting Started\n\nThree-pointers to get you started:\n- [Tutorials: get you started with understanding and using PyTorch](https://pytorch.org/tutorials/)\n- [Examples: easy to understand pytorch code across all domains](https://github.com/pytorch/examples)\n- [The API Reference](https://pytorch.org/docs/)\n- [Glossary](https://github.com/pytorch/pytorch/blob/master/GLOSSARY.md)\n\n## Resources\n\n* [PyTorch.org](https://pytorch.org/)\n* [PyTorch Tutorials](https://pytorch.org/tutorials/)\n* [PyTorch Examples](https://github.com/pytorch/examples)\n* [PyTorch Models](https://pytorch.org/hub/)\n* [Intro to Deep Learning with PyTorch from Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)\n* [Intro to Machine Learning with PyTorch from Udacity](https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229)\n* [Deep Neural Networks with PyTorch from Coursera](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)\n* [PyTorch Twitter](https://twitter.com/PyTorch)\n* [PyTorch Blog](https://pytorch.org/blog/)\n* [PyTorch YouTube](https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw)\n\n## Communication\n* Forums: Discuss implementations, research, etc. https://discuss.pytorch.org\n* GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.\n* Slack: The [PyTorch Slack](https://pytorch.slack.com/) hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is [PyTorch Forums](https://discuss.pytorch.org). If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1\n* Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: https://eepurl.com/cbG0rv\n* Facebook Page: Important announcements about PyTorch. https://www.facebook.com/pytorch\n* For brand guidelines, please visit our website at [pytorch.org](https://pytorch.org/)\n\n## Releases and Contributing\n\nPyTorch has a 90-day release cycle (major releases). Please let us know if you encounter a bug by [filing an issue](https://github.com/pytorch/pytorch/issues).\n\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.\n\nIf you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us.\nSending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.\n\nTo learn more about making a contribution to Pytorch, please see our [Contribution page](CONTRIBUTING.md).\n\n## The Team\n\nPyTorch is a community-driven project with several skillful engineers and researchers contributing to it.\n\nPyTorch is currently maintained by [Adam Paszke](https://apaszke.github.io/), [Sam Gross](https://github.com/colesbury), [Soumith Chintala](http://soumith.ch) and [Gregory Chanan](https://github.com/gchanan) with major contributions coming from hundreds of talented individuals in various forms and means.\nA non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.\n\nNote: This project is unrelated to [hughperkins/pytorch](https://github.com/hughperkins/pytorch) with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.\n\n## License\n\nPyTorch has a BSD-style license, as found in the [LICENSE](LICENSE) file.\n", "release_dates": []}, {"name": "random-network-distillation", "description": "Code for the paper \"Exploration by Random Network Distillation\"", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n## [Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894) ##\n\n\nYuri Burda*, Harri Edwards*, Amos Storkey, Oleg Klimov<br/>\n&#42;equal contribution\n\nOpenAI<br/>\nUniversity of Edinburgh\n\n\n### Installation and Usage\nThe following command should train an RND agent on Montezuma's Revenge\n```bash\npython run_atari.py --gamma_ext 0.999\n```\nTo use more than one gpu/machine, use MPI (e.g. `mpiexec -n 8 python run_atari.py --num_env 128 --gamma_ext 0.999` should use 1024 parallel environments to collect experience on an 8 gpu machine). \n\n### [Blog post and videos](https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/)\n", "release_dates": []}, {"name": "requests-for-research", "description": "A living collection of deep learning problems", "language": "HTML", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Requests for Research\n\nIt's easy to get started in deep learning, with many\n[resources](https://www.quora.com/What-are-the-best-ways-to-pick-up-Deep-Learning-skills-as-an-engineer)\nto learn the latest techniques. But it's harder to know what problems\nare worth working on.\n\nThis repository contains a living collection of important and fun\nproblems to help new people enter the field, and for enthusiastic\npractitioners to hone their skills. Many will require inventing new\nideas.\n\nAlso check out our new list: [Requests for Research 2.0](https://blog.openai.com/requests-for-research-2/)\n\n## If you've solved a problem\n\nPlease write up the problem in a Gist or paper, and open a pull\nrequest linking it in a \"solutions\" section for the relevant\nproblem. (Alternatively, let us know about it in\n[community chat](https://gitter.im/openai/research).)\n\nThe best solutions will contain both code and an explanation of your\nmethodology. Please also feel free to report things you tried that\ndidn't work, or anything else helpful to someone trying to learn how\nto do their own deep learning research.\n\nWe'll accept multiple solutions to each problem, so long as each\nsolution is materially different.\n\n## This repository\n\nThis respository hosts the source for the\n[requests for research](https://openai.com/requests-for-research). Feel\nfree to open a pull request. Especially encouraged are:\n\n- Suggestions for new problems\n- Suggestions for improvements to existing problems\n- Links to your solution.\n\n## Running this repo locally\n\nInstall or upgrade `jekyll` via `gem install jekyll`. You can run this\nrepo locally via:\n\n```\njekyll serve -w\n```\n\nYour content will then be available at `http://127.0.0.1:4000/`. It\nwon't be styled, but that should be enough to get started.\n", "release_dates": []}, {"name": "retask", "description": "Retask is a simple task queue implementation written for human beings. It provides generic solution to create and manage task queues. ", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": null, "release_dates": []}, {"name": "retro", "description": "Retro Games in Gym", "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Maintenance (expect bug fixes and minor updates)\n\n# Gym Retro\n\nGym Retro lets you turn classic video games into [Gym](https://gym.openai.com/) environments for reinforcement learning and comes with integrations for ~1000 games.  It uses various emulators that support the [Libretro API](https://www.libretro.com/index.php/api/), making it fairly easy to add new emulators.\n\nSupported platforms:\n\n- Windows 7, 8, 10\n- macOS 10.13 (High Sierra), 10.14 (Mojave)\n- Linux (manylinux1)\n\nCPU with `SSSE3` or better\n\nSupported Pythons:\n\n- 3.6\n- 3.7\n- 3.8\n\nEach game integration has files listing memory locations for in-game variables, reward functions based on those variables, episode end conditions, savestates at the beginning of levels and a file containing hashes of ROMs that work with these files.\n\nPlease note that ROMs are not included and you must obtain them yourself.  Most ROM hashes are sourced from their respective No-Intro SHA-1 sums.\n\n# Documentation\n\nDocumentation is available at https://retro.readthedocs.io/en/latest/\n\nYou should probably start with the [Getting Started Guide](https://retro.readthedocs.io/en/latest/getting_started.html).\n\n# Contributing\n\n[See CONTRIBUTING.md](https://github.com/openai/retro/blob/master/CONTRIBUTING.md)\n\n# Changelog\n\n[See CHANGES.md](https://github.com/openai/retro/blob/master/CHANGES.md)\n\n# Emulated Systems\n\n- Atari\n\t- Atari2600 (via Stella)\n- NEC\n\t- TurboGrafx-16/PC Engine (via Mednafen/Beetle PCE Fast)\n- Nintendo\n\t- Game Boy/Game Boy Color (via gambatte)\n\t- Game Boy Advance (via mGBA)\n\t- Nintendo Entertainment System (via FCEUmm)\n\t- Super Nintendo Entertainment System (via Snes9x)\n- Sega\n\t- GameGear (via Genesis Plus GX)\n\t- Genesis/Mega Drive (via Genesis Plus GX)\n\t- Master System (via Genesis Plus GX)\n\nSee [LICENSES.md](https://github.com/openai/retro/blob/master/LICENSES.md) for information on the licenses of the individual cores.\n\n# Included ROMs\n\nThe following non-commercial ROMs are included with Gym Retro for testing purposes:\n\n- [the 128 sine-dot](http://www.pouet.net/prod.php?which=2762) by Anthrox\n- [Sega Tween](https://pdroms.de/files/gamegear/sega-tween) by Ben Ryves\n- [Happy 10!](http://www.pouet.net/prod.php?which=52716) by Blind IO\n- [512-Colour Test Demo](https://pdroms.de/files/pcengine/512-colour-test-demo) by Chris Covell\n- [Dekadrive](http://www.pouet.net/prod.php?which=67142) by Dekadence\n- [Automaton](https://pdroms.de/files/atari2600/automaton-minigame-compo-2003) by Derek Ledbetter\n- [Fire](http://privat.bahnhof.se/wb800787/gb/demo/64/) by dox\n- [FamiCON intro](http://www.pouet.net/prod.php?which=53497) by dr88\n- [Airstriker](https://pdroms.de/genesis/airstriker-v1-50-genesis-game) by Electrokinesis\n- [Lost Marbles](https://pdroms.de/files/gameboyadvance/lost-marbles) by Vantage\n\n# Citation\n\nPlease cite using the following BibTeX entry:\n\n```\n@article{nichol2018retro,\n  title={Gotta Learn Fast: A New Benchmark for Generalization in RL},\n  author={Nichol, Alex and Pfau, Vicki and Hesse, Christopher and Klimov, Oleg and Schulman, John},\n  journal={arXiv preprint arXiv:1804.03720},\n  year={2018}\n}\n```\n", "release_dates": ["2021-03-31T16:39:03Z", "2020-05-01T21:17:15Z", "2019-03-15T00:55:26Z"]}, {"name": "retro-baselines", "description": "Publicly releasable baselines for the Retro contest", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# retro-baselines\n\nThis is a set of baseline algorithms for the [Retro Contest](https://github.com/openai/retro-contest).\n\nAlso see [sonic-on-ray](https://github.com/openai/sonic-on-ray), which uses [Ray](https://github.com/ray-project/ray), a library for distributed execution.", "release_dates": []}, {"name": "retro-contest", "description": "OpenAI Retro Contest", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# OpenAI Retro Contest\n", "release_dates": []}, {"name": "retro-movies", "description": null, "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# retro-movies\n\nThis is a collection of human demonstrations of [gym-retro](https://github.com/openai/retro) games in BK2 format.  Right now it consists only of a single demonstration for each level of the [Sonic Benchmark](https://arxiv.org/abs/1804.03720) used by the [Retro Contest](https://contest.openai.com/). These recordings can be used to have the agent start playing from random points sampled from the course of each level, exposing the agent to a lot of areas it may not have seen if it only started from the beginning of the level.\n\nTo convert a movie to a video:\n```sh\npython -m retro.scripts.playback_movie human/SonicAndKnuckles3-Genesis/contest/SonicAndKnuckles3-Genesis-AngelIslandZone.Act1-0000.bk2\nopen human/SonicAndKnuckles3-Genesis/contest/SonicAndKnuckles3-Genesis-AngelIslandZone.Act1-0000.mp4\n```\n\nTo get states from a movie, run [get_states.py](get_states.py)\n```sh\npython -m get_states.py\n```\n\nFor convenience, we have [included statistics on the movies](get_states.results) already included in this repo.\n\nTo record your own movie (assuming you have gym-retro installed):\n```sh\npip install pyglet\npython human-recorder.py --game SonicAndKnuckles3CustomKevin-Genesis --scenario contest --state MushroomHillZone.Custom\n```\n\nHit R or Left Bumper if you're using an Xbox controller to rewind time by a short amount.\n\nWhen you hit ESC or the center Xbox button on the controller, the episode will be recorded to a file (it is not recorded until this point).\n", "release_dates": []}, {"name": "robogym", "description": "Robotics Gym Environments", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status**: Archive (code is provided as-is, no updates expected)\n\n# Robogym\n\nrobogym is a simulation framework that uses [OpenAI gym](https://gym.openai.com/) and [MuJoCo physics simulator](http://mujoco.org/) and provides a variety of robotics environments suited for robot learning in diverse settings.\n\n<p align=\"center\">\n    <img src=\"docs/assets/dactyl/dactyl_full_perpendicular.png\" height=\"300\"/> &nbsp; <img src=\"docs/assets/rearrange.png\" height=\"300\"/>\n</p>\n\n##  Supported Platforms\n\nThis package has been tested on Mac OS Mojave, Catalina and Ubuntu 16.04 LTS, and is probably fine for most recent Mac and Linux operating systems.\n\nRequires **Python 3.7.4 or greater**.\n\n## Installation\n\n\n1.  Install MuJoCo by following the [instructions](https://github.com/openai/mujoco-py#install-mujoco) from the `mujoco-py` package.\n\n2.  To checkout the code and install it, via `pip install`, run:\n\n    ```bash\n    git clone git@github.com:openai/robogym.git\n    cd robogym\n    pip install -e .\n    ```\n\n    Or you can install it directly via:\n\n    ```bash\n    pip install git+https://github.com/openai/robogym.git\n    ```\n\n## Citation\n\nPlease use the below BibTeX entry to cite this framework:\n\n```\n@misc{robogym2020,\n  author={OpenAI},\n  title={{Robogym}},\n  year={2020},\n  howpublished=\"\\url{https://github.com/openai/robogym}\",\n}\n```\n\n# Usage\n\n## Visualizing Environments\n\nYou can visualize and interact with an environment using ```robogym/scripts/examine.py```.\n\nFor example, following scripts visualize the `dactyl/locked.py` environment.\n\n```bash\npython robogym/scripts/examine.py robogym/envs/dactyl/locked.py constants='@{\"randomize\": True}'\n```\n\nNote that `constants='@{\"randomize\": True}` is an argument to set constants for the environment.\n\nSimilarly, you can set parameters of an environment as well. Below shows a command for\n visualizing a block rearrange environment with 5 objects.\n\n```bash\npython robogym/scripts/examine.py robogym/envs/rearrange/blocks.py parameters='@{\"simulation_params\": {\"num_objects\": 5}}'\n```\n\nWe support teleoperation for the rearrange environments via the `--teleoperate` option, which allows users to interact with\n an environment by controlling the robot with a keyboard.\nBelow is an example command for the teleoperation.\n\n```bash\npython robogym/scripts/examine.py robogym/envs/rearrange/blocks.py parameters='@{\"simulation_params\": {\"num_objects\": 5}}' --teleoperate\n```\n\nHold-out environments that are specified via a `jsonnet` config can also be visualized and teleoperated using this mechanism as below\n\n```bash\npython robogym/scripts/examine.py robogym/envs/rearrange/holdouts/configs/rainbow.jsonnet  --teleoperate\n```\n\n## Creating Python environments\n\nThe environments extend OpenAI gym and support the reinforcement learning interface offered by gym, including `step`, `reset`, `render` and `observe` methods.\n\nAll environment implementations are under the `robogym.envs` module and can be instantiated by calling the `make_env`  function. For example, the following code snippet creates a default locked cube environment:\n\n```python\nfrom robogym.envs.dactyl.locked import make_env\nenv = make_env()\n```\n\nSee the section on [customization](#customizing-robotics-environments) for details on how to customize an environment.\n\n# Environments\n\nAll the environment classes are subclasses of `robogym.robot_env.RobotEnv`. The classmethod `RobotEnv.build` is the main entry point for constructing an environment object, pointed by `make_env` in each environment. Customized parameters and constants should be defined by subclasses of `RobotEnvParameters` and `RobotEnvConstants`.\n\nThe physics and simulator setup are wrapped within `robogym.mujoco.simulation_interface.SimulationInterface`. There is a 1-1 mapping between one instance of `SimulationInterface` and one instance of `RobotEnv`.\n\nEach environment contains a `robot` object accessible via `env.robot` that implements the [`RobotInterface`](robogym/robot/robot_interface.py).\n\n## Training / Testing Environments\n\n### Dactyl Environments\n\nDactyl environments utilize a Shadow Robot hand robot simulation with 20 actuated degrees of freedom to perform in-hand manipulation tasks. Below is a full list of environments provided in this category:\n\n|Image|Name|Description|\n|----------|:-------------|:-------------|\n|<img src=\"docs/assets/dactyl/dactyl_locked.png\" width=\"200\"/>|dactyl/locked.py| Manipulate a locked cube with no internal  degrees of freedom to match a target pose|\n|<img src=\"docs/assets/dactyl/dactyl_face_perpendicular.png\" width=\"200\"/>|dactyl/face_perpendicular.py|Manipulate a Rubik's cube with 2 internal degrees of freedom to match a target pose and face angles|\n|<img src=\"docs/assets/dactyl/dactyl_full_perpendicular.png\" width=\"200\"/>|dactyl/full_perpendicular.py|Manipulate a Rubik's cube with full internal 6 degrees of freedom to match a target pose and face angles|\n|<img src=\"docs/assets/dactyl/dactyl_reach.png\" width=\"200\"/>|dactyl/reach.py|Reach task for fingertip target positions|\n\n### Rearrange Environments\n\nThese environments are based on a UR16e robot equipped with a RobotIQ 2f-85 gripper that is able to rearrange a variety of object distributions in a tabletop setting. Several different types of robot control modes are supported as detailed [here](robogym/robot/README.md).\n<p align=\"center\">\n<img src=\"docs/assets/rearrange.png\" width=\"400\"/>\n</p>\n\nVarious goal generators are provided to enable different tasks such as `stack`, `pick-and-place`, `reach` and `rearrange` to be specified on a given object distribution.\nList of all rearrange environments and their configs are described in\n[this document](./docs/list_rearrange_env.md).\n\nBelow is a list of object distributions supported in this category:\n\n|Image|Name|Description|\n|----------|:-------------|:-------------|\n|<img src=\"docs/assets/blocks.png\" width=\"200\"/>|rearrange/blocks.py|Samples blocks of different colors|\n|<img src=\"docs/assets/ycb.png\" width=\"200\"/>|rearrange/ycb.py|Samples from [YCB](https://www.ycbbenchmarks.com/) objects|\n|<img src=\"docs/assets/composer.png\" width=\"200\"/>|rearrange/composer.py|Samples objects that are composed of random meshes that are either basic geom shapes or random convex meshes (decomposed YCB objects)|\n|<img src=\"docs/assets/mixture.png\" width=\"200\"/>|rearrange/mixture.py|Generates objects from a mixture of mesh object distributions (supports ycb/geom mesh datasets)|\n\nFor rearrange environments, we also provide a variety of hold-out tasks that are typically used for evaluation purposes. The goal states of various hold-out environments can be seen in the image grid below.\n<p align=\"center\">\n<img src=\"docs/assets/all_holdouts.png\" width=\"800\"/>\n</p>\n\n## Customizing Robotics Environments\n\nMost robotics environments support customization by providing additional parameters via `constant`\nargument to `make_env`, you can find which constants are supported by each environment by looking\ninto definition of `<EnvName>Constants` class which usually lives under the same file as `make_env`.\nSome commonly supported constants are:\n\n- `randomize`: If true, some randomization will be applied to physics, actions and observations.\n- `mujoco_substeps`: Number of substeps per step for mujoco simulation which can be used to balance between simulation accuracy and training speed.\n- `max_timesteps_per_goal`: Max number of timesteps allowed to achieve each goal before timeout.\n\nSimilarly, there are `parameters` arguments, which can be customized together with `constants`.\nYou can find which parameters are supported by each environment by looking into definition of\n `<EnvName>Parameters`.\n\nBelow is the default settings that we use to train most of the robotics environments:\n\n```python\nenv = make_env(\n    constants={\n        'randomize': True,\n        'mujoco_substeps': 10,\n        'max_timesteps_per_goal': 400\n    },\n    parameters={\n        'n_random_initial_steps': 10,\n    }\n)\n```\n\n## Interface for Environment Randomization\n\nRobogym provides a way to intervene the environment parameters during training to support domain randomization and curriculum learning.\n\nBelow shows an example of intervening the number of objects for blocks (rearrange) environment.\nYou can use this interface to define a curriculum over the number of objects:\n\n```python\nfrom robogym.envs.rearrange.blocks import make_env\n\n# Create an environment with the default number of objects: 5\nenv = make_env(\n    parameters={\n        'simulation_params': {\n            'num_objects': 5,\n            'max_num_objects': 8,\n        }\n    }\n)\n\n# Acquire number of objects parameter interface\nparam = env.unwrapped.randomization.get_parameter(\"parameters:num_objects\")\n\n# Set num_objects: 3 for the next episode\nparam.set_value(3)\n\n# Reset to randomly generate an environment with `num_objects: 3`\nobs = env.reset()\n```\n\nSee the document on [\"Interface for Environment Randomization\"](docs/env_param_interface.md) for more details.\n\n## Create New Rearrange Environments\n\nWe provide a set of tools to help create a customized rearrange environment via teleoperation.\n\nSee the document on [\"Build New Rearrange Environments\"](docs/build_new_rearrange_envs.md) for more details.\n", "release_dates": []}, {"name": "roboschool", "description": "DEPRECATED: Open-source software for robot simulation, integrated with OpenAI Gym.", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# DEPRECATED: Please use [PyBullet](https://github.com/openai/gym/blob/master/docs/environments.md#pybullet-robotics-environments) instead\n\nNEWS\n====\n\n**2019 September 27**\n\n* We are deprecating Roboschool and now recommend using [PyBullet](https://github.com/openai/gym/blob/master/docs/environments.md#pybullet-robotics-environments) instead.\n\n**2017 July 17, Version 1.1**\n\n* All envs version bumped to \u201c-v1\", due to stronger stuck joint punishment, that improves odds of getting a good policy.\n* Flagrun-v1 is much more likely to develop a symmetric gait,\n* FlagrunHarder-v1 has new \"repeat-underlearned\" learning schedule, that allows it to be trained to stand up, walk and turn without falling. \n* Atlas robot model, modified (empty links removed, overly powerful feet weakaned).\n* All -v1 envs are shipped with better zoo policies, compared to May versions.\n* Keyboard-controlled humanoid included.\n\n\nRoboschool\n==========\n\nRelease blog post is here:\n\nhttps://blog.openai.com/roboschool/\n\n\nRoboschool is a long-term project to create simulations useful for research. The roadmap is as follows:\n\n1. Replicate Gym MuJoCo environments.\n2. Take a step away from trajectory-centric fragile MuJoCo tasks.\n3. Explore multiplayer games.\n4. Create tasks with camera RGB image and joints in a tuple.\n5. Teach robots to follow commands, including verbal commands.\n\n\nSome wiki pages:\n\n[Contributing New Environments](https://github.com/openai/roboschool/wiki/Contributing-New-Environments)\n\n[Help Wanted](https://github.com/openai/roboschool/wiki/Help-Wanted)\n\n\n\nEnvironments List\n=================\n\nThe list of Roboschool environments is as follows:\n\n- RoboschoolInvertedPendulum-v1\n- RoboschoolInvertedPendulumSwingup-v1\n- RoboschoolInvertedDoublePendulum-v1\n- RoboschoolReacher-v1\n- RoboschoolHopper-v1\n- RoboschoolWalker2d-v1\n- RoboschoolHalfCheetah-v1\n- RoboschoolAnt-v1\n- RoboschoolHumanoid-v1\n- RoboschoolHumanoidFlagrun-v1\n- RoboschoolHumanoidFlagrunHarder-v1\n- RoboschoolPong-v1\n\nTo obtain this list: `import roboschool, gym; print(\"\\n\".join(['- ' + spec.id for spec in gym.envs.registry.all() if spec.id.startswith('Roboschool')]))`.\n\n\nBasic prerequisites\n===================\nRoboschool is compatible and tested with python3 (3.5 and 3.6), osx and linux. You may be able to compile it with python2.7 (see Installation from source),\nbut that may require non-trivial amount of work. \n\nInstallation\n============\n\nIf you are running Ubuntu or Debian Linux, or OS X, the easiest path to install roboschool is via pip (:\n```bash\npip install roboschool\n```\nNote: in a headless machine (e.g. docker container) you may need to install graphics libraries; this can be achieved via `apt-get install libgl1-mesa-dev`\n\nIf you are running some other Linux/Unix distro, or want the latest and the greatest code, or want to tweak the compiler optimization options, read on...\n\nInstallation from source\n========================\n\nPrerequisites\n-------------\nFirst, make sure you are installing from a github repo (not a source package on pypi). That is, clone this repo and cd into cloned folder:\n```bash\ngit clone https://github.com/openai/roboschool && cd roboschool\n```\n\nThe system-level dependencies of roboschool are qt5 (with opengl), boost-python3 (or boost-python if you are compiling with python2), assimp and cmake. \nLinux-based distros will need patchelf utility to tweak the runtime paths. Also, some version of graphics libraries is required. \nQt5, assimp, cmake and patchelf are rather straightforward to install:\n\n- Ubuntu / Debian: \n\n    ```bash\n    sudo apt-get install qtbase5-dev libqt5opengl5-dev libassimp-dev cmake patchelf\n    ```\n\n- OSX:\n    \n    ```bash\n    brew install qt assimp cmake\n    ```\n\nNext, we'll need boost-python3. On osx `brew install boost-python3` is usually sufficient, however, on linux it is not always available as a system-level package (sometimes it is available, but compiled against wrong version of python). If you are using anaconda/miniconda, boost-python3 can be installed via `conda install boost`. Otherwise, do we despair? Of course not! We install it from source!\nThere is a script `install_boost.sh` that should do most of the heavy lifting - note that it will need sudo\nto install boost-python3 after compilation is done. \n\nNext, need a custom version of bullet physics engine. In both osx and linux its installation is a little involved, fortunately, there is a \nhelper script `install_bullet.sh` that should do it for you. \nFinally, we also need to set up some environment variables (so that pkg-config knows where has the software been installed) - this can be done via sourcing `exports.sh` script\n\nTo summarize, all the prerequisites can be installed as follows:\n- Ubuntu / Debian: \n\n    ```bash\n    sudo apt-get install qtbase5 libqt5opengl5-dev libassimp-dev patchelf cmake\n    ./install_boost.sh\n    ./install_bullet.sh\n    source exports.sh\n    ```\n- Ubuntu / Debian with anaconda:\n\n    ```bash\n    sudo apt-get install qtbase5 libqt5opengl5-dev libassimp-dev patchelf cmake\n    conda install boost\n    ./install_bullet.sh\n    source exports.sh\n    ```\n\n- OSX:\n    \n    ```bash\n    brew install qt assimp boost-python3 cmake\n    ./install_bullet.sh\n    source exports.sh\n    ```\nTo check if that installation is successful, run `pkg-config --cflags Qt5OpenGL assimp bullet` - you should see something resembling compiler options and not \nan error message. Now we are ready to compile the roboschool project itself.\n\nCompile and install\n-------------------\nThe compiler options are configured in the [Makefile](roboschool/cpp-household/Makefile). Feel free to tinker with them or leave those as is. To\ncompile the project code, and then install it as a python package, use the following:\n```bash\ncd roboschool/cpp-household && make clean && make -j4 && cd ../.. && pip install -e .\n```\n\nA simple check if resulting installation is valid:\n```python\nimport roboschool\nimport gym\n\nenv = gym.make('RoboschoolAnt-v1')\nwhile True:\n    env.step(env.action_space.sample())\n    env.render()\n```\nYou can also check the installation running a pretrained agent from the agent zoo, for instance:\n```bash\npython agent_zoo/RoboschoolHumanoidFlagrun_v0_2017may.py\n```\n\nTroubleshooting\n---------------\nA lot of the issues during installation from source are due to missing / incorrect PKG_CONFIG_PATH variable.\nIf the command `pkg-config --cflags Qt5OpenGL assimp bullet` shows an error, you can try manually finding missing `*.pc` files (for instance, for if the `pkg-config` complains about assimp, run `find / -name \"assimp.pc\"` - this is a bit bruteforce, but it works :)) and then adding folder with that files to PKG_CONFIG_PATH. \n\nSometime distros of linux may complain about generated code being not platform-independent, and ask you to recompile something with `-fPIC` option (this was seen on older versions of CentOS). In that case, try removing `-march=native` compilation option in the Makefile. \n\nOn the systems with nvidia drivers present, roboschool sometimes is not be able to find hardware-accelerated libraries. If you see errors\nlike\n```\n.build-release/render-ssao.o: In function `SimpleRender::ContextViewport::_depthlinear_paint(int)':\n/home/peter/dev/roboschool/roboschool/cpp-household/render-ssao.cpp:75: undefined reference to `glBindMultiTextureEXT'\n/home/peter/dev/roboschool/roboschool/cpp-household/render-ssao.cpp:78: undefined reference to `glBindMultiTextureEXT'\ncollect2: error: ld returned 1 exit status\nMakefile:130: recipe for target '../robot-test-tool' failed\n```\nyou can try disabling hardware rendering by setting `ROBOSCHOOL_DISABLE_HARDWARE_RENDER` env variable:\n```bash\nexport ROBOSCHOOL_DISABLE_HARDWARE_RENDER=1\n```\n\nAgent Zoo\n=========\n\nWe have provided a number of pre-trained agents in the `agent_zoo` directory.\n\nTo see a humanoid run towards a random varying target:\n\n```bash\npython agent_zoo/RoboschoolHumanoidFlagrun_v0_2017may.py\n```\n\nTo see three agents in a race:\n\n```bash\npython agent_zoo/demo_race2.py\n```\n", "release_dates": []}, {"name": "robosumo", "description": " Code for the paper \"Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments\"", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\nRoboSumo\n========\n\nThis repository contains a set of competitive multi-agent environments used in the paper [Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments](https://arxiv.org/abs/1710.03641).\n\n<table>\n    <tr>\n        <td><img src=\"/assets/robosumo1.gif?raw=true\" width=\"400\"></td>\n        <td><img src=\"/assets/robosumo2.gif?raw=true\" width=\"400\"></td>\n    </tr>\n</table>\n\n## Installation\n\nRoboSumo depends on `numpy`, `gym`, and `mujoco_py>=1.5` (if you haven't used MuJoCo before, please refer to [the installation guide](https://github.com/openai/mujoco-py)).\nRunning demos with pre-trained policies additionally requires `tensorflow>=1.1.0` and `click`.\n\nThe requirements can be installed via [pip](https://pypi.python.org/pypi/pip) as follows:\n\n```bash\n$ pip install -r requirements.txt\n```\n\nTo install RoboSumo, clone the repository and run `pip install`:\n\n```bash\n$ git clone https://github.com/openai/robosumo\n$ cd robosumo\n$ pip install -e .\n```\n\n## Demos\n\nYou can run demos of the environments using `demos/play.py` script:\n\n```bash\n$ python demos/play.py\n```\n\nThe script allows you to select different opponents as well as different policy architectures and versions for the agents.\nFor details, please refer to the help:\n\n```bash\n$ python demos/play.py --help\n\nUsage: play.py [OPTIONS]\n\nOptions:\n  --env TEXT                    Name of the environment.  [default: RoboSumo-Ant-vs-Ant-v0]\n  --policy-names [mlp|lstm]...  Policy names.  [default: mlp, mlp]\n  --param-versions INTEGER...   Policy parameter versions.  [default: 1, 1]\n  --max_episodes INTEGER        Number of episodes.  [default: 20]\n  --help                        Show this message and exit.\n```\n", "release_dates": []}, {"name": "robot_controllers", "description": "Robot control infrastructure", "language": "C++", "license": null, "readme": "# Robot Controllers\n\nThis is a robot control infrastructure, developed initially for Fetch and Freight, but\ndesigned to be robot-agnostic. In comparison to ros_control, robot_controllers offers\nthe ability to \"stack\" controllers and avoids template-based hardware interfaces.\nrobot_controllers has not be designed with real-time constraints in mind, and is intended\nprimarily for robots where the real-time joint-level controllers are run in hardware,\nas is the case for Fetch and Freight.\n\n * Devel Job Status: [![Build Status](http://build.ros.org/buildStatus/icon?job=Idev__robot_controllers__ubuntu_trusty_amd64)](http://build.ros.org/view/Idev/job/Idev__robot_controllers__ubuntu_trusty_amd64/)\n * AMD64 Debian Job Status: [![Build Status](http://build.ros.org/buildStatus/icon?job=Ibin_uT64__robot_controllers_interface__ubuntu_trusty_amd64__binary)](http://build.ros.org/view/Ibin_uT64/job/Ibin_uT64__robot_controllers_interface__ubuntu_trusty_amd64__binary/)\n", "release_dates": []}, {"name": "rosbridge", "description": "[deprecated] Bridge from Gym to ROS robots", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# rosbridge\n**Warning: abandoned in Oct 2016, when we changed to a different system. It might be a good starting point for using a Fetch robot from Gym, but nobody at OpenAI is maintaining it**\n\nA service implementing a bridge from Gym to ROS robots. Currently supports the [Fetch Research Robot](http://docs.fetchrobotics.com/)\n\n - Runs as a [ROS](http://www.ros.org) node (ie, start it with `roslaunch rosbridge fetch_proxy.launch`.\n\n - Listens on a [ZMQ](http://api.zeromq.org) socket, which an [OpenAI Gym](http://gym.openai.com) `ProxyClient` connects to.\n\n - Converts the action space to ROS commands, and the ROS sensors to an observation space.\n\n## Fetch Research robot Environments\n * See [Docs](http://docs.fetchrobotics.com/), especially the [API](http://docs.fetchrobotics.com/api_overview.html)\n\n### FetchRobot-v0: Action and observation space\n * Action is a tuple with\n  - an 8x1 vector of joint torques, in range [-1 .. +1] corresponding to the following joints:\n    - shoulder_pan_joint\n    - shoulder_lift_joint\n    - upperarm_roll_joint\n    - elbow_flex_joint\n    - forearm_roll_joint\n    - wrist_flex_joint\n    - wrist_roll_joint\n    - l_gripper_finger_joint\n * Observation is a tuple of\n  - An 8x1 vector of joint angles in range [-4 .. +4]\n  - An 8x1 vector of joint velocities in range [-4 .. +4]\n  - A 480x640 array of floats representing distance in meters\n\n### FetchRobotRGB-v0: Action and observation space\n * Action: (same as FetchRobot-v0)\n * Observation is a tuple of\n  - An 8x1 vector of joint angles in range [-4 .. +4] representing radians\n  - An 8x1 vector of joint velocities in range [-4 .. +4] representing radians/sec\n  - A 480x640x3 array of uint8s representing RGB pixel values\n\n\n### Installation on a Fetch:\nSee https://github.com/openai/fetch-config for installation scripts\n", "release_dates": []}, {"name": "safety-gym", "description": "Tools for accelerating safe exploration research.", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Safety Gym\n\nTools for accelerating safe exploration research. \n\n![](safety_gym.png)\n\n\n## Supported Platforms\n\nThis package has been tested on Mac OS Mojave and Ubuntu 16.04 LTS, and is probably fine for most recent Mac and Linux operating systems. \n\nRequires **Python 3.6 or greater.**  \n\n\n## Installation\n\nSafety Gym depends heavily on [mujoco_py](https://github.com/openai/mujoco-py), so the first step of installing Safety Gym is installing MuJoCo. See the mujoco_py documentation for details. Note that mujoco_py **requires Python 3.6 or greater**, so Safety Gym does as well.\n\nAfterwards, simply install Safety Gym by:\n\n```\ngit clone https://github.com/openai/safety-gym.git\n\ncd safety-gym\n\npip install -e .\n```\n\n\n\n## Getting Started\n\nTo use the pre-configured environments from the Safety Gym benchmark suite, simply import the package and then use `gym.make`. For example:\n\n```\nimport safety_gym\nimport gym\n\nenv = gym.make('Safexp-PointGoal1-v0')\n```\n\nFor a complete list of pre-configured environments, see below.\n\nTo create a custom environment using the Safety Gym engine, use the `Engine` class. For example, to build an environment with a car robot, the push task, some hazards, and some vases, with constraints on entering the hazard areas but no constraints on hitting the vases:\n\n```\nfrom safety_gym.envs.engine import Engine\n\nconfig = {\n    'robot_base': 'xmls/car.xml',\n    'task': 'push',\n    'observe_goal_lidar': True,\n    'observe_box_lidar': True,\n    'observe_hazards': True,\n    'observe_vases': True,\n    'constrain_hazards': True,\n    'lidar_max_dist': 3,\n    'lidar_num_bins': 16,\n    'hazards_num': 4,\n    'vases_num': 4\n}\n\nenv = Engine(config)\n```\n\nTo register that custom environment with Gym:\n\n```\nfrom gym.envs.registration import register\n\nregister(id='SafexpTestEnvironment-v0',\n         entry_point='safety_gym.envs.mujoco:Engine',\n         kwargs={'config': config})\n```\n\nFor a full list of configuration options, see the `Engine` [code itself](safety_gym/envs/engine.py). For a description of some common patterns and details that aren't obvious from the code, see the [section below](#using-engine-to-build-custom-environments).\n\nThe API for envs is the same as Gym:\n\n```\nnext_observation, reward, done, info = env.step(action)\n```\n\nThe `info` dict contains information about constraint costs. For example, in the custom environment we just built:\n\n```\n>>> info\n{'cost_hazards': 0.0, 'cost': 0.0}\n```\n\n## Read the Paper for Important Details\n\nMost of the conceptual details for Safety Gym, like what kinds of robots, tasks, and constraints Safety Gym supports, are primarily described in the paper \"Benchmarking Safe Exploration in Deep Reinforcement Learning\" by Alex Ray, Joshua Achiam, and Dario Amodei. The documentation here is meant as a supplement to the paper, to support questions about code and basic use.\n\nIf you use Safety Gym in your paper, please cite:\n\n```\n@article{Ray2019,\n    author = {Ray, Alex and Achiam, Joshua and Amodei, Dario},\n    title = {{Benchmarking Safe Exploration in Deep Reinforcement Learning}},\n    year = {2019}\n}\n```\n\n\n## Benchmark Suite\n\nAn environment in the Safety Gym benchmark suite is formed as a combination of a robot (one of `Point`, `Car`, or `Doggo`), a task (one of `Goal`, `Button`, or `Push`), and a level of difficulty (one of `0`, `1`, or `2`, with higher levels having more challenging constraints). Environments include:\n\n* `Safexp-{Robot}Goal0-v0`: A robot must navigate to a goal.\n* `Safexp-{Robot}Goal1-v0`: A robot must navigate to a goal while avoiding hazards. One vase is present in the scene, but the agent is not penalized for hitting it.\n* `Safexp-{Robot}Goal2-v0`: A robot must navigate to a goal while avoiding more hazards and vases.\n* `Safexp-{Robot}Button0-v0`: A robot must press a goal button.\n* `Safexp-{Robot}Button1-v0`: A robot must press a goal button while avoiding hazards and gremlins, and while not pressing any of the wrong buttons. \n* `Safexp-{Robot}Button2-v0`: A robot must press a goal button while avoiding more hazards and gremlins, and while not pressing any of the wrong buttons.  \n* `Safexp-{Robot}Push0-v0`: A robot must push a box to a goal.\n* `Safexp-{Robot}Push1-v0`: A robot must push a box to a goal while avoiding hazards. One pillar is present in the scene, but the agent is not penalized for hitting it. \n* `Safexp-{Robot}Push2-v0`: A robot must push a box to a goal while avoiding more hazards and pillars.\n\n(To make one of the above, make sure to substitute `{Robot}` for one of `Point`, `Car`, or `Doggo`.)\n\n\n## Comparing Algorithms with Benchmark Scores\n\nWhen using Safety Gym for research, we recommend comparing algorithms using aggregate metrics to represent performance across the entire benchmark suite or a subset of it. The aggregate metrics we recommend in the paper are:\n\n* Average (over environments and random seeds) normalized average (over episodes) return of the final policy.\n* Average normalized constraint violation of the final policy.\n* Average normalized cost rate over training (sum of all costs incurred during training divided by number of environment interaction steps).\n\nWe compute normalized scores using reference statistics from our run of unconstrained PPO, with 10M env steps for environments with Point or Car robots and 100M env steps for environments with the Doggo robot. These reference statistics are available in [the bench folder](safety_gym/bench/characteristic_scores.json), and we provide a [utility function](safety_gym/bench/bench_utils.py#L40) to calculate normalized for an arbitrary environment.\n\n\n## Using Engine to Build Custom Environments\n\n\nAgain, most of the conceptual details for Engine are described in the paper. But here, we'll describe some patterns and code details not covered there.\n\n**Defaults for Sensors:** By default, the only sensors enabled are basic robot sensors: accelerometer, gyro, magnetometer, velocimeter, joint angles, and joint velocities. All other sensors (lidars for perceiving objects in the scene, vision, compasses, amount of time remaining, and a few others) are _disabled_ by default. To use them, you will have to explicitly enable them by passing in flags via the `Engine` config. Note that simply adding an object to a scene will not result in the corresponding sensor for that object becoming enabled, you have to pass the flag.\n\n**Vision:** Vision is included as an option but is fairly minimally supported and we have not yet tested it extensively. Feature requests or bug-fixes related to vision will be considered low-priority relative to other functionality.\n\n**Lidar and Pseudo-Lidar:** Lidar and pseudo-lidar are the main ways to observe objects. Lidar works by ray-tracing (using tools provided by MuJoCo), whereas pseudo-lidar works by looping over all objects in a scene, determining if they're in range, and then filling the appropriate lidar bins with the right values. They both share several details: in both cases, each lidar has a fixed number of bins spaced evenly around a full circle around the robot. \n\nLidar-like observations are object-specific. That is, if you have hazards, vases, and goals in a scene, you would want to turn on the hazards lidar (through `observe_hazards`), the vases lidar (through `observe_vases`), and possibly the goals lidar (through `observe_goal_lidar`) as well. \n\nAll lidar-like observations will be either true lidar or pseudo-lidar, depending on the `lidar_type` flag. By default, `lidar_type='pseudo'`. To use true lidar instead, set `lidar_type='natural'`.\n\nLidar observations are represented visually by \"lidar halos\" that hover above the agent. Each lidar halo has as many orbs as lidar bins, and an orb will light up if an object is in range of its corresponding bin. Lidar halos are nonphysical and do not interact with objects in the scene; they are purely there for the benefit of someone watching a video of the agent, so that it is clear what the agent is observing.\n\nFor pseudo-lidar specifically: normally, lidar-like observations would break the principle about small changes in state resulting in small changes in observation, since a small change in state could move an object from one bin to another.  We add a small \u201calias\u201d signal for each bin into the neighboring bins, which smooths transitions between bins and additionally allows the observation to weakly localize an object within a bin.\n\n**Defaults for Objects and Constraints:** By default, the only thing present in a scene is the robot (which defaults to `Car`). Everything else must be explicitly added. Adding an obstacle object (such as a hazard or a vase) to a scene does _not_ automatically add the constraint; if you want interactions with an obstacle to be constrained, you must also pass the flag to enable the constraint.\n\n**Environment Layouts:** By default, environment layouts are randomly generated at the start of each episode. This behavior can be disabled by setting `randomize_layout=False`, in which case the environment layout is randomized once on initialization, and then it is reset to the same layout at the start of each new episode. Random layout generation works by sampling and can fail: the generator randomly places objects in a scene until there is a conflict (eg two objects overlap unacceptably). If it can't resolve the conflict by just resampling the last object placed, it throws the layout and starts over. If it can't find a valid layout after trying a (large) fixed number of times, `Engine` raises an exception. Details related to random object placement are described below.\n\n**Placements, Locations, and Keepout:** For all of the different kinds of objects you can add to a Safety Gym environment, you can configure where they go in the scene through their `{object}s_placements`, `{object}s_locations`, and `{object}s_keepout` flags. You can set it up so that they are randomly placed around the scene at the start of each episode (through placements), or fixed to specific locations (through locations), and you can control how close they can be to other objects in the scene (through keepout).\n\n`{object}s_placements` should be a list of (xmin, ymin, xmax, ymax) tuples, where each tuple describes a rectangular area where the object can be randomly placed. If none is given, it will default to the full size of the scene (given by the `placements_extents` flag). \n\n`{object}s_locations` should be a list of (x,y) locations where such objects should go exactly. \n\nAt the start of an episode, when an environment layout is sampled, the layout sampler will first satisfy the `{object}s_locations` requirements. Suppose there are going to be 4 objects in the scene (specified with `{object}s_num`), and `{object}s_locations` is a list of 2 (x,y) locations. Then 2 objects will be placed on those locations. Afterwards, the remaining 2 objects will be randomly located according to `{object}s_placements`. If there are more locations than objects, the excess locations will be ignored.\n\n`{object}s_keepout` specifies a radius around an object location that other objects are required to keep out of. Take caution in setting this: if objects and their keepouts are too big, and there are too many objects in the scene, the layout sampler may fail to generate a feasible layout.", "release_dates": []}, {"name": "safety-starter-agents", "description": "Basic constrained RL agents used in experiments for the \"Benchmarking Safe Exploration in Deep Reinforcement Learning\" paper.", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Safety Starter Agents\n\nA companion repo to the paper \"Benchmarking Safe Exploration in Deep Reinforcement Learning,\" containing a variety of unconstrained and constrained RL algorithms.\n\nThis repo contains the implementations of PPO, TRPO, PPO-Lagrangian, TRPO-Lagrangian, and CPO used to obtain the results in the \"Benchmarking Safe Exploration\" paper, as well as experimental implementations of SAC and SAC-Lagrangian not used in the paper.\n\nNote that the PPO implementations here follow the convention from [Spinning Up](https://spinningup.openai.com) rather than [Baselines](https://www.github.com/openai/baselines): they use the early stopping trick, omit observation and reward normalization, and do not use the clipped value loss, among other potential diffs. As a result, while it is easy to fairly compare this PPO to this TRPO, it is not the strongest PPO implementation (in the sense of sample efficiency) and can be improved on substantially.\n\n## Supported Platforms\n\nThis package has been tested on Mac OS Mojave and Ubuntu 16.04 LTS, and is probably fine for most recent Mac and Linux operating systems. \n\nRequires **Python 3.6 or greater.**  \n\n## Installation\n\nTo install this package:\n\n```\ngit clone https://github.com/openai/safety-starter-agents.git\n\ncd safety-starter-agents\n\npip install -e .\n```\n\n**Warning:** Installing this package does **not** install Safety Gym. If you want to use the algorithms in this package to train agents on onstrained RL environments, make sure to install Safety Gym according to the instructions on the [Safety Gym repo](https://www.github.com/openai/safety-gym).\n\n\n## Getting Started\n\n**Example Script:** To run PPO-Lagrangian on the `Safexp-PointGoal1-v0` environment from Safety Gym, using neural networks of size (64,64):\n\n```\nfrom safe_rl import ppo_lagrangian\nimport gym, safety_gym\n\nppo_lagrangian(\n\tenv_fn = lambda : gym.make('Safexp-PointGoal1-v0'),\n\tac_kwargs = dict(hidden_sizes=(64,64))\n\t)\n\n```\n\n\n**Reproduce Experiments from Paper:** To reproduce an experiment from the paper, run:\n\n```\ncd /path/to/safety-starter-agents/scripts\npython experiment.py --algo ALGO --task TASK --robot ROBOT --seed SEED \n\t--exp_name EXP_NAME --cpu CPU\n```\n\nwhere \n\n* `ALGO` is in `['ppo', 'ppo_lagrangian', 'trpo', 'trpo_lagrangian', 'cpo']`.\n* `TASK` is in `['goal1', 'goal2', 'button1', 'button2', 'push1', 'push2']` .\n* `ROBOT` is in `['point', 'car', 'doggo']`.\n* `SEED` is an integer. In the paper experiments, we used seeds of 0, 10, and 20, but results may not reproduce perfectly deterministically across machines.\n* `CPU` is an integer for how many CPUs to parallelize across.\n\n`EXP_NAME` is an optional argument for the name of the folder where results will be saved. The save folder will be placed in `/path/to/safety-starter-agents/data`. \n\n\n**Plot Results:** Plot results with:\n\n```\ncd /path/to/safety-starter-agents/scripts\npython plot.py data/path/to/experiment\n```\n\n**Watch Trained Policies:** Test policies with:\n\n```\ncd /path/to/safety-starter-agents/scripts\npython test_policy.py data/path/to/experiment\n```\n\n\n## Cite the Paper\n\nIf you use Safety Starter Agents code in your paper, please cite:\n\n```\n@article{Ray2019,\n    author = {Ray, Alex and Achiam, Joshua and Amodei, Dario},\n    title = {{Benchmarking Safe Exploration in Deep Reinforcement Learning}},\n    year = {2019}\n}\n```", "release_dates": []}, {"name": "scheduler-plugins", "description": "Repository for out-of-tree scheduler plugins based on scheduler framework.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Scheduler Plugins\n\nRepository for out-of-tree scheduler plugins based on the [scheduler framework](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/).\n\n## Install\n\nContainer images are available in the official scheduler-plugins k8s container registry. There are two images one\nfor the kube-scheduler and one for the controller. See the [Compatibility Matrix section](#compatibility-matrix)\nfor the complete list of images.\n\n```shell\ndocker pull k8s.gcr.io/scheduler-plugins/kube-scheduler:$TAG\ndocker pull k8s.gcr.io/scheduler-plugins/controller:$TAG\n```\n\nYou can find [how to install release image](doc/install.md) here.\n\n## Plugins\n\nThe kube-scheduler binary includes the below list of plugins. They can be configured by creating one or more\n[scheduler profiles](https://kubernetes.io/docs/reference/scheduling/config/#multiple-profiles).\n\n* [Capacity Scheduling](pkg/capacityscheduling/README.md)\n* [Coscheduling](pkg/coscheduling/README.md)\n* [Node Resources](pkg/noderesources/README.md)\n* [Node Resource Topology](pkg/noderesourcetopology/README.md)\n* [Trimaran](pkg/trimaran/README.md)\n\nAdditionally the kube-scheduler binary includes the below list of sample plugins. These plugins are not intended for use in production\nenvironments.\n\n* [Cross Node Preemption](pkg/crossnodepreemption/README.md)\n* [Pod State](pkg/podstate/README.md)\n* [Quality of Service](pkg/qos/README.md)\n\n## Compatibility Matrix\n\nThe below compatibility matrix shows the k8s client package (client-go, apimachinery, etc) versions\nthat the scheduler-plugins are compiled with.\n\nThe minor version of the scheduler-plugins matches the minor version of the k8s client packages that\nit is compiled with. For example scheduler-plugins `v0.18.x` releases are built with k8s `v1.18.x`\ndependencies.\n\nThe scheduler-plugins patch versions come in two different varieties (single digit or three digits).\nThe single digit patch versions (e.g., `v0.18.9`) exactly align with the the k8s client package\nversions that the scheduler plugins are built with. The three digit patch versions, which are built\non demand, (e.g., `v0.18.800`) are used to indicated that the k8s client package versions have not\nchanged since the previous release, and that only scheduler plugins code (features or bug fixes) was\nchanged.\n\nScheduler Plugins  | Compiled With k8s Version | Container Image                                      | Arch           |\n-------------------|---------------------------|------------------------------------------------------|----------------|\nv0.20.10           | v1.20.10                  | k8s.gcr.io/scheduler-plugins/kube-scheduler:v0.20.10 | AMD64<br>ARM64 |\nv0.19.9            | v1.19.9                   | k8s.gcr.io/scheduler-plugins/kube-scheduler:v0.19.9  | AMD64<br>ARM64 |\nv0.19.8            | v1.19.8                   | k8s.gcr.io/scheduler-plugins/kube-scheduler:v0.19.8  | AMD64<br>ARM64 |\nv0.18.9            | v1.18.9                   | k8s.gcr.io/scheduler-plugins/kube-scheduler:v0.18.9  | AMD64          |\n\nController | Compiled With k8s Version | Container Image                                  | Arch           |\n-----------|---------------------------|--------------------------------------------------|----------------|\nv0.20.10   | v1.20.10                  | k8s.gcr.io/scheduler-plugins/controller:v0.20.10 | AMD64<br>ARM64 |\nv0.19.9    | v1.19.9                   | k8s.gcr.io/scheduler-plugins/controller:v0.19.9  | AMD64<br>ARM64 |\nv0.19.8    | v1.19.8                   | k8s.gcr.io/scheduler-plugins/controller:v0.19.8  | AMD64<br>ARM64 |\n\n## Community, discussion, contribution, and support\n\nLearn how to engage with the Kubernetes community on the [community page](http://kubernetes.io/community/).\n\nYou can reach the maintainers of this project at:\n\n- [Slack](https://kubernetes.slack.com/messages/sig-scheduling)\n- [Mailing List](https://groups.google.com/forum/#!forum/kubernetes-sig-scheduling)\n\nYou can find an [instruction how to build and run out-of-tree plugin here](doc/develop.md) .\n\n### Code of conduct\n\nParticipation in the Kubernetes community is governed by the [Kubernetes Code of Conduct](code-of-conduct.md).\n", "release_dates": ["2021-08-27T16:57:29Z", "2021-08-27T16:54:18Z"]}, {"name": "shap-e", "description": "Generate 3D objects conditioned on text or images", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Shap-E\n\nThis is the official code and model release for [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463).\n\n * See [Usage](#usage) for guidance on how to use this repository.\n * See [Samples](#samples) for examples of what our text-conditional model can generate.\n\n# Samples\n\nHere are some highlighted samples from our text-conditional model. For random samples on selected prompts, see [samples.md](samples.md).\n\n<table>\n    <tbody>\n        <tr>\n            <td align=\"center\">\n                <img src=\"samples/a_chair_that_looks_like_an_avocado/2.gif\" alt=\"A chair that looks like an avocado\">\n            </td>\n            <td align=\"center\">\n                <img src=\"samples/an_airplane_that_looks_like_a_banana/3.gif\" alt=\"An airplane that looks like a banana\">\n            </td align=\"center\">\n            <td align=\"center\">\n                <img src=\"samples/a_spaceship/0.gif\" alt=\"A spaceship\">\n            </td>\n        </tr>\n        <tr>\n            <td align=\"center\">A chair that looks<br>like an avocado</td>\n            <td align=\"center\">An airplane that looks<br>like a banana</td>\n            <td align=\"center\">A spaceship</td>\n        </tr>\n        <tr>\n            <td align=\"center\">\n                <img src=\"samples/a_birthday_cupcake/3.gif\" alt=\"A birthday cupcake\">\n            </td>\n            <td align=\"center\">\n                <img src=\"samples/a_chair_that_looks_like_a_tree/2.gif\" alt=\"A chair that looks like a tree\">\n            </td>\n            <td align=\"center\">\n                <img src=\"samples/a_green_boot/3.gif\" alt=\"A green boot\">\n            </td>\n        </tr>\n        <tr>\n            <td align=\"center\">A birthday cupcake</td>\n            <td align=\"center\">A chair that looks<br>like a tree</td>\n            <td align=\"center\">A green boot</td>\n        </tr>\n        <tr>\n            <td align=\"center\">\n                <img src=\"samples/a_penguin/1.gif\" alt=\"A penguin\">\n            </td>\n            <td align=\"center\">\n                <img src=\"samples/ube_ice_cream_cone/3.gif\" alt=\"Ube ice cream cone\">\n            </td>\n            <td align=\"center\">\n                <img src=\"samples/a_bowl_of_vegetables/2.gif\" alt=\"A bowl of vegetables\">\n            </td>\n        </tr>\n        <tr>\n            <td align=\"center\">A penguin</td>\n            <td align=\"center\">Ube ice cream cone</td>\n            <td align=\"center\">A bowl of vegetables</td>\n        </tr>\n    </tbody>\n<table>\n\n# Usage\n\nInstall with `pip install -e .`.\n\nTo get started with examples, see the following notebooks:\n\n* [sample_text_to_3d.ipynb](shap_e/examples/sample_text_to_3d.ipynb) - sample a 3D model, conditioned on a text prompt.\n* [sample_image_to_3d.ipynb](shap_e/examples/sample_image_to_3d.ipynb) - sample a 3D model, conditioned on a synthetic view image. To get the best result, you should remove background from the input image.\n* [encode_model.ipynb](shap_e/examples/encode_model.ipynb) - loads a 3D model or a trimesh, creates a batch of multiview renders and a point cloud, encodes them into a latent, and renders it back. For this to work, install Blender version 3.3.1 or higher, and set the environment variable `BLENDER_PATH` to the path of the Blender executable.\n", "release_dates": []}, {"name": "signup-forms", "description": "Code for the paper \"World of Bits: An Open-Domain Platform for Web-Based Agents\"", "language": "CSS", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Signup Forms\n\nThis repo hosts a collection of signup forms. Each signup form will have a public url:\n\n```\nhttps://openai.github.io/signup-forms/<id>/\n```\n\nThe purpose of this repo is to provide WoB a list of urls for the `SignUp` task.", "release_dates": []}, {"name": "sonic-on-ray", "description": "Training Sonic with RLlib", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Sonic on Ray\n\nThis file describes how to use Sonic with Ray and RLLib. We include\ninstructions on how to get the training running on EC2.\n\n## Running training on a single node\n\nStart a p2.8xlarge with the Deep Learning AMI (Ubuntu). In us-west-2, this is\nami-d2c759aa.\n\nActivate the TensorFlow environment with\n\n```\nsource activate tensorflow_p36\n```\n\nNow install Ray and the RLlib requirements using\n\n```\npip install ray opencv-python\n```\n\nNext we need to install the gym retro environment. Run\n\n```\ngit clone --recursive git@github.com:openai/retro.git gym-retro\ncd gym-retro\npip install -e .\n```\n\nNow clone this repo and install it:\n\n```\ncd ~\ngit clone git@github.com:openai/sonic-on-ray.git\ncd sonic-on-ray\npip install -e .\n```\n\nYou can then run the training with\n\n```\ncd ~/sonic-on-ray\npython train_ppo.py\n```\n\n## Running training on a cluster\n\nFirst install Ray on your laptop with\n\n```\npip install ray\n```\n\nNow clone the sonic-on-ray repo with\n\n```\ngit clone git@github.com:openai/sonic-on-ray.git\n```\n\nAnd start a cluster with\n\n```\nray create_or_update sonic-autoscaler.yaml\n```\n\nAfter the cluster has been started, you will see a message like this:\n\n```\nStarted Ray on this node. You can add additional nodes to the cluster by calling\n\n    ray start --redis-address 172.31.58.176:6379\n\nfrom the node you wish to add. You can connect a driver to the cluster from Python by running\n\n    import ray\n    ray.init(redis_address=\"172.31.58.176:6379\")\n\n[...]\n\nTo login to the cluster, run:\n\n      ssh -i ~/.ssh/ray-autoscaler_us-east-1.pem ubuntu@54.152.27.84\n```\n\nYou can now start the hyperparameter search by sshing into the cluster, running\n\n```\nsource activate tensorflow_p36\n```\n\nand replacing the `ray.init()` call in `~/sonic-on-ray/train_ppo_grid_search.py`\nby the one printed above and then running the script.\n", "release_dates": []}, {"name": "sparse_attention", "description": "Examples of using sparse attention, as in \"Generating Long Sequences with Sparse Transformers\"", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n**Update August 2020:** For an example repository that achieves state-of-the-art modeling performance on CIFAR-10 using Sparse Transformers, please see https://github.com/openai/distribution_augmentation\n\n# Sparse Attention\n\nThis repository contains the sparse attention primitives used in Sparse Transformers (see [blog](https://openai.com/blog/sparse-transformer) and [paper](https://arxiv.org/abs/1904.10509)). Specifically, it includes the following:\n\n1) A faster implementation of normal attention (the upper triangle is not computed, and many operations are fused).\n2) An implementation of \"strided\" and \"fixed\" attention, as in the Sparse Transformers paper.\n3) A simple recompute decorator, which can be adapted for usage with attention.\n\nWe hope this code can further accelerate research into sparse attention.\n\nAn example Transformer implementation which is close to the version we use internally can be found at https://github.com/openai/blocksparse/blob/master/examples/transformer/enwik8.py. \n\n# Overview of kernels\nThe repository contains fused implementations of the attention operation, which takes in `Q`, `K`, `V` matrices (all of dimensionality `batch, time, dim`) representing the queries, keys, and values for a sequence. For every query element, a weighted sum of the values is returned, where the weightings are determined by the scaled matrix product of `Q` and `K^T`.\n\nThe kernels allow specification of block sparsity in the `QK^T` matrix. This means you define a pattern of 0/1s on a `[time/blocksize, time/blocksize]` matrix of blocks, and the values where it is 0 will not be computed, and not be included in the softmax calculation. Additionally, one can define \"callbacks\" on the computed blocks, which will further mask out values in any given block from the softmax (though the matrix product will still be computed for those elements). \n\nBlock sizes of `{8, 16, 32, 64}` are supported, and slight advantages in speed may be seen from using larger blocks.\n\n# Prerequisites\nFor fp32 and blocksize `32`, any NVIDIA GPU past Kepler can be used (i.e. compute capability beyond 3.5).\n\nFor fp16 and blocksize `8, 16, 32, 64`, a GPU with Tensor Cores (e.g. the V100 GPU, compute capability >= 7.0) is required.\n\nThe primary dependency is the OpenAI [blocksparse](https://github.com/openai/blocksparse/) package.\n\nWith CUDA 10 and tensorflow-gpu, you can install blocksparse with `pip install blocksparse`.\n\nFor other setups, you must install blocksparse from source, and directions can be found in the [root of the repository](https://github.com/openai/blocksparse/).\n\n# Examples\n\nRun the following on a non-V100 GPU:\n```\npython attention.py\n```\n\nOn a V100 GPU:\n```\npython attention.py fp16\n```\n\n# General usage\nAn example can be found at the bottom of `attention.py`.\n\n```python\n\nfull_attn_tf = attention_impl(q, k, v, heads=4, attn_mode=\"all\", recompute=True)\nfull_attn_bs = blocksparse_attention_impl(q, k, v, heads=4, attn_mode=\"all\", recompute=True)\n\n# first step of strided attention\nlocal_attn_bs = blocksparse_attention_impl(q, k, v, heads=4, attn_mode=\"local\", local_attn_ctx=32, recompute=True)\nlocal_attn_tf = attention_impl(q, k, v, heads=4, attn_mode=\"local\", local_attn_ctx=32, recompute=True)\n\n# second step of strided attention\nstrided_attn_bs = blocksparse_attention_impl(q, k, v, heads=4, attn_mode=\"strided\", local_attn_ctx=32, recompute=True)\nstrided_attn_tf = attention_impl(q, k, v, heads=4, attn_mode=\"strided\", local_attn_ctx=32, recompute=True)\n\n# # the 'fixed' attention pattern\nfixed = blocksparse_attention_impl(q, k, v, heads=4, attn_mode=\"fixed\", local_attn_ctx=128, num_verts=4, vertsize=1, recompute=True)\n\n```\n\n# Referencing this work\n\nIf you find this helpful in your work, you can consider citing the following:\n\n```\n@article{child2019sparsetransformer,\n  title={Generating Long Sequences with Sparse Transformers},\n  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},\n  journal={URL https://openai.com/blog/sparse-transformers},\n  year={2019}\n}\n```\n", "release_dates": []}, {"name": "sparse_autoencoder", "description": null, "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Sparse autoencoder for GPT2 small\n\nThis repository hosts a sparse autoencoder trained on the GPT2-small model's activations.\nThe autoencoder's purpose is to expand the MLP layer activations into a larger number of dimensions,\nproviding an overcomplete basis of the MLP activation space. The learned dimensions have been\nshown to be more interpretable than the original MLP dimensions.\n\n### Install\n\n```sh\npip install git+https://github.com/openai/sparse_autoencoder.git\n```\n\n### Example usage\n\n```py\nimport torch\nimport blobfile as bf\nimport transformer_lens\nimport sparse_autoencoder\n\n# Load the autoencoder\nlayer_index = 0  # in range(12)\nautoencoder_input = [\"mlp_post_act\", \"resid_delta_mlp\"][0]\nfilename = f\"az://openaipublic/sparse-autoencoder/gpt2-small/{autoencoder_input}/autoencoders/{layer_index}.pt\"\nwith bf.BlobFile(filename, mode=\"rb\") as f:\n    state_dict = torch.load(f)\n    autoencoder = sparse_autoencoder.Autoencoder.from_state_dict(state_dict)\n\n# Extract neuron activations with transformer_lens\nmodel = transformer_lens.HookedTransformer.from_pretrained(\"gpt2\", center_writing_weights=False)\nprompt = \"This is an example of a prompt that\"\ntokens = model.to_tokens(prompt)  # (1, n_tokens)\nprint(model.to_str_tokens(tokens))\nwith torch.no_grad():\n    logits, activation_cache = model.run_with_cache(tokens, remove_batch_dim=True)\nif autoencoder_input == \"mlp_post_act\":\n    input_tensor = activation_cache[f\"blocks.{layer_index}.mlp.hook_post\"]  # (n_tokens, n_neurons)\nelif autoencoder_input == \"resid_delta_mlp\":\n    input_tensor = activation_cache[f\"blocks.{layer_index}.hook_mlp_out\"]  # (n_tokens, n_residual_channels)\n\n# Encode neuron activations with the autoencoder\ndevice = next(model.parameters()).device\nautoencoder.to(device)\nwith torch.no_grad():\n    latent_activations = autoencoder.encode(input_tensor)  # (n_tokens, n_latents)\n```\n\n### Autoencoder settings\n\n- Model used: \"gpt2-small\", 12 layers\n- Autoencoder architecture: see `model.py`\n- Autoencoder input: \"mlp_post_act\" (3072 dimensions) or \"resid_delta_mlp\" (768 dimensions)\n- Number of autoencoder latents: 32768\n- Loss function: see `loss.py`\n- Number of training tokens: ~64M\n- L1 regularization strength: 0.01\n\n### Data files\n\n- `autoencoder_input` is in [\"mlp_post_act\", \"resid_delta_mlp\"]\n- `layer_index` is in range(12) (GPT2-small)\n- `latent_index` is in range(32768)\n\nAutoencoder files:\n`az://openaipublic/sparse-autoencoder/gpt2-small/{autoencoder_input}/autoencoders/{layer_index}.pt`\n\nNeuronRecord files:\n`az://openaipublic/sparse-autoencoder/gpt2-small/{autoencoder_input}/collated_activations/{layer_index}/{latent_index}.json`\n", "release_dates": []}, {"name": "spinningup", "description": "An educational resource to help anyone learn deep reinforcement learning.", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Maintenance (expect bug fixes and minor updates)\n\nWelcome to Spinning Up in Deep RL! \n==================================\n\nThis is an educational resource produced by OpenAI that makes it easier to learn about deep reinforcement learning (deep RL).\n\nFor the unfamiliar: [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning) (RL) is a machine learning approach for teaching agents how to solve tasks by trial and error. Deep RL refers to the combination of RL with [deep learning](http://ufldl.stanford.edu/tutorial/).\n\nThis module contains a variety of helpful resources, including:\n\n- a short [introduction](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) to RL terminology, kinds of algorithms, and basic theory,\n- an [essay](https://spinningup.openai.com/en/latest/spinningup/spinningup.html) about how to grow into an RL research role,\n- a [curated list](https://spinningup.openai.com/en/latest/spinningup/keypapers.html) of important papers organized by topic,\n- a well-documented [code repo](https://github.com/openai/spinningup) of short, standalone implementations of key algorithms,\n- and a few [exercises](https://spinningup.openai.com/en/latest/spinningup/exercises.html) to serve as warm-ups.\n\nGet started at [spinningup.openai.com](https://spinningup.openai.com)!\n\n\nCiting Spinning Up\n------------------\n\nIf you reference or use Spinning Up in your research, please cite:\n\n```\n@article{SpinningUp2018,\n    author = {Achiam, Joshua},\n    title = {{Spinning Up in Deep Reinforcement Learning}},\n    year = {2018}\n}\n```", "release_dates": ["2020-01-30T16:20:04Z"]}, {"name": "spinningup-workshop", "description": "For educational materials related to the spinning up workshops.", "language": "TeX", "license": null, "readme": "# Spinning Up Workshop Materials\n\n## Morning talks schedule:\n\n9-10am: [Intro to RL, Part 1](https://github.com/openai/spinningup-workshop/blob/master/rl_intro/rl_intro.pdf), presented by Joshua Achiam\n\n10:15-11am: [Intro to RL, Part 2](https://github.com/openai/spinningup-workshop/blob/master/rl_intro/rl_intro.pdf), presented by Joshua Achiam\n\n11-11:30am: [Learning Dexterity](https://github.com/openai/spinningup-workshop/tree/master/research_talks), presented by Matthias Plappert\n\n11:30am-12pm: Intro to AI Safety, presented by Dario Amodei\n\n## Breakout sessions:\n\n[Intro to Tensorflow](https://github.com/openai/spinningup-workshop/blob/master/tensorflow_review/tf_review_session.pdf)\n\n[Writing DQN Together](https://github.com/openai/spinningup-workshop/tree/master/writing_dqn_together)\n\n[Advanced RL Q&A](https://github.com/openai/spinningup-workshop/blob/master/rl_intro/rl_intro_part2.pdf)\n\n\n## Project recommendations:\n\nSee [this list](https://docs.google.com/document/d/1pmqV-dAFSRDVYct9m6smP702f9cUIsPYTyamPLUcQZs/edit#) of recommended projects if you need something to hack on!\n", "release_dates": []}, {"name": "staged-recipes", "description": "A place to submit conda recipes before they become fully fledged conda-forge feedstocks", "language": "Python", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "readme": "## About\n\nThis repo is a holding area for recipes destined for a conda-forge feedstock repo. To find out more about conda-forge, see https://github.com/conda-forge/conda-smithy.\n\n[![Join the chat at https://gitter.im/conda-forge/conda-forge.github.io](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/conda-forge/conda-forge.github.io)\n\n\n## Build status\n\n[![Circle CI](https://circleci.com/gh/conda-forge/staged-recipes/tree/master.svg?style=shield)](https://circleci.com/gh/conda-forge/staged-recipes/tree/master) [![Build Status](https://travis-ci.org/conda-forge/staged-recipes.svg?branch=master)](https://travis-ci.org/conda-forge/staged-recipes) [![Build status](https://ci.appveyor.com/api/projects/status/3lju80dibkmowsj5/branch/master?svg=true)](https://ci.appveyor.com/project/conda-forge/staged-recipes/branch/master)\n\n## Getting started\n\n1. Fork this repository.\n2. Make a new folder in `recipes` for your package. Look at the example recipe and our [FAQ](https://github.com/conda-forge/staged-recipes#faq) for help.\n3. Open a pull request. Building of your package will be tested on Windows, Mac and Linux.\n4. When your pull request is merged a new repository, called a feedstock, will be create in the github conda-forge organization, and build/upload of your package will automatically be triggered. Once complete, the package is available on conda-forge.\n\n\n## FAQ\n\n### 1. **How do I start editing the recipe?**\n\nThere are two ways to get started:\n\na. If it is a python package you can generate a skeleton as a starting point with\n`conda skeleton pypi your_package_name`. You do *not* have to use skeleton, and the\nrecipes produced by skeleton will need to be edited.\n\nb. Look at one of [these examples](https://github.com/conda-forge/staged-recipes/tree/master/recipes)\nin this repository and modify it as necessary.\n\nYour final recipe should have no comments and follow the order in the example.\n\n*If there are details you are not sure about please open a pull request. The conda-forge team will be happy to answer your questions.*\n\n### 2. **How do I populate the `hash` field?**\n\nIf your package is on PyPI, you can get the md5 hash from your package's page on PyPI; look for the `md5` link next to the download link for your package.\n\nYou can also generate a hash from the command line on Linux (and Mac if you install the necessary tools below). If you go this route, the `sha256` hash is preferable to the `md5` hash.\n\nTo generate the `md5` hash: `md5 your_sdist.tar.gz`\n\nTo generate the `sha256` hash: `openssl sha256 your_sdist.tar.gz`\n\nYou may need the openssl package, available on conda-forge\n`conda install openssl -c conda-forge`\n\n### 3. **How do I exclude a platform?**\n\nUse the `skip` key in the `build` section along with a selector:\n\n```yaml\nbuild:\n    skip: true  # [win]\n```\n\nA full description of selectors is [in the conda docs](http://conda.pydata.org/docs/building/meta-yaml.html#preprocessing-selectors).\n\n### 4. **What does `numpy x.x` mean?**\n\nIf you have a package which links against numpy you need to build and run against the same version of numpy.\nPutting `numpy x.x` in the build and run requirements ensure that a separate package will be built for each\nversion of numpy that conda-forge builds against.\n\n### 5. **What does the `build: 0` entry mean?**\n\nThe build number is used when the source code for the package has not changed but you need to make a new\nbuild. For example, if one of the dependencies of the package was not properly specified the first time\nyou build a package, then when you fix the dependency and rebuild the package you should increase the build\nnumber.\n\nWhen the package version changes you should reset the build number to `0`.\n\n### 6. **Do I have to import all of my unit tests into the recipe's `test` field?**\n\nNo, you do not.\n\n### 7. **Do all of my package's dependencies have to be in conda(-forge) already?**\n\nShort answer: yes. Long answer: In principle, as long as your dependencies are in at least one of\nyour user's conda channels they will be able to install your package. In practice, that is difficult\nto manage, and we strive to get all dependencies built in conda-forge.\n\n### 8. **When or why do I need to use `python setup.py install --single-version-externally-managed --record record.txt`?**\n\nThese options should be added to setup.py if your project uses setuptools. The goal is to prevent `setuptools` from creating an `egg-info` directory because they do not interact well with conda.\n\n### 9. **Do I need `bld.bat` and/or `build.sh`?**\n\nIn many cases, no. Python packages almost never need it. If the build can be done with one line you can put it in the `script` line of the `build` section.\n\n### 10. What does being a conda-forge feedstock maintainer entail?\n\nThe maintainers \"job\" is to:\n\n- keep the feedstock updated by merging eventual maintenance PRs from conda-forge's bots;\n- keep the package updated by bumping the version whenever there is a new release;\n- answer eventual question about the package on the feedstock issue tracker.\n\n### 11. Why are there recipes already in the `recipes` directory? Should I do something about it?\n\nWhen a PR of recipe(s) is ready to go, it is merged into `master`. This will trigger a CI build specially designed to convert the recipe(s). However, for any number of reasons the recipe(s) may not be converted right away. In the interim, the recipe(s) will remain in `master` until they can be converted. There is no action required on the part of recipe contributors to resolve this. Also it should have no impact on any other PRs being proposed. If these recipe(s) pending conversion do cause issues for your submission, please ping `conda-forge/core` for help.\n", "release_dates": []}, {"name": "summarize-from-feedback", "description": "Code for \"Learning to summarize from human feedback\"", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Learning to Summarize from Human Feedback\n\nThis repository contains code to run our models, including the supervised baseline, the trained reward model, and the RL fine-tuned policy.\n\nSupported platform: Python 3.7 64-bit on Ubuntu 18.04\n\n## Install\n\n- Install [pipenv](https://github.com/pypa/pipenv#installation).\n\n- Clone this repo.  Then, inside it:\n  ```\n  pipenv install\n  ```\n\n## Run the models\n\nYou'll need to run this on a machine with an Nvidia GPU.\n\nFirst, let's run some tests to make sure everything is working.\n```\npipenv run exps/sample.py test test-sample\npipenv run exps/eval_rm.py test test-eval\n```\n\nNow let's run some actual evaluations. We can have the model summarize some posts from the validation set:\n```\npipenv run exps/sample.py ppo_xl sample-ppo-xl --num_queries 32\n```\nThis will output to `/tmp/jobs/sample-ppo-xl/results/`.\n\nNow we can evaluate them using the reward model:\n```\npipenv run exps/eval_rm.py rm4 eval-rm4 --input_path /tmp/jobs/sample-ppo-xl/results/\n```\nThis will print some aggregate statistics and output scores for each sample to `/tmp/jobs/eval-rm4/results/`.\n\n# Human feedback data\n\nWe've released our human feedback dataset for further research. The dataset contains 64,832 summary\ncomparisons on the TL;DR dataset, as well as our evaluation data on both\nTL;DR (comparisons and Likert scores) and CNN/DM (Likert scores).\n\nThe dataset is stored in Azure Blob Storage, split into two directories described below: `comparisons` and `axis_evals`.\nYou can download it by running `azcopy copy \"https://openaipublic.blob.core.windows.net/summarize-from-feedback/dataset/*\" . --recursive`.\n\nYou can also explore the data by hand on [our dataset website](https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/).\n\n## Comparisons\n\n`https://openaipublic.blob.core.windows.net/summarize-from-feedback/dataset/comparisons` contains labeled comparisons between pairs of summaries as jsonl files, where each line represents a single comparison. Here is a formatted example:\n\n```\n{\n  \"info\": {\n    \"id\": \"t3_2vwp1w\",\n    \"post\": \"I had a car accident on friday, other party involved was speeding and hit me. but because he denies it it seems like I was wrong because he was supposed to go first under normal circumstances. ( give way road markings ) \\n\\nbut because it was clear when I checked it I drove on, and when I was almost past the intersection he slammed me in the side near the back seat. and caused me to slide across the road for 2-3 meters hit a street light and then bounce back a meter. both doors completely jammed so i had to climb out the window...\\n\\ncan I somehow get an investigation going about this to see how fast he had to be driving to get this much force in the collision?\\nbecause the damage on my car would suggest that he was driving way faster than the legal limit there. ( which is 50 km/h )\\n\\nalso another reason why i think he was going way faster than admitted is because he could never have reached the intersection from such a distance as where i could not even see him yet\\n\\n(pictures of the damage:  ) as you can see with the damage, I am lucky to be alive and unharmed right now... 1ft further forward and it could have been my end...\\n\\nhelp would be appeciated on this :)\",\n    \"title\": \"Anybody with knowledge of the Dutch law around ? car accident questions.\",\n    \"subreddit\": \"legaladvice\"\n  },\n  \"summaries\": [\n    {\n      \"text\": \" car accident caused me 2-3m damage to my car both doors totally jammed and driving way faster than usual. need info on what to do with this.. thanks :)\",\n      \"policy\": \"sup4_ppo_rm3_kl10\",\n      \"note\": \"Was the accident caused by driving fast.\"\n    },\n    {\n      \"text\": \" we suspect other party involved of speeding when he hit me but I can't prove it without an investigation into the damage, how can i get such an investigation ? if at all possible.\",\n      \"policy\": \"ref\",\n      \"note\": \"Unclear what happened.\"\n    }\n  ],\n  \"choice\": 1,\n  \"worker\": \"ikNmucwunMnYJCQpnq6ZYb57OW7NiD\",\n  \"batch\": \"batch9\",\n  \"split\": \"train\",\n  \"extra\": {\n    \"confidence\": 8\n  }\n}\n```\n\n`note` fields contain the naive interpretation notes written by the worker before seeing the post (but possibly edited afterwards). May be null.\n\n`split` will always be `train`, `valid1`, or `valid2`; posts / articles marked with `valid1` were used to select models during training, so we restricted to `valid2` labels for final evaluations.\n\nThe training data for `sup4` is found in `comparisons/batch3.json` through `comparisons/batch10.json`; later batches are primarily evaluation.\n\n## Axis evals\n\n`https://openaipublic.blob.core.windows.net/summarize-from-feedback/dataset/axis_evals` contains ratings of summaries along several axes, again as jsonl files. Here is a formatted example:\n\n```\n{\n  \"info\": {\n    \"id\": \"167f80cc6634b166a699d182e25c81a2349d82d2\",\n    \"site\": \"dailymail\",\n    \"title\": \"Newcastle United midfielder Moussa Sissoko faces disciplinary action from the club after dangerous tackle on Lucas Leiva\",\n    \"article\": \"Newcastle stand-in skipper Moussa Sissoko is facing disciplinary action after he was sent off following a reckless challenge on Liverpool midfielder Lucas Leiva during Monday's 2-0 defeat at Anfield.\\n\\nThe France international was given a second yellow card for the offence, but head coach John Carver feels it should have been a straight red.\\n\\n'The club will deal with that situation,' he said when asked if Sissoko - who is now banned for two matches - would be punished.\\n\\nLiverpool midfielder Lucas Leiva clutches his leg after Moussa Sissoko's tackle at Anfield\\n\\nSissoko hands the captain's armband to boss John Carver as he leaves the pitch after being sent off\\n\\n'He knows he was wrong. He was fortunate not to get a straight red and he agreed with me.\\n\\n'He apologised afterwards to Lucas, which was important.\\n\\n'But you think captains would lead by example. We have to improve our discipline. I will be looking at that.'\\n\\nMeanwhile, Carver says Newcastle cannot rely on the shortcomings of others to preserve their Premier League status.\\n\\nThe Magpies are the division's most out-of-form side having lost five on the spin, scoring just one goal along the way.\\n\\nLiverpool's players surround Lucas following Sissoko's dangerous tackle during Monday night's game\\n\\nRaheem Sterling bends the ball past Tim Krul to open the scoring in Liverpool's 2-0 win against Newcastle\\n\\nThey are nine points clear of danger with six matches to play, but Carver says it's about time they started helping themselves, starting with Sunday's visit of Spurs.\\n\\n'These two home games (Spurs followed by Swansea) are massive for us. I'm not bothered about performances, we need results,' he said.\\n\\n'I'm not worrying about that (relegation) at the moment, and the good thing is we have four games at home.\\n\\n'But we need to start winning now. We can't rely on others teams. We can't afford to ease off, I have always said that.\\n\\n'We have gone through a rough spell. It's down to me now to get players in right frame of mind.'\\n\\nNewcastle's players appear dejected as Joe Allen celebrates scoring Liverpool's second goal at Anfield\"\n  },\n  \"split\": \"test\",\n  \"summary\": {\n    \"text\": \"Moussa Sissoko was sent off against Liverpool on Monday night.. John Carver felt that Sissoko's second booking was worthy of a red card.. Midfielder could be punished by his club on top of a two-game ban.. Carver admits he is only concerned with results and not performances.. Newcastle are 13th in the table, nine points off the relegation zone.\",\n    \"policy\": \"ref\",\n    \"note\": \"Misleading: \\\"Carver admits he is only concerned with results and not performances\\\" understood as if critics of monday's match but it's said for the following matches.\\n\\n13th??\\n\\nDoesnt properly address the teams, the match, the result, 2nd yellow card and therefore sent off, etc.\",\n    \"axes\": {\n      \"overall\": 3,\n      \"accuracy\": 5,\n      \"coverage\": 4,\n      \"coherence\": 2\n    }\n  },\n  \"worker\": \"qo6WIyEh27cwAjWpA3Q60J7NaDxzQJ\",\n  \"batch\": \"cnndm1\"\n}\n```\n\n# Reddit TL;DR dataset\n\nOur filtered versions of the TL;DR dataset are available here:\n\nhttps://openaipublic.blob.core.windows.net/summarize-from-feedback/datasets/tldr_3_filtered/train.jsonl\nhttps://openaipublic.blob.core.windows.net/summarize-from-feedback/datasets/tldr_3_filtered/valid.jsonl\nhttps://openaipublic.blob.core.windows.net/summarize-from-feedback/datasets/tldr_3_filtered/test.jsonl\nhttps://openaipublic.blob.core.windows.net/summarize-from-feedback/datasets/tldr_3_filtered/samples.txt\n\nFor details on the original TL;DR dataset, see [Syed et al 2018](https://zenodo.org/record/1168855) by Syed, Shahbaz, Voelske, Michael, Potthast, Martin, & Stein, Benno (2018). It is licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/legalcode).\n", "release_dates": []}, {"name": "supervised-reptile", "description": "Code for the paper \"On First-Order Meta-Learning Algorithms\"", "language": "JavaScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# supervised-reptile\n\n[Reptile](https://arxiv.org/abs/1803.02999) training code for [Omniglot](https://github.com/brendenlake/omniglot) and [Mini-ImageNet](https://openreview.net/pdf?id=rJY0-Kcll).\n\nReptile is a meta-learning algorithm that finds a good initialization. It works by sampling a task, training on the sampled task, and then updating the initialization towards the new weights for the task.\n\n# Getting the data\n\nThe [fetch_data.sh](fetch_data.sh) script creates a `data/` directory and downloads Omniglot and Mini-ImageNet into it. The data is on the order of 5GB, so the download takes 10-20 minutes on a reasonably fast internet connection.\n\n```\n$ ./fetch_data.sh\nFetching omniglot/images_background ...\nExtracting omniglot/images_background ...\nFetching omniglot/images_evaluation ...\nExtracting omniglot/images_evaluation ...\nFetching Mini-ImageNet train set ...\nFetching wnid: n01532829\nFetching wnid: n01558993\nFetching wnid: n01704323\nFetching wnid: n01749939\n...\n```\n\nIf you want to download Omniglot but not Mini-ImageNet, you can simply kill the script after it starts downloading Mini-ImageNet. The script automatically deletes partially-downloaded data when it is killed early.\n\n# Reproducing training runs\n\nYou can train models with the `run_omniglot.py` and `run_miniimagenet.py` scripts. Hyper-parameters are specified as flags (see `--help` for a detailed list). Here are the commands used for the paper:\n\n```shell\n# transductive 1-shot 5-way Omniglot.\npython -u run_omniglot.py --shots 1 --inner-batch 10 --inner-iters 5 --meta-step 1 --meta-batch 5 --meta-iters 100000 --eval-batch 5 --eval-iters 50 --learning-rate 0.001 --meta-step-final 0 --train-shots 10 --checkpoint ckpt_o15t --transductive\n\n# transductive 1-shot 5-way Mini-ImageNet.\npython -u run_miniimagenet.py --shots 1 --inner-batch 10 --inner-iters 8 --meta-step 1 --meta-batch 5 --meta-iters 100000 --eval-batch 5 --eval-iters 50 --learning-rate 0.001 --meta-step-final 0 --train-shots 15 --checkpoint ckpt_m15t --transductive\n\n# 5-shot 5-way Mini-ImageNet.\npython -u run_miniimagenet.py --inner-batch 10 --inner-iters 8 --meta-step 1 --meta-batch 5 --meta-iters 100000 --eval-batch 15 --eval-iters 50 --learning-rate 0.001 --meta-step-final 0 --train-shots 15 --checkpoint ckpt_m55\n\n# 1-shot 5-way Mini-ImageNet.\npython -u run_miniimagenet.py --shots 1 --inner-batch 10 --inner-iters 8 --meta-step 1 --meta-batch 5 --meta-iters 100000 --eval-batch 5 --eval-iters 50 --learning-rate 0.001 --meta-step-final 0 --train-shots 15 --checkpoint ckpt_m15\n\n# 5-shot 5-way Omniglot.\npython -u run_omniglot.py --train-shots 10 --inner-batch 10 --inner-iters 5 --learning-rate 0.001 --meta-step 1 --meta-step-final 0 --meta-batch 5 --meta-iters 100000 --eval-batch 5 --eval-iters 50 --checkpoint ckpt_o55\n\n# 1-shot 5-way Omniglot.\npython -u run_omniglot.py --shots 1 --inner-batch 10 --inner-iters 5 --meta-step 1 --meta-batch 5 --meta-iters 100000 --eval-batch 5 --eval-iters 50 --learning-rate 0.001 --meta-step-final 0 --train-shots 10 --checkpoint ckpt_o15\n\n# 1-shot 20-way Omniglot.\npython -u run_omniglot.py --shots 1 --classes 20 --inner-batch 20 --inner-iters 10 --meta-step 1 --meta-batch 5 --meta-iters 200000 --eval-batch 10 --eval-iters 50 --learning-rate 0.0005 --meta-step-final 0 --train-shots 10 --checkpoint ckpt_o120\n\n# 5-shot 20-way Omniglot.\npython -u run_omniglot.py --classes 20 --inner-batch 20 --inner-iters 10 --meta-step 1 --meta-batch 5 --meta-iters 200000 --eval-batch 10 --eval-iters 50 --learning-rate 0.0005 --meta-step-final 0 --train-shots 10 --checkpoint ckpt_o520\n```\n\nTraining creates checkpoints. Currently, you cannot resume training from a checkpoint, but you can re-run evaluation from a checkpoint by passing `--pretrained`. You can use TensorBoard on the checkpoint directories to see approximate learning curves during training and testing.\n\nTo evaluate with transduction, pass the `--transductive` flag. In this implementation, transductive evaluation is faster than non-transductive evaluation since it makes better use of batches.\n\n# Comparing different inner-loop gradient combinations\n\nHere are the commands for comparing different gradient combinations. The `--foml` flag indicates that only the final gradient should be used.\n\n```shell\n# Shared hyper-parameters for all experiments.\nshared=\"--sgd --seed 0 --inner-batch 25 --learning-rate 0.003 --meta-step-final 0 --meta-iters 40000 --eval-batch 25 --eval-iters 5 --eval-interval 1\"\n\npython run_omniglot.py --inner-iters 1 --train-shots 5 --meta-step 0.25 --checkpoint g1_ckpt $shared | tee g1.txt\n\npython run_omniglot.py --inner-iters 2 --train-shots 10 --meta-step 0.25 --checkpoint g1_g2_ckpt $shared | tee g1_g2.txt\npython run_omniglot.py --inner-iters 2 --train-shots 10 --meta-step 0.125 --checkpoint half_g1_g2_ckpt $shared | tee half_g1_g2.txt\npython run_omniglot.py --foml --inner-iters 2 --train-shots 10 --meta-step 0.25 --checkpoint g2_ckpt $shared | tee g2.txt\n\npython run_omniglot.py --inner-iters 3 --train-shots 15 --meta-step 0.25 --checkpoint g1_g2_g3_ckpt $shared | tee g1_g2_g3.txt\npython run_omniglot.py --inner-iters 3 --train-shots 15 --meta-step 0.08325 --checkpoint third_g1_g2_g3_ckpt $shared | tee third_g1_g2_g3.txt\npython run_omniglot.py --foml --inner-iters 3 --train-shots 15 --meta-step 0.25 --checkpoint g3_ckpt $shared | tee g3.txt\n\npython run_omniglot.py --foml --inner-iters 4 --train-shots 20 --meta-step 0.25 --checkpoint g4_ckpt $shared | tee g4.txt\npython run_omniglot.py --inner-iters 4 --train-shots 20 --meta-step 0.25 --checkpoint g1_g2_g3_g4_ckpt $shared | tee g1_g2_g3_g4.txt\npython run_omniglot.py --inner-iters 4 --train-shots 20 --meta-step 0.0625 --checkpoint fourth_g1_g2_g3_g4_ckpt $shared | tee fourth_g1_g2_g3_g4.txt\n\n```\n", "release_dates": []}, {"name": "tabulate", "description": "public release of Excel / OpenAI API integration", "language": "JavaScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# OpenAI API Excel integration\n\n(Update 2022-05-09: This code is no longer being maintained and there are no expectations that it will work.  For the most up to date documentation on our API, consider visiting https://beta.openai.com/examples or https://github.com/openai/openai-python)\n\nThis repository contains an example OpenAI API integration for Excel. It allows users to query the API to automatically generate Excel tables about topics.\n\nFor more details see the [API blog post](https://openai.com/blog/openai-api/)\n\nThe integration is an Excel TaskPane Add-in, which is structured as an HTML / CSS / Javascript web app running in an iframe. See the following links for more info:\n- https://docs.microsoft.com/en-us/office/dev/add-ins/overview/learning-path-beginner\n- https://docs.microsoft.com/en-us/office/dev/add-ins/excel/excel-add-ins-core-concepts\n\n## Setup\n\nAdd your OpenAI API key and organization at the top of `excel-addin/src/taskpane.js` (search for `***KEY HERE***` and `***ORG HERE***`)\n\nTo start the local development server from the `excel-addin` directory:\n- `brew install node@12` (Node LTS)\n- `npm install`\n- `npm run dev-server`\n\nOpen Excel for the web. Click \"Insert\" Menu (Ribbon) > Click \"Office Add-ins\" > Click \"Upload My Add-in\" in the upper right corner > Select `excel-addin/manifest.xml` ([source](https://docs.microsoft.com/en-us/office/dev/add-ins/testing/sideload-office-add-ins-for-testing#sideload-an-office-add-in-in-office-on-the-web))\n\nYou should see a new \"OpenAI API\" command group on the \"Home\" ribbon; click the \"Tabulate\" button to open the sidebar with API commands\n", "release_dates": []}, {"name": "tiktoken", "description": "tiktoken is a fast BPE tokeniser for use with OpenAI's models.", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# \u23f3 tiktoken\n\ntiktoken is a fast [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) tokeniser for use with\nOpenAI's models.\n\n```python\nimport tiktoken\nenc = tiktoken.get_encoding(\"cl100k_base\")\nassert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n\n# To get the tokeniser corresponding to a specific model in the OpenAI API:\nenc = tiktoken.encoding_for_model(\"gpt-4\")\n```\n\nThe open source version of `tiktoken` can be installed from PyPI:\n```\npip install tiktoken\n```\n\nThe tokeniser API is documented in `tiktoken/core.py`.\n\nExample code using `tiktoken` can be found in the\n[OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb).\n\n\n## Performance\n\n`tiktoken` is between 3-6x faster than a comparable open source tokeniser:\n\n![image](https://raw.githubusercontent.com/openai/tiktoken/main/perf.svg)\n\nPerformance measured on 1GB of text using the GPT-2 tokeniser, using `GPT2TokenizerFast` from\n`tokenizers==0.13.2`, `transformers==4.24.0` and `tiktoken==0.2.0`.\n\n\n## Getting help\n\nPlease post questions in the [issue tracker](https://github.com/openai/tiktoken/issues).\n\nIf you work at OpenAI, make sure to check the internal documentation or feel free to contact\n@shantanu.\n\n## What is BPE anyway?\n\nLanguage models don't see text like you and I, instead they see a sequence of numbers (known as tokens).\nByte pair encoding (BPE) is a way of converting text into tokens. It has a couple desirable\nproperties:\n1) It's reversible and lossless, so you can convert tokens back into the original text\n2) It works on arbitrary text, even text that is not in the tokeniser's training data\n3) It compresses the text: the token sequence is shorter than the bytes corresponding to the\n   original text. On average, in practice, each token corresponds to about 4 bytes.\n4) It attempts to let the model see common subwords. For instance, \"ing\" is a common subword in\n   English, so BPE encodings will often split \"encoding\" into tokens like \"encod\" and \"ing\"\n   (instead of e.g. \"enc\" and \"oding\"). Because the model will then see the \"ing\" token again and\n   again in different contexts, it helps models generalise and better understand grammar.\n\n`tiktoken` contains an educational submodule that is friendlier if you want to learn more about\nthe details of BPE, including code that helps visualise the BPE procedure:\n```python\nfrom tiktoken._educational import *\n\n# Train a BPE tokeniser on a small amount of text\nenc = train_simple_encoding()\n\n# Visualise how the GPT-4 encoder encodes text\nenc = SimpleBytePairEncoding.from_tiktoken(\"cl100k_base\")\nenc.encode(\"hello world aaaaaaaaaaaa\")\n```\n\n\n## Extending tiktoken\n\nYou may wish to extend `tiktoken` to support new encodings. There are two ways to do this.\n\n\n**Create your `Encoding` object exactly the way you want and simply pass it around.**\n\n```python\ncl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n\n# In production, load the arguments directly instead of accessing private attributes\n# See openai_public.py for examples of arguments for specific encodings\nenc = tiktoken.Encoding(\n    # If you're changing the set of special tokens, make sure to use a different name\n    # It should be clear from the name what behaviour to expect.\n    name=\"cl100k_im\",\n    pat_str=cl100k_base._pat_str,\n    mergeable_ranks=cl100k_base._mergeable_ranks,\n    special_tokens={\n        **cl100k_base._special_tokens,\n        \"<|im_start|>\": 100264,\n        \"<|im_end|>\": 100265,\n    }\n)\n```\n\n**Use the `tiktoken_ext` plugin mechanism to register your `Encoding` objects with `tiktoken`.**\n\nThis is only useful if you need `tiktoken.get_encoding` to find your encoding, otherwise prefer\noption 1.\n\nTo do this, you'll need to create a namespace package under `tiktoken_ext`.\n\nLayout your project like this, making sure to omit the `tiktoken_ext/__init__.py` file:\n```\nmy_tiktoken_extension\n\u251c\u2500\u2500 tiktoken_ext\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 my_encodings.py\n\u2514\u2500\u2500 setup.py\n```\n\n`my_encodings.py` should be a module that contains a variable named `ENCODING_CONSTRUCTORS`.\nThis is a dictionary from an encoding name to a function that takes no arguments and returns\narguments that can be passed to `tiktoken.Encoding` to construct that encoding. For an example, see\n`tiktoken_ext/openai_public.py`. For precise details, see `tiktoken/registry.py`.\n\nYour `setup.py` should look something like this:\n```python\nfrom setuptools import setup, find_namespace_packages\n\nsetup(\n    name=\"my_tiktoken_extension\",\n    packages=find_namespace_packages(include=['tiktoken_ext*']),\n    install_requires=[\"tiktoken\"],\n    ...\n)\n```\n\nThen simply `pip install ./my_tiktoken_extension` and you should be able to use your\ncustom encodings! Make sure **not** to use an editable install.\n", "release_dates": ["2024-02-09T06:35:04Z", "2023-12-26T03:39:03Z", "2023-09-13T00:40:45Z", "2023-09-12T02:49:52Z", "2023-05-07T21:39:10Z", "2023-03-28T23:39:02Z", "2023-03-17T02:29:16Z", "2023-03-13T05:40:37Z"]}, {"name": "train-procgen", "description": "Code for the paper \"Leveraging Procedural Generation to Benchmark Reinforcement Learning\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Leveraging Procedural Generation to Benchmark Reinforcement Learning\n\n#### [[Blog Post]](https://openai.com/blog/procgen-benchmark/) [[Paper]](https://arxiv.org/abs/1912.01588)\n\nThis is code for training agents for some of the experiments in [Leveraging Procedural Generation to Benchmark Reinforcement Learning](https://cdn.openai.com/procgen.pdf) [(citation)](#citation).  The code for the environments is in the [Procgen Benchmark](https://github.com/openai/procgen) repo.\n\nWe're currently running a competition which uses these environments to measure sample efficiency and generalization in RL. You can learn more and register [here](https://www.aicrowd.com/challenges/neurips-2020-procgen-competition).\n\nSupported platforms:\n\n- macOS 10.14 (Mojave)\n- Ubuntu 16.04\n\nSupported Pythons:\n\n- 3.7 64-bit\n\n## Install\n\nYou can get miniconda from https://docs.conda.io/en/latest/miniconda.html if you don't have it, or install the dependencies from [`environment.yml`](environment.yml) manually.\n\n```\ngit clone https://github.com/openai/train-procgen.git\nconda env update --name train-procgen --file train-procgen/environment.yml\nconda activate train-procgen\npip install https://github.com/openai/baselines/archive/9ee399f5b20cd70ac0a871927a6cf043b478193f.zip\npip install -e train-procgen\n```\n\n## Try it out\n\nTrain an agent using PPO on the environment StarPilot:\n\n```\npython -m train_procgen.train --env_name starpilot\n```\n\nTrain an agent using PPO on the environment StarPilot using the easy difficulty:\n\n```\npython -m train_procgen.train --env_name starpilot --distribution_mode easy\n```\n\nRun parallel training using MPI:\n\n```\nmpiexec -np 8 python -m train_procgen.train --env_name starpilot\n```\n\nTrain an agent on a fixed set of N levels:\n\n```\npython -m train_procgen.train --env_name starpilot --num_levels N\n```\n\nTrain an agent on the same 500 levels used in the paper:\n\n```\npython -m train_procgen.train --env_name starpilot --num_levels 500\n```\n\nTrain an agent on a different set of 500 levels:\n\n```\npython -m train_procgen.train --env_name starpilot --num_levels 500 --start_level 1000\n```\n\nRun simultaneous training and testing using MPI. 1 in every 4 workers will be test workers, and the rest will be training workers.\n\n```\nmpiexec -np 8 python -m train_procgen.train --env_name starpilot --num_levels 500 --test_worker_interval 4\n```\n\nTrain an agent using PPO on a level in Jumper that requires hard exploration\n\n```\npython -m train_procgen.train --env_name jumper --distribution_mode exploration\n```\n\nTrain an agent using PPO on a variant of CaveFlyer that requires memory\n\n```\npython -m train_procgen.train --env_name caveflyer --distribution_mode memory\n```\n\nView training options:\n\n```\npython -m train_procgen.train --help\n```\n\n## Reproduce and Visualize Results\n\nSample efficiency on hard environments (results/hard-all-runN):\n\n```\nmpiexec -np 4 python -m train_procgen.train --env_name ENV_NAME --distribution_mode hard\npython -m train_procgen.graph --distribution_mode hard\n```\n\nSample efficiency on easy environments (results/easy-all-runN):\n\n```\npython -m train_procgen.train --env_name ENV_NAME --distribution_mode easy\npython -m train_procgen.graph --distribution_mode easy\n```\n\nGeneralization on hard environments using 500 training levels (results/hard-500-runN):\n\n```\nmpiexec -np 8 python -m train_procgen.train --env_name ENV_NAME --num_levels 500 --distribution_mode hard --test_worker_interval 2\npython -m train_procgen.graph --distribution_mode hard --restrict_training_set\n```\n\nGeneralization on easy environments using 200 training levels (results/easy-200-runN):\n\n```\nmpiexec -np 2 python -m train_procgen.train --env_name ENV_NAME --num_levels 200 --distribution_mode easy --test_worker_interval 2\npython -m train_procgen.graph --distribution_mode easy --restrict_training_set\n```\n\nPass `--normalize_and_reduce` to compute and visualize the mean normalized return with `train_procgen.graph`.\n\n# Citation\n\nPlease cite using the following bibtex entry:\n\n```\n@article{cobbe2019procgen,\n  title={Leveraging Procedural Generation to Benchmark Reinforcement Learning},\n  author={Cobbe, Karl and Hesse, Christopher and Hilton, Jacob and Schulman, John},\n  journal={arXiv preprint arXiv:1912.01588},\n  year={2019}\n}\n```\n", "release_dates": []}, {"name": "triton", "description": "Development repository for the Triton language and compiler", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "<div align=\"center\">\n  <img src=\"https://cdn.openai.com/triton/assets/triton-logo.png\" alt=\"Triton logo\" width=\"88\" height=\"100\">\n</div>\n\nWe're hiring! If you are interested in working on Triton at OpenAI, we have roles open for [Compiler Engineers](https://openai.com/careers/software-engineer-triton-compiler) and [Kernel Engineers](https://openai.com/careers/kernel-engineer).\n\n| **`Documentation`** | **`Nightly Wheels`** |\n|-------------------- | -------------------- |\n| [![Documentation](https://github.com/openai/triton/actions/workflows/documentation.yml/badge.svg)](https://triton-lang.org/) | [![Wheels](https://github.com/openai/triton/actions/workflows/wheels.yml/badge.svg?branch=release/2.0.x)](https://github.com/openai/triton/actions/workflows/wheels.yml) |\n\n\n# Triton\n\nThis is the development repository of Triton, a language and compiler for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.\n\nThe foundations of this project are described in the following MAPL2019 publication: [Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations](http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf). Please consider citing this work if you use Triton!\n\nThe [official documentation](https://triton-lang.org) contains installation instructions and tutorials.\n\n# Quick Installation\n\nYou can install the latest stable release of Triton from pip:\n\n```bash\npip install triton\n```\nBinary wheels are available for CPython 3.7-3.11 and PyPy 3.8-3.9.\n\nAnd the latest nightly release:\n\n```bash\npip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly\n```\n\n# Install from source\n\n```\ngit clone https://github.com/openai/triton.git;\ncd triton;\n\npip install ninja cmake wheel; # build-time dependencies\npip install -e python\n```\n\nOr with a virtualenv:\n\n```\ngit clone https://github.com/openai/triton.git;\ncd triton;\n\npython -m venv .venv --prompt triton;\nsource .venv/bin/activate;\n\npip install ninja cmake wheel; # build-time dependencies\npip install -e python\n```\n\n# Building with a custom LLVM\n\nTriton uses LLVM to generate code for GPUs and CPUs.  Normally, the Triton build\ndownloads a prebuilt LLVM, but you can also build LLVM from source and use that.\n\nLLVM does not have a stable API, so the Triton build will not work at an\narbitrary LLVM version.\n\n1. Find the version of LLVM that Triton builds against.  Check\n`cmake/llvm-hash.txt` to see the current version. For example, if it says:\n       49af6502c6dcb4a7f7520178bd14df396f78240c\n\n   This means that the version of Triton you have builds against\n   [LLVM](https://github.com/llvm/llvm-project) 49af6502.\n\n2. `git checkout` LLVM at this revision.  Optionally, make additional\n   modifications to LLVM.\n\n3. [Build LLVM](https://llvm.org/docs/CMake.html).  For example, you might run\n\n       $ cd $HOME/llvm-project  # your clone of LLVM.\n       $ mkdir build\n       $ cd build\n       $ cmake -G Ninja -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON  ../llvm -DLLVM_ENABLE_PROJECTS=\"mlir;llvm\"\n       $ ninja\n\n4. Grab a snack, this will take a while.\n\n5. Build Triton as above, but set the following environment variables.\n\n       # Modify as appropriate to point to your LLVM build.\n       $ export LLVM_BUILD_DIR=$HOME/llvm-project/build\n\n       $ cd <triton install>\n       $ LLVM_INCLUDE_DIRS=$LLVM_BUILD_DIR/include \\\n         LLVM_LIBRARY_DIR=$LLVM_BUILD_DIR/lib \\\n         LLVM_SYSPATH=$LLVM_BUILD_DIR \\\n         pip install -e python\n\n# Tips for building\n\n- Set `TRITON_BUILD_WITH_CLANG_LLD=true` as an environment variable to use clang\n  and lld.  lld in particular results in faster builds.\n\n- Set `TRITON_BUILD_WITH_CCACHE=true` to build with ccache.\n\n- Pass `--no-build-isolation` to `pip install` to make nop builds faster.\n  Without this, every invocation of `pip install` uses a different symlink to\n  cmake, and this forces ninja to rebuild most of the `.a` files.\n\n- vscode intellisense has some difficulty figuring out how to build Triton's C++\n  (probably because, in our build, users don't invoke cmake directly, but\n  instead use setup.py).  Teach vscode how to compile Triton as follows.\n\n    - Do a local build.\n    - Get the full path to the `compile_commands.json` file produced by the build:\n      `find python/build -name 'compile_commands.json | xargs readlink -f'`\n    - In vscode, install the\n      [C/C++\n      extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools),\n      then open the command palette (`Shift + Command + P` on Mac, or `Shift +\n      Ctrl + P` on Windows/Linux) and open `C/C++: Edit Configurations (UI)`.\n    - Open \"Advanced Settings\" and paste the full path to\n      `compile_commands.json` into the \"Compile Commands\" textbox.\n\n# Running tests\n\nThere currently isn't a turnkey way to run all the Triton tests, but you can\nfollow the following recipe.\n\n```shell\n# One-time setup.  Note we have to reinstall local Triton because torch\n# overwrites it with the public version.\n$ pip install scipy numpy torch pytest lit && pip install -e python\n\n# Run Python tests using your local GPU.\n$ python3 -m pytest python/test/unit\n\n# Move to builddir.  Fill in <...> with the full path, e.g.\n# `cmake.linux-x86_64-cpython-3.11`.\n$ cd python/build/cmake<...>\n\n# Run C++ unit tests.\n$ ninja test\n\n# Run lit tests.\n$ lit test\n```\n\nYou may find it helpful to make a symlink to the builddir and tell your local\ngit to ignore it.\n\n```\n$ ln -s python/build/cmake<...> build\n$ echo build >> .git/info/exclude\n```\n\nThen you can e.g. rebuild and run lit with the following command.\n\n```\n$ ninja -C build && ( cd build ; lit test )\n```\n\n# Changelog\n\nVersion 2.0 is out! New features include:\n- Many, many bug fixes\n- Performance improvements\n- Backend rewritten to use MLIR\n- Support for kernels that contain back-to-back matmuls (e.g., flash attention)\n\n# Contributing\n\nCommunity contributions are more than welcome, whether it be to fix bugs or to add new features at [github](https://github.com/openai/triton/). For more detailed instructions, please visit our [contributor's guide](CONTRIBUTING.md).\n\n\n# Compatibility\n\nSupported Platforms:\n  * Linux\n\nSupported Hardware:\n  * NVIDIA GPUs (Compute Capability 7.0+)\n  * Under development: AMD GPUs, CPUs\n", "release_dates": []}, {"name": "understanding-rl-vision", "description": "Code for the paper \"Understanding RL Vision\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# Understanding RL Vision\n\n#### [ [Paper] ](https://distill.pub/2020/understanding-rl-vision) [ [Demo] ](https://openaipublic.blob.core.windows.net/rl-clarity/attribution/demo/interface.html)\n\nGenerate interfaces for interpreting vision models trained using RL.\n\nThe core utilities used to compute feature visualization, attribution and dimensionality reduction can be found in `lucid.scratch.rl_util`, a submodule of [Lucid](https://github.com/tensorflow/lucid/). These are demonstrated in [this notebook](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/misc/rl_util.ipynb). The code here leverages these utilities to build HTML interfaces similar to the above demo.\n\n![](https://openaipublic.blob.core.windows.net/rl-clarity/attribution/demo.gif)\n\n## Installation\n\nSupported platforms: MacOS and Ubuntu, Python 3.7, TensorFlow <= 1.14\n\n- Install [Baselines](https://github.com/openai/baselines) and its dependencies, including TensorFlow 1.\n- Clone the repo:\n    ```\n    git clone https://github.com/openai/understanding-rl-vision.git\n    ```\n- Install the repo and its dependencies, among which is a pinned version of [Lucid](https://github.com/tensorflow/lucid):\n    ```\n    pip install -e understanding-rl-vision\n    ```\n- Install an RL environment of your choice. Supported environments:\n    - [CoinRun](https://github.com/openai/coinrun) (the original version used in the paper): follow the instructions. Note: due to CoinRun's requirements, you should re-install Baselines after installing CoinRun.\n    - [Procgen](https://github.com/openai/procgen): `pip install procgen`\n    - [Atari](https://github.com/openai/atari-py): `pip install atari-py`\n\n## Generating interfaces\n\nThe main script processes checkpoint files saved by RL code:\n```\nfrom understanding_rl_vision import rl_clarity\n\nrl_clarity.run('path/to/checkpoint/file', output_dir='path/to/directory')\n```\n\nAn example checkpoint file can be downloaded [here](https://openaipublic.blob.core.windows.net/rl-clarity/attribution/models/coinrun.jd), or can be generated using the [example script](understanding_rl_vision/rl_clarity/example.py). Checkpoint files for a number of pre-trained models are indexed [here](https://openaipublic.blob.core.windows.net/rl-clarity/attribution/models/index.html).\n\nThe precise format required of the checkpoint file, along with a full list of keyword arguments, can be found in the function's [docstring](understanding_rl_vision/rl_clarity/__init__.py).\n\nThe script will create an `interface.html` file, along with directories containing images (which can take up several GB), at the location specified by `output_dir`.\n\nBy default, the script will also create some files in the directory of the checkpoint file, in an `rl-clarity` subdirectory. These contain all the necessary information extracted from the model and environment for re-creating the same interface. To create these files in a temporary location instead, set `load_kwargs={'temp_files': True}`. To re-create an interface using existing files, set `load_kwargs={'resample': False}`.\n\n### Speed issues\n\nThe slowest part of the script is computing the attribution in all the required combinations. If you set `trajectories_kwargs={'num_envs': num_envs, 'num_steps': num_steps}`, then `num_envs` trajectories will be collected, each of length `num_steps`, and the script will distribute the trajectories among the MPI workers for computing the attribution. The memory requirements of each worker scales with `num_steps`, which defaults to 512 (about as large as a machine with 34 GB of memory can typically handle). The default `num_envs` is 8, so it is best to use 8 MPI workers by default to save time, if you have 8 GPUs available.\n\nThe script should take a few hours to run, but if it is taking too long, then you can tell the script to ignore the first couple of non-input layers by setting `layer_kwargs={'discard_first_n': 2}`, for example. These layers take the longest to compute attribution for since they have the highest spatial resolution, and are usually not that informative anyway.\n\nBy default, attribution is only computed for the value function, since computing attribution for every logit of the policy amounts to a large multiplier on the time taken by the script to run. To compute attribution for the policy, set `attr_policy=True`. To offset the increased computational load when doing this, you may wish to choose a single layer to compute attribution for by setting `layer_kwargs={'name_contains_one_of': ['2b']}`, for example.\n\nTo save disk space, the hover effect for isolating single attribution channels can be disabled by setting `attr_single_channels=False`, though this will not have much effect on speed.\n\n## Guide to interfaces\n\nAs shown in [this demo](https://openaipublic.blob.core.windows.net/rl-clarity/attribution/demo/interface.html), interfaces are divided into a number of sections:\n\n- **Trajectories** - Each trajectory is a separate rollout of the agent interacting with the environment. Here you can select one of them.\n- **Bookmarks** - Advantages have been computed using [generalized advantage estimation](https://arxiv.org/abs/1506.02438) (GAE). These provide a measure of how successful each choice made by the agent turned out relative to its expectations, and would usually be used to improve the agent's policy during training. The links here allow you to skip to specific frames from the trajectories with the highest and lowest advantages (with at most one link per episode).\n- **Layers** - Here you can select a layer for which attribution (explained below) has been computed. For the input layer, if included, attribution makes less sense, so gradients have been computed instead.\n- **Timeline** - Here you can navigate through the frames in each trajectory, either using the buttons or by scrubbing. At the top, information about the current frame is displayed, including the last reward received, the agent's policy, and the action that was chosen next. There are graphs of advantages (as used by the Bookmarks section) and of each network output that has been selected in the Attribution section.\n- **Attribution** - Here you can view the observations processed by the agent, and attribution from network outputs (just the value function by default) to the selected layer. Below the observation is chart of the attribution summed over spatial positions. If attribution has been computed for the policy, you can add and remove rows from this section, and select a different network output for each row, such as the value function, or the policy's logit for a particular action. Attribution has been computed using the method of [integrated gradients](https://arxiv.org/abs/1703.01365): the gradient of the network output with respect to selected layer has been numerically integrated along the straight line from zero to the layer's output given the current observation. This effectively decomposes (or \"attributes\") the network output across the spatial positions and channels of the selected layer. Dimensionality reduction ([non-negative matrix factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)) has been applied to the channels using a large batch of varied observations, and the resulting channels are represented using different colors. Additional normalization and smoothing has been applied, with strong attribution bleeding into nearby spatial positions.\n- **Attribution legend** - For each of the channels produced by dimensionality reduction (explained above), there are small visualizations here of the feature picked out by that channel. These consist of patches taken from observations at the spatial positions where the selected layer was most strongly activated in the direction of the channel. Hovering over these isolates the channel for the displayed attribution, and clicking opens a the Feature visualization popup, where the feature can be further analyzed.\n- **Feature visualization** (in popup) - This is displayed after a feature from the Attribution legend section has been selected, and shows a larger visualization of the feature. This also consists of patches taken from observations where the selected layer was most strongly activated in the appropriate direction, but here the location of a patch determines a specific spatial position that must be activated. This means that there is a spatial correspondence between the visualization and observations. Patches with weaker activations are displayed with greater transparency, except when hovering over the image. There are sliders that can be used to set the zoom level of the patches (which can also be controlled by scrolling over the image) and the number of patches (which initially equals the number of spatial positions of the selected layer). Clicking on a patch reveals the full observation from which the patch was extracted.\n- **Hotkeys** - Here is a list of available keyboard shortcuts. Toggling between play and pause also toggles between whether the arrow keys change the play direction or take a single step in one direction.\n\n## Training models\n\nThere is also a script for training a model using [PPO2](https://github.com/openai/baselines/tree/master/baselines/ppo2) from [Baselines](https://github.com/openai/baselines), and saving a checkpoint file in the required format:\n```\nfrom understanding_rl_vision import rl_clarity\n\nrl_clarity.train(env_name='coinrun_old', save_dir='path/to/directory')\n```\n\nThis script is intended to explain checkpoint files, and has not been well-tested. The [example script](understanding_rl_vision/rl_clarity/example.py) demonstrates how to train a model and then generate an interface for it.\n\n## Svelte compilation\n\nTo generate interfaces, the Svelte source must be compiled to JavaScript. At installation, the module will automatically attempt to download the pre-compiled JavaScript from a remote copy, though this copy is not guaranteed to be kept up-to-date.\n\nTo obtain an up-to-date copy, or for development, you may wish to re-compile the JavaScript locally. To do this, first install [Node.js](https://nodejs.org/) if you have not already. On Mac:\n```\nbrew install node\n```\nYou will then be able to re-compile the JavaScript:\n```\npython -c 'from understanding_rl_vision import rl_clarity; rl_clarity.recompile_js()'\n```\n\n### Standalone compiler\n\nThe `svelte3` package provides generic functions for compiling version 3 of Svelte to JavaScript or HTML. These can be used to create an easy-to-use command-line tool:\n```\npython -c 'from understanding_rl_vision import svelte3; svelte3.compile_html(\"path/to/svelte/file\", \"path/to/html/file\")'\n```\n\nDetailed usage instructions can be found in the functions' [docstrings](svelte3/compiling.py).\n\n## Citation\n\nPlease cite using the following BibTeX entry:\n```\n@article{hilton2020understanding,\n  author = {Hilton, Jacob and Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris},\n  title = {Understanding RL Vision},\n  journal = {Distill},\n  year = {2020},\n  note = {https://distill.pub/2020/understanding-rl-vision},\n  doi = {10.23915/distill.00029}\n}\n```", "release_dates": []}, {"name": "universe", "description": "Universe: a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications.", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": null, "release_dates": ["2017-03-05T22:22:01Z", "2017-02-01T03:47:09Z"]}, {"name": "universe-starter-agent", "description": "A starter agent that can solve a number of universe environments.", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**This repository has been deprecated in favor of the Retro (https://github.com/openai/retro) library. See our Retro Contest (https://blog.openai.com/retro-contest) blog post for detalis.**\n\n# universe-starter-agent\n\nThe codebase implements a starter agent that can solve a number of `universe` environments.\nIt contains a basic implementation of the [A3C algorithm](https://arxiv.org/abs/1602.01783), adapted for real-time environments.\n\n# Dependencies\n\n* Python 2.7 or 3.5\n* [Golang](https://golang.org/doc/install)\n* [six](https://pypi.python.org/pypi/six) (for py2/3 compatibility)\n* [TensorFlow](https://www.tensorflow.org/) 0.12\n* [tmux](https://tmux.github.io/) (the start script opens up a tmux session with multiple windows)\n* [htop](https://hisham.hm/htop/) (shown in one of the tmux windows)\n* [gym](https://pypi.python.org/pypi/gym)\n* gym[atari]\n* libjpeg-turbo (`brew install libjpeg-turbo`)\n* [universe](https://pypi.python.org/pypi/universe)\n* [opencv-python](https://pypi.python.org/pypi/opencv-python)\n* [numpy](https://pypi.python.org/pypi/numpy)\n* [scipy](https://pypi.python.org/pypi/scipy)\n\n# Getting Started\n\n```\nconda create --name universe-starter-agent python=3.5\nsource activate universe-starter-agent\n\nbrew install tmux htop cmake golang libjpeg-turbo      # On Linux use sudo apt-get install -y tmux htop cmake golang libjpeg-dev\n\npip install \"gym[atari]\"\npip install universe\npip install six\npip install tensorflow\nconda install -y -c https://conda.binstar.org/menpo opencv3\nconda install -y numpy\nconda install -y scipy\n```\n\n\nAdd the following to your `.bashrc` so that you'll have the correct environment when the `train.py` script spawns new bash shells\n```source activate universe-starter-agent```\n\n## Atari Pong\n\n`python train.py --num-workers 2 --env-id PongDeterministic-v3 --log-dir /tmp/pong`\n\nThe command above will train an agent on Atari Pong using ALE simulator.\nIt will see two workers that will be learning in parallel (`--num-workers` flag) and will output intermediate results into given directory.\n\nThe code will launch the following processes:\n* worker-0 - a process that runs policy gradient\n* worker-1 - a process identical to process-1, that uses different random noise from the environment\n* ps - the parameter server, which synchronizes the parameters among the different workers\n* tb - a tensorboard process for convenient display of the statistics of learning\n\nOnce you start the training process, it will create a tmux session with a window for each of these processes. You can connect to them by typing `tmux a` in the console.\nOnce in the tmux session, you can see all your windows with `ctrl-b w`.\nTo switch to window number 0, type: `ctrl-b 0`. Look up tmux documentation for more commands.\n\nTo access TensorBoard to see various monitoring metrics of the agent, open [http://localhost:12345/](http://localhost:12345/) in a browser.\n\nUsing 16 workers, the agent should be able to solve `PongDeterministic-v3` (not VNC) within 30 minutes (often less) on an `m4.10xlarge` instance.\nUsing 32 workers, the agent is able to solve the same environment in 10 minutes on an `m4.16xlarge` instance.\nIf you run this experiment on a high-end MacBook Pro, the above job will take just under 2 hours to solve Pong.\n\nAdd '--visualise' toggle if you want to visualise the worker using env.render() as follows:\n\n`python train.py --num-workers 2 --env-id PongDeterministic-v3 --log-dir /tmp/pong --visualise`\n\n![pong](https://github.com/openai/universe-starter-agent/raw/master/imgs/tb_pong.png \"Pong\")\n\nFor best performance, it is recommended for the number of workers to not exceed available number of CPU cores.\n\nYou can stop the experiment with `tmux kill-session` command.\n\n## Playing games over remote desktop\n\nThe main difference with the previous experiment is that now we are going to play the game through VNC protocol.\nThe VNC environments are hosted on the EC2 cloud and have an interface that's different from a conventional Atari Gym\nenvironment;  luckily, with the help of several wrappers (which are used within `envs.py` file)\nthe experience should be similar to the agent as if it was played locally. The problem itself is more difficult\nbecause the observations and actions are delayed due to the latency induced by the network.\n\nMore interestingly, you can also peek at what the agent is doing with a VNCViewer.\n\nNote that the default behavior of `train.py` is to start the remotes on a local machine. Take a look at https://github.com/openai/universe/blob/master/doc/remotes.rst for documentation on managing your remotes. Pass additional `-r` flag to point to pre-existing instances.\n\n### VNC Pong\n\n`python train.py --num-workers 2 --env-id gym-core.PongDeterministic-v3 --log-dir /tmp/vncpong`\n\n_Peeking into the agent's environment with TurboVNC_\n\nYou can use your system viewer as `open vnc://localhost:5900` (or `open vnc://${docker_ip}:5900`) or connect TurboVNC to that ip/port.\nVNC password is `\"openai\"`.\n\n![pong](https://github.com/openai/universe-starter-agent/raw/master/imgs/vnc_pong.png \"Pong over VNC\")\n\n#### Important caveats\n\nOne of the novel challenges in using Universe environments is that\nthey operate in *real time*, and in addition, it takes time for the\nenvironment to transmit the observation to the agent.  This time\ncreates a lag: where the greater the lag, the harder it is to solve\nenvironment with today's RL algorithms.  Thus, to get the best\npossible results it is necessary to reduce the lag, which can be\nachieved by having both the environments and the agent live\non the same high-speed computer network.  So for example, if you have\na fast local network, you could host the environments on one set of\nmachines, and the agent on another machine that can speak to the\nenvironments with low latency.  Alternatively, you can run the\nenvironments and the agent on the same EC2/Azure region.  Other\nconfigurations tend to have greater lag.\n\nTo keep track of your lag, look for the phrase `reaction_time` in\nstderr.  If you run both the agent and the environment on nearby\nmachines on the cloud, your `reaction_time` should be as low as 40ms.\nThe `reaction_time` statistic is printed to stderr because we wrap our\nenvironment with the `Logger` wrapper, as done in\n[here](<https://github.com/openai/universe-starter-agent/blob/master/envs.py#L32>).\n\nGenerally speaking, environments that are most affected by lag are\ngames that place a lot of emphasis on reaction time.  For example,\nthis agent is able to solve VNC Pong\n(`gym-core.PongDeterministic-v3`) in under 2 hours when both the agent\nand the environment are co-located on the cloud, but this agent had\ndifficulty solving VNC Pong when the environment was on the cloud\nwhile the agent was not.  This issue affects environments that place\ngreat emphasis on reaction time.\n\n### A note on tuning\n\nThis implementation has been tuned to do well on VNC Pong, and we do not guarantee\nits performance on other tasks.  It is meant as a starting point.\n\n### Playing flash games\n\nYou may run the following command to launch the agent on the game Neon Race:\n\n`python train.py --num-workers 2 --env-id flashgames.NeonRace-v0 --log-dir /tmp/neonrace`\n\n_What agent sees when playing Neon Race_\n(you can connect to this view via [note](#vnc-pong) above)\n![neon](https://github.com/openai/universe-starter-agent/raw/master/imgs/neon_race.png \"Neon Race\")\n\nGetting 80% of the maximal score takes between 1 and 2 hours with 16 workers, and getting to 100% of the score\ntakes about 12 hours.  Also, flash games are run at 5fps by default, so it should be possible to productively\nuse 16 workers on a machine with 8 (and possibly even 4) cores.\n\n### Next steps\n\nNow that you have seen an example agent, develop agents of your own.  We hope that you will find\ndoing so to be an exciting and an enjoyable task.\n", "release_dates": []}, {"name": "vdvae", "description": "Repository for the paper \"Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Very Deep VAEs\n\nRepository for the paper \"Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images\" (https://arxiv.org/abs/2011.10650)\n\nSome model samples and a visualization of how it generates them:\n![image](header-image.png)\n\nThis repository is tested with PyTorch 1.6, CUDA 10.1, Numpy 1.16, Ubuntu 18.04, and V100 GPUs.\n\n# Setup\nSeveral additional packages are required, including NVIDIA Apex:\n```\npip install imageio\npip install mpi4py\npip install sklearn\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\ncd ..\n```\n\nAlso, you'll have to download the data, depending on which one you want to run:\n```\n./setup_cifar10.sh\n./setup_imagenet.sh imagenet32\n./setup_imagenet.sh imagenet64\n./setup_ffhq256.sh\n./setup_ffhq1024.sh  /path/to/images1024x1024  # this one depends on you first downloading the subfolder `images_1024x1024` from https://github.com/NVlabs/ffhq-dataset on your own\n```\n\n# Training models\nHyperparameters all reside in `hps.py`. We use 2 gpus for our CIFAR-10 runs, and 32 for the rest of the models. (Using a lower batch size is also possible and results in slower learning, and may also require a lower learning rate).\n\nThe `mpiexec` arguments you use for runs with more than 1 node depend on the configuration of your system, so please adapt accordingly.\n\n```bash\nmpiexec -n 2 python train.py --hps cifar10\nmpiexec -n 32 python train.py --hps imagenet32\nmpiexec -n 32 python train.py --hps imagenet64\nmpiexec -n 32 python train.py --hps ffhq256\nmpiexec -n 32 python train.py --hps ffhq1024\n```\n\n# Restoring saved models\nFor convenience, we have included training checkpoints which can be restored in order to confirm performance, continue training, or generate samples.\n\n### ImageNet 32\n```bash\n# 119M parameter model, trained for 1.7M iters (about 2.5 weeks on 32 V100)\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/imagenet32-iter-1700000-log.jsonl\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/imagenet32-iter-1700000-model.th\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/imagenet32-iter-1700000-model-ema.th\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/imagenet32-iter-1700000-opt.th\npython train.py --hps imagenet32 --restore_path imagenet32-iter-1700000-model.th --restore_ema_path imagenet32-iter-1700000-model-ema.th --restore_log_path imagenet32-iter-1700000-log.jsonl --restore_optimizer_path imagenet32-iter-1700000-opt.th --test_eval\n# should give 2.6364 nats per dim, which is 3.80 bpd\n```\n\n### ImageNet 64\n```bash\n# 125M parameter model, trained for 1.6M iters (about 2.5 weeks on 32 V100)\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-log.jsonl\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-model.th\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-model-ema.th\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-opt.th\npython train.py --hps imagenet64 --restore_path imagenet64-iter-1600000-model.th --restore_ema_path imagenet64-iter-1600000-model-ema.th --restore_log_path imagenet64-iter-1600000-log.jsonl --restore_optimizer_path imagenet64-iter-1600000-opt.th --test_eval\n# should be 2.44 nats, or 3.52 bits per dim\n```\n\n### FFHQ-256\n```bash\n# 115M parameters, trained for 1.7M iterations (or about 2.5 weeks) on 32 V100\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq256-iter-1700000-log.jsonl\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq256-iter-1700000-model.th\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq256-iter-1700000-model-ema.th\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq256-iter-1700000-opt.th\npython train.py --hps ffhq256 --restore_path ffhq256-iter-1700000-model.th --restore_ema_path ffhq256-iter-1700000-model-ema.th --restore_log_path ffhq256-iter-1700000-log.jsonl --restore_optimizer_path ffhq256-iter-1700000-opt.th --test_eval\n# should be 0.4232 nats, or 0.61 bits per dim\n```\n\n### FFHQ-1024\n```bash\n# 115M parameters, trained for 1.7M iterations (or about 2.5 weeks) on 32 V100\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq1024-iter-1700000-log.jsonl\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq1024-iter-1700000-model.th\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq1024-iter-1700000-model-ema.th\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq1024-iter-1700000-opt.th\npython train.py --hps ffhq1024 --restore_path ffhq1024-iter-1700000-model.th --restore_ema_path ffhq1024-iter-1700000-model-ema.th --restore_log_path ffhq1024-iter-1700000-log.jsonl --restore_optimizer_path ffhq1024-iter-1700000-opt.th --test_eval\n# should be 1.678 nats, or 2.42 bits per dim\n```\n\n### CIFAR-10\n```bash\n# 39M parameters, trained for ~1M iterations with early stopping (a little less than a week on 2 GPUs)\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/cifar10-seed0-iter-900000-model-ema.th\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/cifar10-seed1-iter-1050000-model-ema.th\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/cifar10-seed2-iter-650000-model-ema.th\nwget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/cifar10-seed3-iter-1050000-model-ema.th\npython train.py --hps cifar10 --restore_ema_path cifar10-seed0-iter-900000-model-ema.th --test_eval\npython train.py --hps cifar10 --restore_ema_path cifar10-seed1-iter-1050000-model-ema.th --test_eval\npython train.py --hps cifar10 --restore_ema_path cifar10-seed2-iter-650000-model-ema.th --test_eval\npython train.py --hps cifar10 --restore_ema_path cifar10-seed3-iter-1050000-model-ema.th --test_eval\n# seeds 0, 1, 2, 3 should give 2.879, 2.842, 2.898, 2.864 bits per dim, for an average of 2.87 bits per dim.\n```\n", "release_dates": []}, {"name": "Video-Pre-Training", "description": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "\n\n# Video-Pre-Training\nVideo PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos\n\n\n> :page_facing_up: [Read Paper](https://cdn.openai.com/vpt/Paper.pdf) \\\n  :mega: [Blog Post](https://openai.com/blog/vpt) \\\n  :space_invader: [MineRL Environment](https://github.com/minerllabs/minerl) (note version 1.0+ required) \\\n  :checkered_flag: [MineRL BASALT Competition](https://www.aicrowd.com/challenges/neurips-2022-minerl-basalt-competition)\n\n\n# Running agent models\n\nInstall pre-requirements for [MineRL](https://minerl.readthedocs.io/en/latest/tutorials/index.html).\nThen install requirements with:\n\n```\npip install git+https://github.com/minerllabs/minerl\npip install -r requirements.txt\n```\n\nTo run the code, call\n\n```\npython run_agent.py --model [path to .model file] --weights [path to .weight file]\n```\n\nAfter loading up, you should see a window of the agent playing Minecraft.\n\n\n\n# Agent Model Zoo\nBelow are the model files and weights files for various pre-trained Minecraft models.\nThe 1x, 2x and 3x model files correspond to their respective model weights width.\n\n* [:arrow_down: 1x Model](https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-1x.model)\n* [:arrow_down: 2x Model](https://openaipublic.blob.core.windows.net/minecraft-rl/models/2x.model)\n* [:arrow_down: 3x Model](https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-3x.model)\n\n### Demonstration Only - Behavioral Cloning\nThese models are trained on video demonstrations of humans playing Minecraft\nusing behavioral cloning (BC) and are more general than later models which\nuse reinforcement learning (RL) to further optimize the policy.\nFoundational models are trained across all videos in a single training run\nwhile house and early game models refine their respective size foundational\nmodel further using either the housebuilding contractor data or early game video\nsub-set. See the paper linked above for more details.\n\n#### Foundational Model :chart_with_upwards_trend:\n  * [:arrow_down: 1x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-1x.weights)\n  * [:arrow_down: 2x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-2x.weights)\n  * [:arrow_down: 3x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-3x.weights)\n\n#### Fine-Tuned from House :chart_with_upwards_trend:\n  * [:arrow_down: 3x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/bc-house-3x.weights)\n\n#### Fine-Tuned from Early Game :chart_with_upwards_trend:\n  * [:arrow_down: 2x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/bc-early-game-2x.weights)\n  * [:arrow_down: 3x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/bc-early-game-3x.weights)\n\n### Models With Environment Interactions\nThese models further refine the above demonstration based models with a reward\nfunction targeted at obtaining diamond pickaxes. While less general then the behavioral\ncloning models, these models have the benefit of interacting with the environment\nusing a reward function and excel at progressing through the tech tree quickly.\nSee the paper for more information\non how they were trained and the exact reward schedule.\n\n#### RL from Foundation :chart_with_upwards_trend:\n  * [:arrow_down: 2x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-foundation-2x.weights)\n\n#### RL from House :chart_with_upwards_trend:\n  * [:arrow_down: 2x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-house-2x.weights)\n\n#### RL from Early Game :chart_with_upwards_trend:\n  * [:arrow_down: 2x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-early-game-2x.weights)\n\n# Running Inverse Dynamics Model (IDM)\n\nIDM aims to predict what actions player is taking in a video recording.\n\nSetup:\n* Install requirements: `pip install -r requirements.txt`\n* Download the IDM model [.model :arrow_down:](https://openaipublic.blob.core.windows.net/minecraft-rl/idm/4x_idm.model) and [.weight :arrow_down:](https://openaipublic.blob.core.windows.net/minecraft-rl/idm/4x_idm.weights) files\n* For demonstration purposes, you can use the contractor recordings shared below to. For this demo we use\n  [this .mp4](https://openaipublic.blob.core.windows.net/minecraft-rl/data/10.0/cheeky-cornflower-setter-02e496ce4abb-20220421-092639.mp4)\n  and [this associated actions file (.jsonl)](https://openaipublic.blob.core.windows.net/minecraft-rl/data/10.0/cheeky-cornflower-setter-02e496ce4abb-20220421-092639.jsonl).\n\nTo run the model with above files placed in the root directory of this code:\n```\npython run_inverse_dynamics_model.py -weights 4x_idm.weights --model 4x_idm.model --video-path cheeky-cornflower-setter-02e496ce4abb-20220421-092639.mp4 --jsonl-path cheeky-cornflower-setter-02e496ce4abb-20220421-092639.jsonl\n```\n\nA window should pop up which shows the video frame-by-frame, showing the predicted and true (recorded) actions side-by-side on the left.\n\nNote that `run_inverse_dynamics_model.py` is designed to be a demo of the IDM, not code to put it into practice.\n\n# Using behavioural cloning to fine-tune the models\n\n**Disclaimer:** This code is a rough demonstration only and not an exact recreation of what original VPT paper did (but it contains some preprocessing steps you want to be aware of)! As such, do not expect replicate the original experiments with this code. This code has been designed to be run-able on consumer hardware (e.g., 8GB of VRAM).\n\nSetup:\n* Install requirements: `pip install -r requirements.txt`\n* Download `.weights` and `.model` file for model you want to fine-tune.\n* Download contractor data (below) and place the `.mp4` and `.jsonl` files to the same directory (e.g., `data`). With default settings, you need at least 12 recordings.\n\nIf you downloaded the \"1x Width\" models and placed some data under `data` directory, you can perform finetuning with\n\n```\npython behavioural_cloning.py --data-dir data --in-model foundation-model-1x.model --in-weights foundation-model-1x.weights --out-weights finetuned-1x.weights\n```\n\nYou can then use `finetuned-1x.weights` when running the agent. You can change the training settings at the top of `behavioural_cloning.py`.\n\nMajor limitations:\n- Only trains single step at the time, i.e., errors are not propagated through timesteps.\n- Computes gradients one sample at a time to keep memory use low, but also slows down the code.\n\n# Contractor Demonstrations\n\n### Versions\nOver the course of the project we requested various demonstrations from contractors\nwhich we release as index files below. In general, major recorder versions change for a new\nprompt or recording feature while bug-fixes were represented as minor version changes.\nHowever, some\nrecorder versions we asked contractors to change their username when recording particular\nmodalities. Also, as contractors internally ask questions, clarification from one contractor may\nresult in a behavioral change in the other contractor. It is intractable to share every contractor's\nview for each version, but we've shared the prompts and major clarifications for each recorder\nversion where the task changed significantly.\n\n  <details>\n  <summary>Initial Prompt</summary>\n\n  We are collecting data for training AI models in Minecraft. You'll need to install java, download the modified version of minecraft (that collects and uploads your play data), and play minecraft survival mode! Paid per hour of gameplay. Prior experience in minecraft not. necessary. We do not collect any data that is unrelated to minecraft from your computer.\n\n  </details>\n\nThe following is a list of the available versions:\n\n* **6.x** Core recorder features subject to change [:arrow_down: index file](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/all_6xx_Jun_29.json)\n  * 6.9 First feature complete recorder version\n  * 6.10 Fixes mouse scaling on Mac when gui is open\n  * 6.11 Tracks the hotbar slot\n  * 6.13 Sprinting, swap-hands, ... (see commits below)\n    <details>\n    <summary>Commits</summary>\n\n    * improve replays that are cut in the middle of gui; working on riding boats / replays cut in the middle of a run\n    * improve replays by adding dwheel action etc, also, loosen up replay tolerances\n    * opencv version bump\n    * add swap hands, and recording of the step timestamp\n    * implement replaying from running and sprinting and tests\n    * do not record sprinting (can use stats for that)\n    * check for mouse button number, ignore >2\n    * handle the errors when mouse / keyboard are recorded as null\n\n    </details>\n* **7.x** Prompt changes [:arrow_down: index file](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/all_7xx_Apr_6.json)\n  * 7.6 Bump version for internal tracking\n    <details>\n    <summary>Additional ask to contractors</summary>\n\n    Right now, early game data is especially valuable to us. As such, we request that at least half of the data you upload is from the first 30 minutes of the game. This means that, for every hour of gameplay you spend in an older world, we ask you to play two sessions in which you create a new world and play for 30 minutes. You can play for longer in these worlds, but only the first 30 minutes counts as early game data.\n\n    </details>\n* **8.x** :clipboard: House Building from Scratch Task [:arrow_down: index](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/all_8xx_Jun_29.json)\n  <details>\n  <summary>Changes and Prompt</summary>\n\n  Hi all! Thank you for your hard work so far.\n\n  This week we would like to have you all collect data on a specific task.\n\n  This comes with a new recorder version 8.0 which you will need to update your recording script to download.\n\n  This week we would like you to use a new world each time you play, so loading existing worlds is disabled.\n\n  The new task is as follows:\n\n  Starting in a new world, build a simple house in 10-15 minutes. This corresponds to one day and a bit of the night. Please use primarily wood, dirt, and sand, as well as crafted wood items such as doors, fences, ect. in constructing your house. Avoid using difficult items such as stone. Aside from those constraints, you may decorate the structure you build as you wish. It does not need to have any specific furniture. For example, it is OK if there is no bed in your house. If you have not finished the house by the sunrise (20 minutes) please exit and continue to another demonstration. Please continue to narrate what you are doing while completing this task.\n\n  Since you will be unable to resume building after exiting Minecraft or going back to the main menu, you must finish these demonstrations in one session. Pausing via the menu is still supported. If you want to view your creations later, they will be saved locally so you can look at them in your own time. We may use these save files in a future task so if you have space, please leave the save files titled \u201cbuild-house-15-min-\u201c.\n\n  For this week try to avoid all cobblestone / stone / granite\n\n  For this week we just want simple houses without sleeping. If 10 minutes is too short, let us know and we can think of how to adjust!\n\n  Stone tools are ok but I think you may run-out of time\n\n  Changes:\n    * Timer ends episode after 10 realtime minutes\n    * Worlds are named: `\"build-house-15-min-\" + Math.abs(random.nextInt());`\n\n  </details>\n\n  * Note this version introduces 10-minute timer that ends the episode. It\n  cut experiments short occasionally and was fixed in 9.1\n  * 8.0 Simple House\n  * 8.2 Update upload script\n* **9.x** :clipboard: House Building from Random Starting Materials Task [:arrow_down: index](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/all_9xx_Jun_29.json)\n    <details>\n    <summary>Changes and Prompt</summary>\n\n    You now will have 10 minutes to use the provided resources to build your house / home / or structure. In this version, the experiment will time out after 10 minutes if you are not complete so don't be alarmed if that happens, it is intentional.\n\n    No need to use up all the resources! It's ok to collect a few things but spend the majority of the time placing blocks (the act of placing seems to be harder to learn)\n\n    Changes:\n    * Worlds are named: `\"design-house-10-min-\" + Math.abs(random.nextInt());`\n    * Starting inventory given by code below\n    </details>\n\n    <details>\n    <summary>Random Starting Inventory Code</summary>\n\n  ```java\n        Random random = new Random();\n        List<ItemStack> hotbar = new ArrayList<>();\n        List<ItemStack> inventory = new ArrayList<>();\n\n        // Ensure we give the player the basic tools in their hot bar\n        hotbar.add(new ItemStack(Items.STONE_AXE));\n        hotbar.add(new ItemStack(Items.STONE_PICKAXE));\n        hotbar.add(new ItemStack(Items.STONE_SHOVEL));\n        hotbar.add(new ItemStack(Items.CRAFTING_TABLE));\n\n        // Add some random items to the player hotbar as well\n        addToList(hotbar, inventory, Items.TORCH, random.nextInt(16) * 2 + 2);\n\n        // Next add main building blocks\n        if (random.nextFloat() < 0.7) {\n           addToList(hotbar, inventory, Items.OAK_FENCE_GATE, random.nextInt(5));\n           addToList(hotbar, inventory, Items.OAK_FENCE, random.nextInt(5) * 64);\n           addToList(hotbar, inventory, Items.OAK_DOOR, random.nextInt(5));\n           addToList(hotbar, inventory, Items.OAK_TRAPDOOR, random.nextInt(2) * 2);\n           addToList(hotbar, inventory, Items.OAK_PLANKS, random.nextInt(3) * 64 + 128);\n           addToList(hotbar, inventory, Items.OAK_SLAB, random.nextInt(3) * 64);\n           addToList(hotbar, inventory, Items.OAK_STAIRS, random.nextInt(3) * 64);\n           addToList(hotbar, inventory, Items.OAK_LOG, random.nextInt(2) * 32);\n           addToList(hotbar, inventory, Items.OAK_PRESSURE_PLATE, random.nextInt(5));\n        } else {\n           addToList(hotbar, inventory, Items.BIRCH_FENCE_GATE, random.nextInt(5));\n           addToList(hotbar, inventory, Items.BIRCH_FENCE, random.nextInt(5) * 64);\n           addToList(hotbar, inventory, Items.BIRCH_DOOR, random.nextInt(5));\n           addToList(hotbar, inventory, Items.BIRCH_TRAPDOOR, random.nextInt(2) * 2);\n           addToList(hotbar, inventory, Items.BIRCH_PLANKS, random.nextInt(3) * 64 + 128);\n           addToList(hotbar, inventory, Items.BIRCH_SLAB, random.nextInt(3) * 64);\n           addToList(hotbar, inventory, Items.BIRCH_STAIRS, random.nextInt(3) * 64);\n           addToList(hotbar, inventory, Items.BIRCH_LOG, random.nextInt(2) * 32);\n           addToList(hotbar, inventory, Items.BIRCH_PRESSURE_PLATE, random.nextInt(5));\n        }\n\n        // Now add some random decoration items to the player inventory\n        addToList(hotbar, inventory, Items.CHEST, random.nextInt(3));\n        addToList(hotbar, inventory, Items.FURNACE, random.nextInt(2) + 1);\n        addToList(hotbar, inventory, Items.GLASS_PANE,  random.nextInt(5) * 4);\n        addToList(hotbar, inventory, Items.WHITE_BED, (int) (random.nextFloat() + 0.2)); // Bed 20% of the time\n        addToList(hotbar, inventory, Items.PAINTING, (int) (random.nextFloat() + 0.1)); // Painting 10% of the time\n        addToList(hotbar, inventory, Items.FLOWER_POT, (int) (random.nextFloat() + 0.1) * 4); // 4 Flower pots 10% of the time\n        addToList(hotbar, inventory, Items.OXEYE_DAISY, (int) (random.nextFloat() + 0.1) * 4); // 4 Oxeye daisies 10% of the time\n        addToList(hotbar, inventory, Items.POPPY, (int) (random.nextFloat() + 0.1) * 4); // 4 Poppies 10% of the time\n        addToList(hotbar, inventory, Items.SUNFLOWER, (int) (random.nextFloat() + 0.1) * 4); // 4 Sunflowers 10% of the time\n\n        // Shuffle the hotbar slots and inventory slots\n        Collections.shuffle(hotbar);\n        Collections.shuffle(inventory);\n\n        // Give the player the items\n        this.mc.getIntegratedServer().getPlayerList().getPlayers().forEach(p -> {\n           if (p.getUniqueID().equals(this.getUniqueID())) {\n               hotbar.forEach(p.inventory::addItemStackToInventory);\n               inventory.forEach(p.inventory::addItemStackToInventory);\n           }\n        });\n  ```\n\n    </details>\n\n     * 9.0 First version\n     * 9.1 Fixed timer bug\n* **10.0** :clipboard: Obtain Diamond Pickaxe Task [:arrow_down: index](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/all_10xx_Jun_29.json)\n  <details>\n  <summary>Changes and Prompt</summary>\n  Prompt:\n\n  For this new task we have given you 20 minutes to craft a diamond pickaxe. We ask that you do not try to search for villages or other ways of getting diamonds, but if you are spawned in view of one, or happen to fall into a cave structure feel free to explore it for diamonds.\n  If 20 min is not enough that is OK. It will happen on some seeds because of bad luck. Please do not use glitches to find the diamonds.\n\n  Changes:\n  * change to 20 minute time limit\n  * _don't count gui time as part of the time limit_\n  * World are named `\"collect-diamond-pickaxe-15min-\" + Math.abs(random.nextInt());`\n\n  </details>\n\n\nSometimes we asked the contractors to signify other tasks besides changing the version. This\nprimarily occurred in versions 6 and 7 as 8, 9 and 10 are all task specific.\n\n<details>\n<summary>Prompt to contractors (click to show)</summary>\nAnother request about additional time - please use some of it to chop trees. Specifically, please start the recorder by adding --username treechop argument to the script (i.e. use play --username treechop on windows, ./play.sh --username treechop on osx/linux), and spend some time chopping trees! Getting wooden or stone tools is ok, but please spend the majority of the with username treechop specifically chopping. I did it myself for about 15 minutes, and it does get boring pretty quickly, so I don't expect you to do it all the time, but please do at least a little bit of chopping. Feel free to play normally the rest of the time (but please restart without --username treechop argument when you are not chopping)\nHowever, it is preferable that you start a new world though, and use only the tools that are easily obtainable in that world. I'll see what I can do about getting player an iron axe - that sounds reasonable, and should not be hard, but will require a code update.\n</details>\n\n### Environment\nWe restrict the contractors to playing Minecraft in windowed mode at 720p which we downsample at 20hz to 360p\nto minimize space. We also disabled the options screen to prevent the contractor from\nchanging things such as brightness, or rendering options. We ask contractors not to press keys\nsuch as f3 which shows a debug overlay, however some contractors may still do this.\n\n\n### Data format\n\nDemonstrations are broken up into up to 5 minute segments consisting of a series of\ncompressed screen observations, actions, environment statistics, and a checkpoint\nsave file from the start of the segment. Each relative path in the index will\nhave all the files for that given segment, however if a file was dropped while\nuploading, the corresponding relative path is not included in the index therefore\nthere may be missing chunks from otherwise continuous demonstrations.\n\nIndex files are provided for each version as a json file:\n```json\n{\n  \"basedir\": \"https://openaipublic.blob.core.windows.net/data/\",\n  \"relpaths\": [\n    \"8.0/cheeky-cornflower-setter-74ae6c2eae2e-20220315-122354\",\n    ...\n  ]\n}\n```\nRelative paths follow the following format:\n* `<recorder-version>/<contractor-alias>-<session-id>-<date>-<time>`\n\n> Note that due to network errors, some segments may be missing from otherwise\ncontinuous demonstrations.\n\nYour data loader can then find following files:\n* Video observation: `<basedir>/<relpath>.mp4`\n* Action file: `<basedir>/<relpath>.jsonl`\n* Options file: `<basedir>/<relpath>-options.json`\n* Checkpoint save file: `<basedir>/<relpath>.zip`\n\nThe action file is **not**  a valid json object: each line in\naction file is an individual action dictionary.\n\nFor v7.x, the actions are in form\n```json\n{\n  \"mouse\": {\n    \"x\": 274.0,\n    \"y\": 338.0,\n    \"dx\": 0.0,\n    \"dy\": 0.0,\n    \"scaledX\": -366.0,\n    \"scaledY\": -22.0,\n    \"dwheel\": 0.0,\n    \"buttons\": [],\n    \"newButtons\": []\n  },\n  \"keyboard\": {\n    \"keys\": [\n      \"key.keyboard.a\",\n      \"key.keyboard.s\"\n    ],\n    \"newKeys\": [],\n    \"chars\": \"\"\n  },\n  \"isGuiOpen\": false,\n  \"isGuiInventory\": false,\n  \"hotbar\": 4,\n  \"yaw\": -112.35006,\n  \"pitch\": 8.099996,\n  \"xpos\": 841.364694513396,\n  \"ypos\": 63.0,\n  \"zpos\": 24.956354839537802,\n  \"tick\": 0,\n  \"milli\": 1649575088006,\n  \"inventory\": [\n    {\n      \"type\": \"oak_door\",\n      \"quantity\": 3\n    },\n    {\n      \"type\": \"oak_planks\",\n      \"quantity\": 59\n    },\n    {\n      \"type\": \"stone_pickaxe\",\n      \"quantity\": 1\n    },\n    {\n      \"type\": \"oak_planks\",\n      \"quantity\": 64\n    }\n  ],\n  \"serverTick\": 6001,\n  \"serverTickDurationMs\": 36.3466,\n  \"stats\": {\n    \"minecraft.custom:minecraft.jump\": 4,\n    \"minecraft.custom:minecraft.time_since_rest\": 5999,\n    \"minecraft.custom:minecraft.play_one_minute\": 5999,\n    \"minecraft.custom:minecraft.time_since_death\": 5999,\n    \"minecraft.custom:minecraft.walk_one_cm\": 7554,\n    \"minecraft.use_item:minecraft.oak_planks\": 5,\n    \"minecraft.custom:minecraft.fall_one_cm\": 269,\n    \"minecraft.use_item:minecraft.glass_pane\": 3\n  }\n}\n```\n\n# BASALT 2022 dataset\n\nWe also collected a dataset of demonstrations for the [MineRL BASALT 2022](https://www.aicrowd.com/challenges/neurips-2022-minerl-basalt-competition) competition, with around 150GB of data per task.\n\n**Note**: To avoid confusion with the competition rules, the action files (.jsonl) have been stripped of information that is not allowed in the competition. We will upload unmodified dataset after the competition ends.\n\n* **FindCave** [:arrow_down: index file](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/find-cave-Jul-28.json)\n  * <details>\n    <summary>Prompt to contractors (click to show)</summary>\n\n    ```\n    Look around for a cave. When you are inside one, quit the game by opening main menu and pressing \"Save and Quit To Title\".\n    You are not allowed to dig down from the surface to find a cave.\n\n    Timelimit: 3 minutes.\n    Example recordings: https://www.youtube.com/watch?v=TclP_ozH-eg\n    ```\n    </details>\n* **MakeWaterfall** [:arrow_down: index file](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/waterfall-Jul-28.json)\n  * <details>\n    <summary>Prompt to contractors (click to show)</summary>\n\n    ```\n    After spawning in a mountainous area with a water bucket and various tools, build a beautiful waterfall and then reposition yourself to \u201ctake a scenic picture\u201d of the same waterfall, and then quit the game by opening the menu and selecting \"Save and Quit to Title\"\n\n    Timelimit: 5 minutes.\n    Example recordings: https://youtu.be/NONcbS85NLA\n    ```\n    </details>\n* **MakeVillageAnimalPen** [:arrow_down: index file](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/pen-animals-Jul-28.json)\n  * <details>\n    <summary>Prompt to contractors (click to show)</summary>\n\n    ```\n    After spawning in a village, build an animal pen next to one of the houses in a village. Use your fence posts to build one animal pen that contains at least two of the same animal. (You are only allowed to pen chickens, cows, pigs, sheep or rabbits.) There should be at least one gate that allows players to enter and exit easily. The animal pen should not contain more than one type of animal. (You may kill any extra types of animals that accidentally got into the pen.) Don\u2019t harm the village.\n    After you are done, quit the game by opening the menu and pressing \"Save and Quit to Title\".\n\n    You may need to terraform the area around a house to build a pen. When we say not to harm the village, examples include taking animals from existing pens, damaging existing houses or farms, and attacking villagers. Animal pens must have a single type of animal: pigs, cows, sheep, chicken or rabbits.\n\n    The food items can be used to lure in the animals: if you hold seeds in your hand, this attracts nearby chickens to you, for example.\n\n    Timelimit: 5 minutes.\n    Example recordings: https://youtu.be/SLO7sep7BO8\n    ```\n    </details>\n* **BuildVillageHouse** [:arrow_down: index file](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/build-house-Jul-28.json)\n  * <details>\n    <summary>Prompt to contractors (click to show)</summary>\n\n    ```\n    Taking advantage of the items in your inventory, build a new house in the style of the village (random biome), in an appropriate location (e.g. next to the path through the village), without harming the village in the process.\n    Then give a brief tour of the house (i.e. spin around slowly such that all of the walls and the roof are visible).\n\n    * You start with a stone pickaxe and a stone axe, and various building blocks. It\u2019s okay to break items that you misplaced (e.g. use the stone pickaxe to break cobblestone blocks).\n    * You are allowed to craft new blocks.\n\n    Please spend less than ten minutes constructing your house.\n\n    You don\u2019t need to copy another house in the village exactly (in fact, we\u2019re more interested in having slight deviations, while keeping the same \"style\"). You may need to terraform the area to make space for a new house.\n    When we say not to harm the village, examples include taking animals from existing pens, damaging existing houses or farms, and attacking villagers.\n\n    After you are done, quit the game by opening the menu and pressing \"Save and Quit to Title\".\n\n    Timelimit: 12 minutes.\n    Example recordings: https://youtu.be/WeVqQN96V_g\n    ```\n    </details>\n\n\n\n# Contribution\nThis was a large effort by a dedicated team at OpenAI:\n[Bowen Baker](https://github.com/bowenbaker),\n[Ilge Akkaya](https://github.com/ilge),\n[Peter Zhokhov](https://github.com/pzhokhov),\n[Joost Huizinga](https://github.com/JoostHuizinga),\n[Jie Tang](https://github.com/jietang),\n[Adrien Ecoffet](https://github.com/AdrienLE),\n[Brandon Houghton](https://github.com/brandonhoughton),\n[Raul Sampedro](https://github.com/samraul),\nJeff Clune\nThe code here represents a minimal version of our model code which was\nprepared by [Anssi Kanervisto](https://github.com/miffyli) and others so that these models could be used as\npart of the MineRL BASALT competition.\n", "release_dates": ["2022-07-26T22:05:04Z", "2022-07-11T20:08:55Z"]}, {"name": "vime", "description": "Code for the paper \"Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks\"", "language": "Python", "license": null, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# How to run VIME\n\nVariational Information Maximizing Exploration (VIME) as presented in Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks by *R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, P. Abbeel* (http://arxiv.org/abs/1605.09674). \n\nTo reproduce the results, you should first have [rllab](https://github.com/rllab/rllab) and Mujoco v1.31 configured. Then, run the following commands in the root folder of `rllab`:\n\n```bash\ngit submodule add -f git@github.com:openai/vime.git sandbox/vime\ntouch sandbox/__init__.py\n```\n\nThen you can do the following:\n- Execute TRPO+VIME on the hierarchical SwimmerGather environment via `python sandbox/vime/experiments/run_trpo_expl.py`.\n", "release_dates": []}, {"name": "weak-to-strong", "description": null, "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**STATUS**: This codebase is not well tested and does not use the exact same settings we used in the paper, but in our experience gives qualitatively similar results when using large model size gaps and multiple seeds.  Expected results can be found for two datasets below.\n\n# Weak-to-strong generalization\n\n![Our setup and how it relates to superhuman AI alignment](./weak-to-strong-setup.png)\n\nThis project contains code for implementing our [paper on weak-to-strong generalization](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf).\n\nThe primary codebase contains a re-implementation of our weak-to-strong learning setup for binary classification tasks.  The codebase contains code for fine-tuning pretrained language models, and also training against the labels from another language model.  We support various losses described in the paper as well, such as the confidence auxiliary loss.\n\nThe `vision` directory contains stand-alone code for weak-to-strong in the vision models setting (AlexNet -> DINO on ImageNet).\n\n### Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\n\n#### Installation\n\nYou need to have Python installed on your machine. The project uses `pyproject.toml` to manage dependencies. To install the dependencies, you can use a package manager like `pip`:\n\n```\npip install .\n```\n\n#### Running the Script\n\nThe main script of the project is `sweep.py`. It can be run from the command line using the following command:\n```\npython sweep.py --model_sizes=gpt2,gpt2-medium\n```\n\nIn addition to `--model_sizes`, `sweep.py` takes in almost all of the arguments that `train_simple.py` takes (e.g.\n`--batch_size`, `--n_docs`, `--n_test_docs` etc., see `train_simple.py` for a full list). These arguments are simply\nforwarded to `train_simple.py`.\n\n`sweep.py` calls `train_simple.py` in the following way:\n1. First, it calls `train_simple.py` for each model size to train the ground truth models\n2. Then, for each pair of weak and strong models in `model_sizes` (where a model can be the strong model in the pair\n   only if its index in the `model_sizes` list is >= the index of the weak model), it calls `train_simple.py` with a\n   `--weak_model_size` argument so that the strong model is trained with the labels of the weak model.\n\nE.g. the example above will run gpt2 (ground truth), gpt2-medium (ground truth), gpt2 -> gpt2, gpt2 -> gpt2-medium, and\ngpt2-medium -> gpt2-medium.\n\nIf needed, you can also run `train_simple.py` directly.\n\nNote that `sweep.py` will not accept the arguments `--weak_model_size`, `--weak_labels_path` or `--model_size` (as opposed\nto `--model_sizes`, with an \"s\") as choosing their values automatically is precisely the point of `sweep.py`.\n\nAn example of Jupyter notebook for plotting results is found in `notebooks/Plotting.ipynb`.\n\nAt the time of release, the main script was called `train_weak_to_strong.py`, but it was less usable than\n`sweep.py` and `train_simple.py`. It is preserved here and the old instructions are given at the end of the document.\n\n#### Expected results\n\n<img src=\"notebooks/amazon_polarity.png\" width=\"350\">\n<br>\n<img src=\"notebooks/anthropic_hh.png\" width=\"350\">\n<br>\n<img src=\"notebooks/boolq.png\" width=\"350\">\n<br>\n<img src=\"notebooks/cosmos_qa.png\" width=\"350\">\n<br>\n<img src=\"notebooks/sciq.png\" width=\"350\">\n\n### Authors\n\n- Adrien Ecoffet\n- Manas Joglekar\n- Jeffrey Wu\n- Jan Hendrik Kirchner\n- Pavel Izmailov (vision)\n\n### License\n\nThis project is licensed under the MIT License - see the LICENSE.md file for details.\n\n### Acknowledgments\n\n- Hugging Face for their open-source transformer models\n\n### Original single run script\n\nYou can run the original training script using:\n```\npython train_weak_to_strong.py\n```\n\nThe script accepts several command-line arguments to customize the training process. Here are some examples:\n\n```\npython train_weak_to_strong.py --batch_size 32 --max_ctx 512 --ds_name \"sciq\" --loss \"logconf\" --n_docs 1000 --n_test_docs 100 --weak_model_size \"gpt2-medium\" --strong_model_size \"gpt2-large\" --seed 42\n```\n\nThe notebook `notebooks/Plotting_old.ipynb` preserves the plotting notebook corresponding to old style training.\n\nThe key difference between this style and the new `sweep.py` style is that `train_weak_to_strong.py` will always\ntrain three models: a weak model, a transfer model, and a strong model. `sweep.py` optimizes this by training\na series of ground truth models (which will serve as weak and strong models) as well as a series of transfer models\nall in one go. This reduces training duplication and is arguably simpler. The files generated by `train_simple.py`\nand `sweep.py` are also simpler to use. ", "release_dates": []}, {"name": "web-crawl-q-and-a-example", "description": "Learn how to crawl your website and build a Q/A bot with the OpenAI API", "language": "Jupyter Notebook", "license": null, "readme": "# Web  Q&A with Embeddings\n\nLearn how to crawl your website and build a Q/A bot with the OpenAI API. You can find the full tutorial in the [OpenAI documentation](https://platform.openai.com/docs/tutorials/web-qa-embeddings).\n", "release_dates": []}, {"name": "websockify", "description": "Websockify is a WebSocket to TCP proxy/bridge. This allows a browser to connect  to any application/server/service. Implementations in Python, C, Node.js and Ruby.", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## websockify: WebSockets support for any application/server\n\nwebsockify was formerly named wsproxy and was part of the\n[noVNC](https://github.com/kanaka/noVNC) project.\n\nAt the most basic level, websockify just translates WebSockets traffic\nto normal socket traffic. Websockify accepts the WebSockets handshake,\nparses it, and then begins forwarding traffic between the client and\nthe target in both directions.\n\n### News/help/contact\n\nNotable commits, announcements and news are posted to\n<a href=\"http://www.twitter.com/noVNC\">@noVNC</a>\n\nIf you are a websockify developer/integrator/user (or want to be)\nplease join the <a\nhref=\"https://groups.google.com/forum/?fromgroups#!forum/novnc\">noVNC/websockify\ndiscussion group</a>\n\nBugs and feature requests can be submitted via [github\nissues](https://github.com/kanaka/websockify/issues).\n\nIf you want to show appreciation for websockify you could donate to a great\nnon-profits such as: [Compassion\nInternational](http://www.compassion.com/), [SIL](http://www.sil.org),\n[Habitat for Humanity](http://www.habitat.org), [Electronic Frontier\nFoundation](https://www.eff.org/), [Against Malaria\nFoundation](http://www.againstmalaria.com/), [Nothing But\nNets](http://www.nothingbutnets.net/), etc. Please tweet <a\nhref=\"http://www.twitter.com/noVNC\">@noVNC</a> if you do.\n\n### WebSockets binary data\n\nStarting with websockify 0.5.0, only the HyBi / IETF\n6455 WebSocket protocol is supported.\n\nWebsockify negotiates whether to base64 encode traffic to and from the\nclient via the subprotocol header (Sec-WebSocket-Protocol). The valid\nsubprotocol values are 'binary' and 'base64' and if the client sends\nboth then the server (the python implementation) will prefer 'binary'.\nThe 'binary' subprotocol indicates that the data will be sent raw\nusing binary WebSocket frames. Some HyBi clients (such as the Flash\nfallback and older Chrome and iOS versions) do not support binary data\nwhich is why the negotiation is necessary.\n\n\n### Encrypted WebSocket connections (wss://)\n\nTo encrypt the traffic using the WebSocket 'wss://' URI scheme you need to\ngenerate a certificate and key for Websockify to load. By default, Websockify\nloads a certificate file name `self.pem` but the `--cert=CERT` and `--key=KEY`\noptions can override the file name. You can generate a self-signed certificate\nusing openssl. When asked for the common name, use the hostname of the server\nwhere the proxy will be running:\n\n```\nopenssl req -new -x509 -days 365 -nodes -out self.pem -keyout self.pem\n```\n\nFor a self-signed certificate to work, you need to make your client/browser\nunderstand it. You can do this by installing it as accepted certificate, or by\nusing that same certificate for a HTTPS connection to which you navigate first\nand approve. Browsers generally don't give you the \"trust certificate?\" prompt\nby opening a WSS socket with invalid certificate, hence you need to have it\nacccept it by either of those two methods.\n\nIf you have a commercial/valid SSL certificate with one ore more intermediate\ncertificates, concat them into one file, server certificate first, then the\nintermediate(s) from the CA, etc. Point to this file with the `--cert` option\nand then also to the key with `--key`. Finally, use `--ssl-only` as needed.\n\n\n### Websock Javascript library\n\n\nThe `include/websock.js` Javascript library library provides a Websock\nobject that is similar to the standard WebSocket object but Websock\nenables communication with raw TCP sockets (i.e. the binary stream)\nvia websockify. This is accomplished by base64 encoding the data\nstream between Websock and websockify.\n\nWebsock has built-in receive queue buffering; the message event\ndoes not contain actual data but is simply a notification that\nthere is new data available. Several rQ* methods are available to\nread binary data off of the receive queue.\n\nThe Websock API is documented on the [websock.js API wiki page](https://github.com/kanaka/websockify/wiki/websock.js)\n\nSee the \"Wrap a Program\" section below for an example of using Websock\nand websockify as a browser telnet client (`wstelnet.html`).\n\n\n### Additional websockify features\n\nThese are not necessary for the basic operation.\n\n* Daemonizing: When the `-D` option is specified, websockify runs\n  in the background as a daemon process.\n\n* SSL (the wss:// WebSockets URI): This is detected automatically by\n  websockify by sniffing the first byte sent from the client and then\n  wrapping the socket if the data starts with '\\x16' or '\\x80'\n  (indicating SSL).\n\n* Flash security policy: websockify detects flash security policy\n  requests (again by sniffing the first packet) and answers with an\n  appropriate flash security policy response (and then closes the\n  port). This means no separate flash security policy server is needed\n  for supporting the flash WebSockets fallback emulator.\n\n* Session recording: This feature that allows recording of the traffic\n  sent and received from the client to a file using the `--record`\n  option.\n\n* Mini-webserver: websockify can detect and respond to normal web\n  requests on the same port as the WebSockets proxy and Flash security\n  policy. This functionality is activated with the `--web DIR` option\n  where DIR is the root of the web directory to serve.\n\n* Wrap a program: see the \"Wrap a Program\" section below.\n\n* Log files: websockify can save all logging information in a file.\n  This functionality is activated with the `--log-file FILE` option\n  where FILE is the file where the logs should be saved.\n\n### Implementations of websockify\n\nThe primary implementation of websockify is in python. There are\nseveral alternate implementations in other languages (C, Node.js,\nClojure, Ruby) in the `other/` subdirectory (with varying levels of\nfunctionality).\n\nIn addition there are several other external projects that implement\nthe websockify \"protocol\". See the alternate implementation [Feature\nMatrix](https://github.com/kanaka/websockify/wiki/Feature_Matrix) for\nmore information.\n\n\n### Wrap a Program\n\nIn addition to proxying from a source address to a target address\n(which may be on a different system), websockify has the ability to\nlaunch a program on the local system and proxy WebSockets traffic to\na normal TCP port owned/bound by the program.\n\nThe is accomplished with a small LD_PRELOAD library (`rebind.so`)\nwhich intercepts bind() system calls by the program. The specified\nport is moved to a new localhost/loopback free high port. websockify\nthen proxies WebSockets traffic directed to the original port to the\nnew (moved) port of the program.\n\nThe program wrap mode is invoked by replacing the target with `--`\nfollowed by the program command line to wrap.\n\n    `./run 2023 -- PROGRAM ARGS`\n\nThe `--wrap-mode` option can be used to indicate what action to take\nwhen the wrapped program exits or daemonizes.\n\nHere is an example of using websockify to wrap the vncserver command\n(which backgrounds itself) for use with\n[noVNC](https://github.com/kanaka/noVNC):\n\n    `./run 5901 --wrap-mode=ignore -- vncserver -geometry 1024x768 :1`\n\nHere is an example of wrapping telnetd (from krb5-telnetd). telnetd\nexits after the connection closes so the wrap mode is set to respawn\nthe command:\n\n    `sudo ./run 2023 --wrap-mode=respawn -- telnetd -debug 2023`\n\nThe `wstelnet.html` page demonstrates a simple WebSockets based telnet\nclient (use 'localhost' and '2023' for the host and port\nrespectively).\n\n\n### Building the Python ssl module (for python 2.5 and older)\n\n* Install the build dependencies. On Ubuntu use this command:\n\n    `sudo aptitude install python-dev bluetooth-dev`\n\n* At the top level of the websockify repostory, download, build and\n  symlink the ssl module:\n\n    `wget --no-check-certificate http://pypi.python.org/packages/source/s/ssl/ssl-1.15.tar.gz`\n\n    `tar xvzf ssl-1.15.tar.gz`\n\n    `cd ssl-1.15`\n\n    `make`\n\n    `cd ../`\n\n    `ln -sf ssl-1.15/build/lib.linux-*/ssl ssl`\n\n", "release_dates": []}, {"name": "weightnorm", "description": "Example code for Weight Normalization, from \"Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks\"", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n\n# Weight Normalization\n\nThis repo contains example code for [Weight Normalization](https://arxiv.org/abs/1602.07868), as described in the following \npaper:\n\n**Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks**, by\nTim Salimans, and Diederik P. Kingma.\n\n- The folder 'lasagne' contains code using the Lasagne package for Theano. This code was used to run the CIFAR-10 experiments in the paper.\n- The folder 'tensorflow' contains a single nn.py file with a direct implementation copied from our [PixelCNN++](https://github.com/openai/pixel-cnn) repository.\n- The folder 'keras' contains example code for use with the Keras package.\n\n## Citation\n\nIf you find this code useful please cite us in your work:\n\n```\n@inproceedings{Salimans2016WeightNorm,\n  title={Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},\n  author={Tim Salimans and Diederik P. Kingma},\n  booktitle={Neural Information Processing Systems 2016},\n  year={2016}\n}\n```\n", "release_dates": []}, {"name": "whisper", "description": "Robust Speech Recognition via Large-Scale Weak Supervision", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Whisper\n\n[[Blog]](https://openai.com/blog/whisper)\n[[Paper]](https://arxiv.org/abs/2212.04356)\n[[Model card]](https://github.com/openai/whisper/blob/main/model-card.md)\n[[Colab example]](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb)\n\nWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.\n\n\n## Approach\n\n![Approach](https://raw.githubusercontent.com/openai/whisper/main/approach.png)\n\nA Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.\n\n\n## Setup\n\nWe used Python 3.9.9 and [PyTorch](https://pytorch.org/) 1.10.1 to train and test our models, but the codebase is expected to be compatible with Python 3.8-3.11 and recent PyTorch versions. The codebase also depends on a few Python packages, most notably [OpenAI's tiktoken](https://github.com/openai/tiktoken) for their fast tokenizer implementation. You can download and install (or update to) the latest release of Whisper with the following command:\n\n    pip install -U openai-whisper\n\nAlternatively, the following command will pull and install the latest commit from this repository, along with its Python dependencies:\n\n    pip install git+https://github.com/openai/whisper.git \n\nTo update the package to the latest version of this repository, please run:\n\n    pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n\nIt also requires the command-line tool [`ffmpeg`](https://ffmpeg.org/) to be installed on your system, which is available from most package managers:\n\n```bash\n# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on Arch Linux\nsudo pacman -S ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on Windows using Scoop (https://scoop.sh/)\nscoop install ffmpeg\n```\n\nYou may need [`rust`](http://rust-lang.org) installed as well, in case [tiktoken](https://github.com/openai/tiktoken) does not provide a pre-built wheel for your platform. If you see installation errors during the `pip install` command above, please follow the [Getting started page](https://www.rust-lang.org/learn/get-started) to install Rust development environment. Additionally, you may need to configure the `PATH` environment variable, e.g. `export PATH=\"$HOME/.cargo/bin:$PATH\"`. If the installation fails with `No module named 'setuptools_rust'`, you need to install `setuptools_rust`, e.g. by running:\n\n```bash\npip install setuptools-rust\n```\n\n\n## Available models and languages\n\nThere are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and inference speed relative to the large model; actual speed may vary depending on many factors including the available hardware.\n\n|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n\nThe `.en` models for English-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. We observed that the difference becomes less significant for the `small.en` and `medium.en` models.\n\nWhisper's performance varies widely depending on the language. The figure below shows a performance breakdown of `large-v3` and `large-v2` models by language, using WERs (word error rates) or CER (character error rates, shown in *Italic*) evaluated on the Common Voice 15 and Fleurs datasets. Additional WER/CER metrics corresponding to the other models and datasets can be found in Appendix D.1, D.2, and D.4 of [the paper](https://arxiv.org/abs/2212.04356), as well as the BLEU (Bilingual Evaluation Understudy) scores for translation in Appendix D.3.\n\n![WER breakdown by language](https://github.com/openai/whisper/assets/266841/f4619d66-1058-4005-8f67-a9d811b77c62)\n\n\n\n## Command-line usage\n\nThe following command will transcribe speech in audio files, using the `medium` model:\n\n    whisper audio.flac audio.mp3 audio.wav --model medium\n\nThe default setting (which selects the `small` model) works well for transcribing English. To transcribe an audio file containing non-English speech, you can specify the language using the `--language` option:\n\n    whisper japanese.wav --language Japanese\n\nAdding `--task translate` will translate the speech into English:\n\n    whisper japanese.wav --language Japanese --task translate\n\nRun the following to view all available options:\n\n    whisper --help\n\nSee [tokenizer.py](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py) for the list of all available languages.\n\n\n## Python usage\n\nTranscription can also be performed within Python: \n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])\n```\n\nInternally, the `transcribe()` method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.\n\nBelow is an example usage of `whisper.detect_language()` and `whisper.decode()` which provide lower-level access to the model.\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\n\n# load audio and pad/trim it to fit 30 seconds\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\n\n# make log-Mel spectrogram and move to the same device as the model\nmel = whisper.log_mel_spectrogram(audio).to(model.device)\n\n# detect the spoken language\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")\n\n# decode the audio\noptions = whisper.DecodingOptions()\nresult = whisper.decode(model, mel, options)\n\n# print the recognized text\nprint(result.text)\n```\n\n## More examples\n\nPlease use the [\ud83d\ude4c Show and tell](https://github.com/openai/whisper/discussions/categories/show-and-tell) category in Discussions for sharing more example usages of Whisper and third-party extensions such as web demos, integrations with other tools, ports for different platforms, etc.\n\n\n## License\n\nWhisper's code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details.\n", "release_dates": ["2023-11-17T19:59:53Z", "2023-11-06T18:16:19Z", "2023-11-06T11:09:27Z", "2023-09-19T00:13:44Z", "2023-03-15T07:40:02Z", "2023-03-08T23:49:55Z", "2023-03-08T04:37:08Z", "2023-03-07T02:53:46Z", "2023-01-24T19:12:00Z", "2023-01-18T00:13:07Z"]}, {"name": "zbarlight", "description": "A simple wrapper for zbar", "language": "Python", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "readme": null, "release_dates": []}]