[{"name": "aardvark-prototype", "description": "Aardvark evaluation artifact", "language": "Go", "license": null, "readme": "# Aardvark Prototype Artifact\n\n## Setup\n\nThese instructions should work on Ubuntu 18.04.4.\n\nTo replicate benchmarks, clone this repository and its submodules.\n\nBuild the vector commitment library:\n```\n$ cd veccom-rust\n$ cargo build --release\n$ go test -v\n```\nThe last test command should pass if setup went correctly.\n\nAfterwards, go to `go-algorand` and follow the installation instructions there.\n\n## Benchmarks\n\nTo generate a workload on the machine, run `go-algorand/ledger/bench.sh` (from its directory). Note that this might take a while.\n\nAfter a workload is generated, you can run benchmarks.\n\nTo run validator benchmarks with varying numbers of cores (you'll need `numactl`), run `go-algorand/ledger/cores.sh` (from its directory).\n\nTo run the archive benchmark, execute `go-algorand/ledger/acores.sh` (from its directory).\n", "release_dates": []}, {"name": "algorand-sdk-testing", "description": "Testing framework for Algorand SDKs", "language": "Gherkin", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# algorand-sdk-testing\n\nTesting files for Algorand SDKs\n\n## About\n\nThe files in this repository are used for testing the different Algorand SDK implementations. By writing the tests once and sharing them amongst the SDKs we are able to increase the coverage of our tests, and avoid rewriting similar tests over and over again. In addition to test cases, we have a standard test environment which is managed by docker.\n\nTo define tests we use [cucumber](https://cucumber.io/), and feature files written with [gherkin syntax](https://cucumber.io/docs/gherkin/reference/). Each SDK is responsible for finding a framework which can use these files. There are implementations for [many popular programming languages](https://cucumber.io/docs/installation/).\n\nWe have different feature files for unit and integration tests. The unit tests should be run as a normal part of development to quickly identify bugs and regressions. Integration tests, on the other hand, take much longer to run and require a special test environment. The test environment is made up of multiple services and managed with [docker compose](https://docs.docker.com/compose/).\n\n## Test Descriptions\n\n### Unit Tests\n\nThese reside in the [unit features directory](features/unit)\n\n| tag                                  | description                                                                                                 |\n|--------------------------------------|-------------------------------------------------------------------------------------------------------------|\n| @unit                                | Select all unit tests.                                                                                      |\n| @unit.abijson                        | ABI types and method encoding/decoding unit tests.                                                          |\n| @unit.algod                          | Algod REST API unit tests.                                                                                  |\n| @unit.blocksummary                   | Algod / Indexer block REST API unit tests.                                                                  |\n| @unit.applications                   | Application endpoints added to Algod and Indexer.                                                           |\n| @unit.applications.boxes             | Box endpoints for Algod and Indexer.                                                                        |\n| @unit.atomic_transaction_composer    | ABI / atomic transaction construction unit tests.                                                           |\n| @unit.atc_method_args                | Test that algod's Atomic Transaction Composer assserts that the same number of arguments given as expected. |\n| @unit.client-no-headers              | Tests that clients don't add non-standard http headers when no (or an empty) auth token is provided.        |\n| @unit.dryrun                         | Dryrun endpoint added to Algod.                                                                             |\n| @unit.dryrun.trace.application       | DryrunResult formatting tests.                                                                              |\n| @unit.feetest                        | Fee transaction encoding tests.                                                                             |\n| @unit.indexer                        | Indexer REST API unit tests.                                                                                |\n| @unit.indexer.logs                   | Application logs endpoints added to Indexer.                                                                |\n| @unit.indexer.rekey                  | Rekey endpoints added to Algod and Indexer.                                                                 |\n| @unit.offline                        | Offline account operations.                                                                                 |\n| @unit.ready                          | Test the ready endpoint.                                                                                    |\n| @unit.rekey                          | Rekey Transaction golden tests.                                                                             |\n| @unit.responses                      | REST Client Response serialization tests.                                                                   |\n| @unit.responses.genesis              | REST Client Unit Tests for GetGenesis endpoint.                                                             |\n| @unit.responses.messagepack          | REST Client MessagePack Unit Tests.                                                                         |\n| @unit.responses.messagepack.231      | REST Client MessagePack Unit Tests for Indexer 2.3.1+.                                                      |\n| @unit.responses.participationupdates | REST Client Response serialization test for ParticipationUpdates.                                           |\n| @unit.responses.blocksummary         | REST Client updates for indexer/algod block endpoints.                                                      |\n| @unit.responses.statedelta           | REST Client updates for algod statedelta endpoint.                                                          |\n| @unit.responses.sync                 | REST Client updates for algod sync round endpoints.                                                         |\n| @unit.responses.timestamp            | REST Client updates for timestamp offset endpoint.                                                          |\n| @unit.responses.txngroupdeltas       | REST Client updates for algod transaction group deltas endpoints.                                           |\n| @unit.sourcemap                      | Test the sourcemap endpoint.                                                                                |\n| @unit.statedelta                     | Test the statedelta endpoint.                                                                               |\n| @unit.stateproof.responses           | REST Client Response Tests for State Proof.                                                                 |\n| @unit.stateproof.responses.msgp      | REST Client MessagePack Tests for State Proofs.                                                             |\n| @unit.stateproof.paths               | REST Client Unit Tests for State Proof feature.                                                             |\n| @unit.sync                           | Test the follower sync endpoints.                                                                           |\n| @unit.tealsign                       | Test TEAL signature utilities.                                                                              |\n| @unit.timestamp                      | Test the devmode timestamp offset endpoint.                                                                 |\n| @unit.transactions                   | Transaction encoding tests.                                                                                 |\n| @unit.transactions.keyreg            | Keyreg encoding tests.                                                                                      |\n| @unit.transactions.payment           | Payment encoding tests.                                                                                     |\n| @unit.txngroupdeltas                 | Algod transaction group deltas endpoints.                                                                   |\n\n### Integration Tests\n\nThese reside in the [integration features directory](features/integration)\n\n| tag                    | description                                                                            |\n| ---------------------- | -------------------------------------------------------------------------------------- |\n| @abi                   | Test the Application Binary Interface (ABI) with atomic txn composition and execution. |\n| @algod                 | General tests against algod REST endpoints.                                            |\n| @applications.boxes    | Test application boxes and box references functionality.                               |\n| @applications.verified | Submit all types of application transactions and verify account state.                 |\n| @assets                | Submit all types of asset transactions.                                                |\n| @auction               | Encode and decode bids for an auction.                                                 |\n| @c2c                   | Test Contract to Contract invocations and ingestion.                                   |\n| @compile               | Test the algod compile endpoint.                                                       |\n| @compile.sourcemap     | Test that the algod compile endpoint returns a valid Source Map.                       |\n| @dryrun                | Test the algod dryrun endpoint.                                                        |\n| @dryrun.testing        | Test the testing harness that relies on dryrun endpoint. Python only.                  |\n| @kmd                   | Test the kmd REST endpoints.                                                           |\n| @rekey_v1              | Test rekeying transactions.                                                            |\n| @send                  | Test the ability to submit transactions to algod.                                      |\n| @simulate              | Test the ability to simulate transactions with algod.                                  |\n\n### Test Implementation Status\n\n#### Almost all the tags above are implemented by all 4 of our official SDK's\n\nHowever, a few are not fully supported:\n\n| tag                             | SDK's which implement        |\n| ------------------------------- |------------------------------|\n| @dryrun.testing                 | Python only                  |\n| @unit.c2c                       | missing from Python          |\n| @unit.client-no-headers         | JS only                      |\n| @unit.indexer.rekey             | missing from Python and JS   |\n| @unit.responses.genesis         | missing from Python and Java |\n| @unit.responses.messagepack     | missing from Python          |\n| @unit.responses.messagepack.231 | missing from Python and JS   |\n| @unit.responses.messagepack     | missing from Python and JS   |\n| @unit.transactions.keyreg       | go only                      |\n\n## SDK Overview\n\nFull featured Algorand SDKs have 6 major components. Depending on the compatibility level, certain components may be missing. The components include:\n\n1. REST Clients\n2. Transaction Utilities\n3. Encoding Utilities\n4. Crypto Utilities\n5. Enriched Interaction\n6. dApp Testing and Simulate\n7. SDK Testing\n\n![SDK Overview](docs/SDK%20Components.png)\n\n### REST Client\n\nThe most basic functionality includes the REST clients for communicating with **algod** and **indexer**. These interfaces are defined by OpenAPI specifications:\n\n- kmd v1 (generated at build time at **daemon/kmd/api/swagger.json**)\n- [algod v2](https://github.com/algorand/go-algorand/blob/master/daemon/algod/api/algod.oas2.json)\n- [indexer v2](https://github.com/algorand/indexer/blob/develop/api/indexer.oas2.json)\n\n### Transaction Utilities\n\nOne of the basic features of an Algorand SDK is the ability to construct all types of Algorand transactions. This includes simple transactions [of all types](https://developer.algorand.org/docs/reference/transactions/) and the tooling to configure things like [leases](https://developer.algorand.org/docs/reference/transactions/#common-fields-header-and-type) and [atomic transfers (group transactions)](https://developer.algorand.org/docs/features/atomic_transfers/#group-transactions)\n\n### Encoding Utilities\n\nTo ensure that transactions are compact and can hash consistently, there are some special encoding requirements. The SDKs must provide utilities to work with these encodings. Algorand uses MessagePack as a compact binary-encoded JSON alternative, and fields with default values are excluded from the encoded object. Additionally to ensure consistent hashes, the fields must be alphabetized. Finally, dApp args and return values can be encoded and decoded in accordance with the [ARC-004 ABI-Type Specification](https://arc.algorand.foundation/ARCs/arc-0004#encoding).\n\n### Crypto Utilities\n\nAll things related to crypto to make it easier for developers to work with the blockchain. This includes standards such as ED25519 signing, up through Algorand specific LogicSig and MultiSig utilities. There are also some convenience methods for converting Mnemonics.\n\n### Enriched Interaction\n\nIn certain cases, API's are provided to ease interacting\nwith the Algorand blockchain.\nThis includes [wallet interaction](https://developer.algorand.org/docs/get-details/accounts/create/#wallet-derived-kmd),\nARC-4 dApp interaction via the [Atomic Transaction Composer](https://developer.algorand.org/docs/get-details/atc/?from_query=atomic#template-modal-overlay),\nand [Smart Signature](https://developer.algorand.org/docs/get-details/dapps/smart-contracts/frontend/smartsigs/?from_query=smart%20signatgure#template-modal-overlay) utilities.\n\n### dApp Testing and Simulate\n\nUtilities for testing [Smart Contracts and dApps](https://developer.algorand.org/docs/get-details/dapps/smart-contracts/). This currently includes utilities for using [dry-runs](https://developer.algorand.org/docs/get-details/dapps/smart-contracts/debugging/?from_query=dry#dryrun-rest-endpoint). It also enables interacting with the [simulate REST endpoint](https://developer.algorand.org/docs/rest-apis/algod/?from_query=simulate#post-v2transactionssimulate).\n\n### SDK Testing\n\nThere are besploke unit-tests for each SDK. The details of SDK-specific unit tests are up to the developer's discretion. There are also a large number of cucumber tests stored in this repository which cover various unit-style tests and many integration tests. To assist with working in this environment each SDK must provide tooling to download and install the cucumber files, stand up a Sandbox environment that provides\n**algod** and **indexer** endpoints,\na Dockerfile which configures an environment suitable for building the SDK and running the tests, and 3 `Makefile` targets:\n\n- `make unit`\n- `make integration`\n- `make docker-test`\n\nThe rest of this document relates to details about the Cucumber test.\n\n## How to write tests\n\nTests consist of two components:\n\n- the feature files defined in this repository\n- code snippets that map the text in the feature files to specific functions\n\nThe implementation process will vary by programming language and isn't covered here. Refer to the [Cucumber docs](https://cucumber.io/docs/installation/) for setting up a new SDK.\n\n### Tags\n\nWe use [tags](https://cucumber.io/docs/cucumber/api/#tags), and a simple directory structure to organize our feature files. All cucumber implementations should allow specifying one or more tags to include, or exclude, when running tests.\n\n### Unit tests\n\nAll unit tests should be tagged with `@unit` so that unit tests can be run together during development for quick regression tests. For example, to run unit tests with java a tag filter is provided as follows:\n\n```bash\n~$ mvn test -Dcucumber.filter.tags=\"@unit\"\n```\n\nThis command will vary by cucumber implementation. The specific framework documentation should be referenced for details.\n\n### Adding a new test\n\nWhen testing a new feature, add a new feature file\nor add new scenario to an existing feature file, along with a new tag. For example, the [assets feature file](./features/integration/assets.feature) is tagged with `@assets`. By adding a new tag for each feature we are able to add new tests to this repository without breaking the SDKs.\n\nIn order for this to work, each SDK maintains a whitelist of tags which have been implemented.\n\nIf a new feature file is created, the tag would go at the top of the file. If a new scenario is added the tag would go right above the scenario.\n\nIf possible, please run a formatter on the file modified. There are several, including one built into VSCode Cucumber/Gherkin plugin.\n\n### Implementing tests in the SDK\n\nThe code snippets (or step definitions) live in the SDKs. Each SDK has a script which is able to clone this repository, and copy the tests into the correct locations.\n\nWhen a test fails, the cucumber libraries we use print the code snippets which should be included in the SDK test code. The code snippets are empty functions which should be implemented according to the tests requirements. In many cases some state needs to be modified and stored outside of the functions in order to implement the test. Exactly how this state is managed is up to the developer. Refer to the [cucumber documentation for tips](https://cucumber.io/docs/cucumber/state/) about managing state. There may be better documentation in the specific cucumber language library you're using.\n\n### Running tests\n\nThe SDKs come with a `Makefile` to coordinate running the cucumber test suites. There are 3 main targets:\n\n- **unit**: runs all of the short unit tests.\n- **integration**: runs all integration tests.\n- **harness**: downloads this repo and calls `up.sh` to stand up a sandbox ready for running tests\n- **docker-test**: installs feature file dependencies, starts the test environment, and runs the SDK tests in a docker container.\n\nAt a high level, the **docker-test** target is required to:\n\n1. clone `algorand-sdk-testing`\n2. copy supported feature files from the `features` directory into the SDK\n3. build and start the test environment by calling `./scripts/up.sh` which clones `sandbox` and stands it up\n4. stand up the SDK's Docker container\n5. run all cucumber tests against the `sandbox` containers\n\n### Running tests during development\n\nThis will vary by SDK. By calling **up.sh** the environment is available to the integration tests, and tests can be run locally with an IDE or debugger. This is often significantly faster than waiting for the entire test suite to run.\n\nSome of the tests are stateful and will require restarting the environment before re-running the test.\n\nOnce the test environment is running you can use `make unit` and `make integration` to run tests.\n\n## Integration test environment\n\nAlgorand's [sandbox](https://github.com/algorand/sandbox) is used to manage several containers which work together to provide the test environment. This includes `algod`, `kmd`, `indexer` and a `postgres` database. The services run on specific ports with specific API tokens. Refer to [.env](.env) and to [sandbox'es docker-compose.yml](https://github.com/algorand/sandbox/blob/master/docker-compose.yml) for how these are configured.\n\n![Integration Test Environment](docs/SDK%20Test%20Environment.png)\n\n### Managing the test environment\n\n[up.sh](scripts/up.sh) is used to bring up the test environment. Not surprisingly, [down.sh](scripts/down.sh) brings it all down.\n\nWhen starting the environment, we default to using `go-algorand`'s nightly build. If you're interested in running tests against a specific branch of `go-algorand`, you should set `TYPE=\"source\"` in `.env`\nand set `ALGOD_URL`, and either `ALGOD_BRANCH` or `ALGOD_SHA` appropriately.\n\n`indexer` and even the `sandbox` itself can be configured similarly through `.env`.\n", "release_dates": []}, {"name": "app-service", "description": null, "language": "Java", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# AppService\nApplication Framework\n\n# Build the applicaiton jar\n\n```\nmvn install\n```\n\n# Building and running docker image\n\n```\ndocker build -t app-service-image .\n\ndocker run -p8080:8080 -p8081:8081 -it --rm --name app-service app-service-image \n```\n\n\n\n", "release_dates": []}, {"name": "auction-demo", "description": "An example smart contract NFT auction on Algorand", "language": "Python", "license": null, "readme": "# Algorand Auction Demo\n\nThis demo is an on-chain NFT auction using smart contracts on the Algorand blockchain.\n\n## Usage\n\nThe file `auction/operations.py` provides a set of functions that can be used to create and interact\nwith auctions. See that file for documentation.\n\n## Development Setup\n\nThis repo requires Python 3.6 or higher. We recommend you use a Python virtual environment to install\nthe required dependencies.\n\nSet up venv (one time):\n * `python3 -m venv venv`\n\nActive venv:\n * `. venv/bin/activate` (if your shell is bash/zsh)\n * `. venv/bin/activate.fish` (if your shell is fish)\n\nInstall dependencies:\n* `pip install -r requirements.txt`\n\nRun tests:\n* First, start an instance of [sandbox](https://github.com/algorand/sandbox) (requires Docker): `./sandbox up nightly`\n* `pytest`\n* When finished, the sandbox can be stopped with `./sandbox down`\n\nFormat code:\n* `black .`\n", "release_dates": []}, {"name": "avm-abi", "description": "ARC-4 ABI Reference Implementation", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# avm-abi\n\n[![Go Reference](https://pkg.go.dev/badge/github.com/algorand/avm-abi.svg)](https://pkg.go.dev/github.com/algorand/avm-abi)\n[![codecov](https://codecov.io/gh/algorand/avm-abi/branch/main/graph/badge.svg?token=SCDOA6NAIZ)](https://codecov.io/gh/algorand/avm-abi)\n\nAn implementation of the Algorand [ARC-4](https://arc.algorand.foundation/ARCs/arc-0004) ABI type system.\n\n> The scope of this module is intentionally minimal. If you wish to use a more\n> comprehensive library, consider [Algorand's Go SDK](https://github.com/algorand/go-algorand-sdk),\n> which uses this module internally.\n\n## Installation\n\nThis module can be installed using the `go get` command.\n\n```sh\ngo get github.com/algorand/avm-abi/abi\n```\n", "release_dates": ["2023-01-12T22:01:09Z", "2022-11-29T17:13:27Z", "2022-08-15T20:33:42Z"]}, {"name": "avm-debugger", "description": "Debug Adapter Protocol Debugger for the Algorand Virtual Machine", "language": "JavaScript", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# AVM Debugger\n\n## Summary\n\nThis repo contains an AVM debugger which adheres to the [Debug Adapter Protocol](https://microsoft.github.io/debug-adapter-protocol/).\nThis protocol is used by a variety of clients, and this repo additionally\nimplements a basic client for VS Code.\n\nUnlike traditional debuggers, which typically execute code as it is being\ndebugged, this debugger operates on an execution trace. The trace is created by\nthe [algod simulate API](https://developer.algorand.org/docs/rest-apis/algod/#post-v2transactionssimulate).\nThis debugger is not responsible for compiling programs, assembling transaction groups, or executing\ntransactions/programs. It is only responsible for replaying the execution trace, which must already\nexist.\n\nThis code is based on the [`vscode-mock-debug`](https://github.com/microsoft/vscode-mock-debug) repo.\n\n## Usage\n\nThere are multiple ways to invoke the debug adapter exported by this package.\n\n### CLI\n\nThe debug adapter can be invoked from the command line using the `avm-debug-adapter` command.\n\nIf given no arguments, the debug adapter will use stdin and stdout to process messages.\n\nTo run as a server, use the `--port` option, shown below:\n\n```bash\n$ npm exec avm-debug-adapter -- --port=8080\n>> running as a server, listening on 8080\n```\n\n### Programmatically\n\n```typescript\n// AvmDebugSession is a vscode.DebugAdapter implementation and can be imported\n// directly if you don't want to run it as a server.\nimport { AvmDebugSession } from 'avm-debug-adapter';\n\n// From node, you can create a debug adapter server like so\nimport { Server, nodeFileAccessor } from 'avm-debug-adapter/node';\n\nconst server = new Server({\n  fileAccessor: nodeFileAccessor,\n  port: 8080,\n  onSocketError: (err) => {\n    console.error(err);\n  },\n  onServerError: (err) => {\n    console.error(err);\n  },\n});\n\nconsole.log('Server listening on port ' + server.port());\n\nprocess.on('SIGTERM', () => {\n  server.close();\n});\n```\n\n## Features\n\nSee [FEATURES.md](FEATURES.md) for a list of features this debugger supports.\n\n## Build and Run\n\n1. Clone the repo.\n2. `npm i` to install dependencies.\n3. Open the project folder in VS Code.\n4. From the Run and Debug menu, run the `Extension` configuration to open the AVM Debug extension in another VS Code window.\n5. In the new window, go to its Run and Debug menu to select and launch one of the existing configurations.\n6. You are now in a debugging session of a transaction group. You can step through the transaction group, inspect variables, set breakpoints and more. See [FEATURES.md](FEATURES.md) for more details.\n", "release_dates": ["2024-01-04T21:21:39Z", "2023-12-15T00:00:34Z", "2023-11-27T16:33:28Z", "2023-11-27T16:24:07Z"]}, {"name": "bls_sigs_ref", "description": "Algorand's reference implementation of bls signature scheme", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# BLS signatures draft standard, reference implementations\n\nThis repository contains reference implementations of the\n[BLS signatures draft standard](https://github.com/cfrg/draft-irtf-cfrg-bls-signature).\n\n**Note: this code is WIP**. It has not been audited for security, should\nnot be assumed to be constant-time or otherwise secure, and the details\nmay change at any time as the BLS standard evolves.\n\nThis code started as a fork of [bls12-381_hash](https://github.com/kwantam/bls12-381_hash).\n(That repository also contains an implementation in C.)\n\n## implementation status\n\nPlease see the READMEs in each subdirectory for information on particular\nimplementations. In brief,\n\n- The [Python](python-impl/) and [Rust](rust-impl/)\n  implementations include all functionality currently specified in the\n  standard, plus serialization and deserialization based on\n  [the ZCash spec](https://github.com/zkcrypto/pairing/blob/master/src/bls12_381/README.md).\n\n- The [Sage implementation](sage-impl/) does not implement verification,\n  only hashing, signing, and proof-of-possession generation.\n\n- The Python implementation uses the Python finite field implementation\n  from [Chia's BLS library](https://github.com/chia-network/bls-signatures).\n\n- The Rust implementation is based on the [Rust `pairing_fork` library](https://github.com/algorand/pairing-fork).\n\n# Authors\n\nRiad S. Wahby, Zhenfei Zhang\n\n# License\n\nThis software is (C) 2019 Algorand, Inc.\n\n    Licensed under the MIT license (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n    http://opensource.org/licenses/MIT\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n", "release_dates": ["2019-10-31T18:53:08Z"]}, {"name": "bls_sigs_ref-fork", "description": "unofficial ref impls of the BLS signatures standard", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# BLS signatures draft standard, reference implementations\n\nThis repository contains reference implementations of the\n[BLS signatures draft standard](https://github.com/cfrg/draft-irtf-cfrg-bls-signature).\n\n**Note: this code is WIP**. It has not been audited for security, should\nnot be assumed to be constant-time or otherwise secure, and the details\nmay change at any time as the BLS standard evolves.\n\nThis code started as a fork of [bls12-381_hash](https://github.com/kwantam/bls12-381_hash).\n(That repository also contains an implementation in C.)\n\n## implementation status\n\nPlease see the READMEs in each subdirectory for information on particular\nimplementations. In brief,\n\n- The [Python](python-impl/) and [Rust](rust-impl/)\n  implementations include all functionality currently specified in the\n  standard, plus serialization and deserialization based on\n  [the ZCash spec](https://github.com/zkcrypto/pairing/blob/master/src/bls12_381/README.md).\n\n- The [Sage implementation](sage-impl/) does not implement verification,\n  only hashing, signing, and proof-of-possession generation.\n\n- The Python implementation uses the Python finite field implementation\n  from [Chia's BLS library](https://github.com/chia-network/bls-signatures).\n\n- The Rust implementation is based on the [Rust `pairing_fork` library](https://github.com/algorand/pairing-fork).\n\n# License\n\nThis software is (C) 2019 Riad S. Wahby\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n", "release_dates": []}, {"name": "c-sumhash", "description": null, "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "C-Sumhash\n====================\n\nAlgorand's subset-sum hash function implementation in C.\n\n# Build And Tests\n\n```bash\ngit clone https://github.com/algorand/c-sumhash\nmake\n```\n\nThe ```make``` command builds the library and runs the tests.\nThe output can be found in the build directory:\n```bash\n./build/libsumhash.a\n```\n\n# Usage \n\n```C\n#include <stdio.h>\n#include <string.h>\n\n#include \"include/sumhash512.h\"\n\nint main() {\n    char* input = \"Algorand\";\n    sumhash512_state hash;\n    sumhash512_init(&hash);\n    sumhash512_update(&hash, (uint8_t*)input, strlen(input));\n    uint8_t output [SUMHASH512_DIGEST_SIZE];\n    sumhash512_final(&hash, output);\n\n    return 0;\n}\n```\n\nSimple API usage:\n```C\n#include <stdio.h>\n#include <string.h>\n\n#include \"include/sumhash512.h\"\n\nint main() {\n    char* input = \"Algorand\";\n    uint8_t output [SUMHASH512_DIGEST_SIZE];\n    sumhash512(output, (uint8_t*)input, strlen(input));\n\n    return 0;\n}\n```\n\n\nThe ```include/sumhash512.h``` header contains more information about the functions usage\n\n# Spec\n\nThe specification of the function as well as the security parameters\ncan be found [here](https://github.com/algorand/go-sumhash/tree/master/spec)  \n", "release_dates": []}, {"name": "code-samples", "description": "Code samples tied to tutorials and solutions.", "language": "JavaScript", "license": null, "readme": "# Code Samples\nCode samples for Solutions built on the Algorand blockchain.\n", "release_dates": []}, {"name": "conduit", "description": "Algorand's data pipeline framework.", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "<div style=\"text-align:center\" align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/assets/algorand_logo_mark_white.png\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"docs/assets/algorand_logo_mark_black.png\">\n    <img alt=\"Shows a black Algorand logo light mode and white in dark mode.\" src=\"docs/assets/algorand_logo_mark_black.png\" width=\"200\">\n  </picture>\n\n[![CircleCI](https://img.shields.io/circleci/build/github/algorand/conduit/master?label=master)](https://circleci.com/gh/algorand/conduit/tree/master)\n![Github](https://img.shields.io/github/license/algorand/conduit)\n[![Contribute](https://img.shields.io/badge/contributor-guide-blue?logo=github)](https://github.com/algorand/go-algorand/blob/master/CONTRIBUTING.md)\n</div>\n\n# Algorand Conduit\n\nConduit is a framework for ingesting blocks from the Algorand blockchain into external applications. It is designed as modular plugin system that allows users to configure their own data pipelines for filtering, aggregation, and storage of blockchain data.\n\n<!-- TODO: a cool diagram here that clearly demonstrates data moving through the system -->\n\nFor example, use conduit to:\n* Build a notification system for on chain events.\n* Power a next generation block explorer.\n* Select app specific data and write it to a custom database.\n* Build a custom Indexer for a new [ARC](https://github.com/algorandfoundation/ARCs).\n* Send blockchain data to another streaming data platform for additional processing (e.g. RabbitMQ, Kafka, ZeroMQ).\n* Build an NFT catalog based on different standards.\n\n# System Requirements\n\nFor a simple deployment the following configuration works well:\n* Network: Conduit colocated with Algod follower.\n* Conduit + Algod: 4 CPU and 8 GB of ram.\n  * Storage: algod follower node, 40 GiB, 3000 IOPS minimum.\n  * Deployments allocating less ram might work in conjunction with [GOMEMLIMIT](https://pkg.go.dev/runtime@master#hdr-Environment_Variables) for Algod (and even Conduit). This configuration is not tested, so use with caution and monitor closely.\n\n# Getting Started\n\n## Installation\n\n### Download\n\nThe latest `conduit` binary can be downloaded from the [GitHub releases page](https://github.com/algorand/conduit/releases).\n\n### Docker\n\n[The latest docker image is on docker hub.](https://hub.docker.com/r/algorand/conduit)\n\n### Install from Source\n\n1. Checkout the repo, or download the source, `git clone https://github.com/algorand/conduit.git && cd conduit`\n2. Run `make conduit`.\n3. The binary is created at `cmd/conduit/conduit`.\n\n## Usage\n\nConduit is configured with a YAML file named `conduit.yml`. This file defines the pipeline behavior by enabling and configuring different plugins.\n\n### Create `conduit.yml` configuration file\n\nUse the `conduit init` subcommand to create a configuration template. Place the configuration template in a new data directory. By convention the directory is named `data` and is referred to as the data directory.\n\n```sh\nmkdir data\n./conduit init > data/conduit.yml\n```\n\nA Conduit pipeline is composed of 3 components, [Importers](./conduit/plugins/importers/), [Processors](./conduit/plugins/processors/), and [Exporters](./conduit/plugins/exporters/).\nEvery pipeline must define exactly 1 Importer, exactly 1 Exporter, and can optionally define a series of 0 or more Processors. See a full list of available plugins with `conduit list` or the [plugin documentation page](./conduit/plugins).\n\nHere is an example `conduit.yml` that configures two plugins:\n\n```yaml\nimporter:\n    name: algod\n    config:\n        mode: \"follower\"\n        netaddr: \"http://your-follower-node:1234\"\n        token: \"your API token\"\n\n# no processors defined for this configuration\nprocessors:\n\nexporter:\n    name: file_writer\n    config:\n        # the default config writes block data to the data directory.\n```\n\nThe `conduit init` command can also be used to select which plugins to include in the template. The example below uses the standard algod importer and sends the data to PostgreSQL. This example does not use any processor plugins.\n```sh\n./conduit init --importer algod --exporter postgresql > data/conduit.yml\n```\n\nBefore running Conduit you need to review and modify `conduit.yml` according to your environment.\n\n### Run Conduit\n\nOnce configured, start Conduit with your data directory as an argument:\n```sh\n./conduit -d data\n```\n\n### Full Tutorials\n\n* [Writing Blocks to Files Using Conduit](./docs/tutorials/WritingBlocksToFile.md)\n* [Using Conduit to Populate an Indexer Database](./docs/tutorials/IndexerWriter.md)\n\n# External Plugins\n\nConduit supports external plugins which can be developed by anyone.\n\nFor a list of available plugins and instructions on how to use them, see the [External Plugins](./docs/ExternalPlugins.md) page.\n\n## External Plugin Development\n\nSee the [Plugin Development](./docs/PluginDevelopment.md) page for building a plugin.\n\n# Contributing\n\nContributions are welcome! Please refer to our [CONTRIBUTING](https://github.com/algorand/go-algorand/blob/master/CONTRIBUTING.md) document for general contribution guidelines.\n\n# Migrating from Indexer 2.x\n\nConduit can be used to populate data from an existing [Indexer 2.x](https://github.com/algorand/indexer/) deployment as part of upgrading to Indexer 3.x. The v3 API is 100% backwards compatible with the v2 API.\n\nWe will continue to maintain Indexer 2.x up through November 1. From that point onward, subsequent consensus upgrades will only be compatible with Indexer 3.x when paired with Conduit.\n\nTo migrate, follow the [Using Conduit to Populate an Indexer Database](./docs/tutorials/IndexerWriter.md) tutorial. When you get to the step about setting up postgres, substitute your existing database connection string. Conduit will read the database to initialize the next round.\n", "release_dates": ["2024-01-03T16:21:28Z", "2023-12-20T16:20:09Z", "2023-09-18T17:26:44Z", "2023-09-01T11:20:14Z", "2023-07-21T18:03:11Z", "2023-06-27T18:48:02Z", "2023-04-20T18:08:55Z", "2023-03-22T15:58:05Z"]}, {"name": "conduit-cockroachdb", "description": "Experimental CockroachDB exporter for Conduit.", "language": "Go", "license": null, "readme": "# CockroachDB Exporter\nA CockroachDB exporter plugin for Conduit.\n\n# Quickstart\n```yaml\nmake demo\n```\n\n#### Run Indexer API \n```yaml\nmake indexerapi\n```\n", "release_dates": []}, {"name": "conduit-plugin-template", "description": "Get started with Algorand streaming APIs with Conduit plugin templates project.", "language": "Go", "license": null, "readme": "# Conduit Plugin Templates\n\nThis repository is designed for someone who understands how [Conduit](https://github.com/algorand/conduit)\nworks and would like to extend its functionality by building a plugin.\n\nPlugin templates are implemented in the `plugin` package. There you'll find\n`importer`, `processor` and `exporter` templates. These have no functionality\nand should be modified.\n\n## Overview\n\nThis template project provides you with everything to get started with building\na Conduit plugin. This includes:\n* Boilerplate implementations of each plugin.\n* Main function that registers them in a binary.\n* Scripts to get the stack up and running with a node.\n* Release process to distribute your plugin.\n\n## How to use this repo\n\nBefore anything else, you must decide which type of plugin you should use to\nimplement your idea. An `exporter` is good for sending data to a an external\nresource, such as a database or application. `processor` plugins are useful\nfor applying rules that filter transactions.\n\n### Code\n\nOpen the project root in your preferred Go IDE and navigate to the `plugin` package.\nHere you'll find templates for the different types of plugins. They have no functionality,\nbut you'll find some `TODO` comments to identify things you should modify. In particular\nyou should add configuration parameters and implement the init and data pipeline\nfunctions.\n\nThe different life cycle phases are implemented as different functions on the\ninterface. For more details on this see the [Development](https://github.com/algorand/conduit/blob/master/docs/Development.md) documentation.\n\n### Build\n\nA main function is provided at `cmd/conduit/main.go`. It's configured to load\nthe plugin templates along with the built-in plugins located in the Conduit\nrepository.\n\nBuild with: `make conduit`\n\nThis places the `conduit` binary at the project root.\n\nVerify that your plugin(s) are listed in the resulting binary: `./conduit list`\n\n### Test\n\nJust like the released version of Conduit, a `config.yml` file is required and\nmust be configured. An Algorand node is also required as a data source.\n\nFor convenience, consider running your node with docker. It provides methods\nto configure the network you'll connect to, setup API tokens, and automatically\nconfigure the node for Conduit. Modify the following command to suite your\nneeds. Note that you can leave out the `NETWORK` option to start a private\nnetwork:\n```\ndocker run -d -p 4190:8080 --name conduit-template-follower \\\n  -e ADMIN_TOKEN=aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \\\n  -e TOKEN=aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \\\n  -e PROFILE=conduit \\\n  -e NETWORK=testnet \\\n  algorand/algod:stable\n```\n\nIn the command here the processor and exporter templates are enabled. You may\nchoose to enable just one, or neither if you've renamed the plugin to be unique\nfor your project:\n```\n./conduit init --importer algod --processors processor_template --exporter exporter_template -d conduit_data\n```\n\nFill in the algod address and tokens, this uses the algod config from the\ndocker command above:\n```\nsed -i \\\n  -e \"s,mode: OFF,mode: ON,\n  s,netaddr: \\\"http:\\/\\/url:port\\\",netaddr: \\\"localhost:4190\\\",\n  s, token: \\\"\\\", token: \\\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \\\",\n  s,admin-token: \\\"\\\",admin-token: \\\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \\\",\" \\\n  conduit_data/conduit.yml\n```\n\nInspect `conduit.yml` for correctness. If you've added settings for your plugin\nfill them in now.\n\nAt this point, start conduit: `./conduit -d conduit_data`\n\nIf you make changes to your plugin, be sure to run `make conduit` before\nrestarting the test.\n\nConduit is also able to use fast catchup to start at a specific round. For some\nnetworks, like mainnet, this may take 30 minutes or more to initialize.  To\nstart conduit on round 28000000, use the command:\n`./conduit -d conduit_data -r 28000000`\n\n### Release\n\nThis release process has limited support. Please create an issue here or in the\nConduit repo if you experience problems with it.\n\nGoreleaser can be configured with `make release`. It is setup to cross compile\nfor multiple platforms, in addition to creating multi-architecture Docker\nimages.\n\nThere are some additional dependencies to get everything to work:\n* [goreleaser](https://goreleaser.com/install/)\n* Docker\n* QEMU (for docker multi-arch builds)\n* DockerHub credentials / \"docker login\"\n* Github API Token\n", "release_dates": []}, {"name": "Derivhack-Demo", "description": null, "language": null, "license": null, "readme": null, "release_dates": []}, {"name": "DerivhackExamples", "description": "Examples for the 2019 Derivhack Hackathon", "language": "Java", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Introduction\n\nThe goal of the ISDA Common Domain Model (CDM)  is to allow financial institutions to have a coherent representation of financial instruments and events. This document shows how institutions can use the CDM and the Algorand blockchain to maintain separately owned but coherent financial databases with the following properties:\n\n1. **Coherency**: All institutions participating in a trade agree on the digital representation of that trade at any point in time.\n\n2. **Privacy**: The details of the trade are only revealed to the institutions which participate in it. Any other agent cannot learn anything about the trade.\n\n3. **Lineage**: Any modification in the state of a trade can refer to the previous state, generating a traceable lineage for the history of that trade.\n\n4. **Ease of Use**: Because the Algorand blockchain is a permissionless blockchain, institutions can interact with it using the software of their choice, and without the need to set up their own distributed system. Algorand provides easy to use APIs that read to and write from the blockchain, and SDKs in [Python](https://developer.algorand.org/docs/python-sdk), [Go](https://developer.algorand.org/docs/go-sdk), [Java](https://developer.algorand.org/docs/java-sdk) and [Javascript](https://developer.algorand.org/docs/javascript-sdk). \n\n\n## The Algorand Blockchain\n\nThe Algorand blockchain can process 1000 transactions per second with a latency of less than 5 seconds and ensures transaction finality with point-of-sale speed.\n The Algorand blockchain is a permissionless blockchain with hundreds of independently operating nodes distributed around the world. The Algorand blockchain allows developers to create their applications without having to set up their own distributed systems. \nIn addition, Algorand provides extensive [documentation](https://developer.algorand.org/docs/getting-started), and provides SDKs in four languages (Go, Python, Java and Javascript) to interact with the blockchain. \n\n![Figure 1: Nodes running the Algorand client software around the world](https://github.com/algorand/DerivhackExamples/blob/master/blob/nodes_map.png)\n*Figure 1: Nodes running the Algorand client software around the world*\n\n\n# Installing, Compiling and Running the Code\n\n## Dependencies\n\nRunning the code in this repository requires that you have\n\n1. A Unix-based OS such as Mac OS X or Linux\n2. Java\n3. Maven\n\n## Java and Maven Installation\n### OS X\n\nThese are bash scripts which install Java and Maven and set the correct paths to use them. These scripts are in the `INSTALL` folder and should be run in the following order\n\n1. `install_brew.sh` if the user does not have Hombrew installed (OS X utility to install programs)\n2. `install_java.sh` if the user does not have Java installed. This installs the OpenJDK \n3. `install_maven.sh` if the user does not have Maven installed\n4. `install_mongo.sh` if the user does not have MongoDB installed\n\n\n## Ubuntu\nThese are bash scripts which install Java and Maven and set the correct paths to use them. These scripts are in the `INSTALL` folder and should be run in the following order\n\n1. `install_java_for_ubuntu.sh` if the user does not have Java installed. This installs the OpenJDK \n2. `install_maven_for_ubuntu.sh` if the user does not have Maven installed\n3. `install_mongo_for_ubuntu.sh` if the user does not have MongoDB installed\n\n\n## Java library Installation\n\nThe main directory contains a pom.xml file which Maven uses to download Java libraries that the code depends on, including the Algorand Java SDK, and the Java implementation of the ISDA CDM.\n\nThe code has been tested on a computer running OS X  Version 10.14.5, OpenJDK 13, and Maven version 3.6.1. and on an AWS instance (\"4.15.0-1044-aws\") running Ubuntu 18.04.2 LTS, OpenJDK 11 and Maven version 3.6.0\n\n##  Compilation\nA `settings.xml` file is provided in the project root directory, use it install dependencies as below: \n```bash\nmvn -s settings.xml clean install\n\n```\n\nYou can also run \n```bash\nsh compile.sh\n```\nfrom the root directory.\n\n\n\n\n## Running the Code\nTo run the example code, type \n```bash\nsh run.sh \n```\nin the root directory.\n\nThis script will start a MongoDB service and run the examples for the first three use cases in the hackathon. Ubuntu users need to uncomment the following line to run the mongo service on ubuntu.\n```bash\n##UNCOMMENT THIS LINE FOR UBUNBTU                                                                                                 \n# bash start_mongo_on_ubuntu.sh \n```\n\n\n\n### (OPTIONAL): Starting and Stopping MongoDB\n\nThe code needs to have a Mongo DB service running to persist some information.\nRight now the ```run.sh``` script starts this service automatically if it is not running.\nHowever, we have provided scripts  to start and stop this automatically \n\nTo run the mongodb service, run\n```bash\nsh start_mongo.sh\n```\n\nTo stop the mongodb service, run\n\n```bash\nsh stop_mongo.sh\n``` \n\n# Example Use Cases\n## Execution\nIn the Derivhack Hackathon, users  are given a [trade execution file](https://github.com/algorand/DerivhackExamples/blob/master/Files/UC1_block_execute_BT1.json) and need to \n\n1. Load the JSON file into their system\n2. Create users in their distributed ledger corresponding to the parties in the execution\n3. Create a report of the execution\n\nIn this example, we use the Algorand blockchain to ensure different parties have consistent versions of the file, while keeping their datastores private.  The information stored in the chain includes the global key of the execution, its lineage, and the file path where the user stored the Execution JSON object in their private data store. \n\nThe following function, from the class ```CommitExecution.java``` reads a CDM Event, creates Algorand accounts for all parties in the event. It gets the executing party (Client 1's broker), and has this party send details of the execution to all other parties on the Algorand blockchain.\n\n```java\n public  class CommitExecution {\n\n    public static void main(String [] args) throws Exception{\n        \n        //Read the input arguments and read them into files\n        String fileName = args[0];\n        String fileContents = ReadAndWrite.readFile(fileName);\n\n         //Read the event file into a CDM object using the Rosetta object mapper\n        ObjectMapper rosettaObjectMapper = RosettaObjectMapper.getDefaultRosettaObjectMapper();\n        Event event = rosettaObjectMapper\n                .readValue(fileContents, Event.class);\n        \n        //Create Algorand Accounts for all parties\n        // and persist accounts to filesystem/database\n        List<Party> parties = event.getParty();\n        User user;\n        DB mongoDB = MongoUtils.getDatabase(\"users\");\n        parties.parallelStream()\n                .map(party -> User.getOrCreateUser(party,mongoDB))\n                .collect(Collectors.toList());\n\n        //Get the execution\n        Execution execution = event\n                                .getPrimitive()\n                                .getExecution().get(0)\n                                .getAfter()\n                                .getExecution();\n\n\n        // Get the executing party  reference\n        String executingPartyReference = execution.getPartyRole()\n                .stream()\n                .filter(r -> r.getRole() == PartyRoleEnum.EXECUTING_ENTITY)\n                .map(r -> r.getPartyReference().getGlobalReference())\n                .collect(MoreCollectors.onlyElement());\n\n        // Get the executing party\n        Party executingParty = event.getParty().stream()\n                .filter(p -> executingPartyReference.equals(p.getMeta().getGlobalKey()))\n                .collect(MoreCollectors.onlyElement());\n\n        // Get all other parties\n        List<Party> otherParties =  event.getParty().stream()\n                .filter(p -> !executingPartyReference.equals(p.getMeta().getGlobalKey()))\n                .collect(Collectors.toList());\n\n        // Find or create the executing user\n        User executingUser = User.getOrCreateUser(executingParty, mongoDB);\n       \n        //Send all other parties the contents of the event as a set of blockchain transactions\n        List<User> users = otherParties.\n                            parallelStream()\n                            .map(p -> User.getOrCreateUser(p,mongoDB))\n                            .collect(Collectors.toList());\n\n        List<Transaction> transactions = users\n                                            .parallelStream()\n                                            .map(u->executingUser.sendEventTransaction(u,event,\"execution\"))\n                                            .collect(Collectors.toList());\n        \n    }\n}\n```\n\nThe corresponding shell command to execute this function with the Block trades file is \n```bash\n##Commit the execution file to the blockchain\nmvn -s settings.xml exec:java -Dexec.mainClass=\"com.algorand.demo.CommitExecution\" \\\n -Dexec.args=\"./Files/UC1_block_execute_BT1.json\" -e -q\n```\n\n## Allocation\nThe second use case for Derivhack is allocation of trades. That is, the block trade execution given in use case 1 will be allocated among multiple accounts. Participants are also given a JSON CDM file specifying the [allocation] (https://github.com/algorand/DerivhackExamples/blob/master/Files/UC2_allocation_execution_AT1.json). Since allocations are CDM events, the same logic applies as in the Execution use case. To commit the allocation event to the blockchain, participants can use the following shell command\n\n```bash\nmvn -s settings.xml exec:java -Dexec.mainClass=\"com.algorand.demo.CommitAllocation\" \\\n -Dexec.args=\"./Files/UC2_allocation_execution_AT1.json\" -e -q\n```\n\n\n## Affirmation\nThe third use case is the affirmation of the trade by the clients. In contrast with the other cases, the Participants can look at the classes ```CommitAffirmation.java``` (https://github.com/algorand/DerivhackExamples/blob/master/src/main/java/com/algorand/demo/CommitAffirmation.java) and ```AffirmImpl.java``` (https://github.com/algorand/DerivhackExamples/blob/master/src/main/java/com/algorand/demo/AffirmationImpl.java) for examples on how to derive the Affirmation of a trade from its allocation.\n\nIn the affirmation step, the client produces a CDM affirmation from the Allocation Event,\nand sends the affirmation to the broker over the Algorand Chain.\n\n```java\n\n``` class CommitAffirmation {\npublic static void main(String[] args){\n\n        //Load the database to lookup users\n        DB mongoDB = MongoUtils.getDatabase(\"users\");\n\n        //Load a file with client global keys\n        String allocationFile = args[0];\n        String allocationCDM = ReadAndWrite.readFile(allocationFile);\n        ObjectMapper rosettaObjectMapper = RosettaObjectMapper.getDefaultRosettaObjectMapper();\n        Event allocationEvent = null;\n            try{\n                allocationEvent = rosettaObjectMapper\n                                    .readValue(allocationCDM, Event.class);\n            }\n            catch(java.io.IOException e){\n                e.printStackTrace();\n            }\n                \n       \n        List<Trade> allocatedTrades = allocationEvent.getPrimitive().getAllocation().get(0).getAfter().getAllocatedTrade();\n        //Keep track of the trade index\n        int tradeIndex = 0;\n\n        //Collect the affirmation transaction id and broker key in a file\n        String result = \"\";\n        //For each trade...\n        for(Trade trade: allocatedTrades){\n\n        //Get the broker that we need to send the affirmation to\n        String brokerReference = trade.getExecution().getPartyRole()\n            .stream()\n            .filter(r -> r.getRole() == PartyRoleEnum.EXECUTING_ENTITY)\n            .map(r -> r.getPartyReference().getGlobalReference())\n            .collect(MoreCollectors.onlyElement());\n\n            User broker = User.getUser(brokerReference,mongoDB);\n\n        //Get the client reference for that trade\n        String clientReference = trade.getExecution()\n                                        .getPartyRole()\n                                        .stream()\n                                        .filter(r-> r.getRole()==PartyRoleEnum.CLIENT)\n                                        .map(r->r.getPartyReference().getGlobalReference())\n                                        .collect(MoreCollectors.onlyElement());\n                \n        // Load the client user, with algorand passphrase\n        User user = User.getUser(clientReference,mongoDB);\n        String algorandPassphrase = user.algorandPassphrase;\n\n        // Confirm the user has received the global key of the allocation from the broker\n        String receivedKey = AlgorandUtils.readEventTransaction( algorandPassphrase, allocationEvent.getMeta().getGlobalKey());\n        assert receivedKey == allocationEvent.getMeta().getGlobalKey() : \"Have not received allocation event from broker\";\n            //Compute the affirmation\n            Affirmation affirmation = new AffirmImpl().doEvaluate(allocationEvent,tradeIndex).build();\n                    \n             //Send the affirmation to the broker\n            Transaction transaction = \n                        user.sendAffirmationTransaction(broker, affirmation);\n                    \n            result += transaction.getTx() + \",\" + brokerReference +\"\\n\";\n                    \n                \n            tradeIndex = tradeIndex + 1;\n        }\n        try{\n           ReadAndWrite.writeFile(\"./Files/AffirmationOutputs.txt\", result);\n        }\n        catch(Exception e){\n            e.printStackTrace();\n        }\n    }\n\n}\n```\n\n", "release_dates": []}, {"name": "dilithium", "description": null, "language": "C", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Dilithium\n\n[![Build Status](https://travis-ci.org/pq-crystals/dilithium.svg?branch=master)](https://travis-ci.org/pq-crystals/dilithium) [![Coverage Status](https://coveralls.io/repos/github/pq-crystals/dilithium/badge.svg?branch=master)](https://coveralls.io/github/pq-crystals/dilithium?branch=master)\n\nThis repository contains the official reference implementation of the [Dilithium](https://www.pq-crystals.org/dilithium/) signature scheme, and an optimized implementation for x86 CPUs supporting the AVX2 instruction set. Dilithium is a [finalist](https://csrc.nist.gov/Projects/post-quantum-cryptography/round-3-submissions) in the [NIST PQC](https://csrc.nist.gov/projects/post-quantum-cryptography) standardization project.\n\n## Build instructions\n\nThe implementations contain several test and benchmarking programs and a Makefile to facilitate compilation.\n\n### Prerequisites\n\nSome of the test programs require [OpenSSL](https://openssl.org). If the OpenSSL header files and/or shared libraries do not lie in one of the standard locations on your system, it is necessary to specify their location via compiler and linker flags in the environment variables `CFLAGS`, `NISTFLAGS`, and `LDFLAGS`.\n\nFor example, on macOS you can install OpenSSL via [Homebrew](https://brew.sh) by running\n```sh\nbrew install openssl\n```\nThen, run\n```sh\nexport CFLAGS=\"-I/usr/local/opt/openssl@1.1/include\"\nexport NISTFLAGS=\"-I/usr/local/opt/openssl@1.1/include\"\nexport LDFLAGS=\"-L/usr/local/opt/openssl@1.1/lib\"\n```\nbefore compilation to add the OpenSSL header and library locations to the respective search paths.\n\n### Test programs\n\nTo compile the test programs on Linux or macOS, go to the `ref/` or `avx2/` directory and run\n```sh\nmake\n```\nThis produces the executables\n```sh\ntest/test_dilithium$ALG\ntest/test_vectors$ALG\nPQCgenKAT_sign$ALG\n```\nwhere `$ALG` ranges over the parameter sets 2, 3, 5, 2aes, 3aes, and 5aes.\n\n* `test_dilithium$ALG` tests 10000 times to generate keys, sign a random message of 59 bytes and verify the produced signature. Also, the program will try to verify wrong signatures where a single random byte of a valid signature was randomly distorted. The program will abort with an error message and return -1 if there was an error. Otherwise it will output the key and signature sizes and return 0.\n* `test_vectors$ALG` performs further tests of internal functions and prints deterministically generated test vectors for several intermediate values that occur in the Dilithium algorithms. Namely, a 48 byte seed, the matrix A corresponding to the first 32 bytes of seed, a short secret vector s corresponding to the first 32 bytes of seed and nonce 0, a masking vector y corresponding to the seed and nonce 0, the high bits w1 and the low bits w0 of the vector w = Ay, the power-of-two rounding t1 of w and the corresponding low part t0, and the challenge c for the seed and w1. This program is meant to help to ensure compatibility of independent implementations.\n* `PQCgenKAT_sign$ALG` is the Known Answer Test (KAT) generation program provided by NIST. It computes the official KATs and writes them to the files `PQCsignKAT_$(CRYPTO_ALGNAME).{req,rsp}`.\n\n### Benchmarking programs\n\nFor benchmarking the implementations, we provide speed test programs for x86 CPUs that use the Time Step Counter (TSC) or the actual cycle counter provided by the Performance Measurement Counters (PMC) to measure performance. To compile the programs run\n```sh\nmake speed\n```\nThis produces the executables\n```sh\ntest/test_speed$ALG\n```\nfor all parameter sets `$ALG` as above. The programs report the median and average cycle counts of 10000 executions of various internal functions and the API functions for key generation, signing and verification. By default the Time Step Counter is used. If instead you want to obtain the actual cycle counts from the Performance Measurement Counters export `CFLAGS=\"-DUSE_RDPMC\"` before compilation.\n\nPlease note that the reference implementation in `ref/` is not optimized for any platform, and, since it prioritises clean code, is significantly slower than a trivially optimized but still platform-independent implementation. Hence benchmarking the reference code does not provide representative results.\n\nOur Dilithium implementations are contained in the [SUPERCOP](https://bench.cr.yp.to) benchmarking framework. See [here](http://bench.cr.yp.to/results-sign.html#amd64-kizomba) for current cycle counts on an Intel KabyLake CPU.\n\n## Randomized signing\n\nBy default our code implements Dilithium's deterministic signing mode. To change this to the randomized signing mode, define the `DILITHIUM_RANDOMIZED_SIGNING` preprocessor macro at compilation by either uncommenting the line\n```sh\n//#define DILITHIUM_RANDOMIZED_SIGNING\n```\nin config.h, or adding `-DDILITHIUM_RANDOMIZED_SIGNING` to the compiler flags in the environment variable `CFLAGS`.\n\n## Shared libraries\n\nAll implementations can be compiled into shared libraries by running\n```sh\nmake shared\n```\nFor example in the directory `ref/` of the reference implementation, this produces the libraries\n```sh\nlibpqcrystals_dilithium$ALG_ref.so\n```\nfor all parameter sets `$ALG`, and the required symmetric crypto libraries\n```\nlibpqcrystals_aes256ctr_ref.so\nlibpqcrystals_fips202_ref.so\n```\nAll global symbols in the libraries lie in the namespaces `pqcrystals_dilithium$ALG_ref`, `libpqcrystals_aes256ctr_ref` and `libpqcrystals_fips202_ref`. Hence it is possible to link a program against all libraries simultaneously and obtain access to all implementations for all parameter sets. The corresponding API header file is `ref/api.h`, which contains prototypes for all API functions and preprocessor defines for the key and signature lengths.\n\n## CMake\n\nAlso available is a portable [cmake](https://cmake.org) based build system that permits building the reference implementation.\n\nBy calling \n```\nmkdir build && cd build && cmake .. && cmake --build . && ctest\n```\n\nthe Dilithium reference implementation gets built and tested.\n", "release_dates": []}, {"name": "docs", "description": null, "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Algorand Developer Documentation (Official)\n\nThis is the official repository for the [Algorand Developer Documentation](https://developer.algorand.org/docs/) hosted on the [Algorand Developer Portal](https://developer.algorand.org/).\n\n## Contributing\nLearn how you can contribute in our [Guide to Contributing](./CONTRIBUTING.md).\n\n## License\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nThe documentation is licensed under an MIT license. See [LICENSE file](./LICENSE.md) for details.", "release_dates": []}, {"name": "falcon", "description": null, "language": "C", "license": null, "readme": "DETERMINISTIC FALCON IMPLEMENTATION\n===================================\n\nVersion: 2021-12-03\n\nFalcon is a post-quantum signature algorithm, submitted to NIST's\nPost-Quantum Cryptography project:\n\n   https://csrc.nist.gov/Projects/Post-Quantum-Cryptography\n\nFalcon is based on NTRU lattices, used with a hash-and-sign structure\nand a Fourier-based sampling method that allows efficient signature\ngeneration and verification, while producing and using relatively\ncompact signatures and public keys. The official Falcon Web site is:\n\n   https://falcon-sign.info/\n\nThis implementation slightly extends the official Falcon code to\nsupport a fully deterministic (or \"derandomized\") signing mode; the\ninterface is given in deterministic.h. (This is an alternative to the\nrandomized-hashing mode enabled by the original implementation.) For\nthe motivation for, and specification of, the deterministic mode, see\nfalcon-det.pdf .\n\nThis implementation is written in C and is configurable at compile\ntime through macros which are documented in config.h; each macro is a\nboolean option and can be enabled or disabled in config.h and/or as a\ncommand-line parameter to the compiler. Several implementation\nstrategies are available; however, in all cases, the same API is\nimplemented.\n\n*** CRITICAL SECURITY WARNING ***\n\nFor robust determinism across supported devices, which is needed to\nprevent a potential catastrophic security failure in the deterministic\nmode, it is STRONGLY RECOMMENDED that the following macro settings be\nused, as is done in config.h (see that file for further details):\n\n  - floating-point emulation (FALCON_FPEMU) should be enabled, in lieu\n    of native FP operations.\n\n  - \"fused multiply-add\" (FALCON_FMA) should be disabled, *especially*\n    if native FP operations are enabled.\n\n  - other optimizations like FALCON_AVX2 and FALCON_ASM_CORTEXM4\n    should be disabled as a cautionary measure, unless they are needed\n    for performance and can be thoroughly checked to not affect\n    determinism on the relevant signing devices.\n\n(According to the documentation below, FALCOM_FMA and FALCON_AVX2 have\nno effect when FALCON_FPEMU is enabled, but in config.h they are\nexplicitly disabled as a defensive measure.)\n\n*** END CRITICAL SECURITY WARNING ***\n\nMain options are the following:\n\n  - FALCON_FPNATIVE and FALCON_FPEMU\n\n    If using FALCON_FPNATIVE, then the C 'double' type is used for all\n    floating-point operations. This is the default. This requires the\n    'double' type to implement IEEE-754 semantics, in particular\n    rounding to the exact precision of the 'binary64' type (i.e. \"53\n    bits\"). The Falcon implementation takes special steps to ensure\n    these properties on most common architectures. When using this\n    engine, the code _may_ need to call the standard library function\n    sqrt() (depending on the local architecture), which may in turn\n    require linking with a specific library (e.g. adding '-lm' to the\n    link command on Unix-like systems).\n\n    FALCON_FPEMU does not use the C 'double' type, but instead works\n    over only 64-bit integers and embeds its own emulation of IEEE-754\n    operations. This is slower but portable, since it will work on any\n    machine with a C99-compliant compiler.\n\n  - FALCON_AVX2 and FALCON_FMA\n\n    FALCON_AVX2, when enabled, activates the use of AVX2 compiler\n    intrinsics. This works only on x86 CPU that offer AVX2 opcodes.\n    Use of AVX2 improves performance. FALCON_AVX2 has no effect if\n    FALCON_FPEMU is used.\n\n    FALCON_FMA further enables the use for FMA (\"fused multiply-add\")\n    compiler intrinsics for an extra boost to performance. This\n    setting is ignored unless FALCON_FPNATIVE and FALCON_AVX2 are\n    both used. Occasionally (but rarely), use of FALCON_FMA will\n    change the keys and/or signatures generated from a given random\n    seed, impacting reproducibility of test vectors; however, this\n    has no bearing on the security of normal usage.\n\n  - FALCON_ASM_CORTEXM4\n\n    When enabled, inline assembly routines for FP emulation and SHAKE256\n    will be used. This will work only on the ARM Cortex M3, M4 and\n    compatible CPU. This assembly code is constant-time on the M4, and\n    about twice faster than the generic C code used by FALCON_FPEMU.\n\n\nUSAGE\n-----\n\nSee the Makefile for compilation flags, and config.h for configurable\noptions. Type 'make' to compile: this will generate two binaries called\n'test_falcon' and 'speed'. 'test_falcon' runs unit tests to verify that\neverything computes the expected values. 'speed' runs performance\nbenchmarks on Falcon-256, Falcon-512 and Falcon-1024 (Falcon-256 is a\nreduced version that is faster and smaller than Falcon-512, but provides\nonly reduced security, and not part of the \"official\" Falcon).\n\nApplications that want to use Falcon normally work on the external API,\nwhich is documented in the \"falcon.h\" file. This is the only file that\nan external application needs to use.\n\nFor research purposes, the inner API is documented in \"inner.h\". This\nAPI gives access to many internal functions that perform some elementary\noperations used in Falcon. That API also has some non-obvious\nrequirements, such as alignment on temporary buffers, or the need to\nadjust FPU precision on 32-bit x86 systems.\n\n\nLICENSE\n-------\n\nThis code is provided under the MIT license:\n\n==========================(LICENSE BEGIN)============================\nCopyright (c) 2017-2020  Falcon Project\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n===========================(LICENSE END)=============================\n\nThe main code was written by Thomas Pornin <thomas.pornin@nccgroup.com>, to\nwhom questions may be addressed. I'll endeavour to respond more or less\npromptly.\n\nThe deterministic mode was written by David Lazar\n<lazard@csail.mit.edu>, with input from Chris Peikert\n<chris.peikert@algorand.com> and others from Algorand, Inc.\n", "release_dates": []}, {"name": "ff-zeroize", "description": "This is a fork of zkcrypto/ff crate with zeroize feature", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ff-zeroize\n\n* `ff-zeroize` is a temporary crate that enables `zeroize` features for `ff`crate\n* `ff` is a finite field library written in pure Rust, with no `unsafe{}` code.\n\n## Disclaimers\n\n* This library does not provide constant-time guarantees.\n\n## Usage\n\nAdd the `ff` crate to your `Cargo.toml`:\n\n```toml\n[dependencies]\nff_zeroize = \"0.6.1\"\n```\n\nThe `ff` crate contains `Field`, `PrimeField`, `PrimeFieldRepr` and `SqrtField` traits.\nSee the **[documentation](https://docs.rs/ff/)** for more.\n\n### #![derive(PrimeField)]\n\nIf you need an implementation of a prime field, this library also provides a procedural\nmacro that will expand into an efficient implementation of a prime field when supplied\nwith the modulus. `PrimeFieldGenerator` must be an element of Fp of p-1 order, that is\nalso quadratic nonresidue.\n\nFirst, enable the `derive` crate feature:\n\n```toml\n[dependencies]\nff_zeroize = { version = \"0.6.1\", features = [\"derive\"] }\n```\n\nAnd then use the macro like so:\n\n```rust\nextern crate rand;\n#[macro_use]\nextern crate ff;\n\n#[derive(PrimeField)]\n#[PrimeFieldModulus = \"52435875175126190479447740508185965837690552500527637822603658699938581184513\"]\n#[PrimeFieldGenerator = \"7\"]\nstruct Fp(FpRepr);\n```\n\nAnd that's it! `Fp` now implements `Field` and `PrimeField`. `Fp` will also implement\n`SqrtField` if supported. The library implements `FpRepr` itself and derives\n`PrimeFieldRepr` for it.\n\n## License\n\nLicensed under either of\n\n * Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or\n   http://www.apache.org/licenses/LICENSE-2.0)\n * MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n\nat your option.\n\n### Contribution\n\nUnless you explicitly state otherwise, any contribution intentionally\nsubmitted for inclusion in the work by you, as defined in the Apache-2.0\nlicense, shall be dual licensed as above, without any additional terms or\nconditions.\n", "release_dates": []}, {"name": "generator", "description": "OpenAPI parser/generator.", "language": "Java", "license": null, "readme": "# generator\nThis is a general purpose OpenAPI code generator. It is currently used to completely generate the HTTP code in the Java SDK, and generate some of the HTTP code in our Golang SDK.\n\n# Usage\nWe currently have two HTTP endpoints. One for algod and one for indexer, so in most cases, this tool would be run once with each OpenAPI spec.\n\n### Build as a self-executing jar:\n```\n~$ mvn package -DskipTests\n~$ java -jar target/generator-*-jar-with-dependencies.jar -h\n```\n\nYou'll see that there are a number of subcommands:\n* **java** - the original Java SDK generator.\n* **responses** - generate randomized test files for SDK unit tests.\n* **template** - a generator that uses velocity templates rather than Java code to configure the code generation.\n\n### Code layout\n\nThe command line interface uses JCommander to define the command line interface. See Main.java.\n\nThe main code involves an OpenAPI parser / event generator and several listeners for the actual generation.\n\n![object layout](docs/layout.png)\n\n\n# Templates\n\nThe template subcommand is using [Apache Velocity](https://velocity.apache.org/) as the underlying template engine. Things like variables, loops, and statements are all supported. So business logic can technically be implemented in the template if it's actually necessary.\n\n### Template files\nThere are three phases: **client**, **query**, and **model**. Each phase must provide two templates, one for the file generation and one to specify the filename to be used. If all results should go to the same file. For **query** and **model** generation the template will be executed once for each **query** / **model**. If you want to put everything in one file return the same filename twice in a row and the processing will exit early.\n\n| phase | filename | purpose |\n| ----- | -------- | ------- |\n| client | client.vm          | Client class with functions to call each query. |\n| client | client_filename.vm | File to write to the client output directory. |\n| query  | query.vm           | Template to use for generating query files. |\n| query  | query_filename.vm  | File to write to the query output directory. |\n| model  | model.vm           | Template to use for generating model files. |\n| model  | model_filename.vm  | File to write to the model output directory. |\n\n### Output directories\nThe template command will only run the templates which have an output directory is provided. So if you just want to regenerate models, only use the **-m** option.\n```\n  -c, --clientOutputDir\n    Directory to write client file(s).\n  -m, --modelsOutputDir\n    Directory to write model file(s).\n  -q, --queryOutputDir\n    Directory to write query file(s).\n```\n\n### Property files\nThe template subcommand accepts a **--propertyFiles** option. It can be provided multiple times, or as a comma separated list of files. Property files will be processed and bound to a velocity variable available to templates.\n\n### template variables\n\nFor details on a type you can put it directly into your template. It will be serialized along with its fields for your reference. Here is a high level description of what is available:\n\n| template | variable | type | purpose |\n| -------- | -------- | ---- |------- |\n| all      | str      | `StringHelpers.java` | Some string utilities are available. See `StringHelpers.java` for details. There are simple things like `$str.capitalize(\"someData\")` -> `SomeData`, and also some more complex helpers like `$str.formatDoc($query.doc, \"// \")` which will split the document at the word boundary nearest to 80 characters without going over, and add a prefix to each new line. |\n| all      | order    | `OrderHelpers.java`  | Some ordering utilities available. See `OrderHelpers.java` for details. An example utility function is `$order.propertiesWithOrdering($props, $preferred_order)`, where `$props` is a list of properties and `$preferred_order` is a string list to use when ordering the properties list. |\n| all      | propFile | `Properties` | The contents of all property files are available with this variable. For example if `package=com.algorand.v2.algod` is in the property file, the template may use `${propFile.package}`.\n| all      | models   | `HashMap<StructDef, List<TypeDef>>` | A list of all models. |\n| all      | queries  | `List<QueryDef>` | A list of all queries. |\n| query    | q        | `QueryDef` | The current query definition. |\n| model    | def      | `StructDef` | The current model definition if multiple files are being generated. |\n| model    | props    | `List<TypeDef>` | A list of properties for the current model. |\n\n### Example usage\n\nIn the following example, we are careful to generate the algod code first because the algod models are a strict subset of the indexer models. For that reason, we are able to reuse some overlapping models from indexer in algod.\n```\n~$ java -jar generator*jar template\n        -s algod.oas2.json\n        -t go_templates\n        -c algodClient\n        -m allModels\n        -q algodQueries\n        -p common_config.properties,algod_config.properties\n~$ java -jar generator*jar template\n        -s indexer.oas2.json\n        -t go_templates\n        -c indexerClient\n        -m allModels\n        -q indexerQueries\n        -p common_config.properties,indexer_config.properties\n```\n\n# Test Template\n\nThere is a test template that gives you some basic usage in the **test_templates** directory.\n\nYou can generate the test code in the **output** directory with the following commands:\n```\n~$ mkdir output\n~$ java -jar target/generator-*-jar-with-dependencies.jar \\\n    template \\\n    -s /path/to/a/spec/file/indexer.oas2.json \\\n    -t test_templates/ \\\n    -m output \\\n    -q output \\\n    -c output \\\n    -p test_templates/my.properties\n```\n\n# Golang Template\n\nThe Golang templates are in the **go_templates** directory.\n\nThe Golang HTTP API is only partially generated. The hand written parts were not totally consistent with the spec and that makes it difficult to regenerate them. Regardless, an attempt has been made. In the templates there are some macros which map \"generated\" values to the hand written ones. For example the query types have this mapping:\n```\n#macro ( queryType )\n#if ( ${str.capitalize($q.name)} == \"SearchForAccounts\" )\nSearchAccounts## The hand written client doesn't quite match the spec...\n#elseif ( ${str.capitalize($q.name)} == \"GetStatus\" )\nStatus##\n#elseif ( ${str.capitalize($q.name)} == \"GetPendingTransactionsByAddress\" )\nPendingTransactionInformationByAddress##\n#elseif ( ${str.capitalize($q.name)} == \"GetPendingTransactions\" )\nPendingTransactions##\n#else\n${str.capitalize($q.name)}##\n#end\n#end\n```\n\nOther mappings are more specific to the language, such as the OpenAPI type to SDK type:\n```\n#macro ( toQueryType $param )##\n#if ( $param.algorandFormat == \"RFC3339 String\" )\nstring##\n#elseif ( $param.type == \"integer\" )\nuint64##\n#elseif ( $param.type == \"string\" )\nstring##\n#elseif ( $param.type == \"boolean\" )\nbool##\n#elseif( $param.type == \"binary\" )\nstring##\n#else\nUNHANDLED TYPE\n- ref: $!param.refType\n- type: $!param.type\n- array type: $!param.arrayType\n- algorand format: $!param.algorandFormat\n- format: $!param.format\n##$unknown.type ## force a template failure because $unknown.type does not exist.\n#end\n#end\n```\n\nBecause of this, we are phasing in code generation gradually by skipping some types. The skipped types are specified in the property files:\n\n**common_config.properties**\n```\nmodel_skip=AccountParticipation,AssetParams,RawBlockJson,etc,...\n```\n\n**algod_config.properties**\n```\nquery_skip=Block,BlockRaw,SendRawTransaction,SuggestedParams,etc,...\n```\n\n**indexer_config.properties**\n```\nquery_skip=LookupAssetByID,LookupAccountTransactions,SearchForAssets,LookupAssetBalances,LookupAssetTransactions,LookupBlock,LookupTransactions,SearchForTransactions\n```\n\n# Java Template\nThe Java templates are in the **java_templates** directory.\n\nThese are not used yet, they are the initial experiments for the template engine. Since the Java SDK has used code generation from the beginning, we should be able to fully migrate to the template engine eventually.\n\n# Automation\n\n## Preparing an external repository for automatic code generation\n\nIn general, the automation pipeline will build and run whatever `Dockerfile` is found in a repository's `templates` directory. For instructions on how to configure the `templates` directory, look at the [repository template directory example](./examples/repo_template_dir).\n\nIf you are trying to verify that automatic code generation works as intended, we recommend creating a testing branch from that repository and using the `SKIP_PR=true` environment variable to avoid creating pull requests. If all goes according to plan, generated files should be available in the container's `/repo` directory.\n\n## Setting up the automatic generator\n\nThe automatic generator scripts depend on certain prerequisites that are listed in [automation/REQUIREMENTS.md](./automation/REQUIREMENTS.md). Once those conditions have been satisfied, automatically generating code for external repositories should be as easy as building and running a particular SDK's `templates/Dockerfile` file.\n", "release_dates": []}, {"name": "go-algorand", "description": "Algorand's official implementation in Go. ", "language": "Go", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "| rel/stable <br> [![CircleCI](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fstable.svg?style=svg)](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fstable) | rel/beta  <br> [![CircleCI](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fbeta.svg?style=svg)](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fbeta) | rel/nightly  <br> [![CircleCI](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fnightly.svg?style=svg)](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fnightly) |\n| --- | --- | --- |\n\n# go-algorand\n\nAlgorand's official implementation in Go.\n\nAlgorand is a permissionless, pure proof-of-stake blockchain that delivers\ndecentralization, scalability, security, and transaction finality.\n\n## Getting Started\n\nOur [developer website][developer site url] has the most up to date information\nabout using and installing the Algorand platform.\n\n## Building from source\n\nDevelopment is done using the [Go Programming Language](https://golang.org/).\nThe version of go is specified in the project's [go.mod](go.mod) file. This document assumes that you have a functioning\nenvironment setup. If you need assistance setting up an environment please visit\nthe [official Go documentation website](https://golang.org/doc/).\n\n### Linux / OSX\n\nWe currently strive to support Debian-based distributions with Ubuntu 20.04\nbeing our official release target.\nBuilding on Arch Linux works as well.\nOur core engineering team uses Linux and OSX, so both environments are well\nsupported for development.\n\nOSX only: [Homebrew (brew)](https://brew.sh) must be installed before\ncontinuing. [Here](https://docs.brew.sh/Installation) are the installation\nrequirements.\n\nInitial environment setup:\n```bash\ngit clone https://github.com/algorand/go-algorand\ncd go-algorand\n./scripts/configure_dev.sh\n./scripts/buildtools/install_buildtools.sh\n```\n\nAt this point, you are ready to build go-algorand. We use `make` and have a\nnumber of targets to automate common tasks.\n\n#### build\n```bash\nmake install\n```\n\n#### test\n```bash\n# unit tests\nmake test\n\n# integration tests\nmake integration\n```\n\n#### style and checks\n```bash\nmake fmt\nmake lint\nmake fix\nmake vet\n```\nor alternatively\n```bash\nmake sanity\n```\n\n### Running a node\n\nOnce the software is built you'll find binaries in `${GOPATH}/bin`, and a data\ndirectory will be initialized at `~/.algorand`. Start your node with\n`${GOPATH}/bin/goal node start -d ~/.algorand`, use `${GOPATH}/bin/carpenter -d\n~/.algorand` to see activity. Refer to the [developer website][developer site\nurl] for how to use the different tools.\n\n#### Providing your own data directory\nYou can run a node out of other directories than `~/.algorand` and join networks\nother than mainnet. Just make a new directory and copy into it the\n`genesis.json` file for the network. For example:\n```bash\nmkdir ~/testnet_data\ncp installer/genesis/testnet/genesis.json ~/testnet_data/genesis.json\n${GOPATH}/bin/goal node start -d ~/testnet_data\n```\nGenesis files for mainnet, testnet, and betanet can be found in\n`installer/genesis/`.\n\n## Contributing\n\nPlease refer to our [CONTRIBUTING](CONTRIBUTING.md) document.\n\n\n## Project Layout\n\n`go-algorand` is split into various subsystems containing various packages.\n\n### Core\n\nProvides core functionality to the `algod` and `kmd` daemons, as well as other tools and commands:\n\n  - `crypto` contains the cryptographic constructions we're using for hashing,\n    signatures, and VRFs. There are also some Algorand-specific details here\n    about spending keys, protocols keys, one-time-use signing keys, and how they\n    relate to each other.\n  - `config` holds configuration parameters.  These include parameters used\n    locally by the node as well as parameters that must be agreed upon by the\n    protocol.\n  - `data` defines various types used throughout the codebase.\n     - `basics` hold basic types such as MicroAlgos, account data, and\n       addresses.\n     - `account` defines accounts, including \"root\" accounts (which can\n       spend money) and \"participation\" accounts (which can participate in\n       the agreement protocol).\n     - `transactions` define transactions that accounts can issue against\n       the Algorand state.  These include standard payments and also\n       participation key registration transactions.\n     - `bookkeeping` defines blocks, which are batches of transactions\n       atomically committed to Algorand.\n     - `pools` implement the transaction pool.  The transaction pool holds\n       transactions seen by a node in memory before they are proposed in a\n       block.\n     - `committee` implements the credentials that authenticate a\n       participating account's membership in the agreement protocol.\n  - `ledger` ([README](ledger/README.md)) contains the Algorand Ledger state\n    machine, which holds the sequence of blocks.  The Ledger executes the state\n    transitions that result from applying these blocks.  It answers queries on\n    blocks (e.g., what transactions were in the last committed block?) and on\n    accounts (e.g., what is my balance?).\n  - `protocol` declares constants used to identify protocol versions, tags for\n    routing network messages, and prefixes for domain separation of\n    cryptographic inputs.  It also implements the canonical encoder.\n  - `network` contains the code for participating in a mesh network based on\n    WebSockets. Maintains connection to some number of peers, (optionally)\n    accepts connections from peers, sends point to point and broadcast messages,\n    and receives messages routing them to various handler code\n    (e.g. agreement/gossip/network.go registers three handlers).\n     - `rpcs` contains the HTTP RPCs used by `algod` processes to query one\n       another.\n  - `agreement` ([README](agreement/README.md)) contains the agreement service,\n    which implements Algorand's Byzantine Agreement protocol.  This protocol\n    allows participating accounts to quickly confirm blocks in a fork-safe\n    manner, provided that sufficient account stake is correctly executing the\n    protocol.\n  - `node` integrates the components above and handles initialization and\n    shutdown.  It provides queries into these components.\n\n### Daemon\n\nContains the two daemons which provide Algorand clients with services:\n\n  - `daemon/algod` holds the `algod` daemon, which implements a participating\n    node.  `algod` allows a node to participate in the agreement protocol,\n    submit and confirm transactions, and view the state of the Algorand Ledger.\n     - `daemon/algod/api` ([README](daemon/algod/api/README.md)) is the REST\n       interface used for interactions with algod.\n  - `daemon/kmd` ([README](daemon/kmd/README.md)) holds the `kmd` daemon.  This\n    daemon allows a node to sign transactions.  Because `kmd` is separate from\n    `algod`, `kmd` allows a user to sign transactions on an air-gapped computer.\n\n### Interfacing\n\nAllows developers to interface with the Algorand system:\n\n  - `cmd` holds the primary commands defining entry points into the system.\n     - `cmd/catchupsrv` ([README](cmd/catchupsrv/README.md)) is a tool to\n       assist with processing historic blocks on a new node.\n  - `libgoal` exports a Go interface useful for developers of Algorand clients.\n  - `tools` ([README](tools/README.md)) various tools and utilities without a better place to go.\n  - `tools/debug` holds secondary commands which assist developers during debugging.\n  - `tools/misc` ([README](tools/misc/README.md)) small tools that are sometimes handy in a pinch.\n\n### Deployment\nHelp Algorand developers deploy networks of their own:\n\n  - `nodecontrol`\n  - `docker`\n  - `commandandcontrol` ([README](test/commandandcontrol/README.md)) is a tool to\n    automate a network of algod instances.\n  - `components`\n  - `netdeploy`\n\n### Utilities\nProvides utilities for the various components:\n\n  - `logging` is a wrapper around `logrus`.\n  - `util` contains a variety of utilities, including a codec, a SQLite wrapper,\n    a goroutine pool, a timer interface, node metrics, and more.\n\n### Test\n`test` ([README](test/README.md)) contains end-to-end tests and utilities for the above components.\n\n\n## License\n[![License: AGPL v3](https://img.shields.io/badge/License-AGPL%20v3-blue.svg)](COPYING)\n\nPlease see the [COPYING_FAQ](COPYING_FAQ) for details about how to apply our license.\n\nCopyright (C) 2019-2024, Algorand Inc.\n\n[developer site url]: https://developer.algorand.org/\n", "release_dates": ["2024-02-05T15:30:31Z", "2024-01-29T15:53:12Z", "2024-01-09T15:30:42Z", "2023-12-20T16:03:21Z", "2023-12-07T15:32:59Z", "2023-12-04T15:40:31Z", "2023-11-20T15:57:09Z", "2023-10-25T14:32:09Z", "2023-10-18T15:15:06Z", "2023-10-05T15:02:32Z", "2023-10-03T15:17:51Z", "2023-09-11T18:22:41Z", "2023-08-31T15:43:19Z", "2023-08-09T14:30:41Z", "2023-07-31T16:19:57Z", "2023-06-23T18:26:10Z", "2023-06-22T20:45:17Z", "2023-06-15T18:55:29Z", "2023-06-14T20:03:55Z", "2023-06-08T18:31:20Z", "2023-06-01T15:14:50Z", "2023-04-13T14:30:30Z", "2023-04-12T14:43:28Z", "2023-03-21T21:06:58Z", "2023-03-15T14:28:16Z", "2023-02-03T21:09:54Z", "2023-02-02T20:55:13Z", "2023-01-26T17:42:50Z", "2023-01-20T20:02:35Z", "2023-01-19T23:51:20Z"]}, {"name": "go-algorand-backport-3-12-3", "description": "Algorand's official implementation in Go. ", "language": null, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "| rel/stable <br> [![CircleCI](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fstable.svg?style=svg)](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fstable) | rel/beta  <br> [![CircleCI](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fbeta.svg?style=svg)](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fbeta) | rel/nightly  <br> [![CircleCI](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fnightly.svg?style=svg)](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fnightly) |\n| --- | --- | --- |\n\ngo-algorand\n====================\nAlgorand's official implementation in Go.\n\nAlgorand is a permissionless, pure proof-of-stake blockchain that delivers\ndecentralization, scalability, security, and transaction finality.\n\n## Getting Started ##\n\nOur [developer website][developer site url] has the most up to date information\nabout using and installing the Algorand platform.\n\n## Building from source ##\n\nDevelopment is done using the [Go Programming Language](https://golang.org/).\nThe version of go is specified in the project's [go.mod](go.mod) file. This document assumes that you have a functioning\nenvironment setup. If you need assistance setting up an environment please visit\nthe [official Go documentation website](https://golang.org/doc/).\n\n### Linux / OSX ###\n\nWe currently strive to support Debian-based distributions with Ubuntu 18.04\nbeing our official release target.\nBuilding on Arch Linux works as well.\nOur core engineering team uses Linux and OSX, so both environments are well\nsupported for development.\n\nOSX only: [Homebrew (brew)](https://brew.sh) must be installed before\ncontinuing. [Here](https://docs.brew.sh/Installation) are the installation\nrequirements.\n\nInitial environment setup:\n```bash\ngit clone https://github.com/algorand/go-algorand\ncd go-algorand\n./scripts/configure_dev.sh\n./scripts/buildtools/install_buildtools.sh\n```\n\nAt this point, you are ready to build go-algorand. We use `make` and have a\nnumber of targets to automate common tasks.\n\n#### build\n```bash\nmake install\n```\n\n#### test\n```bash\n# unit tests\nmake test\n\n# integration tests\nmake integration\n```\n\n#### style and checks\n```bash\nmake fmt\nmake lint\nmake fix\nmake vet\n```\nor alternatively\n```bash\nmake sanity\n```\n\n### Running a node\n\nOnce the software is built you'll find binaries in `${GOPATH}/bin`, and a data\ndirectory will be initialized at `~/.algorand`. Start your node with\n`${GOPATH}/bin/goal node start -d ~/.algorand`, use `${GOPATH}/bin/carpenter -d\n~/.algorand` to see activity. Refer to the [developer website][developer site\nurl] for how to use the different tools.\n\n#### Providing your own data directory\nYou can run a node out of other directories than `~/.algorand` and join networks\nother than mainnet. Just make a new directory and copy into it the\n`genesis.json` file for the network. For example:\n```bash\nmkdir ~/testnet_data\ncp installer/genesis/testnet/genesis.json ~/testnet_data/genesis.json\n${GOPATH}/bin/goal node start -d ~/testnet_data\n```\nGenesis files for mainnet, testnet, and betanet can be found in\n`installer/genesis/`.\n\n## Contributing (Code, Documentation, Bugs, Etc) ##\n\nPlease refer to our [CONTRIBUTING](CONTRIBUTING.md) document.\n\n\n## Project Layout ##\n\n`go-algorand` is split into various subpackages.\n\nThe following packages provide core functionality to the `algod` and `kmd`\ndaemons, as well as other tools and commands:\n\n  - `crypto` contains the cryptographic constructions we're using for hashing,\n    signatures, and VRFs. There are also some Algorand-specific details here\n    about spending keys, protocols keys, one-time-use signing keys, and how they\n    relate to each other.\n  - `config` holds configuration parameters.  These include parameters used\n    locally by the node as well as parameters that must be agreed upon by the\n    protocol.\n  - `data` defines various types used throughout the codebase.\n     - `basics` hold basic types such as MicroAlgos, account data, and\n       addresses.\n     - `account` defines accounts, including \"root\" accounts (which can\n       spend money) and \"participation\" accounts (which can participate in\n       the agreement protocol).\n     - `transactions` define transactions that accounts can issue against\n       the Algorand state.  These include standard payments and also\n       participation key registration transactions.\n     - `bookkeeping` defines blocks, which are batches of transactions\n       atomically committed to Algorand.\n     - `pools` implement the transaction pool.  The transaction pool holds\n       transactions seen by a node in memory before they are proposed in a\n       block.\n     - `committee` implements the credentials that authenticate a\n       participating account's membership in the agreement protocol.\n  - `ledger` ([README](ledger/README.md)) contains the Algorand Ledger state\n    machine, which holds the sequence of blocks.  The Ledger executes the state\n    transitions that result from applying these blocks.  It answers queries on\n    blocks (e.g., what transactions were in the last committed block?) and on\n    accounts (e.g., what is my balance?).\n  - `protocol` declares constants used to identify protocol versions, tags for\n    routing network messages, and prefixes for domain separation of\n    cryptographic inputs.  It also implements the canonical encoder.\n  - `network` contains the code for participating in a mesh network based on\n    WebSockets. Maintains connection to some number of peers, (optionally)\n    accepts connections from peers, sends point to point and broadcast messages,\n    and receives messages routing them to various handler code\n    (e.g. agreement/gossip/network.go registers three handlers).\n     - `rpcs` contains the HTTP RPCs used by `algod` processes to query one\n       another.\n  - `agreement` ([README](agreement/README.md)) contains the agreement service,\n    which implements Algorand's Byzantine Agreement protocol.  This protocol\n    allows participating accounts to quickly confirm blocks in a fork-safe\n    manner, provided that sufficient account stake is correctly executing the\n    protocol.\n  - `node` integrates the components above and handles initialization and\n    shutdown.  It provides queries into these components.\n\n`daemon` defines the two daemons which provide Algorand clients with services:\n\n  - `daemon/algod` holds the `algod` daemon, which implements a participating\n    node.  `algod` allows a node to participate in the agreement protocol,\n    submit and confirm transactions, and view the state of the Algorand Ledger.\n     - `daemon/algod/api` ([README](daemon/algod/api/README.md)) is the REST\n       interface used for interactions with algod.\n  - `daemon/kmd` ([README](daemon/kmd/README.md)) holds the `kmd` daemon.  This\n    daemon allows a node to sign transactions.  Because `kmd` is separate from\n    `algod`, `kmd` allows a user to sign transactions on an air-gapped computer.\n\nThe following packages allow developers to interface with the Algorand system:\n\n  - `cmd` holds the primary commands defining entry points into the system.\n     - `cmd/catchupsrv` ([README](cmd/catchupsrv/README.md)) is a tool to\n       assist with processing historic blocks on a new node.\n  - `libgoal` exports a Go interface useful for developers of Algorand clients.\n  - `tools` ([README](tools/README.md)) various tools and utilities without a better place to go.\n  - `tools/debug` holds secondary commands which assist developers during debugging.\n  - `tools/misc` ([README](tools/misc/README.md)) small tools that are sometimes handy in a pinch.\n\nThe following packages contain tools to help Algorand developers deploy networks\nof their own:\n\n  - `nodecontrol`\n  - `docker`\n  - `commandandcontrol` ([README](test/commandandcontrol/README.md)) is a tool to\n    automate a network of algod instances.\n  - `components`\n  - `netdeploy`\n\nA number of packages provide utilities for the various components:\n\n  - `logging` is a wrapper around `logrus`.\n  - `util` contains a variety of utilities, including a codec, a SQLite wrapper,\n    a goroutine pool, a timer interface, node metrics, and more.\n\n`test` ([README](test/README.md)) contains end-to-end tests and utilities for the above components.\n\n\n## License\n[![License: AGPL v3](https://img.shields.io/badge/License-AGPL%20v3-blue.svg)](COPYING)\n\nPlease see the [COPYING_FAQ](COPYING_FAQ) for details about how to apply our license.\n\nCopyright (C) 2019-2023, Algorand Inc.\n\n[developer site url]: https://developer.algorand.org/\n", "release_dates": []}, {"name": "go-algorand-doc", "description": "Documentation for Algorand's Go implementation", "language": "Shell", "license": null, "readme": "# go-algorand-doc\n\n## Getting started with TestNet\n\nThis Repo contains the installers for the Algorand Blockchain TestNet.\n\nTo get started, refer to the [Node Setup Guide](https://developer.algorand.org/docs/introduction-installing-node). All developer documentation is available at [Developer.algorgand.org](https://developer.algorand.org).\n\nWe are using many different communication channels for discussing TestNet.  Our official channel for TestNet support is https://community.algorand.com/.  We will also be monitoring this Github repo for issues and discussions.\n\n\n## Node hardware requirements (subject to change)\nAt this time, we're expecting participants to run standalone Nodes and not Relays, so the hardware requirements are fairly minimal.  You need 4-8GB RAM, 100GB HDD/SSD, and 10Mbit broadband.  The more cores in your CPU the better, but generally 4 cores are more than enough for a single node.  There are diminishing returns after that.  There is no specific GPU-optimized code, so your graphics card should have no impact.\n\n## Once you have a running node\nEnsure you have enabled telemetry and send us your [Node name and GUID](https://www.algorand.com/testnet-tasks-telemetry-registration/) so we can correlate telemetry properly. This process is explained in the [Node Setup Guide](https://developer.algorand.org/docs/introduction-installing-node).\n\nIt's important that you are configured to update regularly or you risk being disconnected from the network and unable to connect until after you update. Not to mention falling behind in features and bug fixes.  We recommend setting up a CRON job as outlined in the [Configuring Auto-Update Guide](https://developer.algorand.org/docs/configure-auto-update).  If you want to manually check for an update, use `./update.sh -d ~/node/data` as discussed in the Setup Guide.\n\n## Using GOAL\nRun `goal --help` to get help.\n\nMost commands need you to specify the location of the data folder.  You can set ALGORAND_DATA in the environment if you want to skip that step when using `goal`.\n\ne.g.\n\n    ./goal node status -d ~/node/data\n    ./goal account new -d ~/node/data\n\nOnce you create an account, you can use our [Dispenser](https://bank.testnet.algorand.network) to transfer tokens to your account.\n\nThe `./goal clerk` command is used to generate your own transactions.\n\nWe currently have a [dashboard](http://r1.algorand.network:5001) running for TestNet, which displays the view of the blockchain from one of our Relays.\n\nWe sometimes have a script running that's generating random transactions between some test accounts, generating ~4 TPS on the network.\n\n## Writing your own client\nRefer to our [Using the SDKs and REST APIs](https://developer.algorand.org/docs/using-sdks-and-rest-apis) documentation as a starting point for writing your own clients.\n\nLet us know where you would like more documentation and we'll look at prioritizing that.\n\nThanks, and **Welcome to Algorand's TestNet!**\n", "release_dates": []}, {"name": "go-algorand-merge-queue-testing", "description": "Temporary repository to test Github Merge Queue", "language": null, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "| rel/stable <br> [![CircleCI](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fstable.svg?style=svg)](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fstable) | rel/beta  <br> [![CircleCI](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fbeta.svg?style=svg)](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fbeta) | rel/nightly  <br> [![CircleCI](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fnightly.svg?style=svg)](https://circleci.com/gh/algorand/go-algorand/tree/rel%2Fnightly) |\n| --- | --- | --- |\n\ngo-algorand\n====================\nAlgorand's official implementation in Go.\n\nAlgorand is a permissionless, pure proof-of-stake blockchain that delivers\ndecentralization, scalability, security, and transaction finality.\n\n## Getting Started ##\n\nOur [developer website][developer site url] has the most up to date information\nabout using and installing the Algorand platform.\n\n## Building from source ##\n\nDevelopment is done using the [Go Programming Language](https://golang.org/).\nThe version of go is specified in the project's [go.mod](go.mod) file. This document assumes that you have a functioning\nenvironment setup. If you need assistance setting up an environment please visit\nthe [official Go documentation website](https://golang.org/doc/).\n\n### Linux / OSX ###\n\nWe currently strive to support Debian-based distributions with Ubuntu 18.04\nbeing our official release target.\nBuilding on Arch Linux works as well.\nOur core engineering team uses Linux and OSX, so both environments are well\nsupported for development.\n\nOSX only: [Homebrew (brew)](https://brew.sh) must be installed before\ncontinuing. [Here](https://docs.brew.sh/Installation) are the installation\nrequirements.\n\nInitial environment setup:\n```bash\ngit clone https://github.com/algorand/go-algorand\ncd go-algorand\n./scripts/configure_dev.sh\n./scripts/buildtools/install_buildtools.sh\n```\n\nAt this point, you are ready to build go-algorand. We use `make` and have a\nnumber of targets to automate common tasks.\n\n#### build\n```bash\nmake install\n```\n\n#### test\n```bash\n# unit tests\nmake test\n\n# integration tests\nmake integration\n```\n\n#### style and checks\n```bash\nmake fmt\nmake lint\nmake fix\nmake vet\n```\nor alternatively\n```bash\nmake sanity\n```\n\n### Running a node\n\nOnce the software is built you'll find binaries in `${GOPATH}/bin`, and a data\ndirectory will be initialized at `~/.algorand`. Start your node with\n`${GOPATH}/bin/goal node start -d ~/.algorand`, use `${GOPATH}/bin/carpenter -d\n~/.algorand` to see activity. Refer to the [developer website][developer site\nurl] for how to use the different tools.\n\n#### Providing your own data directory\nYou can run a node out of other directories than `~/.algorand` and join networks\nother than mainnet. Just make a new directory and copy into it the\n`genesis.json` file for the network. For example:\n```bash\nmkdir ~/testnet_data\ncp installer/genesis/testnet/genesis.json ~/testnet_data/genesis.json\n${GOPATH}/bin/goal node start -d ~/testnet_data\n```\nGenesis files for mainnet, testnet, and betanet can be found in\n`installer/genesis/`.\n\n## Contributing (Code, Documentation, Bugs, Etc) ##\n\nPlease refer to our [CONTRIBUTING](CONTRIBUTING.md) document.\n\n\n## Project Layout ##\n\n`go-algorand` is split into various subpackages.\n\nThe following packages provide core functionality to the `algod` and `kmd`\ndaemons, as well as other tools and commands:\n\n  - `crypto` contains the cryptographic constructions we're using for hashing,\n    signatures, and VRFs. There are also some Algorand-specific details here\n    about spending keys, protocols keys, one-time-use signing keys, and how they\n    relate to each other.\n  - `config` holds configuration parameters.  These include parameters used\n    locally by the node as well as parameters that must be agreed upon by the\n    protocol.\n  - `data` defines various types used throughout the codebase.\n     - `basics` hold basic types such as MicroAlgos, account data, and\n       addresses.\n     - `account` defines accounts, including \"root\" accounts (which can\n       spend money) and \"participation\" accounts (which can participate in\n       the agreement protocol).\n     - `transactions` define transactions that accounts can issue against\n       the Algorand state.  These include standard payments and also\n       participation key registration transactions.\n     - `bookkeeping` defines blocks, which are batches of transactions\n       atomically committed to Algorand.\n     - `pools` implement the transaction pool.  The transaction pool holds\n       transactions seen by a node in memory before they are proposed in a\n       block.\n     - `committee` implements the credentials that authenticate a\n       participating account's membership in the agreement protocol.\n  - `ledger` ([README](ledger/README.md)) contains the Algorand Ledger state\n    machine, which holds the sequence of blocks.  The Ledger executes the state\n    transitions that result from applying these blocks.  It answers queries on\n    blocks (e.g., what transactions were in the last committed block?) and on\n    accounts (e.g., what is my balance?).\n  - `protocol` declares constants used to identify protocol versions, tags for\n    routing network messages, and prefixes for domain separation of\n    cryptographic inputs.  It also implements the canonical encoder.\n  - `network` contains the code for participating in a mesh network based on\n    WebSockets. Maintains connection to some number of peers, (optionally)\n    accepts connections from peers, sends point to point and broadcast messages,\n    and receives messages routing them to various handler code\n    (e.g. agreement/gossip/network.go registers three handlers).\n     - `rpcs` contains the HTTP RPCs used by `algod` processes to query one\n       another.\n  - `agreement` ([README](agreement/README.md)) contains the agreement service,\n    which implements Algorand's Byzantine Agreement protocol.  This protocol\n    allows participating accounts to quickly confirm blocks in a fork-safe\n    manner, provided that sufficient account stake is correctly executing the\n    protocol.\n  - `node` integrates the components above and handles initialization and\n    shutdown.  It provides queries into these components.\n\n`daemon` defines the two daemons which provide Algorand clients with services:\n\n  - `daemon/algod` holds the `algod` daemon, which implements a participating\n    node.  `algod` allows a node to participate in the agreement protocol,\n    submit and confirm transactions, and view the state of the Algorand Ledger.\n     - `daemon/algod/api` ([README](daemon/algod/api/README.md)) is the REST\n       interface used for interactions with algod.\n  - `daemon/kmd` ([README](daemon/kmd/README.md)) holds the `kmd` daemon.  This\n    daemon allows a node to sign transactions.  Because `kmd` is separate from\n    `algod`, `kmd` allows a user to sign transactions on an air-gapped computer.\n\nThe following packages allow developers to interface with the Algorand system:\n\n  - `cmd` holds the primary commands defining entry points into the system.\n     - `cmd/catchupsrv` ([README](cmd/catchupsrv/README.md)) is a tool to\n       assist with processing historic blocks on a new node.\n  - `libgoal` exports a Go interface useful for developers of Algorand clients.\n  - `tools` ([README](tools/README.md)) various tools and utilities without a better place to go.\n  - `tools/debug` holds secondary commands which assist developers during debugging.\n  - `tools/misc` ([README](tools/misc/README.md)) small tools that are sometimes handy in a pinch.\n\nThe following packages contain tools to help Algorand developers deploy networks\nof their own:\n\n  - `nodecontrol`\n  - `docker`\n  - `commandandcontrol` ([README](test/commandandcontrol/README.md)) is a tool to\n    automate a network of algod instances.\n  - `components`\n  - `netdeploy`\n\nA number of packages provide utilities for the various components:\n\n  - `logging` is a wrapper around `logrus`.\n  - `util` contains a variety of utilities, including a codec, a SQLite wrapper,\n    a goroutine pool, a timer interface, node metrics, and more.\n\n`test` ([README](test/README.md)) contains end-to-end tests and utilities for the above components.\n\n\n## License\n[![License: AGPL v3](https://img.shields.io/badge/License-AGPL%20v3-blue.svg)](COPYING)\n\nPlease see the [COPYING_FAQ](COPYING_FAQ) for details about how to apply our license.\n\nCopyright (C) 2019-2023, Algorand Inc.\n\n[developer site url]: https://developer.algorand.org/\n", "release_dates": []}, {"name": "go-algorand-sdk", "description": "Algorand Golang SDK", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# go-algorand-sdk\n\n[![Go Report Card](https://goreportcard.com/badge/github.com/algorand/go-algorand-sdk)](https://goreportcard.com/report/github.com/algorand/go-algorand-sdk/v2)\n[![GoDoc](https://godoc.org/github.com/algorand/go-algorand-sdk?status.svg)](https://godoc.org/github.com/algorand/go-algorand-sdk/v2)\n\nThe Algorand golang SDK provides:\n\n- HTTP clients for the algod (agreement) and kmd (key management) APIs\n- Standalone functionality for interacting with the Algorand protocol, including transaction signing, message encoding, etc.\n\n## Documentation\n\nFull documentation is available [on pkg.go.dev](https://pkg.go.dev/github.com/algorand/go-algorand-sdk/v2). You can also self-host the documentation by running `godoc -http=:8099` and visiting `http://localhost:8099/pkg/github.com/algorand/go-algorand-sdk/v2` in your web browser.\n\nAdditional developer documentation and examples can be found on [developer.algorand.org](https://developer.algorand.org/docs/sdks/go/)\n\n## Package Overview\n\nIn `client/`, the `kmd` packages provide HTTP clients for the Key Management Daemon. It is responsible for managing spending key material, signing transactions, and managing wallets.\nIn `client/v2` the `algod` package contains a client for the Algorand protocol daemon HTTP API. You can use it to check the status of the blockchain, read a block, look at transactions, or submit a signed transaction.\nIn `client/v2` the `indexer` package contains a client for the Algorand Indexer API. You can use it to query historical transactions or make queries about the current state of the chain.\n\n`transaction` package contains Transaction building utility functions.\n\n`types` contains the data structures you'll use when interacting with the network, including addresses, transactions, multisig signatures, etc. \n\n`encoding` contains the `json` and `msgpack` packages, which can be used to serialize messages for the algod/kmd APIs and the network.\n\n`mnemonic` contains support for turning 32-byte keys into checksummed, human-readable mnemonics (and going from mnemonics back to keys).\n\n## SDK Development\n\nRun tests with `make docker-test`. To set up the sandbox-based test harness without standing up the go-algorand docker image use `make harness`.\n\nWe use golangci-lint to run linters on our codebase. Please run `make lint` before you submit a PR to make sure it conforms to linter standards.\n\nWe use cucumber testing for all of our SDKs, including this one. Please refer to [algorand-sdk-testing](https://github.com/algorand/algorand-sdk-testing#readme) for guidance and existing tests that you may need to update. Depending on the type of update you with to contribute, you may also need to have corresponding updates in the other SDKs (Java, JS, and Python). Feel welcome to ask for collaboration on that front. \n\n## Quick Start\n\nTo download the SDK, open a terminal and use the `go get` command.\n\n```sh\ngo get -u github.com/algorand/go-algorand-sdk/...\n```\n", "release_dates": ["2023-12-15T19:14:38Z", "2023-09-18T18:09:29Z", "2023-06-15T22:31:11Z", "2023-05-09T17:31:11Z", "2023-01-03T21:26:52Z", "2022-12-06T20:13:08Z", "2022-11-03T21:49:56Z", "2022-10-12T17:37:45Z", "2022-09-19T15:52:36Z", "2022-09-02T15:22:47Z", "2022-08-17T20:09:02Z", "2022-07-27T16:21:06Z", "2022-06-21T19:27:20Z", "2022-06-03T13:39:56Z", "2022-04-28T20:10:35Z", "2022-03-29T12:33:24Z", "2022-03-23T21:17:03Z", "2022-03-18T22:10:48Z", "2022-03-02T21:31:33Z", "2022-02-15T22:21:09Z", "2022-01-14T16:32:05Z", "2021-12-31T00:03:26Z", "2021-12-07T17:29:13Z", "2021-10-04T23:37:40Z", "2021-08-03T14:28:00Z", "2021-07-07T16:52:21Z", "2021-06-21T16:30:56Z", "2021-06-17T22:04:33Z", "2021-05-21T20:58:51Z", "2021-04-27T19:55:31Z"]}, {"name": "go-codec", "description": "idiomatic codec and rpc lib for msgpack, cbor, json, etc. msgpack.org[Go]", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "[![Sourcegraph](https://sourcegraph.com/github.com/ugorji/go/-/badge.svg?v=2)](https://sourcegraph.com/github.com/ugorji/go/-/tree/codec)\n[![Build Status](https://travis-ci.org/ugorji/go.svg?branch=master)](https://travis-ci.org/ugorji/go)\n[![codecov](https://codecov.io/gh/ugorji/go/branch/master/graph/badge.svg?v=2)](https://codecov.io/gh/ugorji/go)\n[![GoDoc](http://img.shields.io/badge/go-documentation-blue.svg?style=flat-square)](http://godoc.org/github.com/ugorji/go/codec)\n[![rcard](https://goreportcard.com/badge/github.com/ugorji/go/codec?v=2)](https://goreportcard.com/report/github.com/ugorji/go/codec)\n[![License](http://img.shields.io/badge/license-mit-blue.svg?style=flat-square)](https://raw.githubusercontent.com/ugorji/go/master/LICENSE)\n\n# go/codec\n\nThis repository contains the `go-codec` library,\na High Performance and Feature-Rich Idiomatic encode/decode and rpc library for\n\n  - msgpack: https://github.com/msgpack/msgpack\n  - binc:    http://github.com/ugorji/binc\n  - cbor:    http://cbor.io http://tools.ietf.org/html/rfc7049\n  - json:    http://json.org http://tools.ietf.org/html/rfc7159 \n\nFor more information:\n\n  - [see the codec/Readme for quick usage information](https://github.com/ugorji/go/tree/master/codec#readme)\n  - [view the API on godoc](http://godoc.org/github.com/ugorji/go/codec)\n  - [read the detailed usage/how-to primer](http://ugorji.net/blog/go-codec-primer)\n\nInstall using:\n\n    go get github.com/algorand/go-codec\n\n", "release_dates": ["2023-06-15T16:40:38Z", "2023-06-15T16:40:22Z", "2022-08-23T15:31:56Z", "2022-08-23T15:31:28Z", "2022-03-22T15:41:41Z", "2022-03-22T15:41:57Z", "2019-07-08T23:03:30Z", "2019-07-08T23:34:27Z", "2019-07-02T11:22:33Z", "2019-07-01T19:57:07Z", "2019-07-01T19:55:45Z", "2020-07-31T17:16:33Z", "2020-07-31T17:37:13Z"]}, {"name": "go-deadlock", "description": "Online deadlock detection in go (golang)", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2023-09-27T14:48:14Z", "2022-03-22T16:43:18Z", "2020-07-29T19:26:29Z"]}, {"name": "go-fuzz", "description": "Randomized testing for Go", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# go-fuzz: randomized testing for Go\n\nGo-fuzz is a coverage-guided [fuzzing solution](http://en.wikipedia.org/wiki/Fuzz_testing) for testing of Go packages.\nFuzzing is mainly applicable to packages that parse complex inputs (both text\nand binary), and is especially useful for hardening of systems that parse inputs\nfrom potentially malicious users (e.g. anything accepted over a network).\n\n## Usage\n\nFirst, you need to write a test function of the form:\n```go\nfunc Fuzz(data []byte) int\n```\nData is a random input generated by go-fuzz, note that in most cases it is\ninvalid. The function must return 1 if the fuzzer should increase priority\nof the given input during subsequent fuzzing (for example, the input is\nlexically correct and was parsed successfully); -1 if the input must not be\nadded to corpus even if gives new coverage; and 0 otherwise; other values are\nreserved for future use.\n\nThe `Fuzz` function must be in a package that `go-fuzz` can import. This means\nthe code you want to test can't be in package `main`.  Fuzzing `internal`\npackages is supported, however.\n\nIn its basic form the Fuzz function just parses the input, and\ngo-fuzz ensures that it does not panic, crash the program, allocate insane\namount of memory nor hang. Fuzz function can also do application-level checks,\nwhich will make testing more efficient (discover more bugs). For example,\nFuzz function can serialize all inputs that were successfully deserialized,\nthus ensuring that serialization can handle everything deserialization can\nproduce. Or, Fuzz function can deserialize-serialize-deserialize-serialize\nand check that results of first and second serialization are equal. Or, Fuzz\nfunction can feed the input into two different implementations (e.g. dumb and\noptimized) and check that the output is equal. To communicate application-level\nbugs Fuzz function should panic (os.Exit(1) will work too, but panic message\ncontains more info). Note that Fuzz function should not output to stdout/stderr,\nit will slow down fuzzing and nobody will see the output anyway. The exception\nis printing info about a bug just before panicking.\n\nHere is an example of a simple Fuzz function for image/png package:\n```go\npackage png\n\nimport (\n\t\"bytes\"\n\t\"image/png\"\n)\n\nfunc Fuzz(data []byte) int {\n\tpng.Decode(bytes.NewReader(data))\n\treturn 0\n}\n```\n\nA more useful Fuzz function would look like:\n```go\nfunc Fuzz(data []byte) int {\n\timg, err := png.Decode(bytes.NewReader(data))\n\tif err != nil {\n\t\tif img != nil {\n\t\t\tpanic(\"img != nil on error\")\n\t\t}\n\t\treturn 0\n\t}\n\tvar w bytes.Buffer\n\terr = png.Encode(&w, img)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn 1\n}\n```\n\nThe second step is collection of initial input corpus. Ideally, files in the\ncorpus are as small as possible and as diverse as possible. You can use inputs\nused by unit tests and/or generate them. For example, for an image decoding\npackage you can encode several small bitmaps (black, random noise, white with\nfew non-white pixels) with different levels of compressions and use that as the\ninitial corpus. Go-fuzz will deduplicate and minimize the inputs. So throwing in\na thousand of inputs is fine, diversity is more important.\n\nPut the initial corpus into the workdir/corpus directory (in our case\n```examples/png/corpus```). Go-fuzz will add own inputs to the corpus directory.\nConsider committing the generated inputs to your source control system, this\nwill allow you to restart go-fuzz without losing previous work.\n\nThe [go-fuzz-corpus repository](https://github.com/dvyukov/go-fuzz-corpus) contains \na bunch of examples of test functions and initial input corpuses for various packages.\n\nThe next step is to get go-fuzz:\n\n```\n$ go get -u github.com/dvyukov/go-fuzz/go-fuzz github.com/dvyukov/go-fuzz/go-fuzz-build\n```\n\nThen, download the corpus and build the test program with necessary instrumentation:\n```\n$ go get -d github.com/dvyukov/go-fuzz-corpus\n$ cd $GOPATH/src/github.com/dvyukov/go-fuzz-corpus\n$ cd png\n$ go-fuzz-build\n```\nThis will produce png-fuzz.zip archive.\n\nNote that go-fuzz [does not support modules yet](https://github.com/dvyukov/go-fuzz/issues/195).\n`go-fuzz-build` disables modules by setting environment variable `GO111MODULE=off` during the build.\n\nNow we are ready to go:\n```\n$ go-fuzz\n```\n\nGo-fuzz will generate and test various inputs in an infinite loop. Workdir is\nused to store persistent data like current corpus and crashers, it allows fuzzer\nto continue after restart. Discovered bad inputs are stored in workdir/crashers\ndir; where file without a suffix contains binary input, file with .quoted suffix\ncontains quoted input that can be directly copied into a reproducer program or a\ntest, file with .output suffix contains output of the test on this input. Every\nfew seconds go-fuzz prints logs to stderr of the form:\n```\n2015/04/25 12:39:53 workers: 500, corpus: 186 (42s ago), crashers: 3,\n     restarts: 1/8027, execs: 12009519 (121224/sec), cover: 2746, uptime: 1m39s\n```\nWhere ```workers``` means number of tests running in parallel (set with -procs\nflag). ```corpus``` is current number of interesting inputs the fuzzer has\ndiscovered, time in brackets says when the last interesting input was\ndiscovered. ```crashers``` is number of discovered bugs (check out\nworkdir/crashers dir). ```restarts``` is the rate with which the fuzzer restarts\ntest processes. The rate should be close to 1/10000 (which is the planned\nrestart rate); if it is considerably higher than 1/10000, consider fixing already\ndiscovered bugs which lead to frequent restarts. ```execs``` is total number of\ntest executions, and the number in brackets is the average speed of test\nexecutions. ```cover``` is number of bits set in a hashed coverage bitmap, if this number\ngrows fuzzer uncovers new lines of code; size of the bitmap is 64K; ideally ```cover```\nvalue should be less than ~5000, otherwise fuzzer can miss new interesting inputs\ndue to hash collisions. And finally ```uptime``` is uptime of the process. This same\ninformation is also served via http (see the ```-http``` flag).\n\n## libFuzzer support\n\ngo-fuzz-build can also generate an archive file\nthat can be used with [libFuzzer](https://llvm.org/docs/LibFuzzer.html)\ninstead of go-fuzz (requires linux).\n\nSample usage:\n\n```\n$ cd $GOPATH/src/github.com/dvyukov/go-fuzz-corpus/fmt\n$ go-fuzz-build -libfuzzer  # produces fmt.a\n$ clang -fsanitize=fuzzer fmt.a -o fmt.libfuzzer\n$ ./fmt.libfuzzer\n```\n\nWhen run with `-libfuzzer`, go-fuzz-build adds the additional build tag\n`gofuzz_libfuzzer` when building code.\n\n## Continuous Fuzzing\n\nJust as unit-testing, fuzzing is better done continuously.\n\nCurrently there are 2 services that offer continuous fuzzing based on go-fuzz:\n\n- [fuzzit.dev](https://fuzzit.dev/) ([tutorial](https://github.com/fuzzitdev/example-go))\n- [fuzzbuzz.io](https://fuzzbuzz.io/) ([tutorial](https://docs.fuzzbuzz.io/getting-started/find-your-first-bug-in-go))\n\n## Random Notes\n\ngo-fuzz-build builds the program with gofuzz build tag, this allows to put the\nFuzz function implementation directly into the tested package, but exclude it\nfrom normal builds with ```// +build gofuzz``` directive.\n\nIf your inputs contain a checksum, it can make sense to append/update the checksum\nin the ```Fuzz``` function. The chances that go-fuzz will generate the correct\nchecksum are very low, so most work will be in vain otherwise.\n\nGo-fuzz can utilize several machines. To do this, start the coordinator process\nseparately:\n```\n$ go-fuzz -workdir=examples/png -coordinator=127.0.0.1:8745\n```\nIt will manage persistent corpus and crashers and coordinate work of worker processes.\nThen run one or more worker processes as:\n```\n$ go-fuzz -bin=./png-fuzz.zip -worker=127.0.0.1:8745 -procs=10\n```\n\n## External Articles\n\n- [go-fuzz github.com/arolek/ase](https://medium.com/@dgryski/go-fuzz-github-com-arolek-ase-3c74d5a3150c): A step-by-step tutorial\n- [DNS parser, meet Go fuzzer](https://blog.cloudflare.com/dns-parser-meet-go-fuzzer/): A success story with suggestions on how to write the ```Fuzz``` function\n- [Automated Testing with go-fuzz](https://speakerdeck.com/filosottile/automated-testing-with-go-fuzz)\n- [Going down the rabbit hole with go-fuzz](https://mijailovic.net/2017/07/29/go-fuzz/)\n- [Fuzzing markdown parser with go-fuzz](https://blog.kowalczyk.info/article/n/fuzzing-markdown-parser-written-in-go.html)\n\n## History rewrite\n\ngo-fuzz repository history was recently rewritten to exclude examples directory\nto reduce total repository size and download time (see\n[#88](https://github.com/dvyukov/go-fuzz/issues/88),\n[#114](https://github.com/dvyukov/go-fuzz/issues/114) and\nhttps://github.com/dvyukov/go-fuzz-corpus). Unfortunately, that means that\n`go get -u` command will fail if you had a previous version installed.\nPlease remove $GOPATH/github.com/dvyukov/go-fuzz before running `go get` again.\n\n## Credits and technical details\n\nGo-fuzz fuzzing logic is heavily based on [american fuzzy lop](http://lcamtuf.coredump.cx/afl/),\nso refer to [AFL readme](http://lcamtuf.coredump.cx/afl/README.txt) if you are\ninterested in technical details. AFL is written and maintained by\n[Michal Zalewski](http://lcamtuf.coredump.cx/). Some of the mutations employed\nby go-fuzz are inspired by work done by Mateusz Jurczyk, Gynvael Coldwind and\n[Felix Gr\u00f6bert](https://twitter.com/fel1x).\n\n## Trophies\n\n- [spec: non-integral constant can be converted to int](https://github.com/golang/go/issues/11350) **fixed**\n- [cmd/compile: out of fixed registers](https://github.com/golang/go/issues/11352) **fixed**\n- [cmd/compile: truncates constants](https://github.com/golang/go/issues/11326) **fixed**\n- [cmd/compile: overflow in int -> string](https://github.com/golang/go/issues/11330) **fixed**\n- [cmd/compile: bad HMUL](https://github.com/golang/go/issues/11358) **fixed**\n- [cmd/compile: treecopy Name](https://github.com/golang/go/issues/11361) **fixed**\n- [cmd/compile: accepts invalid identifiers](https://github.com/golang/go/issues/11359) **fixed**\n- [cmd/compile: hangs compiling hex fp constant](https://github.com/golang/go/issues/11364) **fixed**\n- [cmd/compile: mishandles int->complex conversion](https://github.com/golang/go/issues/11365) **fixed**\n- [cmd/compile: allows to define blank methods on builtin types](https://github.com/golang/go/issues/11366) **fixed**\n- [cmd/compile: mis-calculates a constant](https://github.com/golang/go/issues/11369) **fixed**\n- [cmd/compile: interface conversion panic](https://github.com/golang/go/issues/11540) **fixed**\n- [cmd/compile: nil pointer dereference](https://github.com/golang/go/issues/11588) **fixed**\n- [cmd/compile: nil pointer dereference (2)](https://github.com/golang/go/issues/11666) **fixed**\n- [cmd/compile: internal compiler error: plain block b3 len(Succs)==2, want 1](https://github.com/golang/go/issues/11589) **fixed**\n- [cmd/compile: internal compiler error: b3.Succs has duplicate block b3](https://github.com/golang/go/issues/11593) **fixed**\n- [cmd/compile: internal compiler error: newname nil](https://github.com/golang/go/issues/11610) **fixed**\n- [cmd/compile: accepts invalid function type](https://github.com/golang/go/issues/11595) **fixed**\n- [cmd/compile: internal compiler error: getinarg: not a func int](https://github.com/golang/go/issues/11596) **fixed**\n- [cmd/compile: hangs converting int const to complex64](https://github.com/golang/go/issues/11597) **fixed**\n- [cmd/compile: nil deref in error message](https://github.com/golang/go/issues/11614) **fixed**\n- [cmd/compile: use of untyped nil in switch](https://github.com/golang/go/issues/11668) **fixed**\n- [cmd/compile: implicitly converts complex constant to integer](https://github.com/golang/go/issues/11669) **fixed**\n- [cmd/compile: assignment to entry in nil map](https://github.com/golang/go/issues/11670) **fixed**\n- [cmd/compile: does not diagnose constant division by zero](https://github.com/golang/go/issues/11674) **fixed**\n- [cmd/compile: does not detect a missing return](https://github.com/golang/go/issues/11698) **fixed**\n- [cmd/compile: symbol \"\"._.args_stackmap listed multiple times](https://github.com/golang/go/issues/11699) **fixed**\n- [cmd/compile: \"0\"[0] should not be a constant](https://github.com/golang/go/issues/11370) **fixed**\n- [cmd/compile: unexpected %!(NOVERB)](https://github.com/golang/go/issues/13266) **fixed**\n- [cmd/compile: wrong line number in error message](https://github.com/golang/go/issues/13267) **fixed**\n- [cmd/compile: not-deterministic output](https://github.com/golang/go/issues/13268) **fixed**\n- [cmd/compile: parsing problem](https://github.com/golang/go/issues/13270) **fixed**\n- [cmd/compile: compiles incorrect program](https://github.com/golang/go/issues/13272) **fixed**\n- [cmd/compile: does not compile correct program](https://github.com/golang/go/issues/13273)\n- [cmd/compile: compiles incorrect program (2)](https://github.com/golang/go/issues/13274) **fixed**\n- [cmd/compile: internal compiler error: want FUNC, but have int](https://github.com/golang/go/issues/17038) **fixed**\n- [cmd/compile: nil deref](https://github.com/golang/go/issues/24447) **fixed**\n- [cmd/asm: index out of range](https://github.com/golang/go/issues/11759) **fixed**\n- [cmd/asm: index out of range (2)](https://github.com/golang/go/issues/12466) **fixed**\n- [cmd/asm: index out of range (3)](https://github.com/golang/go/issues/12467) **fixed**\n- [cmd/asm: index out of range (4)](https://github.com/golang/go/issues/12657) **fixed**\n- [cmd/asm: slice bounds out of range](https://github.com/golang/go/issues/11760) **fixed**\n- [cmd/asm: hang](https://github.com/golang/go/issues/11764) **fixed**\n- [cmd/asm: hang (2)](https://github.com/golang/go/issues/12469) **fixed**\n- [cmd/asm: hang (3)](https://github.com/golang/go/issues/12656) **fixed**\n- [cmd/asm: nil deref](https://github.com/golang/go/issues/11765) **fixed**\n- [cmd/asm: nil deref (2)](https://github.com/golang/go/issues/12468) **fixed**\n- [cmd/asm: nil deref (3)](https://github.com/golang/go/issues/12614) **fixed**\n- [cmd/asm: nil deref (4)](https://github.com/golang/go/issues/12627) **fixed**\n- [cmd/asm: nil deref (5)](https://github.com/golang/go/issues/12655) **fixed**\n- [cmd/asm: cannot happen: slice col](https://github.com/golang/go/issues/12654) **fixed**\n- [cmd/asm: unactionable \"invalid local variable type 0\"](https://github.com/golang/go/issues/12658)\n- [internal/trace: index out of range](https://github.com/golang/go/issues/11766) **fixed**\n- [internal/trace: index out of range (2)](https://github.com/golang/go/issues/11769) **fixed**\n- [internal/trace: nil deref](https://github.com/golang/go/issues/11767) **fixed**\n- [internal/trace: nil deref (2)](https://github.com/golang/go/issues/11770) **fixed**\n- [fmt: Printf loops on invalid verb spec](https://github.com/golang/go/issues/10674) **fixed**\n- [fmt: incorrect overflow detection](https://github.com/golang/go/issues/10695) **fixed**\n- [fmt: index out of range](https://github.com/golang/go/issues/10675) **fixed**\n- [fmt: index out of range (2)](https://github.com/golang/go/issues/10745) **fixed**\n- [fmt: index out of range (3)](https://github.com/golang/go/issues/10770) **fixed**\n- [fmt: index out of range (4)](https://github.com/golang/go/issues/10771) **fixed**\n- [fmt: index out of range (5)](https://github.com/golang/go/issues/10945) **fixed**\n- [fmt: index out of range (6)](https://github.com/golang/go/issues/11376) **fixed**\n- [regexp: slice bounds out of range](https://github.com/golang/go/issues/11176) **fixed**\n- [regexp: slice bounds out of range (2)](https://github.com/golang/go/issues/11178) **fixed**\n- [regexp: LiteralPrefix lies about completeness](https://github.com/golang/go/issues/11172)\n- [regexp: LiteralPrefix lies about completeness (2)](https://github.com/golang/go/issues/11175)\n- [regexp: POSIX regexp takes 4 seconds to execute](https://github.com/golang/go/issues/11181)\n- [regexp: confusing behavior on invalid utf-8 sequences](https://github.com/golang/go/issues/11185)\n- [regexp: considers \"\\Q\\E*\" as valid regexp](https://github.com/golang/go/issues/11187) **fixed**\n- [time: allows signs for year/tz in format string](https://github.com/golang/go/issues/11128)\n- [time: RFC3339 time.Parse can not parse string that come from time.Format](https://github.com/golang/go/issues/20555)\n- [time: Parse panic: runtime error: index out of range](https://github.com/golang/go/issues/21113) **fixed**\n- [math/big: incorrect string->Float conversion](https://github.com/golang/go/issues/11341) **fixed**\n- [math/big: MakeFromLiteral with 0 mantissa and large exponent hangs](https://github.com/golang/go/issues/16176) **fixed**\n- [net/http: can't send star request](https://github.com/golang/go/issues/11202) **fixed**\n- [net/http: allows empty header names](https://github.com/golang/go/issues/11205) **fixed**\n- [net/http: allows invalid characters in header values](https://github.com/golang/go/issues/11207) **fixed**\n- [net/http: allows %-encoding after \\[\\]](https://github.com/golang/go/issues/11208) **fixed**\n- [net/mail: ParseAddress/String corrupt address](https://github.com/golang/go/issues/11292) **fixed**\n- [net/mail: parses invalid address](https://github.com/golang/go/issues/11293) **fixed**\n- [net/mail: fails to escape address](https://github.com/golang/go/issues/11294) **fixed**\n- [net/textproto: fails to trim header value](https://github.com/golang/go/issues/11204) **fixed**\n- [archive/zip: cap out of range](https://github.com/golang/go/issues/10956) **fixed**\n- [archive/zip: bad file size](https://github.com/golang/go/issues/10957) **fixed**\n- [archive/zip: unexpected EOF](https://github.com/golang/go/issues/11144) **fixed**\n- [archive/zip: file with wrong checksum is successfully decompressed](https://github.com/golang/go/issues/11146) **fixed**\n- [archive/zip: unexpected EOF when reading archive](https://github.com/golang/go/issues/12449) **fixed**\n- [archive/tar: slice bounds out of range](https://github.com/golang/go/issues/10959) **fixed**\n- [archive/tar: slice bounds out of range (2)](https://github.com/golang/go/issues/10960) **fixed**\n- [archive/tar: slice bounds out of range (3)](https://github.com/golang/go/issues/10966) **fixed**\n- [archive/tar: slice bounds out of range (4)](https://github.com/golang/go/issues/10967) **fixed**\n- [archive/tar: slice bounds out of range (5)](https://github.com/golang/go/issues/11167) **fixed**\n- [archive/tar: deadly hang](https://github.com/golang/go/issues/10968) **fixed**\n- [archive/tar: invalid memory address or nil pointer dereference](https://github.com/golang/go/issues/11168) **fixed**\n- [archive/tar: invalid memory address or nil pointer dereference (2)](https://github.com/golang/go/issues/12435) **fixed**\n- [archive/tar: Reader.Next returns nil header](https://github.com/golang/go/issues/11169) **fixed**\n- [archive/tar: Writer incorrectly encodes header data](https://github.com/golang/go/issues/11171) **fixed**\n- [archive/tar: incorrectly claims huge file size](https://github.com/golang/go/issues/12434)\n- [archive/tar: reader returns bogus headers](https://github.com/golang/go/issues/12436) **fixed**\n- [encoding/gob: panic: drop](https://github.com/golang/go/issues/10272) **fixed**\n- [encoding/gob: makeslice: len out of range](https://github.com/golang/go/issues/10273) [3 bugs] **fixed**\n- [encoding/gob: stack overflow](https://github.com/golang/go/issues/10415) **fixed**\n- [encoding/gob: excessive memory consumption](https://github.com/golang/go/issues/10490) **fixed**\n- [encoding/gob: decoding hangs](https://github.com/golang/go/issues/10491) **fixed**\n- [encoding/gob: pointers to zero values are not initialized in Decode](https://github.com/golang/go/issues/11119)\n- [encoding/gob: crash on malicious input](https://github.com/golang/go/issues/24447)\n- [encoding/xml: allows invalid comments](https://github.com/golang/go/issues/11112)\n- [encoding/json: detect circular data structures when encoding](https://github.com/golang/go/issues/10769)\n- [encoding/asn1: index out of range](https://github.com/golang/go/issues/11129) **fixed**\n- [encoding/asn1: incorrectly handles incorrect utf8 strings](https://github.com/golang/go/issues/11126) **fixed**\n- [encoding/asn1: slice is lost during marshal/unmarshal](https://github.com/golang/go/issues/11130) **fixed**\n- [encoding/asn1: call of reflect.Value.Type on zero Value](https://github.com/golang/go/issues/11127) **fixed**\n- [encoding/asn1: Unmarshal accepts negative dates](https://github.com/golang/go/issues/11134) **fixed**\n- [encoding/pem: can't decode encoded message](https://github.com/golang/go/issues/10980) **fixed**\n- [crypto:x509: input not full blocks](https://github.com/golang/go/issues/11215) **fixed**\n- [crypto/x509: division by zero](https://github.com/golang/go/issues/11233) **fixed**\n- [image/jpeg: unreadByteStuffedByte call cannot be fulfilled](https://github.com/golang/go/issues/10387) **fixed**\n- [image/jpeg: index out of range](https://github.com/golang/go/issues/10388) **fixed**\n- [image/jpeg: invalid memory address or nil pointer dereference](https://github.com/golang/go/issues/10389) **fixed**\n- [image/jpeg: Decode hangs](https://github.com/golang/go/issues/10413) **fixed**\n- [image/jpeg: excessive memory usage](https://github.com/golang/go/issues/10532) **fixed**\n- [image/png: slice bounds out of range](https://github.com/golang/go/issues/10414) **fixed**\n- [image/png: slice bounds out of range (2)](https://github.com/golang/go/issues/12545) **fixed**\n- [image/png: interface conversion: color.Color is color.NRGBA, not color.RGBA](https://github.com/golang/go/issues/10423) **fixed**\n- [image/png: nil deref](https://github.com/golang/go/issues/10493) **fixed**\n- [image/gif: image block is out of bounds](https://github.com/golang/go/issues/10676) **fixed**\n- [image/gif: Decode returns an image with empty palette](https://github.com/golang/go/issues/11150) **fixed**\n- [image/gif: LoopCount changes on round trip](https://github.com/golang/go/issues/11287) **fixed**\n- [image/gif: Disposal is corrupted after round trip](https://github.com/golang/go/issues/11288)\n- [image/gif: EOF instead of UnexpectedEOF](https://github.com/golang/go/issues/11390)\n- [compress/flate: hang](https://github.com/golang/go/issues/10426) **fixed**\n- [compress/lzw: compress/decompress corrupts data](https://github.com/golang/go/issues/11142) **fixed**\n- [text/template: leaks goroutines on errors](https://github.com/golang/go/issues/10574#ref-issue-71873016)\n- [text/template: Call using string as type int](https://github.com/golang/go/issues/10800) **fixed**\n- [text/template: Call using complex128 as type string](https://github.com/golang/go/issues/10946) **fixed**\n- [text/template: stack overflow](https://github.com/golang/go/issues/15618)\n- [html/template: unidentified node type in allIdents](https://github.com/golang/go/issues/10610) **fixed**\n- [html/template: unidentified node type in allIdents (2)](https://github.com/golang/go/issues/10801) **fixed**\n- [html/template: unidentified node type in allIdents (3)](https://github.com/golang/go/issues/11118) **fixed**\n- [html/template: unidentified node type in allIdents (4)](https://github.com/golang/go/issues/11356) **fixed**\n- [html/template: escaping {{else}} is unimplemented](https://github.com/golang/go/issues/10611) **fixed**\n- [html/template: runtime error: slice bounds out of range](https://github.com/golang/go/issues/10612) **fixed**\n- [html/template: runtime error: slice bounds out of range (2)](https://github.com/golang/go/issues/10613) **fixed**\n- [html/template: invalid memory address or nil pointer dereference](https://github.com/golang/go/issues/10615) **fixed**\n- [html/template: panic: Call using zero Value argument](https://github.com/golang/go/issues/10634) **fixed**\n- [html/template: nil pointer dereference](https://github.com/golang/go/issues/10673) **fixed**\n- [html/template: slice bounds out of range](https://github.com/golang/go/issues/10799) **fixed**\n- [mime: ParseMediaType parses invalid media types](https://github.com/golang/go/issues/11289) **fixed**\n- [mime: Parse/Format corrupt parameters](https://github.com/golang/go/issues/11290) **fixed**\n- [mime: Parse/Format corrupt parameters (2)](https://github.com/golang/go/issues/11291) **fixed**\n- [go/constant: hang evaluating \"-6e-1886451601\"](https://github.com/golang/go/issues/20228) **fixed**\n- [go/constant, math/big: panic while constructing constant \"1i/1E-612198397\"](https://github.com/golang/go/issues/20227)\n- [go/scanner: accepts floating point literals with no decimals after E](https://github.com/golang/go/issues/17621)  **fixed**\n- [go/parser: eats \\r in comments](https://github.com/golang/go/issues/11151)\n- [go/format: turns correct program into incorrect one](https://github.com/golang/go/issues/11274)\n- [go/format: non-idempotent format](https://github.com/golang/go/issues/11275) **fixed**\n- [go/format: adds }](https://github.com/golang/go/issues/11276) **fixed**\n- [go/types: panics on invalid constant](https://github.com/golang/go/issues/11325) **fixed**\n- [go/types: compiling hangs](https://github.com/golang/go/issues/11327) **fixed**\n- [go/types: stupid shift](https://github.com/golang/go/issues/11328) **fixed**\n- [go/types: line number out of range](https://github.com/golang/go/issues/11329)\n- [go/types: assertion failed](https://github.com/golang/go/issues/11347) **fixed**\n- [go/types: converts fp constant to string](https://github.com/golang/go/issues/11353) **fixed**\n- [go/types: converts complex constant to string](https://github.com/golang/go/issues/11357) **fixed**\n- [go/types: misses '-' in error message](https://github.com/golang/go/issues/11367) **fixed**\n- [go/types: compiles invalid program with overflow](https://github.com/golang/go/issues/11368)\n- [go/types: allows duplicate switch cases](https://github.com/golang/go/issues/11578) **fixed**\n- [go/types: can shift complex numbers](https://github.com/golang/go/issues/11594) **fixed**\n- [go/types: parses comma terminated fields](https://github.com/golang/go/issues/11611) **fixed**\n- [go/types: int overflow in switch expression](https://github.com/golang/go/issues/11667) **fixed**\n- [go/types: allows multiple-value in switch and case](https://github.com/golang/go/issues/11687) **fixed**\n- [go/types: invalid error message for valid conversion to complex64](https://github.com/golang/go/issues/11590) **fixed**\n- [debug/elf: index out of range](https://github.com/golang/go/issues/10996)\n- [debug/elf: makeslice: len out of range](https://github.com/golang/go/issues/10997) **fixed**\n- [debug/elf: slice bounds out of range](https://github.com/golang/go/issues/10999)\n- [debug/pe: panic on interface conversion](https://github.com/golang/go/issues/30250)\n- [debug/pe: slice bounds out of range](https://github.com/golang/go/issues/30253)\n- [x/image/webp: index out of range](https://github.com/golang/go/issues/10383) **fixed**\n- [x/image/webp: invalid memory address or nil pointer dereference](https://github.com/golang/go/issues/10384) **fixed**\n- [x/image/webp: excessive memory consumption](https://github.com/golang/go/issues/10790)\n- [x/image/webp: excessive memory consumption (2)](https://github.com/golang/go/issues/11395)\n- [x/image/tiff: integer divide by zero](https://github.com/golang/go/issues/10393) **fixed**\n- [x/image/tiff: index out of range](https://github.com/golang/go/issues/10394) **fixed**\n- [x/image/tiff: slice bounds out of range](https://github.com/golang/go/issues/10395) **fixed**\n- [x/image/tiff: index out of range](https://github.com/golang/go/issues/10597) **fixed**\n- [x/image/tiff: slice bounds out of range](https://github.com/golang/go/issues/10596) **fixed**\n- [x/image/tiff: integer divide by zero](https://github.com/golang/go/issues/10711) **fixed**\n- [x/image/tiff: index out of range](https://github.com/golang/go/issues/10712) **fixed**\n- [x/image/tiff: index out of range](https://github.com/golang/go/issues/11386)\n- [x/image/tiff: excessive memory consumption](https://github.com/golang/go/issues/11389)\n- [x/image/{tiff,bmp}: EOF instead of UnexpectedEOF](https://github.com/golang/go/issues/11391)\n- [x/image/bmp: hang on degenerate image](https://github.com/golang/go/issues/10746) **fixed**\n- [x/image/bmp: makeslice: len out of range](https://github.com/golang/go/issues/10396) **fixed**\n- [x/image/bmp: out of memory](https://github.com/golang/go/issues/10399) **fixed**\n- [x/net/icmp: runtime error: slice bounds out of range](https://github.com/golang/go/issues/10951)\n- [x/net/html: void element <link> has child nodes](https://github.com/golang/go/issues/10535)\n- [x/net/spdy: unexpected EOF](https://github.com/golang/go/issues/10539) **fixed**\n- [x/net/spdy: EOF](https://github.com/golang/go/issues/10540) **fixed**\n- [x/net/spdy: fatal error: runtime: out of memory](https://github.com/golang/go/issues/10542) **fixed**\n- [x/net/spdy: stream id zero is disallowed](https://github.com/golang/go/issues/10543) **fixed**\n- [x/net/spdy: processing of 35 bytes takes 7 seconds](https://github.com/golang/go/issues/10544) **fixed**\n- [x/net/spdy: makemap: size out of range](https://github.com/golang/go/issues/10545) **fixed**\n- [x/net/spdy: makeslice: len out of range](https://github.com/golang/go/issues/10547) **fixed**\n- [x/crypto/ssh: Server panic on invalid input](https://github.com/golang/go/issues/11348) **fixed**\n- [x/crypto/openpgp: ReadMessage(): Panic on invalid input in packet.nextSubpacket](https://github.com/golang/go/issues/11503) **fixed**\n- [x/crypto/openpgp: ReadMessage(): Panic on invalid input in packet.PublicKeyV3.setFingerPrintAndKeyId](https://github.com/golang/go/issues/11504) **fixed**\n- [x/crypto/openpgp: ReadMessage(): Panic on invalid input in math/big.nat.div](https://github.com/golang/go/issues/11505) **fixed**\n- [gccgo: bogus index out of bounds](https://github.com/golang/go/issues/11522) **fixed**\n- [gccgo: does not see stupidness of shift count](https://github.com/golang/go/issues/11524) **fixed**\n- [gccgo: bogus integer constant overflow](https://github.com/golang/go/issues/11525) **fixed**\n- [gccgo: segmentation fault](https://github.com/golang/go/issues/11526) **fixed**\n- [gccgo: segmentation fault (2)](https://github.com/golang/go/issues/11536) **fixed**\n- [gccgo: segmentation fault (3)](https://github.com/golang/go/issues/11558) **fixed**\n- [gccgo: segmentation fault (4)](https://github.com/golang/go/issues/11559) **fixed**\n- [gccgo: internal compiler error in set_type](https://github.com/golang/go/issues/11537) **fixed**\n- [gccgo: internal compiler error in global_variable_set_init](https://github.com/golang/go/issues/11541) **fixed**\n- [gccgo: internal compiler error: in wide_int_to_tree](https://github.com/golang/go/issues/11542) **fixed**\n- [gccgo: internal compiler error in wide_int_to_tree (2)](https://github.com/golang/go/issues/12618) **fixed**\n- [gccgo: internal compiler error in record_var_depends_on](https://github.com/golang/go/issues/11543) **fixed**\n- [gccgo: internal compiler error in Builtin_call_expression](https://github.com/golang/go/issues/11544) **fixed**\n- [gccgo: internal compiler error in check_bounds](https://github.com/golang/go/issues/11545) **fixed**\n- [gccgo: internal compiler error in do_determine_type](https://github.com/golang/go/issues/11546) **fixed**\n- [gccgo: internal compiler error in do_determine_type (2)](https://github.com/golang/go/issues/12937) **fixed**\n- [gccgo: internal compiler error in backend_numeric_constant_expression](https://github.com/golang/go/issues/11548) **fixed**\n- [gccgo: internal compiler error in type_size](https://github.com/golang/go/issues/11554) **fixed**\n- [gccgo: internal compiler error in type_size (2)](https://github.com/golang/go/issues/11555) **fixed**\n- [gccgo: internal compiler error in type_size (3)](https://github.com/golang/go/issues/11556) **fixed**\n- [gccgo: internal compiler error in do_get_backend](https://github.com/golang/go/issues/11560) **fixed**\n- [gccgo: internal compiler error in do_get_backend (2)](https://github.com/golang/go/issues/12325) **fixed**\n- [gccgo: internal compiler error in do_get_backend (3)](https://github.com/golang/go/issues/12617) **fixed**\n- [gccgo: internal compiler error in do_get_backend (4)](https://github.com/golang/go/issues/12939) **fixed**\n- [gccgo: internal compiler error in create_tmp_var](https://github.com/golang/go/issues/11568) **fixed**\n- [gccgo: internal compiler error in methods](https://github.com/golang/go/issues/11579) **fixed**\n- [gccgo: internal compiler error in do_flatten](https://github.com/golang/go/issues/11592) **fixed**\n- [gccgo: internal compiler error in do_flatten (2)](https://github.com/golang/go/issues/12319) **fixed**\n- [gccgo: internal compiler error in do_flatten (3)](https://github.com/golang/go/issues/12320) **fixed**\n- [gccgo: internal compiler error in declare_function](https://github.com/golang/go/issues/11557) **fixed**\n- [gccgo: internal compiler error: in define](https://github.com/golang/go/issues/12316) **fixed**\n- [gccgo: internal compiler error: in do_export](https://github.com/golang/go/issues/12321)\n- [gccgo: internal compiler error in do_lower](https://github.com/golang/go/issues/12615) **fixed**\n- [gccgo: internal compiler error in insert](https://github.com/golang/go/issues/12616) **fixed**\n- [gccgo: internal compiler error in uniform_vector_p](https://github.com/golang/go/issues/12935) **fixed**\n- [gccgo: accepts invalid UTF-8](https://github.com/golang/go/issues/11527) **fixed**\n- [gccgo: spurious expected newline error](https://github.com/golang/go/issues/11528) **fixed**\n- [gccgo: can apply ^ to true](https://github.com/golang/go/issues/11529) **fixed**\n- [gccgo: hangs](https://github.com/golang/go/issues/11530) **fixed**\n- [gccgo: hangs (2)](https://github.com/golang/go/issues/11531) **fixed**\n- [gccgo: hangs (3)](https://github.com/golang/go/issues/11539) **fixed**\n- [gccgo: rejects valid imaginary literal](https://github.com/golang/go/issues/11532) **fixed**\n- [gccgo: rejects valid fp literal](https://github.com/golang/go/issues/11533) **fixed**\n- [gccgo: accepts program with invalid identifier](https://github.com/golang/go/issues/11535) **fixed**\n- [gccgo: accepts program with invalid identifier (2)](https://github.com/golang/go/issues/11547) **fixed**\n- [gccgo: compiles weird construct](https://github.com/golang/go/issues/11561) **fixed**\n- [gccgo: can do bitwise or on fp constants](https://github.com/golang/go/issues/11566) **fixed**\n- [gccgo: treats nil as type](https://github.com/golang/go/issues/11567) **fixed**\n- [gccgo: does not understand greek capiltal letter yot](https://github.com/golang/go/issues/11569) **fixed**\n- [gccgo: does not understand CUNEIFORM SIGN DUG TIMES MI](https://github.com/golang/go/issues/12322) **fixed**\n- [gccgo: allows to refer to builtin function not in call expression](https://github.com/golang/go/issues/11570) **fixed**\n- [gccgo: bogus incompatible types in binary expression error](https://github.com/golang/go/issues/11572) **fixed**\n- [gccgo: allows multiple definitions of a function](https://github.com/golang/go/issues/11573) **fixed**\n- [gccgo: can shift by complex number](https://github.com/golang/go/issues/11574) **fixed**\n- [gccgo: knowns unknown escape sequence](https://github.com/golang/go/issues/11575) **fixed**\n- [gccgo: internal compiler error in start_function](https://github.com/golang/go/issues/11576) **fixed**\n- [gccgo: internal compiler error: in start_function (2)](https://github.com/golang/go/issues/12324) **fixed**\n- [gccgo: heap-buffer-overflow in Lex::skip_cpp_comment](https://github.com/golang/go/issues/11577) **fixed**\n- [gccgo: does not convert untyped complex 0i to int in binary operation involving an int](https://github.com/golang/go/issues/11563)\n- [gccgo: does not detect missing return](https://github.com/golang/go/issues/11591) **fixed**\n- [gccgo: invalid error message for valid conversion to complex64](https://github.com/golang/go/issues/11615)\n- [gccgo: can shift complex numbers](https://github.com/golang/go/issues/11616) **fixed**\n- [gccgo: does not error on unused var](https://github.com/golang/go/issues/12317) **fixed**\n- [gccgo: treats 0 as channel](https://github.com/golang/go/issues/12323) **fixed**\n- [gccgo: does not recognize unused import](https://github.com/golang/go/issues/12326) **fixed**\n- [gccgo: can shift by string](https://github.com/golang/go/issues/12936) **fixed**\n- [github.com/golang/protobuf: call of reflect.Value.SetMapIndex on zero Value](https://github.com/golang/protobuf/issues/27) **fixed**\n- [github.com/golang/protobuf: call of reflect.Value.Interface on zero Value in MarshalText](https://github.com/golang/protobuf/issues/33) **fixed**\n- [github.com/golang/protobuf: Invalid map is successfully decoded](https://github.com/golang/protobuf/issues/34)\n- [github.com/golang/protobuf: MarshalText incorrectly handles unknown bytes](https://github.com/golang/protobuf/issues/35)\n- [github.com/golang/protobuf: MarshalText fails and prints to stderr](https://github.com/golang/protobuf/issues/36)\n- [github.com/golang/protobuf: Unmarshaling errors for packed fields](https://github.com/golang/protobuf/issues/76) **fixed**\n- [Equal prints to stderr and fails on what's handled by Marshal/Unmarshal](https://github.com/golang/protobuf/issues/114)\n- [code.google.com/p/freetype-go: 42 crashers](https://code.google.com/p/freetype-go/issues/detail?id=17) [42 bugs]\n- [github.com/cryptix/wav: 2 panics in header decoding](https://github.com/cryptix/wav/commit/2f49a0df0d213ee323f694e7bdee8b8a097dc698#diff-f86b763600291cbceee077a33133434a) **fixed**\n- [github.com/spf13/hugo: 7 crashers](https://github.com/spf13/hugo/search?q=go-fuzz&type=Issues) **7 fixed**\n- [github.com/Sereal/Sereal: 8 crashers](https://github.com/Sereal/Sereal/commit/c254cc3f2c48caffee6cd04ea8100a0150357a44) **fixed**\n- [github.com/bradfitz/http2: Server.handleConn hangs](https://github.com/bradfitz/http2/issues/53) **fixed**\n- [github.com/bradfitz/http2: nil pointer dereference in hpack.HuffmanDecode](https://github.com/bradfitz/http2/issues/56) **fixed**\n- [github.com/bradfitz/http2: serverConn.readFrames goroutine leak](https://github.com/bradfitz/http2/issues/58)\n- [github.com/golang/snappy: index out of range panic](https://github.com/golang/snappy/issues/11) **fixed**\n- [github.com/bkaradzic/go-lz4: slice bounds out of range](https://github.com/bkaradzic/go-lz4/commit/b8d4dc7b31511bf5f39dfdb02d2ea7662eb8407c) **fixed**\n- [github.com/kurin/blazer: string escape/unescape edge-cases, need to escape filename in DownloadFileByName()](https://github.com/kurin/blazer/issues/32) **fixed**\n- [github.com/gocql/gocql: slice bounds out of range](https://github.com/gocql/gocql/commit/332853ab7b3c719dd67c657394139491c1f6deb7) **fixed**\n- [github.com/gocql/gocql: slice bounds out of range](https://github.com/gocql/gocql/commit/58d90fab97daa2d9edd6e7a1b2a22bee8ce12c72) **fixed**\n- [github.com/mdlayher/aoe: binary marshal/unmarshal inconsistency](https://github.com/mdlayher/aoe/commit/286c87727b95c9491e08cd909c93ac4a42218ee6) **fixed**\n- [github.com/mdlayher/arp: slice bounds out of range](https://github.com/mdlayher/arp/commit/f237799bcd57ff8bb4773eb819a4b852875f01d0) **fixed**\n- [github.com/mdlayher/ethernet: slice bounds out of range](https://github.com/mdlayher/ethernet/commit/da795f9b6b07b56d87722e5871e66519ac94f489) **fixed**\n- [github.com/mdlayher/ndp: multiple crashers](https://github.com/mdlayher/ndp/commit/4356b107e450bdeae53069b7016d6342f8a4230d) **fixed**\n- [github.com/mdlayher/netlink: slice bounds out of range](https://github.com/mdlayher/netlink/commit/1149baf1f02296bf9eac4a351d66861ecd7ed2a6) **fixed**\n- [github.com/mdlayher/netlink: slice bounds out of range](https://github.com/mdlayher/netlink/commit/2861fca5d483a101d696320cf2bbf54424a886db) **fixed**\n- [github.com/russross/blackfriday: index out of range panic in scanLinkRef](https://github.com/russross/blackfriday/issues/172) **fixed**\n- [github.com/russross/blackfriday: index out of range panic in isReference](https://github.com/russross/blackfriday/issues/173) **fixed**\n- [github.com/rwcarlsen/goexif: index out of range](https://github.com/rwcarlsen/goexif/issues/39)\n- [github.com/tdewolff/minify: 8 crashers](https://github.com/tdewolff/minify/wiki) **fixed**\n- [github.com/youtube/vitess/go/vt/sqlparser: index out of range](https://github.com/youtube/vitess/issues/767) **fixed**\n- [github.com/youtube/vitess/go/vt/sqlparser: statement serialized incorrectly](https://github.com/youtube/vitess/issues/797) **fixed**\n- [github.com/youtube/vitess/go/vt/sqlparser: statement serialized incorrectly (2)](https://github.com/youtube/vitess/issues/798)\n- [gopkg.in/mgo.v2/bson: slice bounds out of range](https://github.com/go-mgo/mgo/issues/116) **fixed**\n- [gopkg.in/mgo.v2/bson: Document is corrupted](https://github.com/go-mgo/mgo/issues/117) **fixed**\n- [gopkg.in/mgo.v2/bson: Attempted to marshal empty Raw document](https://github.com/go-mgo/mgo/issues/120) **fixed**\n- [cockroachdb/cockroach: crash on x % 0](https://github.com/cockroachdb/cockroach/pull/1799) **fixed**\n- [cockroachdb/cockroach: panic when dealing with empty sql ident](https://github.com/cockroachdb/cockroach/pull/1808) **fixed**\n- [cockroachdb/cockroach: parse literals more like Postgres](https://github.com/cockroachdb/cockroach/pull/1807) **fixed**\n- [cockroachdb/cockroach: SELECT (\"*\") parse oddities](https://github.com/cockroachdb/cockroach/issues/1810) **fixed**\n- [cockroachdb/cockroach: weird QualifiedName.Base panics on reproduce](https://github.com/cockroachdb/cockroach/issues/1938)\n- [github.com/google/open-location-code: Extremely long codes can cause underflow errors](https://github.com/google/open-location-code/issues/12)\n- [github.com/akrennmair/gopcap: incorrectly formed IP, UDP, TCP, ICMP packets can cause out of range errors](https://github.com/akrennmair/gopcap/commit/00e11033259acb75598ba416495bb708d864a010) **fixed**\n- [github.com/gogo/protobuf: gogofast generates Unmarshal code that can panic](https://github.com/gogo/protobuf/issues/86) **fixed**\n- [github.com/DHowett/go-plist: Various panics found through go-fuzz](https://github.com/DHowett/go-plist/issues/15)\n- [github.com/streadway/amqp: go-fuzz fixes](https://github.com/streadway/amqp/pull/151)\n- [github.com/andybalholm/cascadia: panic when parsing selectors like `:contains(`](https://github.com/andybalholm/cascadia/commit/3ad29d1ad1c4f2023e355603324348cf1f4b2d48) **fixed**\n- [github.com/Azure/go-pkcs12: panic on malformed certificates](https://github.com/Azure/go-pkcs12/issues/25)\n- [github.com/nats-io/gnatsd: panic on malformed input](https://github.com/nats-io/gnatsd/issues/95)\n- [github.com/miekg/dns: 8 crashers](https://github.com/miekg/dns/pull/237) **fixed**\n- [github.com/influxdb/influxdb: index out of range](https://github.com/influxdb/influxdb/pull/3570) **fixed**\n- [collectd.org/network: 2 crashers](https://github.com/collectd/go-collectd/pull/6) **fixed**\n- [collectd.org/network: index out of range](https://github.com/collectd/go-collectd/issues/10) **fixed**\n- [github.com/arolek/ase: 2 crashers](https://github.com/arolek/ase/pull/18) **fixed**\n- [github.com/lytics/confl: infinite loop on malformed input](https://github.com/lytics/confl/issues/6) **fixed**\n- [github.com/zeebo/bencode: reject strings with negative length](https://github.com/zeebo/bencode/pull/15) **fixed**\n- [github.com/hydrogen18/stalecucumber: 4 crashers](https://github.com/hydrogen18/stalecucumber/pull/5)\n- [github.com/gonum/blas: cgo indexing error](https://github.com/gonum/blas/issues/133) **fixed**\n- [OpenBLAS: incorrect idamax with NaN value](https://github.com/xianyi/OpenBLAS/issues/624)\n- [github.com/eaburns/flac: 3 crashers](https://github.com/eaburns/flac/pull/6)\n- [github.com/yvasiyarov/php_session_decoder: 4 crashers](https://github.com/yvasiyarov/php_session_decoder/pull/15)\n- [xi2.org/x/xz: index out of bounds](https://github.com/xi2/xz/issues/3) **fixed**\n- [github.com/pierrec/lz4: 2 crashers](https://github.com/pierrec/lz4/commit/0b67ae4bb1ab03691079e38dddbc3909d68de64f) **fixed**\n- [github.com/dustin/go-coap: slice bounds out of range (1)](https://github.com/dustin/go-coap/commit/979f9a1787fc3091ba5c337a6d1d903432ce2007) **fixed**\n- [github.com/dustin/go-coap: slice bounds out of range (2)](https://github.com/dustin/go-coap/commit/a2260b92ac405c9c63c4a89c15bb705a3f2928bf) **fixed**\n- [github.com/dgryski/go-quicklz: many array-out-of-bounds issues](https://github.com/dgryski/go-quicklz/commit/6897f36a2bb707fe5b294fb9c06b7e086ab9503b) **fixed**\n- [github.com/rasky/go-lzo: possible infinite loop with single byte input](https://github.com/rasky/go-lzo/commit/22d79fde8006d605b307e3d58b775d9c1f756d52) **fixed**\n- [github.com/ulikunitz/xz: panic in lzma.writeRep](https://github.com/ulikunitz/xz/issues/3)\n- [github.com/Preetam/sflow: excessive memory consumption](https://github.com/Preetam/sflow/issues/29) **fixed**\n- [github.com/hashicorp/go-version: unhandled value out of range](https://github.com/hashicorp/go-version/pull/11) **fixed**\n- [github.com/atlassian/gostatsd: Return an error instead of nil when parseline gets nil/empty input](https://github.com/atlassian/gostatsd/pull/5)\n- [github.com/flynn/flynn/pkg/syslog/rfc5424: off-by-one](https://github.com/flynn/flynn/commit/a6bc68a86a36652b1d06d66052da67c1425faa2f) **fixed**\n- [github.com/flynn/flynn/json5: decoder out of sync with scanner](https://github.com/flynn/json5/commit/29a96735397827a2923c9b669ee3188d601d9153) **fixed**\n- [github.com/flynn/flynn/json5: broken carriage return parsing](https://github.com/flynn/json5/commit/7620272ed63390e979cf5882d2fa0506fe2a8db5) **fixed**\n- [github.com/ipfs/go-ipfs: nil pointer deference in DHT RPC handler](https://github.com/ipfs/go-ipfs/pull/3200) **fixed**\n- [github.com/buger/goreplay: fix panic in http headers parser function](https://github.com/buger/goreplay/pull/411) **fixed**\n- [github.com/digitalocean/captainslog: incomplete timestamp caused panic](https://github.com/digitalocean/captainslog/pull/27/commits/d01c85621defb6fbbde22071de18f69bb3a74836) **fixed**\n- [github.com/jlaffaye/ftp: panic: runtime error: index out of range](https://github.com/jlaffaye/ftp/issues/97) **fixed**\n- [github.com/unidoc/unidoc: panic: interface conversion: pdf.PdfObject is nil, not *pdf.PdfObjectInteger](https://github.com/unidoc/unidoc/issues/77) **fixed**\n- [github.com/unidoc/unidoc: panic: runtime error: invalid memory address or nil pointer dereference](https://github.com/unidoc/unidoc/issues/79) **fixed**\n- [github.com/unidoc/unidoc: runtime: goroutine stack exceeds 1000000000-byte limit](https://github.com/unidoc/unidoc/issues/80) **fixed**\n- [github.com/spenczar/tdigest: check slice bounds when unmarshaling](https://github.com/spenczar/tdigest/commit/91fdfcefc42381c63363418ad38d7d33233f79cd) **fixed**\n- [github.com/spenczar/tdigest: check expected invariants while unmarshaling](https://github.com/spenczar/tdigest/commit/1ee0185dad51692b26b4ee6f9e111349f0cdb581) **fixed**\n- [github.com/vcabbage/amqp: index out of range](https://github.com/vcabbage/amqp/commit/69a58fc911413722779226664a5a858b84758f94) **fixed**\n- [github.com/gomarkdown/markdown: inifinite loop](https://github.com/gomarkdown/markdown/commit/5d96569c5a0d3cd46d961eddbb61e936e627774c) **fixed**\n- [github.com/gomarkdown/markdown: inifinite loop](https://github.com/gomarkdown/markdown/commit/e0fc813169b926a2182bc6554888eb37d12261f7) **fixed**\n- [github.com/gomarkdown/markdown: index out of range](https://github.com/gomarkdown/markdown/commit/5dd4b50fe81eda60f173e242ece05f24c5cc5cec) **fixed**\n- [github.com/hajimehoshi/go-mp3: index out of range (1)](https://github.com/hajimehoshi/go-mp3/commit/22bc0be280079723dbb8e10295db01e925dc5640) **fixed**\n- [github.com/hajimehoshi/go-mp3: index out of range (2)](https://github.com/hajimehoshi/go-mp3/commit/81bb838ef7ce492a3ea9d097a781ae1ed7f318b9) **fixed**\n- [github.com/hajimehoshi/go-mp3: index out of range (3)](https://github.com/hajimehoshi/go-mp3/commit/3c185f92b8dbceefa913b64cae634ba47f452769) **fixed**\n- [github.com/dhowden/tag: slice bounds out of range (1)](https://github.com/dhowden/tag/commit/737d3560ddb3e8e7319a5e9c494ccd749150d675) **fixed**\n- [github.com/dhowden/tag: slice bounds out of range (2)](https://github.com/dhowden/tag/commit/1582ebc2a4525aeaccf7138ffd37e56f5117e49e) **fixed**\n- [github.com/dhowden/tag: len out of range (3)](https://github.com/dhowden/tag/commit/d449289c5e6fec9ad6a68a9e850f22fe14fa7c97) **fixed**\n- [github.com/dhowden/tag: slice bounds out of range (4)](https://github.com/dhowden/tag/commit/d2206af145611b630d612027486ffd9129bd3e09) **fixed**\n- [github.com/tealeg/xlsx: slice bounds out of range (1)](https://github.com/tealeg/xlsx/commit/d40e2bb185733dd4bc3c4a1929c35ee844ed3379) **fixed**\n- [github.com/hashicorp/hcl: crasher (logic error)](https://github.com/hashicorp/hcl/pull/239) **fixed**\n- [github.com/hashicorp/hcl: crasher (off-by-one)](https://github.com/hashicorp/hcl/pull/240) **fixed**\n- [github.com/hashicorp/hcl: format produces unparsable output (1)](https://github.com/hashicorp/hcl/pull/241) **fixed**\n- [github.com/hashicorp/hcl: format produces unparsable output (2)](https://github.com/hashicorp/hcl/pull/243) **fixed**\n- [github.com/hashicorp/hcl: format produces unparsable output (3)](https://github.com/hashicorp/hcl/pull/244) **fixed**\n- [github.com/hashicorp/hcl: format produces unparsable output (4)](https://github.com/hashicorp/hcl/pull/245) **fixed**\n- [github.com/francoispqt/gojay: panic on malformed JSON integers](https://github.com/francoispqt/gojay/issues/27) **fixed**\n- [github.com/francoispqt/gojay: panic on malformed JSON floats](https://github.com/francoispqt/gojay/issues/32) **fixed**\n- [github.com/eapache/go-xerial-snappy multiple panics with malformed inputs](https://github.com/eapache/go-xerial-snappy/commit/58803384a8be76cd0f84789b302c7b52d791d95f) **fixed**\n- [github.com/trustelem/zxcvbn: multiple panics in password strength estimator](https://github.com/trustelem/zxcvbn/issues/1) **fixed**\n- https://github.com/google/syzkaller: 6 crashers (\n[1](https://github.com/google/syzkaller/commit/7c7ded697e6322b0975f061b7e268fe44f585dab),\n[2](https://github.com/google/syzkaller/commit/3b37734422dc0cb40100287bbb3628d8d946c271),\n[3](https://github.com/google/syzkaller/commit/f400a0da0fcd3e4d27d915b57c54f504813ef1d3),\n[4](https://github.com/google/syzkaller/commit/967dc02d70f8e3d027738295977762cd4fbed5c7),\n[5](https://github.com/google/syzkaller/commit/78b7ec0fbe23a5c674401123053d6372ea3ca9c6),\n[6](https://github.com/google/syzkaller/commit/413e41473838fb74ccc081784afd6ddbbd44b797))\n- [github.com/chai2010/guetzli-go: index out of range](https://github.com/chai2010/guetzli-go/issues/11)\n- [github.com/pixiv/go-libjpeg: segmentation violation (1)](https://github.com/pixiv/go-libjpeg/issues/51) **fixed**\n- [github.com/pixiv/go-libjpeg: segmentation violation (2)](https://github.com/pixiv/go-libjpeg/issues/58)\n- [github.com/pixiv/go-libjpeg: panic on encoding after decoding](https://github.com/pixiv/go-libjpeg/issues/55) **fixed**\n- [github.com/z7zmey/php-parser: index out of range and nil pointer dereference](https://github.com/z7zmey/php-parser/issues/98)\n- [github.com/uber/makisu: index out of range (1)](https://github.com/uber/makisu/issues/266) **fixed**\n- [github.com/uber/makisu: index out of range (2)](https://github.com/uber/makisu/issues/271)\n\n**If you find some bugs with go-fuzz and are comfortable with sharing them, I would like to add them to this list.** Please either send a pull request for README.md (preferable) or file an issue. If the source code is closed, you can say just \"found N bugs in project X\". Thank you.\n", "release_dates": []}, {"name": "go-stateproof-verification", "description": null, "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "\nState Proof Verification\n====================\n\n\nA Go implementation of functionality required to verify Algorand state proofs.\nThe functions exported in stateproof.verifier provide the verification interface.\n\n# Install\n\n```bash\ngo get github.com/algorand/go-stateproof-verification\n```\nAlternatively the same can be achieved if you use import in a package:\n\n```bash\nimport \"github.com/algorand/go-stateproof-verification\"\n```\nand run go get without parameters.\n\n# Usage\n\nCreate a verifier and verify state proof messages using the appropriate state proofs.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"github.com/algorand/go-stateproof-verification/stateproof\"\n\t\"github.com/algorand/go-stateproof-verification/stateproofcrypto\"\n\n\t\"github.com/algorand/go-algorand-sdk/crypto\"\n\t\"github.com/algorand/go-algorand-sdk/types\"\n)\n\nfunc main() {\n\t// verifiedVotersCommitment is the VotersCommitment extracted from the previously verified state proof message.\n\tvar verifiedVotersCommitment stateproofcrypto.GenericDigest\n\t// verifiedVotersCommitment is the LnProvenWeight extracted from the previously verified state proof message.\n\tvar verifiedLnProvenWeight uint64\n\t\n\t// We create a verifier using the aforementioned previously verified data.\n\tverifier := stateproof.MkVerifierWithLnProvenWeight(verifiedVotersCommitment, verifiedLnProvenWeight)\n\n\t// stateProof is the proof used in verification, retrieved from the Algorand blockchain using the API.\n\tvar stateProof stateproof.StateProof\n\t// stateProofMessage is the message the proof attests to, retrieved from the Algorand blockchain using the API.\n\tvar stateProofMessage types.Message\n\n\t// We hash the state proof message using the Algorand SDK. the resulting hash is of the form\n\t// sha256(\"spm\" || msgpack(stateProofMessage)).\n\tmessageHash := stateproofcrypto.MessageHash(crypto.HashStateProofMessage(&stateProofMessage))\n\n\t// We verify the message using the message hash and the state proof.\n\terr := verifier.Verify(stateProofMessage.LastAttestedRound, messageHash, &stateProof)\n\tif err != nil {\n\t\tfmt.Printf(\"State proof verification failed: %s\\n\", err)\n\t}\n}\n\n\n```\n\n# Testing\n\n```go\ngo test ./test\n```", "release_dates": ["2022-09-01T17:52:02Z"]}, {"name": "go-sumhash", "description": null, "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "\nSumhash\n====================\n\n\nA Go implementation of Algorand\u2019s subset-sum hash function.\nThe library exports the subset sum hash function via a `hash.Hash` interface.\n\n\n# Install\n\n```bash\ngo get github.com/algorand/go-sumhash\n```\nAlternatively the same can be achieved if you use import in a package:\n\n```bash\nimport \"github.com/algorand/go-sumhash\"\n```\nand run go get without parameters.\n\n# Usage \n\nConstruct a sumhash instance with block size of 512.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/algorand/go-sumhash\"\n)\n\nfunc main() {\n\th := sumhash.New512(nil)\n\tinput := []byte(\"sumhash input\")\n\t_, _ = h.Write(input)\n\n\tsum := h.Sum(nil)\n\tfmt.Printf(\"subset sum hash value: %X\", sum)\n}\n\n```\n\n# Testing\n\n```go\ngo test ./...\n```\n\n# Spec\n\nThe specification of the function as well as the security parameters\ncan be found [here](https://github.com/algorand/go-sumhash/tree/master/spec)", "release_dates": ["2022-02-25T15:55:52Z"]}, {"name": "go-yamux", "description": null, "language": null, "license": {"key": "mpl-2.0", "name": "Mozilla Public License 2.0", "spdx_id": "MPL-2.0", "url": "https://api.github.com/licenses/mpl-2.0", "node_id": "MDc6TGljZW5zZTE0"}, "readme": "# Yamux\n\nYamux (Yet another Multiplexer) is a multiplexing library for Golang.\nIt relies on an underlying connection to provide reliability\nand ordering, such as TCP or Unix domain sockets, and provides\nstream-oriented multiplexing. It is inspired by SPDY but is not\ninteroperable with it.\n\nYamux features include:\n\n* Bi-directional streams\n  * Streams can be opened by either client or server\n  * Server-side push support\n* Flow control\n  * Avoid starvation\n  * Back-pressure to prevent overwhelming a receiver\n* Keep Alives\n  * Enables persistent connections over a load balancer\n* Efficient\n  * Enables thousands of logical streams with low overhead\n\n## Documentation\n\nFor complete documentation, see the associated [Godoc](http://godoc.org/github.com/libp2p/go-yamux).\n\n## Specification\n\nThe full specification for Yamux is provided in the `spec.md` file.\nIt can be used as a guide to implementors of interoperable libraries.\n\n## Usage\n\nUsing Yamux is remarkably simple:\n\n```go\n\nfunc client() {\n    // Get a TCP connection\n    conn, err := net.Dial(...)\n    if err != nil {\n        panic(err)\n    }\n\n    // Setup client side of yamux\n    session, err := yamux.Client(conn, nil)\n    if err != nil {\n        panic(err)\n    }\n\n    // Open a new stream\n    stream, err := session.Open()\n    if err != nil {\n        panic(err)\n    }\n\n    // Stream implements net.Conn\n    stream.Write([]byte(\"ping\"))\n}\n\nfunc server() {\n    // Accept a TCP connection\n    conn, err := listener.Accept()\n    if err != nil {\n        panic(err)\n    }\n\n    // Setup server side of yamux\n    session, err := yamux.Server(conn, nil)\n    if err != nil {\n        panic(err)\n    }\n\n    // Accept a stream\n    stream, err := session.Accept()\n    if err != nil {\n        panic(err)\n    }\n\n    // Listen for a message\n    buf := make([]byte, 4)\n    stream.Read(buf)\n}\n\n```\n\n---\nThe last gx published version of this module was: 1.1.5: QmUNMbRUsVYHi1D14annF7Rr7pQAX7TNLwpRCa975ojKnw\n", "release_dates": []}, {"name": "graphtrace", "description": "System for tracing timing and propagation of messages across a mesh network", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# graphtrace tool for tracing message propagation around a software mesh network\n\n## messages\n\n\"varuint\" is as per Google Protobuf or Go encoding/binary VarUint\n\nepoch start is 2020-01-01 00:00:00 UTC\n\ntime is varuint microseconds since epoch\n\nPing - acheive Network Time Protocol\n\n```\n0x01\nsender's time\nprevious message's time or 0\n```\n\nTrace\n\n```\n0x02\nsender's time\nmessage id: varuint length followed by bytes; effectively length-prefixed []byte\n```\n", "release_dates": ["2022-03-22T16:53:34Z"]}, {"name": "graviton", "description": "\ud83e\uddd1\u200d\ud83d\udd2c verify your TEAL program by experiment and observation", "language": "Jupyter Notebook", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "<!-- markdownlint-disable no-inline-html -->\n<!-- markdownlint-disable first-line-h1 -->\n<p align=\"center\"><img  width=100%  src=\"https://infura-ipfs.io/ipfs/QmRVnM9EaCk3u9p42b5ppLjwxUPv54EDQihmkNyFxqKuVi\"  border=\"0\" /></p>\n\n<p align=\"center\">\n    <a href=\"https://algorand.com\"><img src=\"https://img.shields.io/badge/Powered by-Algorand-teal.svg\" alt=\"Algorand\" /></a>\n    <a><img src=\"https://visitor-badge.glitch.me/badge?page_id=algorand.graviton&right_color=green\" /></a>\n</p>\n\n# About\n\nGraviton is a software toolkit for blackbox testing of smart contracts written in TEAL.\n\n## [Tutorial](./graviton/README.md)\n\n## Local Installation\n\nThe following instructions assume that you have `make` available in your local environment. In Mac OS and Linux this is most likely already available and in Windows one way to install is with [chocolatey](https://chocolatey.org/) and the command `choco install make`.\n\nTo install all dependencies:\n\n```sh\nmake pip-notebooks\n```\n\n## Running Blackbox Integration Tests against a Sandbox\n\n### Prereq - Install and Symbolically Link to the Sandbox\n\nIf you would like to use the [Makefile](./Makefile) without modification and with full functionality, you should create a symbolic link to  [the algorand sandbox repo](https://github.com/algorand/sandbox) as described here. There are many ways to accomplish this. Assuming you have cloned ***the sandbox*** into the path  `/path/to/algorand/sandbox/` and that you've `cd`'ed into the cloned `graviton` directory you should create a symbolic link as follows:\n\n#### Linux / Mac OS\n\n```sh\nln -s /path/to/algorand/sandbox/ sandbox\n```\n\n#### Windows 10+\n\n```sh\nmklink sandbox \\path\\to\\algorand\\sandbox\n```\n\n<!-- TODO: Re-do this using the docker image as in PyTEAL  -->\n### Test the Sandbox\n\nWith your sandbox running to test that the sandbox is running properly, use the following:\n\n```sh\nmake local-sandbox-test\n```\n\n### Run the Integration Tests\n\n```sh\nmake integration-test\n```\n\n## Running and Testing Jupyter Notebooks\n\nTo run the notebook `notebooks/quadratic_factoring_game.ipynb` for example:\n\n```sh\nmake local-notebook NOTEBOOK=notebooks/quadratic_factoring_game.ipynb\n```\n\nTo non-interactively run all the jupyter notebook tests:\n\n```sh\nmake notebooks-test\n```\n\n## Ensuring that all is Copacetic Before Pushing to Github\n\nTo test in your local environment that everything is looking good before pushing to Github, it is recommended that you run `make all-tests`\n\nIf you would like to simulate the github actions locally, you'll need to install [nektos act](https://github.com/nektos/act/wiki/Installation). On Mac OS with [Docker](https://docs.docker.com/desktop/mac/install/) previously installed you can use `brew install act`; on the other hand, on Linux and Windows follow the installation instructions in the nextos repo link above.\n\nOnce `act` is available you can simulate all the github actions integration tests with:\n\n```sh\nmake local-gh-simulate\n```\n", "release_dates": ["2023-02-06T21:52:09Z", "2023-01-17T19:23:35Z", "2023-01-11T22:13:23Z", "2023-01-03T22:23:12Z", "2022-12-14T14:58:54Z", "2022-11-08T15:32:49Z"]}, {"name": "indexer", "description": "searchable history and current state", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "<div style=\"text-align:center\" align=\"center\">\n  <picture>\n    <img src=\"./docs/assets/algorand_logo_mark_black.svg\" alt=\"Algorand\" width=\"400\">\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"./docs/assets/algorand_logo_mark_white.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"./docs/assets/algorand_logo_mark_black.svg\">\n  </picture>\n\n[![CircleCI](https://img.shields.io/circleci/build/github/algorand/indexer/develop?label=develop)](https://circleci.com/gh/algorand/indexer/tree/develop)\n[![CircleCI](https://img.shields.io/circleci/build/github/algorand/indexer/master?label=master)](https://circleci.com/gh/algorand/indexer/tree/master)\n![Github](https://img.shields.io/github/license/algorand/indexer)\n[![Contribute](https://img.shields.io/badge/contributor-guide-blue?logo=github)](https://github.com/algorand/go-algorand/blob/master/CONTRIBUTING.md)\n</div>\n\n# Algorand Indexer\n\nThe Indexer is an API that provides search capabilities for Algorand on-chain data. Data is written by [Conduit](https://github.com/algorand/conduit/blob/master/docs/tutorials/IndexerWriter.md) to populate a PostgreSQL compatible database.\n\n## Building from source ##\n\nDevelopment is done using the [Go Programming Language](https://golang.org/), the version is specified in the project's [go.mod](go.mod) file.\n\nRun `make` to build Indexer, the binary is located at `cmd/algorand-indexer/algorand-indexer`.\n\n# Requirements\n\nAll recommendations here should be used as a starting point. Further benchmarking should be done to verify performance is acceptible for any application using Indexer.\n\n## Versions\n\n* Database: [Postgres 14](https://www.postgresql.org/download/)\n\n## System\n\nFor a simple deployment the following configuration works well:\n* Network: Indexer, Conduit, Algod and PostgreSQL should all be on the same network.\n* Indexer: 1 CPU and 1GB of ram. Scale up for systems with high query volume.\n* Conduit + Algod: 4 CPU and 8 GB of ram.\n  * Storage: algod follower nodes, 40 GiB, 3000 IOPS minimum.\n  * Deployments allocating less ram might work in conjunction with [GOMEMLIMIT](https://pkg.go.dev/runtime@master#hdr-Environment_Variables) for Algod (and even Conduit). This configuration is not tested, so use with caution and monitor closely.\n* Database: When hosted on AWS a `db.r5.xlarge` instance works well.\n\nA database with replication can be used to scale read volume. Configure a single Conduit writer with multiple Indexer readers.\n\n# Quickstart\n\nIndexer is part of the [sandbox](https://github.com/algorand/sandbox) private network configurations, which you can use to get started.\n\n# Features\n\n- Search and filter accounts, transactions, assets, and asset balances with many different parameters.\n- Pagination of results.\n- Enriched transaction and account data:\n  - Confirmation round (block containing the transaction)\n  - Confirmation time\n  - Signature type\n  - Close amounts\n  - Create/delete rounds.\n- Human readable field names instead of the space optimized protocol level names.\n\n# Contributing\n\nContributions welcome! Please refer to our [CONTRIBUTING](https://github.com/algorand/go-algorand/blob/master/CONTRIBUTING.md) document.\n\n<!-- USAGE_START_MARKER -->\n\n# Usage\n\nConfigure Indexer with a PostgreSQL compatible database. Conduit should be used to populate the database and one or more Indexer daemons can be run to serve the API. For further isolation, a `readonly` user can be created for the database.\n```\n~$ algorand-indexer daemon --data-dir /tmp --postgres \"user=readonly password=YourPasswordHere {other connection string options for your database}\"\n```\n\nThe Postgres backend does specifically note the username \"readonly\" and changes behavior to avoid writing to the database. But the primary benefit is that Postgres can enforce restricted access to this user. This can be configured with:\n```sql\nCREATE USER readonly LOGIN PASSWORD 'YourPasswordHere';\nREVOKE ALL ON ALL TABLES IN SCHEMA public FROM readonly;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO readonly;\n```\n\n## Authorization\n\nWhen `--token your-token` is provided, an authentication header is required. For example:\n```\n~$ curl localhost:8980/transactions -H \"X-Indexer-API-Token: your-token\"\n```\n\n## Disabling Parameters\n\nThe Indexer has the ability to selectively enable or disable parameters for endpoints.  Disabling a \"required\" parameter will result in the entire endpoint being disabled while disabling an \"optional\" parameter will cause an error to be returned only if the parameter is provided.\n\n### Viewing the Current Configuration\n\nThe Indexer has a default set of disabled parameters.  To view the disabled parameters issue:\n```\n~$ algorand-indexer api-config\n```\n\nThis will output ONLY the disabled parameters in a YAML configuration.  To view all parameters (both enabled and disabled) issue:\n\n```\n~$ algorand-indexer api-config --all\n```\n\n### Interpreting The Configuration\n\nBelow is a snippet of the output from `algorand-indexer api-config`:\n\n```\n/v2/accounts:\n    optional:\n        - currency-greater-than: disabled\n        - currency-less-than: disabled\n/v2/assets/{asset-id}/transactions:\n    optional:\n        - note-prefix: disabled\n        - tx-type: disabled\n        - sig-type: disabled\n        - before-time: disabled\n        - after-time: disabled\n        - currency-greater-than: disabled\n        - currency-less-than: disabled\n        - address-role: disabled\n        - exclude-close-to: disabled\n        - rekey-to: disabled\n    required:\n        - asset-id: disabled\n```\n\nSeeing this we know that the `/v2/accounts` endpoint will return an error if either `currency-greater-than` or `currency-less-than` is provided.  Additionally, because a \"required\" parameter is provided for `/v2/assets/{asset-id}/transactions` then we know this entire endpoint is disabled.  The optional parameters are provided so that you can understand what else is disabled if you enable all \"required\" parameters.\n\n**NOTE: An empty parameter configuration file results in all parameters being ENABLED.**\n\nFor more information on disabling parameters see the [Disabling Parameters Guide](docs/DisablingParametersGuide.md).\n\n## Metrics\n\nThe `/metrics` endpoint is configured with the `--metrics-mode` option and configures if and how [Prometheus](https://prometheus.io/) formatted metrics are generated.\n\nThere are different settings:\n\n| Setting | Description |\n| ------- | ----------- |\n| ON      | Metrics for each REST endpoint in addition to application metrics. |\n| OFF     | No metrics endpoint. |\n| VERBOSE | Separate metrics for each combination of query parameters. This option should be used with caution, there are many combinations of query parameters which could cause extra memory load depending on usage patterns. |\n\n## Connection Pool Settings\n\nOne can set the maximum number of connections allowed in the local connection pool by using the `--max-conn` setting.  It is recommended to set this number to be below the database server connection pool limit.\n\nIf the maximum number of connections/active queries is reached, subsequent connections will wait until a connection becomes available, or timeout according to the read-timeout setting.\n\n# Settings\n\nSettings can be provided from the command line, a configuration file, or an environment variable\n\n| Command Line Flag (long)      | (short) | Config File                   | Environment Variable                  |\n|-------------------------------|---------|-------------------------------|---------------------------------------|\n| postgres                      | P       | postgres-connection-string    | INDEXER_POSTGRES_CONNECTION_STRING    |\n| data-dir                      | i       | data                          | INDEXER_DATA                          |\n| pidfile                       |         | pidfile                       | INDEXER_PIDFILE                       |\n| server                        | S       | server-address                | INDEXER_SERVER_ADDRESS                |\n| token                         | t       | api-token                     | INDEXER_API_TOKEN                     |\n| metrics-mode                  |         | metrics-mode                  | INDEXER_METRICS_MODE                  |\n| logfile                       | f       | logfile                       | INDEXER_LOGFILE                       |\n| loglevel                      | l       | loglevel                      | INDEXER_LOGLEVEL                      |\n| max-conn                      |         | max-conn                      | INDEXER_MAX_CONN                      |\n| write-timeout                 |         | write-timeout                 | INDEXER_WRITE_TIMEOUT                 |\n| read-timeout                  |         | read-timeout                  | INDEXER_READ_TIMEOUT                  |\n| max-api-resources-per-account |         | max-api-resources-per-account | INDEXER_MAX_API_RESOURCES_PER_ACCOUNT |\n| max-transactions-limit        |         | max-transactions-limit        | INDEXER_MAX_TRANSACTIONS_LIMIT        |\n| default-transactions-limit    |         | default-transactions-limit    | INDEXER_DEFAULT_TRANSACTIONS_LIMIT    |\n| max-accounts-limit            |         | max-accounts-limit            | INDEXER_MAX_ACCOUNTS_LIMIT            |\n| default-accounts-limit        |         | default-accounts-limit        | INDEXER_DEFAULT_ACCOUNTS_LIMIT        |\n| max-assets-limit              |         | max-assets-limit              | INDEXER_MAX_ASSETS_LIMIT              |\n| default-assets-limit          |         | default-assets-limit          | INDEXER_DEFAULT_ASSETS_LIMIT          |\n| max-balances-limit            |         | max-balances-limit            | INDEXER_MAX_BALANCES_LIMIT            |\n| default-balances-limit        |         | default-balances-limit        | INDEXER_DEFAULT_BALANCES_LIMIT        |\n| max-applications-limit        |         | max-applications-limit        | INDEXER_MAX_APPLICATIONS_LIMIT        |\n| default-applications-limit    |         | default-applications-limit    | INDEXER_DEFAULT_APPLICATIONS_LIMIT    |\n| enable-all-parameters         |         | enable-all-parameters         | INDEXER_ENABLE_ALL_PARAMETERS         |\n\n## Command line\n\nThe command line arguments always take priority over the config file and environment variables.\n\n```\n~$ ./algorand-indexer daemon --data-dir /tmp --pidfile /var/lib/algorand/algorand-indexer.pid --postgres \"host=mydb.mycloud.com user=postgres password=password dbname=mainnet\"`\n```\n\n## Data Directory\n\nThe Indexer data directory is the location where the Indexer can store and/or load data needed for configuration. The data directory is effectively stateless, so it does not need to be persisted across deployments as long as the configuration is supplied.\n\n**It is a required argument for Indexer daemon operation. Supply it to the Indexer via the `--data-dir`/`-i` flag.**\n\nFor more information on the data directory see [Indexer Data Directory](docs/DataDirectory.md).\n\n### Auto-Loading Configuration\n\nThe Indexer will scan the data directory at startup and load certain configuration files if they are present.  The files are as follows:\n\n- `indexer.yml` - Indexer Configuration File\n- `api_config.yml` - API Parameter Enable/Disable Configuration File\n\n**NOTE:** It is not allowed to supply both the command line flag AND have an auto-loading configuration file in the data directory.  Doing so will result in an error.\n\nTo see an example of how to use the data directory to load a configuration file check out the [Disabling Parameters Guide](docs/DisablingParametersGuide.md).\n\n## Example environment variable\n\nEnvironment variables are also available to configure indexer. Environment variables override settings in the config file and are overridden by command line arguments.\n\nThe same indexer configuration from earlier can be made in bash with the following:\n```\n~$ export INDEXER_POSTGRES_CONNECTION_STRING=\"host=mydb.mycloud.com user=postgres password=password dbname=mainnet\"\n~$ export INDEXER_PIDFILE=\"/var/lib/algorand/algorand-indexer.pid\"\n~$ export INDEXER_DATA=\"/tmp\"\n~$ ./algorand-indexer daemon\n```\n\n\n## Configuration file\nDefault values are placed in the configuration file. They can be overridden with environment variables and command line arguments.\n\nThe configuration file must named **indexer.yml** and placed in the data directory (see above). The filepath may be set on the CLI using `--configfile` or `-c` but this functionality is deprecated.\n\nHere is an example **indexer.yml** file:\n```\npostgres-connection-string: \"host=mydb.mycloud.com user=postgres password=password dbname=mainnet\"\npidfile: \"/var/lib/algorand/algorand-indexer.pid\"\n```\n\nPlace this file in the data directory (`/tmp/data-dir` in this example) and supply it to the Indexer daemon:\n```\n~$ ./algorand-indexer daemon --data-dir /tmp/data-dir\n```\n\n\n# Systemd\n\n`/lib/systemd/system/algorand-indexer.service` can be partially overridden by creating `/etc/systemd/system/algorand-indexer.service.d/local.conf`. The most common things to override will be the command line and pidfile. The overriding local.conf file might be this:\n\n```\n[Service]\nExecStart=\nExecStart=/usr/bin/algorand-indexer daemon --data-dir /tmp --pidfile /var/lib/algorand/algorand-indexer.pid --postgres \"host=mydb.mycloud.com user=postgres password=password dbname=mainnet\"\nPIDFile=/var/lib/algorand/algorand-indexer.pid\n\n```\n\nThe systemd unit file can be found in source at [misc/systemd/algorand-indexer.service](misc/systemd/algorand-indexer.service)\n\nNote that the service assumes an `algorand` group and user. If the [Algorand package](https://github.com/algorand/go-algorand/) has already been installed on the same machine, this group and user has already been created.  However, if the Indexer is running stand-alone, the group and user will need to be created before starting the daemon:\n\n```\nadduser --system --group --home /var/lib/algorand --no-create-home algorand\n```\n\nOnce configured, turn on your daemon with:\n\n```bash\nsudo systemctl enable algorand-indexer\nsudo systemctl start algorand-indexer\n```\n\nIf you wish to run multiple indexers on one server under systemd, see the comments in `/lib/systemd/system/algorand-indexer@.service` or [misc/systemd/algorand-indexer@.service](misc/systemd/algorand-indexer@.service)\n\n# Unique Database Configurations\n\n## Load balancing\nIf Conduit is deployed with a clustered database using multiple readers behind a load balancer, query discrepancies are possible due to database replication lag. Users should check the `current-round` response field and be prepared to retry queries when stale data is detected.\n\n## Custom indices\nDifferent application workloads will require different custom indices in order to make queries perform well. More information is available in [PostgresqlIndexes.md](docs/PostgresqlIndexes.md).\n\n## Transaction results order\n\nThe order transactions are returned in depends on whether or not an account address filter is used.\n\nWhen searching by an account, results are returned most recent first. The intent being that a wallet application would want to display the most recent transactions. A special index is used to make this case performant.\n\nFor all other transaction queries, results are returned oldest first. This is because it is the physical order they would normally be written in, so it is going to be faster.\n\n<!-- USAGE_END_MARKER_LINE -->\n\n# Migrating from Indexer v2\n\nIf you were previously using Indexer 2.x you will need to reconfigure your deployment to include [Conduit](https://github.com/algorand/conduit). The data loading component has moved from Indexer 2.x to Conduit.\n\nSee the [Indexer 2.x to 3.x Migration FAQ](docs/MigrationFAQ.md) for common questions.\n\n[Additional info around configuring Conduit can be found here.](https://github.com/algorand/conduit#migrating-from-indexer-2x)\n", "release_dates": ["2024-01-03T16:20:54Z", "2023-12-31T14:02:13Z", "2023-12-20T16:20:25Z", "2023-10-20T15:43:10Z", "2023-09-18T17:26:09Z", "2023-08-16T15:05:05Z", "2023-06-21T19:47:24Z", "2023-06-05T16:16:53Z", "2023-01-30T18:26:13Z", "2023-01-20T18:51:29Z", "2023-01-06T20:22:06Z", "2023-01-04T01:40:18Z", "2022-12-15T16:28:01Z", "2022-11-30T17:01:37Z", "2022-11-28T17:55:41Z", "2022-11-14T18:23:37Z", "2022-11-03T20:05:53Z", "2022-10-24T16:09:08Z", "2022-10-07T14:38:36Z", "2022-09-02T14:34:48Z", "2022-08-29T15:58:34Z", "2022-08-20T21:18:34Z", "2022-08-16T17:08:11Z", "2022-08-11T16:00:51Z", "2022-07-27T13:29:06Z", "2022-07-09T22:46:30Z", "2022-07-09T22:46:21Z", "2022-07-01T13:11:21Z", "2022-06-29T18:43:34Z", "2022-06-24T13:49:47Z"]}, {"name": "java-algorand-junit5", "description": "JUnit5 plugin prototype", "language": "Java", "license": null, "readme": "# This is a prototype! Do not use!\n\n# Algorand JUnit Plugin\n\nThis plugin aims to simplify integration testing for the Algorand blockchain by providing private networks via standard JUnit5 annotations.\n\n## Dependencies\nThis plugin depends on a number of programs which must all be available on the search path:\n* goran\n* goal\n* algod\n* kmd\n\n**TODO: provide these binaries in a convenient installation package.**\n\n## Installation\n\n**TODO: publish to Maven Central**\n\n\n## Usage\n\nSeveral annotations are provided for managing private networks.\n\n### @AlgorandPrivateNetwork\n\nThis is a **test level** annotation used to get access to the `AlgorandNetwork` meta object. The meta object contains a number of useful constructs including an initialized `AlgodClient` object configured with the private network, and an array of Accounts and their private keys for immediate use.\n\nThe network can be configured with several annotation arguments. See the examples for how to turn on **devMode**. Inspect the object for other options.\n#### Example\n\n```java\npublic class TestAlgorandExtension {\n    @Test\n    @AlgorandPrivateNetwork\n    public void simpleStatusRequest(AlgorandNetwork network) {\n        var status = network.client.GetStatus().execute();\n        assertThat(status.isSuccessful()).isTrue();\n    }\n\n    @Test\n    @AlgorandPrivateNetwork(devMode = true)\n    public void devModeAndTransactionTest(AlgorandNetwork network) {\n        // Initialize account.\n        var mnemonic = network.networkMetadata.Accounts[0].Mnemonic;\n        var acct = new Account(mnemonic);\n        var newAcct = new Account();\n\n        // Create transaction.\n        long xferAmount = 100000;\n        var txn = Transaction.PaymentTransactionBuilder()\n                    .lookupParams(network.client)\n                    .sender(acct.getAddress())\n                    .receiver(newAcct.getAddress())\n                    .amount(xferAmount)\n                    .build();\n\n        // Sign and submit transaction.\n        var stxn = acct.signTransaction(txn);\n        var stxnPayload = Encoder.encodeToMsgPack(stxn);\n        var submit = network.client.RawTransaction()\n                    .rawtxn(Encoder.encodeToMsgPack(stxn))\n                    .execute();\n        assertThat(submit.isSuccessful()).isTrue();\n\n        // Confirm reciept.\n        assertThatCode(() -> {\n                Utils.waitForConfirmation(network.client, submit.body().txId, 5);\n        }).doesNotThrowAnyException();\n        var acctInfo = network.client.AccountInformation(newAcct.getAddress()).execute();\n        assertThat(acctInfo.isSuccessful()).isTrue();\n        assertThat(acctInfo.body().amount).isEqualTo(xferAmount);\n    }\n}\n```\n\n### @AlgorandDevServer\nTo optimize network access, a **class level** annotation is also provided. This allows configuring resources to share across multiple tests. This can be convenient when there are many tests because it allows you to avoid extra setup and teardown operations and speed up the test.\n\nIt can be configured with several annotation arguments, inspect the object for details.\n# Example\n```java\n@AlgorandDevServer(idleTimeoutSeconds = 60)\npublic class TestAlgorandSharedDevServer {\n    @Test\n    @AlgorandPrivateNetwork\n    public void test(AlgorandNetwork network) {\n        validate(network);\n    }\n\n    @Test\n    @AlgorandPrivateNetwork\n    public void test2(AlgorandNetwork network) {\n        validate(network);\n    }\n}\n```\n", "release_dates": []}, {"name": "java-algorand-sdk", "description": "Algorand SDK for Java7+ to interact with the Algorand network", "language": "Java", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# java-algorand-sdk\n\n[![Build Status](https://travis-ci.com/algorand/java-algorand-sdk.svg?branch=master)](https://travis-ci.com/algorand/java-algorand-sdk?branch=master)\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.algorand/algosdk/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.algorand/algosdk/)\n\nAlgoSDK is a Java library for communicating and interacting with the Algorand network. It contains a REST client for accessing `algod` instances over the web,\nand also exposes functionality for generating keypairs, mnemonics, creating transactions, signing transactions, and serializing data across the network.\n\n\n# Prerequisites\n\nJava 7+ and Android `minSdkVersion` 16+\n\n# Installation\n\nMaven:\n\n```xml\n<dependency>\n    <groupId>com.algorand</groupId>\n    <artifactId>algosdk</artifactId>\n    <version>2.4.0</version>\n</dependency>\n```\n\n# Quickstart\n\nThis program connects to a running [sandbox](https://github.com/algorand/sandbox) private network, creates a payment transaction between two of the accounts, signs it with kmd, and reads result from Indexer.\n```java\nimport com.algorand.algosdk.account.Account;\nimport com.algorand.algosdk.crypto.Address;\nimport com.algorand.algosdk.kmd.client.ApiException;\nimport com.algorand.algosdk.kmd.client.KmdClient;\nimport com.algorand.algosdk.kmd.client.api.KmdApi;\nimport com.algorand.algosdk.kmd.client.model.*;\nimport com.algorand.algosdk.transaction.SignedTransaction;\nimport com.algorand.algosdk.transaction.Transaction;\nimport com.algorand.algosdk.util.Encoder;\nimport com.algorand.algosdk.v2.client.common.AlgodClient;\nimport com.algorand.algosdk.v2.client.common.IndexerClient;\nimport com.algorand.algosdk.v2.client.common.Response;\nimport com.algorand.algosdk.v2.client.model.PendingTransactionResponse;\nimport com.algorand.algosdk.v2.client.model.PostTransactionsResponse;\nimport com.algorand.algosdk.v2.client.model.TransactionsResponse;\n\nimport java.io.IOException;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\npublic class Main {\n    private static String token = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\";\n    private static KmdApi kmd = null;\n\n    public static void main(String[] args) throws Exception {\n        // Initialize algod/indexer v2 clients.\n        AlgodClient algod = new AlgodClient(\"http://localhost\", 4001, token);\n        IndexerClient indexer = new IndexerClient(\"http://localhost\", 8980);\n\n        // Initialize KMD v1 client\n        KmdClient kmdClient = new KmdClient();\n        kmdClient.setBasePath(\"http://localhost:4002\");\n        kmdClient.setApiKey(token);\n        kmd = new KmdApi(kmdClient);\n\n        // Get accounts from sandbox.\n        String walletHandle = getDefaultWalletHandle();\n        List<Address> accounts  = getWalletAccounts(walletHandle);\n\n        // Create a payment transaction\n        Transaction tx1 = Transaction.PaymentTransactionBuilder()\n                .lookupParams(algod) // lookup fee, firstValid, lastValid\n                .sender(accounts.get(0))\n                .receiver(accounts.get(1))\n                .amount(1000000)\n                .noteUTF8(\"test transaction!\")\n                .build();\n\n        // Sign with KMD\n        SignedTransaction stx1a = signTransactionWithKMD(tx1, walletHandle);\n        byte[] stx1aBytes = Encoder.encodeToMsgPack(stx1a);\n\n        // Sign with private key\n        byte[] privateKey = lookupPrivateKey(accounts.get(0), walletHandle);\n        Account account = new Account(privateKey);\n        SignedTransaction stx1b = account.signTransaction(tx1);\n        byte[] stx1bBytes = Encoder.encodeToMsgPack(stx1b);\n\n        // KMD and signing directly should both be the same.\n        if (!Arrays.equals(stx1aBytes, stx1bBytes)) {\n            throw new RuntimeException(\"KMD disagrees with the manual signature!\");\n        }\n\n        // Send transaction\n        Response<PostTransactionsResponse> post = algod.RawTransaction().rawtxn(stx1aBytes).execute();\n        if (!post.isSuccessful()) {\n            throw new RuntimeException(\"Failed to post transaction\");\n        }\n\n        // Wait for confirmation\n        boolean done = false;\n        while (!done) {\n            Response<PendingTransactionResponse> txInfo = algod.PendingTransactionInformation(post.body().txId).execute();\n            if (!txInfo.isSuccessful()) {\n                throw new RuntimeException(\"Failed to check on tx progress\");\n            }\n            if (txInfo.body().confirmedRound != null) {\n                done = true;\n            }\n        }\n\n        // Wait for indexer to index the round.\n        Thread.sleep(5000);\n\n        // Query indexer for the transaction\n        Response<TransactionsResponse> transactions = indexer.searchForTransactions()\n                .txid(post.body().txId)\n                .execute();\n\n        if (!transactions.isSuccessful()) {\n            throw new RuntimeException(\"Failed to lookup transaction\");\n        }\n\n        System.out.println(\"Transaction received! \\n\" + transactions.toString());\n    }\n\n    public static SignedTransaction signTransactionWithKMD(Transaction tx, String walletHandle) throws IOException, ApiException {\n        SignTransactionRequest req = new SignTransactionRequest();\n        req.transaction(Encoder.encodeToMsgPack(tx));\n        req.setWalletHandleToken(walletHandle);\n        req.setWalletPassword(\"\");\n        byte[] stxBytes = kmd.signTransaction(req).getSignedTransaction();\n        return Encoder.decodeFromMsgPack(stxBytes, SignedTransaction.class);\n    }\n\n    public static byte[] lookupPrivateKey(Address addr, String walletHandle) throws ApiException {\n        ExportKeyRequest req = new ExportKeyRequest();\n        req.setAddress(addr.toString());\n        req.setWalletHandleToken(walletHandle);\n        req.setWalletPassword(\"\");\n        return kmd.exportKey(req).getPrivateKey();\n    }\n\n    public static String getDefaultWalletHandle() throws ApiException {\n        for (APIV1Wallet w : kmd.listWallets().getWallets()) {\n            if (w.getName().equals(\"unencrypted-default-wallet\")) {\n                InitWalletHandleTokenRequest tokenreq = new InitWalletHandleTokenRequest();\n                tokenreq.setWalletId(w.getId());\n                tokenreq.setWalletPassword(\"\");\n                return kmd.initWalletHandleToken(tokenreq).getWalletHandleToken();\n            }\n        }\n        throw new RuntimeException(\"Default wallet not found.\");\n    }\n\n    public static List<Address> getWalletAccounts(String walletHandle) throws ApiException, NoSuchAlgorithmException {\n        List<Address> accounts = new ArrayList<>();\n\n        ListKeysRequest keysRequest = new ListKeysRequest();\n        keysRequest.setWalletHandleToken(walletHandle);\n        for (String addr : kmd.listKeysInWallet(keysRequest).getAddresses()) {\n            accounts.add(new Address(addr));\n        }\n\n        return accounts;\n    }\n}\n```\n\n# Documentation\n\nJavadoc can be found at [https://algorand.github.io/java-algorand-sdk](https://algorand.github.io/java-algorand-sdk). <br />\nAdditional resources and code samples are located at [https://developer.algorand.org](https://developer.algorand.org).\n\n# Cryptography\n\nAlgoSDK depends on `org.bouncycastle:bcprov-jdk15on:1.61` for `Ed25519` signatures, `sha512/256` digests, and deserializing `X.509`-encoded `Ed25519` private keys.\nThe latter is the only explicit dependency on an external crypto library - all other references are abstracted through the JCA.\n\n# Java 9+\n\nWhen using cryptographic functionality, and Java9+, you may run into the following warning:\n```\nWARNING: Illegal reflective access by org.bouncycastle.jcajce.provider.drbg.DRBG\n```\nThis is known behavior, caused by more restrictive language features in Java 9+, that Bouncy Castle has yet to support. This warning can be suppressed safely. We will monitor\ncryptographic packages for updates or alternative implementations.\n\n# Contributing to this Project\n\n## build\n\nThis project uses Maven.\n\n### **To build**\n```\n~$ mvn package\n```\n\n### **To test**\nWe are using separate version targets for production and testing to allow using JUnit5 for tests. Some IDEs, like IDEA\ndo not support this very well. To workaround the issue a special `ide` profile should be enabled if your IDE does not\nsupport mixed `target` and `testTarget` versions. Regardless of IDE support, the tests can be run from the command line.\nIn this case `clean` is used in case an incremental build was made by the IDE with Java8.\n```\n~$ mvn clean test\n```\n\nThere is also a special integration test environment, and shared tests. To run these use the Makefile:\n```\n~$ make docker-test\n```\n\nTo stand up the test harness, without running the entire test suite use the Makefile:\n```\n~$ make harness\n```\nYou can then run specific cucumber-based unit and integration tests directly.\n\n\n## deploying artifacts\n\nThe generated pom file provides maven compatibility and deploy capabilities.\n```\nmvn clean install\nmvn clean deploy -P github,default\nmvn clean site -P github,default  # for javadoc\nmvn clean deploy -P release,default\n```\n\n# Testing\n\nMany cross-SDK tests are defined in [algorand-sdk-testing](https://github.com/algorand/algorand-sdk-testing/). Some are integration tests with additional dependencies. These dependencies are containerized in a docker file, which can be executed with `make docker-test`.\n\nIt is occasionally useful to run locally, or against alternate integration branches. To do this:\n1. Install feature files for your test branch \"./run_integration_tests.sh -feature-only -test-branch <branch here>\"\n2. Run locally with `make integration` and `make unit`, or from the IDE by running \"RunCucumberUnitTest.java\"\n\n# Android Support\n\nSignificant work has been taken to ensure Android compatibility (in particular for `minSdkVersion` 16). Note that the\ndefault crypto provider on Android does not provide `ed25519` signatures, so you will need to provide your own (e.g. `BouncyCastle`).\n\n# Algod V2 and Indexer Code Generation\nThe classes `com.algorand.algosdk.v2.client.algod.\\*`, `com.algorand.algosdk.v2.client.indexer.\\*`, `com.algorand.algosdk.v2.client.common.AlgodClient`, and `com.algorand.algosdk.v2.client.common.IndexerClient` are generated from OpenAPI specifications in: `algod.oas2.json` and `indexer.oas2.json`.\n\nThe specification files can be obtained from:\n- [algod.oas2.json](https://github.com/algorand/go-algorand/blob/master/daemon/algod/api/algod.oas2.json)\n- [indexer.oas2.json](https://github.com/algorand/indexer/blob/master/api/indexer.oas2.json)\n\nA testing framework can also be generated with: `com.algorand.sdkutils.RunQueryMapperGenerator` and the tests run from `com.algorand.sdkutils.RunAlgodV2Tests` and `com.algorand.sdkutils.RunIndexerTests`\n\n## Regenerate the Client Code\n\nThe actual generation is done using the `generate_java.sh` script in the [generator](https://github.com/algorand/generator/) repo.\n\n# Updating the `kmd` REST client\nThe `kmd` REST client has not been upgraded to use the new code generation, it is still largely autogenerated by `swagger-codegen`. [https://github.com/swagger-api/swagger-codegen]\n\nTo regenerate the clients, first, check out the latest `swagger-codegen` from the github repo. (In particular, the Homebrew version\nis out of date and fails to handle raw byte arrays properly). Note OpenAPI 2.0 doesn't support unsigned types. Luckily we don't have any\nuint32 types in algod, so we can do a lossless type-mapping fromt uint64->int64 (Long) -> BigInteger:\n\n```\ncurl http://localhost:8080/swagger.json | sed -e 's/uint32/int64/g' > temp.json\nswagger-codegen generate -i temp.json -l java -c config.json\n```\n\n`config.json` looks like:\n```json\n{\n  \"library\": \"okhttp-gson\",\n  \"java8\": false,\n  \"hideGenerationTimestamp\": true,\n  \"serializableModel\": false,\n  \"supportJava6\": true,\n  \"invokerPackage\": \"com.algorand.algosdk.{kmd or algod}.client\",\n  \"apiPackage\": \"com.algorand.algosdk.{kmd or algod}.client.api\",\n  \"modelPackage\": \"com.algorand.algosdk.{kmd or algod}.client.model\"\n}\n```\n\nMake sure you convert all `uint32` types to `Long` types.\n\nThe generated code (as of April 2019) has one circular dependency involving\n`client.Pair`. The `client` package depends on `client.auth`, but `client.auth`\nuses `client.Pair` which is in the `client` package. One more problem is that\n`uint64` is not a valid format in OpenAPI 2.0; however, we need to send large\nintegers to the `algod` API (`kmd` is fine). To resolve this, we do the\nfollowing manual pass on generated code:\n\n- Move `Pair.java` into the `client.lib` package\n- Find-and-replace `Integer` with `BigInteger` (for uint64), `Long` (for uint32), etc. in `com.algorand.algosdk.algod` and subpackages (unnecessary for algod)\n- Run an `Optimize Imports` operation on generated code, to minimize dependencies.\n\nNote that msgpack-java is good at using the minimal representation.\n", "release_dates": ["2023-10-25T18:31:12Z", "2023-09-19T19:41:43Z", "2023-06-16T15:45:41Z", "2023-05-09T14:53:11Z", "2023-01-03T19:06:53Z", "2022-12-06T16:06:48Z", "2022-11-10T19:47:58Z", "2022-11-07T14:46:20Z", "2022-10-12T14:29:42Z", "2022-09-22T21:12:47Z", "2022-09-06T20:25:32Z", "2022-08-31T20:57:45Z", "2022-08-18T20:41:44Z", "2022-06-21T19:46:12Z", "2022-06-02T20:47:24Z", "2022-05-02T15:02:18Z", "2022-03-23T17:48:13Z", "2022-03-17T21:03:08Z", "2022-03-02T21:48:09Z", "2022-02-15T22:08:16Z", "2022-01-14T21:46:19Z", "2021-12-30T20:36:58Z", "2021-12-08T02:29:20Z", "2021-10-12T19:36:55Z", "2021-09-29T21:50:41Z", "2021-08-03T16:59:26Z", "2021-06-23T22:40:56Z", "2021-04-26T19:19:54Z"]}, {"name": "js-algorand-sdk", "description": "The official JavaScript SDK for Algorand.", "language": "TypeScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# js-algorand-sdk\n\n[![CircleCI](https://dl.circleci.com/status-badge/img/gh/algorand/js-algorand-sdk/tree/develop.svg?style=svg)](https://dl.circleci.com/status-badge/redirect/gh/algorand/js-algorand-sdk/tree/develop) [![npm version](https://badge.fury.io/js/algosdk.svg)](https://www.npmjs.com/package/algosdk)\n\nAlgoSDK is the official JavaScript library for communicating with the Algorand network. It's designed for modern browsers and Node.js.\n\n## Installation\n\n### [Node.js](https://nodejs.org/en/download/)\n\n```\n$ npm install algosdk\n```\n\n> This package provides TypeScript types, but you will need [TypeScript](https://www.typescriptlang.org/) version 4.2 or higher to use them properly.\n\n### Browser\n\nInclude a minified browser bundle directly in your HTML like so:\n\n```html\n<script\n  src=\"https://unpkg.com/algosdk@v2.7.0/dist/browser/algosdk.min.js\"\n  integrity=\"sha384-OP8U0zDUgTdYdeyxnrhicwju6SuPxm2tx4WaTYDeP5JiMS/OyifldTK5Y3vzPK9K\"\n  crossorigin=\"anonymous\"\n></script>\n```\n\nor\n\n```html\n<script\n  src=\"https://cdn.jsdelivr.net/npm/algosdk@v2.7.0/dist/browser/algosdk.min.js\"\n  integrity=\"sha384-OP8U0zDUgTdYdeyxnrhicwju6SuPxm2tx4WaTYDeP5JiMS/OyifldTK5Y3vzPK9K\"\n  crossorigin=\"anonymous\"\n></script>\n```\n\nInformation about hosting the package for yourself, finding the browser bundles of previous versions, and computing the SRI hash is [available here](FAQ.md).\n\n## Quick Start\n\n```javascript\nconst token = 'Your algod API token';\nconst server = 'http://127.0.0.1';\nconst port = 8080;\nconst client = new algosdk.Algodv2(token, server, port);\n\n(async () => {\n  console.log(await client.status().do());\n})().catch((e) => {\n  console.log(e);\n});\n```\n\n## Documentation\n\nDocumentation for this SDK is available here: https://algorand.github.io/js-algorand-sdk/. Additional resources are available on https://developer.algorand.org.\n\n## Examples\n\nRunning examples requires access to a running node. Follow the instructions in Algorand's [developer resources](https://developer.algorand.org/docs/run-a-node/setup/install/) to install a node on your computer.\n\n**As portions of the codebase are written in TypeScript, example files cannot be run directly using `node`**. Please refer to the instructions described in the [examples/README.md](examples/README.md) file for more information regarding running the examples.\n\n## SDK Development\n\n### Building\n\nTo build a new version of the library, run:\n\n```bash\nnpm run build\n```\n\n### Generating Documentation\n\nTo generate the documentation website, run:\n\n```bash\nnpm run docs\n```\n\nThe static website will be located in the `docs/` directory.\n\n### Testing\n\nWe have two test suites: mocha tests in this repo, and the Algorand SDK test suite from https://github.com/algorand/algorand-sdk-testing.\n\n#### Node.js\n\nTo run the mocha tests in Node.js, run:\n\n```bash\nnpm test\n```\n\nTo run the SDK test suite in Node.js, run:\n\n```bash\nmake docker-test\n```\n\n#### Browsers\n\nThe test suites can also run in browsers. To do so, set the environment variable `TEST_BROWSER` to\none of our supported browsers. Currently we support testing in `chrome` and `firefox`. When\n`TEST_BROWSER` is set, the mocha and SDK test suites will run in that browser.\n\nFor example, to run mocha tests in Chrome:\n\n```bash\nTEST_BROWSER=chrome npm test\n```\n\nAnd to run SDK tests in Firefox:\n\n```bash\nTEST_BROWSER=firefox make docker-test\n```\n\n### Code Style\n\nThis project enforces a modified version of the [Airbnb code style](https://github.com/airbnb/javascript).\n\nWe've setup linters and formatters to help catch errors and improve the development experience:\n\n- [Prettier](https://prettier.io/) \u2013 ensures that code is formatted in a readable way.\n- [ESLint](https://eslint.org/) \u2014 checks code for antipatterns as well as formatting.\n\n> If using the Visual Studio Code editor with the [recommended extensions](.vscode/extensions.json), ESLint errors should be highlighted in red and the Prettier extension should format code on every save.\n\n#### Precommit Hook\n\nThe linters and formatters listed above should run automatically on each commit to catch errors early and save CI running time.\n\n## License\n\njs-algorand-sdk is licensed under an MIT license. See the [LICENSE](https://github.com/algorand/js-algorand-sdk/blob/master/LICENSE) file for details.\n", "release_dates": ["2023-10-20T19:10:03Z", "2023-09-21T14:11:24Z", "2023-08-17T13:39:09Z", "2023-06-16T13:23:53Z", "2023-05-08T20:22:16Z", "2023-03-23T13:42:26Z", "2023-01-27T15:56:49Z", "2023-01-03T18:02:56Z", "2022-12-15T19:17:23Z", "2022-12-05T19:28:22Z", "2022-11-10T18:29:27Z", "2022-11-03T14:43:46Z", "2022-10-12T19:18:24Z", "2022-09-19T13:25:26Z", "2022-09-06T17:55:57Z", "2022-08-30T20:46:27Z", "2022-08-22T15:26:44Z", "2022-07-21T15:53:57Z", "2022-07-05T18:13:31Z", "2022-06-22T14:43:39Z", "2022-06-02T21:02:02Z", "2022-05-02T18:09:12Z", "2022-03-23T19:45:58Z", "2022-03-18T13:51:01Z", "2022-03-07T23:45:21Z", "2022-02-15T22:14:16Z", "2022-01-26T16:54:19Z", "2022-01-13T19:42:20Z", "2021-12-28T23:09:52Z", "2021-11-23T22:49:11Z"]}, {"name": "ledger-app-algorand", "description": null, "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Ledger Algorand app\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![GithubActions](https://github.com/Zondax/ledger-algorand/actions/workflows/main.yml/badge.svg)](https://github.com/Zondax/ledger-algorand/blob/main/.github/workflows/main.yaml)\n\n---\n\n![zondax_light](docs/zondax_light.png#gh-light-mode-only)\n![zondax_dark](docs/zondax_dark.png#gh-dark-mode-only)\n\n_Please visit our website at [zondax.ch](https://www.zondax.ch)_\n\n---\n\nThis project contains the Algorand app for Ledger Nano S Nano S+ and X.\n\n- Ledger Nano S/S+/X Algorand app\n- Specs / Documentation\n- C++ unit tests\n- Zemu tests\n\n## ATTENTION\n\nPlease:\n\n- **Do not use in production**\n- **Do not use a Ledger device with funds for development purposes.**\n- **Have a separate and marked device that is used ONLY for development and testing**\n\n\n## Download and install a prerelease\n\n*Once the app is approved by Ledger, it will be available in their app store (Ledger Live).\nYou can get builds generated by Github Actions from the release tab. THESE ARE UNVETTED DEVELOPMENT RELEASES*\n\nDownload a release from here (https://github.com/Zondax/ledger-algorand/releases). You only need `installer.sh`\n\nIf the file is not executable, run\n```sh\nchmod +x ./installer.sh\n```\n\nthen run:\n\n```sh\n./installer.sh load\n```\n\n# Development\n\n## Preconditions\n\n- Be sure you checkout submodules too:\n\n    ```\n    git submodule update --init --recursive\n    ```\n\n- Install Docker CE\n    - Instructions can be found here: https://docs.docker.com/install/\n\n- We only officially support Ubuntu. Install the following packages:\n   ```\n   sudo apt update && apt-get -y install build-essential git wget cmake \\\n  libssl-dev libgmp-dev autoconf libtool\n   ```\n\n- Install `node > v13.0`. We typically recommend using `n`\n\n- You will need python 3 and then run\n    - `make deps`\n\n- This project requires Ledger firmware 2.0\n    - The current repository keeps track of Ledger's SDK but it is possible to override it by changing the git submodule.\n\n*Warning*: Some IDEs may not use the same python interpreter or virtual enviroment as the one you used when running `pip`.\nIf you see conan is not found, check that you installed the package in the same interpreter as the one that launches `cmake`.\n\n## How to build ?\n\n> We like clion or vscode but let's have some reproducible command line steps\n>\n\n- Building the app itself\n\n    If you installed the what is described above, just run:\n    ```bash\n    make\n    ```\n\n## Running tests\n\n- Running rust tests (x64)\n\n    If you installed the what is described above, just run:\n    ```bash\n    make rust_test\n    ```\n\n- Running C/C++ tests (x64)\n\n    If you installed the what is described above, just run:\n    ```bash\n    make cpp_test\n    ```\n\n- Running device emulation+integration tests!!\n\n   ```bash\n    Use Zemu! Explained below!\n    ```\n\n## How to test with Zemu?\n\n> What is Zemu?? Great you asked!!\n> As part of this project, we are making public a beta version of our internal testing+emulation framework for Ledger apps.\n>\n> Npm Package here: https://www.npmjs.com/package/@zondax/zemu\n>\n> Repo here: https://github.com/Zondax/zemu\n\nLet's go! First install everything:\n> At this moment, if you change the app you will need to run `make` before running the test again.\n\n```bash\nmake zemu_install\n```\n\nThen you can run JS tests:\n\n```bash\nmake zemu_test\n```\n\nTo run a single specific test:\n\n> At the moment, the recommendation is to run from the IDE. Remember to run `make` if you change the app.\n\n## Using a real device\n\n### How to prepare your DEVELOPMENT! device:\n\n>  You can use an emulated device for development. This is only required if you are using a physical device\n>\n>    **Please do not use a Ledger device with funds for development purposes.**\n>>\n>    **Have a separate and marked device that is used ONLY for development and testing**\n\n   There are a few additional steps that increase reproducibility and simplify development:\n\n**1 - Ensure your device works in your OS**\n- In Linux hosts it might be necessary to adjust udev rules, etc.\n\n  Refer to Ledger documentation: https://support.ledger.com/hc/en-us/articles/115005165269-Fix-connection-issues\n\n**2 - Set a test mnemonic**\n\nMany of our integration tests expect the device to be configured with a known test mnemonic.\n\n- Plug your device while pressing the right button\n\n- Your device will show \"Recovery\" in the screen\n\n- Double click\n\n- Run `make dev_init`. This will take about 2 minutes. The device will be initialized to:\n\n   ```\n   PIN: 5555\n   Mnemonic: equip will roof matter pink blind book anxiety banner elbow sun young\n   ```\n\n**3 - Add a development certificate**\n\n- Plug your device while pressing the right button\n\n- Your device will show \"Recovery\" in the screen\n\n- Click both buttons at the same time\n\n- Enter your pin if necessary\n\n- Run `make dev_ca`. The device will receive a development certificate to avoid constant manual confirmations.\n\n\n### Loading into your development device\n\nThe Makefile will build the firmware in a docker container and leave the binary in the correct directory.\n\n- Build\n\n   ```\n   make                # Builds the app\n   ```\n\n- Upload to a device\n   The following command will upload the application to the ledger. _Warning: The application will be deleted before uploading._\n   ```\n   make load          # Builds and loads the app to the device\n   ```\n\n## APDU Specifications\n\n- [APDU Protocol](./docs/APDUSPEC.md)\n", "release_dates": ["2023-10-03T21:13:55Z", "2023-06-12T17:25:03Z", "2022-09-16T02:28:54Z", "2022-08-03T13:56:04Z"]}, {"name": "ledgerfuzz", "description": null, "language": "C", "license": null, "readme": null, "release_dates": []}, {"name": "libsodium", "description": "A modern, portable, easy to use crypto library", "language": "C", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": null, "release_dates": []}, {"name": "light-client-poc", "description": null, "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "\nLight Client PoC\n====================\n\n\nA Go implementation of a light client, able to ingest Algorand state proofs and transactions.\n\n# Background\n\nThis implementation contains both code comprising the light client itself and encoded assets needed to demonstrate the light client's functionalities.\n\nAssets include data required to initialize the Oracle, state proof data to advance the Oracle's state and data required to verify a transaction using the Oracle's state. All assets were generated using a private Algorand network.\n\nThe code is intended to be used as a guide to building light clients.\n\n\n# Reading Order\n\n1. main.go - read in its entirety for a general overview of the light client.\n2. oracle.go - start from the commentary on the Oracle struct, followed by the commentary on AdvanceState. Branch out as needed.\n3. transactionVerifier.go - start from the commentary on verifyTransaction. Branch out as needed.\n\n# Running the Light Client\nBuilding the light client using\n```bash\ngo build\n```\nand running the resulting binary will cause the light client to operate on the assets in encodedassets.\nAn Oracle will be initialized using the data in the [genesis folder](encodedassets/genesis), its state will advance using the data in the [state proof folder](encodedassets/stateproofverification) and a transaction will be verified using the data in the [transaction verification folder](encodedassets/transactionverification).\n", "release_dates": []}, {"name": "logstash-input-http", "description": "Logstash HTTP Input Plugin with Configurable Response", "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Logstash Plugin\n\n[![Travis Build Status](https://travis-ci.com/logstash-plugins/logstash-input-http.svg)](https://travis-ci.com/logstash-plugins/logstash-input-http)\n\nThis is a plugin for [Logstash](https://github.com/elastic/logstash).\n\nIt is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.\n\n## Documentation\n\nLogstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).\n\n- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive\n- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide\n\n## Need Help?\n\nNeed help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.\n\n## Developing\n\n### 1. Plugin Developement and Testing\n\n#### Code\n- To get started, you'll need JRuby with the Bundler gem installed.\n\n- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).\n\n- Install dependencies\n```sh\nbundle install\n```\n\n#### Test\n\n- Update your dependencies\n\n```sh\nbundle install\n```\n\n- Run tests\n\n```sh\nbundle exec rspec\n```\n\n### 2. Running your unpublished Plugin in Logstash\n\n#### 2.1 Run in a local Logstash clone\n\n- Edit Logstash `Gemfile` and add the local plugin path, for example:\n```ruby\ngem \"logstash-filter-awesome\", :path => \"/your/local/logstash-filter-awesome\"\n```\n- Install plugin\n```sh\n# Logstash 2.3 and higher\nbin/logstash-plugin install --no-verify\n\n# Prior to Logstash 2.3\nbin/plugin install --no-verify\n\n```\n- Run Logstash with your plugin\n```sh\nbin/logstash -e 'filter {awesome {}}'\n```\nAt this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.\n\n#### 2.2 Run in an installed Logstash\n\nYou can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:\n\n- Build your plugin gem\n```sh\ngem build logstash-filter-awesome.gemspec\n```\n- Install the plugin from the Logstash home\n```sh\n# Logstash 2.3 and higher\nbin/logstash-plugin install --no-verify\n\n# Prior to Logstash 2.3\nbin/plugin install --no-verify\n\n```\n- Start Logstash and proceed to test the plugin\n\n## Contributing\n\nAll contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.\n\nProgramming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying \"send patches or die\" - you will not see that here.\n\nIt is more important to the community that you are able to contribute.\n\nFor more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.", "release_dates": []}, {"name": "messagepack-debugger", "description": "Visually compare MessagePack objects to assist with debugging.", "language": "Kotlin", "license": null, "readme": "# MessagePack Debugger\n\nView and compare messagepack encoded objects. Put the file path or base64 encoded MessagePack objects into the text fields and they will be displayed as a tree with any differences highlighted red.\n\n# Dependencies\n\nJava 12 or higher.\nJavaFX 13 or higher.\n\nHonestly, your best bet is probably to install the latest version of Java / Maven and build tha artifact yourself.\n\n\n# Usage\nDownload the jar file from the releases tab and run with **java -jar**:\n```\n~$ java -jar messagepack-debugger.jar\n```\n\n### Arguments\n\nThe first 2 arguments will be used to initialize the UI during launch, you can provide any combination of file paths and base64 encoded objects:\n```\n~$ java -jar messagepack-debugger.jar <object-1> <object-2>\n```\n\nHere is an example launching the UI with two encoded objects:\n```\n~$ java -jar messagepack-debugger.jar \\\n          gqRsc2lngqNhcmeRxAhwcmVpbWFnZaFsxJcBIAToBwEAwM8kJgMg5pqWHm8tX3rIZgeSZVK+mCNe0zNjyoiRi7nJOKkVtvkgEHZhE08h/HwCIj1Qq56zYAvD/8NxJCOh5Hux+anb9V8g/ryguxRKWk6ntDikaBrIDmyhBby2B/xWUyXJVpX2ohMxASIOMRAjEhAxBzIDEhAxCCQSEDEJKBItAykSEDEJKhIxAiUNEBEQo3R4boelY2xvc2XEIOaalh5vLV96yGYHkmVSvpgjXtMzY8qIkYu5yTipFbb5o2ZlZc0D6KJmdgGiZ2jEIBB2YRNPIfx8AiI9UKues2ALw//DcSQjoeR7sfmp2/Vfomx2ZKNzbmTEIAXpFmK85H3IBcFHiLtBPZNeppVgQscpIfaGLin5FHM5pHR5cGWjcGF5 \\\n          gqRsc2lngqNhcmeRxAhwcmVpbWFnZaFsxJcBIAToBwEAwM8kJgMg5pqWHm8tX3rIZgeSZVK+mCNe0zNjyoiRi7nJOKkVtvkgEHZhE08h/HwCIj1Qq56zYAvD/8NxJCOh5Hux+anb9V8g/ryguxRKWk6ntDikaBrIDmyhBby2B/xWUyXJVpX2ohMxASIOMRAjEhAxBzIDEhAxCCQSEDEJKBItASkSEDEJKhIxAiUNEBEQo3R4boelY2xvc2XEIOaalh5vLV96yGYHkmVSvpgjXtMzY8qIkYu5yTipFbb5o2ZlZc0D6KJmdgGiZ2jEIH+DsWV/8fxTuS3BgUih1l38LUsfo9Z3KErd0gASbZBpomx2ZKNzbmTEIChyiO42rPQZmq42un3UDl1H3kZii2K4CElLvSrIU+oqpHR5cGWjcGF5\n```\n\nYou can also launch with a transaction files:\n```\n~$ java -jar messagepack-debugger /home/will/algorand/java-algorand-sdk/expected\n```\n\nOr any combination of the two:\n```\njava -jar target/messagepack-debugger.jar \\\n       /home/will/algorand/messagepack-debugger/expected \\\n       gqRsc2lngaFsxLcBIAoAAcCWsQICkE4EuWBkHsDEByYBIP68oLsUSlpOp7Q4pGgayA5soQW8tgf8VlMlyVaV9qITMRYiEjEQIxIQMQEkDhAyBCMSQABVMgQlEjEIIQQNEDEJMgMSEDMBECEFEhAzAREhBhIQMwEUKBIQMwETMgMSEDMBEiEHHTUCNQExCCEIHTUENQM0ATQDDUAAJDQBNAMSNAI0BA8QQAAWADEJKBIxAiEJDRAxBzIDEhAxCCISEBCjdHhuiaNhbXTNJxCjZmVlzQmKomZ2zQfQomdoxCB/g7Flf/H8U7ktwYFIodZd/C1LH6PWdyhK3dIAEm2QaaNncnDEIDLoxg2J+e7ixq7CMmuLHIDdr+YAIdIgdnb95185yupUomx2zQu4o3JjdsQgzUWoS2tM1RvtuQQyGhwdXUqCByLFEPBRXHZwnSqBvgajc25kxCBd4Wnq60VaWXeVmuPt0x1XgMQSyP4nUnlh5G2Rhyo+taR0eXBlo3BheYKjc2lnxEAnRhPWfzTensUjfRxaJ6mFOiJQYVoIpsSjI0koev2BnomM8EXLatV+wZotqG44I+q4Y3GX4kShGPXpTjbL3ugLo3R4boqkYWFtdM0LuKRhcmN2xCD+vKC7FEpaTqe0OKRoGsgObKEFvLYH/FZTJclWlfaiE6NmZWXNCg6iZnbNB9CiZ2jEIH+DsWV/8fxTuS3BgUih1l38LUsfo9Z3KErd0gASbZBpo2dycMQgMujGDYn57uLGrsIya4scgN2v5gAh0iB2dv3nXznK6lSibHbNC7ijc25kxCDNRahLa0zVG+25BDIaHB1dSoIHIsUQ8FFcdnCdKoG+BqR0eXBlpWF4ZmVypHhhaWTNMDk=\n```\n\n# Screenshot\n![Screenshot showing a single transaction file being inspected](/screenshots/load_file.png?raw=true \"Screenshot showing a single transaction file being inspected\")\n![Screenshot showing different objects loaded](/screenshots/file_and_base64_string.png?raw=true \"Screenshot showing different objects loaded\")\n![Screenshot showing an unordered comparison](/screenshots/unordered.png?raw=true \"Screenshot showing an unordered comparison\")\n![Screenshot showing transaction groups](/screenshots/compare_transaction_group_files.png?raw=true \"Screenshot showing transaction groups\")\n\n\n# Development\nTo start from the IDE run the main function in **com.algorand.msgpack.debugger.app.MyApp.kt**. You may need to configure your IDE to run the maven install target.\n\nStart from the command line with maven:\n```\nmvn compile javafx:run\n```\n\n# Artifacts\n\nTo build a jar with bundled dependencies. After the build finishes you should have a new file at **target/messagepack-debugger-<VERSION>-jar-with-dependencies.jar**:\n```\nmvn package\n```\n", "release_dates": ["2020-02-11T17:08:47Z"]}, {"name": "msgp", "description": null, "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "MessagePack Code Generator\n=======\n\nBased on [github.com/tinylib/msgp](https://github.com/tinylib/msgp)\n", "release_dates": ["2023-10-03T19:17:47Z", "2023-10-03T18:52:44Z", "2023-08-22T14:07:58Z", "2023-08-15T21:07:11Z", "2023-08-11T18:30:30Z", "2023-07-11T18:36:33Z", "2022-10-17T19:45:19Z", "2022-05-17T15:45:31Z", "2022-05-09T21:35:56Z", "2022-03-22T16:38:17Z", "2021-10-19T07:49:53Z", "2021-07-20T21:19:56Z", "2021-02-02T14:41:36Z", "2021-01-22T14:52:45Z", "2020-06-11T16:25:02Z", "2020-06-11T14:40:09Z", "2020-06-03T12:48:15Z", "2020-06-03T12:39:00Z", "2020-06-03T01:59:41Z", "2020-05-23T15:57:48Z"]}, {"name": "msgpack-javascript", "description": "@msgpack/msgpack - MessagePack for JavaScript / msgpack.org[JavaScript/TypeScript/ECMA-262]", "language": "TypeScript", "license": {"key": "isc", "name": "ISC License", "spdx_id": "ISC", "url": "https://api.github.com/licenses/isc", "node_id": "MDc6TGljZW5zZTEw"}, "readme": "# MessagePack for JavaScript/ECMA-262 <!-- omit in toc -->\n\n[![npm version](https://img.shields.io/npm/v/@msgpack/msgpack.svg)](https://www.npmjs.com/package/@msgpack/msgpack) ![CI](https://github.com/msgpack/msgpack-javascript/workflows/CI/badge.svg) [![codecov](https://codecov.io/gh/msgpack/msgpack-javascript/branch/master/graphs/badge.svg)](https://codecov.io/gh/msgpack/msgpack-javascript) [![minzip](https://badgen.net/bundlephobia/minzip/@msgpack/msgpack)](https://bundlephobia.com/result?p=@msgpack/msgpack) [![tree-shaking](https://badgen.net/bundlephobia/tree-shaking/@msgpack/msgpack)](https://bundlephobia.com/result?p=@msgpack/msgpack)\n\nThis library is an implementation of **MessagePack** for TypeScript and JavaScript, providing a compact and efficient binary serialization format. Learn more about MessagePack at:\n\nhttps://msgpack.org/\n\nThis library serves as a comprehensive reference implementation of MessagePack for JavaScript with a focus on accuracy, compatibility, interoperability, and performance.\n\nAdditionally, this is also a universal JavaScript library. It is compatible not only with browsers, but with Node.js or other JavaScript engines that implement ES2015+ standards. As it is written in [TypeScript](https://www.typescriptlang.org/), this library bundles up-to-date type definition files (`d.ts`).\n\n\\*Note that this is the second edition of \"MessagePack for JavaScript\". The first edition, which was implemented in ES5 and never released to npmjs.com, is tagged as [`classic`](https://github.com/msgpack/msgpack-javascript/tree/classic).\n\n## Synopsis\n\n```typescript\nimport { deepStrictEqual } from \"assert\";\nimport { encode, decode } from \"@msgpack/msgpack\";\n\nconst object = {\n  nil: null,\n  integer: 1,\n  float: Math.PI,\n  string: \"Hello, world!\",\n  binary: Uint8Array.from([1, 2, 3]),\n  array: [10, 20, 30],\n  map: { foo: \"bar\" },\n  timestampExt: new Date(),\n};\n\nconst encoded: Uint8Array = encode(object);\n\ndeepStrictEqual(decode(encoded), object);\n```\n\n## Table of Contents\n\n- [Synopsis](#synopsis)\n- [Table of Contents](#table-of-contents)\n- [Install](#install)\n- [API](#api)\n  - [`encode(data: unknown, options?: EncoderOptions): Uint8Array`](#encodedata-unknown-options-encoderoptions-uint8array)\n    - [`EncoderOptions`](#encoderoptions)\n  - [`decode(buffer: ArrayLike<number> | BufferSource, options?: DecoderOptions): unknown`](#decodebuffer-arraylikenumber--buffersource-options-decoderoptions-unknown)\n    - [`DecoderOptions`](#decoderoptions)\n      - [`IntMode`](#intmode)\n  - [`decodeMulti(buffer: ArrayLike<number> | BufferSource, options?: DecoderOptions): Generator<unknown, void, unknown>`](#decodemultibuffer-arraylikenumber--buffersource-options-decoderoptions-generatorunknown-void-unknown)\n  - [`decodeAsync(stream: ReadableStreamLike<ArrayLike<number> | BufferSource>, options?: DecoderOptions): Promise<unknown>`](#decodeasyncstream-readablestreamlikearraylikenumber--buffersource-options-decoderoptions-promiseunknown)\n  - [`decodeArrayStream(stream: ReadableStreamLike<ArrayLike<number> | BufferSource>, options?: DecoderOptions): AsyncIterable<unknown>`](#decodearraystreamstream-readablestreamlikearraylikenumber--buffersource-options-decoderoptions-asynciterableunknown)\n  - [`decodeMultiStream(stream: ReadableStreamLike<ArrayLike<number> | BufferSource>, options?: DecoderOptions): AsyncIterable<unknown>`](#decodemultistreamstream-readablestreamlikearraylikenumber--buffersource-options-decoderoptions-asynciterableunknown)\n  - [Reusing Encoder and Decoder instances](#reusing-encoder-and-decoder-instances)\n- [Extension Types](#extension-types)\n  - [ExtensionCodec context](#extensioncodec-context)\n  - [Handling BigInt](#handling-bigint)\n  - [The temporal module as timestamp extensions](#the-temporal-module-as-timestamp-extensions)\n- [Decoding a Blob](#decoding-a-blob)\n- [MessagePack Specification](#messagepack-specification)\n  - [MessagePack Mapping Table](#messagepack-mapping-table)\n- [Prerequisites](#prerequisites)\n  - [ECMA-262](#ecma-262)\n  - [NodeJS](#nodejs)\n  - [TypeScript Compiler / Type Definitions](#typescript-compiler--type-definitions)\n- [Benchmark](#benchmark)\n- [Distribution](#distribution)\n  - [NPM / npmjs.com](#npm--npmjscom)\n  - [CDN / unpkg.com](#cdn--unpkgcom)\n- [Deno Support](#deno-support)\n- [Maintenance](#maintenance)\n  - [Testing](#testing)\n  - [Continuous Integration](#continuous-integration)\n  - [Release Engineering](#release-engineering)\n  - [Updating Dependencies](#updating-dependencies)\n- [License](#license)\n\n## Install\n\nThis library is published to `npmjs.com` as [@msgpack/msgpack](https://www.npmjs.com/package/@msgpack/msgpack).\n\n```shell\nnpm install @msgpack/msgpack\n```\n\n## API\n\n### `encode(data: unknown, options?: EncoderOptions): Uint8Array`\n\nIt encodes `data` into a single MessagePack-encoded object, and returns a byte array as `Uint8Array`. It throws errors if `data` is, or includes, a non-serializable object such as a `function` or a `symbol`.\n\nfor example:\n\n```typescript\nimport { encode } from \"@msgpack/msgpack\";\n\nconst encoded: Uint8Array = encode({ foo: \"bar\" });\nconsole.log(encoded);\n```\n\nIf you'd like to convert an `uint8array` to a NodeJS `Buffer`, use `Buffer.from(arrayBuffer, offset, length)` in order not to copy the underlying `ArrayBuffer`, while `Buffer.from(uint8array)` copies it:\n\n```typescript\nimport { encode } from \"@msgpack/msgpack\";\n\nconst encoded: Uint8Array = encode({ foo: \"bar\" });\n\n// `buffer` refers the same ArrayBuffer as `encoded`.\nconst buffer: Buffer = Buffer.from(encoded.buffer, encoded.byteOffset, encoded.byteLength);\nconsole.log(buffer);\n```\n\n#### `EncoderOptions`\n\n| Name                | Type           | Default                       |\n| ------------------- | -------------- | ----------------------------- |\n| extensionCodec      | ExtensionCodec | `ExtensionCodec.defaultCodec` |\n| context             | user-defined   | -                             |\n| forceBigIntToInt64  | boolean        | false                         |\n| useRawBinaryStrings | boolean        | false                         |\n| maxDepth            | number         | `100`                         |\n| initialBufferSize   | number         | `2048`                        |\n| sortKeys            | boolean        | false                         |\n| forceFloat32        | boolean        | false                         |\n| forceIntegerToFloat | boolean        | false                         |\n| ignoreUndefined     | boolean        | false                         |\n\nTo skip UTF-8 decoding of strings, `useRawBinaryStrings` can be set to `true`. In this case, strings are decoded into `Uint8Array`.\n\n### `decode(buffer: ArrayLike<number> | BufferSource, options?: DecoderOptions): unknown`\n\nIt decodes `buffer` that includes a MessagePack-encoded object, and returns the decoded object typed `unknown`.\n\n`buffer` must be an array of bytes, which is typically `Uint8Array` or `ArrayBuffer`. `BufferSource` is defined as `ArrayBuffer | ArrayBufferView`.\n\nThe `buffer` must include a single encoded object. If the `buffer` includes extra bytes after an object or the `buffer` is empty, it throws `RangeError`. To decode `buffer` that includes multiple encoded objects, use `decodeMulti()` or `decodeMultiStream()` (recommended) instead.\n\nfor example:\n\n```typescript\nimport { decode } from \"@msgpack/msgpack\";\n\nconst encoded: Uint8Array;\nconst object = decode(encoded);\nconsole.log(object);\n```\n\nNodeJS `Buffer` is also acceptable because it is a subclass of `Uint8Array`.\n\n#### `DecoderOptions`\n\n| Name           | Type           | Default                                                                              |\n| -------------- | -------------- | ------------------------------------------------------------------------------------ |\n| extensionCodec | ExtensionCodec | `ExtensionCodec.defaultCodec`                                                        |\n| context        | user-defined   | -                                                                                    |\n| useBigInt64    | boolean        | false                                                                                |\n| intMode        | IntMode        | `IntMode.AS_ENCODED` if `useBigInt64` is `true` or `IntMode.UNSAFE_NUMBER` otherwise |\n| maxStrLength   | number         | `4_294_967_295` (UINT32_MAX)                                                         |\n| maxBinLength   | number         | `4_294_967_295` (UINT32_MAX)                                                         |\n| maxArrayLength | number         | `4_294_967_295` (UINT32_MAX)                                                         |\n| maxMapLength   | number         | `4_294_967_295` (UINT32_MAX)                                                         |\n| maxExtLength   | number         | `4_294_967_295` (UINT32_MAX)                                                         |\n\nYou can use `max${Type}Length` to limit the length of each type decoded.\n\n`intMode` determines whether decoded integers should be returned as numbers or bigints in different circumstances. The possible values are [described below](#intmode).\n\n##### `IntMode`\n\nThe `IntMode` enum defines different options for decoding integers. They are described below:\n\n- `IntMode.UNSAFE_NUMBER`: Always returns the value as a number. Be aware that there will be a loss of precision if the value is outside the range of `Number.MIN_SAFE_INTEGER` to `Number.MAX_SAFE_INTEGER`.\n- `IntMode.SAFE_NUMBER`: Always returns the value as a number, but throws an error if the value is outside of the range of `Number.MIN_SAFE_INTEGER` to `Number.MAX_SAFE_INTEGER`.\n- `IntMode.MIXED`: Returns all values inside the range of `Number.MIN_SAFE_INTEGER` to `Number.MAX_SAFE_INTEGER` as numbers and all values outside that range as bigints.\n- `IntMode.AS_ENCODED`: Returns all values encoded as int64/uint64 as bigints and all other integers as numbers.\n- `IntMode.BIGINT`: Always returns the value as a bigint, even if it is small enough to safely fit in a number.\n\n### `decodeMulti(buffer: ArrayLike<number> | BufferSource, options?: DecoderOptions): Generator<unknown, void, unknown>`\n\nIt decodes `buffer` that includes multiple MessagePack-encoded objects, and returns decoded objects as a generator. See also `decodeMultiStream()`, which is an asynchronous variant of this function.\n\nThis function is not recommended to decode a MessagePack binary via I/O stream including sockets because it's synchronous. Instead, `decodeMultiStream()` decodes a binary stream asynchronously, typically spending less CPU and memory.\n\nfor example:\n\n```typescript\nimport { decode } from \"@msgpack/msgpack\";\n\nconst encoded: Uint8Array;\n\nfor (const object of decodeMulti(encoded)) {\n  console.log(object);\n}\n```\n\n### `decodeAsync(stream: ReadableStreamLike<ArrayLike<number> | BufferSource>, options?: DecoderOptions): Promise<unknown>`\n\nIt decodes `stream`, where `ReadableStreamLike<T>` is defined as `ReadableStream<T> | AsyncIterable<T>`, in an async iterable of byte arrays, and returns decoded object as `unknown` type, wrapped in `Promise`.\n\nThis function works asynchronously, and might CPU resources more efficiently compared with synchronous `decode()`, because it doesn't wait for the completion of downloading.\n\nThis function is designed to work with whatwg `fetch()` like this:\n\n```typescript\nimport { decodeAsync } from \"@msgpack/msgpack\";\n\nconst MSGPACK_TYPE = \"application/x-msgpack\";\n\nconst response = await fetch(url);\nconst contentType = response.headers.get(\"Content-Type\");\nif (contentType && contentType.startsWith(MSGPACK_TYPE) && response.body != null) {\n  const object = await decodeAsync(response.body);\n  // do something with object\n} else {\n  /* handle errors */\n}\n```\n\n### `decodeArrayStream(stream: ReadableStreamLike<ArrayLike<number> | BufferSource>, options?: DecoderOptions): AsyncIterable<unknown>`\n\nIt is alike to `decodeAsync()`, but only accepts a `stream` that includes an array of items, and emits a decoded item one by one.\n\nfor example:\n\n```typescript\nimport { decodeArrayStream } from \"@msgpack/msgpack\";\n\nconst stream: AsyncIterator<Uint8Array>;\n\n// in an async function:\nfor await (const item of decodeArrayStream(stream)) {\n  console.log(item);\n}\n```\n\n### `decodeMultiStream(stream: ReadableStreamLike<ArrayLike<number> | BufferSource>, options?: DecoderOptions): AsyncIterable<unknown>`\n\nIt is alike to `decodeAsync()` and `decodeArrayStream()`, but the input `stream` must consist of multiple MessagePack-encoded items. This is an asynchronous variant for `decodeMulti()`.\n\nIn other words, it could decode an unlimited stream and emits a decoded item one by one.\n\nfor example:\n\n```typescript\nimport { decodeMultiStream } from \"@msgpack/msgpack\";\n\nconst stream: AsyncIterator<Uint8Array>;\n\n// in an async function:\nfor await (const item of decodeMultiStream(stream)) {\n  console.log(item);\n}\n```\n\nThis function is available since v2.4.0; previously it was called as `decodeStream()`.\n\n### Reusing Encoder and Decoder instances\n\n`Encoder` and `Decoder` classes are provided to have better performance by reusing instances:\n\n```typescript\nimport { deepStrictEqual } from \"assert\";\nimport { Encoder, Decoder } from \"@msgpack/msgpack\";\n\nconst encoder = new Encoder();\nconst decoder = new Decoder();\n\nconst encoded: Uint8Array = encoder.encode(object);\ndeepStrictEqual(decoder.decode(encoded), object);\n```\n\nAccording to our benchmark, reusing `Encoder` instance is about 20% faster\nthan `encode()` function, and reusing `Decoder` instance is about 2% faster\nthan `decode()` function. Note that the result should vary in environments\nand data structure.\n\n`Encoder` and `Decoder` take the same options as `encode()` and `decode()` respectively.\n\n## Extension Types\n\nTo handle [MessagePack Extension Types](https://github.com/msgpack/msgpack/blob/master/spec.md#extension-types), this library provides `ExtensionCodec` class.\n\nThis is an example to setup custom extension types that handles `Map` and `Set` classes in TypeScript:\n\n```typescript\nimport { encode, decode, ExtensionCodec } from \"@msgpack/msgpack\";\n\nconst extensionCodec = new ExtensionCodec();\n\n// Set<T>\nconst SET_EXT_TYPE = 0; // Any in 0-127\nextensionCodec.register({\n  type: SET_EXT_TYPE,\n  encode: (object: unknown): Uint8Array | null => {\n    if (object instanceof Set) {\n      return encode([...object], { extensionCodec });\n    } else {\n      return null;\n    }\n  },\n  decode: (data: Uint8Array) => {\n    const array = decode(data, { extensionCodec }) as Array<unknown>;\n    return new Set(array);\n  },\n});\n\n// Map<T>\nconst MAP_EXT_TYPE = 1; // Any in 0-127\nextensionCodec.register({\n  type: MAP_EXT_TYPE,\n  encode: (object: unknown): Uint8Array => {\n    if (object instanceof Map) {\n      return encode([...object], { extensionCodec });\n    } else {\n      return null;\n    }\n  },\n  decode: (data: Uint8Array) => {\n    const array = decode(data, { extensionCodec }) as Array<[unknown, unknown]>;\n    return new Map(array);\n  },\n});\n\nconst encoded = encode([new Set<any>(), new Map<any, any>()], { extensionCodec });\nconst decoded = decode(encoded, { extensionCodec });\n```\n\nEnsure you include your extensionCodec in any recursive encode and decode statements!\n\nNote that extension types for custom objects must be `[0, 127]`, while `[-1, -128]` is reserved for MessagePack itself.\n\n#### ExtensionCodec context\n\nWhen you use an extension codec, it might be necessary to have encoding/decoding state to keep track of which objects got encoded/re-created. To do this, pass a `context` to the `EncoderOptions` and `DecoderOptions`:\n\n```typescript\nimport { encode, decode, ExtensionCodec } from \"@msgpack/msgpack\";\n\nclass MyContext {\n  track(object: any) { /*...*/ }\n}\n\nclass MyType { /* ... */ }\n\nconst extensionCodec = new ExtensionCodec<MyContext>();\n\n// MyType\nconst MYTYPE_EXT_TYPE = 0 // Any in 0-127\nextensionCodec.register({\n  type: MYTYPE_EXT_TYPE,\n  encode: (object, context) => {\n    if (object instanceof MyType) {\n      context.track(object); // <-- like this\n      return encode(object.toJSON(), { extensionCodec, context });\n    } else {\n      return null;\n    }\n  },\n  decode: (data, extType, context) => {\n    const decoded = decode(data, { extensionCodec, context });\n    const my = new MyType(decoded);\n    context.track(my); // <-- and like this\n    return my;\n  },\n});\n\n// and later\nimport { encode, decode } from \"@msgpack/msgpack\";\n\nconst context = new MyContext();\n\nconst encoded = = encode({myType: new MyType<any>()}, { extensionCodec, context });\nconst decoded = decode(encoded, { extensionCodec, context });\n```\n\n#### Handling BigInt\n\n**Decoding**\n\nThis library does not handle decoding BigInt by default, but you have three options to decode using BigInt's:\n\n- Set `useBigInt64: true` to decode MessagePack's `int64`/`uint64` into a BigInt\n- Set `intMode` to exert [greater control](#intmode) over BigInt handling\n- Define a custom `ExtensionCodec` to map bigint to a MessagePack extension type\n\n**Encoding**\n\nThis library will encode a BigInt into a MessagePack int64/uint64 if it is > 32-bit OR you set `forceBigIntToInt64` to `true`. This library will encode a `number` that is > 32-bit into a MessagePack int64/uint64 if it is > 32-bit.\n\nIf you set `forceBigIntToInt64` to `true` note:\n\n- A bigint is encoded in 8 byte binaries even if it's a small integer\n- A bigint must be smaller than the max value of the uint64 and larger than the min value of the int64. Otherwise the behavior is undefined.\n\n**Custom codec**\n\nAlternatively, you can define a custom codec to handle bigint like this:\n\n```typescript\nimport { deepStrictEqual } from \"assert\";\nimport { encode, decode, ExtensionCodec } from \"@msgpack/msgpack\";\n\n// to define a custom codec:\nconst BIGINT_EXT_TYPE = 0; // Any in 0-127\nconst extensionCodec = new ExtensionCodec();\nextensionCodec.register({\n  type: BIGINT_EXT_TYPE,\n  encode(input: unknown): Uint8Array | null {\n    if (typeof input === \"bigint\") {\n      if (input <= Number.MAX_SAFE_INTEGER && input >= Number.MIN_SAFE_INTEGER) {\n        return encode(Number(input));\n      } else {\n        return encode(String(input));\n      }\n    } else {\n      return null;\n    }\n  },\n  decode(data: Uint8Array): bigint {\n    const val = decode(data);\n    if (!(typeof val === \"string\" || typeof val === \"number\")) {\n      throw new DecodeError(`unexpected BigInt source: ${val} (${typeof val})`);\n    }\n    return BigInt(val);\n  },\n});\n\n// to use it:\nconst value = BigInt(Number.MAX_SAFE_INTEGER) + BigInt(1);\nconst encoded: = encode(value, { extensionCodec });\ndeepStrictEqual(decode(encoded, { extensionCodec }), value);\n```\n\n#### The temporal module as timestamp extensions\n\nThere is a proposal for a new date/time representations in JavaScript:\n\n- https://github.com/tc39/proposal-temporal\n\nThis library maps `Date` to the MessagePack timestamp extension by default, but you can re-map the temporal module (or [Temporal Polyfill](https://github.com/tc39/proposal-temporal/tree/main/polyfill)) to the timestamp extension like this:\n\n```typescript\nimport { Instant } from \"@std-proposal/temporal\";\nimport { deepStrictEqual } from \"assert\";\nimport {\n  encode,\n  decode,\n  ExtensionCodec,\n  EXT_TIMESTAMP,\n  encodeTimeSpecToTimestamp,\n  decodeTimestampToTimeSpec,\n} from \"@msgpack/msgpack\";\n\n// to define a custom codec\nconst extensionCodec = new ExtensionCodec();\nextensionCodec.register({\n  type: EXT_TIMESTAMP, // override the default behavior!\n  encode(input: unknown): Uint8Array | null {\n    if (input instanceof Instant) {\n      const sec = input.seconds;\n      const nsec = Number(input.nanoseconds - BigInt(sec) * BigInt(1e9));\n      return encodeTimeSpecToTimestamp({ sec, nsec });\n    } else {\n      return null;\n    }\n  },\n  decode(data: Uint8Array): Instant {\n    const timeSpec = decodeTimestampToTimeSpec(data);\n    const sec = BigInt(timeSpec.sec);\n    const nsec = BigInt(timeSpec.nsec);\n    return Instant.fromEpochNanoseconds(sec * BigInt(1e9) + nsec);\n  },\n});\n\n// to use it\nconst instant = Instant.fromEpochMilliseconds(Date.now());\nconst encoded = encode(instant, { extensionCodec });\nconst decoded = decode(encoded, { extensionCodec });\ndeepStrictEqual(decoded, instant);\n```\n\nThis will become default in this library with major-version increment, if the temporal module is standardized.\n\n## Decoding a Blob\n\n[`Blob`](https://developer.mozilla.org/en-US/docs/Web/API/Blob) is a binary data container provided by browsers. To read its contents, you can use `Blob#arrayBuffer()` or `Blob#stream()`. `Blob#stream()`\nis recommended if your target platform support it. This is because streaming\ndecode should be faster for large objects. In both ways, you need to use\nasynchronous API.\n\n```typescript\nasync function decodeFromBlob(blob: Blob): unknown {\n  if (blob.stream) {\n    // Blob#stream(): ReadableStream<Uint8Array> (recommended)\n    return await decodeAsync(blob.stream());\n  } else {\n    // Blob#arrayBuffer(): Promise<ArrayBuffer> (if stream() is not available)\n    return decode(await blob.arrayBuffer());\n  }\n}\n```\n\n## MessagePack Specification\n\nThis library is compatible with the \"August 2017\" revision of MessagePack specification at the point where timestamp ext was added:\n\n- [x] str/bin separation, added at August 2013\n- [x] extension types, added at August 2013\n- [x] timestamp ext type, added at August 2017\n\nThe living specification is here:\n\nhttps://github.com/msgpack/msgpack\n\nNote that as of June 2019 there're no official \"version\" on the MessagePack specification. See https://github.com/msgpack/msgpack/issues/195 for the discussions.\n\n### MessagePack Mapping Table\n\nThe following table shows how JavaScript values are mapped to [MessagePack formats](https://github.com/msgpack/msgpack/blob/master/spec.md) and vice versa.\n\nThe mapping of integers varies on the setting of `intMode`.\n\n| Source Value          | MessagePack Format   | Value Decoded          |\n| --------------------- | -------------------- | ---------------------- |\n| null, undefined       | nil                  | null (\\*1)             |\n| boolean (true, false) | bool family          | boolean (true, false)  |\n| number (53-bit int)   | int family           | number or bigint (\\*2) |\n| number (64-bit float) | float family         | number (64-bit float)  |\n| bigint                | int family           | number or bigint (\\*2) |\n| string                | str family           | string (\\*3)           |\n| ArrayBufferView       | bin family           | Uint8Array (\\*4)       |\n| Array                 | array family         | Array                  |\n| Object                | map family           | Object (\\*5)           |\n| Date                  | timestamp ext family | Date (\\*6)             |\n| bigint                | int family           | bigint                 |\n\n* \\*1 Both `null` and `undefined` are mapped to `nil` (`0xC0`) type, and are decoded into `null`\n* \\*2 MessagePack ints are decoded as either numbers or bigints depending on the [IntMode](#intmode) used during decoding.\n* \\*3 If you'd like to skip UTF-8 decoding of strings, set `useRawBinaryStrings: true`. In this case, strings are decoded into `Uint8Array`.\n* \\*4 Any `ArrayBufferView`s including NodeJS's `Buffer` are mapped to `bin` family, and are decoded into `Uint8Array`\n* \\*5 In handling `Object`, it is regarded as `Record<string, unknown>` in terms of TypeScript\n* \\*6 MessagePack timestamps may have nanoseconds, which will lost when it is decoded into JavaScript `Date`. This behavior can be overridden by registering `-1` for the extension codec.\n\nIf you set `useBigInt64: true`, the following mapping is used:\n\n| Source Value                      | MessagePack Format   | Value Decoded         |\n| --------------------------------- | -------------------- | --------------------- |\n| null, undefined                   | nil                  | null                  |\n| boolean (true, false)             | bool family          | boolean (true, false) |\n| **number (32-bit int)**           | int family           | number                |\n| **number (except for the above)** | float family         | number                |\n| **bigint**                        | int64 / uint64       | bigint (\\*5)          |\n| string                            | str family           | string                |\n| ArrayBufferView                   | bin family           | Uint8Array            |\n| Array                             | array family         | Array                 |\n| Object                            | map family           | Object                |\n| Date                              | timestamp ext family | Date                  |\n\n* \\*6 If the bigint is larger than the max value of uint64 or smaller than the min value of int64, then the behavior is undefined.\n\n* \\*7 If the bigint is larger than the max value of uint64 or smaller than the min value of int64, then the behavior is undefined.\n## Prerequisites\n\nThis is a universal JavaScript library that supports major browsers and NodeJS.\n\n### ECMA-262\n\n- ES2015 language features\n- ES2018 standard library, including:\n  - Typed arrays (ES2015)\n  - Async iterations (ES2018)\n  - Features added in ES2015-ES2022\n- whatwg encodings (`TextEncoder` and `TextDecoder`)\n\nES2022 standard library used in this library can be polyfilled with [core-js](https://github.com/zloirock/core-js).\n\nIE11 is no longer supported. If you'd like to use this library in IE11, use v2.x versions.\n\n### NodeJS\n\nNodeJS v14 is required.\n\n### TypeScript Compiler / Type Definitions\n\nThis module requires type definitions of `AsyncIterator`, `SourceBuffer`, whatwg streams, and so on. They are provided by `\"lib\": [\"ES2021\", \"DOM\"]` in `tsconfig.json`.\n\nRegarding the TypeScript compiler version, only the latest TypeScript is tested in development.\n\n## Benchmark\n\nRun-time performance is not the only reason to use MessagePack, but it's important to choose MessagePack libraries, so a benchmark suite is provided to monitor the performance of this library.\n\nV8's built-in JSON has been improved for years, esp. `JSON.parse()` is [significantly improved in V8/7.6](https://v8.dev/blog/v8-release-76), it is the fastest deserializer as of 2019, as the benchmark result bellow suggests.\n\nHowever, MessagePack can handles binary data effectively, actual performance depends on situations. You'd better take benchmark on your own use-case if performance matters.\n\nBenchmark on NodeJS/v18.1.0 (V8/10.1)\n\n| operation                                         |      op |   ms |   op/s |\n| ------------------------------------------------- | ------: | ---: | -----: |\n| buf = Buffer.from(JSON.stringify(obj));           |  902100 | 5000 | 180420 |\n| obj = JSON.parse(buf.toString(\"utf-8\"));          |  898700 | 5000 | 179740 |\n| buf = require(\"msgpack-lite\").encode(obj);        |  411000 | 5000 |  82200 |\n| obj = require(\"msgpack-lite\").decode(buf);        |  246200 | 5001 |  49230 |\n| buf = require(\"@msgpack/msgpack\").encode(obj);    |  843300 | 5000 | 168660 |\n| obj = require(\"@msgpack/msgpack\").decode(buf);    |  489300 | 5000 |  97860 |\n| buf = /_ @msgpack/msgpack _/ encoder.encode(obj); | 1154200 | 5000 | 230840 |\n| obj = /_ @msgpack/msgpack _/ decoder.decode(buf); |  448900 | 5000 |  89780 |\n\nNote that `JSON` cases use `Buffer` to emulate I/O where a JavaScript string must be converted into a byte array encoded in UTF-8, whereas MessagePack modules deal with byte arrays.\n\n## Distribution\n\n### NPM / npmjs.com\n\nThe NPM package distributed in npmjs.com includes both ES2015+ and ES5 files:\n\n- `dist/` is compiled into ES2019 with CommomJS, provided for NodeJS v10\n- `dist.es5+umd/` is compiled into ES5 with UMD\n  - `dist.es5+umd/msgpack.min.js` - the minified file\n  - `dist.es5+umd/msgpack.js` - the non-minified file\n- `dist.es5+esm/` is compiled into ES5 with ES modules, provided for webpack-like bundlers and NodeJS's ESM-mode\n\nIf you use NodeJS and/or webpack, their module resolvers use the suitable one automatically.\n\n### CDN / unpkg.com\n\nThis library is available via CDN:\n\n```html\n<script crossorigin src=\"https://unpkg.com/@msgpack/msgpack\"></script>\n```\n\nIt loads `MessagePack` module to the global object.\n\n## Deno Support\n\nYou can use this module on Deno.\n\nSee `example/deno-*.ts` for examples.\n\n`deno.land/x` is not supported yet.\n\n## Maintenance\n\n### Testing\n\nFor simple testing:\n\n```\nnpm run test\n```\n\n### Continuous Integration\n\nThis library uses Travis CI.\n\ntest matrix:\n\n- TypeScript targets\n  - `target=es2019` / `target=es5`\n- JavaScript engines\n  - NodeJS, browsers (Chrome, Firefox, Safari, IE11, and so on)\n\nSee [test:\\* in package.json](./package.json) and [.travis.yml](./.travis.yml) for details.\n\n### Release Engineering\n\n```console\n# run tests on NodeJS, Chrome, and Firefox\nmake test-all\n\n# edit the changelog\ncode CHANGELOG.md\n\n# bump version\nnpm version patch|minor|major\n\n# run the publishing task\nmake publish\n```\n\n### Updating Dependencies\n\n```console\nnpm run update-dependencies\n```\n\n## License\n\nCopyright 2019 The MessagePack community.\n\nThis software uses the ISC license:\n\nhttps://opensource.org/licenses/ISC\n\nSee [LICENSE](./LICENSE) for details.\n", "release_dates": []}, {"name": "msgpack-lite", "description": "Fast Pure JavaScript MessagePack Encoder and Decoder / msgpack.org[JavaScript]", "language": "JavaScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# msgpack-lite [![npm version](https://badge.fury.io/js/msgpack-lite.svg)](http://badge.fury.io/js/msgpack-lite) [![Build Status](https://travis-ci.com/algorand/msgpack-lite.svg?branch=master)](https://travis-ci.com/algorand/msgpack-lite)\n\nFast Pure JavaScript MessagePack Encoder and Decoder\n\n[![Sauce Test Status](https://saucelabs.com/browser-matrix/msgpack-lite.svg)](https://saucelabs.com/u/msgpack-lite)\n\nOnline demo: [http://kawanet.github.io/msgpack-lite/](http://kawanet.github.io/msgpack-lite/)\n\n### Features\n\n- Pure JavaScript only (No node-gyp nor gcc required)\n- Faster than any other pure JavaScript libraries on node.js v4\n- Even faster than node-gyp C++ based [msgpack](https://www.npmjs.com/package/msgpack) library (**90% faster** on encoding)\n- Streaming encoding and decoding interface is also available. It's more faster.\n- Ready for [Web browsers](https://saucelabs.com/u/msgpack-lite) including Chrome, Firefox, Safari and even IE8\n- [Tested](https://travis-ci.org/kawanet/msgpack-lite) on Node.js v0.10, v0.12, v4, v5 and v6 as well as Web browsers\n\n### Encoding and Decoding MessagePack\n\n```js\nvar msgpack = require(\"msgpack-lite\");\n\n// encode from JS Object to MessagePack (Buffer)\nvar buffer = msgpack.encode({\"foo\": \"bar\"});\n\n// decode from MessagePack (Buffer) to JS Object\nvar data = msgpack.decode(buffer); // => {\"foo\": \"bar\"}\n\n// if encode/decode receives an invalid argument an error is thrown\n```\n\n### Writing to MessagePack Stream\n\n```js\nvar fs = require(\"fs\");\nvar msgpack = require(\"msgpack-lite\");\n\nvar writeStream = fs.createWriteStream(\"test.msp\");\nvar encodeStream = msgpack.createEncodeStream();\nencodeStream.pipe(writeStream);\n\n// send multiple objects to stream\nencodeStream.write({foo: \"bar\"});\nencodeStream.write({baz: \"qux\"});\n\n// call this once you're done writing to the stream.\nencodeStream.end();\n```\n\n### Reading from MessagePack Stream\n\n```js\nvar fs = require(\"fs\");\nvar msgpack = require(\"msgpack-lite\");\n\nvar readStream = fs.createReadStream(\"test.msp\");\nvar decodeStream = msgpack.createDecodeStream();\n\n// show multiple objects decoded from stream\nreadStream.pipe(decodeStream).on(\"data\", console.warn);\n```\n\n### Decoding MessagePack Bytes Array\n\n```js\nvar msgpack = require(\"msgpack-lite\");\n\n// decode() accepts Buffer instance per default\nmsgpack.decode(Buffer([0x81, 0xA3, 0x66, 0x6F, 0x6F, 0xA3, 0x62, 0x61, 0x72]));\n\n// decode() also accepts Array instance\nmsgpack.decode([0x81, 0xA3, 0x66, 0x6F, 0x6F, 0xA3, 0x62, 0x61, 0x72]);\n\n// decode() accepts raw Uint8Array instance as well\nmsgpack.decode(new Uint8Array([0x81, 0xA3, 0x66, 0x6F, 0x6F, 0xA3, 0x62, 0x61, 0x72]));\n```\n\n### Command Line Interface\n\nA CLI tool bin/msgpack converts data stream from JSON to MessagePack and vice versa.\n\n```sh\n$ echo '{\"foo\": \"bar\"}' | ./bin/msgpack -Jm | od -tx1\n0000000    81  a3  66  6f  6f  a3  62  61  72\n\n$ echo '{\"foo\": \"bar\"}' | ./bin/msgpack -Jm | ./bin/msgpack -Mj\n{\"foo\":\"bar\"}\n```\n\n### Installation\n\n```sh\n$ npm install --save msgpack-lite\n```\n\n### Tests\n\nRun tests on node.js:\n\n```sh\n$ make test\n```\n\nRun tests on browsers:\n\n```sh\n$ make test-browser-local\nopen the following url in a browser:\nhttp://localhost:4000/__zuul\n```\n\n### Browser Build\n\nBrowser version [msgpack.min.js](https://rawgit.com/kawanet/msgpack-lite/master/dist/msgpack.min.js) is also available. 50KB minified, 14KB gziped.\n\n```html\n<!--[if lte IE 9]>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.1.10/es5-shim.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/json3/3.3.2/json3.min.js\"></script>\n<![endif]-->\n<script src=\"https://rawgit.com/kawanet/msgpack-lite/master/dist/msgpack.min.js\"></script>\n<script>\n// encode from JS Object to MessagePack (Uint8Array)\nvar buffer = msgpack.encode({foo: \"bar\"});\n\n// decode from MessagePack (Uint8Array) to JS Object\nvar array = new Uint8Array([0x81, 0xA3, 0x66, 0x6F, 0x6F, 0xA3, 0x62, 0x61, 0x72]);\nvar data = msgpack.decode(array);\n</script>\n```\n\n### MessagePack With Browserify\n\nStep #1: write some code at first.\n\n```js\nvar msgpack = require(\"msgpack-lite\");\nvar buffer = msgpack.encode({\"foo\": \"bar\"});\nvar data = msgpack.decode(buffer);\nconsole.warn(data); // => {\"foo\": \"bar\"}\n```\n\nProceed to the next steps if you prefer faster browserify compilation time.\n\nStep #2: add `browser` property on `package.json` in your project. This refers the global `msgpack` object instead of including whole of `msgpack-lite` source code.\n\n```json\n{\n  \"dependencies\": {\n    \"msgpack-lite\": \"*\"\n  },\n  \"browser\": {\n    \"msgpack-lite\": \"msgpack-lite/global\"\n  }\n}\n```\n\nStep #3: compile it with [browserify](https://www.npmjs.com/package/browserify) and [uglifyjs](https://www.npmjs.com/package/uglify-js).\n\n```sh\nbrowserify src/main.js -o tmp/main.browserify.js -s main\nuglifyjs tmp/main.browserify.js -m -c -o js/main.min.js\ncp node_modules/msgpack-lite/dist/msgpack.min.js js/msgpack.min.js\n```\n\nStep #4: load [msgpack.min.js](https://rawgit.com/kawanet/msgpack-lite/master/dist/msgpack.min.js) before your code.\n\n```html\n<script src=\"js/msgpack.min.js\"></script>\n<script src=\"js/main.min.js\"></script>\n```\n\n### Interoperability\n\nIt is tested to have basic compatibility with other Node.js MessagePack modules below:\n\n- [https://www.npmjs.com/package/msgpack](https://www.npmjs.com/package/msgpack) (1.0.2)\n- [https://www.npmjs.com/package/msgpack-js](https://www.npmjs.com/package/msgpack-js) (0.3.0)\n- [https://www.npmjs.com/package/msgpack-js-v5](https://www.npmjs.com/package/msgpack-js-v5) (0.3.0-v5)\n- [https://www.npmjs.com/package/msgpack-unpack](https://www.npmjs.com/package/msgpack-unpack) (2.1.1)\n- [https://github.com/msgpack/msgpack-javascript](https://github.com/msgpack/msgpack-javascript) (msgpack.codec)\n- [https://www.npmjs.com/package/msgpack5](https://www.npmjs.com/package/msgpack5) (3.3.0)\n- [https://www.npmjs.com/package/notepack](https://www.npmjs.com/package/notepack) (0.0.2)\n\n### Benchmarks\n\nA benchmark tool `lib/benchmark.js` is available to compare encoding/decoding speed\n(operation per second) with other MessagePack modules.\nIt counts operations of [1KB JSON document](https://github.com/kawanet/msgpack-lite/blob/master/test/example.json) in 10 seconds.\n\n```sh\n$ npm install msgpack msgpack-js msgpack-js-v5 msgpack-unpack msgpack5 notepack\n$ npm run benchmark 10\n```\n\noperation                                                 |   op   |   ms  |  op/s \n--------------------------------------------------------- | -----: | ----: | -----:\nbuf = Buffer(JSON.stringify(obj));                        | 1055200 | 10000 | 105520\nobj = JSON.parse(buf);                                    | 863800 | 10000 |  86380\nbuf = require(\"msgpack-lite\").encode(obj);                | 969100 | 10000 |  96910\nobj = require(\"msgpack-lite\").decode(buf);                | 600300 | 10000 |  60030\nbuf = require(\"msgpack\").pack(obj);                       | 503500 | 10001 |  50344\nobj = require(\"msgpack\").unpack(buf);                     | 560200 | 10001 |  56014\nbuf = Buffer(require(\"msgpack.codec\").msgpack.pack(obj)); | 653500 | 10000 |  65349\nobj = require(\"msgpack.codec\").msgpack.unpack(buf);       | 367500 | 10001 |  36746\nbuf = require(\"msgpack-js-v5\").encode(obj);               | 189500 | 10002 |  18946\nobj = require(\"msgpack-js-v5\").decode(buf);               | 408900 | 10000 |  40890\nbuf = require(\"msgpack-js\").encode(obj);                  | 189200 | 10000 |  18920\nobj = require(\"msgpack-js\").decode(buf);                  | 375600 | 10002 |  37552\nbuf = require(\"msgpack5\")().encode(obj);                  | 110500 | 10009 |  11040\nobj = require(\"msgpack5\")().decode(buf);                  | 165500 | 10000 |  16550\nbuf = require(\"notepack\")().encode(obj);                  | 847800 | 10000 |  84780\nobj = require(\"notepack\")().decode(buf);                  | 599800 | 10000 |  59980\nobj = require(\"msgpack-unpack\").decode(buf);              |  48100 | 10002 |   4809\n\nStreaming benchmark tool `lib/benchmark-stream.js` is also available.\nIt counts milliseconds for 1,000,000 operations of 30 bytes fluentd msgpack fragment.\nThis shows streaming encoding and decoding are super faster.\n\n```sh\n$ npm run benchmark-stream 2\n```\n\noperation (1000000 x 2)                          |   op    |  ms   |  op/s \n------------------------------------------------ | ------: | ----: | -----:\nstream.write(msgpack.encode(obj));               | 1000000 |  3027 | 330360\nstream.write(notepack.encode(obj));              | 1000000 |  2012 | 497017\nmsgpack.Encoder().on(\"data\",ondata).encode(obj); | 1000000 |  2956 | 338294\nmsgpack.createEncodeStream().write(obj);         | 1000000 |  1888 | 529661\nstream.write(msgpack.decode(buf));               | 1000000 |  2020 | 495049\nstream.write(notepack.decode(buf));              | 1000000 |  1794 | 557413\nmsgpack.Decoder().on(\"data\",ondata).decode(buf); | 1000000 |  2744 | 364431\nmsgpack.createDecodeStream().write(buf);         | 1000000 |  1341 | 745712\n\nTest environment: msgpack-lite 0.1.14, Node v4.2.3, Intel(R) Xeon(R) CPU E5-2666 v3 @ 2.90GHz\n\n### MessagePack Mapping Table\n\nThe following table shows how JavaScript objects (value) will be mapped to \n[MessagePack formats](https://github.com/msgpack/msgpack/blob/master/spec.md)\nand vice versa.\n\nSource Value|MessagePack Format|Value Decoded\n----|----|----\nnull, undefined|nil format family|null\nBoolean (true, false)|bool format family|Boolean (true, false)\nNumber (32bit int)|int format family|Number (int or double)\nNumber (64bit double)|float format family|Number (double)\nString|str format family|String\nBuffer|bin format family|Buffer\nArray|array format family|Array\nMap|map format family|Map (if `usemap=true`)\nObject (plain object)|map format family|Object (or Map if `usemap=true`)\nObject (see below)|ext format family|Object (see below)\n\nNote that both `null` and `undefined` are mapped to nil `0xC1` type.\nThis means `undefined` value will be *upgraded* to `null` in other words.\n\n### Extension Types\n\nThe MessagePack specification allows 128 application-specific extension types. \nThe library uses the following types to make round-trip conversion possible\nfor JavaScript native objects.\n\nType|Object|Type|Object\n----|----|----|----\n0x00||0x10|\n0x01|EvalError|0x11|Int8Array\n0x02|RangeError|0x12|Uint8Array\n0x03|ReferenceError|0x13|Int16Array\n0x04|SyntaxError|0x14|Uint16Array\n0x05|TypeError|0x15|Int32Array\n0x06|URIError|0x16|Uint32Array\n0x07||0x17|Float32Array\n0x08||0x18|Float64Array\n0x09||0x19|Uint8ClampedArray\n0x0A|RegExp|0x1A|ArrayBuffer\n0x0B|Boolean|0x1B|Buffer\n0x0C|String|0x1C|\n0x0D|Date|0x1D|DataView\n0x0E|Error|0x1E|\n0x0F|Number|0x1F|\n\nOther extension types are mapped to built-in ExtBuffer object.\n\n### Custom Extension Types (Codecs)\n\nRegister a custom extension type number to serialize/deserialize your own class instances.\n\n```js\nvar msgpack = require(\"msgpack-lite\");\n\nvar codec = msgpack.createCodec();\ncodec.addExtPacker(0x3F, MyVector, myVectorPacker);\ncodec.addExtUnpacker(0x3F, myVectorUnpacker);\n\nvar data = new MyVector(1, 2);\nvar encoded = msgpack.encode(data, {codec: codec});\nvar decoded = msgpack.decode(encoded, {codec: codec});\n\nfunction MyVector(x, y) {\n  this.x = x;\n  this.y = y;\n}\n\nfunction myVectorPacker(vector) {\n  var array = [vector.x, vector.y];\n  return msgpack.encode(array); // return Buffer serialized\n}\n\nfunction myVectorUnpacker(buffer) {\n  var array = msgpack.decode(buffer);\n  return new MyVector(array[0], array[1]); // return Object deserialized\n}\n```\n\nThe first argument of `addExtPacker` and `addExtUnpacker` should be an integer within the range of 0 and 127 (0x0 and 0x7F). `myClassPacker` is a function that accepts an instance of `MyClass`, and should return a buffer representing that instance. `myClassUnpacker` is the opposite: it accepts a buffer and should return an instance of `MyClass`.\n\nIf you pass an array of functions to `addExtPacker` or `addExtUnpacker`, the value to be encoded/decoded will pass through each one in order. This allows you to do things like this:\n\n```js\ncodec.addExtPacker(0x00, Date, [Number, msgpack.encode]);\n```\n\nYou can also pass the `codec` option to `msgpack.Decoder(options)`, `msgpack.Encoder(options)`, `msgpack.createEncodeStream(options)`, and `msgpack.createDecodeStream(options)`.\n\nIf you wish to modify the default built-in codec, you can access it at `msgpack.codec.preset`.\n\n### Custom Codec Options\n\n`msgpack.createCodec()` function accepts some options.\n\nIt does NOT have the preset extension types defined when no options given.\n\n```js\nvar codec = msgpack.createCodec();\n```\n\n`preset`: It has the preset extension types described above.\n\n```js\nvar codec = msgpack.createCodec({preset: true});\n```\n\n`safe`: It runs a validation of the value before writing it into buffer. This is the default behavior for some old browsers which do not support `ArrayBuffer` object.\n\n```js\nvar codec = msgpack.createCodec({safe: true});\n```\n\n`useraw`: It uses `raw` formats instead of `bin` and `str`.\n\n```js\nvar codec = msgpack.createCodec({useraw: true});\n```\n\n`int64`: It decodes msgpack's `int64`/`uint64` formats with [int64-buffer](https://github.com/kawanet/int64-buffer) object.\n\n```js\nvar codec = msgpack.createCodec({int64: true});\n```\n\n`binarraybuffer`: It ties msgpack's `bin` format with `ArrayBuffer` object, instead of `Buffer` object.\n\n```js\nvar codec = msgpack.createCodec({binarraybuffer: true, preset: true});\n```\n\n`uint8array`: It returns Uint8Array object when encoding, instead of `Buffer` object.\n\n```js\nvar codec = msgpack.createCodec({uint8array: true});\n```\n\n`usemap`: Uses the global JavaScript Map type, if available, to unpack\nMessagePack map elements.\n\n```js\nvar codec = msgpack.createCodec({usemap: true});\n```\n\n`canonical`: Encodes object's keys in alphabetical order to ensure canonical representation.\n\n```js\nvar codec = msgpack.createCodec({canonical: true});\n```\n\n### Compatibility Mode\n\nThe [compatibility mode](https://github.com/kawanet/msgpack-lite/issues/22) respects for [msgpack's old spec](https://github.com/msgpack/msgpack/blob/master/spec-old.md). Set `true` to `useraw`.\n\n```js\n// default mode handles both str and bin formats individually\nmsgpack.encode(\"Aa\"); // => <Buffer a2 41 61> (str format)\nmsgpack.encode(new Buffer([0x41, 0x61])); // => <Buffer c4 02 41 61> (bin format)\n\nmsgpack.decode(new Buffer([0xa2, 0x41, 0x61])); // => 'Aa' (String)\nmsgpack.decode(new Buffer([0xc4, 0x02, 0x41, 0x61])); // => <Buffer 41 61> (Buffer)\n\n// compatibility mode handles only raw format both for String and Buffer\nvar options = {codec: msgpack.createCodec({useraw: true})};\nmsgpack.encode(\"Aa\", options); // => <Buffer a2 41 61> (raw format)\nmsgpack.encode(new Buffer([0x41, 0x61]), options); // => <Buffer a2 41 61> (raw format)\n\nmsgpack.decode(new Buffer([0xa2, 0x41, 0x61]), options); // => <Buffer 41 61> (Buffer)\nmsgpack.decode(new Buffer([0xa2, 0x41, 0x61]), options).toString(); // => 'Aa' (String)\n```\n\n### Repository\n\n- [https://github.com/kawanet/msgpack-lite](https://github.com/kawanet/msgpack-lite)\n\n### See Also\n\n- [http://msgpack.org/](http://msgpack.org/)\n\n### License\n\nThe MIT License (MIT)\n\nCopyright (c) 2015-2016 Yusuke Kawasaki\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n", "release_dates": []}, {"name": "msgpackdiff", "description": "A command line tool for comparing MessagePack objects", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# msgpackdiff\n\nThis is a command line tool written in Go that provides a way to compare two arbitrary\n[MessagePack](https://msgpack.org/) encoded objects for equality. It uses the\n[algorand/msgp](https://github.com/algorand/msgp) library to parse MessagePack objects.\n\n## Build\n\nTo build the tool, clone the repo and run `make build`. This will place a copy of the `msgpackdiff`\nbinary in the `./bin` folder. You can place this binary somewhere on your system path to invoke it\nfrom anywhere.\n\n## Usage\n\nThe tool can be invoked from the command line like so:\n\n```\nmsgpackdiff (flags) [A] [B]\n```\n\n`[A]` and `[B]` are required arguments and should be replaced with actual MessagePack objects.\n`msgpackdiff` will parse each object and compare them for equality. If the objects are equal, the\nprogram will exit with status code 0. If they are unequal, the progarm will exit with status code 1\nand print the parts that differ to stdout.\n\n### Object encoding\nThe objects `[A]` and `[B]` can be any of the following:\n* A base64 encoded string of a MessagePack object.\n* A path to a file that contains only the MessagePack object. The conents may be binary or a base64\n  encoded string.\n\n### Flags\n* `--brief` enables quiet mode, which causes the program to refrain from outputting a detailed\n  report if the objects are different. If the objects are equal, the program will output nothing.\n* `--ignore-empty` causes the tool to ignore differences that can be explained by one MessagePack\n  object omitting empty fields and the other keeping them. For example, if the objects\n  `{\"key\": \"val\"}` and `{\"key\": \"val\", \"extraKey\": \"\"}` were encoded with MessagePack and compared with\n  this flag enabled, they would be considered equivalent since the value for `extraKey` is the empty\n  string, which is the default string value in Go.\n* `--ignore-order` disables strict ordering of fields. Without this flag, each object must define\n  the same fields in the same order, resulting in the following objects being considered different:\n  `{\"a\": 1, \"b\": 2}`, `{\"b\": 2, \"a\": 1}`. However with this flag, the objects would be considered\n  equivalent. This affects the order of all fields in objects, regardless of whether they are in the\n  top level object or subobjects.\n* `--flexible-types` disables strict type comparisons. Without this flag, int, uint, float32,\n  float64, complex64, and complex128 types will never be compared to each other since they are\n  assumed to be unequal due to their type. However with this flag, values belonging to these\n  different numerical types will be cast to the larger type and compared using Go's == operator. In\n  some cases this comparison will be inaccurate, such as between int64 and float32, since neither\n  type can hold all values of the other.\n  NOTE: This flag does not change the behavior of comparing different types within the int8/16/32/64\n  family, which are always compared with each other regardless of what length they are. The same is\n  true for the uint8/16/32/64 family, but not between the int and uint families.\n* `--context` adjusts the number of nearby fields to show in difference reports. Defaults to 3.\n", "release_dates": []}, {"name": "mule", "description": "General automation framework", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "![Mule logo](docs/img/mule-logo.png)\n\n# Mule\n\nMule is a generalized automation framework for organizing the execution of defined automation in a declarative yaml format. To consume it, a user only needs to write a yaml file that defines tasks they wish to execute and jobs that execute their order. Below is an example of a basic mule.yaml file. For more information, see our [getting started guide](docs/getting_started.md)!\n\n```yaml\ntasks:\n- task: Echo\n  name: A\n  message: A\n\njobs:\n  job-a:\n    tasks:\n    - Echo.A\n```\n\n## Install\n\nTo install mule using pip, use:\n\n```\npip install mulecli\n```\n\n## Git Hooks\n\nThere are two custom `pre-commit` hooks that can be easily installed by running the `install.sh` script found within the `./scripts/hooks/` directory:\n\n```\n./scripts/hooks/install.sh\n```\n\nThese will install two local `pre-commit` hooks in `./.git/config`. The hooks have two dependencies, [pytest] and [pycodestyle]. There are no default filters for [pycodestyle], that is left up to the developer.\n\n`pytest` is already a dependency of `mule`. To install `pycodestyle`:\n\n```\npython -m pip pycodestyle\n```\n\nOnce installed, they can be ignored when committing, if needed:\n\n```\ngit commit --no-verify\n```\n\n# Navigation\n* [Getting Started Guide](docs/getting_started.md)\n* [Task Documentation](docs/tasks/README.md)\n* [How to Contribute](docs/contribution.md)\n\n[pytest](https://pypi.org/project/pytest/)\n[pycodestyle](https://pypi.org/project/pycodestyle/)\n\n", "release_dates": []}, {"name": "node-ui", "description": "Terminal UI for remote Algorand node management.", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# **DISCLAIMER**\nThis project is not supported, and is subject to change. There are no compatibility guarantees. It was developed during a hackathon and isn't perfect. If you see any strange artifacts, try resizing the window to help the program re-orient itself.\n\n# Algod Node UI\n\nTerminal UI for remote Algorand node management.\n\n![Example Screenshot](images/demo.png)\n\n# Install\n## Download\nSee the GitHub releases and download the binary for your platform.\n\n## Source\nUse go1.20.5 or later and build with `make`.\n\n# Usage\nWith no options, the UI will be displayed instead of starting a service.\n\n## Local Algod\n```\n~$ ALGORAND_DATA=path/to/data/dir ./nodeui\n```\n## Remote Algod\n```\n~$ ./nodeui -t <algod api token> -u http://<url>\n```\n\n# Run as a service\n\nThe preferred method for running the node UI is as a service running alongside algod. By passing a port using `-p` or `--tui-port` an SSH server is started and can host the UI for multiple clients.\n\nA tool like [wishlist](https://github.com/charmbracelet/wishlist#wishlist) can be used to interactively select between multiple node deployments. In the screenshot below you can see a sample ssh config file, and the UI wishlist provides to select which nodeui to connect to.\n\n![Wishlist Example](images/wishlist_example.png)\n\n# Features\n\n## Status\n\nRealtime node status, including detailed fast-catchup progress.\n\n## Block Explorer\n\nDisplay realtime block data, drill down into a block to see all of the transactions and transaction details.\n\n## Utilities\n\nStart a fast catchup with the press of a key, and more (if you build it)!\n\n## Built in documentation\n\n[Kind of](tui/internal/bubbles/about/help.go).\n\n# Architecture\n\nBuilt using [Bubble Tea](https://github.com/charmbracelet/bubbletea). Node information is collected from the Algod REST API using the [go SDK](https://github.com/algorand/go-algorand-sdk), and from reading files on disk.\n\nEach box on the screen is a \"bubble\", they manage state independently with an event update loop. Events are passed to each bubble, which have the option of consuming the event and/or passing it along to any nested bubbles. When processing the event, they may optionally add follow-up tasks which the scheduling engine would execute asynchronously. Follow-up tasks may optionally create more events which would be processed in turn using the same mechanism.\n\nWhen displaying the UI, each bubble is asked to renders itself and they are finally joined together for final rendering using [lipgloss](https://github.com/charmbracelet/lipgloss). Web development aficionado may recognize this pattern as [The Elm Architecture](https://guide.elm-lang.org/architecture/).\n\nThere are some quirks to this approach. The main one is that bubbletea is a rendering engine, NOT a window manager. This means that things like window heights and widths must be self-managed. Any mismanagement leads to very strange artifacts as the rendering engine tries to fit too many, or too few lines to a fixed sized terminal.\n\n# Contributing\n\nContributions are welcome! There are no plans to actively maintain this project, so if you find it useful please consider helping out.\n\n# How to create a new release\n\n1. Create a tag: `git tag -a v_._._ -m \"v_._._\" && git push origin v_._._`\n2. Push the tag.\n3. CI should create a release, attach it to GitHub and publish images to docker hub.\n", "release_dates": ["2023-07-07T15:30:56Z", "2022-09-15T12:48:03Z", "2022-09-07T15:19:51Z", "2022-09-06T20:25:00Z", "2022-08-19T13:14:08Z", "2022-06-06T15:38:55Z", "2022-06-05T17:24:14Z"]}, {"name": "node_exporter", "description": "Exporter for machine metrics", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Node exporter\n\n[![CircleCI](https://circleci.com/gh/prometheus/node_exporter/tree/master.svg?style=shield)][circleci]\n[![Buildkite status](https://badge.buildkite.com/94a0c1fb00b1f46883219c256efe9ce01d63b6505f3a942f9b.svg)](https://buildkite.com/prometheus/node-exporter)\n[![Docker Repository on Quay](https://quay.io/repository/prometheus/node-exporter/status)][quay]\n[![Docker Pulls](https://img.shields.io/docker/pulls/prom/node-exporter.svg?maxAge=604800)][hub]\n[![Go Report Card](https://goreportcard.com/badge/github.com/prometheus/node_exporter)][goreportcard]\n\n**THIS FORK HAS ONE ADDITIONAL [gateway.go](https://github.com/algorand/node_exporter/blob/master/collector/gateway.go) COLLECTOR  FOR ALGOD WITH BUFFERING AND OTHER FEATURES**\n\nPrometheus exporter for hardware and OS metrics exposed by \\*NIX kernels, written\nin Go with pluggable metric collectors.\n\nThe [WMI exporter](https://github.com/martinlindhe/wmi_exporter) is recommended for Windows users.\n\n## Collectors\n\nThere is varying support for collectors on each operating system. The tables\nbelow list all existing collectors and the supported systems.\n\nCollectors are enabled by providing a `--collector.<name>` flag.\nCollectors that are enabled by default can be disabled by providing a `--no-collector.<name>` flag.\n\n### Enabled by default\n\nName     | Description | OS\n---------|-------------|----\narp | Exposes ARP statistics from `/proc/net/arp`. | Linux\nbcache | Exposes bcache statistics from `/sys/fs/bcache/`. | Linux\nbonding | Exposes the number of configured and active slaves of Linux bonding interfaces. | Linux\nboottime | Exposes system boot time derived from the `kern.boottime` sysctl. | Darwin, Dragonfly, FreeBSD, NetBSD, OpenBSD\nconntrack | Shows conntrack statistics (does nothing if no `/proc/sys/net/netfilter/` present). | Linux\ncpu | Exposes CPU statistics | Darwin, Dragonfly, FreeBSD, Linux\ndiskstats | Exposes disk I/O statistics. | Darwin, Linux\nedac | Exposes error detection and correction statistics. | Linux\nentropy | Exposes available entropy. | Linux\nexec | Exposes execution statistics. | Dragonfly, FreeBSD\nfilefd | Exposes file descriptor statistics from `/proc/sys/fs/file-nr`. | Linux\nfilesystem | Exposes filesystem statistics, such as disk space used. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD\nhwmon | Expose hardware monitoring and sensor data from `/sys/class/hwmon/`. | Linux\ninfiniband | Exposes network statistics specific to InfiniBand and Intel OmniPath configurations. | Linux\nipvs | Exposes IPVS status from `/proc/net/ip_vs` and stats from `/proc/net/ip_vs_stats`. | Linux\nloadavg | Exposes load average. | Darwin, Dragonfly, FreeBSD, Linux, NetBSD, OpenBSD, Solaris\nmdadm | Exposes statistics about devices in `/proc/mdstat` (does nothing if no `/proc/mdstat` present). | Linux\nmeminfo | Exposes memory statistics. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD\nnetclass | Exposes network interface info from `/sys/class/net/` | Linux\nnetdev | Exposes network interface statistics such as bytes transferred. | Darwin, Dragonfly, FreeBSD, Linux, OpenBSD\nnetstat | Exposes network statistics from `/proc/net/netstat`. This is the same information as `netstat -s`. | Linux\nnfs | Exposes NFS client statistics from `/proc/net/rpc/nfs`. This is the same information as `nfsstat -c`. | Linux\nnfsd | Exposes NFS kernel server statistics from `/proc/net/rpc/nfsd`. This is the same information as `nfsstat -s`. | Linux\nsockstat | Exposes various statistics from `/proc/net/sockstat`. | Linux\nstat | Exposes various statistics from `/proc/stat`. This includes boot time, forks and interrupts. | Linux\ntextfile | Exposes statistics read from local disk. The `--collector.textfile.directory` flag must be set. | _any_\ntime | Exposes the current system time. | _any_\ntimex | Exposes selected adjtimex(2) system call stats. | Linux\nuname | Exposes system information as provided by the uname system call. | Linux\nvmstat | Exposes statistics from `/proc/vmstat`. | Linux\nwifi | Exposes WiFi device and station statistics. | Linux\nxfs | Exposes XFS runtime statistics. | Linux (kernel 4.4+)\nzfs | Exposes [ZFS](http://open-zfs.org/) performance statistics. | [Linux](http://zfsonlinux.org/)\n\n### Disabled by default\n\nName     | Description | OS\n---------|-------------|----\nbuddyinfo | Exposes statistics of memory fragments as reported by /proc/buddyinfo. | Linux\ndevstat | Exposes device statistics | Dragonfly, FreeBSD\ndrbd | Exposes Distributed Replicated Block Device statistics (to version 8.4) | Linux\ninterrupts | Exposes detailed interrupts statistics. | Linux, OpenBSD\nksmd | Exposes kernel and system statistics from `/sys/kernel/mm/ksm`. | Linux\nlogind | Exposes session counts from [logind](http://www.freedesktop.org/wiki/Software/systemd/logind/). | Linux\nmeminfo\\_numa | Exposes memory statistics from `/proc/meminfo_numa`. | Linux\nmountstats | Exposes filesystem statistics from `/proc/self/mountstats`. Exposes detailed NFS client statistics. | Linux\nntp | Exposes local NTP daemon health to check [time](./docs/TIME.md) | _any_\nqdisc | Exposes [queuing discipline](https://en.wikipedia.org/wiki/Network_scheduler#Linux_kernel) statistics | Linux\nrunit | Exposes service status from [runit](http://smarden.org/runit/). | _any_\nsupervisord | Exposes service status from [supervisord](http://supervisord.org/). | _any_\nsystemd | Exposes service and system status from [systemd](http://www.freedesktop.org/wiki/Software/systemd/). | Linux\ntcpstat | Exposes TCP connection status information from `/proc/net/tcp` and `/proc/net/tcp6`. (Warning: the current version has potential performance issues in high load situations.) | Linux\n\n### Textfile Collector\n\nThe textfile collector is similar to the [Pushgateway](https://github.com/prometheus/pushgateway),\nin that it allows exporting of statistics from batch jobs. It can also be used\nto export static metrics, such as what role a machine has. The Pushgateway\nshould be used for service-level metrics. The textfile module is for metrics\nthat are tied to a machine.\n\nTo use it, set the `--collector.textfile.directory` flag on the Node exporter. The\ncollector will parse all files in that directory matching the glob `*.prom`\nusing the [text\nformat](http://prometheus.io/docs/instrumenting/exposition_formats/).\n\nTo atomically push completion time for a cron job:\n```\necho my_batch_job_completion_time $(date +%s) > /path/to/directory/my_batch_job.prom.$$\nmv /path/to/directory/my_batch_job.prom.$$ /path/to/directory/my_batch_job.prom\n```\n\nTo statically set roles for a machine using labels:\n```\necho 'role{role=\"application_server\"} 1' > /path/to/directory/role.prom.$$\nmv /path/to/directory/role.prom.$$ /path/to/directory/role.prom\n```\n\n### Filtering enabled collectors\n\nThe `node_exporter` will expose all metrics from enabled collectors by default.  This is the recommended way to collect metrics to avoid errors when comparing metrics of different families.\n\nFor advanced use the `node_exporter` can be passed an optional list of collectors to filter metrics. The `collect[]` parameter may be used multiple times.  In Prometheus configuration you can use this syntax under the [scrape config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<scrape_config>).\n\n```\n  params:\n    collect[]:\n      - foo\n      - bar\n```\n\nThis can be useful for having different Prometheus servers collect specific metrics from nodes.\n\n## Building and running\n\nPrerequisites:\n\n* [Go compiler](https://golang.org/dl/)\n* RHEL/CentOS: `glibc-static` package.\n\nBuilding:\n\n    go get github.com/algorand/node_exporter\n    cd ${GOPATH-$HOME/go}/src/github.com/algorand/node_exporter\n    make\n    ./node_exporter <flags>\n\nTo see all available configuration flags:\n\n    ./node_exporter -h\n\n## Running tests\n\n    make test\n\n\n## Using Docker\nThe `node_exporter` is designed to monitor the host system. It's not recommended\nto deploy it as a Docker container because it requires access to the host system.\nBe aware that any non-root mount points you want to monitor will need to be bind-mounted\ninto the container.\nIf you start container for host monitoring, specify `path.rootfs` argument.\nThis argument must match path in bind-mount of host root. The node\\_exporter will use\n`path.rootfs` as prefix to access host filesystem.\n\n```bash\ndocker run -d \\\n  --net=\"host\" \\\n  --pid=\"host\" \\\n  -v \"/:/host:ro,rslave\" \\\n  quay.io/prometheus/node-exporter \\\n  --path.rootfs /host\n```\n\nOn some systems, the `timex` collector requires an additional Docker flag,\n`--cap-add=SYS_TIME`, in order to access the required syscalls.\n\n## Using a third-party repository for RHEL/CentOS/Fedora\n\nThere is a [community-supplied COPR repository](https://copr.fedorainfracloud.org/coprs/ibotty/prometheus-exporters/) which closely follows upstream releases.\n\n[travis]: https://travis-ci.org/prometheus/node_exporter\n[hub]: https://hub.docker.com/r/prom/node-exporter/\n[circleci]: https://circleci.com/gh/prometheus/node_exporter\n[quay]: https://quay.io/repository/prometheus/node-exporter\n[goreportcard]: https://goreportcard.com/report/github.com/prometheus/node_exporter\n", "release_dates": []}, {"name": "oapi-codegen", "description": "Generate Go client and server boilerplate from OpenAPI 3 specifications", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "OpenAPI Client and Server Code Generator\n----------------------------------------\n\nThis package contains a set of utilities for generating Go boilerplate code for\nservices based on\n[OpenAPI 3.0](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md)\nAPI definitions. When working with services, it's important to have an API\ncontract which servers and clients both implement to minimize the chances of\nincompatibilities. It's tedious to generate Go models which precisely correspond to\nOpenAPI specifications, so let our code generator do that work for you, so that\nyou can focus on implementing the business logic for your service.\n\nWe have chosen to focus on [Echo](https://github.com/labstack/echo) as\nour default HTTP routing engine, due to its speed and simplicity for the generated\nstubs, and [Chi](https://github.com/go-chi/chi), and [Gin](https://github.com/gin-gonic/gin)\nhave also been added by contributors as additional routers. We chose Echo because\nthe `Context` object is a mockable interface, and it allows for some advanced\ntesting.\n\nThis package tries to be too simple rather than too generic, so we've made some\ndesign decisions in favor of simplicity, knowing that we can't generate strongly\ntyped Go code for all possible OpenAPI Schemas. If there is a way to accomplish\nsomething via utility code or reflection, it's probably a better approach than\ncode generation, which is fragile due to the very dynamic nature of OpenAPI and\nthe very static nature of Go.\n\n## Overview\n\nWe're going to use the OpenAPI example of the\n[Expanded Petstore](https://github.com/OAI/OpenAPI-Specification/blob/master/examples/v3.0/petstore-expanded.yaml)\nin the descriptions below, please have a look at it.\n\nIn order to create a Go server to serve this exact schema, you would have to\nwrite a lot of boilerplate code to perform all the marshalling and unmarshalling\ninto objects which match the OpenAPI 3.0 definition. The code generator in this\ndirectory does a lot of that for you. You would run it like so:\n\n    go get github.com/algorand/oapi-codegen/cmd/oapi-codegen\n    oapi-codegen petstore-expanded.yaml  > petstore.gen.go\n\nLet's go through that `petstore.gen.go` file to show you everything which was\ngenerated.\n\n\n## Generated Server Boilerplate\n\nThe `/components/schemas` section in OpenAPI defines reusable objects, so Go\ntypes are generated for these. The Pet Store example defines `Error`, `Pet`,\n`Pets` and `NewPet`, so we do the same in Go:\n```go\n// Error defines model for Error.\ntype Error struct {\n    // Error code\n    Code int32 `json:\"code\"`\n\n    // Error message\n    Message string `json:\"message\"`\n}\n\n// NewPet defines model for NewPet.\ntype NewPet struct {\n    // Name of the pet\n    Name string `json:\"name\"`\n\n    // Type of the pet\n    Tag *string `json:\"tag,omitempty\"`\n}\n\n// Pet defines model for Pet.\ntype Pet struct {\n    // Unique id of the pet\n    Id int64 `json:\"id\"`\n\n    // Name of the pet\n    Name string `json:\"name\"`\n\n    // Type of the pet\n    Tag *string `json:\"tag,omitempty\"`\n}\n\n// Type definition for component schema \"Pets\"\ntype Pets []Pet\n```\n\nIt's best to define objects under `/components` field in the schema, since\nthose will be turned into named Go types. If you use inline types in your\nhandler definitions, we will generate inline, anonymous Go types, but those\nare more tedious to deal with since you will have to redeclare them at every\npoint of use.\n\nFor each element in the `paths` map in OpenAPI, we will generate a Go handler\nfunction in an interface object. Here is the generated Go interface for our\nEcho server.\n\n```go\ntype ServerInterface interface {\n    //  (GET /pets)\n    FindPets(ctx echo.Context, params FindPetsParams) error\n    //  (POST /pets)\n    AddPet(ctx echo.Context) error\n    //  (DELETE /pets/{id})\n    DeletePet(ctx echo.Context, id int64) error\n    //  (GET /pets/{id})\n    FindPetById(ctx echo.Context, id int64) error\n}\n```\n\nThese are the functions which you will implement yourself in order to create\na server conforming to the API specification. Normally, all the arguments and\nparameters are stored on the `echo.Context` in handlers, so we do the tedious\nwork of of unmarshaling the JSON automatically, simply passing values into\nyour handlers.\n\nNotice that `FindPetById` takes a parameter `id int64`. All path arguments\nwill be passed as arguments to your function, since they are mandatory.\n\nRemaining arguments can be passed in headers, query arguments or cookies. Those\nwill be written to a `params` object. Look at the `FindPets` function above, it\ntakes as input `FindPetsParams`, which is defined as follows:\n ```go\n// Parameters object for FindPets\ntype FindPetsParams struct {\n    Tags  *[]string `json:\"tags,omitempty\"`\n    Limit *int32   `json:\"limit,omitempty\"`\n}\n```\n\nThe HTTP query parameter `limit` turns into a Go field named `Limit`. It is\npassed by pointer, since it is an optional parameter. If the parameter is\nspecified, the pointer will be non-`nil`, and you can read its value.\n\nIf you changed the OpenAPI specification to make the parameter required, the\n`FindPetsParams` structure will contain the type by value:\n```go\ntype FindPetsParams struct {\n    Tags  *[]string `json:\"tags,omitempty\"`\n    Limit int32     `json:\"limit\"`\n}\n```\n\n### Registering handlers\nThere are a few ways of registering your http handler based on the type of server generated i.e. `-generate server` or `-generate chi-server`\n\n<details><summary><code>Echo</code></summary>\n\nCode generated using `-generate server`.\n\nThe usage of `Echo` is out of scope of this doc, but once you have an\necho instance, we generate a utility function to help you associate your handlers\nwith this autogenerated code. For the pet store, it looks like this:\n```go\nfunc RegisterHandlers(router codegen.EchoRouter, si ServerInterface) {\n    wrapper := ServerInterfaceWrapper{\n        Handler: si,\n    }\n    router.GET(\"/pets\", wrapper.FindPets)\n    router.POST(\"/pets\", wrapper.AddPet)\n    router.DELETE(\"/pets/:id\", wrapper.DeletePet)\n    router.GET(\"/pets/:id\", wrapper.FindPetById)\n}\n```\n\nThe wrapper functions referenced above contain generated code which pulls\nparameters off the `Echo` request context, and unmarshals them into Go objects.\n\nYou would register the generated handlers as follows:\n```go\nfunc SetupHandler() {\n    var myApi PetStoreImpl  // This implements the pet store interface\n    e := echo.New()\n    petstore.RegisterHandlers(e, &myApi)\n    ...\n}\n```\n\n</summary></details>\n\n<details><summary><code>Chi</code></summary>\n\nCode generated using `-generate chi-server`.\n\n```go\ntype PetStoreImpl struct {}\nfunc (*PetStoreImpl) GetPets(w http.ResponseWriter, r *http.Request) {\n    // Implement me\n}\n\nfunc SetupHandler() {\n    var myApi PetStoreImpl\n\n    r := chi.NewRouter()\n    r.Mount(\"/\", Handler(&myApi))\n}\n```\n</summary></details>\n\n<details><summary><code>Gin</code></summary>\n\nCode generated using `-generate gin`.\n\nThe usage of `gin` is out of scope of this doc, but once you have an\ngin instance, we generate a utility function to help you associate your handlers\nwith this autogenerated code. For the pet store, it looks like this:\n```go\n// RegisterHandlersWithOptions creates http.Handler with additional options\nfunc RegisterHandlersWithOptions(router *gin.Engine, si ServerInterface, options GinServerOptions) *gin.Engine {\n\twrapper := ServerInterfaceWrapper{\n\t\tHandler:            si,\n\t\tHandlerMiddlewares: options.Middlewares,\n\t}\n\n\trouter.GET(options.BaseURL+\"/pets\", wrapper.FindPets)\n\trouter.POST(options.BaseURL+\"/pets\", wrapper.AddPet)\n\trouter.DELETE(options.BaseURL+\"/pets/:id\", wrapper.DeletePet)\n\trouter.GET(options.BaseURL+\"/pets/:id\", wrapper.FindPetByID)\n\treturn router\n}\n```\n\n```go\nimport (\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/deepmap/oapi-codegen/examples/petstore-expanded/gin/api\"\n\tmiddleware \"github.com/deepmap/oapi-codegen/pkg/gin-middleware\"\n)\n\ntype PetStoreImpl struct {}\nfunc (*PetStoreImpl) GetPets(w http.ResponseWriter, r *http.Request) {\n    // Implement me\n}\n\nfunc SetupHandler() {\n    var myApi PetStoreImpl\n\n    r := gin.Default()\n\t  r.Use(middleware.OapiRequestValidator(swagger))\n    r = api.RegisterHandlers(r, petStore)\n}\n```\n</summary></details>\n\n<details><summary><code>net/http</code></summary>\n\n[Chi](https://github.com/go-chi/chi) is 100% compatible with `net/http` allowing the following with code generated using `-generate chi-server`.\n\n```go\ntype PetStoreImpl struct {}\nfunc (*PetStoreImpl) GetPets(w http.ResponseWriter, r *http.Request) {\n    // Implement me\n}\n\nfunc SetupHandler() {\n    var myApi PetStoreImpl\n\n    http.Handle(\"/\", Handler(&myApi))\n}\n```\n</summary></details>\n\n#### Additional Properties in type definitions\n\n[OpenAPI Schemas](https://swagger.io/specification/#schemaObject) implicitly\naccept `additionalProperties`, meaning that any fields provided, but not explicitly\ndefined via properties on the schema are accepted as input, and propagated. When\nunspecified, the `additionalProperties` field is assumed to be `true`.\n\nAdditional properties are tricky to support in Go with typing, and require\nlots of boilerplate code, so in this library, we assume that `additionalProperties`\ndefaults to `false` and we don't generate this boilerplate. If you would like\nan object to accept `additionalProperties`, specify a schema for `additionalProperties`.\n\nSay we declared `NewPet` above like so:\n```yaml\n    NewPet:\n      required:\n        - name\n      properties:\n        name:\n          type: string\n        tag:\n          type: string\n      additionalProperties:\n        type: string\n```\n\nThe Go code for `NewPet` would now look like this:\n```go\n// NewPet defines model for NewPet.\ntype NewPet struct {\n\tName                 string            `json:\"name\"`\n\tTag                  *string           `json:\"tag,omitempty\"`\n\tAdditionalProperties map[string]string `json:\"-\"`\n}\n```\n\nThe additionalProperties, of type `string` become `map[string]string`, which maps\nfield names to instances of the `additionalProperties` schema.\n```go\n// Getter for additional properties for NewPet. Returns the specified\n// element and whether it was found\nfunc (a NewPet) Get(fieldName string) (value string, found bool) {...}\n\n// Setter for additional properties for NewPet\nfunc (a *NewPet) Set(fieldName string, value string) {...}\n\n// Override default JSON handling for NewPet to handle additionalProperties\nfunc (a *NewPet) UnmarshalJSON(b []byte) error {...}\n\n// Override default JSON handling for NewPet to handle additionalProperties\nfunc (a NewPet) MarshalJSON() ([]byte, error) {...}w\n```\n\nThere are many special cases for `additionalProperties`, such as having to\ndefine types for inner fields which themselves support additionalProperties, and\nall of them are tested via the `internal/test/components` schemas and tests. Please\nlook through those tests for more usage examples.\n\n## Generated Client Boilerplate\n\nOnce your server is up and running, you probably want to make requests to it. If\nyou're going to do those requests from your Go code, we also generate a client\nwhich is conformant with your schema to help in marshaling objects to JSON. It\nuses the same types and similar function signatures to your request handlers.\n\nThe interface for the pet store looks like this:\n\n```go\n// The interface specification for the client above.\ntype ClientInterface interface {\n\n\t// FindPets request\n\tFindPets(ctx context.Context, params *FindPetsParams, reqEditors ...RequestEditorFn) (*http.Response, error)\n\n\t// AddPet request with JSON body\n\tAddPet(ctx context.Context, body NewPet, reqEditors ...RequestEditorFn) (*http.Response, error)\n\n\t// DeletePet request\n\tDeletePet(ctx context.Context, id int64, reqEditors ...RequestEditorFn) (*http.Response, error)\n\n\t// FindPetById request\n\tFindPetById(ctx context.Context, id int64, reqEditors ...RequestEditorFn) (*http.Response, error)\n}\n```\n\nA Client object which implements the above interface is also generated:\n\n```go\n// Client which conforms to the OpenAPI3 specification for this service.\ntype Client struct {\n    // The endpoint of the server conforming to this interface, with scheme,\n    // https://api.deepmap.com for example.\n    Server string\n\n    // HTTP client with any customized settings, such as certificate chains.\n    Client http.Client\n\n    // A callback for modifying requests which are generated before sending over\n    // the network.\n    RequestEditors []func(ctx context.Context, req *http.Request) error\n}\n```\n\nEach operation in your OpenAPI spec will result in a client function which\ntakes the same arguments. It's difficult to handle any arbitrary body that\nSwagger supports, so we've done some special casing for bodies, and you may get\nmore than one function for an operation with a request body.\n\n1) If you have more than one request body type, meaning more than one media\n type, you will have a generic handler of this form:\n\n        AddPet(ctx context.Context, contentType string, body io.Reader)\n\n2) If you have only a JSON request body, you will get:\n\n        AddPet(ctx context.Context, body NewPet)\n\n3) If you have multiple request body types, which include a JSON type you will\n get two functions. We've chosen to give the JSON version a shorter name, as\n we work with JSON and don't want to wear out our keyboards.\n\n        AddPet(ctx context.Context, body NewPet)\n        AddPetWithBody(ctx context.Context, contentType string, body io.Reader)\n\nThe Client object above is fairly flexible, since you can pass in your own\n`http.Client` and a request editing callback. You can use that callback to add\nheaders. In our middleware stack, we annotate the context with additional\ninformation such as the request ID and function tracing information, and we\nuse the callback to propagate that information into the request headers. Still, we\ncan't foresee all possible usages, so those functions call through to helper\nfunctions which create requests. In the case of the pet store, we have:\n\n```go\n// Request generator for FindPets\nfunc NewFindPetsRequest(server string, params *FindPetsParams) (*http.Request, error) {...}\n\n// Request generator for AddPet with JSON body\nfunc NewAddPetRequest(server string, body NewPet) (*http.Request, error) {...}\n\n// Request generator for AddPet with non-JSON body\nfunc NewAddPetRequestWithBody(server string, contentType string, body io.Reader) (*http.Request, error) {...}\n\n// Request generator for DeletePet\nfunc NewDeletePetRequest(server string, id int64) (*http.Request, error) {...}\n\n// Request generator for FindPetById\nfunc NewFindPetByIdRequest(server string, id int64) (*http.Request, error) {...}\n```\n\nYou can call these functions to build an `http.Request` from Go objects, which\nwill correspond to your request schema. They map one-to-one to the functions on\nthe client, except that we always generate the generic non-JSON body handler.\n\nThere are some caveats to using this code.\n- exploded, form style query arguments, which are the default argument format\n in OpenAPI 3.0 are undecidable. Say that I have two objects, one composed of\n the fields `(name=bob, id=5)` and another which has `(name=shoe, color=brown)`.\n The first parameter is named `person` and the second is named `item`. The\n default marshaling style for query args would result in\n `/path/?name=bob,id=5&name=shoe,color=brown`. In order to tell what belongs\n to which object, we'd have to look at all the parameters and try to deduce it,\n but we're lazy, so we didn't. Don't use exploded form style arguments if\n you're passing around objects which have similar field names. If you\n used unexploded form parameters, you'd have\n `/path/?person=name,bob,id,5&item=name,shoe,color,brown`, which an be\n parsed unambiguously.\n\n- Parameters can be defined via `schema` or via `content`. Use the `content` form\n for anything other than trivial objects, they can marshal to arbitrary JSON\n structures. When you send them as cookie (`in: cookie`) arguments, we will\n URL encode them, since JSON delimiters aren't allowed in cookies.\n\n## Using SecurityProviders\n\nIf you generate client-code, you can use some default-provided security providers\nwhich help you to use the various OpenAPI 3 Authentication mechanism.\n\n\n```go\n    import (\n        \"github.com/algorand/oapi-codegen/pkg/securityprovider\"\n    )\n\n    func CreateSampleProviders() error {\n        // Example BasicAuth\n        // See: https://swagger.io/docs/specification/authentication/basic-authentication/\n        basicAuthProvider, basicAuthProviderErr := securityprovider.NewSecurityProviderBasicAuth(\"MY_USER\", \"MY_PASS\")\n        if basicAuthProviderErr != nil {\n            panic(basicAuthProviderErr)\n        }\n\n        // Example BearerToken\n        // See: https://swagger.io/docs/specification/authentication/bearer-authentication/\n        bearerTokenProvider, bearerTokenProviderErr := securityprovider.NewSecurityProviderBearerToken(\"MY_TOKEN\")\n        if bearerTokenProviderErr != nil {\n            panic(bearerTokenProviderErr)\n        }\n\n        // Example ApiKey provider\n        // See: https://swagger.io/docs/specification/authentication/api-keys/\n        apiKeyProvider, apiKeyProviderErr := securityprovider.NewSecurityProviderApiKey(\"query\", \"myApiKeyParam\", \"MY_API_KEY\")\n        if apiKeyProviderErr != nil {\n            panic(apiKeyProviderErr)\n        }\n\n        // Example providing your own provider using an anonymous function wrapping in the\n        // InterceptoFn adapter. The behaviour between the InterceptorFn and the Interceptor interface\n        // are the same as http.HandlerFunc and http.Handler.\n        customProvider := func(req *http.Request, ctx context.Context) error {\n            // Just log the request header, nothing else.\n            log.Println(req.Header)\n            return nil\n        }\n\n        // Exhaustive list of some defaults you can use to initialize a Client.\n        // If you need to override the underlying httpClient, you can use the option\n        //\n        // WithHTTPClient(httpClient *http.Client)\n        //\n        client, clientErr := NewClient(\"https://api.deepmap.com\", WithRequestEditorFn(apiKeyProvider.Intercept))\n\n        return nil\n    }\n```\n\n## Extensions\n\n`oapi-codegen` supports the following extended properties:\n\n- `x-go-type`: specifies Go type name. It allows you to specify the type name for a schema, and\n  will override any default value. This extended property isn't supported in all parts of\n  OpenAPI, so please refer to the spec as to where it's allowed. Swagger validation tools will\n  flag incorrect usage of this property.\n- `x-go-name`: specifies Go field name. It allows you to specify the field name for a schema, and\n  will override any default value. This extended property isn't supported in all parts of\n  OpenAPI, so please refer to the spec as to where it's allowed. Swagger validation tools will\n  flag incorrect usage of this property.\n- `x-oapi-codegen-extra-tags`: adds extra Go field tags to the generated struct field. This is\n  useful for interfacing with tag based ORM or validation libraries. The extra tags that\n  are added are in addition to the regular json tags that are generated. If you specify your \n  own `json` tag, you will override the default one. \n\n    ```yaml\n    components:\n      schemas:\n        Object:\n          properties:\n            name:\n              type: string\n              x-oapi-codegen-extra-tags:\n                tag1: value1\n                tag2: value2\n    ```\n  In the example above, field `name` will be declared as: \n  \n  ```\n  Name string `json:\"name\" tag1:\"value1\" tag2:\"value2\"`\n  ```\n  \n\n\n## Using `oapi-codegen`\n\nThe default options for `oapi-codegen` will generate everything; client, server,\ntype definitions and embedded swagger spec, but you can generate subsets of\nthose via the `-generate` flag. It defaults to `types,client,server,spec`, but\nyou can specify any combination of those.\n\n- `types`: generate all type definitions for all types in the OpenAPI spec. This\n will be everything under `#components`, as well as request parameter, request\n body, and response type objects.\n- `server`: generate the Echo server boilerplate. `server` requires the types in the\n same package to compile.\n- `chi-server`: generate the Chi server boilerplate. This code is dependent on\n that produced by the `types` target.\n- `client`: generate the client boilerplate. It, too, requires the types to be\n present in its package.\n- `spec`: embed the OpenAPI spec into the generated code as a gzipped blob. This\n- `skip-fmt`: skip running `goimports` on the generated code. This is useful for debugging\n the generated file in case the spec contains weird strings.\n- `skip-prune`: skip pruning unused components from the spec prior to generating\n the code.\n- `import-mapping`: specifies a map of references external OpenAPI specs to go\n Go include paths. Please see below.\n\nSo, for example, if you would like to produce only the server code, you could\nrun `oapi-codegen -generate types,server`. You could generate `types` and\n`server` into separate files, but both are required for the server code.\n\n`oapi-codegen` can filter paths base on their tags in the openapi definition.\nUse either `-include-tags` or `-exclude-tags` followed by a comma-separated list\nof tags. For instance, to generate a server that serves all paths except those\ntagged with `auth` or `admin`, use the argument, `-exclude-tags=\"auth,admin\"`.\nTo generate a server that only handles `admin` paths, use the argument\n`-include-tags=\"admin\"`. When neither of these arguments is present, all paths\nare generated.\n\n`oapi-codegen` can filter schemas based on the option `--exclude-schemas`, which is\na comma separated list of schema names. For instance, `--exclude-schemas=Pet,NewPet`\nwill exclude from generation schemas `Pet` and `NewPet`. This allow to have a\nin the same package a manually defined structure or interface and refer to it\nin the openapi spec.\n\nSince `go generate` commands must be a single line, all the options above can make\nthem pretty unwieldy, so you can specify all of the options in a configuration\nfile via the `--config` option. Please see the test under\n[`/internal/test/externalref/`](https://github.com/deepmap/oapi-codegen/blob/master/internal/test/externalref/externalref.cfg.yaml)\nfor an example. The structure of the file is as follows:\n    \n```yaml\noutput:\n  externalref.gen.go\npackage: externalref\ngenerate:\n  - types\n  - skip-prune\nimport-mapping:\n  ./packageA/spec.yaml: github.com/deepmap/oapi-codegen/internal/test/externalref/packageA\n  ./packageB/spec.yaml: github.com/deepmap/oapi-codegen/internal/test/externalref/packageB\n```\n\nHave a look at [`cmd/oapi-codegen/oapi-codegen.go`](https://github.com/deepmap/oapi-codegen/blob/master/cmd/oapi-codegen/oapi-codegen.go#L48) \nto see all the fields on the configuration structure.\n\n### Import Mappings\n\nOpenAPI specifications may contain references to other OpenAPI specifications,\nand we need some additional information in order to be able to generate correct\nGo code.\n\nAn external reference looks like this:\n\n    $ref: ./some_spec.yaml#/components/schemas/Type\n\nWe assume that you have already generated the boilerplate code for `./some_spec.yaml`\nusing `oapi-codegen`, and you have a package which contains the generated code,\nlet's call it `github.com/deepmap/some-package`. You need to tell `oapi-codegen` that\n`some_spec.yaml` corresponds to this package, and you would do it by specifying\nthis command line argument:\n\n    -import-mapping=./some_spec.yaml:github.com/deepmap/some-package\n\nThis tells us that in order to resolve references generated from `some_spec.yaml` we\nneed to import `github.com/deepmap/some-package`. You may specify multiple mappings\nby comma separating them in the form `key1:value1,key2:value2`.\n\n## What's missing or incomplete\n\nThis code is still young, and not complete, since we're filling it in as we\nneed it. We've not yet implemented several things:\n\n- `oneOf`, `anyOf` are not supported with strong Go typing. This schema:\n\n        schema:\n          oneOf:\n            - $ref: '#/components/schemas/Cat'\n            - $ref: '#/components/schemas/Dog'\n\n    will result in a Go type of `interface{}`. It will be up to you\n    to validate whether it conforms to `Cat` and/or `Dog`, depending on the\n    keyword. It's not clear if we can do anything much better here given the\n    limits of Go typing.\n\n    `allOf` is supported, by taking the union of all the fields in all the\n    component schemas. This is the most useful of these operations, and is\n    commonly used to merge objects with an identifier, as in the\n    `petstore-expanded` example.\n\n- `patternProperties` isn't yet supported and will exit with an error. Pattern\n properties were defined in JSONSchema, and the `kin-openapi` Swagger object\n knows how to parse them, but they're not part of OpenAPI 3.0, so we've left\n them out, as support is very complicated.\n\n\n## Making changes to code generation\n\nThe code generator uses a tool to inline all the template definitions into\ncode, so that we don't have to deal with the location of the template files.\nWhen you update any of the files under the `templates/` directory, you will\nneed to regenerate the template inlines:\n\n    go generate ./pkg/codegen/templates\n\nAll this command does is inline the files ending in `.tmpl` into the specified\nGo file.\n\nAfterwards you should run `go generate ./...`, and the templates will be updated\n accordingly.\n\nAlternatively, you can provide custom templates to override built-in ones using\nthe `-templates` flag specifying a path to a directory containing templates\nfiles. These files **must** be named identically to built-in template files\n(see `pkg/codegen/templates/*.tmpl` in the source code), and will be interpreted\non-the-fly at run time. Example:\n\n    $ ls -1 my-templates/\n    client.tmpl\n    typedef.tmpl\n    $ oapi-codegen \\\n        -templates my-templates/ \\\n        -generate types,client \\\n        petstore-expanded.yaml\n\nOverriding default type mappings can be accomplished with `-type-mappings`.\nFor example use unsigned integers with `-type-mappings integer:uint32`.\n", "release_dates": ["2022-10-28T20:43:15Z", "2022-10-28T16:29:15Z", "2022-10-28T13:39:47Z", "2022-10-27T21:33:33Z", "2022-03-22T16:58:08Z"]}, {"name": "pairing-plus", "description": "This is a fork of pairing library with additional efficiency functionality improvement.", "language": "Rust", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# pairing\n\n[![Crates.io](https://img.shields.io/crates/v/pairing-plus.svg)](https://crates.io/crates/pairing-plus)\n[![Build Status](https://travis-ci.com/algorand/pairing-plus.svg)](https://travis-ci.com/algorand/pairing-plus)\n\nThis is a Rust crate for using pairing-friendly elliptic curves. Currently, only the [BLS12-381](https://z.cash/blog/new-snark-curve.html) construction is implemented.\n\n## [Documentation](https://docs.rs/pairing-plus/)\n\nBring the `pairing-plus` crate into your project just as you normally would.\n\n## Security Warnings\n\nThis library does not make any guarantees about constant-time operations, memory access patterns, or resistance to side-channel attacks.\n\n## License\n\nLicensed under\n\n * MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n\n Unless you explicitly state otherwise, any contribution intentionally\n submitted for inclusion in the work by you, as defined in the MIT\n license, shall be licensed as above, without any additional terms or\n conditions.\n", "release_dates": ["2019-10-31T18:17:54Z"]}, {"name": "paralithium", "description": null, "language": "C", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Paralithium\n\nThis repository contains the reference implementation of Paralithium, a variant of the [Dilithium](https://www.pq-crystals.org/dilithium/) signature scheme.\nThe motivation behind Paralithium can be found in the [Paralithium specification PDF](spec/paralithium.pdf). \nDilithium is a [finalist](https://csrc.nist.gov/Projects/post-quantum-cryptography/round-3-submissions) in the [NIST PQC](https://csrc.nist.gov/projects/post-quantum-cryptography) standardization project.\n\n## Build instructions\n\nThe implementations contain several test and benchmarking programs and a Makefile to facilitate compilation.\n\n### Prerequisites\n\nSome of the test programs require [OpenSSL](https://openssl.org). If the OpenSSL header files and/or shared libraries do not lie in one of the standard locations on your system, it is necessary to specify their location via compiler and linker flags in the environment variables `CFLAGS`, `NISTFLAGS`, and `LDFLAGS`.\n\nFor example, on macOS you can install OpenSSL via [Homebrew](https://brew.sh) by running\n```sh\nbrew install openssl\n```\nThen, run\n```sh\nexport CFLAGS=\"-I/usr/local/opt/openssl@1.1/include\"\nexport NISTFLAGS=\"-I/usr/local/opt/openssl@1.1/include\"\nexport LDFLAGS=\"-L/usr/local/opt/openssl@1.1/lib\"\n```\nbefore compilation to add the OpenSSL header and library locations to the respective search paths.\n\n### Test programs\n\nTo compile the test programs on Linux or macOS, go to the `ref/` or `avx2/` directory and run\n```sh\nmake\n```\nThis produces the executables\n```sh\ntest/test_dilithium$ALG\ntest/test_vectors$ALG\nPQCgenKAT_sign$ALG\n```\nwhere `$ALG` ranges over the parameter sets 2, 3, 5, 2aes, 3aes, and 5aes.\n\n* `test_dilithium$ALG` tests 10000 times to generate keys, sign a random message of 59 bytes and verify the produced signature. Also, the program will try to verify wrong signatures where a single random byte of a valid signature was randomly distorted. The program will abort with an error message and return -1 if there was an error. Otherwise it will output the key and signature sizes and return 0.\n* `test_vectors$ALG` performs further tests of internal functions and prints deterministically generated test vectors for several intermediate values that occur in the Dilithium algorithms. Namely, a 48 byte seed, the matrix A corresponding to the first 32 bytes of seed, a short secret vector s corresponding to the first 32 bytes of seed and nonce 0, a masking vector y corresponding to the seed and nonce 0, the high bits w1 and the low bits w0 of the vector w = Ay, the power-of-two rounding t1 of w and the corresponding low part t0, and the challenge c for the seed and w1. This program is meant to help to ensure compatibility of independent implementations.\n* `PQCgenKAT_sign$ALG` is the Known Answer Test (KAT) generation program provided by NIST. It computes the official KATs and writes them to the files `PQCsignKAT_$(CRYPTO_ALGNAME).{req,rsp}`.\n\n### Benchmarking programs\n\nFor benchmarking the implementations, we provide speed test programs for x86 CPUs that use the Time Step Counter (TSC) or the actual cycle counter provided by the Performance Measurement Counters (PMC) to measure performance. To compile the programs run\n```sh\nmake speed\n```\nThis produces the executables\n```sh\ntest/test_speed$ALG\n```\nfor all parameter sets `$ALG` as above. The programs report the median and average cycle counts of 10000 executions of various internal functions and the API functions for key generation, signing and verification. By default the Time Step Counter is used. If instead you want to obtain the actual cycle counts from the Performance Measurement Counters export `CFLAGS=\"-DUSE_RDPMC\"` before compilation.\n\nPlease note that the reference implementation in `ref/` is not optimized for any platform, and, since it prioritises clean code, is significantly slower than a trivially optimized but still platform-independent implementation. Hence benchmarking the reference code does not provide representative results.\n\nOur Dilithium implementations are contained in the [SUPERCOP](https://bench.cr.yp.to) benchmarking framework. See [here](http://bench.cr.yp.to/results-sign.html#amd64-kizomba) for current cycle counts on an Intel KabyLake CPU.\n\n## Randomized signing\n\nBy default our code implements Dilithium's deterministic signing mode. To change this to the randomized signing mode, define the `DILITHIUM_RANDOMIZED_PROOF` preprocessor macro at compilation by either uncommenting the line\n```sh\n//#define DILITHIUM_RANDOMIZED_PROOF\n```\nin config.h, or adding `-DDILITHIUM_RANDOMIZED_PROOF` to the compiler flags in the environment variable `CFLAGS`.\n\nSimilarly, the macro `DILITHIUM_RANDOMIZED_SALT` controls whether the salt generated during signing is random or deterministic.\n\n## Shared libraries\n\nAll implementations can be compiled into shared libraries by running\n```sh\nmake shared\n```\nFor example in the directory `ref/` of the reference implementation, this produces the libraries\n```sh\nlibpqcrystals_dilithium$ALG_ref.so\n```\nfor all parameter sets `$ALG`, and the required symmetric crypto libraries\n```\nlibpqcrystals_aes256ctr_ref.so\nlibpqcrystals_fips202_ref.so\n```\nAll global symbols in the libraries lie in the namespaces `pqcrystals_dilithium$ALG_ref`, `libpqcrystals_aes256ctr_ref` and `libpqcrystals_fips202_ref`. Hence it is possible to link a program against all libraries simultaneously and obtain access to all implementations for all parameter sets. The corresponding API header file is `ref/api.h`, which contains prototypes for all API functions and preprocessor defines for the key and signature lengths.\n\n## CMake\n\nAlso available is a portable [cmake](https://cmake.org) based build system that permits building the reference implementation.\n\nBy calling \n```\nmkdir build && cd build && cmake .. && cmake --build . && ctest\n```\n\nthe Dilithium reference implementation gets built and tested.\n", "release_dates": []}, {"name": "pixel", "description": "Algorand's implementation of pixel consensus signature ", "language": "Rust", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": null, "release_dates": []}, {"name": "pixel_param", "description": "A rust implementation for parameter generations of Pixel signature scheme", "language": "Rust", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# pixel_param\n[![Build Status](https://travis-ci.com/algorand/pixel_param.svg?token=cs332z4omsgc9ykLW8pu&branch=master)](https://travis-ci.com/algorand/pixel_param)\n\n\nA rust implementation for parameter generations of Pixel signature scheme.\n\n## Structure\nThe public parameter consists of the following elements\n* `depth`: the depth of the time tree, one byte\n* `ciphersuite`: the ciphersuite id, one byte\n* `g2`: the group generator for PixelG2 group\n* `h`: a PixelG1 element,\n* `hlist`: `D+1` PixelG1 elements `h_0, h_1, ..., h_d`\n\n``` Rust\npub struct PubParam {\n    depth: usize,                       // the depth of the time vector\n    ciphersuite: u8,\n    g2: PixelG2,\n    h: PixelG1,                     // h\n    hlist: [PixelG1; CONST_D + 1],  // h_0, h_1, ..., h_d\n}\n```\n## Dependencies\n* This crate uses `HKDF`, instantiated with `SHA512` to extract and\nextend the seed.\n  * syntax:\n    * `HKDF-Extract(salt , seed) -> secret`\n    * `HKDF-Expand(secret, public_info, length_of_new_secret) -> new_secret`\n* This crate uses BLS' `hash_to_curve` function to hash an extended secret\nto a group element.\n  * syntax: `hash_to_group(input, ciphersuite) -> Gx`\n  * This function has been refactored to the pairing library.\n  \n## The procedure\n* Input: ciphersuite id, tentatively supports `0x00` and `0x01`;\n* Input: a seed from the upper level, needs to be at least `32` bytes long;\n* Output: a public parameter;\n* Error: seed is too short, or ciphersuite is not supported\n* Steps:\n  1. set `g2 = PixelG2::one`; this is the default generator of bls12-381 curve\n  2. extract the randomness from the seed:\n  `m = HKDF-Extract(DOM_SEP_PARAM_GEN , seed)`\n  3. generate `h` as follows\n    * `info = \"H2G_h\"`\n    * `t = HKDF-Expand(m, info, 32)`\n    * `h = hash_to_group(t, ciphersuite)`\n  4. generate `h_0 ... h_{d+1}` as follows:\n    * `info = \"H2G_h\" | I2OSP(i,1)`\n    * `t = HKDF-Expand(m, info, 32)`\n    * `h = hash_to_group(t, ciphersuite)`\n  5. output   \n  `PubParam {CONST_D, ciphersuite, g2, h, hlist}`\n\n\n# Functionalities\n* Get the default public parameter:\n  ``` rust\n  PubParam::default() -> PubParam;\n  ```\n  The default parameter is pre-computed using a seed that is set to\n  the initial vector of SHA512, and a ciphersuite identifier of `0x00`.\n  ``` rust\n    SHA512_IV: [u8; 64] = [\n        0x6a, 0x09, 0xe6, 0x67, 0xf3, 0xbc, 0xc9, 0x08, 0xbb, 0x67, 0xae, 0x85, 0x84, 0xca, 0xa7, 0x3b,\n        0x3c, 0x6e, 0xf3, 0x72, 0xfe, 0x94, 0xf8, 0x2b, 0xa5, 0x4f, 0xf5, 0x3a, 0x5f, 0x1d, 0x36, 0xf1,\n        0x51, 0x0e, 0x52, 0x7f, 0xad, 0xe6, 0x82, 0xd1, 0x9b, 0x05, 0x68, 0x8c, 0x2b, 0x3e, 0x6c, 0x1f,\n        0x1f, 0x83, 0xd9, 0xab, 0xfb, 0x41, 0xbd, 0x6b, 0x5b, 0xe0, 0xcd, 0x19, 0x13, 0x7e, 0x21, 0x79,\n    ];\n  ```\n\n  The seed we will be using for the default public parameter generation\n  is set to the same as the SHA512's initial vector.\n  see: https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.180-4.pdf#page=20\n\n  ```\n       6a09e667f3bcc908\n       bb67ae8584caa73b\n       3c6ef372fe94f82b\n       a54ff53a5f1d36f1\n       510e527fade682d1\n       9b05688c2b3e6c1f\n       1f83d9abfb41bd6b\n       5be0cd19137e2179\n  ```\n  The \u201cbig-endian\u201d convention is used when\n   expressing both 32- and 64-bit words, so that within each word, the most\n   significant bit is stored in the left-most bit position.\"\n  For example, the 32-bit string\n  ```\n                1010 0001 0000 0011 1111 1110 0010 0011\n  ```\n  can be expressed as `a103fe23`, and the 64-bit string\n  ```\n                1010 0001 0000 0011 1111 1110 0010 0011\n                0011 0010 1110 1111 0011 0000 0001 1010\n  ```              \n  can be expressed as `a103fe2332ef301a`.\n\n* Get various elements from the public parameter:\n  ``` rust\n  fn depth(&self) -> usize;\n  fn ciphersuite(&self) -> u8;\n  fn g2(&self) -> PixelG2 ;\n  fn h(&self) -> PixelG1;\n  fn hlist(&self) ->  [PixelG1; d+1];\n  ```\n\n* Serialization:\n  * each a public parameter is a blob: `|ciphersuite id| depth | g2 | h | hlist |`\n\n  ``` rust\n  const PP_LEN_COMPRESSED;        // size in bytes of public parameter, compressed\n  const PP_LEN_UNCOMPRESSED;      // size in bytes of public parameter, uncompressed\n  fn size(&self, compressed: bool) -> usize;    // same as above\n  fn serialize<W: Write>(&self, writer: &mut W, compressed: bool) -> Result<()>;\n  fn deserialize<R: Read>(reader: &mut R) -> Result<(PubParam, bool)>;\n  ```\n  The `reader` and `writer` is assumed\n  to have allocated sufficient memory, or an error will be returned.\n  The deserialize function will also return a flag where the parameter blob\n  was compressed or not.\n", "release_dates": []}, {"name": "pointproofs", "description": null, "language": "Rust", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "\n<!--\nCREDIT: http://patorjk.com/software/taag\n         _____  .__                                          .___\n        /  _  \\ |  |    ____   ________________    ____    __| _/\n       /  /_\\  \\|  |   / ___\\ /  _ \\_  __ \\__  \\  /    \\  / __ |\n      /    |    \\  |__/ /_/  >  <_> )  | \\// __ \\|   |  \\/ /_/ |\n      \\____|__  /____/\\___  / \\____/|__|  (____  /___|  /\\____ |\n            \\/     /_____/                   \\/     \\/      \\/\n\n      __________      .__        __                              _____       \n      \\______   \\____ |__| _____/  |______________  ____   _____/ ____\\______\n       |     ___/  _ \\|  |/    \\   __\\____ \\_  __ \\/  _ \\ /  _ \\   __\\/  ___/\n       |    |  (  <_> )  |   |  \\  | |  |_> >  | \\(  <_> |  <_> )  |  \\___ \\\n       |____|   \\____/|__|___|  /__| |   __/|__|   \\____/ \\____/|__| /____  >\n                              \\/     |__|                                 \\/\n-->\n\n\n# Pointproofs\n[![Build Status](https://travis-ci.com/algorand/pointproofs.svg?branch=master)](https://travis-ci.com/algorand/pointproofs)\n\nThis is Algorand's implementation of __Pointproofs: Aggregating Proofs for Multiple Vector Commitments__.\nThis implementation uses bls12-381 curve.\n\n\n## Documentation\n* [Spec](https://github.com/algorand/pointproofs/blob/master/SPEC.md)\n* [Preprint](https://eprint.iacr.org/2020/419).\n\n\n## Code status\n\n* Version 0.1.\n* This code is __NOT__ production-ready yet. It passed two external audits, but additional auditing and testing is required before deployment\n\n## Use this library directly\n* Install rust and cargo toolchain\n* Build library: `cargo build --release`\n* Run example: `cargo run`\n* Run tests: `cargo test [-- --ignore] [--release]`\n* Benchmark: `cargo bench`\n  * see `benches` folder for more options\n* Documentation: `cargo doc --open`\n\n## C wrapper\n* generate the header: `make`\n* test C wrapper: `make test`\n\n## Dependency\n* `Pairing-plus` library: [stable](https://crates.io/crates/pairing-plus) [dev](https://github.com/algorand/pairing-plus).\n  * A fork of zkcrypto's pairing library; with additional functions such as `hash to groups`\n  and performance improvements such as `sum of product`.\n* `pointproofs-paramgen`: [stable](TBD) [dev](https://github.com/algorand/pointproofs-paramgen)\n  * This crate is used to generate the so called _common reference string_ in an MPC manner.\n  * A sample CRS is provided with the code for testing purpose.\n\n## License\n\nMIT\n\n\n## Citation\n\n``` bibtex\n@misc{Algo20,\n    author    = {Algorand},\n    title     = {Source code for Pointproofs},\n    note      = \"\\url{https://github.com/algorand/pointproofs}\",\n    year      = {2020},\n}\n```\n\n\n\n## Performance\n* dimension = 1024\n* AWS with Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30 GHz\n\n|operation | cost|\n|:---|---:|\n| commit_new | 54.34 ms|\n| proof_new | 54.41 ms |\n| single commit 8 proof aggregate | 1.55 ms |\n| verification (with proof deserialize) |  4.69 ms |\n\n\nSee [here](https://github.com/algorand/pointproofs/blob/master/benchmark.md) for more data.\n", "release_dates": []}, {"name": "pointproofs-paramgen", "description": null, "language": "Rust", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Pointproofs vector commitment parameter generation\n[![Build Status](https://travis-ci.com/algorand/pointproofs-paramgen.svg?branch=master)](https://travis-ci.com/algorand/pointproofs-paramgen)\n\n## Usage\n\nThis crate generates parameters for [Pointproofs](https://github.com/algorand/pointproofs) vector commitment schemes.\n\n```\ninit params.out parameter_n\n```\nGenerate starting parameters (with no entropy) for `parameter_n` and stores them in `params.out`.\n\n\n```\nevolve id_string params.in params.out\n```\nReads old params from `params.in`, rerandomizes them and writes them (with a proof of knowledge of the mixed-in exponent) to `params.out`, using `id_string` as your identity.\n\n\n```\nverify id_string params.old params.new\n```\nGiven assumed-good old params and a newly rerandomized version (with a proof of knowledge of the mixed-in exponent), verify that the new parameters were rerandomized correctly (i.e., check that the parameters are self-consistent and that the proof is correct for prover identity `id_string`).\n\n```\nfinalize beacon_value params.in params.final\n```\nGiven assumed-good params in `params.in` and the value of the shared random beacon, output the final set of parameters to `params.final`.\n\n## Sample param\n\nA sample file `crs.param` is provided for testing purpose. It supports vectors\nof dimensions = 8. This file shall __NOT__ be used in products.\n\n## Security notes\n\n* The \"evolve\" operation is NOT CONSTANT TIME and should not be run in a setting where an attacker can precisely measure runtime.\n\n* [consistencycheck.pdf](./consistencycheck.pdf) contains a description and security proof for the probabilistic consistency check used as part of the `verify` operation.\n\n* [usage.md](./usage.md) describes how to carry out a secure multiparty computation to generate parameters using this tool.\n\n* [security.pdf](./security.pdf) gives a security proof of said multiparty protocol.\n\n* This code is NOT production ready yet. It passed one external audit, but additional auditing and testing is required before deployment.\n", "release_dates": []}, {"name": "py-algorand-sdk", "description": "Algorand Python SDK", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# py-algorand-sdk\n\n[![PyPI version](https://badge.fury.io/py/py-algorand-sdk.svg)](https://badge.fury.io/py/py-algorand-sdk)\n[![Documentation Status](https://readthedocs.org/projects/py-algorand-sdk/badge/?version=latest&style=flat)](https://py-algorand-sdk.readthedocs.io/en/latest)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\nA python library for interacting with the Algorand network.\n\n## Installation\n\nRun `$ pip3 install py-algorand-sdk` to install the package.\n\nAlternatively, choose a [distribution file](https://pypi.org/project/py-algorand-sdk/#files), and run `$ pip3 install [file name]`.\n\n## Supported Python versions\n\npy-algorand-sdk's minimum Python version policy attempts to balance several constraints.\n\n* Make it easy for the community to use py-algorand-sdk by minimizing or excluding the need to customize Python installations.\n* Provide maintainers with access to newer language features that produce more robust software.\n\nGiven these constraints, the minimum Python version policy is:\nTarget Python version on newest [Ubuntu LTS](https://wiki.ubuntu.com/Releases) released >= 6 months ago.\n\nThe rationale is:\n\n* If a major Linux OS distribution bumps a Python version, then it's sufficiently available to the community for us to upgrade.\n* The 6 month time buffer ensures we delay upgrades until the community starts using a recently released LTS version.\n\n## SDK Development\n\nInstall dependencies\n\n* `pip3 install -r requirements.txt`\n\nRun tests\n\n* `make docker-test`\n\nSet up the Algorand Sandbox based test-harness without running the tests\n\n* `make harness`\n\nFormat code\n\n* `black .`\n\nUpdate `algosdk/__init__.pyi` which allows downstream developers importing `algosdk` and using VSCode's PyLance to have improved type analysis\n\n* `make generate-init`\n\nLint types\n\n* `make mypy` (or `mypy algosdk`)\n\nCheck all lints required by the C.I. process\n\n* `make lint`\n\nRun non-test-harness related unit tests\n\n* `make pytest-unit`\n\nWe use cucumber testing for all of our SDKs, including this one. Please refer to [algorand-sdk-testing](https://github.com/algorand/algorand-sdk-testing#readme) for guidance and existing tests that you may need to update. Depending on the type of update you wish to contribute, you may also need to have corresponding updates in the other SDKs (Go, JS, and Java). Feel welcome to ask for collaboration on that front. \n\n## Quick start\n\nHere's a simple example you can run without a node.\n\n```python\nfrom algosdk import account, encoding\n\n# generate an account\nprivate_key, address = account.generate_account()\nprint(\"Private key:\", private_key)\nprint(\"Address:\", address)\n\n# check if the address is valid\nif encoding.is_valid_address(address):\n    print(\"The address is valid!\")\nelse:\n    print(\"The address is invalid.\")\n```\n\n## Node setup\n\nFollow the instructions in Algorand's [developer resources](https://developer.algorand.org/docs/run-a-node/setup/install/) to install a node on your computer.\nYou can also set up a local [Algorand Sandbox](https://github.com/algorand/sandbox) with `make harness`.\n\n## Running examples/example.py\n\nBefore running [example.py](https://github.com/algorand/py-algorand-sdk/blob/master/examples/example.py), start kmd on a private network or testnet node:\n\n```bash\n./goal kmd start -d [data directory]\n```\n\nNext, create a wallet and an account:\n\n```bash\n./goal wallet new [wallet name] -d [data directory]\n```\n\n```bash\n./goal account new -d [data directory] -w [wallet name]\n```\n\nVisit the [Algorand dispenser](https://bank.testnet.algorand.network/) and enter the account address to fund your account.\n\nNext, in [tokens.py](https://github.com/algorand/py-algorand-sdk/blob/master/examples/tokens.py), either update the tokens and addresses, or provide a path to the data directory. Alternatively, `tokens.py` also defaults to the sandbox harness configurations for algod and kmd, which can be brought up by running `make harness`.\n\nYou're now ready to run example.py!\n\n## Documentation\n\nDocumentation for the Python SDK is available at [py-algorand-sdk.readthedocs.io](https://py-algorand-sdk.readthedocs.io/en/latest/).\n\n## License\n\npy-algorand-sdk is licensed under an MIT license. See the [LICENSE](https://github.com/algorand/py-algorand-sdk/blob/master/LICENSE) file for details.\n", "release_dates": ["2023-09-20T17:26:39Z", "2023-08-17T13:31:17Z", "2023-06-14T18:49:19Z", "2023-05-08T14:09:14Z", "2023-03-23T20:07:53Z", "2023-03-20T16:58:03Z", "2023-03-14T19:34:32Z", "2023-01-03T23:39:39Z", "2022-12-05T19:07:42Z", "2022-11-10T18:28:45Z", "2022-11-02T19:58:33Z", "2022-10-12T17:25:16Z", "2022-09-19T15:39:15Z", "2022-09-02T14:53:25Z", "2022-08-30T18:35:51Z", "2022-08-18T20:00:17Z", "2022-07-25T11:17:45Z", "2022-06-16T13:39:01Z", "2022-06-02T20:43:58Z", "2022-05-04T19:19:24Z", "2022-05-02T15:06:08Z", "2022-04-21T17:13:17Z", "2022-03-23T16:38:28Z", "2022-03-18T13:42:14Z", "2022-03-02T21:53:17Z", "2022-02-15T21:55:08Z", "2022-01-14T17:32:10Z", "2022-01-03T23:06:47Z", "2021-11-26T18:26:35Z", "2021-10-04T23:21:08Z"]}, {"name": "py-exebot", "description": "Bot for executing code off-chain.", "language": null, "license": null, "readme": "py-exebot\n====================\n\nLicense\n[![License: AGPL v3](https://img.shields.io/badge/License-AGPL%20v3-blue.svg)](COPYING)\n\nPlease see the [COPYING_FAQ](COPYING_FAQ) for details about how to apply our license.\n\nCopyright (C) 2019-2020, Algorand Inc.\n\n[developer site url]: https://developer.algorand.org/\n", "release_dates": []}, {"name": "pyteal", "description": "Algorand Smart Contracts in Python", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": " <!-- markdownlint-disable-file MD041 -->\n\n![PyTeal logo](https://github.com/algorand/pyteal/blob/master/docs/pyteal.png?raw=true)\n\n# PyTeal: Algorand Smart Contracts in Python\n\n[![Build Status](https://github.com/algorand/pyteal/actions/workflows/build.yml/badge.svg)](https://github.com/algorand/pyteal/actions)\n[![PyPI version](https://badge.fury.io/py/pyteal.svg)](https://badge.fury.io/py/pyteal)\n[![Documentation Status](https://readthedocs.org/projects/pyteal/badge/?version=latest)](https://pyteal.readthedocs.io/en/latest/?badge=latest)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\nPyTeal is a Python language binding for [Algorand Smart Contracts (ASC1s)](https://developer.algorand.org/docs/features/asc1/).\n\nAlgorand Smart Contracts are implemented using a new language that is stack-based,\ncalled [Transaction Execution Approval Language (TEAL)](https://developer.algorand.org/docs/features/asc1/teal/).\n\nHowever, TEAL is essentially an assembly language. With PyTeal, developers can express smart contract logic purely using Python.\nPyTeal provides high level, functional programming style abstractions over TEAL and does type checking at construction time.\n\n## Install\n\nPyTeal requires Python version >= 3.10.\n\nIf your operating system (OS) Python version < 3.10, we recommend:\n* Rather than override the OS Python version, install Python  >= 3.10 alongside the OS Python version.\n* Use [pyenv](https://github.com/pyenv/pyenv#installation) or similar tooling to manage multiple Python versions.\n\n### Recommended: Install from PyPi\n\nInstall the latest official release from PyPi:\n\n* `pip install pyteal`\n\n### Install Latest Commit\n\nIf needed, it's possible to install directly from the latest commit on master to use unreleased features:\n\n> **WARNING:** Unreleased code is experimental and may not be backwards compatible or function properly. Use extreme caution when installing PyTeal this way.\n\n* `pip install git+https://github.com/algorand/pyteal`\n\n## Documentation\n\n* [PyTeal Docs](https://pyteal.readthedocs.io/)\n* `docs/` ([README](docs/README.md)) contains raw docs.\n\n## Development Setup\n\nSetup venv (one time):\n\n* `python3 -m venv venv`\n\nActive venv:\n\n* `. venv/bin/activate` (if your shell is bash/zsh)\n* `. venv/bin/activate.fish` (if your shell is fish)\n\nPip install PyTeal in editable state with dependencies:\n\n* `make setup-development`\n* OR if you don't have `make` installed:\n  * `pip install -e . && pip install -r requirements.txt`\n\nFormat code:\n\n* `black .`\n\nLint using flake8:\n\n* `flake8 docs examples pyteal scripts tests *.py`\n\nType checking using mypy:\n\n* `mypy pyteal scripts`\n\nRun unit tests:\n\n* `pytest pyteal tests/unit`\n\nRun integration tests (assumes a developer-mode `algod` is available on port 4001):\n\n* `pytest tests/integration`\n\nStand up developer-mode algod on ports 4001, 4002 and `tealdbg` on port 9392 (assumes [Docker](https://www.docker.com/) is available on your system):\n\n* `docker-compose up -d`\n\nTear down and clean up resources for the developer-mode algod stood up above:\n\n* `docker-compose down`\n", "release_dates": ["2024-01-04T16:39:29Z", "2024-01-02T20:06:39Z", "2023-06-06T20:21:35Z", "2023-04-18T18:41:09Z", "2023-03-20T14:36:06Z", "2023-02-13T19:40:55Z", "2023-01-24T19:17:56Z", "2023-01-19T19:12:36Z", "2022-11-03T16:40:19Z", "2022-11-03T16:01:29Z", "2022-10-24T16:47:28Z", "2022-09-07T19:51:43Z", "2022-09-06T20:19:56Z", "2022-08-22T18:25:18Z", "2022-08-15T17:18:38Z", "2022-08-01T17:38:02Z", "2022-07-25T17:12:36Z", "2022-05-11T14:35:32Z", "2022-05-03T14:25:37Z", "2022-05-03T13:13:24Z", "2022-05-02T21:30:11Z", "2022-05-02T20:19:54Z", "2022-04-15T20:13:27Z", "2022-04-12T20:28:18Z", "2022-03-07T23:37:28Z", "2022-02-24T19:37:52Z", "2021-11-09T16:04:32Z", "2021-09-29T15:29:07Z", "2021-08-31T20:34:41Z", "2021-04-08T20:38:18Z"]}, {"name": "pyteal-utils", "description": null, "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# pyteal-utils\n\n*EXPERIMENTAL WIP*\n\nThere is no guarantee to the API of this repository. It is subject to change without a tagged release.\n\nThis repository is meant to contain PyTEAL utility methods common in many Smart Contract programs.\n\n## Contents\n\n- [pyteal-utils](#pyteal-utils)\n  - [Contents](#contents)\n  - [Utils](#utils)\n    - [Inline Assembly](#inline-assembly)\n    - [Iter](#iter)\n    - [Math](#math)\n    - [Storage](#storage)\n    - [Strings](#strings)\n    - [Transactions](#transactions)\n  - [Contributing](#contributing)\n  - [Prerequisites](#prerequisites)\n    - [Set up your PyTEAL environment](#set-up-your-pyteal-environment)\n\n## Utils\n\n### Inline Assembly\n\n- `InlineAssembly` - Can be used to inject TEAL source directly into a PyTEAL program\n\n### Iter\n\n- `accumulate`\n- `iterate` - Provides a convenience method for calling a method n times\n\n### Math\n\n- `odd` - Returns 1 if `x` is odd\n- `even` - Returns 1 if `x` is even\n- `factorial` - Returns `x! = x * x-1 * x-2 * ...`\n- `wide_factorial` - Returns `x! = x * x-1 * x-2 * ...`\n- `wide_power`\n- `exponential` - Approximates `e ** x` for `n` iterations\n- `log2`\n- `log10` - Returns log base `10` of the integer passed\n- `ln` - Returns natural log of `x` of the integer passed\n- `pow10` - Returns `10 ** x`\n- `max` - Returns the maximum of 2 integers\n- `min` - Returns the minimum of 2 integers\n- `div_ceil` - Returns the result of division rounded up to the next integer\n- `saturation` - Returns an output that is the value of _n_ bounded to the _upper_ and _lower_ saturation values\n\n### Storage\n\n- `GlobalBlob` - Class holding static methods to work with the global storage of an application as a binary large object\n- `LocalBlob` - Class holding static methods to work with the local storage of an application as a binary large object\n- `global_must_get` - Returns the result of a global storage MaybeValue if it exists, else Assert and fail the program\n- `global_get_else` - Returns the result of a global storage MaybeValue if it exists, else return a default value\n- `local_must_get` - Returns the result of a loccal storage MaybeValue if it exists, else Assert and fail the program\n- `local_get_else` - Returns the result of a local storage MaybeValue if it exists, else return a default value\n\n### Strings\n\n- `atoi` - Converts a byte string representing a number to the integer value it represents\n- `itoa` - Converts an integer to the ascii byte string it represents\n- `witoa` - Converts an byte string interpreted as an integer to the ascii byte string it represents\n- `head` - Gets the first byte from a bytestring, returns as bytes\n- `tail` - Returns the string with the first character removed\n- `suffix` - Returns the last n bytes of a given byte string\n- `prefix` - Returns the first n bytes of a given byte string\n- `rest`\n- `encode_uvarint` - Returns the uvarint encoding of an integer\n\n### Transactions\n\n- `assert_common_checks` - Calls all txn checker assert methods\n- `assert_min_fee` - Checks that the fee for a transaction is exactly equal to the current min fee\n- `assert_no_rekey` - Checks that the rekey_to field is empty, Assert if it is set\n- `assert_no_close_to` - Checks that the close_remainder_to field is empty, Assert if it is set\n- `assert_no_asset_close_to` - Checks that the asset_close_to field is empty, Assert if it is set\n\nCommon inner transaction operations\n\n- `pay`\n- `axfer`\n\n## Contributing\n\nAs [PyTEAL](https://github.com/algorand/pyteal) user, your contribution is extremely valuable to grow PyTEAL utilities!\n\nPlease follow the [contribution guide](https://github.com/algorand/pyteal-utils/blob/main/CONTRIBUTING.md)!\n\n## Prerequisites\n\n- [poetry](https://python-poetry.org/)\n- [pre-commit](https://pre-commit.com/)\n- [py-algorand-sdk](https://github.com/algorand/py-algorand-sdk)\n- [pyteal](https://github.com/algorand/pyteal)\n- [pytest](https://docs.pytest.org/)\n- [Docker Compose](https://docs.docker.com/compose/install/)\n- [Algorand Sandbox](https://github.com/algorand/sandbox)\n\n### Set up your PyTEAL environment\n\n1. Set up the [sandbox](https://github.com/algorand/sandbox) and start it (`dev` mode recommended): `./sandbox up dev`\n2. Clone this repo: `git clone https://github.com/algorand/pyteal-utils.git` and `cd` into the `pyteal-utils` directory\n3. Install Python dependecies: `poetry install`\n4. Activate a virual env: `poetry shell`\n5. Configure pre-commit hooks: `pre-commit install`\n", "release_dates": []}, {"name": "reach-auction", "description": "Getting Started With Reach", "language": "JavaScript", "license": null, "readme": "# reach-auction\nGetting Started With Reach\n\n\n## Install Reach\n\nReach is designed to work on POSIX systems with [make](https://en.wikipedia.org/wiki/Make_(software)), [Docker](https://www.docker.com/get-started), and [Docker Compose](https://docs.docker.com/compose/install/) installed. The best way to install Docker on Mac and Windows is with [Docker Desktop](https://www.docker.com/products/docker-desktop).\n\n\nTo confirm everything is installed try to run the following three commands and see no errors\n\n``` bash\n$ make --version\n$ docker --version\n$ docker-compose --version\n```\n\nIf you\u2019re using Windows, consult [the guide to using Reach on Windows](https://docs.reach.sh/guide-windows.html).\n\nOnce you've confirmed that the Reach prerequisites are installed, choose a directory for this project such as:\n\n``` bash\n$ mkdir -p ~/reach && cd ~/reach\n```\n\n## Clone the Reach Auction demo application\n\nClone the repository using the following commands.\n\n```bash\ngit clone https://github.com/algorand/reach-auction.git \n\n```\n\nNavigate to the project folder\n\n``` bash\ncd reach_auction\n```\n\nNext, download Reach by running\n\n``` bash\n$ curl https://docs.reach.sh/reach -o reach ; chmod +x reach\n```\n\nConfirm the download worked by running\n\n``` bash\n$ ./reach version\n```\n\nSince Reach is Dockerized, when first used, the images it uses need to be downloaded. This will happen automatically when used for the first time, but can be done manually now by running\n\n``` bash\n$ ./reach update\n```\n\nYou\u2019ll know that everything is in order if you can run\n\n``` bash\n$ ./reach compile --help\n```\n\nTo determine the current version is installed, run\n\n``` bash\n$ ./reach hashes\n```\n\nOutput should look similar to:\n\n``` bash\nreach: fb449c94\nreach-cli: fb449c94\nreact-runner: fb449c94\nrpc-server: fb449c94\nrunner: fb449c94\ndevnet-algo: fb449c94\ndevnet-cfx: fb449c94\ndevnet-eth: fb449c94\n```\n\nAll of the hashes listed should be the same and then visit the #releases channel on the [Reach Discord Server](https://discord.gg/9kbHPfwbwn) to see the current hashes.\n\nMore information: Detailed Reach install instructions can be found in the [docs](https://developer.algorand.org/docs/get-started/dapps/reach/). ", "release_dates": []}, {"name": "sandbox", "description": "Algorand node sandbox", "language": "Shell", "license": null, "readme": "# Algorand Sandbox\n\nThis is a fast way to create and configure an Algorand development environment with [Algod](https://github.com/algorand/go-algorand) and [Indexer](https://github.com/algorand/indexer).\n\n**Docker Compose** _MUST_ be installed. [Instructions](https://docs.docker.com/compose/install/).\n\nOn a _Windows_ machine, **Docker Desktop** comes with the necessary tools. Please see the [Windows](#windows) section in getting started for more details.\n\n**Warning**: Algorand Sandbox is _not_ meant for production environments and should _not_ be used to store secure Algorand keys. Updates may reset all the data and keys that are stored.\n\n## Usage\n\nUse the **sandbox** command to interact with the Algorand Sandbox.\n\n```plain\nsandbox commands:\n  up    [config]  -> start the sandbox environment.\n  down            -> tear down the sandbox environment.\n  reset           -> reset the containers to their initial state.\n  clean           -> stops and deletes containers and data directory.\n  test            -> runs some tests to demonstrate usage.\n  enter [algod||conduit||indexer||indexer-db]\n                  -> enter the sandbox container.\n  dump [algod||conduit||indexer||indexer-db]\n                  -> dump log information for a container.\n  tail [algod||conduit||indexer||indexer-db]\n                  -> tail log information for a container.\n  version         -> print binary versions.\n  copyTo <file>   -> copy <file> into the algod. Useful for offline transactions, offline LogicSigs & TEAL work.\n  copyFrom <file> -> copy <file> from the algod. Useful for offline transactions, offline LogicSigs & TEAL work.\n\nalgorand commands:\n  logs            -> stream algorand logs with the carpenter utility.\n  status          -> get node status.\n  goal (args)     -> run goal command like 'goal node status'.\n  tealdbg (args)  -> run tealdbg command to debug program execution.\n\nspecial flags for 'up' command:\n  -v|--verbose           -> display verbose output when starting standbox.\n  -s|--skip-fast-catchup -> skip catchup when connecting to real network.\n  -i|--interactive       -> start docker compose in interactive mode.\n```\n\nSandbox creates the following API endpoints:\n\n- `algod`:\n  - address: `http://localhost:4001`\n  - token: `aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa`\n- `kmd`:\n  - address: `http://localhost:4002`\n  - token: `aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa`\n- `indexer`:\n  - address: `http://localhost:8980`\n- `algod (follower)`:\n  - address: `http://localhost:3999`\n  - token: `aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa`\n- `conduit`:\n  - address: `http://localhost:3998/metrics`\n\n## Getting Started\n\n### Ubuntu and macOS\n\nMake sure the docker daemon is running and docker-compose is installed.\n\nOpen a terminal and run:\n\n```bash\ngit clone https://github.com/algorand/sandbox.git\n```\n\nIn whatever local directory the sandbox should reside. Then:\n\n```bash\ncd sandbox\n./sandbox up\n```\n\nThis will run the `sandbox` shell script with the default configuration. See the [Basic Configuration](#basic-configuration) for other options.\n\n<!-- markdownlint-disable-file MD034 -->\n\nNote for Ubuntu: You may need to alias `docker` to `sudo docker` or follow the steps in https://docs.docker.com/install/linux/linux-postinstall so that a non-root user can use the command `docker`.\n\nRun the test command for examples of how to interact with the environment:\n\n```bash\n./sandbox test\n```\n\n### Windows\n\nNote: Be sure to use the latest version of Windows 10. Older versions may not work properly.\n\nNote: While installing the following programs, several restarts may be required for windows to recognize the new software correctly.\n\n#### Option 1: Using WSL 2\n\nThe [installation instructions](https://docs.docker.com/desktop/windows/install/) for Docker Desktop contain some of this but are repeated here.\n\n1. In order to work with Docker Desktop on windows, a prerequisite is **WSL2** and [install instructions are available here](https://docs.microsoft.com/en-us/windows/wsl/install).\n2. Install **Docker Desktop** using the [instructions available here](https://docs.docker.com/desktop/windows/install/).\n3. We recommend using the official Windows Terminal, [available in the app store here](https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701).\n4. Install whatever distribution of Linux desired.\n5. Open the Windows Terminal with the distribution installed in the previous step and follow the instruction for Ubuntu and macOS above.\n\n**Tip**: If you are using VSCode, do not forget to use the WSL2 terminal inside VSCode too. You may follow [this Microsoft tutorial](https://learn.microsoft.com/en-us/windows/wsl/tutorials/wsl-vscode).\n\n#### Option 2: Using Git for Windows/ MSYS 2 (not recommended)\n\nThis option is not fully tested and may cause issues.\nIt is recommended to use WSL 2.\n\n1. Install Git for Windows: https://gitforwindows.org/\n2. Install and launch Docker for Windows: https://docs.docker.com/get-docker\n3. Open \"Git Bash\" and follow the instruction for Ubuntu and macOS above, in the \"Git Bash\" terminal.\n\n##### Troubleshooting\n\n- If you see\n\n  ```plain\n  the input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'.\n  ```\n\n  check that you are using the latest versions of: Docker, Git for Windows, and Windows 10.\n\n  If this does not solve the issue, [open an issue](https://github.com/algorand/sandbox/issues) with all the versions with all the software used, as well as all the commands typed.\n\n- If you see\n\n  ```plain\n  Error response from daemon: open \\\\.\\pipe\\docker_engine_linux: The system cannot find the file specified.\n  ```\n\n  check that Docker is running.\n\n## Basic Configuration\n\nSandbox supports two primary modes of operation. By default, a [private network](#private-network) will be created, which is only available from the local environment. There are also configurations available for the [public networks](#public-network) which will attempt to connect to one of the long running Algorand networks and allow interaction with it.\n\nTo specify which configuration to run:\n\n```sh\n./sandbox up $CONFIG\n```\n\nWhere `$CONFIG` is specified as one of the configurations in the sandbox directory.\n\nFor example to run a `dev` mode network, run:\n\n```sh\n./sandbox up dev\n```\n\nTo switch the configuration:\n\n```sh\n./sandbox down\n./sandbox clean\n./sandbox up $NEW_CONFIG\n```\n\n### Private Network\n\nIf no configuration is specified the sandbox will be started with the `release` configuration which is a private network. The other private network configurations are those not suffixed with `net`. Namely these are `beta`, `dev` and `nightly`.\n\nThe private network environment creates and funds a number of accounts in the algod containers local `kmd` ready to use for testing transactions. These accounts can be reviewed using `./sandbox goal account list`.\n\nPrivate networks also include an `Indexer` API service configured to synchronize against the private network. Because it doesn't require catching up to one of the long running networks it also starts very quickly.\n\nThe `dev` configuration starts a private network using the latest release with these algod configuration customizations:\n* `\"DevMode\": true` - In dev mode, every transaction being sent to the node automatically generates a new block, rather than wait for a new round in real time. This is extremely useful for fast e2e testing of an application.\n* `\"RewardsPoolBalance\": 0` - Prevents participation rewards by overriding the initial rewards pool balance.  In a variety of test scenarios, participation rewards obscure testing Algo balances.\n\nIt takes a long time to generate participation keys, so the default configurations use the `NETWORK_NUM_ROUNDS` parameter to limit how many are created. Unless the default value is changed, the network will stall after 24 hours. Some configurations have been changed so that they can be run for over a week. Review the setting to make sure it is suitable for how you would like to use the sandbox.\n\n### Public Network\n\nThe `mainnet`, `testnet`, `betanet`, and `devnet` configurations configure the sandbox to connect to one of those long running networks. Once started it will automatically attempt to catchup to the latest round. Catchup tends to take a while and a progress bar will be displayed to illustrate of the progress.\n\nDue to technical limitations, this configuration does not contain preconfigured accounts that may be immediately transact with, and Indexer is not available. A new wallet and accounts may be created or imported at will using the [goal wallet new](https://developer.algorand.org/docs/clis/goal/wallet/new/) command to create a wallet and the [goal account import](https://developer.algorand.org/docs/clis/goal/account/import/) or [goal account new](https://developer.algorand.org/docs/clis/goal/account/new/) commands. If a `testnet` configuration is used, please visit the [TestNet Dispenser](https://bank.testnet.algorand.network/) to fund the newly created account.\n\n## Advanced configurations\n\nThe sandbox environment is completely configured using the `config.*` files in the root of this repository. For example, the default configuration for **config.nightly** is:\n\n```bash\nexport ALGOD_CHANNEL=\"nightly\"\nexport ALGOD_URL=\"\"\nexport ALGOD_BRANCH=\"\"\nexport ALGOD_SHA=\"\"\nexport NETWORK=\"\"\nexport NETWORK_TEMPLATE=\"images/algod/future_template.json\"\nexport NETWORK_NUM_ROUNDS=300000\nexport NETWORK_BOOTSTRAP_URL=\"\"\nexport NETWORK_GENESIS_FILE=\"\"\nexport NODE_ARCHIVAL=\"\"\nexport INDEXER_URL=\"https://github.com/algorand/indexer\"\nexport INDEXER_BRANCH=\"develop\"\nexport INDEXER_SHA=\"\"\nexport INDEXER_DISABLED=\"\"\nexport INDEXER_ENABLE_ALL_PARAMETERS=\"false\"\nexport CONDUIT_URL=\"https://github.com/algorand/conduit\"\nexport CONDUIT_BRANCH=\"master\"\nexport CONDUIT_SHA=\"\"\nexport CONDUIT_DISABLED=\"\"\n```\n\nIndexer and Conduit are always built from source since these can be done quickly. For most configurations, algod will be installed using our standard release channels, but building from source is also available by setting the git URL, branch and optionally a specific SHA commit hash.\n\nThe **up** command looks for the config extension based on the argument provided. With a custom configuration pointed to a fork, the sandbox will start using the fork:\n\n```bash\nexport ALGOD_CHANNEL=\"\"\nexport ALGOD_URL=\"https://github.com/<user>/go-algorand\"\nexport ALGOD_BRANCH=\"my-test-branch\"\nexport ALGOD_SHA=\"\"\nexport ALGOD_BOOTSTRAP_URL=\"\"\nexport ALGOD_GENESIS_FILE=\"\"\nexport INDEXER_URL=\"https://github.com/<user>/indexer\"\nexport INDEXER_BRANCH=\"develop\"\nexport INDEXER_SHA=\"\"\nexport INDEXER_DISABLED=\"\"\nexport INDEXER_ENABLE_ALL_PARAMETERS=\"false\"\nexport CONDUIT_URL=\"https://github.com/<user>/conduit\"\nexport CONDUIT_BRANCH=\"master\"\nexport CONDUIT_SHA=\"\"\nexport CONDUIT_DISABLED=\"\"\n```\n\n### Indexer Query Parameters\n\nBy default Indexer disables many query parameters which are known to have performance problems without specially configured databases. You can identify these parameters if your Indexer calls return a response like the following:\n```\n{\"message\":\"provided disabled parameter: tx-type\"}\n```\n\nTo override the disabled parameters and enable everything, add the following to your `config.<name>` file:\n```\nexport INDEXER_ENABLE_ALL_PARAMETERS=\"true\"\n```\n\n## Working with files\n\nSome Algorand commands require using a file for the input. For example working with TEAL programs. In some other cases like working with Logical signatures or transactions offline the output from a LogicSig or transaction may be needed.\n\nTo stage a file use the `copyTo` command. The file will be placed in the algod data directory, which is where sandbox executes `goal`. This means the files can be used without specifying their full path.\n\nTo copy a file from sandbox (algod instance) use the `copyFrom` command. The file will be copied to sandbox directory on host filesystem.\n\n### copyTo example\n\nthese commands will stage two TEAL programs then use them in a `goal` command:\n\n```sh\n~$ ./sandbox copyTo approval.teal\n~$ ./sandbox copyTo clear.teal\n~$ ./sandbox goal app create --approval-prog approval.teal --clear-prog clear.teal --creator YOUR_ACCOUNT  --global-byteslices 1 --global-ints 1 --local-byteslices 1 --local-ints 1\n```\n\n### copyFrom example\n\nthese commands will create and copy a signed logic transaction file, created by `goal`, to be sent or communicated off the chain (e.g. by email or as a QR Code) and submitted else where:\n\n```bash\n~$ ./sandbox goal clerk send -f <source-account> -t <destination-account> --fee 1000 -a 1000000 -o \"unsigned.txn\"\n~$ ./sandbox goal clerk sign --infile unsigned.txn --outfile signed.txn\n~$ ./sandbox copyFrom \"signed.txn\"\n```\n\n## Errors\n\nIf something goes wrong, check the `sandbox.log` file and `./sandbox dump [service]` for details.\n\n## Debugging for teal developers\n\nFor detailed information on how to debug smart contracts and use tealdbg CLI,please consult with [Algorand Development Portal :: Debugging smart contracts](https://developer.algorand.org/docs/get-details/dapps/smart-contracts/debugging/).\n\nAlgorand smart contract debugging process uses `tealdbg` command line of algod instance(algod container in sandbox).\n\n**Note**: Always use `tealdbg` with `--listen 0.0.0.0` or `--listen [IP ADDRESS]` flags, if access is needed to tealdbg from outside of algod docker container!\n\n### tealdbg examples\n\nDebugging smart contract with Chrome Developer Tools (CDT):\n`~$ ./sandbox tealdbg debug ${TEAL_PROGRAM} -f cdt -d dryrun.json`\n\nDebugging smart contract with Web Interface (primal web UI)\n`~$ ./sandbox tealdbg debug ${TEAL_PROGRAM} -f web -d dryrun.json`\n\nThe debugging endpoint port (default 9392) is forwarded directly to the host machine and can be used directly by Chrome Dev Tools for debugging Algorand TEAL smart comtracts (Goto url chrome://inspect/ and configure port 9392 before using please).\n\nNote: If a different port is needed than the default, it may be changed by running `tealdbg --port YOUR_PORT` then modifying the docker-compose.yml file and change all occurances of mapped 9392 port with the desired one.\n\n## ADVANCED: Sandbox Interactive Debugging with VSCode's `Remote - Container` Extension\n\nFor those looking to develop or extend **algod** or **indexer** it's highly recommended to test and debug using a realistic environment. Being able to interactively debug code with breakpoints and introspect the stack as the Algorand daemon communicates with a live network is quite useful. Here are steps that you can take if you want to run an interactive debugger with an `indexer` running on the sandbox. Analogous instructions work for `algod` as well.\n\nBefore starting, make sure you have VS-Code and have installed the [Remote - Containers Extension](https://code.visualstudio.com/docs/remote/containers-tutorial).\n\n1. Inside [docker_compose.yml](./docker-compose.yml) add the key/val `privileged: true` under the `indexer:` service\n2. Start the sandbox with `./sandbox up YOUR_CONFIG` and wait for it to be fully up and running\n\n- you may need to run a `./sandbox clean` first\n- you can verify by seeing healthy output from `./sandbox test`\n<!-- markdownlint-disable-file MD029 -->\n\n3. In VS Code...\n4. Go to the [Command Palette](https://code.visualstudio.com/docs/getstarted/tips-and-tricks#_command-palette) (on a Mac it's **SHIFT-COMMAND-P**) and enter `Remote - Containers: Attach to Running Container`\n5. The container of interest, e.g. `/algorand-sandbox-indexer`, should pop up and you should choose it\n6. The first time you attach to a container, you'll get the option of choosing which top-level directory _inside the container_ to attach the file browser to. The default HOME (`/opt/indexer` in the case of indexer) is usually your best choice\n7. Next, VS Code should auto-detect that you're running a `go` based project and suggest various extensions to add into the container enviroment. You should do this\n8. Now navigate to the file you'd like to debug (e.g. `api/handlers.go`) and add a breakpoint as you usually would\n9. You'll need to identify the PID of the indexer process so you can attach to it. Choose **Terminal** &rarr; **New Terminal** from the menu and run `ps | egrep \"daemon|PID\"`. Note the resulting PID\n10. Now start the debugger with `F5`. It should give you the option to `attach to a process` and generate a `launch.json` with `processId: 0` for you\n11. Modify the `launch.json` with the correct `processId`. Below I provide an example of a `launch.json`\n12. Now you're ready to rumble! If you hit your sandbox endpoint with a well formatted request, you should end up reaching and pausing at your break point. For **indexer**, you would request against port 8980. See the `curl` example below\n\n### Example `launch.json`\n\n```json\n{\n  // Use IntelliSense to learn about possible attributes.\n  // Hover to view descriptions of existing attributes.\n  // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"Attach to Process\",\n      \"type\": \"go\",\n      \"request\": \"attach\",\n      \"mode\": \"local\",\n      \"processId\": YOUR_PID_HERE\n    }\n  ]\n}\n```\n\n### Example `curl` command\n\n```bash\n~$ curl \"localhost:8980/v2/accounts\"\n```\n\n## Deep technical details\n\n### About pseudo-TTY issues or why do we use the `-T` flag in `dc exec -T` everywhere?\n\nWindows Msys / Git Bash does not have a pseudo-TTY.\nBecause of that, any time a command requires a pseudo-TTY, it fails with error:\n\n  ```plain\n  the input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'.\n  ```\n\nUnfortunately prefixing all commands with `winpty` is not an option because `winpty` will break when piping or using command substitution (\\`...\\` and `$()`).\nThe adopted solution is to use a pseudo-TTY only when absolutely required, that is when executing an interactive command such as `bash` inside `docker`.\n\nSee comments around the commands `dc` and `dc_pty` in the file `sandbox`.\n", "release_dates": []}, {"name": "smart-contracts", "description": "Example stateful and stateless smart contracts.", "language": "JavaScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# smart-contracts\nExample stateful and stateless smart contracts.\n", "release_dates": []}, {"name": "sortition", "description": null, "language": "C++", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# sortition\n\nThis package provides an implementation of cryptographic sortition used by [go-algorand](http://github.com/algorand/go-algorand), previously located in \n[github.com/algorand/go-algorand/data/committee/sortition](https://github.com/algorand/go-algorand/commits/master/data/committee/sortition).\n\nPlease visit the [go-algorand README](https://github.com/algorand/go-algorand/blob/master/README.md) for more information on building and using this software.\n\n## License\n\nPlease see the [COPYING_FAQ](COPYING_FAQ) for details about how to apply our license.\n\nCopyright (C) 2019-2023, Algorand Inc.\n", "release_dates": []}, {"name": "state-proof-query-service", "description": null, "language": null, "license": null, "readme": "# state-proof-query-service", "release_dates": []}, {"name": "stateful-teal-auction-demo", "description": "A demo of an auction on stateful TEAL.", "language": "Shell", "license": null, "readme": "These scripts assume that\n- `goal` is in your $PATH\n- `algorand-indexer` is in your $PATH\n- $ALGORAND_DATA is set correctly\n- $INDEXER_URL is set correctly (\"localhost:8980\" is the Indexer's default)\n- `~/demo-node` contains an Algorand node with a consensus version that\n  supports stateful TEAL\n\n# Non-interactive demo\n\nRun\n\n```\n$ ./demo.sh\n```\n\nThis will create a 16-tranche auction with a lookback of 4.\n(These parameters and others are specified in default-parameters.json.)\nThis will launch the auction and have four bidders continually bid\nuntil the auction terminates.\n\n# Application Design\n\n## Security model\n\nThe main accounts in the auction consist of the seller, which is\nselling its tokens in the auction, the auction administrator, which\npeforms administrative tasks on behalf of the auction, and bidder\naccounts, which can be any other account in the system.\n\nThis auction utilizes an escrow account, which holds funds that\nare committed to the auction (either the bid or sale tokens).\n\nThis application seeks to achieve the following properties:\n1. Once an auction series is started, all tranches in the auction\n   obey the tranche formula committed by the parameters established\n   at initialization.\n2. An auction is open if and only if the seller has committed\n   enough tokens to fill a tranche.\n3. Any bidder commits bid tokens to the auction exactly when the\n   bid is entered.  All inputs to the tranche size calculation\n   formula reflect successful bids.\n4. Once a bid is committed, the bidder is guaranteed to be able\n   to claim units of the sale token according to the pricing\n   formula, once the deadline for the particular tranche has\n   passed.\n5. Once a bid is committed, the seller is guaranteed to be able\n   to reclaim tokens exactly when it closes the tranche.\n6. A tranche will not be closed unless either all bids have been\n   paid out or destroyed.\n7. A bid must be paid out if a bidder has opted into the sale token.\n   A bid will only be destroyed if the bidder has not opted in.\n   (Note that a bidder can temporarily delay an auction by\n   wasting fees by opting in and out repeatedly).\n8. The auction series may only be destroyed and deallocated from\n   the blockchain once all tranches have concluded.\n\n## States\n\nThe auction series, as corresponding to a particular application ID,\ncan be considered to reside in one of several logical states.\n\n1. INVALID - The auction series has not been created yet.\n2. READY - The auction series has been created, and an auction for\n   the next tranche may be opened by the auction administrator.\n3. OPEN - The auction is accepting bids for the current tranche.\n4. CLOSED - The auction is no longer accepting bids for the latest\n   tranche.  Bidders may now cash out their bids for the sale token.\n5. TERMINATED - The auction series has concluded.\n\nInitially, the application creation transaction from the\nadministrator creates an application ID and moves the state of that\ninstance to READY.\n\nIf an auction is READY, the auction administrator can, in a group\ntransaction with the seller, atomically change the state to OPEN\nand fund the escrow, so long as tranches remain to be sold off.\n\nOnce the auction is OPEN, it will remain OPEN for all blocks with\na block header containing a timestamp lower than the auction\ndeadline.  After this deadline has passed (in the sequence of blocks),\nthe auction becomes CLOSED.\n\nWhen an auction is CLOSED, bids may be cashed out for units of the\nsale token.  Once all bids have been cashed out, the auction\nadministrator can send all bid tokens and residual sale tokens\npresent in the escrow account to the seller and atomically set the\nauction state to be READY.\n\nIf an auction is READY but no tranches remain, the auction may\nbe deallocated and moved to TERMINATED by an application deletion\ntransaction.\n\n# Interactive demo\n\n## Environment setup and initialization\n\nTo set everything up,\n\n```\n$ ./setup-env.sh\n$ export AUCTION_DATA=~/last-auction\n```\n\nSet parameters in $AUCTION_DATA/creator/parameters.json as desired.\nWhen you would like to initialize the series of auctions,\n\n```\n$ ./init-auction-series.sh\n```\n\nThis script writes all parameters into stateful TEAL global storage\nand returns the application ID which is needed to run the auction.\nAt this point, the application is ready to support the first auction\ntranche.\n\n## Running an auction\n\nTo start an auction for a particular tranche,\n\n```\n$ ./open-auction.sh\n```\n\nThis changes the state of the auction to OPEN, atomically funding\nthe escrow with the sale token.\n\n## Entering bids\n\nTo get the latest state of the auction, run\n\n```\n$ ./auction-status.sh\n```\n\nIf the returned state is OPEN, you may enter a bid.\nYou may enter a bid on behalf of a particular user with\n\n```\n$ \"${AUCTION_ROOT}/<user>/scripts/enter-bid.sh\" <amount>\n```\n\nTo input a round of bids from various bidders, distributed almost\nuniformly at random from a few values, use\n\n```\n$ ./enter-various-bids.sh\n```\n\n## Closing an auction and paying out bids\n\nTo wait for an auction to close,\n\n```\n$ ./wait-for-auction-close.sh\n```\n\nAfter the state of the auction is CLOSED, you must pay out the bids\nbefore auctioning off a new tranche.  To payout an auction,\n\n```\n$ ./close-auction.sh\n```\n\n## Cleanup\n\nOnce all auctions have completed, clean up auction state with\n\n```\n$ ./shutdown-auction-series.sh\n```\n\n# Visualization\n\nTo generate stats from the indexer, run\n\n```\n$ ./statfile.sh <escrow-addr> <data-file1> <data-file2>\n```\n\nNote that you'll need the address of the escrow account to do this.\n\nYou can plot these stats in a file `<out>.pdf` with\n\n```\n$ python auction-plot.py --bidfile <data-file1> --salesfile <data-file2> --outfile <out>.pdf\n```\n\n# Communication requirements\n\nThere are several assumptions about the environment:\n\n1. Setting up the tokens which will be sold and bid with.\n2. Making the application source code hashes available to bidders.\n3. Having the seller opt into the bid token.\n4. Funding the bidders with the bid token.\n5. The auction administrator needs access to an indexer service.\n\nsetup-env.sh initializes these in the demo.\n\nAfter create-auction-series.sh is executed, an application ID is returned.\nThis application ID must be given to all bidders so that they know where to bid.\n\nopen-auction-txn.sh and open-auction-bcast.sh rely on the seller\nto sign a transaction which funds the auction.\n\ndischarge-bid.sh relies on access to the escrow script.\nAfter the auction series has started, and the application ID is known,\nanyone can derive this script similarly to setup-env.sh.\n", "release_dates": []}, {"name": "symteal", "description": "Symteal is an automated verification framework for Algorand ", "language": "Racket", "license": null, "readme": "[![Build Status](https://travis-ci.com/algorand/symteal.svg?branch=master)](https://travis-ci.com/algorand/symteal)\n# Symteal\n\nSymteal is a symbolic execution engine for Algorand Smart Contract and Transactions\n\n## Installing Dependencies \n\nSymteal is built using Rosette. The easiest way to install Rosette is from Racket's package manager:\n\n* Download and install Racket 7.0 or later from http://racket-lang.org\n\n* Use Racket's `raco` tool to install Rosette and lens\n\n  ```\n  $ raco pkg install rosette\n  $ raco pkg install lens\n  ```\n\n* For better solver backend, install [Boolector 3.1.0](https://boolector.github.io/)", "release_dates": []}, {"name": "tealfuzz", "description": null, "language": "Go", "license": null, "readme": "# tealfuzz\n## Getting started\n- Ensure you have [installed docker](https://docs.docker.com/install/)\n- `git clone https://github.com/algorand/tealfuzz`\n- `cd tealfuzz && cd docker && make build && make fuzz`\n", "release_dates": []}, {"name": "tealsign-demo", "description": null, "language": "Shell", "license": null, "readme": null, "release_dates": []}, {"name": "tealviewer", "description": null, "language": "JavaScript", "license": null, "readme": null, "release_dates": []}, {"name": "vrf", "description": "Some tools for debugging/testing the VRF", "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "`libsodium-vanilla-wrapper/` contains a C implementation of the VRF and a test executable. To build it, just link against vanilla libsodium, e.g. `gcc -lsodium main.c vrf.c`. No libsodium-fork needed!\n\nThis code has not been audited yet! Use at your own risk.\n\nThis is essentially just the VRF code from libsodium-fork but adapted to use the point / scalar arithmetic functions that libsodium now exports rather than internal libsodium functions. Despite my best efforts, it may have a few subtle differences in behavior from libsodium-fork. In particular, at the moment this implementation will explicitly reject proofs where gamma is not on the main subgroup or is of low order, whereas libsodium-fork will not. (I haven't thought too hard about whether it's possible to make a valid proof where gamma is not on the main subgroup or is of low order; if not then this difference doesn't matter because libsodium-fork would eventually reject the proof anyway.) There may be other differences I missed -- definitely this code should be carefully checked before using it in production. For go-algorand it may be wise to do a protocol-upgrade just in case.\n\nOne thing to note: The libsodium docs say that the `crypto_scalarmult_ed25519_noclamp` and `crypto_scalarmult_ed25519_base_noclamp` functions return an error code if passed a 0 scalar. When verifying a proof, we will pass a 0 scalar to these functions if the `c` or `s` in the proof is zero. As of libsodium 1.0.18, it appears that despite returning an error code these functions will give the correct answer (the identity point) so this isn't a problem. However, if this behavior changes in a future version of libsodium, we'll need to handle this case then.\n\nThe other directories are from 2018 and not as interesting.\n\n`python/` contains a (slow, variable-time) Python implementation of the VRF (to generate test vectors and validate the C implementation)\nIn particular, `debug.py` can be given a (hex-encoded) secret key and will output the (hex-encoded) public key, the VRF proof for \"hello\", and the corresponding VRF output hash.\nThe python implementation uses djb's reference python implementation of ed25519, which works with Python 2 only.\n\n`libsodium-fork-wrapper/` contains a `test.go`, a command line tool that wraps our libsodium fork. To build, place your built `libsodium.a` library into `libsodium-wrapper/libs/` and update the hardcoded include path in the `// #cgo CFLAGS: ` line near the top of `test.go`. Alternatively, configure libsodium-fork with `--prefix=/tmp`, install it with `make install`, and then run `test.go` with `LD_LIBRARY_PATH=/tmp/lib`\nTo use:\n\t* `go run test.go keygen` will generate a keypair and create two files `vrf.priv` and `vrf.pub` containing the hex-encoded private and public key, respectively.\n\t* `go run test.go prove \"hello\"` will use the private key in `vrf.priv` to output a (hex-encoded) proof for the string \"hello\"\n\t* `go run test.go verify {hex-encoded proof} \"hello\"` will use the public key in `vrf.pub` to verify the given (hex-encoded) proof for the string \"hello\", and if verification succeeds, output the VRF hash\n", "release_dates": []}, {"name": "walletconnect-automation", "description": "Automation for deploying walletconnect to eks", "language": "Shell", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Wallet Connect Bridge Automation\n\nThis deploys https://github.com/aktionariat/walletconnect-bridge.git to our EKS clusters. While we are sharing this automation for others to benefit, the Algorand team does NOT make warranties regarding the stability / reliability of the referenced bridge implementation. Please research and make decisions around use at your own discretion.\n\nFor 2.0 support, see the v2.0 branch. It uses https://github.com/WalletConnect/walletconnect-monorepo.git\n\n## Scripts\n\n### scripts/build.sh\n\nThis builds a docker image for the walletconnect bridge. It currently versions through a timestamp and will produce two docker images, for example:\n\n```\nwalletconnect/relay-server:latest\nwalletconnect/relay-server:latest-java\nwalletconnect/relay-server:1633462163-java\n```\n\n### scripts/push.sh\n\n```\n$ scripts/push.sh -h\nUsage: scripts/push.sh <-i IMAGE> [-r AWS_REGION] [-h]\n```\n\nThis script pushes images to ECR. It will check the aws account that the shell it runs in has credentials to talk to and creates the ECR repo if it does not already exist. It then pushes the image with the appropriate tag.\n\n```\nscripts/push.sh -i walletconnect/relay-server:1633456298-java\n```\n\n### scripts/deploy.sh\n\n```\n$ scripts/deploy.sh -h\nUsage: scripts/deploy.sh [-l VERSION] [-r AWS_REGION] [-n NAMESPACE] [-c CLASSIFIER] [-h]\n```\n\nThis script will deploy to the kubernetes cluster that shell it runs in has access to. When ingress is enabled, it is very opinionated about running with nginx ingress controller, external-dns and lets-encrypt. If you would like to use the settings shown here for your ingress rule, make sure that your lb supporting the ingress controller can handle the timeout.\n\n### scripts/status.sh\n\n```\nscripts/status.sh -h\nUsage: scripts/status.sh [-n NAMESPACE] [-c CLASSIFIER] [-h]\n```\n\nThis script shows some data about a deployed service.\n\n```\nscripts/status.sh\nVERSION: 1633456298-java\nENDPOINT: wss://wallet-connect.default.dev.example.com/\nNAME                                                     READY   STATUS    RESTARTS   AGE\nwallet-connect-bridge-default-default-59669996d4-vr6bd   1/1     Running   0          15m\n```\n\n\n## Test\n\n### Run DAPP locally\n\nClone the algorand example dapp repository.\n\n```\ngit clone https://github.com/algorand/walletconnect-example-dapp\n```\n\nGo in the page config and edit it to use your endpoint. You can find this using the status script shown before.\n\nYou need to change bridge variable in `src/App.tsx` to do this. https://github.com/algorand/walletconnect-example-dapp/blob/master/src/App.tsx#L179\n\nNext you need to start the app. You will get a notification to allow your shell to use chrome. Please approve this.\n\n```\nnpm install\nnpm run start\n```\n\nAfter following these steps you should see something like the following.\n\n![dapp-example](docs/demo-dapp-example.png)\n\n### Wallet Connect Flow\n\nOnce you have a demo app running and configured to use your wallet connect bridge endpoint, you can try to register it with a wallet.\n\nNavigate to the dapp in your browser, probably running in http://localhost:3000.\n\n![dapp-example](docs/demo-dapp-example.png)\n\nClick \"Connect to WalletConnect\" and you will see a QR code\n\n![copy-demo-QR-code](docs/copy-demo-QR-code.png)\n\nThis QR code is what you need to use to integrate the demo dapp with your wallet. Copy the QR code and then you can navigate to https://test.walletconnect.org/ and test it out.\n\n![demo-wallet-home](docs/demo-wallet-home.png)\n\nPaste the copied QR code where it says \"Paste wc: url\"\n\nCheck the console logs in dev tools if you run into any issues while working with this site.\n\n![demo-wallet-prompt](docs/demo-wallet-prompt.png)\n\nYou will be prompted to either accept or reject integrating with your demo dapp. Click \"Approve\"\n\n![demo-wallet-success](docs/demo-wallet-success.png)\n\nIf you see the following screen, your integration has been successful!\n\n", "release_dates": []}, {"name": "walletconnect-example-dapp", "description": "Example WalletConnect Dapp on Algorand", "language": "TypeScript", "license": {"key": "lgpl-3.0", "name": "GNU Lesser General Public License v3.0", "spdx_id": "LGPL-3.0", "url": "https://api.github.com/licenses/lgpl-3.0", "node_id": "MDc6TGljZW5zZTEy"}, "readme": "# WalletConnect V1 Example Dapp\n\nWalletConnect V1 is being sunset, so it is recommended to look at [WalletConnect](https://github.com/WalletConnect) V2 examples for guidance instead of this repo. \n\n## Develop\n\n```bash\nnpm run start\n```\n\n## Test\n\n```bash\nnpm run test\n```\n\n## Build\n\n```bash\nnpm run build\n```\n", "release_dates": []}, {"name": "walletconnect-monorepo", "description": "WalletConnect Monorepo ", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# WalletConnect v1.x.x\n\nOpen protocol for connecting Wallets to Dapps - https://walletconnect.org\n\n## Packages\n\n| SDK           | Current Version                                                                                      | Description |\n| ------------- | ---------------------------------------------------------------------------------------------------- | ----------- |\n| walletconnect | [![npm version](https://badge.fury.io/js/walletconnect.svg)](https://badge.fury.io/js/walletconnect) | SDK         |\n\n| Clients               | Current Version                                                                                                              | Description       |\n| --------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ----------------- |\n| @walletconnect/core   | [![npm version](https://badge.fury.io/js/%40walletconnect%2Fcore.svg)](https://badge.fury.io/js/%40walletconnect%2Fcore)     | Core Client       |\n| @walletconnect/client | [![npm version](https://badge.fury.io/js/%40walletconnect%2Fclient.svg)](https://badge.fury.io/js/%40walletconnect%2Fclient) | Isomorphic Client |\n\n| Providers                        | Current Version                                                                                                                                    | Description       |\n| -------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------- |\n| @walletconnect/ethereum-provider | [![npm version](https://badge.fury.io/js/%40walletconnect%2Fethereum-provider.svg)](https://badge.fury.io/js/%40walletconnect%2Fethereum-provider) | Ethereum Provider |\n| @walletconnect/truffle-provider  | [![npm version](https://badge.fury.io/js/%40walletconnect%2Ftruffle-provider.svg)](https://badge.fury.io/js/%40walletconnect%2Ftruffle-provider)   | Truffle Provider  |\n| @walletconnect/web3-provider     | [![npm version](https://badge.fury.io/js/%40walletconnect%2Fweb3-provider.svg)](https://badge.fury.io/js/%40walletconnect%2Fweb3-provider)         | Web3 Provider     |\n| @walletconnect/web3-subprovider  | [![npm version](https://badge.fury.io/js/%40walletconnect%2Fweb3-subprovider.svg)](https://badge.fury.io/js/%40walletconnect%2Fweb3-subprovider)   | Web3 Subprovider  |\n\n| Helpers                          | Current Version                                                                                                                                    | Description       |\n| -------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------- |\n| @walletconnect/browser-utils     | [![npm version](https://badge.fury.io/js/%40walletconnect%2Fbrowser-utils.svg)](https://badge.fury.io/js/%40walletconnect%2Fbrowser-utils)         | Browser Utilities |\n| @walletconnect/http-connection   | [![npm version](https://badge.fury.io/js/%40walletconnect%2Fhttp-connection.svg)](https://badge.fury.io/js/%40walletconnect%2Fhttp-connection)     | HTTP Connection   |\n| @walletconnect/iso-crypto        | [![npm version](https://badge.fury.io/js/%40walletconnect%2Fiso-crypto.svg)](https://badge.fury.io/js/%40walletconnect%2Fiso-crypto)               | Isomorphic Crypto |\n| @walletconnect/qrcode-modal      | [![npm version](https://badge.fury.io/js/%40walletconnect%2Fqrcode-modal.svg)](https://badge.fury.io/js/%40walletconnect%2Fqrcode-modal)           | QR Code Modal     |\n| @walletconnect/react-native-dapp | [![npm version](https://badge.fury.io/js/%40walletconnect%2Freact-native-dapp.svg)](https://badge.fury.io/js/%40walletconnect%2Freact-native-dapp) | React-Native Dapp |\n| @walletconnect/signer-connection | [![npm version](https://badge.fury.io/js/%40walletconnect%2Fsigner-connection.svg)](https://badge.fury.io/js/%40walletconnect%2Fsigner-connection) | Signer Connection |\n| @walletconnect/socket-transport  | [![npm version](https://badge.fury.io/js/%40walletconnect%2Fsocket-transport.svg)](https://badge.fury.io/js/%40walletconnect%2Fsocket-transport)   | Socket Transport  |\n| @walletconnect/types             | [![npm version](https://badge.fury.io/js/%40walletconnect%2Ftypes.svg)](https://badge.fury.io/js/%40walletconnect%2Ftypes)                         | Typescript Types  |\n| @walletconnect/utils             | [![npm version](https://badge.fury.io/js/%40walletconnect%2Futils.svg)](https://badge.fury.io/js/%40walletconnect%2Futils)                         | Utility Library   |\n\n`## Quick Start`\n\nFind quick start examples for your platform at https://docs.walletconnect.org/quick-start\n\n## Documentation\n\nRead more about WalletConnect protocol and how to use our Clients at https://docs.walletconnect.org\n\n## Contributors\n\nThis project exists thanks to all the people who contribute.\n<a href=\"https://github.com/WalletConnect/walletconnect-monorepo/graphs/contributors\"><img src=\"https://opencollective.com/walletconnect/contributors.svg?width=890&button=false\" /></a>\n\nAll contributions are welcome! Feel free to create an Issue or make a PR in this repository\n\n## License\n\nApache 2.0\n", "release_dates": ["2022-08-18T19:48:43Z", "2022-08-18T18:09:09Z", "2022-08-18T18:06:42Z"]}, {"name": "wallet_addresses", "description": "Algorand Inc.'s Wallet Addresses: https://github.com/algorand/wallet_addresses", "language": null, "license": null, "readme": "# wallet_addresses\nAlgorand Inc.'s Wallet Addresses: https://github.com/algorand/wallet_addresses\n", "release_dates": []}, {"name": "websocket", "description": "A WebSocket implementation for Go.", "language": "Go", "license": {"key": "bsd-2-clause", "name": "BSD 2-Clause \"Simplified\" License", "spdx_id": "BSD-2-Clause", "url": "https://api.github.com/licenses/bsd-2-clause", "node_id": "MDc6TGljZW5zZTQ="}, "readme": "# Gorilla WebSocket\n\nGorilla WebSocket is a [Go](http://golang.org/) implementation of the\n[WebSocket](http://www.rfc-editor.org/rfc/rfc6455.txt) protocol.\n\n[![Build Status](https://travis-ci.org/gorilla/websocket.svg?branch=master)](https://travis-ci.org/gorilla/websocket)\n[![GoDoc](https://godoc.org/github.com/gorilla/websocket?status.svg)](https://godoc.org/github.com/gorilla/websocket)\n\n### Documentation\n\n* [API Reference](http://godoc.org/github.com/gorilla/websocket)\n* [Chat example](https://github.com/gorilla/websocket/tree/master/examples/chat)\n* [Command example](https://github.com/gorilla/websocket/tree/master/examples/command)\n* [Client and server example](https://github.com/gorilla/websocket/tree/master/examples/echo)\n* [File watch example](https://github.com/gorilla/websocket/tree/master/examples/filewatch)\n\n### Status\n\nThe Gorilla WebSocket package provides a complete and tested implementation of\nthe [WebSocket](http://www.rfc-editor.org/rfc/rfc6455.txt) protocol. The\npackage API is stable.\n\n### Installation\n\n    go get github.com/gorilla/websocket\n\n### Protocol Compliance\n\nThe Gorilla WebSocket package passes the server tests in the [Autobahn Test\nSuite](https://github.com/crossbario/autobahn-testsuite) using the application in the [examples/autobahn\nsubdirectory](https://github.com/gorilla/websocket/tree/master/examples/autobahn).\n\n### Gorilla WebSocket compared with other packages\n\n<table>\n<tr>\n<th></th>\n<th><a href=\"http://godoc.org/github.com/gorilla/websocket\">github.com/gorilla</a></th>\n<th><a href=\"http://godoc.org/golang.org/x/net/websocket\">golang.org/x/net</a></th>\n</tr>\n<tr>\n<tr><td colspan=\"3\"><a href=\"http://tools.ietf.org/html/rfc6455\">RFC 6455</a> Features</td></tr>\n<tr><td>Passes <a href=\"http://autobahn.ws/testsuite/\">Autobahn Test Suite</a></td><td><a href=\"https://github.com/gorilla/websocket/tree/master/examples/autobahn\">Yes</a></td><td>No</td></tr>\n<tr><td>Receive <a href=\"https://tools.ietf.org/html/rfc6455#section-5.4\">fragmented</a> message<td>Yes</td><td><a href=\"https://code.google.com/p/go/issues/detail?id=7632\">No</a>, see note 1</td></tr>\n<tr><td>Send <a href=\"https://tools.ietf.org/html/rfc6455#section-5.5.1\">close</a> message</td><td><a href=\"http://godoc.org/github.com/gorilla/websocket#hdr-Control_Messages\">Yes</a></td><td><a href=\"https://code.google.com/p/go/issues/detail?id=4588\">No</a></td></tr>\n<tr><td>Send <a href=\"https://tools.ietf.org/html/rfc6455#section-5.5.2\">pings</a> and receive <a href=\"https://tools.ietf.org/html/rfc6455#section-5.5.3\">pongs</a></td><td><a href=\"http://godoc.org/github.com/gorilla/websocket#hdr-Control_Messages\">Yes</a></td><td>No</td></tr>\n<tr><td>Get the <a href=\"https://tools.ietf.org/html/rfc6455#section-5.6\">type</a> of a received data message</td><td>Yes</td><td>Yes, see note 2</td></tr>\n<tr><td colspan=\"3\">Other Features</tr></td>\n<tr><td><a href=\"https://tools.ietf.org/html/rfc7692\">Compression Extensions</a></td><td>Experimental</td><td>No</td></tr>\n<tr><td>Read message using io.Reader</td><td><a href=\"http://godoc.org/github.com/gorilla/websocket#Conn.NextReader\">Yes</a></td><td>No, see note 3</td></tr>\n<tr><td>Write message using io.WriteCloser</td><td><a href=\"http://godoc.org/github.com/gorilla/websocket#Conn.NextWriter\">Yes</a></td><td>No, see note 3</td></tr>\n</table>\n\nNotes:\n\n1. Large messages are fragmented in [Chrome's new WebSocket implementation](http://www.ietf.org/mail-archive/web/hybi/current/msg10503.html).\n2. The application can get the type of a received data message by implementing\n   a [Codec marshal](http://godoc.org/golang.org/x/net/websocket#Codec.Marshal)\n   function.\n3. The go.net io.Reader and io.Writer operate across WebSocket frame boundaries.\n  Read returns when the input buffer is full or a frame boundary is\n  encountered. Each call to Write sends a single frame message. The Gorilla\n  io.Reader and io.WriteCloser operate on a single WebSocket message.\n\n", "release_dates": ["2023-04-04T21:33:00Z", "2022-03-22T16:50:41Z", "2021-11-17T15:20:42Z", "2021-10-22T18:55:19Z", "2021-03-25T18:54:46Z"]}, {"name": "xorfilter", "description": "Go library implementing xor filters", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# xorfilter: Go library implementing xor filters\n\nForked from [https://github.com/FastFilter/xorfilter](https://github.com/FastFilter/xorfilter)\n\nBloom filters are used to quickly check whether an element is part of a set.\nXor filters are a faster and more concise alternative to Bloom filters.\nThey are also smaller than cuckoo filters.\n\n\n* Thomas Mueller Graf,  Daniel Lemire, [Xor Filters: Faster and Smaller Than Bloom and Cuckoo Filters](https://arxiv.org/abs/1912.08258), Journal of Experimental Algorithmics 25 (1), 2020. DOI: 10.1145/3376122\n\n\n<img src=\"figures/comparison.png\" width=\"50%\"/>\n\n\nWe are assuming that your set is made of 64-bit integers. If you have strings\nor other data structures, you need to hash them first to a 64-bit integer. It\nis not important to have a good hash function, but collision should be unlikely\n(~1/2^64).\n\nThe current implementation has a false positive rate of about 0.3% and a memory usage\nof less than 9 bits per entry for sizeable sets.\n\nYou construct the filter as follows starting from a slice of 64-bit integers:\n\n```Go\nfilter,_ := xorfilter.Populate(keys) // keys is of type []uint64\n```\nIt returns an object of type `Xor8`. The 64-bit integers would typically be hash values of your objects.\n\nYou can then query it as follows:\n\n\n```Go\nfilter.Contains(v) // v is of type uint64\n```\n\nIt will *always* return true if v was part of the initial construction (`Populate`) and almost always\nreturn false otherwise.\n\nAn xor filter is immutable, it is concurrent. The expectation is that you build it once and use it many times.\n\nThough the filter itself does not use much memory, the construction of the filter needs many bytes of memory per set entry. \n\nFor persistence, you only need to serialize the following data structure:\n\n```Go\ntype Xor8 struct {\n\tSeed         uint64\n\tBlockLength  uint32\n\tFingerprints []uint8\n}\n```\n\n# Duplicate keys\n\nWhen constructing the filter, you should ensure that there is no duplicate keys. If you think\nthat this might happen, then you should check the error condition. \n\n```Go\nfilter,err := xorfilter.Populate(keys) // keys is of type []uint64\nif err != nil {\n\t// you have duplicate keys, de-duplicate them?\n}\n```\n\nEffectively, an error is returned when the filter could not be build after `MaxIterations`\niterations (default to 100).\n\n\n# Implementations of xor filters in other programming languages\n\n* [Erlang](https://github.com/mpope9/exor_filter)\n* Rust: [1](https://github.com/bnclabs/xorfilter), [2](https://github.com/codri/xorfilter-rs), [3](https://github.com/Polochon-street/rustxorfilter)\n* [C++](https://github.com/FastFilter/fastfilter_cpp)\n* [Java](https://github.com/FastFilter/fastfilter_java)\n* [C](https://github.com/FastFilter/xor_singleheader)\n* [C99](https://github.com/skeeto/xf8)\n* [Python](https://github.com/GreyDireWolf/pyxorfilter)\n* [C#](https://github.com/MichaelStromberg-Illumina/SaOptimization/blob/16d40594eeebc6593ddf6ff42bb79eb06a8099a0/NirvanaCommon/Xor8.cs)\n", "release_dates": ["2021-08-31T21:59:32Z"]}, {"name": "zexe", "description": "Rust library for decentralized private computation", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<h1 align=\"center\">ZEXE (Zero knowledge EXEcution)</h1>\n\n<p align=\"center\">\n    <img src=\"https://github.com/scipr-lab/zexe/workflows/CI/badge.svg?branch=master\">\n    <a href=\"https://github.com/scipr-lab/zexe/blob/master/AUTHORS\"><img src=\"https://img.shields.io/badge/authors-SCIPR%20Lab-orange.svg\"></a>\n    <a href=\"https://github.com/scipr-lab/zexe/blob/master/LICENSE-APACHE\"><img src=\"https://img.shields.io/badge/license-APACHE-blue.svg\"></a>\n    <a href=\"https://github.com/scipr-lab/zexe/blob/master/LICENSE-MIT\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg\"></a>\n    <a href=\"https://deps.rs/repo/github/scipr-lab/zexe\"><img src=\"https://deps.rs/repo/github/scipr-lab/zexe/status.svg\"></a>\n</p>\n\n___ZEXE___ (pronounced */zeks\u0113/*) is a Rust library for decentralized private computation.\n\n\nThis library was initially developed as part of the paper *\"[ZEXE: Enabling Decentralized Private Computation][zexe]\"*, and it is released under the MIT License and the Apache v2 License (see [License](#license)).\n\n**WARNING:** This is an academic proof-of-concept prototype, and in particular has not received careful code review. This implementation is NOT ready for production use.\n\n## Overview\n\nThis library implements a ledger-based system that enables users to execute offline computations and subsequently produce publicly-verifiable transactions that attest to the correctness of these offline executions. The transactions contain *zero-knowledge succinct arguments* (zkSNARKs) attesting to the correctness of the offline computations, and provide strong notions of privacy and succinctness.\n\n- **Privacy** - transactions reveal no information about the offline computation.\n- **Succinctness** - transactions can be validated in time that is independent of the offline computation.\n- **Application isolation** - malicious applications cannot affect the execution of honest applications.\n- **Application interaction** -  applications can safely communicate with each other.\n\nInformally, the library provides the ability to create transactions that run arbitrary (Turing-complete) scripts on hidden data stored on the ledger. In more detail, the library implements a cryptographic primitive known as *decentralized private computation* (DPC) schemes, which are described in detail in the [ZEXE paper][zexe].\n\n## Directory structure\n\nThis repository contains several Rust crates that implement the different building blocks of ZEXE. The high-level structure of the repository is as follows.\n\n* [`algebra-core`](algebra-core): Rust crate that provides generic arithmetic for finite fields and elliptic curves\n* [`algebra`](algebra): Rust crate that provides concrete instantiations of some finite fields and elliptic curves\n* [`crypto-primitives`](crypto-primitives): Rust crate that implements some useful cryptographic primitives (and constraints for them)\n* [`dpc`](dpc): Rust crate that implements DPC schemes (the main cryptographic primitive in this repository)\n* [`ff-fft`](ff-fft): Rust crate that provides efficient finite field polynomial arithmetic based on finite field FFTs\n* [`r1cs-core`](r1cs-core): Rust crate that defines core interfaces for a Rank-1 Constraint System (R1CS)\n* [`r1cs-std`](r1cs-std): Rust crate that provides various gadgets used to construct R1CS\n* [`gm17`](gm17): Rust crate that implements the zkSNARK of [Groth and Maller][GM17]\n* [`groth16`](groth16): Rust crate that implements the zkSNARK of [Groth][Groth16]\n\n\nIn addition, there is a  [`bench-utils`](bench-utils) crate which contains infrastructure for benchmarking. This crate includes macros for timing code segments and is used for profiling the building blocks of ZEXE.\n\n[GM17]: https://ia.cr/2017/540\n[Groth16]: https://ia.cr/2016/260\n\n\n## Build guide\n\nThe library compiles on the `stable` toolchain of the Rust compiler. To install the latest version of Rust, first install `rustup` by following the instructions [here](https://rustup.rs/), or via your platform's package manager. Once `rustup` is installed, install the Rust toolchain by invoking:\n```bash\nrustup install stable\n```\n\nAfter that, use `cargo`, the standard Rust build tool, to build the library:\n```bash\ngit clone https://github.com/scipr-lab/zexe.git\ncd zexe/dpc\ncargo build --release\n```\n\nThis library comes with unit tests for each of the provided crates. Run the tests with:\n```bash\ncargo test\n```\n\nThis library comes with benchmarks for the following crates:\n\n- [`algebra`](algebra)\n- [`dpc`](dpc)\n\nThese benchmarks require the nightly Rust toolchain; to install this, run `rustup install nightly`. Then, to run benchmarks, run the following command:\n```bash\ncargo +nightly bench\n```\n\nCompiling with `adcxq`, `adoxq` and `mulxq` instructions can lead to a 30-70% speedup. These are available on most `x86_64` platforms (Broadwell onwards for Intel and Ryzen onwards for AMD). Run the following command:\n```bash\nRUSTFLAGS=\"-C target-feature=+bmi2,+adx\" cargo +nightly test/build/bench --features asm\n```\nTip: If optimising for performance, your mileage may vary with passing `--emit=asm` to `RUSTFLAGS`.\n\nTo bench `algebra-benches` with greater accuracy, especially for functions with execution times on the order of nanoseconds, use the `n_fold` feature to run selected functions 1000x per iteration. To run with multiple features, make sure to double quote the features.\n```bash\ncargo +nightly bench --features \"n_fold bls12_381\"\n```\n\n\n## License\n\nZEXE is licensed under either of the following licenses, at your discretion.\n\n * Apache License Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)\n * MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n\nUnless you explicitly state otherwise, any contribution submitted for inclusion in ZEXE by you shall be dual licensed as above (as defined in the Apache v2 License), without any additional terms or conditions.\n\n[zexe]: https://ia.cr/2018/962\n\n## Reference paper\n\n[_ZEXE: Enabling Decentralized Private Computation_][zexe]    \n[Sean Bowe](https://www.github.com/ebfull), Alessandro Chiesa, Matthew Green, Ian Miers, [Pratyush Mishra](https://www.github.com/pratyush), [Howard Wu](https://www.github.com/howardwu)    \n*IEEE S&P 2020* (*IACR ePrint Report 2018/962*)\n\n## Acknowledgements\n\nThis work was supported by:\na Google Faculty Award;\nthe National Science Foundation;\nthe UC Berkeley Center for Long-Term Cybersecurity;\nand donations from the Ethereum Foundation, the Interchain Foundation, and Qtum.\n\nSome parts of the finite field arithmetic, elliptic curve arithmetic, FFTs, and multi-threading infrastructure in the `algebra` crate have been adapted from code in the [`ff`](https://github.com/zkcrypto/ff), [`pairing`](https://github.com/zkcrypto/pairing), and [`bellman`](https://github.com/zkcrypto/bellman) crates, developed by [Sean Bowe](https://www.github.com/ebfull) and others from Zcash.\n", "release_dates": []}, {"name": "ziggurat-algorand", "description": "Algorand x Ziggurat", "language": null, "license": null, "readme": "# Ziggurat x Algorand\n\nThe Ziggurat implementation for Algorand's `algod` nodes.\n\n## Prerequisites\n\nZiggurat is written in stable Rust; you can install the Rust toolchain by following the official instructions [here](https://www.rust-lang.org/learn/get-started).\n\n## Getting started\n\n### Preconditions\n\n1. Clone this repository.\n2. Build [go-algorand](https://github.com/algorand/go-algorand) from source.\n3. Run the setup script:\n```zsh\n tools/setup_env.sh\n```\nIn case algorand files are installed in a specific location, export that location to the `ALGORAND_BIN_PATH`\nenvironment variable and rerun the setup script:\n```zsh\n export ALGORAND_BIN_PATH=\"$HOME/node\"   # example path\n tools/setup_env.sh\n```\n\n### Run tests\nRun conformance and resistance tests with the following command:\n\n```zsh\n cargo +stable test\n```\n\n### Run performance tests\nCreate a package of IP addresses (4k addresses) which are required for performance tests.\n\n_NOTE: To run the `ips.py` script below, the user must be in sudoers file in order to use this script.\nScript uses `ip`/`ipconfig` commands which require the sudo privilages._\n\nFrom the root repository directory, depending on your OS, run one of the following commands.\n\n#### Preconditions under Linux\nGenerate dummy devices with addresses:\n```zsh\n python3 ./tools/ips.py --subnet 1.1.0.0/20 --file src/tools/ips.rs --dev_prefix test_zeth\n```\n\n#### Preconditions under MacOS\nAdd the whole subnet to the loopback device - can be also used on Linux (device name - Linux: `lo`, MacOS: `lo0`):\n```zsh\n python3 ./tools/ips.py --subnet 1.1.0.0/20 --file src/tools/ips.rs --dev lo0\n```\nIncrease the limit for the number of file descriptors:\n```zsh\n ulimit -n 65536\n```\n\n#### Run tests\nRun performance tests with the following command:\n```zsh\n cargo +stable test performance --features performance\n```\n\n## Test Status\n\nShort overview of test cases and their current status. In case of failure, the behaviour observed is usually documented in the test case.\n\nThese results were obtained by running the test suite against [Algorand v3.12.2-stable](https://github.com/algorand/go-algorand/releases/tag/v3.12.2-stable) (181490e3).\n\n| Legend |               |\n| :----: | ------------- |\n|   \u2713    | pass          |\n|   \u2716    | fail          |\n|   -    | unimplemented |\n\n### Conformance\n\n|             Test Case             | Algod  | Additional Information                                                      |\n| :-------------------------------: | :----: | :-------------------------------------------------------------------------- |\n| [001](SPEC.md#ZG-CONFORMANCE-001) |   \u2713    |                                                                             |\n| [002](SPEC.md#ZG-CONFORMANCE-002) |   \u2713    |                                                                             |\n| [003](SPEC.md#ZG-CONFORMANCE-003) |   \u2713    |                                                                             |\n| [004](SPEC.md#ZG-CONFORMANCE-004) |   \u2713    |                                                                             |\n| [005](SPEC.md#ZG-CONFORMANCE-005) |   \u2713    |                                                                             |\n| [006](SPEC.md#ZG-CONFORMANCE-006) |   \u2713    |                                                                             |\n| [007](SPEC.md#ZG-CONFORMANCE-007) |   \u2713    |                                                                             |\n| [008](SPEC.md#ZG-CONFORMANCE-008) |   \u2713    |                                                                             |\n| [009](SPEC.md#ZG-CONFORMANCE-009) |   \u2716    | The PingReply handler doesn't exist anymore in the go-algorand codebase     |\n| [010](SPEC.md#ZG-CONFORMANCE-010) |  \u2713/\u2716   | Only BlockAndCert request is supported, other type requests are unsupported |\n| [011](SPEC.md#ZG-CONFORMANCE-011) |   \u2713    |                                                                             |\n| [012](SPEC.md#ZG-CONFORMANCE-012) |   \u2713    |                                                                             |\n| [013](SPEC.md#ZG-CONFORMANCE-013) |   \u2713    |                                                                             |\n\n### Performance\n\n|             Test Case             | Algod  | Additional Information                                                      |\n|:---------------------------------:| :----: | :-------------------------------------------------------------------------- |\n| [001](SPEC.md#ZG-PERFORMANCE-001) |   \u2713    |                                                                             |\n| [002](SPEC.md#ZG-PERFORMANCE-002) |   \u2713    |                                                                             |\n\n### Resistance\n\n|             Test Case             | Algod  | Additional Information                                                                     |\n| :-------------------------------: | :----: | :----------------------------------------------------------------------------------------- |\n| [001](SPEC.md#ZG-RESISTANCE-001)  |   \u2716    | The node doesn't reject the connection in case a small amount of random data is sent       |\n| [002](SPEC.md#ZG-RESISTANCE-002)  |  \u2713/\u2716   | The procedure accepts sometimes invalid requests (should be improved)                      |\n| [003](SPEC.md#ZG-RESISTANCE-003)  |   \u2716    | The node doesn't reject the connection in most scenarios                                   |\n| [004](SPEC.md#ZG-RESISTANCE-004)  |  \u2713/\u2716   | The node won't reject the connection for enormously long and invalid messages              |\n", "release_dates": []}]