# -*- coding: utf-8 -*-
"""crawler all 31 orgs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ezkHIlRYH5QojTw_dJB9GJoU5XEaIIcf
"""

from google.colab import drive
drive.mount('/content/drive')

!ls -l /content/drive/MyDrive/OSS

from google.colab import drive
drive.mount('/content/drive')

# users are orgs names
users = [
   'xxx ',
   'xxx'
    ]

from logging import error
import requests
import base64
import time
import json

# class Repo:
#   def __init__(self, name: str, desc: str, readme: str):
#     self.name = name
#     self.desc = desc
#     self.readme = readme

readme_variants = ["README.md", "README", "README.txt", "readme.md", "readme"]

TOKEN = 'xxxxxxxxxxxxxxxxxxxxx'

headers = {
    'Authorization': f'token {TOKEN}'
}

BASE_PATH = '/content/drive/MyDrive/unprocessed orgs'

def get_readme(repo):
    for readme_filename in readme_variants:
        readme_url = repo['url'] + f"/contents/{readme_filename}"
        response = requests.get(readme_url, headers=headers)
        # time.sleep(0.1)
        if response.status_code == 200:
            readme_data = response.json()
            if type(readme_data) is not str:
              readme_data = readme_data['content']
            return base64.b64decode(readme_data).decode()
    return None

fields = ['name', 'description', 'language', 'license']

def get_repositories(username, start_url=None):
    subfoldername = username.replace('/', '_')
    dir_path = f'{BASE_PATH}/{subfoldername}'
    if not os.path.exists(dir_path):
      os.mkdir(dir_path)
    log_path = f'{dir_path}/log'

    repos_path = f'{dir_path}/repos'

    i = 0
    url = f"https://api.github.com/users/{username}/repos" if start_url is None else start_url
    if os.path.exists(repos_path):
      with open(repos_path, 'r') as f:
        saved_repos = json.load(f)
    else:
      saved_repos = []
    while url:
      with open(log_path, 'a') as log:
        log.write(f'START\t{url}')
      response = requests.get(url, headers=headers)

      if response.status_code != 200:
          raise Exception(f"Failed to fetch repos: {response.content}")
      data = response.json()
      for repo in data:
        try:
          saved_repo = {}
          for field in fields:
            saved_repo[field] = repo[field]
          saved_repo['readme'] = get_readme(repo)
          saved_repo['release_dates'] = [(release['published_at'] if type(release) is not str else release) for release in requests.get('https://api.github.com/repos/{}/{}/releases'.format(username, repo['name']), headers=headers).json()]
          saved_repos.append(saved_repo)
          print('{}: {:<4}{}; \t'.format(username, i, repo['name']), end='')
        except Exception as e:
          repo_name = saved_repo["name"] if 'name' in saved_repo else 'UNDEFINED'
          print(f'\nException: repo{i}: {repo_name}. Reason: {e}\n')
        i += 1
      # check point
      with open(repos_path, 'w') as f:
        json.dump(saved_repos, f)
      with open(log_path, 'a') as log:
        log.write(f'FINISH\t{url}\n')
      url = response.links.get('next', {}).get('url', None)
    print()
    return saved_repos

import os
# file_dir = os.listdir(BASE_PATH)
# completed = list(file_dir)

for user in users:
  # if user in completed:
  #   print(f'skip {user}')
  #   continue
  print(f'start {user}')
  # if os.path.exists(f'{BASE_PATH}/{user}/log'):
  #   print('WARNING: not supposed to start over')
  repos = get_repositories(user)
  # with open(f'{PATH}/{user}.json', 'w') as f:
  #   json.dump(repos, f)