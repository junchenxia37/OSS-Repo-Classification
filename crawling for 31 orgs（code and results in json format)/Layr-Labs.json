[{"name": "alpha-playground-avs-using-nethermind-spec", "description": "playground-avs repo that adheres to nethermind's cli spec", "language": "Python", "license": null, "readme": "# Mock Tap\n\nThis is a mock package for an EigenLayer node software following the specification described [here](https://eigen.nethermind.io/docs/packaging/). It is used to test and demonstrate the node software package structure. The current services in the node software include a consensus client setup for Ethereum. Although this is not a genuine EigenLayer node software, it helps us to test the package structure. In the future, we will update the services to a real EigenLayer node software.\n\n## Version v2.0\n\n Starts a Python FastApi server with an `option/{option_target}` endpoint. Requests should use `option_target` for the environment variable KEY you want to test on. Suppose an environment variable with key `KEY` was provided as an option to an instance of the mock-avs node. In that case, the `option/KEY` endpoint should return the value of the `KEY` environment variable. If a key was not provided, a 404 response along with an error message will be returned.", "release_dates": []}, {"name": "avs-sync", "description": null, "language": "Go", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# AvsSync\n\n### Motivation\n\nCurrently, when an AVS is being serviced by some operators, the operators for it are delegated some stake.\n\nThe architecture going over how the on chain contracts are set up for an AVS is outlined [here in the AVS middleware contracts repo](https://github.com/Layr-Labs/eigenlayer-middleware#registries). An important one to note for AvsSync is the StakeRegistry \"which keeps track of the stakes of different operators for different quorums at different times\". AvsSync calls the RegistryCoordinator entrypoint, but this call is proxied to the StakeRegistry contract.\n\nThat is, every time an operator registers to serve an AVS or deregisters to no longer serve the AVS, the stake they\u2019ve been delegated to enters or exits (respectively) the relevant quorums in the AVS.\n\nAvsSync is a cron job executable that AVS teams and/or operators can run so that AVS operators keep up-to-date with the latest stake state for the quorums of the AVS they\u2019re in.\n\n### Problem\n\nUpon registration/deregistration and the updating of some quorums with new stake amounts, the current implementation of the StakeRegistry contract only updates the stake amount for the operator performing the action against the AVS. The rest of the operators in the AVS who are in the updated quorums do not have the latest view of the state of the quorum, meaning the current model is not push based.\n\n### Solution\n\nAvsSync implements this update in a pull model by having AVS teams run a cron job as part of their software deployment stack that periodically calls `UpdateOperatorsForQuorum` on the `RegistryCoordinator` (which proxies the call to the `StakeRegistry`). AvsSync periodically (once per day by default) queries the current set of registered operators for each quorum and updates their stake amounts (by calling the DelegationManager contract).\n\n### Configuration\n\nAvsSync is configured via flags passed as arguments, or via environment variables for the respective flags. The list of flags is listed in [flags.go](./flags.go)\n\n### Dependencies\n\nAvsSync makes use of [`eigensdk-go`](https://github.com/Layr-Labs/eigensdk-go), and requires an ethereum node running at `--eth-http-url` to be able to make calls to the chain.\n\n### Running AvsSync\n\nJust run\n```\ndocker run github.com/layr-labs/avs-sync --flags\n```\nHere's an example of flags for running against an anvil image (fake addresses, this won't work):\n```\ndocker run github.com/layr-labs/avs-sync \\\n   --ecdsa-private-key ac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80 \\\n   --eth-http-url http://localhost:8545 \\\n   --registry-coordinator-addr 0x5FbDB2315678afecb367f032d93F642f64180aa3 \\\n   --operator-state-retriever-addr 0xe7f1725E7734CE288F8367e1Bb143E90bb3F0512 \\\n   --sync-interval 24h\n```\n\n### Running AvsSync Locally (from source)\n\nAvsSync can be run directly (passing the necessary flags) by:\n```\ngo run . --flags\n```\nor also by building and installing it so that it can be run from any directory:\n```\ngo install .\navs-sync --flags\n```\n\n### Building AvsSync Docker Image (for eigenlayer devs)\n\nYou can build and push the image to our repository (will require ghcr login) by running:\n```\nmake docker-build\n## need to login if not\ndocker login ghcr.io -u USER_NAME\nmake docker-push\n```\n\n### Testing\n\n#### Against saved anvil db state\n\nThe test is run using an eigencert deployment saved [anvil db state file](./tests/eigenlayer-eigencert-eigenda-strategies-deployed-operators-registered-with-eigenlayer-anvil-state.json). It also requires the [ContractsRegistry bindings](./bindings/ContractsRegistry/binding.go), which we copied here from eigencert. \n\nThe test can be run via:\n```\nmake test\n```\n\n#### Against a goerli fork\n\nThe most recent eigenDA m2 deployment is accessible [here](https://docs.google.com/spreadsheets/d/1UgXnn_9U5mQ6jvj_y1oKjxTG5q0IoYdFvM1704inPGg/edit#gid=0). We can test avssync against this deployment by running a goerli fork and running the tests against it.\nFirst create a .env file by copying the example .env.example file, and adjust the variables as needed. You should only need to enter a private key that has goerli eth.\n\nThen run\n```\nmake start-anvil-goerli-fork\n```\nand in a separate terminal\n```\nmake run-avs-sync\n```\n", "release_dates": []}, {"name": "chaind", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# chaind\n\n[![Tag](https://img.shields.io/github/tag/wealdtech/chaind.svg)](https://github.com/wealdtech/chaind/releases/)\n[![License](https://img.shields.io/github/license/wealdtech/chaind.svg)](LICENSE)\n[![GoDoc](https://godoc.org/github.com/wealdtech/chaind?status.svg)](https://godoc.org/github.com/wealdtech/chaind)\n![Lint](https://github.com/wealdtech/chaind/workflows/golangci-lint/badge.svg)\n[![Go Report Card](https://goreportcard.com/badge/github.com/wealdtech/chaind)](https://goreportcard.com/report/github.com/wealdtech/chaind)\n\n`chaind` is a process that reads information from an Ethereum 2 beacon node and stores it in a database for reporting and analysis.\n\n## Table of Contents\n\n- [Install](#install)\n  - [Binaries](#binaries)\n  - [Docker](#docker)\n  - [Source](#source)\n- [Usage](#usage)\n- [Maintainers](#maintainers)\n- [Contribute](#contribute)\n- [License](#license)\n\n## Install\n\n### Binaries\n\nBinaries for the latest version of `chaind` can be obtained from [the releases page](https://github.com/wealdtech/chaind/releases/latest  ).\n\n### Docker\n\nYou can obtain the latest version of `chaind` using docker with:\n\n```\ndocker pull wealdtech/chaind\n```\n\n### Source\n\n`chaind` is a standard Go binary which can be installed with:\n\n```sh\nGO111MODULE=on go get github.com/wealdtech/chaind\n```\n\n## Usage\nData gathers four pieces of information from the beacon node, broken down by the modules that obtain the information:\n\n  - **Proposer duties** The proposer duties module provides information on the validator expected to propose a beacon block at a given slot;\n  - **Beacon committees** The beacon committees module provides information on the validators expected to attest to a beacon block at a given slot;\n  - **Validators** The validators module provides information on the current statue of validators.  It can also obtain information on the validators' balances and effective balances at a given epoch;\n  - **Blocks** The blocks module provides information on blocks proposed for each slot.  This includes:\n    - the block structure\n    - attestations\n    - proposer slashings\n    - attester slashings\n    - deposits\n    - voluntary exits; and\n  - **Ethereum 1 deposits** The Ethereum 1 deposits module provides information on deposits made on the Ethereum 1 network;\n  - **Finalizer** The finalizer module augments the information present in the database from finalized states.  This includes:\n    - the canonical state of blocks.\n\nIn addition, the summarizer module takes the finalized information and generates summary statistics at the validator, block and epoch level.\n\n## Requirements to run `chaind`\n### Database\nAt current the only supported backend is PostgreSQL.  Once you have a  PostgreSQL instance you will need to create a user and database that `chaind` can use, for example run the following commands as the PostgreSQL superuser (`postgres` on most linux installations):\n\n```sh\n# This command creates a user named 'chain' and will prompt for a password.\ncreateuser chain -P\n# This command creates a database named 'chain' owned by the 'chain' user.\ncreatedb -E UTF8 --owner=chain chain\n```\n\n### Beacon node\n`chaind` supports Teku and Lighthouse beacon nodes.  The current state of obtaining data from beacon nodes is as follows:\n\n  - Teku: must be run in [archive mode](https://docs.teku.consensys.net/en/latest/Reference/CLI/CLI-Syntax/#data-storage-mode) to allow `chaind` to obtain historical data\n  - Lighthouse: Make sure to run with `--slots-per-restore-point 64 --reconstruct-historic-states`, else fetching historical information will be **very** slow. For more information on the trade off between Freezer DB size and fetching performance, please refer to [Database Configuration](https://lighthouse-book.sigmaprime.io/advanced_database.html) in the Lighthouse Book.\n\nAt current Prysm is not supported due to its lack of Altair-related information in its gRPC and HTTP APIs.  We expect to be able to support Prysm again soon.\n\n`chaind` supports all execution nodes.  The current state of obtaining data from execution nodes is as follows:\n\n  - geth: must run with `--txlookuplimit 0` to ensure that all deposit transactions can be obtained\n\n### Example\nTo start a Teku node suitable for `chaind` download Teku and run the following command:\n\n```sh\nteku --rest-api-enabled --data-storage-mode=archive\n```\n\nOnce Teku has finished syncing, run:\n\n```sh\nchaind --eth2client-address=http://localhost:5051/\n```\n\n### Managing the database size\nTwo tables take up the majority of the database size.  These are:\n\n- `t_validator_balances` the balance and effective balance of each validator at each epoch\n- `t_validator_epoch_summaries` the expected and actual actions undertaken by each validator each epoch\n\nThese tables, along with their indices, take over 90% of the space required for the database on mainnet and goerli.  The `t_validator_day_summaries` table contains the same information but aggregated per day (a day being 00:00 to 00:00 UTC).  If the information in this table is sufficient for your needs you can prune old data from the larger tables, keeping their size down and hence managing the overall space requirement for `chaind`.\n\nPruning data removes data older than a certain age from the two database tables mentioned above.  Pruning is set for each table individually and so it is possible to prune either or both of the tables, and to have different retentions for each.  For example, the following configuration:\n\n```yaml\nsummarizer:\n  validators:\n    balance-retention: \"P6M\"\n    epoch-retention: \"P1Y\"\n```\n\nThis will store 6 month's worth of balances, and 1 year's worth of epoch summaries.  Retention periods are [ISO 8601 durations](https://en.wikipedia.org/wiki/ISO_8601#Durations).  Note that if it is not desired to retain any balance or epoch summary data then the retention can be set to \"PT0s\".\n\n## Upgrading `chaind`\n`chaind` should upgrade automatically from earlier versions.  Note that the upgrade process can take a long time to complete, especially where data needs to be refetched or recalculated.  `chaind` should be left to complete the upgrade, to avoid the situation where additional fields are not fully populated.  If chaind is ever stopped or crashes while upgrading and this situation does happen, one should rerun `chaind` with the options `--blocks.start-slot=0 --blocks.refetch=true` to force `chaind` to refetch all blocks.\n\n## Querying `chaind`\n`chaind` attempts to lay its data out in a standard fashion for a SQL database, mirroring the data structures that are present in Ethereum 2.  There are some places where the structure or data deviates from the specification, commonly to provide additional information or to make the data easier to query with SQL.  It is recommended that the [notes on the tables](docs/tables.md) are read before attempting to write any complicated queries.\n\n## Configuring `chaind`\nThe minimal requirements for `chaind` are references to the database and beacon node, for example:\n\n```sh\nchaind --chaindb.url=postgres://chain:secret@localhost:5432 --eth2client.address=localhost:5051\n```\n\nHere, `chaindb.url` is the URL of a local PostgreSQL database with password 'secret' and 'eth2client.address' is the address of a supported beacon client node (gRPC for Prysm, HTTP for Teku and Lighthouse).\n\n`chaind` allows additional configuration for itself and its modules.  It takes configuration from the command line, environment variables or a configuration file, but for the purposes of explaining the configuration options the configuration file is used.  This should be in the home directory and called `.chaind.yml`.  Alternatively, the configuration file can be placed in a different directory and referenced by `--base-dir`, for example `--base-dir=/home/user/config/chaind`; in this case the file should be called `chaind.yml` (without the leading period).\n\n```yaml\n# log-level is the base log level of the process.\n# 'info' should be a suitable log level, unless detailed information is\n# required in which case 'debug' or 'trace' can be used.\nlog-level: info\n# log-file specifies that log output should go to a file.  If this is not\n# present log output will be to stderr.\nlog-file: /var/log/chaind.log\nchaindb:\n  # url is the URL of the PostgreSQL database.\n  url: postgres://chain:secret@localhost:5432\n  max-connections: 16\n# eth2client contains configuration for the Ethereum 2 client.\neth2client:\n  # log-level is the log level of the specific module.  If not present the base log\n  # level will be used.\n  log-level: debug\n  # address is the address of the beacon node.\n  address: localhost:5051\n# eth1client contains configuration for the Ethereum 1 client.\neth1client:\n  # address is the address of the Ethereum 1 node.\n  address: localhost:8545\n# blocks contains configuration for obtaining block-related information.\nblocks:\n  # enable states if this module will be operational.\n  enable: true\n  # address is a separate connection for this module.  If not present then\n  # chaind will use the eth2client connection.\n  address: localhost:5051\n  # start-slot is the slot from which to start.  chaind should keep track of this itself,\n  # however if you wish to start from a later slot this can be set.\n  # start-slot: 2000\n  # refetch will refetch block data from a beacon node even if it has already has a block\n  # in its database.\n  # refetch: false\n# validators contains configuration for obtaining validator-related information.\nvalidators:\n  enable: true\n  # balances contains configuration for obtaining validator balances.  This is\n  # a separate configuration flag for two reasons.  First, it can take a long\n  # time to retrieve this information.  Second, the information can be\n  # derived from the data obtained by the other modules.\n  balances:\n    enable: false\n# beacon-committees contains configuration for obtaining beacon committee-related\n# information.\nbeacon-committees:\n  enable: true\n# proposer-duties contains configuration for obtaining proposer duty-related\n# information.\nproposer-duties:\n  enable: true\n# finalizer updates tables with information available for finalized states.\nfinalizer:\n  enable: true\n# eth1deposits contains information about transactions made to the deposit contract\n# on the Ethereum 1 network.\neth1deposits:\n  enable: false\n  # start-block is the block from which to start fetching deposits.  chaind should\n  # keep track of this itself, however if you wish to start from a different block this\n  # can be set.\n  # start-block: 500\n```\n\n## Support\n\nWe gratefully acknowledge the Ethereum Foundation for supporting chaind through their grant FY21-0360, which allowed collection of Ethereum 1 deposits.\n\n## Maintainers\n\nJim McDonald: [@mcdee](https://github.com/mcdee).\n\n## Contribute\n\nContributions welcome. Please check out [the issues](https://github.com/wealdtech/chaind/issues).\n\n## License\n\n[Apache-2.0](LICENSE) \u00a9 2020 Weald Technology Trading.\n", "release_dates": []}, {"name": "eigenda", "description": "Secure, high-throughput, and decentralized Data Availability", "language": "Go", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "![Unit Tests](https://github.com/Layr-Labs/eigenda/actions/workflows/unit-tests.yml/badge.svg)\n![Integration Tests](https://github.com/Layr-Labs/eigenda/actions/workflows/integration-tests.yml/badge.svg)\n![Linter](https://github.com/Layr-Labs/eigenda/actions/workflows/golangci-lint.yml/badge.svg)\n![Contracts](https://github.com/Layr-Labs/eigenda/actions/workflows/test-contracts.yml/badge.svg)\n![Go Coverage](https://github.com/Layr-Labs/eigenda/wiki/coverage.svg)\n\n# EigenDA\n\n## Overview\n\nEigenDA is a secure, high-throughput, and decentralized data availability (DA) service built on top of Ethereum using the [EigenLayer](https://github.com/Layr-Labs/eigenlayer-contracts) restaking primitives.\n\nTo understand more how EigenDA works and how it transforms the modern landscape of data availability, continue reading [EigenDA introduction](https://www.blog.eigenlayer.xyz/intro-to-eigenda-hyperscale-data-availability-for-rollups/).\n\nTo dive deep into the technical details, continue reading [EigenDA protocol spec](https://github.com/Layr-Labs/eigenda/blob/master/docs/spec/overview.md).\n\nIf you're interested in integrating your rollup with EigenDA, please fill out the [EigenDA questionnaire](https://docs.google.com/forms/d/e/1FAIpQLSez6PG-BL6C6Mc4QY1M--vbV219OGL_0Euv2zhJ1HmcUiU7cw/viewform).\n\n## API Documentation\n\nThe EigenDA public API is documented [here](https://github.com/Layr-Labs/eigenda/tree/master/api/docs).\n\n## Operating EigenDA Node\n\nIf you want to be an EigenDA operator and run a node, please clone [Operator Setup Guide](https://github.com/Layr-Labs/eigenda-operator-setup) GitHub repo and follow the instructions there.\n\n## Contributing\nWe welcome all contributions! There are many ways to contribute to the project, including but not limited to:\n\n- Opening a PR\n- [Submitting feature requests or bugs](https://github.com/Layr-Labs/eigenda/issues/new/choose)\n- Improving our product or contribution documentation\n- Voting on [open issues](https://github.com/Layr-Labs/eigenda/issues) or\n  contributing use cases to a feature request\n\n## Contact\n\n- [Open an Issue](https://github.com/Layr-Labs/eigenda/issues/new/choose)\n- [EigenLayer/EigenDA forum](https://forum.eigenlayer.xyz/c/eigenda/9)\n- [Email](mailto:eigenda-support@eigenlabs.org)\n- [Follow us on Twitter](https://twitter.com/eigenlayer)\n", "release_dates": ["2024-02-08T22:48:50Z", "2024-01-26T00:42:13Z", "2024-01-17T00:30:15Z", "2024-01-10T01:16:41Z", "2023-12-21T23:38:05Z", "2023-12-13T22:57:10Z", "2023-12-05T23:10:00Z", "2023-11-22T23:29:11Z", "2023-11-14T23:11:26Z"]}, {"name": "eigenda-examples", "description": null, "language": "Rust", "license": null, "readme": "# EigenDA Clients and Examples\n\n## Rust Demo\n\n```\ncd eigenda-examples\ngit submodule update --init --recursive --force\ncd rust_example\ncargo run\n```\n", "release_dates": []}, {"name": "eigenda-operator-setup", "description": "Quick start setup guide for EigenDA", "language": "Shell", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "## Installation of EigenDA using docker\n\n### Prerequisites:\n* Docker installed\n* [EigenLayer CLI](https://github.com/Layr-Labs/eigenlayer-cli) installed\n\n> NOTE: For any Docker based commands, if you have installed as root then you might have to append `sudo` in front of the command.\n\n### Core operations\n* Register your operator to EigenLayer using [EigenLayer CLI](https://github.com/Layr-Labs/eigenlayer-cli/blob/master/README.md)\n\n### Setup EigenDA\nThe easiest way to set up EigenDA is to clone the repo and follow the instructions below.\n\n#### Clone repo\nClone this repo and execute the following commands:\n```bash\ngit clone https://github.com/Layr-Labs/eigenda-operator-setup.git\ncd eigenda-operator-setup\ncp .env.example .env\n```\nUpdate the `TODO` sections in the  `.env` file given in the root directory of the repository with your own details.:\n\n### Create some local folders which are required by EigenDA\n```bash\nmkdir -p $HOME/.eigenlayer/eigenda/logs\nmkdir -p $HOME/.eigenlayer/eigenda/db\n```\n\n### Operator Networking Security Setup\nRetrieval Setup:\n\nIn order for users to retrieve data from your node, you will need to open access to retrieval ports. \n\nEnsure the port specified as `NODE_RETRIEVAL_PORT` in the [.env](https://github.com/Layr-Labs/eigenda-operator-setup/blob/master/.env.example#L17) has open access to the public internet.\n\nFor users with private IP e.g. connecting to the Internet via a router, you may need to perform port forwarding to open the retrieval ports. \nUse a web browser and navigate to http://192.168.0.1 and set-up port forwarding according to instruction of your router.\n\nDispersal Setup:\n\nIn order to limit traffic from the EigenLabs hosted Disperser, please restrict your node's ingress traffic to be allowed by the list provided below and port number set as `NODE_DISPERSAL_PORT` in the [.env](https://github.com/Layr-Labs/eigenda-operator-setup/blob/master/.env#L14) in the below setup.\n\n* `3.221.120.68/32`\n* `52.2.226.152/32`\n* `18.214.113.214/32`\n\nFor users with private IP, you may need to perform port forwarding to open the dispersal ports. Refer to retrieval setup for more details.\n\n### Opt-in into EigenDA\nThis command also downloads the latest SRS points (~8 GB) if they don't exist and can take upto 10 minutes to complete for the first time based on your network speed.\n```bash\n./run.sh opt-in\n```\nIt will use the `NODE_HOSTNAME` from [.env](.env.example) as your current IP.\n\n### Run EigenDA\nExecute the following command to start the docker containers:\n```\ndocker compose up -d\n```\nIt will start the node and nginx containers and if you do `docker ps` you should see something like this:\n![image](./images/node-up.png)\n\nyou can view the logs using:\n```\ndocker logs -f <container_id>\n```\nIf you have successfully opted in to EigenDA and correctly running your EigenDA software, you should see the following logs for your EigenDA container:\n\n[![image](./images/eigenda-logs.png)](./images/eigenda-logs.png)\n\nThe following example log messages confirm that your EigenDA node software is up and running:\n\n```\n2024/01/09 23:42:28 maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined\n2024/01/09 23:42:28 Initializing Node\n2024/01/09 23:42:32     Reading G1 points (33554432 bytes) takes 13.362879ms\n2024/01/09 23:42:36     Parsing takes 3.60454026s\n2024/01/09 23:42:36     Reading G2 points (67108864 bytes) takes 28.110653ms\n2024/01/09 23:43:37     Parsing takes 1m1.676967232s\nnumthread 16\nINFO [01-09|23:43:38.284|github.com/Layr-Labs/eigenda/common/logging/logging.go:65] Starting metrics server at port :9092    caller=logging.go:65\nINFO [01-09|23:43:38.284|github.com/Layr-Labs/eigenda/node/node.go:170]             Enabled metrics                          socket=:9092 caller=node.go:170\nINFO [01-09|23:43:38.284|github.com/Layr-Labs/eigenda/common/logging/logging.go:65] Starting node api server at address localhost:9091 caller=logging.go:65\nINFO [01-09|23:43:38.284|github.com/Layr-Labs/eigenda/node/node.go:174]             Enabled node api                         port=9091 caller=node.go:174\nINFO [01-09|23:43:38.284|github.com/Layr-Labs/eigenda/common/logging/logging.go:65] The node has successfully started. Note: if it's not opted in on https://goerli.eigenlayer.xyz/avs/eigenda, then please follow the EigenDA operator guide section in docs.eigenlayer.xyz to register caller=logging.go:65\nINFO [01-09|23:43:38.284|github.com/Layr-Labs/eigensdk-go/nodeapi/nodeapi.go:240]   node api server running                  addr=localhost:9091 caller=nodeapi.go:240\nINFO [01-09|23:43:38.284|github.com/Layr-Labs/eigenda/node/node.go:391]             Start checkCurrentNodeIp goroutine in background to detect the current public IP of the operator node caller=node.go:391\nINFO [01-09|23:43:38.284|github.com/Layr-Labs/eigenda/node/grpc/server.go:95]       port                                     32005=address [::]:32005=\"GRPC Listening\" caller=server.go:95\nINFO [01-09|23:43:38.284|github.com/Layr-Labs/eigenda/node/node.go:220]             Start expireLoop goroutine in background to periodically remove expired batches on the node caller=node.go:220\nINFO [01-09|23:43:38.284|github.com/Layr-Labs/eigenda/node/node.go:368]             Start checkRegisteredNodeIpOnChain goroutine in background to subscribe the operator socket change events onchain caller=node.go:368\nINFO [01-09|23:43:38.284|github.com/Layr-Labs/eigenda/node/grpc/server.go:119]      port                                     32004=address [::]:32004=\"GRPC Listening\" caller=server.go:119\n```\n\nThe following example log messages confirm that your node is receiving traffic from the Disperser. If you do not see these log messages then either you have not successfully [opted-in to EigenDA](#opt-in-into-eigenda) or your [network security group](#operator-networking-security-setup) might not be setup correctly.\n\n```\nDEBUG[01-09|23:44:10.078|github.com/Layr-Labs/eigenda/node/node.go:298]             Store batch took                         duration:=5.831581ms caller=node.go:298\nBatch verify 13 frames of 512 symbols out of 1 blobs\nBatch verify 450 frames of 2 symbols out of 50 blobs\nDEBUG[01-09|23:44:10.153|github.com/Layr-Labs/eigenda/node/node.go:318]             Validate batch took                      duration:=80.907297ms caller=node.go:318\nTRACE[01-09|23:44:10.153|github.com/Layr-Labs/eigenda/node/node.go:329]             Signed batch header hash                 pubkey=0x2543eddc5dd2d29190be84f323e17cef8f795970d71cc14db635a613b86ae3942bb9f8787d7197b230d450210c694361a2100531d150f5a94c2905a224c4ee390beba2c7e3166506359b7ac43fe9603e7bd981b28447c3ed6b28a7d263274cc717263cb88a192ccaaa76bb68308beaa01ef93b862b98c86ba48b69f8c153ad27 caller=node.go:329\nDEBUG[01-09|23:44:10.153|github.com/Layr-Labs/eigenda/node/node.go:332]             Sign batch took                          duration=\"365.481\u00b5s\" caller=node.go:332\nINFO [01-09|23:44:10.153|github.com/Layr-Labs/eigenda/node/node.go:334]             StoreChunks succeeded                    caller=node.go:334\nDEBUG[01-09|23:44:10.153|github.com/Layr-Labs/eigenda/node/node.go:336]             Exiting process batch                    duration=81.474727ms caller=node.go:336\nDEBUG[01-09|23:44:59.727|github.com/Layr-Labs/eigenda/node/node.go:298]             Store batch took                         duration:=3.972838ms  caller=node.go:298\nBatch verify 8 frames of 4 symbols out of 1 blobs\nBatch verify 432 frames of 2 symbols out of 48 blobs\nDEBUG[01-09|23:44:59.805|github.com/Layr-Labs/eigenda/node/node.go:318]             Validate batch took                      duration:=82.711666ms caller=node.go:318\nTRACE[01-09|23:44:59.806|github.com/Layr-Labs/eigenda/node/node.go:329]             Signed batch header hash                 pubkey=0x2543eddc5dd2d29190be84f323e17cef8f795970d71cc14db635a613b86ae3942bb9f8787d7197b230d450210c694361a2100531d150f5a94c2905a224c4ee390beba2c7e3166506359b7ac43fe9603e7bd981b28447c3ed6b28a7d263274cc717263cb88a192ccaaa76bb68308beaa01ef93b862b98c86ba48b69f8c153ad27 caller=node.go:329\nDEBUG[01-09|23:44:59.806|github.com/Layr-Labs/eigenda/node/node.go:332]             Sign batch took                          duration=\"370.048\u00b5s\" caller=node.go:332\nINFO [01-09|23:44:59.806|github.com/Layr-Labs/eigenda/node/node.go:334]             StoreChunks succeeded                    caller=node.go:334\nDEBUG[01-09|23:44:59.806|github.com/Layr-Labs/eigenda/node/node.go:336]             Exiting process batch                    duration=83.241162ms caller=node.go:336\nDEBUG[01-09|23:45:49.698|github.com/Layr-Labs/eigenda/node/node.go:298]             Store batch took                         duration:=4.118867ms  caller=node.go:298\nBatch verify 477 frames of 2 symbols out of 53 blobs\nDEBUG[01-09|23:45:49.771|github.com/Layr-Labs/eigenda/node/node.go:318]             Validate batch took                      duration:=77.685497ms caller=node.go:318\nTRACE[01-09|23:45:49.771|github.com/Layr-Labs/eigenda/node/node.go:329]             Signed batch header hash                 pubkey=0x2543eddc5dd2d29190be84f323e17cef8f795970d71cc14db635a613b86ae3942bb9f8787d7197b230d450210c694361a2100531d150f5a94c2905a224c4ee390beba2c7e3166506359b7ac43fe9603e7bd981b28447c3ed6b28a7d263274cc717263cb88a192ccaaa76bb68308beaa01ef93b862b98c86ba48b69f8c153ad27 caller=node.go:329\nDEBUG[01-09|23:45:49.771|github.com/Layr-Labs/eigenda/node/node.go:332]             Sign batch took                          duration=\"345.3\u00b5s\"   caller=node.go:332\nINFO [01-09|23:45:49.772|github.com/Layr-Labs/eigenda/node/node.go:334]             StoreChunks succeeded                    caller=node.go:334\nDEBUG[01-09|23:45:49.772|github.com/Layr-Labs/eigenda/node/node.go:336]             Exiting process batch                    duration=78.216395ms caller=node.go:336\n```\n\nTear down container\n```bash\ndocker compose down\n```\n### Opt-out into EigenDA\n```bash\n./run.sh opt-out\n```\n\n### Update Quorums (Optional)\nIf you want to update the quorums, update the `NODE_QUORUM_ID_LIST` in your .env file\nto have a command separated value of new quorums (for ex: `0,1`) then running the following command:\n> **_NOTE:_** This command sends two transactions (opt-out and opt-in again). \n> This means if it opts-in again it will have to go via churner and will check the TVL based on lowest stake if the network is full.\n```bash\n./run.sh update-quorums\n```\n\n### Upgrade your node\n\nUpgrade the AVS software for your EigenDA service setup by following the steps below:\n\n**Step 1:** Pull the latest repo and update version\n\n```\ncd eigenda-operator-setup\ngit pull\n```\nUpdate the `MAIN_SERVICE_IMAGE` in your `.env` file with the latest EigenDA version as per the release notes.\n\n> **_NOTE:_** If there are any specific instructions that needs to be followed for any upgrade, those instructions will be given with the release notes of the specific release. Please check the latest [release notes](https://github.com/Layr-Labs/eigenda-operator-setup/releases) on GitHub and follow the instructions before starting the services again.\n\n**Step 2:** Pull the latest Docker images\n\n```\ndocker compose pull\n```\n\n**Step 3:** Stop the existing services\n\n```\ndocker compose down\n```\n\n**Step 4:** Start your services again\n\nMake sure your `.env` file still has correct values in the TODO sections before you restart your node.\n\n```\ndocker compose up -d\n```\n\n## Metrics and Dashboard\n\n### Quickstart\nEigenDA provides a quickstart guide to run the Prometheus, Grafana, and Node exporter stack.\nCheckout the README [here](monitoring/README.md) for more details. If you want to manually set this up, follow the steps below.\n\n### Metrics\nTo check if the metrics are being emitted, run the following command:\n```bash\ncurl http://localhost:<NODE_METRICS_PORT>/metrics\n```\n\nYou should see something like\n```\n# HELP eigen_performance_score The performance metric is a score between 0 and 100 and each developer can define their own way of calculating the score. The score is calculated based on the performance of the Node and the performance of the backing services.\n# TYPE eigen_performance_score gauge\neigen_performance_score{avs_name=\"da-node\"} 100\n# HELP eigen_registered_stakes Operator stake in <quorum> of <avs_name>'s StakeRegistry contract\n# TYPE eigen_registered_stakes gauge\neigen_registered_stakes{avs_name=\"da-node\",quorum_name=\"eth_quorum\",quorum_number=\"0\"} 2.654867142483745e+19\n# HELP eigen_rpc_request_duration_seconds Duration of json-rpc <method> in second\n...\n```\n### Prometheus\n[Prometheus](https://prometheus.io/download) is being used to scrape the metrics from the EigenDA node.\n\nCreate the following file in `$HOME/.eigenlayer/config/prometheus.yml`\n```yaml\nglobal:\n  scrape_interval: 15s # By default, scrape targets every 15 seconds.\n\n  # Attach these labels to any time series or alerts when communicating with\n  # external systems (federation, remote storage, Alertmanager).\n  external_labels:\n    monitor: \"codelab-monitor\"\n\n# A scrape configuration containing exactly one endpoint to scrape:\n# Here it's Prometheus itself.\nscrape_configs:\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n  - job_name: \"prometheus\"\n\n    # Override the global default and scrape targets from this job every 5 seconds.\n    scrape_interval: 5s\n\n    static_configs:\n      # Point to the same endpoint that EigenDA is publishing on\n      - targets: [\"localhost:<NODE_METRICS_PORT>\"]\n```\n\nStart Prometheus\n```bash\nprometheus --config.file=\"$HOME/.eigenlayer/config/prometheus.yml\"\n```\n\nIf you want to use Docker, follow [this](https://prometheus.io/docs/prometheus/latest/installation/#volumes-bind-mount) link.\n```bash\ndocker run -d \\\n    -p 9090:9090 \\\n    -v ~/.eigenlayer/config/prometheus.yml:/etc/prometheus/prometheus.yml \\\n    prom/prometheus\n```\n\n### Grafana\nGrafana is used to visualize the metrics from the EigenDA node.\n\nYou can use [OSS Grafana](https://grafana.com/oss/grafana/) for it or any other Dashboard provider.\n\nStart the Grafana server\n```bash\ngrafana server\n```\nYou can also use [Docker](https://grafana.com/docs/grafana/latest/setup-grafana/installation/docker/)\n```bash\ndocker run -d -p 3000:3000 --name=grafana grafana/grafana-enterprise\n```\n\nYou should be able to navigate to `http://localhost:3000` and login with `admin`/`admin`.\nYou will need to add a datasource to Grafana. You can do this by navigating to `http://localhost:3000/datasources` and adding a Prometheus datasource. By default, the Prometheus server is running on `http://localhost:9090`. You can use `http://prometheus:9090` as the server URL for the datasource.\n\n#### Useful Dashboards\nEigenDA provides a set of Grafana dashboards that provide insights into key performance indicators and health metrics of an EigenDA node. These dashboards can be accessed [here](monitoring/dashboards).\nOnce you have Grafana setup, they should be automatically imported.\n\n### Node exporter\nEigenDA emits DA specific metrics but, it's also important to keep track of the node's health. For this, we will use [Node Exporter](https://prometheus.io/docs/guides/node-exporter/) which is a Prometheus exporter for hardware and OS metrics exposed by *NIX kernels, written in Go with pluggable metric collectors.\nInstall the binary or use Docker to [run](https://hub.docker.com/r/prom/node-exporter) it.\n\n```bash\ndocker pull prom/node-exporter\ndocker run -d -p 9100:9100 --name node-exporter prom/node-exporter\n```\n\n## Troubleshooting\n* If you see the following error:\n    ```\n    permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get \"http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json\": dial unix /var/run/docker.sock: connect: permission denied\n    ```\n    Use the same command by prepending `sudo` in front of it.\n", "release_dates": ["2024-02-08T22:57:01Z", "2024-01-26T02:00:29Z", "2024-01-20T21:24:44Z", "2024-01-10T20:05:08Z", "2023-12-21T23:47:44Z", "2023-12-15T19:20:49Z", "2023-11-29T01:04:28Z", "2023-11-15T00:09:36Z"]}, {"name": "eigenda-rollups", "description": "Genesis files and configurations for various eigenda-rollups", "language": null, "license": null, "readme": null, "release_dates": []}, {"name": "eigenlayer-avs-playgrounds", "description": "Basic repo for AVSs to test their integrations with eigenlayer and learn about our contract APIs", "language": "Solidity", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "**OUTDATED**: Please check out our [incredible-squaring-avs](https://github.com/Layr-Labs/incredible-squaring-avs) repo for the most up-to-date avs demo with offchain code integration.\n\n# Eigenlayer AVS playgrounds\n\nThe purpose of this AVS playground is:\n- learn how, as an AVS developer, can get your AVS contracts to interact with the interfaces that are provided by EigenLayer, \n- learn how you can do e2e testing for your AVS contracts,\n- learn about registry contracts and understand how you can incorporate/use into your AVS contracts.\n\nBefore you delve further in AVS playground, it is essential that you are thinking deeply about the logic for your AVS contracts, namely, task submission and storage logic, slashing logic, registry contracts or you are implementing them. At either stage, you will find the AVS playground very useful. \n\nAnother important point about AVS playground is that operators are not running any off-chain node software. For AVS playground, we just deployed registry contracts.\n\n\n## Installation\n\n```\ngit clone --recursive git@github.com:Layr-Labs/eigenlayer-avs-playgrounds.git\n```\n\nMake sure to clone with the `--recursive` flag to get the submodules (eigenlayer contracts and forge-test dependencies).\n\n## Dependencies\n\nYou will need to [install foundry](https://book.getfoundry.sh/getting-started/installation). Also make sure to run `foundryup` to be on the latest version.\n\n## Eigenlayer contracts\n\nWe have deployed a parallel set of contracts on goerli, with all functionality unpaused, for middleware teams to test with. The contract addresses can be found [here](./script/output/5/eigenlayer_deployment_output.json).\nThe easiest way to start integrating with these contracts is to fork goerli on a local `anvil` chain. You can install anvil using this [guide](https://book.getfoundry.sh/getting-started/installation):\n\n```\nanvil --fork-url https://goerli.infura.io/v3/9aa3d95b3bc440fa88ea12eaa4456161\n```\n\nIf the above URL is not working, choose another one from https://chainlist.org/?testnets=true&search=goerli.\n\n## Deploy the playgroundAVS contracts\n\nIn a separate terminal, run\n\n```\nexport RPC_URL=http://localhost:8545\nexport PRIVATE_KEY=0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80\nmake deploy-avs\n```\n\nThis deploys the playgroundAVS service manager contract (which you will need to modify to contain your own AVS' slashing logic) as well as a suite of registry contracts (which can most likely be used as-is for most AVSs).\n\n## Makefile starting point\n\nAt any point run `make` to get info on the different possible commands.\n\n<img src=\"./images/makefile.png\" style=\"width: 100%\"/>\n\n## Operator and staker interactions\n\n\n\nTo register operators with EigenLayer, we use the following command: \n```\nmake register-operators-with-eigenlayer\n```\n\nTo register operators with dummy registry contracts, we use the following command:\n```\nmake register-operators-with-avs\n```\n\nFor the staker to delegate to the operator, we use the following command:\n```\nmake staker-delegate-to-operators:\n```\n\n\nAt any point, to know the status of your operators and stakers, run the STATUS_PRINTERS functions\n\n```\nmake print-operators-status\n```\nand\n```\nmake print-stakers-status\n```\n\nThe playground also supports stakers to queue withdrawal request for eithdrawing from EigenLayer and then complete their withdrawals.\nFor a lot more detail and explanation of each command in detail, look at the [runbook](./docs/runbook.md).\n\n## Playbooks\n\nAfter having deployed all contracts, you can interact with by running the different playbook scripts found in [script/playbooks](./script/playbooks/). These follow the structure outlined in the [AVS-guide](https://github.com/Layr-Labs/eigenlayer-contracts/blob/master/docs/AVS-Guide.md). Also have a look at the [AVS Smart Contracts Template Architecture](https://docs.google.com/document/d/1b_a5Xx5DugM_lWPOdv-vJ3wQnT20IwZ8e8RL6TtlgoM/edit?usp=sharing) doc to understand the registry contracts and how they interact with the service manager and eigenlayer contracts.\n", "release_dates": []}, {"name": "eigenlayer-cli", "description": "EigenLayer CLI ", "language": "Go", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "![Tests](https://github.com/Layr-Labs/eigenlayer-cli/actions/workflows/tests.yml/badge.svg)\n![Linter](https://github.com/Layr-Labs/eigenlayer-cli/actions/workflows/golangci-lint.yml/badge.svg)\n![Build](https://github.com/Layr-Labs/eigenlayer-cli/actions/workflows/build.yml/badge.svg)\n\n# EigenLayer CLI\n\nEigenLayer CLI is used to manage core operator functionalities like local key management, operator registration and updates.\n\n<!-- TOC -->\n* [EigenLayer CLI](#eigenlayer-cli)\n  * [Supported Operating Systems](#supported-operating-systems)\n  * [Install `eigenlayer` CLI using a binary](#install-eigenlayer-cli-using-a-binary)\n    * [Installing in a custom location](#installing-in-a-custom-location)\n  * [Install `eigenlayer` CLI using Go](#install-eigenlayer-cli-using-go)\n  * [Install `eigenlayer` CLI from source](#install-eigenlayer-cli-from-source)\n  * [Documentation](#documentation)\n<!-- TOC -->\n\n## Supported Operating Systems\n| Operating System | Architecture |\n|------------------|--------------|\n| Linux            | amd64        |\n| Linux            | arm64        |\n| Darwin           | amd64        |\n| Darwin           | arm64        |\n\n\n## Install `eigenlayer` CLI using a binary\nTo download a binary for the latest release, run:\n```bash\ncurl -sSfL https://raw.githubusercontent.com/layr-labs/eigenlayer-cli/master/scripts/install.sh | sh -s\n```\nThe binary will be installed inside the `~/bin` directory.\n\nTo add the binary to your path, run:\n```bash\nexport PATH=$PATH:~/bin\n```\n\n### Installing in a custom location\nTo download the binary in a custom location, run:\n```bash\ncurl -sSfL https://raw.githubusercontent.com/layr-labs/eigenlayer-cli/master/scripts/install.sh | sh -s -- -b <custom_location>\n```\n\n## Install `eigenlayer` CLI using Go\n\nFirst, install the Go programming language following the [official instructions](https://go.dev/doc/install). You need at least the `1.21` version.\n\n> Eigenlayer is only supported on **Linux**. Make sure you install Go for Linux in a Linux environment (e.g. WSL2, Docker, etc.)\n\nThis command will install the `eigenlayer` executable along with the library and its dependencies in your system:\n\n> As the repository is private, you need to set the `GOPRIVATE` variable properly by running the following command: `export GOPRIVATE=github.com/Layr-Labs/eigenlayer-cli,$GOPRIVATE`. Git will automatically resolve the private access if your Git user has all the required permissions over the repository.\n\n```bash\ngo install github.com/Layr-Labs/eigenlayer-cli/cmd/eigenlayer@latest\n```\n\nThe executable will be in your `$GOBIN` (`$GOPATH/bin`).\n\nTo check if the `GOBIN` is not in your PATH, you can execute `echo $GOBIN` from the Terminal. If it doesn't print anything, then it is not in your PATH. To add `GOBIN` to your PATH, add the following lines to your `$HOME/.profile`:\n\n```bash\nexport GOBIN=$GOPATH/bin\nexport PATH=$GOBIN:$PATH\n```\n\n> Changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, run the shell commands directly or execute them from the profile using a command such as `source $HOME/.profile`.\n\n## Install `eigenlayer` CLI from source\n\nWith this method, you generate the binary manually (need Go installed), downloading and compiling the source code:\n\n```bash\ngit clone https://github.com/Layr-Labs/eigenlayer-cli.git\ncd eigenlayer-cli\nmkdir -p build\ngo build -o build/eigenlayer cmd/eigenlayer/main.go\n```\n\nor if you have `make` installed:\n\n```bash\ngit clone https://github.com/Layr-Labs/eigenlayer-cli.git\ncd eigenlayer-cli\nmake build\n```\n\nThe executable will be in the `build` folder.\n\n---\nIn case you want the binary in your PATH (or if you used the [Using Go](#install-eigenlayer-cli-using-go) method and you don't have `$GOBIN` in your PATH), please copy the binary to `/usr/local/bin`:\n\n```bash\n# Using Go\nsudo cp $GOPATH/bin/eigenlayer /usr/local/bin/\n\n# Build from source\nsudo cp eigenlayer-cli/build/eigenlayer /usr/local/bin/\n```\n\n## Documentation\nPlease refer to the full documentation [here](https://docs.eigenlayer.xyz/operator-guides/operator-installation).\n\nLinks to specific sections are provided below.\n* [Create Keys](https://docs.eigenlayer.xyz/operator-guides/operator-installation#create-keys)\n* [Import Keys](https://docs.eigenlayer.xyz/operator-guides/operator-installation#import-keys)\n* [List Keys](https://docs.eigenlayer.xyz/operator-guides/operator-installation#list-keys)\n* [Export Keys](https://docs.eigenlayer.xyz/operator-guides/operator-installation#export-keys)\n* [Fund Wallet with ETH](https://docs.eigenlayer.xyz/operator-guides/operator-installation#fund-ecdsa-wallet)\n* [Register Operator](https://docs.eigenlayer.xyz/operator-guides/operator-installation#registration)\n* [Operator Status](https://docs.eigenlayer.xyz/operator-guides/operator-installation#checking-status-of-registration)\n* [Metadata Updates](https://docs.eigenlayer.xyz/operator-guides/operator-installation#metadata-updates)\n* [Frequently Asked Questions](https://docs.eigenlayer.xyz/operator-guides/faq)\n* [Troubleshooting](https://docs.eigenlayer.xyz/operator-guides/troubleshooting)\n\nIf you see any issues in documentation please create an issue or PR [here](https://github.com/Layr-Labs/eigenlayer-docs)", "release_dates": ["2024-02-16T20:55:04Z", "2024-02-09T22:23:44Z", "2024-02-09T00:01:12Z", "2024-01-31T19:34:39Z", "2024-01-09T18:32:05Z", "2023-12-11T20:29:22Z"]}, {"name": "eigenlayer-contracts", "description": null, "language": "Solidity", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "<a name=\"introduction\"/></a>\n\n# EigenLayer\n\nEigenLayer is a set of smart contracts deployed on Ethereum that enable restaking of assets to secure new services. This repo contains the EigenLayer core contracts, whose currently-supported assets include beacon chain ETH and several liquid staking tokens (LSTs). Users use these contracts to deposit and withdraw these assets, as well as delegate them to operators providing services to AVSs.\n\nThe most up to date mainnet and testnet deployment addresses can be found on our [docs site](https://docs.eigenlayer.xyz/eigenlayer/deployed-contracts).\n\n## Getting Started\n\n* [Documentation](#documentation)\n* [Building and Running Tests](#building-and-running-tests)\n\n## Documentation\n\n### Basics\n\nTo get a basic understanding of EigenLayer, check out [You Could've Invented EigenLayer](https://www.blog.eigenlayer.xyz/ycie/). Note that some of the document's content describes features that do not exist yet (like the Slasher). To understand more about how restakers and operators interact with EigenLayer, check out these guides:\n* [Restaking User Guide](https://docs.eigenlayer.xyz/restaking-guides/restaking-user-guide)\n* [Operator Guide](https://docs.eigenlayer.xyz/operator-guides/operator-introduction)\n\n### Deep Dive\n\nThe most up-to-date and technical documentation can be found in [/docs](/docs). If you're a shadowy super coder, this is a great place to get an overview of the contracts before diving into the code.\n\nTo get an idea of how users interact with these contracts, check out our integration tests: [/src/test/integration](./src/test/integration/).\n\n## Building and Running Tests\n\nThis repository uses Foundry. See the [Foundry docs](https://book.getfoundry.sh/) for more info on installation and usage. If you already have foundry, you can build this project and run tests with these commands:\n\n```\nfoundryup\n\nforge build\nforge test\n```\n\n### Running Fork Tests\n\nWe have a few fork tests against ETH mainnet. Passing these requires the environment variable `RPC_MAINNET` to be set. See `.env.example` for an example. Once you've set up your environment, `forge test` should show these fork tests passing.\n\nAdditionally, to run all tests in a forked environment, [install yq](https://mikefarah.gitbook.io/yq/v/v3.x/). Then, set up your environment using this script to read from `config.yml`:\n\n`source source-env.sh [goerli|local]`\n\nThen run the tests:\n\n`forge test --fork-url [RPC_URL]`\n\n### Running Static Analysis\n\n1. Install [solhint](https://github.com/protofire/solhint), then run:\n\n`solhint 'src/contracts/**/*.sol'`\n\n2. Install [slither](https://github.com/crytic/slither), then run:\n\n`slither .`\n\n### Generate Inheritance and Control-Flow Graphs\n\n1. Install [surya](https://github.com/ConsenSys/surya/) and graphviz:\n\n```\nnpm i -g surya\n\napt install graphviz\n```\n\n2. Then, run:\n\n```\nsurya inheritance ./src/contracts/**/*.sol | dot -Tpng > InheritanceGraph.png\n\nsurya mdreport surya_report.md ./src/contracts/**/*.sol\n```\n\n", "release_dates": ["2024-02-09T16:56:02Z", "2023-11-07T16:13:26Z", "2023-08-02T20:31:17Z", "2023-08-02T20:23:30Z"]}, {"name": "eigenlayer-docs", "description": null, "language": "JavaScript", "license": null, "readme": "# Website\n\nThis website is built using [Docusaurus 2](https://docusaurus.io/), a modern static website generator.\n\n### Installation\n\n```\nyarn\n```\n\n### Local Development\n\n```\nyarn start\n```\n\nThis command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.\n\n### Build\n\n```\nyarn build\n// to test out the fully built site\nyarn serve\n```\n\nThis command generates static content into the `build` directory and can be served using any static contents hosting service.\n\n### Deployment\n\nUsing SSH:\n\n```\nUSE_SSH=true yarn deploy\n```\n\nNot using SSH:\n\n```\nGIT_USER=<Your GitHub username> yarn deploy\n```\n\nIf you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the `gh-pages` branch.\n", "release_dates": []}, {"name": "eigenlayer-middleware", "description": null, "language": "Solidity", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "[core-docs-dev]: https://github.com/Layr-Labs/eigenlayer-contracts/tree/dev/docs\n[core-repo]: https://github.com/Layr-Labs/eigenlayer-contracts\n\n# EigenLayer Middleware\n\nEigenLayer is a set of smart contracts deployed on Ethereum that enable restaking of assets to secure new services called AVSs (actively validated services). The core contracts that enable these features can be found in the [`eigenlayer-contracts` repo][core-repo].\n\nThis repo contains smart contracts used to create an AVS that interacts with the EigenLayer core contracts. Because these contracts are meant to be used by any AVS, there is no single deployment. However, you can see EigenDA's deployment info on our [docs site](https://docs.eigenlayer.xyz/eigenda/deployed-contracts).\n\n## Getting Started\n\n* [Documentation](#documentation)\n* [Building and Running Tests](#building-and-running-tests)\n\n## Documentation\n\n### Basics\n\nTo get a basic understanding of EigenLayer, check out [You Could've Invented EigenLayer](https://www.blog.eigenlayer.xyz/ycie/). Note that some of the document's content describes features that do not exist yet (like the Slasher). To understand more about how restakers and operators interact with EigenLayer, check out these guides:\n* [Restaking User Guide](https://docs.eigenlayer.xyz/restaking-guides/restaking-user-guide)\n* [Operator Guide](https://docs.eigenlayer.xyz/operator-guides/operator-introduction)\n\nMost of this content is intro-level and describes user interactions with the EigenLayer core contracts, but it should give you a good enough starting point.\n\n### Deep Dive\n\nFor shadowy super-coders:\n* The most up-to-date technical documentation can be found in [/docs](/docs).\n* To get an idea of how users interact with these contracts, check out the integration tests: [/test/integration](./test/integration)\n* To explore the EigenLayer core contracts, check out the core repo technical docs [here][core-docs-dev].\n\n## Building and Running Tests\n\nThis repository uses Foundry. See the [Foundry docs](https://book.getfoundry.sh/) for more info on installation and usage. If you already have foundry, you can build this project and run tests with these commands:\n\n```sh\nfoundryup\n\nforge build\nforge test\n```", "release_dates": ["2024-02-08T20:16:11Z", "2023-11-17T00:42:02Z"]}, {"name": "eigenpod-proofs-generation", "description": null, "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "WARNING: Please note that this repository is a work in progress and is currently unaudited. Use it with caution as it may contain unfinished features or bugs.\"\n# Introduction\nThis repository allows users to generate the proofs necessary to prove consensus layer state on EigenLayer, specifically the EigenPods system.  These proofs are used for verifying 1) that withdrawal credentials of a validator are pointed to an EigenPod, 2) that changes in balance of a validator due to slashing, etc can be propagated to EigenLayer's smart contracts 3) Prove a validator's withdrawal on the consensus layer, which then allows a staker to withdraw their validator's ETH from their EigenPod. Specifically, these proofs are passed as inputs to the verifyWithdrawalCredentials(), verifyBalanceUpdates() and verifyAndProcessWithdrawals() functions, see [here](https://github.com/Layr-Labs/eigenlayer-contracts/blob/master/src/contracts/interfaces/IEigenPod.sol) for the exact function interface definitions.\n\n\n## How to Retrieve Data\n\nAn important note is that this CLI is designed to be used with inputs that can be retrieved from a consensus layer full node, [here](https://ethereum.github.io/beacon-APIs/) is the relevant API specification.  These are the api ednpoints that are required to retrieve the 3 consensus layer object required to generate proofs with this CLI:\n\n### Beacon State\n[This](https://ethereum.github.io/beacon-APIs/#/Debug/getStateV2) is the entire consensus layer [state](https://github.com/ethereum/consensus-specs/blob/dev/specs/phase0/beacon-chain.md#beaconstate) object at a given slot.  The following endpoint returns this object:\n```\n/beacon/eth/v2/debug/beacon/states/[SLOT_NUMBER]\n```\n### Beacon Block\n[This](https://ethereum.github.io/beacon-APIs/#/Beacon/getBlockV2) is the [beacon block](https://github.com/ethereum/consensus-specs/blob/dev/specs/phase0/beacon-chain.md#beaconstate) object.  The following endpoint returns this object:\n```\nbeacon/eth/v2/beacon/blocks/[SLOT_NUMBER]\n```\n### Beacon Block Header\n[This](https://ethereum.github.io/beacon-APIs/#/Beacon/getBlockHeader) is the [block header](https://github.com/ethereum/consensus-specs/blob/dev/specs/phase0/beacon-chain.md#beaconblockheader) for a beacon block.  The following endpoint returns this object:\n```\n/beacon/eth/v1/beacon/headers/[SLOT_NUMBER]\n```\n\n\n# How to Generate the Proofs with the Proof Generation library\nThis package allows you to generate withdrawal credential proofs, withdrawal proofs and balance update proofs. Please note that in order to run the sample commands, you must unzip the state files included in the data repo.  To generate the proofs using this library, run the following commands:\n\n## Build the Executable\n\n```bash\n$ cd generation\n$ go build\n$ cd ..\n```\n\n### Generate Validator Withdrawal Credential Proof\nHere is the command:\n```bash\n$ ./generation/generation \\\n    -command ValidatorFieldsProof \\\n    -oracleBlockHeaderFile [ORACLE_BLOCK_HEADER_FILE_PATH] \\\n    -stateFile [STATE_FILE_PATH] \\\n    -validatorIndex [VALIDATOR_INDEX] \\\n    -outputFile [OUTPUT_FILE_PATH] \\\n    -chainID [CHAIN_ID]\n```\nHere is a breakdown of the inputs here:\n- \u201cCommand\u201d aka the type of proof being generated\n- \u201coracleBlockHeaderFile\u201d is the path to the oracle block header file, that we are proving all of this against\n- \u201cstateFile\u201d is the consensus state from that slot, containing the validator information\n- \u201cvalidatorIndex\u201d is the index of the validator being proven inside state.validators\n- \u201coutputFile\u201d - setting this will write the proofs to a json file\n- \u201cchainID\u201d this parameter allows certain constants to be set depending on whether the proof is being generated for a goerli or mainnet state.\n\nHere is an example of running this command with the sample state/block files in the `/data` folder\n```bash\n./generation/generation \\\n  -command ValidatorFieldsProof \\\n  -oracleBlockHeaderFile \"./data/deneb_goerli_block_header_7431952.json\" \\\n  -stateFile \"./data/deneb_goerli_slot_7431952.json\" \\\n  -validatorIndex 302913 \\\n  -outputFile \"withdrawal_credential_proof_302913.json\" \\\n  -chainID 5\n```\n### Generate Withdrawal Proof\nHere is the command:\n```bash\n$ ./generation/generation \\\n  -command WithdrawalFieldsProof \\\n  -oracleBlockHeaderFile [ORACLE_BLOCK_HEADER_FILE_PATH] \\\n  -stateFile [STATE_FILE_PATH] \\\n  -validatorIndex [VALIDATOR_INDEX] \\\n  -outputFile [OUTPUT_FILE_PATH] \\\n  -chainID [CHAIN_ID] \\\n  -historicalSummariesIndex [HISTORICAL_SUMMARIES_INDEX] \\\n  -blockHeaderIndex [BLOCK_HEADER_INDEX] \\\n  -historicalSummaryStateFile [HISTORICAL_SUMMARY_STATE_FILE_PATH] \\\n  -blockHeaderFile [BLOCK_HEADER_FILE_PATH] \\\n  -blockBodyFile [BLOCK_BODY_FILE_PATH] \\\n  -withdrawalIndex [WITHDRAWAL_INDEX]\n```\nHere is an example of running this command with the sample state/block files in the `/data` folder\n```bash\n./generation/generation \\\n  -command WithdrawalFieldsProof \\\n  -oracleBlockHeaderFile ./data/deneb_goerli_block_header_7431952.json \\\n  -stateFile ./data/deneb_goerli_slot_7431952.json \\\n  -validatorIndex 627559 \\\n  -outputFile full_withdrawal_proof_627559.json \\\n  -chainID 5 \\\n  -historicalSummariesIndex 271 \\\n  -blockHeaderIndex 8191 \\\n  -historicalSummaryStateFile ./data/deneb_goerli_slot_7421952.json \\\n  -blockHeaderFile  data/deneb_goerli_block_header_7421951.json \\\n  -blockBodyFile data/deneb_goerli_block_7421951.json \\\n  -withdrawalIndex 0\n```\nHere is a breakdown of the inputs here:\n- \u201cCommand\u201d aka the type of proof being generated\n- \u201coracleBlockHeaderFile\u201d is the path to the oracle block header file, that we are proving all of this against\n- \u201cstateFile\u201d is the consensus state from that slot, containing the validator information\n- \u201cvalidatorIndex\u201d is the index of the validator being proven inside state.validators\n- \u201coutputFile\u201d - setting this will write the proofs to a json file\n- \u201cchainID\u201d this parameter allows certain constants to be set depending on whether the proof is being generated for a goerli or mainnet state.\n- \"historicalSummariesIndex\" Is the index in the historical summaries field of the oracle state (\u201cstateFile\u201d).  You can calculate this like this:\n  ```\n  (withdrawal_slot - FIRST_CAPELLA_SLOT) // SLOTS_PER_HISTORICAL_ROOT\n  ```\n  where `FIRST_CAPELLA_SLOT` on mainnet is 6209536 and `SLOTS_PER_HISTORICAL_ROOT` is 8192. Note that `withdrawal_slot` is the slot number of the block containing the withdrawal you want to prove.\n- \"blockHeaderIndex\" -  this is the blockheaderRoot's index within the historical summaries entry, which can be calculated like this:\n  ```\n  withdrawal_slot mod SLOTS_PER_HISTORICAL_ROOT\n  ```\n\n- \"historicalSummaryStateFile\" This is the beacon state at the slot such that:\nhistorical_summary_state.slot = `SLOTS_PER_HISTORICAL_ROOT` * (withdrawal_slot // `SLOTS_PER_HISTORICAL_ROOT`) + 1.\n\n- blockHeaderFile  - blockHeader from the withdrawal slot\n- blockBodyFile\" Is the block body file from the withdrawal slot\n- withdrawalIndex Is the index of the withdrawal within the block (between 0 and 15)\n\n\n### Generate a Balance Update Proof.  \n```bash\n$ ./generation/generation \\\n  - command BalanceUpdateProof \\\n  -oracleBlockHeaderFile [ORACLE_BLOCK_HEADER_FILE_PATH] \\\n  -stateFile [STATE_FILE_PATH] \\\n  -validatorIndex [VALIDATOR_INDEX] \\\n  -outputFile [OUTPUT_FILE_PATH] \\\n  -chainID [CHAIN_ID]\n```\nHere is a breakdown of the inputs here:\n- \u201cCommand\u201d aka the type of proof being generated\n- \u201coracleBlockHeaderFile\u201d is the path to the oracle block header file, that we are proving all of this against\n- \u201cstateFile\u201d is the consensus state from that slot, containing the validator information\n- \u201cvalidatorIndex\u201d is the index of the validator being proven inside state.validators\n- \u201coutputFile\u201d - setting this will write the proofs to a json file\n- \u201cchainID\u201d this parameter allows certain constants to be set depending on whether the proof is being generated for a goerli or mainnet state.\n\nHere is an example of running this command with the sample state/block files in the `/data` folder:\n```bash\n./generation/generation \\\n  -command BalanceUpdateProof \\\n  -oracleBlockHeaderFile \"./data/deneb_goerli_block_header_7431952.json\" \\\n  -stateFile \"./data/deneb_goerli_slot_7431952.json\" \\\n  -validatorIndex 302913 \\\n  -outputFile \"withdrawal_credential_proof_302913.json\" \\\n  -chainID 5\n```\n\n# Proof Generation Input Glossary\n- `oracleBlockHeaderFile` is the block header of the oracle block header root being used to make the proof\n- `stateFile` is the associated state file of the oracle block being used\n- `validatorIndex` is the index of the validator being proven for in the consensus layer\n- `outputFile` is the location where the generated proof will be written to\n- `chainID` is the chainID (either goerli = 5 or mainnet = 1) being generated for.\n- `historicalSummariesIndex` refer to *What Are Historical Summary Proofs?* secion.  This is the index of the historical summary we're gonna use to prove the withdrawal\n- `historicalSummaryStateFile` state file corresponding to the `state_summary_root` stored in the historical summary we're gonna use.\n- `blockHeaderIndex` index of the block header that contains the withdrawal being proven\n- `blockHeaderFile` file containing the block header that contains the withdrawal being proven\n- `blockBodyFile` file containing the block body that contains the withdrawal being proven\n- `withdrawalIndex` index of the withdrawal being proven within the block (there are 16 withdrawals per block).\n\n\n\n# What Are Historical Summary Proofs?\nEvery block contains 16 withdrawals and any given beacon state stores the last 8192 block roots.  Thus if a withdrawal was within the last 8192 blocks, we can prove any withdrawal against one of the block roots, and then prove that block root against the state root.  However, what happens when we need to prove something from further in the past than 8192 blocks? That is where historical summaries come in. \n\tEvery 8192 blocks, the state transition function takes a \u201csnapshot\u201d of the state.block_roots that are stored in the beacon state by taking the hash tree root of the state.block_roots, and adding that root to state.historical_summaries.  Then, state.block_roots is cleared and the next 8192 block_roots will be added to state.block_roots.  Thus, to prove an old withdrawal, we need to take the extra step of retrieving the state at the slot at which the snapshot that contains the root of the block when the withdrawal was included. Refer [here](https://github.com/ethereum/consensus-specs/blob/dev/specs/capella/beacon-chain.md#historicalsummary) for the beacon chain specs for historical summaries. \n\n\n\n\n\n", "release_dates": ["2024-02-29T17:56:28Z", "2024-02-29T14:39:01Z", "2024-02-10T23:52:51Z", "2024-02-10T20:30:45Z"]}, {"name": "eigensdk-go", "description": "Go SDK for building AVSs on Eigenlayer", "language": "Go", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "![Unit Tests](https://github.com/Layr-Labs/eigensdk-go/actions/workflows/unit-tests.yml/badge.svg)\n![Linter](https://github.com/Layr-Labs/eigensdk-go/actions/workflows/golangci-lint.yml/badge.svg)\n![Go Coverage](https://github.com/Layr-Labs/eigensdk-go/wiki/coverage.svg)\n\n<p align=\"center\"><b>\n\ud83d\udea7 Under active development. EIGENSDK-GO is rapidly being upgraded, features are being added, interfaces will have breaking changes \ud83d\udea7\n</b><p>\n\n**Do not use it in Production, testnet only.**\n\n## EigenSDK\nThis SDK provides a set of primitive Go modules for developing AVSs used in EigenLayer\n\n## Installation\n```\ngo get github.com/Layr-Labs/eigensdk-go\n```\n\n## Modules\nWe support following modules right now. \n> **_NOTE:_** All modules are in active development and interfaces might change. \n* [Logging](./logging/README.md)\n* [Signer](./signer/README.md)\n* [ChainIO](./chainio/README.md)\n* [Services](./services/README.md)\n\n## Development\nClone the repo\n```\ngit clone https://github.com/Layr-Labs/eigensdk-go.git\n```\nInitialize git submodules\n```\ngit submodule update --init\n```\n\nFollow the [contribution guidelines](CONTRIBUTING.md) to contribute to eigensdk-go\n\n## Security Bugs\nPlease report security vulnerabilities to security@eigenlabs.org. Do NOT report security bugs via Github Issues.", "release_dates": ["2024-02-14T20:20:18Z", "2024-01-31T22:12:17Z", "2023-12-21T16:59:03Z", "2023-11-02T21:34:09Z", "2023-10-31T17:45:40Z", "2023-10-17T20:30:26Z", "2023-10-06T21:34:00Z", "2023-10-05T19:40:19Z", "2023-10-02T23:20:33Z", "2023-09-29T15:12:58Z"]}, {"name": "foundry", "description": "Foundry is a blazing fast, portable and modular toolkit for Ethereum application development written in Rust.", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<img src=\".github/logo.png\" alt=\"Foundry logo\" align=\"right\" width=\"120\" />\n\n## Foundry\n\n![Github Actions][gha-badge] [![Telegram Chat][tg-badge]][tg-url] [![Telegram Support][tg-support-badge]][tg-support-url]\n\n[gha-badge]: https://img.shields.io/github/actions/workflow/status/foundry-rs/foundry/test.yml?branch=master\n[tg-badge]: https://img.shields.io/endpoint?color=neon&logo=telegram&label=chat&style=flat-square&url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Ffoundry_rs\n[tg-url]: https://t.me/foundry_rs\n[tg-support-badge]: https://img.shields.io/endpoint?color=neon&logo=telegram&label=support&style=flat-square&url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Ffoundry_support\n[tg-support-url]: https://t.me/foundry_support\n\n**Foundry is a blazing fast, portable and modular toolkit for Ethereum application development written in Rust.**\n\nFoundry consists of:\n\n-   [**Forge**](./crates/forge): Ethereum testing framework (like Truffle, Hardhat and DappTools).\n-   [**Cast**](./crates/cast): Swiss army knife for interacting with EVM smart contracts, sending transactions and getting chain data.\n-   [**Anvil**](./crates/anvil): Local Ethereum node, akin to Ganache, Hardhat Network.\n-   [**Chisel**](./crates/chisel): Fast, utilitarian, and verbose solidity REPL.\n\n**Need help getting started with Foundry? Read the [\ud83d\udcd6 Foundry Book][foundry-book] (WIP)!**\n\n![Demo](.github/demo.gif)\n\n## Installation\n\nSee the [installation guide](https://book.getfoundry.sh/getting-started/installation) in the book.\n\nIf you're experiencing any issues while installing, check out [Getting Help](#getting-help) and the [FAQ](https://book.getfoundry.sh/faq).\n\n## Forge\n\n### Features\n\n-   **Fast & flexible compilation pipeline**\n    -   Automatic Solidity compiler version detection & installation (under `~/.svm`)\n    -   **Incremental compilation & caching**: Only changed files are re-compiled\n    -   Parallel compilation\n    -   Non-standard directory structures support (e.g. [Hardhat repos](https://twitter.com/gakonst/status/1461289225337421829))\n-   **Tests are written in Solidity** (like in DappTools)\n-   **Fast fuzz testing** with shrinking of inputs & printing of counter-examples\n-   **Fast remote RPC forking mode**, leveraging Rust's async infrastructure like tokio\n-   **Flexible debug logging**\n    -   DappTools-style, using `DsTest`'s emitted logs\n    -   Hardhat-style, using the popular `console.sol` contract\n-   **Portable (5-10MB) & easy to install** without requiring Nix or any other package manager\n-   **Fast CI** with the [Foundry GitHub action][foundry-gha].\n\n### How Fast?\n\nForge is quite fast at both compiling (leveraging [ethers-solc][ethers-solc]) and testing.\n\nSee the benchmarks below. More benchmarks can be found in the [v0.2.0 announcement post][benchmark-post] and in the [Convex Shutdown Simulation][convex] repository.\n\n**Testing Benchmarks**\n\n| Project                            | Forge | DappTools | Speedup |\n| ---------------------------------- | ----- | --------- | ------- |\n| [transmissions11/solmate][solmate] | 2.8s  | 6m34s     | 140x    |\n| [reflexer-labs/geb][geb]           | 0.4s  | 23s       | 57.5x   |\n| [Rari-Capital/vaults][vaults]      | 0.28s | 6.5s      | 23x     |\n\n_Note: In the above benchmarks, compilation was always skipped_\n\n**Compilation Benchmarks**\n\n<img alt=\"Compilation benchmarks\" src=\".github/compilation-benchmark.png\" width=\"693px\" />\n\n**Takeaway: Forge compilation is consistently faster by a factor of 1.7-11.3x, depending on the amount of caching involved.**\n\n## Cast\n\nCast is a swiss army knife for interacting with Ethereum applications from the command line.\n\nMore documentation can be found in the [cast package](./crates/cast).\n\n## Configuration\n\n### Using `foundry.toml`\n\nFoundry is designed to be very configurable. You can configure Foundry using a file called [`foundry.toml`](./crates/config) in the root of your project, or any other parent directory. See [config package](./crates/config/README.md#all-options) for all available options.\n\nConfiguration can be arbitrarily namespaced by profiles. The default profile is named `default` (see [\"Default Profile\"](./crates/config/README.md#default-profile)).\n\nYou can select another profile using the `FOUNDRY_PROFILE` environment variable. You can also override parts of your configuration using `FOUNDRY_` or `DAPP_` prefixed environment variables, like `FOUNDRY_SRC`.\n\n`forge init` creates a basic, extendable `foundry.toml` file.\n\nTo see your current configuration, run `forge config`. To see only basic options (as set with `forge init`), run `forge config --basic`. This can be used to create a new `foundry.toml` file with `forge config --basic > foundry.toml`.\n\nBy default `forge config` shows the currently selected foundry profile and its values. It also accepts the same arguments as `forge build`.\n\n### DappTools Compatibility\n\nYou can re-use your `.dapprc` environment variables by running `source .dapprc` before using a Foundry tool.\n\n### Additional Configuration\n\nYou can find additional setup and configurations guides in the [Foundry Book][foundry-book]:\n\n-   [Setting up VSCode][vscode-setup]\n-   [Shell autocompletions][shell-setup]\n\n## Contributing\n\nSee our [contributing guidelines](./CONTRIBUTING.md).\n\n## Getting Help\n\nFirst, see if the answer to your question can be found in [book][foundry-book], or in the relevant crate.\n\nIf the answer is not there:\n\n-   Join the [support Telegram][tg-support-url] to get help, or\n-   Open a [discussion](https://github.com/foundry-rs/foundry/discussions/new) with your question, or\n-   Open an issue with [the bug](https://github.com/foundry-rs/foundry/issues/new)\n\nIf you want to contribute, or follow along with contributor discussion, you can use our [main telegram](https://t.me/foundry_rs) to chat with us about the development of Foundry!\n\n## Acknowledgements\n\n-   Foundry is a clean-room rewrite of the testing framework [DappTools](https://github.com/dapphub/dapptools). None of this would have been possible without the DappHub team's work over the years.\n-   [Matthias Seitz](https://twitter.com/mattsse_): Created [ethers-solc](https://github.com/gakonst/ethers-rs/tree/master/ethers-solc/) which is the backbone of our compilation pipeline, as well as countless contributions to ethers, in particular the `abigen` macros.\n-   [Rohit Narurkar](https://twitter.com/rohitnarurkar): Created the Rust Solidity version manager [svm-rs](https://github.com/roynalnaruto/svm-rs) which we use to auto-detect and manage multiple Solidity versions.\n-   [Brock Elmore](https://twitter.com/brockjelmore): For extending the VM's cheatcodes and implementing [structured call tracing](https://github.com/foundry-rs/foundry/pull/192), a critical feature for debugging smart contract calls.\n-   All the other [contributors](https://github.com/foundry-rs/foundry/graphs/contributors) to the [ethers-rs](https://github.com/gakonst/ethers-rs) & [foundry](https://github.com/foundry-rs/foundry) repositories and chatrooms.\n\n[foundry-book]: https://book.getfoundry.sh\n[foundry-gha]: https://github.com/foundry-rs/foundry-toolchain\n[ethers-solc]: https://github.com/gakonst/ethers-rs/tree/master/ethers-solc/\n[solmate]: https://github.com/transmissions11/solmate/\n[geb]: https://github.com/reflexer-labs/geb\n[vaults]: https://github.com/rari-capital/vaults\n[benchmark-post]: https://www.paradigm.xyz/2022/03/foundry-02#blazing-fast-compilation--testing\n[convex]: https://github.com/mds1/convex-shutdown-simulation\n[vscode-setup]: https://book.getfoundry.sh/config/vscode.html\n[shell-setup]: https://book.getfoundry.sh/config/shell-autocompletion.html\n", "release_dates": []}, {"name": "incredible-squaring-avs", "description": "Basic repo demoing a simple AVS middleware with full eigenlayer integration", "language": "Go", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Incredible Squaring AVS\n\n<b> Do not use it in Production, testnet only. </b>\n\nBasic repo demoing a simple AVS middleware with full eigenlayer integration. See this [video walkthrough](https://www.loom.com/share/50314b3ec0f34e2ba386d45724602d76?sid=9d68d8cb-d2d5-4123-bd06-776de2076de0).\n\n## Dependencies\n\nYou will need [foundry](https://book.getfoundry.sh/getting-started/installation) and [zap-pretty](https://github.com/maoueh/zap-pretty) to run the examples below.\n```\ncurl -L https://foundry.paradigm.xyz | bash\nfoundryup\ngo install github.com/maoueh/zap-pretty@latest\n```\n\n## Running via make\n\nThis simple session illustrates the basic flow of the AVS. The makefile commands are hardcoded for a single operator, but it's however easy to create new operator config files, and start more operators manually (see the actual commands that the makefile calls).\n\nStart anvil in a separate terminal:\n\n```bash\nmake start-anvil-chain-with-el-and-avs-deployed\n```\n\nThe above command starts a local anvil chain from a [saved state](./tests/anvil/avs-and-eigenlayer-deployed-anvil-state.json) with eigenlayer and incredible-squaring contracts already deployed (but no operator registered).\n\nStart the aggregator:\n\n```bash\nmake start-aggregator\n```\n\nRegister the operator with eigenlayer and incredible-squaring, and then start the process:\n\n```bash\nmake cli-setup-operator\nmake start-operator\n```\n\n## Running via docker compose\n\nWe wrote a [docker-compose.yml](./docker-compose.yml) file to run and test everything on a single machine. It will start an anvil instance, loading a [state](./tests/anvil/avs-and-eigenlayer-deployed-anvil-state.json) where the eigenlayer and incredible-squaring contracts are deployed, start the aggregator, and finally one operator, along with prometheus and grafana servers. The grafana server will be available at http://localhost:3000, with user and password both set to `admin`. We have created a simple [grafana dashboard](./grafana/provisioning/dashboards/AVSs/incredible_squaring.json) which can be used as a starting example and expanded to include AVS specific metrics. The eigen metrics should not be added to this dashboard as they will be exposed on the main eigenlayer dashboard provided by the eigenlayer-cli.\n\n## Avs Task Description\n\nThe architecture of the AVS contains:\n\n- [Eigenlayer core](https://github.com/Layr-Labs/eigenlayer-contracts/tree/master) contracts\n- AVS contracts\n  - [ServiceManager](contracts/src/IncredibleSquaringServiceManager.sol) which will eventually contain slashing logic but for M2 is just a placeholder.\n  - [TaskManager](contracts/src/IncredibleSquaringTaskManager.sol) which contains [task creation](contracts/src/IncredibleSquaringTaskManager.sol#L75) and [task response](contracts/src/IncredibleSquaringTaskManager.sol#L102) logic.\n  - The [challenge](contracts/src/IncredibleSquaringTaskManager.sol#L185) logic could be separated into its own contract, but we have decided to include it in the TaskManager for this simple task.\n  - Set of [registry contracts](https://github.com/Layr-Labs/eigenlayer-middleware) to manage operators opted in to this avs\n- Task Generator\n  - in a real world scenario, this could be a separate entity, but for this simple demo, the aggregator also acts as the task generator\n- Aggregator\n  - aggregates BLS signatures from operators and posts the aggregated response to the task manager\n  - For this simple demo, the aggregator is not an operator, and thus does not need to register with eigenlayer or the AVS contract. It's IP address is simply hardcoded into the operators' config.\n- Operators\n  - Square the number sent to the task manager by the task generator, sign it, and send it to the aggregator\n\n![](./diagrams/architecture.png)\n\n1. A task generator (in our case, same as the aggregator) publishes tasks once every regular interval (say 10 blocks, you are free to set your own interval) to the IncredibleSquaringTaskManager contract's [createNewTask](contracts/src/IncredibleSquaringTaskManager.sol#L78) function. Each task specifies an integer `numberToBeSquared` for which it wants the currently opted-in operators to determine its square `numberToBeSquared^2`. `createNewTask` also takes `quorumNumbers` and `quorumThresholdPercentage` which requests that each listed quorum (we only use quorumNumber 0 in incredible-squaring) needs to reach at least thresholdPercentage of operator signatures.\n\n2. A [registry](https://github.com/Layr-Labs/eigenlayer-middleware/blob/master/src/BLSRegistryCoordinatorWithIndices.sol) contract is deployed that allows any eigenlayer operator with at least 1 delegated [mockerc20](contracts/src/ERC20Mock.sol) token to opt-in to this AVS and also de-register from this AVS.\n\n3. [Operator] The operators who are currently opted-in with the AVS need to read the task number from the Task contract, compute its square, sign on that computed result (over the BN254 curve) and send their taskResponse and signature to the aggregator.\n\n4. [Aggregator] The aggregator collects the signatures from the operators and aggregates them using BLS aggregation. If any response passes the [quorumThresholdPercentage](contracts/src/IIncredibleSquaringTaskManager.sol#L36) set by the task generator when posting the task, the aggregator posts the aggregated response to the Task contract.\n\n5. If a response was sent within the [response window](contracts/src/IncredibleSquaringTaskManager.sol#L119), we enter the [Dispute resolution] period.\n   - [Off-chain] A challenge window is launched during which anyone can [raise a dispute](contracts/src/IncredibleSquaringTaskManager.sol#L171) in a DisputeResolution contract (in our case, this is the same as the TaskManager contract)\n   - [On-chain] The DisputeResolution contract resolves that a particular operator\u2019s response is not the correct response (that is, not the square of the integer specified in the task) or the opted-in operator didn\u2019t respond during the response window. If the dispute is resolved, the operator will be frozen in the Registration contract and the veto committee will decide whether to veto the freezing request or not.\n\nBelow is a more detailed uml diagram of the aggregator and operator processes:\n\n![](./diagrams/uml.png)\n\n## Avs node spec compliance\n\nEvery AVS node implementation is required to abide by the [Eigenlayer AVS Node Specification](https://eigen.nethermind.io/). We suggest reading through the whole spec, including the keys management section, but the hard requirements are currently only to:\n- implement the [AVS Node API](https://eigen.nethermind.io/docs/category/avs-node-api)\n- implement the [eigen prometheus metrics](https://eigen.nethermind.io/docs/category/metrics)\n\nIf you are using golang, you can use our [metrics](https://github.com/Layr-Labs/eigensdk-go/tree/master/metrics) and [nodeapi](https://github.com/Layr-Labs/eigensdk-go/tree/master/nodeapi) implementation in the [eigensdk](https://github.com/Layr-Labs/eigensdk-go), just like this repo does. Otherwise, you will have to implement it on your own.\n\n## StakeUpdates Cronjob\n\nAVS Registry contracts have a stale view of operator shares in the delegation manager contract. In order to update their stake table, they need to periodically call the [StakeRegistry.updateStakes()](https://github.com/Layr-Labs/eigenlayer-middleware/blob/f171a0812126bbb0bb6d44f53c622591a643e987/src/StakeRegistry.sol#L76) function. We are currently writing a cronjob binary to do this for you, will be open sourced soon!\n\n## Integration Tests\n\nSee the integration tests [README](tests/anvil/README.md) for more details.\n", "release_dates": []}, {"name": "nitro", "description": "Nitro goes vroom and fixes everything. This fork adds EigenDA support.", "language": "Go", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "<br />\n<p align=\"center\">\n  <a href=\"https://arbitrum.io/\">\n    <img src=\"https://arbitrum.io/assets/arbitrum/logo_color.png\" alt=\"Logo\" width=\"80\" height=\"80\">\n  </a>\n\n  <h3 align=\"center\">Arbitrum Nitro + EigenDA</h3>\n\n  <p align=\"center\">\n    <a href=\"https://developer.arbitrum.io/\"><strong>Next Generation Ethereum L2 Technology \u00bb</strong></a>\n    <br />\n  </p>\n</p>\n\nThis is a fork of Arbitrum Nitro developed by [AltLayer](https://altlayer.io/), in a technical partnership with EigenLabs.\n\n## About Arbitrum Nitro\n\n<img src=\"https://arbitrum.io/assets/arbitrum/logo_color.png\" alt=\"Logo\" width=\"80\" height=\"80\">\n\nNitro is the latest iteration of the Arbitrum technology. It is a fully integrated, complete\nlayer 2 optimistic rollup system, including fraud proofs, the sequencer, the token bridges, \nadvanced calldata compression, and more.\n\nSee the live docs-site [here](https://developer.arbitrum.io/) (or [here](https://github.com/OffchainLabs/arbitrum-docs) for markdown docs source.)\n\nSee [here](./audits) for security audit reports.\n\nThe Nitro stack is built on several innovations. At its core is a new prover, which can do Arbitrum\u2019s classic \ninteractive fraud proofs over WASM code. That means the L2 Arbitrum engine can be written and compiled using \nstandard languages and tools, replacing the custom-designed language and compiler used in previous Arbitrum\nversions. In normal execution, \nvalidators and nodes run the Nitro engine compiled to native code, switching to WASM if a fraud proof is needed. \nWe compile the core of Geth, the EVM engine that practically defines the Ethereum standard, right into Arbitrum. \nSo the previous custom-built EVM emulator is replaced by Geth, the most popular and well-supported Ethereum client.\n\nThe last piece of the stack is a slimmed-down version of our ArbOS component, rewritten in Go, which provides the \nrest of what\u2019s needed to run an L2 chain: things like cross-chain communication, and a new and improved batching \nand compression system to minimize L1 costs.\n\nEssentially, Nitro runs Geth at layer 2 on top of Ethereum, and can prove fraud over the core engine of Geth \ncompiled to WASM.\n\nArbitrum One successfully migrated from the Classic Arbitrum stack onto Nitro on 8/31/22. (See [state migration](https://developer.arbitrum.io/migration/state-migration) and [dapp migration](https://developer.arbitrum.io/migration/dapp_migration) for more info).\n\n## License\n\nWe currently have Nitro [licensed](./LICENSE) under a Business Source License, similar to our friends at Uniswap and Aave, with an \"Additional Use Grant\" to ensure that everyone can have full comfort using and running nodes on all public Arbitrum chains.\n\n## Contact\n\nDiscord - [Arbitrum](https://discord.com/invite/5KE54JwyTs)\n\nTwitter: [Arbitrum](https://twitter.com/arbitrum)\n\n\n", "release_dates": []}, {"name": "optimism", "description": "Optimism is Ethereum, scaled.", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "<div align=\"center\">\n  <br />\n  <br />\n  <a href=\"https://optimism.io\"><img alt=\"Optimism\" src=\"https://raw.githubusercontent.com/ethereum-optimism/brand-kit/main/assets/svg/OPTIMISM-R.svg\" width=600></a>\n  <br />\n  <h3><a href=\"https://optimism.io\">Optimism</a> is Ethereum, scaled.</h3>\n  <br />\n</div>\n\n## What is Optimism?\n\n[Optimism](https://www.optimism.io/) is a project dedicated to scaling Ethereum's technology and expanding its ability to coordinate people from across the world to build effective decentralized economies and governance systems. The [Optimism Collective](https://app.optimism.io/announcement) builds open-source software for running L2 blockchains and aims to address key governance and economic challenges in the wider cryptocurrency ecosystem. Optimism operates on the principle of **impact=profit**, the idea that individuals who positively impact the Collective should be proportionally rewarded with profit. **Change the incentives and you change the world.**\n\nIn this repository, you'll find numerous core components of the OP Stack, the decentralized software stack maintained by the Optimism Collective that powers Optimism and forms the backbone of blockchains like [OP Mainnet](https://explorer.optimism.io/) and [Base](https://base.org). Designed to be \"aggressively open source,\" the OP Stack encourages you to explore, modify, extend, and test the code as needed. Although not all elements of the OP Stack are contained here, many of its essential components can be found within this repository. By collaborating on free, open software and shared standards, the Optimism Collective aims to prevent siloed software development and rapidly accelerate the development of the Ethereum ecosystem. Come contribute, build the future, and redefine power, together.\n\n## Documentation\n\n- If you want to build on top of OP Mainnet, refer to the [Optimism Documentation](https://docs.optimism.io)\n- If you want to build your own OP Stack based blockchain, refer to the [OP Stack Guide](https://docs.optimism.io/stack/getting-started)\n- If you want to contribute to the OP Stack, check out the [Protocol Specs](./specs)\n\n## Community\n\nGeneral discussion happens most frequently on the [Optimism discord](https://discord.gg/optimism).\nGovernance discussion can also be found on the [Optimism Governance Forum](https://gov.optimism.io/).\n\n## Contributing\n\nRead through [CONTRIBUTING.md](./CONTRIBUTING.md) for a general overview of the contributing process for this repository.\nUse the [Developer Quick Start](./CONTRIBUTING.md#development-quick-start) to get your development environment set up to start working on the Optimism Monorepo.\nThen check out the list of [Good First Issues](https://github.com/ethereum-optimism/optimism/labels/D-good-first-issue) to find something fun to work on!\nTypo fixes are welcome; however, please create a single commit with all of the typo fixes & batch as many fixes together in a PR as possible. Spammy PRs will be closed.\n\n## Security Policy and Vulnerability Reporting\n\nPlease refer to the canonical [Security Policy](https://github.com/ethereum-optimism/.github/blob/master/SECURITY.md) document for detailed information about how to report vulnerabilities in this codebase.\nBounty hunters are encouraged to check out [the Optimism Immunefi bug bounty program](https://immunefi.com/bounty/optimism/).\nThe Optimism Immunefi program offers up to $2,000,042 for in-scope critical vulnerabilities.\n\n## Directory Structure\n\n<pre>\n\u251c\u2500\u2500 <a href=\"./docs\">docs</a>: A collection of documents including audits and post-mortems\n\u251c\u2500\u2500 <a href=\"./op-bindings\">op-bindings</a>: Go bindings for Bedrock smart contracts.\n\u251c\u2500\u2500 <a href=\"./op-batcher\">op-batcher</a>: L2-Batch Submitter, submits bundles of batches to L1\n\u251c\u2500\u2500 <a href=\"./op-bootnode\">op-bootnode</a>: Standalone op-node discovery bootnode\n\u251c\u2500\u2500 <a href=\"./op-chain-ops\">op-chain-ops</a>: State surgery utilities\n\u251c\u2500\u2500 <a href=\"./op-challenger\">op-challenger</a>: Dispute game challenge agent\n\u251c\u2500\u2500 <a href=\"./op-e2e\">op-e2e</a>: End-to-End testing of all bedrock components in Go\n\u251c\u2500\u2500 <a href=\"./op-heartbeat\">op-heartbeat</a>: Heartbeat monitor service\n\u251c\u2500\u2500 <a href=\"./op-node\">op-node</a>: rollup consensus-layer client\n\u251c\u2500\u2500 <a href=\"./op-preimage\">op-preimage</a>: Go bindings for Preimage Oracle\n\u251c\u2500\u2500 <a href=\"./op-program\">op-program</a>: Fault proof program\n\u251c\u2500\u2500 <a href=\"./op-proposer\">op-proposer</a>: L2-Output Submitter, submits proposals to L1\n\u251c\u2500\u2500 <a href=\"./op-service\">op-service</a>: Common codebase utilities\n\u251c\u2500\u2500 <a href=\"./op-wheel\">op-wheel</a>: Database utilities\n\u251c\u2500\u2500 <a href=\"./ops-bedrock\">ops-bedrock</a>: Bedrock devnet work\n\u251c\u2500\u2500 <a href=\"./packages\">packages</a>\n\u2502   \u251c\u2500\u2500 <a href=\"./packages/chain-mon\">chain-mon</a>: Chain monitoring services\n\u2502   \u251c\u2500\u2500 <a href=\"./packages/common-ts\">common-ts</a>: Common tools for building apps in TypeScript\n\u2502   \u251c\u2500\u2500 <a href=\"./packages/contracts-ts\">contracts-ts</a>: ABI and Address constants\n\u2502   \u251c\u2500\u2500 <a href=\"./packages/contracts-bedrock\">contracts-bedrock</a>: Bedrock smart contracts\n\u2502   \u251c\u2500\u2500 <a href=\"./packages/core-utils\">core-utils</a>: Low-level utilities that make building Optimism easier\n\u2502   \u2514\u2500\u2500 <a href=\"./packages/sdk\">sdk</a>: provides a set of tools for interacting with Optimism\n\u251c\u2500\u2500 <a href=\"./proxyd\">proxyd</a>: Configurable RPC request router and proxy\n\u2514\u2500\u2500 <a href=\"./specs\">specs</a>: Specs of the rollup starting at the Bedrock upgrade\n</pre>\n\n## Branching Model\n\n### Active Branches\n\n| Branch          | Status                                                                           |\n| --------------- | -------------------------------------------------------------------------------- |\n| [master](https://github.com/ethereum-optimism/optimism/tree/master/)                   | Accepts PRs from `develop` when intending to deploy to production.                  |\n| [develop](https://github.com/ethereum-optimism/optimism/tree/develop/)                 | Accepts PRs that are compatible with `master` OR from `release/X.X.X` branches.                    |\n| release/X.X.X                                                                          | Accepts PRs for all changes, particularly those not backwards compatible with `develop` and `master`. |\n\n### Overview\n\nThis repository generally follows [this Git branching model](https://nvie.com/posts/a-successful-git-branching-model/).\nPlease read the linked post if you're planning to make frequent PRs into this repository.\n\n### Production branch\n\nThe production branch is `master`.\nThe `master` branch contains the code for latest \"stable\" releases.\nUpdates from `master` **always** come from the `develop` branch.\n\n### Development branch\n\nThe primary development branch is [`develop`](https://github.com/ethereum-optimism/optimism/tree/develop/).\n`develop` contains the most up-to-date software that remains backwards compatible with the latest experimental [network deployments](https://community.optimism.io/docs/useful-tools/networks/).\nIf you're making a backwards compatible change, please direct your pull request towards `develop`.\n\n**Changes to contracts within `packages/contracts-bedrock/src` are usually NOT considered backwards compatible and SHOULD be made against a release candidate branch**.\nSome exceptions to this rule exist for cases in which we absolutely must deploy some new contract after a release candidate branch has already been fully deployed.\nIf you're changing or adding a contract and you're unsure about which branch to make a PR into, default to using the latest release candidate branch.\nSee below for info about release candidate branches.\n\n### Release candidate branches\n\nBranches marked `release/X.X.X` are **release candidate branches**.\nChanges that are not backwards compatible and all changes to contracts within `packages/contracts-bedrock/src` MUST be directed towards a release candidate branch.\nRelease candidates are merged into `develop` and then into `master` once they've been fully deployed.\nWe may sometimes have more than one active `release/X.X.X` branch if we're in the middle of a deployment.\nSee table in the **Active Branches** section above to find the right branch to target.\n\n## Releases\n\n### Changesets\n\nWe use [changesets](https://github.com/changesets/changesets) to mark packages for new releases.\nWhen merging commits to the `develop` branch you MUST include a changeset file if your change would require that a new version of a package be released.\n\nTo add a changeset, run the command `pnpm changeset` in the root of this monorepo.\nYou will be presented with a small prompt to select the packages to be released, the scope of the release (major, minor, or patch), and the reason for the release.\nComments within changeset files will be automatically included in the changelog of the package.\n\n### Triggering Releases\n\nReleases can be triggered using the following process:\n\n1. Create a PR that merges the `develop` branch into the `master` branch.\n2. Wait for the auto-generated `Version Packages` PR to be opened (may take several minutes).\n3. Change the base branch of the auto-generated `Version Packages` PR from `master` to `develop` and merge into `develop`.\n4. Create a second PR to merge the `develop` branch into the `master` branch.\n\nAfter merging the second PR into the `master` branch, packages will be automatically released to their respective locations according to the set of changeset files in the `develop` branch at the start of the process.\nPlease carry this process out exactly as listed to avoid `develop` and `master` falling out of sync.\n\n**NOTE**: PRs containing changeset files merged into `develop` during the release process can cause issues with changesets that can require manual intervention to fix.\nIt's strongly recommended to avoid merging PRs into develop during an active release.\n\n## License\n\nAll other files within this repository are licensed under the [MIT License](https://github.com/ethereum-optimism/optimism/blob/master/LICENSE) unless stated otherwise.\n", "release_dates": ["2024-03-03T19:54:48Z"]}, {"name": "orbit-setup-script", "description": null, "language": "TypeScript", "license": null, "readme": "# orbit-setup-script\n\nThese scripts will help you fund newly generated batch-poster and validator addresses, configure an Orbit chain, and deploy bridge contracts on both L2 and L3 chains.\n\n## Instructions\n\nOnce you\u2019ve downloaded both config files from the [Orbit Deployment UI](https://orbit.arbitrum.io/), please follow the steps below to complete local deployment of your Orbit chain. For more details and step-by-step instructions, check out the [documentation](https://developer.arbitrum.io/launch-orbit-chain/orbit-quickstart).\n\n1. Clone the\u00a0https://github.com/OffchainLabs/orbit-setup-script repository, and run\u00a0`yarn install`. Then, move\u00a0both the `nodeConfig.json`\u00a0and `orbitSetupScriptConfig.json` files into the\u00a0`config`\u00a0directory within the cloned repository\n2. Launch Docker, and in the base directory, run\u00a0`docker-compose up -d`. This will launch the node with a public RPC reachable at\u00a0http://localhost:8449/\u00a0 and a corresponding BlockScout explorer instance, viewable at\u00a0http://localhost:4000/\n3. Then, add the private key for the wallet you used to deploy the rollup contracts earlier in the following command, and run it:\u00a0`PRIVATE_KEY=\"0xYourPrivateKey\" L2_RPC_URL=\"<https://sepolia-rollup.arbitrum.io/rpc>\" L3_RPC_URL=\"<[http://localhost:8449](http://localhost:8449/)>\" yarn run setup`\n4. The Orbit chain is now up. You can find all information about the newly deployed chain in the `outputInfo.json` file which is created in the main directory of script folder\n5. Optionally, to track logs, run the following command within the base directory:\u00a0`docker-compose logs -f nitro`\n\n## AltLayer Instructions\n\n1. Prepare `orbitSetupScriptConfig.json`\n2. Execute bridge setup\n   ```\n   mkdir bridge-setup-result\n   mkdir -p bridge-setup-result/inputs\n   mkdir -p bridge-setup-result/outputs\n   docker run --rm \\\n      --volume $(pwd)/bridge-setup-result/inputs/orbitSetupScriptConfig.json:/app/config/orbitSetupScriptConfig.json \\\n      --volume $(pwd)/bridge-setup-result/outputs/outputInfo.json:/app/outputInfo.json \\\n      305587085711.dkr.ecr.us-west-2.amazonaws.com/orbit-setup:latest 2>&1 | tee bridge-setup-result/docker-run-log.txt\n   ```\n", "release_dates": []}, {"name": "tendermint", "description": "\u27c1 Tendermint Core (BFT Consensus) in Go", "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Tendermint\n\n![banner](docs/tendermint-core-image.jpg)\n\n[Byzantine-Fault Tolerant][bft] [State Machine Replication][smr]. Or\n[Blockchain], for short.\n\n[![Version][version-badge]][version-url]\n[![API Reference][api-badge]][api-url]\n[![Go version][go-badge]][go-url]\n[![Discord chat][discord-badge]][discord-url]\n[![License][license-badge]][license-url]\n[![Sourcegraph][sg-badge]][sg-url]\n\n| Branch | Tests                              | Linting                         |\n|--------|------------------------------------|---------------------------------|\n| main   | [![Tests][tests-badge]][tests-url] | [![Lint][lint-badge]][lint-url] |\n\nTendermint Core is a Byzantine Fault Tolerant (BFT) middleware that takes a\nstate transition machine - written in any programming language - and securely\nreplicates it on many machines.\n\nFor protocol details, refer to the [Tendermint Specification](./spec/README.md).\n\nFor detailed analysis of the consensus protocol, including safety and liveness\nproofs, read our paper, \"[The latest gossip on BFT\nconsensus](https://arxiv.org/abs/1807.04938)\".\n\n## Documentation\n\nComplete documentation can be found on the\n[website](https://docs.tendermint.com/).\n\n## Releases\n\nPlease do not depend on `main` as your production branch. Use\n[releases](https://github.com/tendermint/tendermint/releases) instead.\n\nTendermint has been in the production of private and public environments, most\nnotably the blockchains of the Cosmos Network. we haven't released v1.0 yet\nsince we are making breaking changes to the protocol and the APIs. See below for\nmore details about [versioning](#versioning).\n\nIn any case, if you intend to run Tendermint in production, we're happy to help.\nYou can contact us [over email](mailto:hello@interchain.io) or [join the\nchat](https://discord.gg/cosmosnetwork).\n\nMore on how releases are conducted can be found [here](./RELEASES.md).\n\n## Security\n\nTo report a security vulnerability, see our [bug bounty\nprogram](https://hackerone.com/cosmos). For examples of the kinds of bugs we're\nlooking for, see [our security policy](SECURITY.md).\n\nWe also maintain a dedicated mailing list for security updates. We will only\never use this mailing list to notify you of vulnerabilities and fixes in\nTendermint Core. You can subscribe [here](http://eepurl.com/gZ5hQD).\n\n## Minimum requirements\n\n| Requirement | Notes             |\n|-------------|-------------------|\n| Go version  | Go 1.18 or higher |\n\n### Install\n\nSee the [install instructions](./docs/introduction/install.md).\n\n### Quick Start\n\n- [Single node](./docs/introduction/quick-start.md)\n- [Local cluster using docker-compose](./docs/networks/docker-compose.md)\n- [Remote cluster using Terraform and Ansible](./docs/networks/terraform-and-ansible.md)\n\n## Contributing\n\nPlease abide by the [Code of Conduct](CODE_OF_CONDUCT.md) in all interactions.\n\nBefore contributing to the project, please take a look at the [contributing\nguidelines](CONTRIBUTING.md) and the [style guide](STYLE_GUIDE.md). You may also\nfind it helpful to read the [specifications](./spec/README.md), and familiarize\nyourself with our [Architectural Decision Records\n(ADRs)](./docs/architecture/README.md) and\n[Request For Comments (RFCs)](./docs/rfc/README.md).\n\n## Versioning\n\n### Semantic Versioning\n\nTendermint uses [Semantic Versioning](http://semver.org/) to determine when and\nhow the version changes. According to SemVer, anything in the public API can\nchange at any time before version 1.0.0\n\nTo provide some stability to users of 0.X.X versions of Tendermint, the MINOR\nversion is used to signal breaking changes across Tendermint's API. This API\nincludes all publicly exposed types, functions, and methods in non-internal Go\npackages as well as the types and methods accessible via the Tendermint RPC\ninterface.\n\nBreaking changes to these public APIs will be documented in the CHANGELOG.\n\n### Upgrades\n\nIn an effort to avoid accumulating technical debt prior to 1.0.0, we do not\nguarantee that breaking changes (ie. bumps in the MINOR version) will work with\nexisting Tendermint blockchains. In these cases you will have to start a new\nblockchain, or write something custom to get the old data into the new chain.\nHowever, any bump in the PATCH version should be compatible with existing\nblockchain histories.\n\nFor more information on upgrading, see [UPGRADING.md](./UPGRADING.md).\n\n### Supported Versions\n\nBecause we are a small core team, we have limited capacity to ship patch\nupdates, including security updates. Consequently, we strongly recommend keeping\nTendermint up-to-date. Upgrading instructions can be found in\n[UPGRADING.md](./UPGRADING.md).\n\nCurrently supported versions include:\n\n- v0.34.x\n- v0.37.x (release candidate)\n\n## Resources\n\n### Libraries\n\n- [Cosmos SDK](http://github.com/cosmos/cosmos-sdk); A framework for building\n  applications in Golang\n- [Tendermint in Rust](https://github.com/informalsystems/tendermint-rs)\n- [ABCI Tower](https://github.com/penumbra-zone/tower-abci)\n\n### Applications\n\n- [Cosmos Hub](https://hub.cosmos.network/)\n- [Terra](https://www.terra.money/)\n- [Celestia](https://celestia.org/)\n- [Anoma](https://anoma.network/)\n- [Vocdoni](https://docs.vocdoni.io/)\n\n### Research\n\n- [The latest gossip on BFT consensus](https://arxiv.org/abs/1807.04938)\n- [Master's Thesis on Tendermint](https://atrium.lib.uoguelph.ca/xmlui/handle/10214/9769)\n- [Original Whitepaper: \"Tendermint: Consensus Without Mining\"](https://tendermint.com/static/docs/tendermint.pdf)\n- [Tendermint Core Blog](https://medium.com/tendermint/tagged/tendermint-core)\n- [Cosmos Blog](https://blog.cosmos.network/tendermint/home)\n\n## Join us\n\nTendermint Core is maintained by [Interchain GmbH](https://interchain.io).\nIf you'd like to work full-time on Tendermint Core,\n[we're hiring](https://interchain-gmbh.breezy.hr/)!\n\nFunding for Tendermint Core development comes primarily from the\n[Interchain Foundation](https://interchain.io), a Swiss non-profit. The\nTendermint trademark is owned by [Tendermint Inc.](https://tendermint.com), the\nfor-profit entity that also maintains [tendermint.com](https://tendermint.com).\n\n[bft]: https://en.wikipedia.org/wiki/Byzantine_fault_tolerance\n[smr]: https://en.wikipedia.org/wiki/State_machine_replication\n[Blockchain]: https://en.wikipedia.org/wiki/Blockchain\n[version-badge]: https://img.shields.io/github/v/release/tendermint/tendermint.svg\n[version-url]: https://github.com/tendermint/tendermint/releases/latest\n[api-badge]: https://camo.githubusercontent.com/915b7be44ada53c290eb157634330494ebe3e30a/68747470733a2f2f676f646f632e6f72672f6769746875622e636f6d2f676f6c616e672f6764646f3f7374617475732e737667\n[api-url]: https://pkg.go.dev/github.com/tendermint/tendermint\n[go-badge]: https://img.shields.io/badge/go-1.18-blue.svg\n[go-url]: https://github.com/moovweb/gvm\n[discord-badge]: https://img.shields.io/discord/669268347736686612.svg\n[discord-url]: https://discord.gg/cosmosnetwork\n[license-badge]: https://img.shields.io/github/license/tendermint/tendermint.svg\n[license-url]: https://github.com/tendermint/tendermint/blob/main/LICENSE\n[sg-badge]: https://sourcegraph.com/github.com/tendermint/tendermint/-/badge.svg\n[sg-url]: https://sourcegraph.com/github.com/tendermint/tendermint?badge\n[tests-url]: https://github.com/tendermint/tendermint/actions/workflows/tests.yml\n[tests-badge]: https://github.com/tendermint/tendermint/actions/workflows/tests.yml/badge.svg?branch=main\n[lint-badge]: https://github.com/tendermint/tendermint/actions/workflows/lint.yml/badge.svg\n[lint-url]: https://github.com/tendermint/tendermint/actions/workflows/lint.yml\n", "release_dates": []}]