[{"name": ".github", "description": null, "language": null, "license": null, "readme": null, "release_dates": []}, {"name": "actions-dev-kit", "description": null, "language": "HTML", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Action Development Kit (ADK)\n\n## Purpose\n\nAmazon CodeCatalyst provides software development teams one place to plan work, collaborate on code, and build, test, and deploy applications with continuous integration/continuous delivery (CI/CD) tools. For more information, see [What is Amazon CodeCatalyst?](https://docs.aws.amazon.com/codecatalyst/latest/userguide/welcome.html)\n\nWith the CodeCatalyst Action Development Kit (ADK), you can build, test, and publish actions to the CodeCatalyst actions catalog, where other users can add them to workflows. This ADK provides tooling and support to help you develop actions using libraries and frameworks.\n\nIn CodeCatalyst, an action is the main building block of a workflow. The actions you author define a logical unit of work to perform during a workflow run. By creating actions and workflows, you can automate procedures that describe how to build, test, and deploy your code as part of a continuous integration and continuous delivery (CI/CD) system. For more information, see [Working with actions](https://docs.aws.amazon.com/codecatalyst/latest/userguide/workflows-actions.html).\n\n\n## ADK Components\n\nThere are two components of the ADK:\n\n1. ADK software development kit (SDK)\n\nA set of library interfaces you can use to interact with action matadata and CodeCatalyst resources, including actions, workflows, secrets, logs, input variables, output variables, artifacts, and reports.\n\n    \n#### Sample Usage\n\n```\n// @ts-ignore\nimport * as core from '@aws/codecatalyst-adk-core';\n// @ts-ignore\nimport * as project from '@aws/codecatalyst-project';\n// @ts-ignore\nimport { RunSummaryLevel, RunSummaries } from '@aws/codecatalyst-run-summaries';\n// @ts-ignore\nimport * as space from '@aws/codecatalyst-space';\n\nconst destinationBucket = core.getInput('DestinationBucketName')\n    # => Maps to the destination bucket configuration in CodeCatalyst workflow definition\nconst srcDir = core.getInput('SourcePath')\n    # => Maps to the src dir configuration in CodeCatalyst workflow definition\nconsole.log(\"Running action S3 Publish Action\")\nlet cmd = `aws s3 sync ${srcDir} s3://${destinationBucket}/${space.getSpace().name}/${project.getProject().name}/`\nconst cmdOutput = core.command(cmd)\nconsole.log(cmdOutput.stdout)\n\nif (cmdOutput.code != 0) {\n  core.setFailed(cmdOutput.stderr)\n} else {\n  core.setOutputVariable(\"Files\", cmdOutput.stdOut)\n}\n```\n\n2. ADK command line interface (CLI)\n\nTool to interact with a set of commands you can use to create, validate, and test actions.\n\n#### Sample Usage\n\n```\n>> adk init --lang typescript --space <CODECATALYST-SPACE-NAME> --proj <CODECATALYST-PROJECT-NAME> --repo <CODECATALYST-REPO-NAME> --action <ACTION-NAME>\n...\n\n>> adk bootstrap\n...\n\n>> adk validate\nvalidating...\nMissing README file...\naction.yml parsing failed...\n```\n\nThe following list contains the ADK CLI commands and information about how to use each command:\n  * init \u2013 Initializes the ADK project locally and produces required configuration files with specific language support.\n  * bootstrap \u2013 Bootstraps CodeCatalyst action code by reading the action definition file. The ADK SDK is used to develop actions.\n  * validate \u2013 Validates the action definition and README file.\n  * version \u2013 Returns the current version of ADK.\n  * help \u2013 Shows the current set of commands.\n\n  \n\n## Installation Guide\n\n### Prerequisites\n1. Download the [latest version of npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm). It is best to use a Node version manager like [nvm](https://github.com/nvm-sh/nvm) to install Node.js and npm.\n2. Run the following Lerna command: `npm install -g lerna`.\n    * [Lerna](https://lerna.js.org/) is a fast, modern build system for managing and publishing multiple JavaScript/TypeScript packages from the same repository.\n3. Run the following command to install yarn: ``npm install --global yarn``\n    * [Yarn](https://yarnpkg.com/) is a package manager that doubles down as project manager. You can you work on one-shot projects or large monorepos, as a hobbyist or an enterprise user.\n\n\n### Install ADK CLI\n\n1. Run the following npm command to install the ADK CLI package globally:\n   \n   * `npm install -g @aws/codecatalyst-adk`\n\n2. Validate that ADK is running with the following command: `adk help`\n\n\n## Development\n\n### Build\nRun the following build command:\n```\n$ ./build.sh\n```\n\n### Testing\nRun the following test command:\n\n```\n$ yarn run test-all\n```\n\n### Contribute\n\nYou can contribute to the ADK by completing development on a feature branch and creating a pull request:\n\n* Create a branch from **main** and name it **feature-*** branch (e.g. **feature-add-init-command**). Creating the feature branch creates CI validation workflow against the feature- branch.\n* Update the code in your new branch **feature-***.\n* Once you're done with feature development, create a pull request from source **feature-*** branch to destination main branch. This triggers a CI workflow in your**feature-*** branch. Update the pull request with the workflow run in the description section of the pull request.\n* Add reviewers from the reviewer section. Reviewers can be anyone within the organization, but at least one must be a developer from the AEF team.\n* Once you have all the approvals in your pull request, merge the pull request from the UI by choosing squash (not fast forward merge) and reducing the number of commits to just one from the feature branch. This makes rollbacks easy if you have one commit per feature branch. If you have large amount of changes in your pull request, it's best to rethink your development strategy to iteratively develop and push code.\n* On merge, release workflows within the ADK repository will kick-off. This should automatically bump the version of the ADK package for consumption.\n\n\n### Test Coverage Expectations\n\n- branches: 90%\n- statements: 90%\n- functions: 90%\n- lines: 90%\n\n## ChangeLog\n\n[Changelog](./CHANGELOG.md)\n\n## Security\nSee CONTRIBUTING for more information.\n\n## License\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": []}, {"name": "amazon-chime-sdk-android", "description": "An Android client library for integrating multi-party communications powered by the Amazon Chime service.", "language": "Kotlin", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon Chime SDK for Android\n[Amazon Chime SDK Project Board](https://aws.github.io/amazon-chime-sdk-js/modules/projectboard.html)\n\n> Note: If building with the SDK source code, the `development` branch contains bleeding-edge changes that may not build with the publically available Chime media library or may not be as stable as [public releases](https://github.com/aws/amazon-chime-sdk-android/releases).\n\n## Build video calling, audio calling, and screen sharing applications powered by Amazon Chime.\n\nThe Amazon Chime SDK for Android makes it easy to add collaborative audio calling,\nvideo calling, and screen share viewing features to Android applications by\nusing the same infrastructure services that power meetings on the Amazon\nChime service.\n\nThis Amazon Chime SDK for Android works by connecting to meeting session\nresources that you have created in your AWS account. The SDK has everything\nyou need to build custom calling and collaboration experiences in your\nAndroid application, including methods to: configure meeting sessions, list\nand select audio devices, switch video devices, start and stop screen share\nviewing, receive callbacks when media events occur such as volume changes,\nand manage meeting features such as audio mute and video tile bindings.\n\nWe also have an [Amazon Chime SDK Project Board](https://aws.github.io/amazon-chime-sdk-js/modules/projectboard.html) where you can find community requests and their statuses.\n\nTo get started, see the following resources:\n\n* [Amazon Chime](https://aws.amazon.com/chime)\n* [Amazon Chime Developer Guide](https://docs.aws.amazon.com/chime/latest/dg/what-is-chime.html)\n* [Amazon Chime SDK API Reference](http://docs.aws.amazon.com/chime/latest/APIReference/Welcome.html)\n* [SDK Documentation](https://aws.github.io/amazon-chime-sdk-android/amazon-chime-sdk/)\n\nAnd review the following guides:\n\n* [API Overview](guides/api_overview.md)\n* [Getting Started](guides/getting_started.md)\n* [Frequently Asked Questions (FAQ)](#frequently-asked-questions)\n* [Custom Video Sources, Processors, and Sinks](guides/custom_video.md)\n* [Video Pagination with Active Speaker-Based Policy](guides/video_pagination.md)\n* [Content Share](guides/content_share.md)\n* [Meeting Events](guides/meeting_events.md)\n* [Event Ingestion](guides/event_ingestion.md)\n* [Configuring Remote Video Subscription](guides/configuring_remote_video_subscription.md)\n* [Background Video Filters](guides/background_video_filters.md)\n\n## Setup\n\n> NOTE: If you just want to run demo application, skip to [Running the demo app](#running-the-demo-app)\n\nThe Mobile SDKs for Android could be downloaded from the Maven Central repository, by integrated into your Android project's Gradle files, or you can be directly embedded via .aar files.\n\nFor the purpose of setup, your project's root folder will be referred to as `root`.\n\n### From Maven\n\nTo obtain the dependencies from Maven, add the dependencies to your app's (module-level) `build.gradle`.\n\nUpdate `build.gradle` in `root/app` and add the following under `dependencies`:\n\n```\ndependencies {\n    implementation 'software.aws.chimesdk:amazon-chime-sdk-media:$MEDIA_VERSION'\n    implementation 'software.aws.chimesdk:amazon-chime-sdk:$SDK_VERSION'\n}\n```\nThe version numbers could be obtained from the latest [release](https://github.com/aws/amazon-chime-sdk-android/releases/latest).\n\nIf you don't need video and content share functionality, or software video codec support, you could use `amazon-chime-sdk-media-no-video-codecs` instead to reduce size. Exclude the usage of `amazon-chime-sdk-media` module and/or `amazon-chime-sdk-machine-learning` module from the transitive dependency of `amazon-chime-sdk`:\n\n```\ndependencies {\n    implementation 'software.aws.chimesdk:amazon-chime-sdk-media-no-video-codecs:$MEDIA_VERSION'\n    implementation ('software.aws.chimesdk:amazon-chime-sdk:$MEDIA_VERSION') {\n        exclude module: 'amazon-chime-sdk-media'\n        exclude module: 'amazon-chime-sdk-machine-learning'\n    }\n}\n```\n\n### Manually download SDK binaries\nTo include the SDK binaries in your own project, follow these steps.\n\n#### 1. Download binaries\n\nDownload `amazon-chime-sdk` and `amazon-chime-sdk-media` binaries from the latest [release](https://github.com/aws/amazon-chime-sdk-android/releases/latest).\n\nIf you like to use more machine learning features, e.g. background blur/replacement, also download the `amazon-chime-sdk-machine-learning` binary from the latest [release](https://github.com/aws/amazon-chime-sdk-android/releases/latest). Otherwise, you can ignore all references to `amazon-chime-sdk-machine-learning` in the instructions below.\n\nIf you don't need video and content share functionality, or software video codec support, you could use `amazon-chime-sdk-media-no-video-codecs` instead of `amazon-chime-sdk-media` to exclude software video codecs support and reduce size. If you do, you can treat all references to `amazon-chime-sdk-media` as `amazon-chime-sdk-media-no-video-codecs` in the instructions below. \n\n**NOTE: We do not support mixing and matching binaries from different releases.**\n\nUnzip them and copy the aar files to `root/app/libs`\n\n#### 2. Update gradle files\n\nUpdate `build.gradle` in `root` by adding the following under `repositories` in `allprojects`:\n\n```\nallprojects {\n   repositories {\n      jcenter()\n      flatDir {\n        dirs 'libs'\n      }\n   }\n}\n```\n\nUpdate `build.gradle` in `root/app` and add the following under `dependencies`:\n\n```\nimplementation(name: 'amazon-chime-sdk', ext: 'aar')\nimplementation(name: 'amazon-chime-sdk-media', ext: 'aar')\n```\n\nIf you are using `amazon-chime-sdk-machine-learning` library, then add below statement as well under `dependencies`:\n\n```\nimplementation(name: 'amazon-chime-sdk-machine-learning', ext: 'aar')\n```\n\nUpdate `build.gradle` in `root/app` under `compileOptions`:\n\n```\ncompileOptions {\n    sourceCompatibility JavaVersion.VERSION_1_8\n    targetCompatibility JavaVersion.VERSION_1_8\n}\n```\n\n## Running the demo app\n\n> NOTE: This is just to run demo application and use SDK as code instead of aar library.\n\nTo run the demo application, follow these steps.\n\n> NOTE: Please make sure that you are running on ARM supported devices (real devices) or simulator with arm supported. We do not support x86 currently, so simulators with x86 will not work.\n\n### 1. Deploy serverless demo\n\nDeploy the serverless demo from [amazon-chime-sdk-js](https://github.com/aws/amazon-chime-sdk-js), which returns `https://xxxxx.xxxxx.xxx.com/Prod/`\n\nProvide `https://xxxxx.xxxxx.xxx.com/Prod/` for mobile demo app.\n\n### 2. Download binary\n\nDownload `amazon-chime-sdk-media` binary from the latest [release](https://github.com/aws/amazon-chime-sdk-android/releases/latest).\n\nDownload `amazon-chime-sdk-machine-learning` binary for machine learning features.\n\nUnzip and copy the aar files to `amazon-chime-sdk-android/amazon-chime-sdk/libs`\n\n### 3. Update demo app\n\nUpdate `test_url` in `strings.xml` at the path `amazon-chime-sdk-android/app/src/main/res/values`\nwith the URL of the serverless demo deployed in Step 1.\n\n> NOTE: use `https://xxxxx.xxxxx.xxx.com/Prod/`\n\n## Reporting a suspected vulnerability\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our\n[vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public GitHub issue.\n\n## Usage\n  - [Starting a session](#starting-a-session)\n  - [Device](#device)\n  - [Audio](#audio)\n  - [Video](#video)\n  - [Screen and content share](#screen-and-content-share)\n  - [Metrics](#metrics)\n  - [Data Message](#data-message)\n  - [Stopping a session](#stopping-a-session)\n  - [Amazon Voice Focus](#amazon-voice-focus)\n  - [Custom Video Source](#custom-video-source)\n  - [Background Blur and Replacement](#background-blur-and-replacement)\n\n### Starting a session\n\n#### Use case 1. Start a session.\n\nYou need to start the meeting session to start sending and receiving audio. Make sure that the user has granted audio permission first.\n\nStart a session with default configurations:\n```kotlin\nmeetingSession.audioVideo.start()\n```\n\nStart a session with custom configurations:\n\n```kotlin\nmeetingSession.audioVideo.start(audioVideoConfiguration)\n```\n\nThere are 4 configurations available in `audioVideoConfiguration`:\n- `audioMode`\n- `audioStreamType`\n- `audioRecordingPresetOverride`\n- `enableAudioRedundancy`\n\nAudioMode: The default audio format is Stereo/48KHz i.e Stereo Audio with 48KHz sampling rate (Stereo48K). Other supported audio formats include Mono/48KHz (Mono48K) or Mono/16KHz (Mono16K). You can specify a non-default audio mode in `AudioVideoConfiguration`, and then start the meeting session.\n\nAudioStreamType: The default value is ```VoiceCall```. The available options are ```VoiceCall``` and ```Music```, they are equivalent of `STREAM_VOICE_CALL` and `STREAM_MUSIC` respectively in [AudioManager](https://developer.android.com/reference/android/media/AudioManager). This configuration is for addressing the audio volume [issue](https://github.com/aws/amazon-chime-sdk-android/issues/296) on Oculus Quest 2. If you don't know what it is, you probably don't need to worry about it. For more information, please refer to Android documentation: [STREAM_VOICE_CALL](https://developer.android.com/reference/android/media/AudioManager#STREAM_VOICE_CALL), [STREAM_MUSIC](https://developer.android.com/reference/android/media/AudioManager#STREAM_MUSIC).\n\n> Note: Even though there are more available stream options in Android, currently only *STREAM_VOICE_CALL* and *STREAM_MUSIC* are supported in Amazon Chime SDK for Android.\n\nAudioRecordingPresetOverride: The default value is ```None```. The available options are ```None```, ```Generic```, ```Camcorder```, ```VoiceRecognition``` and ```VoiceCommunication```. These are equivalent to the options\nmentioned [here](https://android.googlesource.com/platform/frameworks/wilhelm/+/master/include/SLES/OpenSLES_AndroidConfiguration.h) under *Android AudioRecorder configuration*.\n\nEnableAudioRedundancy: The default value is true. When enabled, the SDK will send redundant audio data on detecting packet loss to help reduce its effects on audio quality. More details can be found in the\n*Redundant Audio* section.\n\n#### Use case 2. Add an observer to receive audio and video session life cycle events.\n\n> Note: To avoid missing any events, add an observer before the session starts. You can remove the observer by calling meetingSession.audioVideo.removeAudioVideoObserver(observer).\n\n```kotlin\nval observer = object : AudioVideoObserver {\n    override fun onAudioSessionStartedConnecting(reconnecting: Boolean) {\n        if (reconnecting) {\n            // e.g. the network connection is dropped\n        }\n    }\n    override fun onAudioSessionStarted(reconnecting: Boolean) {\n        // Meeting session starts.\n        // Can use realtime, devices APIs.\n    }\n    override fun onAudioSessionDropped(reconnecting: Boolean) {}\n    override fun onAudioSessionStopped(sessionStatus: MeetingSessionStatus) {\n        // See the \"Stopping a session\" section for details.\n    }\n    override fun onAudioSessionCancelledReconnect() {}\n    override fun onConnectionRecovered() {}\n    override fun onConnectionBecamePoor() {}\n    override fun onVideoSessionStartedConnecting() {}\n    override fun onVideoSessionStarted(sessionStatus: MeetingSessionStatus) {\n        // Video session starts.\n        // Can use video APIs.\n    }\n    override fun onVideoSessionStopped(sessionStatus: MeetingSessionStatus) {}\n}\n\nmeetingSession.audioVideo.addAudioVideoObserver(observer)\n```\n\n### Device\n\n#### Use case 3. List audio devices.\n\nList available audio devices for the meeting.\n\n```kotlin\nval audioDevices = meetingSession.audioVideo.listAudioDevices()\n\n// A list of MediaDevice objects\naudioDevices.forEach {\n    logger.info(TAG, \"Device type: ${it.type}, label: ${it.label}\")\n}\n```\n\n#### Use case 4. Choose an audio device by passing a `MediaDevice` object.\n\n> Note: You should call chooseAudioDevice after the session started, or it'll be a no-op. You should also call chooseAudioDevice with one of the devices returned from listAudioDevices.\n\n```kotlin\n// Filter out OTHER type which is currently not supported for selection\nval audioDevices = meetingSession.audioVideo.listAudioDevices().filter {\n    it.type != MediaDeviceType.OTHER)\n}\nval device = /* An item from audioDevices */\nmeetingSession.audioVideo.chooseAudioDevice(device)\n```\n\n#### Use case 5. Switch cameras.\n\n> Note: switchCamera() is a no-op if you are using a custom camera capture source. Please refer to the [Custom Video](https://github.com/aws/amazon-chime-sdk-android/blob/master/guides/custom_video.md#implementing-a-custom-video-source-and-transmitting) for more details.\n\n\nSwitch between the front or back camera on the device, if available.\n\n```kotlin\nmeetingSession.audioVideo.switchCamera()\n```\n\n#### Use case 6. Add an observer to receive the updated device list.\n\nAdd a `DeviceChangeObserver` to receive a callback when a new audio device connects or when an audio device disconnects. `onAudioDeviceChanged` includes an updated device list.\n\n```kotlin\nval observer = object: DeviceChangeObserver {\n    override fun onAudioDeviceChanged(freshAudioDeviceList: List<MediaDevice>) {\n        // A list of updated MediaDevice objects\n        freshAudioDeviceList.forEach {\n            logger.info(TAG, \"Device type: ${it.type}, label: ${it.label}\")\n        }\n    }\n}\n\nmeetingSession.audioVideo.addDeviceChangeObserver(observer)\n```\n\n#### Use case 7. Get currently selected audio device.\n\n> Note: `getActiveAudioDevice` API requires API level 24 or higher.\n\n```kotlin\nif (Build.VERSION.SDK_INT >= Build.VERSION_CODES.N) {\n    val activeAudioDevice = meetingSession.audioVideo.getActiveAudioDevice()\n}\n```\n\nFor lower API levels, builders can achieve the same by tracking the selected device with the following logic:\n\n```kotlin\nvar activeAudioDevice: MediaDevice? = null\noverride fun onAudioDeviceChanged(freshAudioDeviceList: List<MediaDevice>) {\n    val device = /* An item from freshAudioDeviceList */\n    meetingSession.audioVideo.chooseAudioDevice(device)\n    activeAudioDevice = device // Update current device\n}\n```\n\n### Audio\n\n#### Use case 8. Choose the audio configuration.\n\n> When joining a meeting, each configuration will have a default if not explicitly specified when starting the audio session.\n> \n> Supported AudioMode options: *Mono/16KHz*, *Mono/48KHz*, and *Stereo/48KHz*. Default is *Stereo/48KHz*.\n> Supported AudioStreamType options: *VoiceCall* and *Music*. Default is *VoiceCall*\n> Supported AudioRecordingPresetOverride options: *None*, *Generic*, *Camcorder*, *VoiceRecognition* and *VoiceCommunication*. Default is *None*.\n> Supported enableAudioRedundancy options: *true* and *false*. Default is *true*.\n\n```kotlin\nmeetingSession.audioVideo.start() // starts the audio video session with defaults mentioned above\n\nmeetingSession.audioVideo.start(audioVideoConfiguration) // starts the audio video session with the specified [AudioVideoConfiguration]\n```\n\n> Note: So far, you've added observers to receive device and session lifecycle events. In the following use cases, you'll use the real-time API methods to send and receive volume indicators and control mute state.\n\n#### Use case 9. Mute and unmute an audio input.\n\n```kotlin\nval muted = meetingSession.audioVideo.realtimeLocalMute() // returns true if muted, false if failed\n\nval unmuted = meetingSession.audioVideo.realtimeLocalUnmute() // returns true if unmuted, false if failed\n```\n\n#### Use case 10. Add an observer to receive realtime events such as volume changes/signal change/muted status attendees.\n\nYou can use this to build real-time indicators UI and get them updated for changes delivered by the array.\n\n\n> Note: These callbacks will only include the delta from the previous callback.\n\n```kotlin\nval observer = object : RealtimeObserver {\n    override fun onVolumeChanged(volumeUpdates: Array<VolumeUpdate>) {\n        volumeUpdates.forEach { (attendeeInfo, volumeLevel) ->\n            logger.info(TAG, \"${attendeeInfo.attendeeId}'s volume changed: \" +\n                $volumeLevel // Muted, NotSpeaking, Low, Medium, High\n            )\n        }\n    }\n\n    override fun onSignalStrengthChanged(signalUpdates: Array<SignalUpdate>) {\n        signalUpdates.forEach { (attendeeInfo, signalStrength) ->\n            logger.info(TAG, \"${attendeeInfo.attendeeId}'s signal strength changed: \" +\n                $signalStrength // None, Low, High\n            )\n        }\n    }\n\n    override fun onAttendeesJoined(attendeeInfo: Array<AttendeeInfo>) {\n        attendeeInfo.forEach { logger.info(TAG, \"${attendeeInfo.attendeeId} joined the meeting\") }\n    }\n\n    override fun onAttendeesLeft(attendeeInfo: Array<AttendeeInfo>) {\n        attendeeInfo.forEach { logger.info(TAG, \"${attendeeInfo.attendeeId} left the meeting\") }\n    }\n\n    override fun onAttendeesDropped(attendeeInfo: Array<AttendeeInfo>) {\n        attendeeInfo.forEach { logger.info(TAG, \"${attendeeInfo.attendeeId} dropped from the meeting\") }\n    }\n\n    override fun onAttendeesMuted(attendeeInfo: Array<AttendeeInfo>) {\n        attendeeInfo.forEach { logger.info(TAG, \"${attendeeInfo.attendeeId} muted\") }\n    }\n\n    override fun onAttendeesUnmuted(attendeeInfo: Array<AttendeeInfo>) {\n        attendeeInfo.forEach { logger.info(TAG, \"${attendeeInfo.attendeeId} unmuted\") }\n    }\n}\n\nmeetingSession.audioVideo.addRealtimeObserver(observer)\n```\n\n#### Use case 11. Detect active speakers and active scores of speakers.\n\nYou can use the `onActiveSpeakerDetected` event to enlarge or emphasize the most active speaker\u2019s video tile if available. By setting the `scoreCallbackIntervalMs` and implementing `onActiveSpeakerScoreChanged`, you can receive scores of the active speakers periodically.\n\n```kotlin\nval observer = object : ActiveSpeakerObserver {\n    override fun onActiveSpeakerDetected(attendeeInfo: Array<AttendeeInfo>) {\n        if (attendeeInfo.isNotEmpty()) {\n            logger.info(TAG, \"${attendeeInfo[0].attendeeId} is the most active speaker\")\n        }\n    }\n\n    // Set to receive onActiveSpeakerScoreChanged event at interval of 1s\n    override val scoreCallbackIntervalMs: Int? get() = 1000\n\n    override fun onActiveSpeakerScoreChanged(scores: Map<AttendeeInfo, Double>) {\n        val scoreString = scores.map { entry -> \"${entry.key.attendeeId}: ${entry.value}\" }.joinToString(\",\")\n        logger.info(TAG, \"Scores of active speakers are: $scoreString\")\n    }\n}\n\n// Calculating the active speaker base on the SDK provided policy, you can provide any custom algorithm\nmeetingSession.audioVideo.addActiveSpeakerObserver(DefaultActiveSpeakerPolicy(), observer)\n```\n\n### Video\n\n> Note: You'll need to bind the video to a `VideoRenderView` to render it.\n>\n> A local video tile can be identified using the `isLocalTile` property.\n>\n> A content video tile can be identified using the `isContent` property. See [Screen and content share](#screen-and-content-share).\n>\n> A tile is created with a new tile ID when the same remote attendee restarts the video.\n\n\nYou can find more details on adding/removing/viewing video from [Building a meeting application on android using the Amazon Chime SDK](https://aws.amazon.com/blogs/business-productivity/building-a-meeting-application-on-android-using-the-amazon-chime-sdk/).\n\n#### Use case 12. Start receiving remote videos.\n\nYou can call `startRemoteVideo` to start receiving remote videos, as this doesn\u2019t happen by default.\n\n```kotlin\nmeetingSession.audioVideo.startRemoteVideo()\n```\n\n#### Use case 13. Stop receiving remote videos.\n\n`stopRemoteVideo` stops receiving remote videos and triggers `onVideoTileRemoved` for existing remote videos.\n\n```kotlin\nmeetingSession.audioVideo.stopRemoteVideo()\n```\n\n#### Use case 14. View remote videos.\n\n```kotlin\nval observer = object : VideoTileObserver {\n    override fun onVideoTileAdded(tileState: VideoTileState) {\n        // Ignore local video (see View local video), content video (seeScreen and content share)\n        if(tileState.isLocalTile || tileState.isContent) return\n\n        val videoRenderView = /* a VideoRenderView object in your application to show the video */\n        meetingSession.audioVideo.bindVideoView(videoRenderView, tileState.tileId)\n    }\n\n    override onVideoTileRemoved(tileState: VideoTileState) {\n        // unbind video view to stop viewing the tile\n        audioVideo.unbindVideoView(tileState.tileId)\n    }\n}\n\nmeetingSession.audioVideo.addVideoTileObserver(observer)\n```\n\nFor more advanced video tile management, take a look at [Video Pagination](https://github.com/aws/amazon-chime-sdk-android/blob/master/guides/video_pagination.md).\n\n#### Use case 15. Start sharing your video.\n\n```kotlin\n// Use internal camera capture for the local video\nmeetingSession.audioVideo.startLocalVideo()\n\n// Use internal camera capture and set configuration for the video, e.g. maxBitRateKbps\n// If maxBitRateKbps is not set, it will be self adjusted depending on number of users and videos in the meeting\n// This can be called multiple times to dynamically adjust video configuration\nval localVideoConfig = LocalVideoConfiguration(600)\nmeetingSession.audioVideo.startLocalVideo(localVideoConfig)\n\n// You can switch camera to change the video input device\nmeetingSession.audioVideo.switchCamera()\n\n// Or you can inject custom video source for local video, see custom video guide\n```\n\n#### Use case 16. Stop sharing your video.\n\n```kotlin\nmeetingSession.audioVideo.stopLocalVideo()\n```\n\n#### Use case 17. View local video.\n\n```kotlin\nval observer = object : VideoTileObserver {\n    override fun onVideoTileAdded(tileState: VideoTileState) {\n        // onVideoTileAdded is called after startLocalVideo\n        val localVideoRenderView = /* a VideoRenderView object to show local video */\n\n        if (tileState.isLocalTile) {\n            audioVideo.bindVideoView(localVideoRenderView, tileState.tileId)\n        }\n    }\n\n    override onVideoTileRemoved(tileState: VideoTileState) {\n        // onVideoTileRemoved is called after stopLocalVideo\n        if (tileState.isLocalTile) {\n            logger.info(TAG, \"Local video is removed\")\n            audioVideo.unbindVideoView(tileState.tileId)\n        }\n    }\n}\n\nmeetingSession.audioVideo.addVideoTileObserver(observer)\n```\n\n### Screen and content share\n\n> Note: When you or other attendees share content (e.g., screen capture or any other VideoSource object), the content attendee (attendee-id#content) joins the session and shares content as if a regular attendee shares a video.\n>\n> For example, your attendee ID is \"my-id\". When you call `meetingSession.audioVideo.startContentShare`, the content attendee \"my-id#content\" will join the session and share your content.\n\n#### Use case 18. Start sharing your screen or content.\n\n```kotlin\nval observer = object : ContentShareObserver {\n    override fun onContentShareStarted() {\n        logger.info(TAG, \"Content share started\")\n    }\n\n    override fun onContentShareStopped(status: ContentShareStatus) {\n        logger.info(TAG, \"Content share stopped with status ${status.statusCode}\")\n    }\n}\n\nmeetingSession.audioVideo.addContentShareObserver(observer)\nval contentShareSource = /* a ContentShareSource object, can use DefaultScreenCaptureSource for screen share or any subclass with custom video source */\n// ContentShareSource object is not managed by SDK, builders need to start, stop, release accordingly\nmeetingSession.audioVideo.startContentShare(contentShareSource)\n```\n\nYou can set configuration for content share, e.g. maxBitRateKbps. Actual quality achieved may vary throughout the call depending on what system and network can provide.\n```kotlin\nval contentShareConfig = LocalVideoConfiguration(200)\nmeetingSession.audioVideo.startContentShare(contentShareSource, contentShareConfig)\n```\n\nSee [Content Share](https://github.com/aws/amazon-chime-sdk-android/blob/master/guides/content_share.md) for more details.\n\n#### Use case 19. Stop sharing your screen or content.\n\n```kotlin\nmeetingSession.audioVideo.stopContentShare()\n```\n\n#### Use case 20. View attendee content or screens.\n\nChime SDK allows two simultaneous content shares per meeting. Remote content shares will trigger `onVideoTileAdded`, while local share will not. To render the video for preview, add a `VideoSink` to the `VideoSource` in the `ContentShareSource`.\n\n```kotlin\nval observer = object : VideoTileObserver {\n    override fun onVideoTileAdded(tileState: VideoTileState) {\n        if (tileState.isContent) {\n            // tileState.attendeeId is formatted as \"attendee-id#content\"\n            val attendeeId = tileState.attendeeId\n            // Get the attendee ID from \"attendee-id#content\"\n            val baseAttendeeId = DefaultModality(attendeeId).base()\n            logger.info(TAG, \"$baseAttendeeId is sharing screen\")\n\n            val contentVideoRenderView = /* a VideoRenderView object in your application to show the content video */\n            meetingSession.audioVideo.bindVideoView(contentVideoRenderView, tileState.tileId)\n        }\n    }\n\n    override onVideoTileRemoved(tileState: VideoTileState) {\n        // unbind video view to stop viewing the tile\n        meetingSession.audioVideo.unbindVideoView(tileId)\n    }\n}\n\nmeetingSession.audioVideo.addVideoTileObserver(observer)\n```\n\n### Metrics\n\n#### Use case 21. Add an observer to receive the meeting metrics.\n\nSee `ObservableMetric` for more available metrics and to monitor audio, video, and content share quality.\n\n```kotlin\nval observer = object: MetricsObserver {\n    override fun onMetricsReceived(metrics: Map<ObservableMetric, Any>) {\n        metrics.forEach { (metricsName, metricsValue) ->\n            logger.info(TAG, \"$metricsName : $metricsValue\")\n        }\n    }\n}\n\nmeetingSession.audioVideo.addMetricsObserver(observer)\n```\n\n### Data Message\n\n#### Use case 22. Add  an observer to receive data message.\n\nYou can receive real-time messages from multiple topics after starting the meeting session.\n\n> Note: Data messages sent from local participant will not trigger this callback unless it's throttled.\n\n```kotlin\nval YOUR_ATTENDEE_ID = meetingSession.configuration.credentials.attendeeId\n\nval observer = object: DataMessageObserver {\n    override fun onDataMessageReceived(dataMessage: DataMessage) {\n        // A throttled message is returned by backend\n        if (!dataMessage.throttled) {\n            logger.info(TAG, \"[${dataMessage.timestampMs}][{$dataMessage.senderAttendeeId}] : ${dataMessage.text()}\")\n    }\n}\n\n// You can subscribe to multiple topics.\nconst val DATA_MESSAGE_TOPIC = \"chat\"\nmeetingSession.audioVideo.addRealtimeDataMessageObserver(DATA_MESSAGE_TOPIC, observer)\n```\n\n#### Use case 23. Send data message.\n\nYou can send real time message to any topic, to which the observers that have subscribed will be notified.\n\n> Note: Topic needs to be alpha-numeric and it can include hyphen and underscores. Data cannot exceed 2kb and lifetime is optional but positive integer.\n\n```kotlin\nconst val DATA_MESSAGE_TOPIC = \"chat\"\nconst val DATA_MESSAGE_LIFETIME_MS = 1000\n\n// Send \"Hello Chime\" to any subscribers who are listening to \"chat\" topic with 1 seconds of lifetime\nmeetingSession.audioVideo.realtimeSendDataMessage(\n    DATA_MESSAGE_TOPIC,\n    \"Hello Chime\",\n    DATA_MESSAGE_LIFETIME_MS\n)\n```\n\n### Stopping a session\n\n> Note: Make sure to remove all the observers and release resources you have added to avoid any memory leaks.\n\n#### Use case 24. Stop a session.\n\n```kotlin\nval observer = object: AudioVideoObserver {  \n    override fun onAudioSessionStopped(sessionStatus: MeetingSessionStatus) {\n        // This is where meeting ended.\n        // You can do some clean up work here.\n    }\n\n    override fun onVideoSessionStopped(sessionStatus: MeetingSessionStatus) {\n        // This will be invoked as well.\n    }\n}\n\nmeetingSession.audioVideo.addAudioVideoObserver(observer)\nmeetingSession.audioVideo.stop()\n```\n\n### Amazon Voice Focus\n\nAmazon Voice Focus reduces the background noise in the meeting for better meeting experience. For more details, see [Amazon Voice Focus](https://github.com/aws/amazon-chime-sdk-android/blob/master/guides/api_overview.md#11-using-amazon-voice-focus-optional).\n\n#### Use case 25. Enable/Disable Amazon Voice Focus.\n\n```kotlin\nval enbabled = meetingSession.audioVideo.realtimeSetVoiceFocusEnabled(true) // enabling Amazon Voice Focus successful\n\nval disabled = meetingSession.audioVideo.realtimeSetVoiceFocusEnabled(false) // disabling Amazon Voice Focus successful\n```\n\n### Custom Video Source\n\nCustom video source allows you to control the video, such as applying a video filter. For more details, see [Custom Video](https://github.com/aws/amazon-chime-sdk-android/blob/master/guides/custom_video.md).\n\n### Background Blur and Replacement\n\nBackground Blur/Replacement allows you to apply blur on or replace background of your video with an image. For more details, see [BackgroundFilter](guides/background_video_filters.md).\n\n### Redundant audio\n\nStarting from version 0.18.3, the SDK starts sending redundant audio data to our servers on detecting packet loss\nto help reduce its effect on audio quality. Redundant audio packets are only sent out for packets containing active\naudio, i.e. speech or music. This may increase the bandwidth consumed by audio to up to 3 times the normal amount\ndepending on the amount of packet loss detected. The SDK will automatically stop sending redundant data if it hasn't\ndetected any packet loss for 5 minutes.\n\nIf you need to disable this feature, you can do so through the AudioVideoConfiguration before starting the session.\n\n```kotlin\nmeetingSession.audioVideo.start(AudioVideoConfiguration(enableAudioRedundancy = false))\n```\n\nWhile there is an option to disable the feature, we recommend keeping it enabled for improved audio quality.\nOne possible reason to disable it might be if your customers have very strict bandwidth limitations.\n\n## Frequently Asked Questions\n\nRefer to [General FAQ](https://aws.github.io/amazon-chime-sdk-js/modules/faqs.html) for Amazon Chime SDK.\n\n### Debugging\n\n#### How can I get Amazon Chime SDK logs for debugging?\nApplications can get logs from Chime SDK by passing instances of Logger when creating [MeetingSession](https://aws.github.io/amazon-chime-sdk-android/amazon-chime-sdk/com.amazonaws.services.chime.sdk.meetings.session/-meeting-session/index.html). Amazon Chime SDK has some default implementations of logger that your application can use, such as [ConsoleLogger](https://aws.github.io/amazon-chime-sdk-android/amazon-chime-sdk/com.amazonaws.services.chime.sdk.meetings.utils.logger/-console-logger/index.html) which logs into console. `ConsoleLogger` is set to `INFO` level as default. Therefore, in order to get all logs, including media logs, create logger by following:\n```kotlin\nval logger = ConsoleLogger(LogLevel.VERBOSE)\n```\n\n#### Remote attendees cannot hear my audio, what do I do?\nThe SDK uses [OpenSL ES](https://developer.android.com/ndk/guides/audio/opensl/opensl-for-android) underneath which requires the setting of recording presets while opening the connection to the microphone device. We have discovered that there isn't a specific preset value that works well on all possible android devices. The SDK uses a default preset of `VoiceCommunication` which we have arrived at after running some tests on the devices in our possession. If this default preset does not work and is leading to the remote party not being able to hear you, please try starting the session with a different recording preset by specifying `audioRecordingPresetOverride` in the `AudioVideoConfiguration` that is passed into the start API.\n```kotlin\n// Creating a config where the preset is overriden with Generic (for example)\nval audioVideoConfig = AudioVideoConfiguration(audioRecordingPresetOverride = AudioRecordingPresetOverride.Generic)\n// Start Audio Video\naudioVideo.start(audioVideoConfig)\n```\n\n## Notice\n\nThe use of background replacement is subject to additional notice. You and your end users are responsible for all Content (including any images) uploaded for use with background replacement, and must ensure that such Content does not violate the law, infringe or misappropriate the rights of any third party, or otherwise violate a material term of your agreement with Amazon (including the documentation, the AWS Service Terms, or the Acceptable Use Policy).\n\n---\n\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n", "release_dates": ["2024-02-16T20:17:26Z", "2023-12-21T18:14:15Z", "2023-10-02T20:43:52Z", "2023-06-27T23:20:51Z", "2023-05-16T18:46:59Z", "2023-03-17T00:32:40Z", "2023-01-27T18:55:08Z", "2022-12-03T01:44:35Z", "2022-11-16T21:46:46Z", "2022-10-21T17:52:04Z", "2022-08-25T23:07:36Z", "2022-08-12T23:50:49Z", "2022-07-28T23:54:53Z", "2022-07-14T23:07:27Z", "2022-06-18T00:06:11Z", "2022-06-03T22:26:19Z", "2022-05-19T21:07:15Z", "2022-05-12T17:53:35Z", "2022-04-21T23:27:35Z", "2022-04-08T00:41:04Z", "2022-03-21T21:38:56Z", "2022-03-11T01:22:53Z", "2022-02-25T01:35:08Z", "2022-02-10T23:32:11Z", "2022-01-31T21:13:54Z", "2021-12-21T23:00:42Z", "2021-11-12T01:05:06Z", "2021-11-01T21:55:40Z", "2021-09-02T18:16:11Z", "2021-07-21T22:17:18Z"]}, {"name": "amazon-chime-sdk-component-library-react", "description": "Amazon Chime React Component Library with integrations with the Amazon Chime SDK.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon Chime SDK React Components Library\n\n<a href=\"https://www.npmjs.com/package/amazon-chime-sdk-component-library-react\"><img src=\"https://img.shields.io/npm/v/amazon-chime-sdk-component-library-react?style=flat-square\"></a>\n<a href=\"https://github.com/aws/amazon-chime-sdk-component-library-react\"><img src=\"https://github.com/aws/amazon-chime-sdk-component-library-react/workflows/CI%20Workflow/badge.svg\"></a>\n\n[Amazon Chime SDK Project Board](https://github.com/orgs/aws/projects/12)\n\n[Amazon Chime SDK React Components Library Documentation](https://aws.github.io/amazon-chime-sdk-component-library-react/)\n\n[Amazon Chime SDK for JavaScript Library](https://github.com/aws/amazon-chime-sdk-js/)\n\nThe Amazon Chime SDK makes it easy to add collaborative audio calling, video calling, and screen share features to web applications by using the same infrastructure services that power millions of Amazon Chime online meetings.\n\nThe Amazon Chime SDK React Component Library supplies client-side state management and reusable UI components for common web interfaces used in audio and video conferencing applications, including: video tile grids, microphone activity indicators, and call controls. All components come with a simple, modern design, and can be used as-is or restyled with a custom theme. In addition to UI components, the library leverages Reacts' state management tools such as Providers and Hooks to connect to the Amazon Chime SDK for JavaScript and pass data to the UI layer, simplifying state synchronization so that developers can concentrate on building engaging experiences.\n\nGuidance on consuming these SDK Components is available at [Amazon Chime SDK React Component Library Documentation](https://aws.github.io/amazon-chime-sdk-component-library-react/). Our Storybook documentation also captures a [Quick Start Guide](https://aws.github.io/amazon-chime-sdk-component-library-react/?path=/docs/quick-starts--page).\n\nThe [Amazon Chime SDK Project Board](https://github.com/orgs/aws/projects/12) captures the status of community feature requests across all our repositories. The descriptions of the columns on the board are captured in this [guide](https://aws.github.io/amazon-chime-sdk-js/modules/projectboard.html).\n\n### To get started, see the following resources\n\n[Amazon Chime SDK](https://aws.amazon.com/chime/chime-sdk/)\n\n[Amazon Chime SDK Pricing](https://aws.amazon.com/chime/chime-sdk/pricing/)\n\n[Supported Browsers](https://docs.aws.amazon.com/chime/latest/dg/meetings-sdk.html#mtg-browsers)\n\n[Amazon Chime SDK API Reference](https://docs.aws.amazon.com/chime-sdk/latest/APIReference/welcome.html)\n\n[Amazon Chime SDK for JavaScript Documentation](https://aws.github.io/amazon-chime-sdk-js/#amazon-chime-sdk-for-javascript)\n\n[Migration from V1 to V2](https://aws.github.io/amazon-chime-sdk-component-library-react/?path=/docs/migration-to-v2--page)\n\n[Migration from V2 to V3](https://aws.github.io/amazon-chime-sdk-component-library-react/?path=/docs/migration-to-v3--page)\n\n## Examples\n\n[Amazon Chime Meeting Demo](https://github.com/aws-samples/amazon-chime-sdk/tree/main/apps/meeting)\n\n[Messaging Demo](https://github.com/aws-samples/amazon-chime-sdk/tree/main/apps/chat)\n\n[Building Breakout Room Experiences with the Amazon Chime SDK React Component Library](https://github.com/aws-samples/amazon-chime-sdk-meetings-breakout-rooms/tree/main)\n\n[Quickly launch an Amazon Chime SDK application with AWS Amplify](https://aws.amazon.com/blogs/business-productivity/quickly-launch-an-amazon-chime-sdk-application-with-aws-amplify/)\n\n## Installation and Development\n\nIf you are adding this library to your existing application, add `amazon-chime-sdk-component-library-react` and the necessary peer dependencies to your project.\n\n```\nnpm install --save amazon-chime-sdk-component-library-react amazon-chime-sdk-js styled-components styled-system\n```\n\nOtherwise clone the repo and install the dependencies.\n\n## Contributing to the component library\n\n### To generate dependencies\n\n```\ngit clone https://github.com/aws/amazon-chime-sdk-component-library-react.git\nnpm install\n```\n\n### Build\n\n```\nnpm run build\n```\n\nOnce you build, check and resolve any warnings you may get like unresolved dependencies or circular dependencies. Remove these as it will help in bundling the library warning/error free.\n\n### To run the Storybook server locally\n\n`amazon-chime-sdk-component-library-react` uses [Storybook](https://storybook.js.org/) for documentation.\n\n```\nnpm start\n```\n\n### Test\n\nRun all unit test suites.\n\n```\nnpm run test\n```\n\nRun an individual unit test suite.\n\n```\nnpm run test -- <filepath>\n```\n\nRun all unit test suites in watch mode\n\n```\nnpm run test -- --watch\n```\n\nRun all snapshot test suites. [Docker](https://docs.docker.com/install/) is required to run Puppeteer in a Docker container.\n\n```\nnpm run test:snapshots\n```\n\n#### Troubleshooting\n\n> Error 1: Service 'chromium' failed to build : toomanyrequests: You have reached your pull rate limit\n\nYou may need to create a [Docker Hub](https://hub.docker.com/) account and [authenticate pull request from Docker Hub](https://docs.docker.com/docker-hub/download-rate-limit/#how-do-i-authenticate-pull-requests). Unauthenticated (anonymous) users will have the limits enforced via IP.\n\n> Error 2: Timeout when you run `npm run test:snapshots`\n\nYou have 2 options:\n\n1. Set an environment variable `WAIT_ON_TIMEOUT=600000` (e.g. 10 minutes. By default, it's 5 minutes). It will wait for a longer time while checking for the server to respond.\n2. Start storybook server locally by running `npm start`, then run `npm run test:snapshots`.\n\nRun an individual snapshot test suite, make sure that storybook server is running locally before kicking off the test.\n\n```\nnpm run test:snapshots-path -- <filepath>\n```\n\nRun an individual snapshot test suite and override existing snapshot(s).\n\n```\nnpm run test:snapshots-path -- <filepath> -u\n```\n\nA code coverage summary will be printed at the end of each `npm run test` run. Full coverage including coverage for each file is generated in a `lcov-report` html file that can be rendered in the browser. This is generated in a `/coverage` directory on each test run.\n\n## Troubleshooting and Support\nReview the resources given in the ReadMe and use our [client documentation](https://aws.github.io/amazon-chime-sdk-component-library-react/) for guidance on how to develop on this library. Additionally, search our [issues database](https://github.com/aws/amazon-chime-sdk-component-library-react/issues) to see if your issue is already addressed. If not please cut us an [issue](https://github.com/aws/amazon-chime-sdk-component-library-react/issues/new/choose) using the provided templates.\n\nIf you have more questions, or require support for your business, you can reach out to [AWS Customer support](https://pages.awscloud.com/GLOBAL-aware-GC-Amazon-Chime-SDK-2020-reg.html). You can review our support plans [here](https://aws.amazon.com/premiumsupport/plans/?nc=sn&loc=1).\n\n## Notice\n\nThe use of Amazon Voice Focus via this SDK involves the downloading and execution of code at runtime by end users.\n\nThe use of Amazon Voice Focus, Amazon Chime Echo Reduction, Background Blur and Background Replacement runtime code is subject to additional notices. See [this Amazon Voice Focus NOTICES file](https://static.sdkassets.chime.aws/workers/NOTICES.txt), [background blur and background replacement NOTICES file](https://static.sdkassets.chime.aws/bgblur/workers/NOTICES.txt) for details. You agree to make these additional notices available to all end users who use Amazon Voice Focus and background blur runtime code via this SDK.\n\nThe use of TensorFlow runtime code referenced above may be subject to additional license requirements. See the licenses page for TensorFlow.js [here](https://github.com/tensorflow/tfjs/blob/master/LICENSE) and TensorFlow.js models [here](https://github.com/tensorflow/tfjs-models/blob/master/LICENSE) for details.\n\nYou and your end users are responsible for all Content (including any images) uploaded for use with background replacement, and must ensure that such Content does not violate the law, infringe or misappropriate the rights of any third party, or otherwise violate a material term of your agreement with Amazon (including the documentation, the AWS Service Terms, or the Acceptable Use Policy).\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n", "release_dates": ["2023-06-08T20:42:51Z", "2023-04-27T22:55:45Z", "2023-01-11T01:42:11Z", "2022-10-27T23:41:47Z", "2022-09-15T19:22:26Z", "2022-05-06T18:03:48Z", "2022-04-13T20:27:06Z", "2022-03-22T22:28:14Z", "2022-02-05T01:02:26Z", "2022-01-24T22:49:41Z", "2022-01-06T23:19:48Z", "2021-11-19T22:11:18Z", "2021-10-26T00:47:49Z", "2021-10-21T16:52:33Z", "2021-10-05T22:31:57Z", "2021-09-30T19:00:55Z", "2021-09-02T01:16:58Z", "2021-08-31T16:42:07Z", "2021-07-29T21:46:00Z", "2021-07-16T18:32:40Z", "2021-06-16T19:16:52Z", "2021-05-28T18:46:05Z", "2021-05-14T21:40:40Z", "2021-04-14T22:05:52Z", "2021-03-25T23:43:38Z", "2021-03-11T19:28:37Z", "2021-02-26T01:06:55Z", "2021-02-12T01:47:23Z", "2021-02-08T22:44:48Z", "2020-12-15T02:59:33Z"]}, {"name": "amazon-chime-sdk-cpp", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon Chime SDK for C++\r\n[Amazon Chime SDK Project Board](https://aws.github.io/amazon-chime-sdk-js/modules/projectboard.html)\r\n\r\n## Build video calling, audio calling, and screen sharing applications powered by the Amazon Chime SDK.\r\n\r\nThe Windows client library builds on the C++ signaling client library by including a pre-compiled WebRTC implementation for Windows. So now, developers do not have to select, build and test a WebRTC library for their application. Developers still retain access to the audio and video frames for low-level integrations with microphone, speaker, and video devices.\r\n\r\nWe also have an [Amazon Chime SDK Project Board](https://aws.github.io/amazon-chime-sdk-js/modules/projectboard.html) where you can find community requests and their statuses.\r\n\r\nTo get started, see the following resources:\r\n\r\n* [Amazon Chime](https://aws.amazon.com/chime)\r\n* [Amazon Chime Developer Guide](https://docs.aws.amazon.com/chime/latest/dg/what-is-chime.html)\r\n* [Amazon Chime SDK API Reference](http://docs.aws.amazon.com/chime/latest/APIReference/Welcome.html)\r\n\r\n## Setup\r\n\r\nWe currently distribute a Windows Binary of the C++ SDK, you can find the set up instructions [here](https://github.com/aws/amazon-chime-sdk-cpp/blob/main/guides/setup_windows.md).\r\n\r\n## Reporting a suspected vulnerability\r\n\r\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](https://aws.amazon.com/security/vulnerability-reporting/). Please do not create a public GitHub issue.\r\n\r\n## API Overview\r\n\r\nC++ SDK API usage can be found [here](https://github.com/aws/amazon-chime-sdk-cpp/guides/api_overview.md) with code snippets.\r\n", "release_dates": ["2023-11-17T02:00:30Z", "2023-06-15T23:27:09Z", "2023-02-03T22:28:58Z"]}, {"name": "amazon-chime-sdk-ios", "description": "An iOS client library for integrating multi-party communications powered by the Amazon Chime service.", "language": "Swift", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon Chime SDK for iOS\n[Amazon Chime SDK Project Board](https://aws.github.io/amazon-chime-sdk-js/modules/projectboard.html)\n\n> Note: If building with the SDK source code, the `development` branch contains bleeding-edge changes that may not build with the publically available Chime media library or may not be as stable as [public releases](https://github.com/aws/amazon-chime-sdk-ios/releases).\n\n## Build video calling, audio calling, and screen sharing applications powered by Amazon Chime.\n\nThe Amazon Chime SDK for iOS makes it easy to add collaborative audio calling,\nvideo calling, and screen share viewing features to iOS applications by\nusing the same infrastructure services that power meetings on the Amazon\nChime service.\n\nThis Amazon Chime SDK for iOS works by connecting to meeting session\nresources that you have created in your AWS account. The SDK has everything\nyou need to build custom calling and collaboration experiences in your\niOS application, including methods to: configure meeting sessions, list\nand select audio devices, switch video devices, start and stop screen share\nviewing, receive callbacks when media events occur such as volume changes,\nand manage meeting features such as audio mute and video tile bindings.\n\nWe also have an [Amazon Chime SDK Project Board](https://aws.github.io/amazon-chime-sdk-js/modules/projectboard.html) where you can find community requests and their statuses.\n\nTo get started, see the following resources:\n\n* [Amazon Chime](https://aws.amazon.com/chime)\n* [Amazon Chime Developer Guide](https://docs.aws.amazon.com/chime/latest/dg/what-is-chime.html)\n* [Amazon Chime SDK API Reference](http://docs.aws.amazon.com/chime/latest/APIReference/Welcome.html)\n* [SDK Documentation](https://aws.github.io/amazon-chime-sdk-ios/)\n\nAnd review the following guides:\n* [API Overview](guides/api_overview.md)\n* [Getting Started](guides/getting_started.md)\n* [Frequently Asked Questions (FAQ)](#frequently-asked-questions)\n* [Custom Video Sources, Processors, and Sinks](guides/custom_video.md)\n* [Video Pagination with Active Speaker-Based Policy](guides/video_pagination.md)\n* [Content Share](guides/content_share.md)\n* [Meeting Events](guides/meeting_events.md)\n* [Event Ingestion](guides/event_ingestion.md)\n* [Configuring Remote Video Subscription](guides/configuring_remote_video_subscriptions.md)\n* [Background Video Filters](guides/background_video_filters.md)\n\n## Include Amazon Chime SDK in Your Project\nYou can integrate Amazon Chime SDK in your project from SPM, CocoaPods or binaries through Github release.\n\nFor the purpose of setup, your project's root folder (where you can find your `.xcodeproj` file) will be referred to as `root`.\n\n### From CocoaPods\n1. The Amazon Chime SDK is available through [CocoaPods](http://cocoapods.org/). If you have not installed CocoaPods, install CocoaPods by running the command:\n    ```\n    $ gem install cocoapods\n    $ pod setup\n    ```\n    Depending on your system settings, you may have to use sudo for installing cocoapods as follows:\n\n    ```\n    $ sudo gem install cocoapods\n    $ pod setup\n    ```\n2. In root directory (the directory where your *.xcodeproj file is), run the following to create a Podfile in your project:\n    ```\n    $ pod init\n    ```\n3. Edit the `Podfile` to include `AmazonChimeSDK-Bitcode` into your project if you need bitcode:\n    ```\n    target 'YourTarget' do\n        pod 'AmazonChimeSDK-Bitcode'\n        ...\n    end\n    ```\n    If you don't need bitcode, you can add `AmazonChimeSDK-No-Bitcode` instead:\n    ```\n    target 'YourTarget' do\n        pod 'AmazonChimeSDK-No-Bitcode'\n        ...\n    end\n    ```\n    If you don't need video and content share functionality, or software video codec support, you can use `AmazonChimeSDKMediaNoVideoCodecs-Bitcode` for bitcode support or `AmazonChimeSDKMediaNoVideoCodecs-No-Bitcode` to reduce size:\n    ```\n    target 'YourTarget' do\n        pod 'AmazonChimeSDKMediaNoVideoCodecs-Bitcode'\n        ...\n    end\n    ```\n4. (Optional) If you want to use background blur and replacement features, add:\n    ```\n    target 'YourTarget' do\n      pod 'AmazonChimeSDKMachineLearning-Bitcode'\n      ...\n    end\n    ```\n    If you don't need bitcode, you can add `AmazonChimeSDKMachineLearning-No-Bitcode` instead:\n    ```\n    target 'YourTarget' do\n        pod 'AmazonChimeSDKMachineLearning-No-Bitcode'\n        ...\n    end\n    ```\n5. Then run the following command to install pods:\n   ```\n   $ pod install --repo-update\n   ```\n6. To open your project, open the newly generated `*.xcworkspace` file in the root directory using XCode. You can do this by issuing the following command in your project folder\n   ```\n    $ xed .\n   ```\n   Note: Do *NOT* use *.xcodeproj to open project.\n7. If you are using background blur and replacement features, under `Build Settings` tab, under the `Linking` section, add `-framework AmazonChimeSDKMachineLearning` to `Other Linker Flags`.\n<p align=\"center\">\n<img src=\"./media/cocoapods_machine_learning.png\" alt=\"image\" width=\"80%\"/>\n</p>\n\n### From SPM\nThe Amazon Chime SDK is available through [SPM](https://github.com/aws/amazon-chime-sdk-ios-spm). If you don't need video and content share functionality, or software video codec support, you could choose to use [no video codecs SPM](https://github.com/aws/amazon-chime-sdk-ios-no-video-codecs-spm) instead.\n\n1. Open your project in Xcode\n\n2. Go to **File** > **Swift Packages** > **Add Package Dependency...**\n\n3. In the field **Enter package repository URL**, enter \"https://github.com/aws/amazon-chime-sdk-ios-spm\". To use no video codecs media module instead, enter \"https://github.com/aws/amazon-chime-sdk-ios-no-video-codecs-spm\".\n\n4. Enter the latest version(e.g. `0.23.1`) and click **Next**.\n\n5. Choose packages for your project and click **Finish**. `AmazonChimeSDK` and `AmazonChimeSDKMedia` are required. Check `AmazonChimeSDKMachineLearning` if you'd like to use background blur and background replacement. \n\n\n### From Github Release Binaries\n\n#### 1. Download Binaries\n\n* Download the `AmazonChimeSDK` and `AmazonChimeSDKMedia` binaries from the latest [release](https://github.com/aws/amazon-chime-sdk-ios/releases/latest).\n  * If you'd like to use background blur and background replacement, also download the `AmazonChimeSDKMachineLearning` binary. Otherwise, you can ignore all references to `AmazonChimeSDKMachineLearning` in the instructions below.\n  * If you don't need video and content share functionality, or software video codec support, you could use `AmazonChimeSDKMediaNoVideoCodecs` binary instead of `AmazonChimeSDKMedia` and treat all references to `AmazonChimeSDKMedia` as `AmazonChimeSDKMediaNoVideoCodecs` in the instructions below.\n\n**NOTE: We do not support mixing and matching binaries from different releases.**\n\n* Unzip and copy the `.framework`s or `.xcframework`s to `root`, which depends on which framework your project uses.  For Xcode12.3 and later, please use `.xcframework` if you have compile issue. `.xcframework` is available after Amazon Chime SDK iOS v0.15.0\n\n#### 2. Update Project File\n\n* Open your `.xcodeproj` file in Xcode and click on your build target.\n\n* Under `Build Settings` tab,\n\n  * add `$(PROJECT_DIR)` to `Framework Search Path`.\n  * add `@executable_path/Frameworks` to `Runpath Search Paths`.\n\n  * under `Linking` section, add the following two flags in `Other Linker Flags`:\n    * `-lc++`\n    * `-ObjC`\n\n<p align=\"center\">\n<img src=\"./media/build_setting.png\" alt=\"image\" width=\"80%\"/>\n</p>\n\n* Under `General` tab, look for `Frameworks, Libraries, and Embedded Content` section. Click on +, then `Add Others`, then `Add Files`.\n\n  * If you are using traditional `.framework`, specify the location of `AmazonChimeSDK.framework`, `AmazonChimeSDKMedia.framework`, and `AmazonChimeSDKMachineLearning.framework` from Step 1. If you have compile error while using traditional `.framework`, which occurs in Xcode 12.3 and later, please use `.xcframework` instead, which is available after Amazon Chime SDK iOS v0.15.0.\n  * If you are using `.xcframework`, specify the location of `AmazonChimeSDK.xcframework`, `AmazonChimeSDKMedia.xcframework`, and `AmazonChimeSDKMachineLearning.xcframework` from Step 1.\n  * For `AmazonChimeSDK.framework` `AmazonChimeSDKMedia.framework` and frameworks, verify that `Embed & Sign` is selected under the `Embed` option. For `AmazonChimeSDKMachineLearning.framework`, select `Do Not Embed`.\n\n<p align=\"center\">\n<img src=\"./media/xcframework_setting.png\" alt=\"image\" width=\"80%\"/>\n</p>\n\n## Running the Demo App\n\nTo run the demo application, follow these steps.\n\n### 1. Clone the Git Repo\n\n`git clone git@github.com:aws/amazon-chime-sdk-ios.git`\n\n### 2. Import Amazon Chime SDK\n#### From CocoaPods\nFor both targets in `/AmazonChimeSDKDemo/Podfile`, replace `AMAZON_CHIME_SDK_VERSION` with a specific SDK version, e.g. `0.19.3` or remove it if utilize the latest version of Amazon Chime SDK.\n\nUnder `/AmazonChimeSDKDemo`, run the following command to install pods:\n```\n$ pod install --repo-update\n```\n\n#### Or From Downloaded Binary\n* Download `AmazonChimeSDKMedia` and `AmazonChimeSDKMachineLearning` binaries with bitcode support from the latest [release](https://github.com/aws/amazon-chime-sdk-ios/releases/latest).\n\n* Unzip and copy `AmazonChimeSDKMedia.xcframework` to `amazon-chime-sdk-ios/AmazonChimeSDK` folder and `AmazonChimeSDKMachineLearning.xcframework`  to `amazon-chime-sdk-ios/AmazonChimeSDKDemo` folder.\n\n### 3. Deploy Serverless Demo\n\nDeploy the serverless demo from [amazon-chime-sdk-js](https://github.com/aws/amazon-chime-sdk-js)\n\n### 4. Update AmazonChimeSDKDemo Project File\n\n* `AmazonChimeDemoSDKBroadcast.appex` is a Broadcast Extension for device level screen sharing used by AmazonChimeSDKDemo, verify that `Embed without Signing` is selected under the `Embed` option. Remove it from `Frameworks, Libraries, and Embedded Content` section if you do not wish to test this.\n\n<p align=\"center\">\n<img src=\"./media/xcframework_setting.png\" alt=\"image\" width=\"80%\"/>\n</p>\n\n* For each target, under `Signing & Capabilities` tab,\n  \n  * `Signing` section, use your own Apple Developer team and Bundle Identifier.\n  * (Optional)`App Groups` section, select your own app groups if you wish to test sharing device level screen capture. See [Content Share](https://github.com/aws/amazon-chime-sdk-ios/blob/master/guides/content_share.md) for more details.\n\n<p align=\"center\">\n<img src=\"./media/signing_capabilities.jpeg\" alt=\"image\" width=\"80%\"/>\n<img src=\"./media/app_groups.jpeg\" alt=\"image\" width=\"80%\"/>\n</p>\n\n### 5. Update Demo App\n\n* Update server URL and region:\n\n  * For Swift demo, update `AppConfiguration.swift` with the server URL and region of the serverless demo.\n  * For ObjC demo, update `ViewControllerObjC.h` with the server URL and region of the serverless demo.\n\n* (Optional) Update `broadcastBundleId` and `appGroupId` in BOTH `AppConfiguration.swift` and `SampleHandler.swift` with the broadcast upload extension bundle ID and App Group ID if you want to test sharing device level screen capture. See [Content Share](https://github.com/aws/amazon-chime-sdk-ios/blob/master/guides/content_share.md) for more details.\n\n### 6. Use Demo App to Join Meeting\n#### Run the AmazonChimeSDKDemoPods target with Amazon Chime SDK from CocoaPods\nOpen `AmazonChimeSDKDemo.xcworkspace` file in `AmazonChimeSDKDemo/` using XCode, select the `AmazonChimeSDKDemoPods` from the scheme dropdown list in the top bar of Xcode IDE, choose a build device and click the run button.\n\n#### Run the AmazonChimeSDKDemo target with Downloaded Amazon Chime SDK Binaries\nOpen `AmazonChimeSDKDemo.xcworkspace` file in `AmazonChimeSDKDemo/` using XCode, select the `AmazonChimeSDKDemo` from the scheme dropdown list in the top bar of Xcode IDE, choose a build device and click the run button.\n\nOn the joining screen, choose to join the meeting without `CallKit` or join via `CallKit` incoming/outgoing call. Since the demo app does not have Push Notification, it delays joining via incoming call by 10 seconds to give user enough time to background the app or lock the screen to mimic the behavior.\n\n## Reporting a Suspected Vulnerability\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our\n[vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public GitHub issue.\n\n## Usage\n- [Starting a session](#starting-a-session)\n- [Device](#device)\n- [Audio](#audio)\n- [Video](#video)\n- [Screen and content share](#screen-and-content-share)\n- [Metrics](#metrics)\n- [Data Message](#data-message)\n- [Stopping a session](#stopping-a-session)\n- [Amazon Voice Focus](#amazon-voice-focus)\n- [Custom Video Source](#custom-video-source)\n\n### Starting a session\n\n#### Use case 1. Start a session.\n\nYou need to start the meeting session to start sending and receiving audio. Make sure that the user has granted audio permission first.\n\n```swift\nmeetingSession.audioVideo.start()\n```\n\nThe default configurations are:\n* audio format is Stereo/48KHz i.e Stereo Audio with 48KHz sampling rate (stereo48K)\n* call kit disabled\n* audio redundancy enabled\n\nYou can specify non-default options in `AudioVideoConfiguration`, and then start the meeting session.\n\n```swift\nvar audioVideoConfig = AudioVideoConfiguration(audioMode: .mono48k, callKitEnabled: true, enableAudioRedundancy: false)\nmeetingSession.audioVideo.start(audioVideoConfiguration: audioVideoConfig)\n```\n\n#### Use case 2. Add an observer to receive audio and video session life cycle events.\n\n> Note: To avoid missing any events, add an observer before the session starts. You can remove the observer by calling meetingSession.audioVideo.removeAudioVideoObserver(observer).\n\n```swift\nclass MyAudioVideoObserver: AudioVideoObserver {\n    func audioSessionDidStartConnecting(reconnecting: Bool) {\n        if (reconnecting) {\n            // e.g. the network connection is dropped.\n        }\n    }\n    func audioSessionDidStart(reconnecting: Bool) {\n        // Meeting session starts.\n        // Can use realtime, devices APIs.\n    }\n    func audioSessionDidDrop() {}\n    func audioSessionDidStopWithStatus(sessionStatus: MeetingSessionStatus) {\n         // See the \"Stopping a session\" section for details.\n    }\n    func audioSessionDidCancelReconnect() {}\n    func connectionDidRecover() {}\n    func connectionDidBecomePoor() {}\n    func videoSessionDidStartConnecting() {}\n    func videoSessionDidStartWithStatus(sessionStatus: MeetingSessionStatus) {\n        // Video session starts.\n        // Can use video APIs.\n    }\n    func videoSessionDidStopWithStatus(sessionStatus: MeetingSessionStatus) {}\n\n    meetingSession.audioVideo.addAudioVideoObserver(observer: self)\n}\n```\n\n### Device\n\n#### Use case 3. List audio devices.\n\nList available audio devices for the meeting.\n\n```swift\n// An list of MediaDevice objects\nlet audioDevices = meetingSession.audioVideo.listAudioDevices()\n\nfor device in audioDevices {\n    logger.info(msg: \"Device type: \\(device.type), label: \\(device.label)\")\n}\n```\n\n#### Use case 4. Choose audio device by passing `MediaDevice` object.\n\n> Note: You should call this after the session started or it\u2019ll be no-op. You should call chooseAudioDevice with one of devices returned from listAudioDevices().\n\n```swift\nlet audioDevices = audioVideo.listAudioDevices()\nval device = /* An item from audioDevices */\nmeetingSession.audioVideo.chooseAudioDevice(mediaDevice: device)           \n```\n\n#### Use case 5. Switch between camera.\n\n> Note: switchCamera() is no-op if you are using custom camera capture source. Please refer to [Custom Video](https://github.com/aws/amazon-chime-sdk-ios/blob/master/guides/custom_video.md#implementing-a-custom-video-source-and-transmitting) for more details.\n\n\nSwitch to use front or back camera on the device, if available.\n\n```swift\nmeetingSession.audioVideo.switchCamera()\n```\n\n#### Use case 6. Add an observer to receive the updated device list.\n\nAdd a `DeviceChangeObserver` to receive a callback when a new audio device connects or when an audio device disconnects. `audioDeviceDidChange` includes an updated device list.\n\n```swift\nclass MyDeviceChangeObserver: DeviceChangeObserver {\n    func audioDeviceDidChange(freshAudioDeviceList: [MediaDevice]) {\n        // A list of updated MediaDevice objects\n        for device in freshAudioDeviceList {\n            logger.info(msg: \"Device type: \\(device.type), label: \\(device.label)\")\n        }    \n    }\n\n    meetingSession.audioVideo.addDeviceChangeObserver(observer: self)\n}\n```\n\n#### Use case 7. Get currently selected audio device.\n\n```swift\nlet activeAudioDevice = meetingSession.audioVideo.getActiveAudioDevice()\n```\n\n### Audio\n\n#### Use case 8. Choose the audio configuration.\n\n> When joining a meeting, *Mono/16KHz*, *Mono/48KHz* and *Stereo/48KHz* are supported. *Stereo/48KHz* will be set as the default audio mode if not explicitly specified when starting the audio session.\n\n```swift\nmeetingSession.audioVideo.start() // starts the audio video session with Stereo/48KHz audio, callkit disabled and audio redundancy enabled\n\nmeetingSession.audioVideo.start(audioVideoConfiguration) // starts the audio video session with the specified [AudioVideoConfiguration]\n```\n\n> Note: So far, you've added observers to receive device and session lifecycle events. In the following use cases, you'll use the real-time API methods to send and receive volume indicators and control mute state.\n\n#### Use case 9. Mute and unmute an audio input.\n\n```swift\nlet muted = meetingSession.audioVideo.realtimeLocalMute() // returns true if muted, false if failed\n\nlet unmuted = meetingSession.audioVideo.realtimeLocalUnmute // returns true if unmuted, false if failed\n```\n\n#### Use case 10. Add an observer to observe realtime events such as volume changes/signal change/muted status of a specific attendee.\n\nYou can use this to build real-time indicators UI and get them updated for changes delivered by the array.\n\n> Note: These callbacks will only include the delta from the previous callback.\n\n```swift\nclass MyRealtimeObserver: RealtimeObserver {\n    func volumeDidChange(volumeUpdates: [VolumeUpdate]) {\n        for currentVolumeUpdate in volumeUpdates {\n            // Muted, NotSpeaking, Low, Medium, High\n            logger.info(msg: \"\\(currentVolumeUpdate.attendeeInfo.attendeeId)'s volume changed: \\(currentVolumeUpdate.volumeLevel)\")\n        }\n    }\n    func signalStrengthDidChange(signalUpdates: [SignalUpdate]) {\n        for currentSignalUpdate in signalUpdates {\n            // None, Low, High\n            logger.info(msg: \"\\(currentSignalUpdate.attendeeInfo.attendeeId)'s signal strength changed: \\(currentSignalUpdate.signalStrength)\")\n        }\n    }\n    func attendeesDidJoin(attendeeInfo: [AttendeeInfo]) {\n        for currentAttendeeInfo in attendeeInfo {\n            logger.info(msg: \"\\(currentAttendeeInfo.attendeeId) joined the meeting\")\n        }\n    }\n    func attendeesDidLeave(attendeeInfo: [AttendeeInfo]) {\n        for currentAttendeeInfo in attendeeInfo {\n            logger.info(msg: \"\\(currentAttendeeInfo.attendeeId) left the meeting\")\n        }\n    }\n    func attendeesDidDrop(attendeeInfo: [AttendeeInfo]) {\n        for currentAttendeeInfo in attendeeInfo {\n            logger.info(msg: \"\\(currentAttendeeInfo.attendeeId) dropped from the meeting\")\n        }\n    }\n    func attendeesDidMute(attendeeInfo: [AttendeeInfo]) {\n        for currentAttendeeInfo in attendeeInfo {\n            logger.info(msg: \"\\(currentAttendeeInfo.attendeeId) muted\")\n        }\n    }\n    func attendeesDidUnmute(attendeeInfo: [AttendeeInfo]) {\n        for currentAttendeeInfo in attendeeInfo {\n            logger.info(msg: \"\\(currentAttendeeInfo.attendeeId) unmuted\")\n        }\n    }\n\n    meetingSession.audioVideo.addRealtimeObserver(observer: self)\n}\n```\n\n#### Use case 11. Detect active speakers and active scores of speakers.\n\nYou can use the `activeSpeakerDidDetect` event to enlarge or emphasize the most active speaker\u2019s video tile if available. By setting the `scoreCallbackIntervalMs` and implementing `activeSpeakerScoreDidChange`, you can receive scores of the active speakers periodically.\n\n```swift\nclass MyActiveSpeakerObserver: ActiveSpeakerObserver {\n    let activeSpeakerObserverId = UUID().uuidString\n\n    var observerId: String {\n        return activeSpeakerObserverId\n    }\n\n    func activeSpeakerDidDetect(attendeeInfo: [AttendeeInfo]) {\n        if !attendeeInfo.isEmpty {\n            logger.info(msg: \"\\(attendeeInfo[0].attendeeId) is the most active speaker\")\n        }\n    }\n\n    var scoresCallbackIntervalMs: Int {\n        return 1000 // 1 second\n    }\n\n    func activeSpeakerScoreDidChange(scores: [AttendeeInfo: Double]) {\n       let scoresInString = scores.map { (score) -> String in\n            let (key, value) = score\n            return \"\\(key.attendeeId): \\(value)\"\n       }.joined(separator: \",\")\n       logger.info(msg: \"Scores of active speakers are: \\(scoresInString)\")\n    }\n\n    // Calculating the active speaker base on the SDK provided policy, you can provide any custom algorithm\n    meetingSession.audioVideo.addActiveSpeakerObserver(policy: DefaultActiveSpeakerPolicy(), observer: self)\n}\n```\n\n### Video\n\n> Note: You will need to bind the video to `VideoRenderView` in order to display the video.\n>\n> A local video tile can be identified using `isLocalTile` property.\n>\n> A content video tile can be identified using `isContent` property. See [Screen and content share](#screen-and-content-share).\n>\n> A tile is created with a new tile ID when the same remote attendee restarts the video.\n\n\n\nYou can find more details on adding/removing/viewing video from [Building a meeting application on ios using the Amazon Chime SDK](https://aws.amazon.com/blogs/business-productivity/building-a-meeting-application-on-ios-using-the-amazon-chime-sdk/).\n\n#### Use case 12. Start receiving remote video.\n\nYou can call `startRemoteVideo` to start receiving remote videos, as this doesn\u2019t happen by default.\n\n```swift\nmeetingSession.audioVideo.startRemoteVideo()\n```\n\n#### Use case 13. Stop receiving remote video.\n\n`stopRemoteVideo` stops receiving remote videos and triggers `onVideoTileRemoved` for existing remote videos.\n\n```swift\nmeetingSession.audioVideo.stopRemoteVideo()\n```\n\n#### Use case 14. View remote video tile.\n\n```swift\nclass MyVideoTileObserver: VideoTileObserver {\n    func videoTileDidAdd(tileState: VideoTileState) {\n        // Ignore local video (see View local video), content video (see Screen and content share)\n        if tileState.isLocalTile || tileState.isContent {\n            return\n        }\n\n       let videoRenderView = /* a VideoRenderView object in your application to show the video */\n       meetingSession.audioVideo.bind(videoView: videoRenderView, tileId: tileState.tileId)\n    }\n\n    func videoTileDidRemove(tileState: VideoTileState) {\n        // unbind video view to stop viewing the tile\n        meetingSession.audioVideo.unbindVideoView(tileId: tileState.tileId)\n    }\n\n   meetingSession.audioVideo.addVideoTileObserver(observer: self)\n}\n```\n\n#### Use case 15. Start sharing your video.\n\n```swift\n// Use internal camera capture for the local video\nmeetingSession.audioVideo.startLocalVideo()\n\n// Use internal camera capture and set configuration for the video, e.g. simulcastEnabled, maxBitRateKbps\n// If maxBitRateKbps is not set, it will be self adjusted depending on number of users and videos in the meeting\n// This can be called multiple times to enable/disable simulcast and adjust video max bit rate on the fly\nlet localVideoConfig = LocalVideoConfiguration(simulcastEnabled: true\uff0c maxBitRateKbps: 600)\nmeetingSession.audioVideo.startLocalVideo(config: localVideoConfig)\n\n// You can switch camera to change the video input device\nmeetingSession.audioVideo.switchCamera()\n\n// Or you can inject custom video source for local video, see custom video guide\n```\n\n#### Use case 16. Stop sharing your video.\n\n```swift\nmeetingSession.audioVideo.stopLocalVideo()\n```\n\n#### Use case 17. View local video.\n\n> Note: The local video should be mirrored. Set VideoRenderView.mirror = true\n\n```swift\nclass MyVideoTileObserver: VideoTileObserver {\n    func videoTileDidAdd(tileState: VideoTileState) {\n            if tileState.isLocalTile {\n                let localVideoView = /* a VideoRenderView object in your application to show the video */\n                meetingSession.audioVideo.bind(videoView: localVideoView, tileId: tileState.tileId)\n            }\n        }\n    }\n\n    func videoTileDidRemove(tileState: VideoTileState) {\n        // unbind video view to stop viewing the tile\n        meetingSession.audioVideo.unbindVideoView(tileId: tileState.tileId)\n    }\n\n    meetingSession.audioVideo.addVideoTileObserver(observer: self)\n}\n```\n\nFor more advanced video tile management, take a look at  [Video Pagination](https://github.com/aws/amazon-chime-sdk-ios/blob/master/guides/video_pagination.md).\n\n### Screen and content share\n\n> Note: When you or other attendees share content (e.g. screen capture or any other VideoSource object), the content attendee (attendee-id#content) joins the session and shares content as if a regular attendee shares a video.\n>\n> For example, your attendee ID is \"my-id\". When you call `meetingSession.audioVideo.startContentShare`, the content attendee \"my-id#content\" will join the session and share your content.\n\n#### Use case 18. Start sharing your screen or content.\n\n```swift\nclass MyContentShareObserver: ContentShareObserver {\n    func contentShareDidStart() {\n        logger.info(msg: \"Content Share has started\")\n    }\n\n    func contentShareDidStop(status: ContentShareStatus){\n        logger.info(msg: \"Content Share has stopped\")\n    }\n\n    meetingSession.audioVideo.addContentShareObserver(observer: self)\n    let contentShareSource = /* a ContentShareSource object, can use InAppScreenCaptureSource for screen share or any subclass with custom video source */\n    // ContentShareSource object is not managed by SDK, builders need to start, stop, release accordingly\n    meetingSession.audioVideo.startContentShare(source: contentShareSource)\n}\n```\n\nYou can set configuration for content share, e.g. maxBitRateKbps. Actual quality achieved may vary throughout the call depending on what system and network can provide.\n```swift\nlet contentShareConfig = LocalVideoConfiguration(maxBitRateKbps: 200)\nmeetingSession.audioVideo.startContentShare(source: contentShareSource, config: contentShareConfig)\n```\nSee [Content Share](https://github.com/aws/amazon-chime-sdk-ios/blob/master/guides/content_share.md) for more details.\n\n#### Use case 19. Stop sharing your screen or content.\n```swift\nmeetingSession.audioVideo.stopContentShare()\n```\n\n#### Use case 20. View attendee content or screens.\n\nChime SDK allows two simultaneous content shares per meeting. Remote content shares will trigger `onVideoTileAdded`, while local share will not. To render the video for preview, add a `VideoSink` to the `VideoSource` in the `ContentShareSource`.\n\n```swift\nclass MyVideoTileObserver: VideoTileObserver {\n    func videoTileDidAdd(tileState: VideoTileState) {\n        if (tileState.isContent) {\n            // tileState.attendeeId is formatted as \"attendee-id#content\"\n            let attendeeId = tileState.attendeeId\n            // Get the attendee ID from \"attendee-id#content\"\n            let baseAttendeeId = DefaultModality(attendeeId).base()\n            logger.info(msg: \"$baseAttendeeId is sharing screen\")\n\n            let screenVideoView = /* a VideoRenderView object in your application to show the video */\n            meetingSession.audioVideo.bindVideoView(videoView: screenVideoView, tileId: tileState.tileId)\n        }\n    }\n\n    func videoTileDidRemove(tileState: VideoTileState) {\n        meetingSession.audioVideo.unbindVideoView(tileId: tileState.tileId)\n    }\n\n    meetingSession.audioVideo.addVideoTileObserver(observer: self)\n}\n```\n\n### Metrics\n\n#### Use case 21. Add an observer to receive the meeting metrics.\n\nSee `ObservableMetric` for more available metrics and to monitor audio, video, and content share quality.\n\n```swift\nclass MyMetricsObserver: MetricsObserver {\n    func metricsDidReceive(metrics: [AnyHashable: Any]) {\n        logger.info(msg: \"Media metrics have been received: \\(metrics)\")\n    }\n\n    meetingSession.audioVideo.addMetricsObserver(observer: self)\n}\n```\n\n### Data Message\n\n#### Use case 22. Add an observer to receive data message.\n\nYou can receive real-time messages from multiple topics after starting the meeting session.\n\n```swift\nclass MyDataMessageObserver: DataMessageObserver {\n    let dataMessageTopic = \"chat\"\n    // A throttled message is returned by backend from local sender\n    func dataMessageDidReceived(dataMessage: DataMessage) {\n        logger.info(msg: \"\\(dataMessage.timestampMs) \\(dataMessage.text()) \\(dataMessage.senderAttendeeId)\")\n    }\n\n    // You can also subscribe to multiple topics.\n    meetingSession.audioVideo.addRealtimeDataMessageObserver(topic: dataMessageTopic, observer: self)\n}\n```\n\n#### Use case 23. Send data message.\n\nYou can send real time message to any topic, to which the observers that have subscribed will be notified.\n\n> Note: Topic needs to be alpha-numeric and it can include hyphen and underscores. Data cannot exceed 2kb and lifetime is optional but positive integer.\n\n```swift\nlet dataMessageTopic = \"chat\"\nlet dataMessageLifetimeMs = 1000\n\ndo {\n    // Send \"Hello Chime\" to any subscribers who are listening to \"chat\" topic with 1 seconds of lifetime\n    try meetingSession\n        .audioVideo\n        .realtimeSendDataMessage(topic: dataMessageTopic,\n                                 data: \"Hello Chime\",\n                                 lifetimeMs: dataMessageLifetimeMs)\n} catch let err as SendDataMessageError {\n    logger.error(msg: \"Failed to send message! \\(err)\")\n} catch {\n    logger.error(msg: \"Unknown error \\(error.localizedDescription)\")\n}\n```\n\n### Stopping a session\n\n> Note: Make sure to remove all the observers and release resources you have added to avoid any memory leaks.\n\n#### Use case 24. Stop a session.\n\n```swift\nclass MyAudioVideoObserver: AudioVideoObserver {\n    func audioSessionDidStopWithStatus(sessionStatus: MeetingSessionStatus) {\n        // This is where meeting ended.\n        // You can do some clean up work here.\n    }\n\n    func videoSessionDidStopWithStatus(sessionStatus: MeetingSessionStatus) {\n        // This will be invoked as well.\n    }\n\n    meetingSession.audioVideo.addAudioVideoObserver(observer: self)\n    meetingSession.audioVideo.stop()\n}\n```\n\n### Amazon Voice Focus\n\nAmazon Voice Focus reduces the background noise in the meeting for better meeting experience. For more details, see [Amazon Voice Focus](https://github.com/aws/amazon-chime-sdk-ios/blob/master/guides/api_overview.md#11-using-amazon-voice-focus-optional).\n\n#### Use case 25. Enable/Disable Amazon Voice Focus.\n\n```swift\nlet enabled = audioVideo.realtimeSetVoiceFocusEnabled(enabled: true) // enabling Amazon Voice Focus successful\n\nlet disabled = audioVideo.realtimeSetVoiceFocusEnabled(enabled: false) // disabling Amazon Voice Focus successful\n```\n\n### Custom Video Source\n\nCustom video source allows you to control the video, such as applying a video filter. For more details, see [Custom Video](https://github.com/aws/amazon-chime-sdk-ios/blob/master/guides/custom_video.md).\n\n### Redundant Audio\n\nStarting from version 0.23.3, the SDK starts sending redundant audio data to our servers on detecting packet loss\nto help reduce its effect on audio quality. Redundant audio packets are only sent out for packets containing active\naudio, i.e. speech or music. This may increase the bandwidth consumed by audio to up to 3 times the normal amount\ndepending on the amount of packet loss detected. The SDK will automatically stop sending redundant data if it hasn't\ndetected any packet loss for 5 minutes.\n\nIf you need to disable this feature, you can do so through the AudioVideoConfiguration before starting the session.\n\n```swift\nmeetingSession.audioVideo.start(AudioVideoConfiguration(enableAudioRedundancy: false))\n```\n\nWhile there is an option to disable the feature, we recommend keeping it enabled for improved audio quality.\nOne possible reason to disable it might be if your customers have very strict bandwidth limitations.\n\n\n## Frequently Asked Questions\n\nRefer to [General FAQ](https://aws.github.io/amazon-chime-sdk-js/modules/faqs.html) for Amazon Chime SDK.\n\n### Debugging\n\n#### How can I get Amazon Chime SDK logs for debugging?\nApplications can get logs from Chime SDK by passing instances of Logger when creating [MeetingSession](https://aws.github.io/amazon-chime-sdk-ios/Protocols/MeetingSession.html). Amazon Chime SDK has some default implementations of logger that your application can use, such as [ConsoleLogger](https://aws.github.io/amazon-chime-sdk-ios/Classes/ConsoleLogger.html) which logs into console. `ConsoleLogger` is set to `INFO` level as default. Therefore, in order to get all logs, including media logs, create logger by following:\n```swift\nlogger = ConsoleLogger(name: \"logger\", level: .DEFAULT)\n```\n\n## Notice\nYou and your end users are responsible for all Content (including any images) uploaded for use with background replacement, and must ensure that such Content does not violate the law, infringe or misappropriate the rights of any third party, or otherwise violate a material term of your agreement with Amazon (including the documentation, the AWS Service Terms, or the Acceptable Use Policy).\n\n---\n\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n", "release_dates": ["2024-02-16T20:22:04Z", "2023-12-21T18:22:13Z", "2023-10-02T20:55:18Z", "2023-06-27T23:31:33Z", "2023-05-16T18:50:51Z", "2023-03-17T00:39:46Z", "2023-01-27T18:59:20Z", "2022-12-03T01:47:49Z", "2022-11-16T21:50:18Z", "2022-10-21T18:03:12Z", "2022-09-08T22:29:56Z", "2022-08-12T23:54:14Z", "2022-07-28T23:59:25Z", "2022-07-14T23:02:49Z", "2022-06-30T19:03:02Z", "2022-06-18T00:10:15Z", "2022-06-03T22:21:39Z", "2022-05-19T21:03:46Z", "2022-05-12T17:53:43Z", "2022-04-21T23:21:59Z", "2022-04-08T00:39:02Z", "2022-03-21T21:37:02Z", "2022-03-11T01:17:36Z", "2022-02-25T01:46:50Z", "2022-02-10T23:57:31Z", "2021-12-21T22:56:19Z", "2021-11-01T21:53:03Z", "2021-10-15T00:02:03Z", "2021-09-30T22:21:05Z", "2021-07-21T22:15:33Z"]}, {"name": "amazon-chime-sdk-ios-no-video-codecs-spm", "description": null, "language": "Swift", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# amazon-chime-sdk-ios-no-video-codecs-spm\n# Swift Package Manager support for Amazon Chime SDK for iOS\n\nThis repository enables Swift Package Manager support for the [Amazon Chime Mobile SDK for iOS](https://github.com/aws/amazon-chime-sdk-ios) No Video Codecs by vending a Manifest file (`Package.swift`) that links to binary targets for the SDKs.\n\n## Adding Amazon Chime SDK No Video Codecs for iOS via Swift Package Manager\n\n1. Open your project in Xcode\n\n2. Go to **File** > **Swift Packages** > **Add Package Dependency...**\n\n3. In the field **Enter package repository URL**, enter \"https://github.com/aws/amazon-chime-sdk-ios-no-video-codecs-spm\"\n\n4. Pick the latest version and click **Next**.\n\n5. Choose the packages required for your project and click **Finish**\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n## Notice\n\nYou and your end users are responsible for all Content (including any images) uploaded for use with background replacement, and must ensure that such Content does not violate the law, infringe or misappropriate the rights of any third party, or otherwise violate a material term of your agreement with Amazon (including the documentation, the AWS Service Terms, or the Acceptable Use Policy).\n", "release_dates": ["2024-02-21T23:27:24Z", "2024-02-09T01:34:47Z"]}, {"name": "amazon-chime-sdk-ios-spm", "description": null, "language": "Swift", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# amazon-chime-sdk-ios-spm\n# Swift Package Manager support for Amazon Chime SDK for iOS\n\nThis repository enables Swift Package Manager support for the [Amazon Chime Mobile SDK for iOS](https://github.com/aws/amazon-chime-sdk-ios) by vending a Manifest file (`Package.swift`) that links to binary targets for the SDKs.\n\n## Adding Amazon Chime SDK for iOS via Swift Package Manager\n\n1. Open your project in Xcode\n\n2. Go to **File** > **Swift Packages** > **Add Package Dependency...**\n\n3. In the field **Enter package repository URL**, enter \"https://github.com/aws/amazon-chime-sdk-ios-spm\"\n\n4. Pick the latest version and click **Next**.\n\n5. Choose the packages required for your project and click **Finish**\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n## Notice\n\nYou and your end users are responsible for all Content (including any images) uploaded for use with background replacement, and must ensure that such Content does not violate the law, infringe or misappropriate the rights of any third party, or otherwise violate a material term of your agreement with Amazon (including the documentation, the AWS Service Terms, or the Acceptable Use Policy).\n", "release_dates": ["2024-02-21T23:23:29Z", "2024-01-05T00:15:23Z", "2023-10-16T22:14:48Z", "2023-05-19T00:54:09Z"]}, {"name": "amazon-chime-sdk-js", "description": "A JavaScript client library for integrating multi-party communications powered by the Amazon Chime service.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon Chime SDK for JavaScript\n\n[Amazon Chime SDK Project Board](https://github.com/orgs/aws/projects/12)\n\n[Amazon Chime SDK React Components](https://github.com/aws/amazon-chime-sdk-component-library-react)\n\n<a href=\"https://www.npmjs.com/package/amazon-chime-sdk-js\"><img src=\"https://img.shields.io/npm/v/amazon-chime-sdk-js?style=flat-square\"></a>\n<a href=\"https://github.com/aws/amazon-chime-sdk-js/actions?query=workflow%3A%22Deploy+Demo+App+Workflow%22\"><img src=\"https://github.com/aws/amazon-chime-sdk-js/workflows/Deploy%20Demo%20App%20Workflow/badge.svg\"></a>\n\n## Build video calling, audio calling, messaging, and screen sharing applications powered by the Amazon Chime SDK\n\nThe Amazon Chime SDK is a set of real-time communications components that developers can\nuse to quickly add messaging, audio, video, and screen sharing capabilities to their web or\nmobile applications.\n\nDevelopers can build on AWS's global communications infrastructure to deliver\nengaging experiences in their applications. For example, they can add video to a\nhealth application so patients can consult remotely with doctors on health\nissues, or create customized audio prompts for integration with the public\ntelephone network.\n\nThe Amazon Chime SDK for JavaScript works by connecting to meeting session\nresources that you create in your AWS account. The SDK has everything\nyou need to build custom calling and collaboration experiences in your\nweb application, including methods to configure meeting sessions, list and\nselect audio and video devices, start and stop screen share and screen share\nviewing, receive callbacks when media events such as volume changes occur, and\ncontrol meeting features such as audio mute and video tile bindings.\n\nIf you are building a React application, consider using the [Amazon Chime SDK React Component Library](https://github.com/aws/amazon-chime-sdk-component-library-react) that supplies client-side state management and reusable UI components for common web interfaces used in audio and video conferencing applications. Amazon Chime also offers [Amazon Chime SDK for iOS](https://github.com/aws/amazon-chime-sdk-ios) and [Amazon Chime SDK for Android](https://github.com/aws/amazon-chime-sdk-android) for native mobile application development.\n\nThe [Amazon Chime SDK Project Board](https://github.com/orgs/aws/projects/12) captures the status of community feature requests across all our repositories. The descriptions of the columns on the board are captured in this [guide](https://aws.github.io/amazon-chime-sdk-js/modules/projectboard.html).\n\n## Resources\n\n- [Amazon Chime SDK Overview](https://aws.amazon.com/chime/chime-sdk/)\n- [Pricing](https://aws.amazon.com/chime/chime-sdk/pricing/)\n- [Supported Browsers](https://docs.aws.amazon.com/chime-sdk/latest/dg/meetings-sdk.html#mtg-browsers)\n- [Getting Started Guides](guides/20_Builders_Journey.md)\n- [Developer Guide](https://docs.aws.amazon.com/chime-sdk/latest/dg/meetings-sdk.html)\n- [Control Plane API Reference](https://docs.aws.amazon.com/chime-sdk/latest/APIReference/welcome.html)\n- [Frequently Asked Questions (FAQ)](https://aws.github.io/amazon-chime-sdk-js/modules/faqs.html)\n\n## Blog posts\n\nIn addition to the below, here is a list of [all blog posts about the Amazon Chime SDK](https://aws.amazon.com/blogs/business-productivity/tag/amazon-chime-sdk/).\n\n### High level\n\n- [High Level Architecture \u2014 Building a Meeting Application With the Amazon Chime SDK](https://aws.amazon.com/blogs/business-productivity/building-a-meeting-application-using-the-amazon-chime-sdk/)\n- [Understanding security in Amazon Chime Application and SDK](https://aws.amazon.com/blogs/business-productivity/understanding-security-in-the-amazon-chime-application-and-sdk/)\n\n### Frontend\n\n- [Transforming Audio and Shared Content](https://aws.amazon.com/blogs/business-productivity/transforming-audio-and-shared-content-in-the-amazon-chime-sdk-for-javascript/)\n- [Quickly Launch an Amazon Chime SDK Application With AWS Amplify](https://aws.amazon.com/blogs/business-productivity/quickly-launch-an-amazon-chime-sdk-application-with-aws-amplify/)\n\n### Full stack and PSTN\n\n- [Capturing Amazon Chime SDK Meeting Content](https://aws.amazon.com/blogs/business-productivity/capture-amazon-chime-sdk-meetings-using-media-capture-pipelines/)\n- [Monitoring and Troubleshooting With Amazon Chime SDK Meeting Events](https://aws.amazon.com/blogs/business-productivity/monitoring-and-troubleshooting-with-amazon-chime-sdk-meeting-events/)\n- [Build Meetings features into your Amazon Chime SDK messaging application](https://aws.amazon.com/blogs/business-productivity/build-meeting-features-into-your-amazon-chime-sdk-messaging-application/)\n- [Using the Amazon Chime SDK to Create Automated Outbound Call Notifications](https://aws.amazon.com/blogs/business-productivity/using-the-amazon-chime-sdk-to-create-automated-outbound-call-notifications/)\n- [Building voice menus and call routing with the Amazon Chime SDK](https://aws.amazon.com/blogs/business-productivity/building-voice-menus-and-call-routing-with-the-amazon-chime-sdk/)\n\n### Messaging\n\n- [Use channel flows to remove profanity and sensitive content from messages in Amazon Chime SDK messaging](https://aws.amazon.com/blogs/business-productivity/use-channel-flows-to-remove-profanity-and-sensitive-content-from-messages-in-amazon-chime-sdk-messaging/)\n- [Automated Moderation and Sentiment Analysis Blog (example using Kinesis Data Streams)](https://aws.amazon.com/blogs/business-productivity/automated-moderation-and-sentiment-analysis-with-amazon-chime-sdk-messaging)\n- [Build chat applications in iOS and Android with Amazon Chime SDK messaging](https://aws.amazon.com/blogs/business-productivity/build-chat-applications-in-ios-and-android-with-amazon-chime-sdk-messaging/)\n- [Building chat features into your application with Amazon Chime SDK messaging](https://aws.amazon.com/blogs/business-productivity/build-chat-features-into-your-application-with-amazon-chime-sdk-messaging/)\n- [Integrate your Identity Provider with Amazon Chime SDK Messaging](https://aws.amazon.com/blogs/business-productivity/integrate-your-identity-provider-with-amazon-chime-sdk-messaging/)\n- [Creating Read-Only Chat Channels for Announcements](https://aws.amazon.com/blogs/business-productivity/creating-read-only-chat-channels-for-announcements-with-amazon-chime-sdk-messaging/)\n- [Real-time Collaboration Using Amazon Chime SDK messaging](https://aws.amazon.com/blogs/business-productivity/real-time-collaboration-using-amazon-chime-sdk-messaging/)\n- [Building a Live Streaming Chat Application](https://aws.amazon.com/blogs/business-productivity/build-a-live-streaming-chat-application-using-amazon-ivs-and-amazon-chime-sdk)\n\n### Media Pipelines\n\n- [Capture Amazon Chime SDK Meetings Using Media Capture Pipelines](https://aws.amazon.com/blogs/business-productivity/capture-amazon-chime-sdk-meetings-using-media-capture-pipelines/)\n- [Amazon Chime SDK launches live connector for streaming](https://aws.amazon.com/blogs/business-productivity/amazon-chime-sdk-launches-live-connector-for-streaming/)\n\n### Webinars and videos\n\n- [Webinar: Creating Classroom Experiences Using the Amazon Chime SDK](https://www.youtube.com/watch?v=S8T-0xfvXJ8)\n\n## JavaScript SDK Guides\n\nThe following developer guides cover specific topics for a technical audience.\n\n- [API Overview](https://aws.github.io/amazon-chime-sdk-js/modules/apioverview.html)\n- [Frequently Asked Questions (FAQ)](https://aws.github.io/amazon-chime-sdk-js/modules/faqs.html)\n- [Content Share](https://aws.github.io/amazon-chime-sdk-js/modules/contentshare.html)\n- [Quality, Bandwidth, and Connectivity](https://aws.github.io/amazon-chime-sdk-js/modules/qualitybandwidth_connectivity.html)\n- [Simulcast](https://aws.github.io/amazon-chime-sdk-js/modules/simulcast.html)\n- [Meeting Events](https://aws.github.io/amazon-chime-sdk-js/modules/meetingevents.html)\n- [Integrating Amazon Voice Focus and Echo Reduction Into Your Application](https://aws.github.io/amazon-chime-sdk-js/modules/amazonvoice_focus.html)\n- [Adding Frame-By-Frame Processing to an Outgoing Video Stream](https://aws.github.io/amazon-chime-sdk-js/modules/videoprocessor.html)\n- [Adding Background Filtering to an Outgoing Video Stream](https://aws.github.io/amazon-chime-sdk-js/modules/backgroundfilter_videofx_processor.html)\n- [Adapting Video to Limited Bandwidth Using a Priority-Based Video Downlink Policy](https://aws.github.io/amazon-chime-sdk-js/modules/prioritybased_downlink_policy.html)\n- [Client Event Ingestion](https://aws.github.io/amazon-chime-sdk-js/modules/clientevent_ingestion.html)\n- [Content Security Policy](https://aws.github.io/amazon-chime-sdk-js/modules/contentsecurity_policy.html)\n- [Managing Video Quality for Different Video Layouts](https://aws.github.io/amazon-chime-sdk-js/modules/videolayout.html)\n\n## Migration Guides\n\n- [Migration from V1.0 to V2.0](https://aws.github.io/amazon-chime-sdk-js/modules/migrationto_2_0.html)\n- [Migration from V2.0 to V3.0](https://aws.github.io/amazon-chime-sdk-js/modules/migrationto_3_0.html)\n\n## Developer Guides\n\nThe following developer guides cover the Amazon Chime SDK more broadly.\n\n- [Messaging developer guide](https://docs.aws.amazon.com/chime-sdk/latest/dg/using-the-messaging-sdk.html)\n- [Media Pipelines developer guide](https://docs.aws.amazon.com/chime-sdk/latest/dg/media-pipelines.html)\n\n## Examples\n\n- [Amazon Chime SDK Samples](https://github.com/aws-samples/amazon-chime-sdk) \u2014 Amazon Chime SDK Samples repository\n- [Meeting Demo](https://github.com/aws/amazon-chime-sdk-js/tree/main/demos/browser) \u2014 A browser\n  meeting application with a local server\n- [Serverless Meeting Demo](https://github.com/aws/amazon-chime-sdk-js/tree/main/demos/serverless) \u2014 A self-contained serverless meeting application\n- [Single JS](https://github.com/aws-samples/amazon-chime-sdk/tree/main/utils/singlejs) \u2014 A script to bundle the SDK into a single `.js` file\n- [Transcription and Media Capture Demo](https://github.com/aws-samples/amazon-chime-media-capture-pipeline-demo) - A demo to demonstrate transcription and media capture capabilities\n- [Virtual Classroom](https://aws.amazon.com/blogs/business-productivity/building-a-virtual-classroom-application-using-the-amazon-chime-sdk/) \u2014 An online classroom built with Electron and React\n- [Live Events](https://aws.amazon.com/blogs/opensource/how-to-deploy-a-live-events-solution-built-with-the-amazon-chime-sdk/) \u2014 Interactive live events solution\n- [Amazon Chime SDK Smart Video Sending Demo](https://aws.amazon.com/blogs/business-productivity/amazon-chime-sdk-smart-video-sending-demo/) \u2014 Demo showcasing how to dynamically display up to 25 video tiles from a pool of up to 250 meeting attendees\n- [Amazon Chime SDK and Amazon Connect Integration](https://aws.amazon.com/blogs/business-productivity/build-a-video-contact-center-with-amazon-connect-and-amazon-chime-sdk/) \u2014 Build a video contact center with Amazon Connect and Amazon Chime SDK\n- [Device Integration](https://aws.amazon.com/blogs/business-productivity/using-the-amazon-chime-sdk-for-3rd-party-devices/) \u2014 Using the Amazon Chime SDK for 3rd party devices\n- [Messaging](https://aws.amazon.com/blogs/business-productivity/build-chat-features-into-your-application-with-amazon-chime-sdk-messaging/) \u2014 Build chat features into your application with Amazon Chime SDK messaging\n- [Load Testing Applications](https://aws.amazon.com/blogs/business-productivity/load-testing-applications-built-with-the-amazon-chime-sdk/) \u2014 A tool to load test audio-video communication applications\n\n### PSTN Audio Examples\n\n- [PSTN Dial In](https://github.com/aws-samples/chime-sipmediaapplication-samples) \u2014 Add PSTN dial-in capabilities to your Amazon Chime SDK Meeting using SIP media application\n- [Outbound Call Notifications](https://github.com/aws-samples/amazon-chime-sma-outbound-call-notifications) \u2014 Send meeting reminders with SIP media application and get real time results back\n- [Update In-Progress Call](https://github.com/aws-samples/amazon-chime-sma-update-call) - Update an in-progress SIP media application call via API call\n\n## Troubleshooting and Support\n\nReview the resources given in the README and use our [client documentation](https://aws.github.io/amazon-chime-sdk-js/) for guidance on how to develop on the Chime SDK for JavaScript. Additionally, search our [issues database](https://github.com/aws/amazon-chime-sdk-js/issues) and [FAQs](https://aws.github.io/amazon-chime-sdk-js/modules/faqs.html) to see if your issue is already addressed. If not please cut us an [issue](https://github.com/aws/amazon-chime-sdk-js/issues/new/choose) using the provided templates.\n\nThe blog post [Monitoring and Troubleshooting With Amazon Chime SDK Meeting Events](https://aws.amazon.com/blogs/business-productivity/monitoring-and-troubleshooting-with-amazon-chime-sdk-meeting-events/) goes into detail about how to use meeting events to troubleshoot your application by logging to Amazon CloudWatch.\n\nIf you have more questions, or require support for your business, you can reach out to [AWS Customer support](https://pages.awscloud.com/GLOBAL-aware-GC-Amazon-Chime-SDK-2020-reg.html). You can review our support plans [here](https://aws.amazon.com/premiumsupport/plans/?nc=sn&loc=1).\n\n## WebRTC Resources\n\nThe Amazon Chime SDK for JavaScript uses WebRTC, the real-time communication API supported in most modern browsers. Here are some general resources on WebRTC.\n\n- [WebRTC Basics](https://www.html5rocks.com/en/tutorials/webrtc/basics/)\n- [WebRTC Org - Getting started, presentation, samples, tutorials, books and more resources](https://webrtc.github.io/webrtc-org/start/)\n- [High Performance Browser Networking - WebRTC (Browser APIs and Protocols)](https://hpbn.co/webrtc/)\n- [MDN - WebRTC APIs](https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API)\n\n## Installation\n\nMake sure you have Node.js version 18 or higher. Node 20 is recommended and supported.\n\nTo add the Amazon Chime SDK for JavaScript into an existing application,\ninstall the package directly from npm:\n\n```\nnpm install amazon-chime-sdk-js --save\n```\n\nNote that the Amazon Chime SDK for JavaScript targets ES2015, which is fully compatible with\nall supported browsers.\n\n## Setup\n\n### Meeting session\n\nCreate a meeting session in your client application.\n\n```js\nimport {\n  ConsoleLogger,\n  DefaultDeviceController,\n  DefaultMeetingSession,\n  LogLevel,\n  MeetingSessionConfiguration\n} from 'amazon-chime-sdk-js';\n\nconst logger = new ConsoleLogger('MyLogger', LogLevel.INFO);\nconst deviceController = new DefaultDeviceController(logger);\n\n// You need responses from server-side Chime API. See below for details.\nconst meetingResponse = /* The response from the CreateMeeting API action */;\nconst attendeeResponse = /* The response from the CreateAttendee or BatchCreateAttendee API action */;\nconst configuration = new MeetingSessionConfiguration(meetingResponse, attendeeResponse);\n\n// In the usage examples below, you will use this meetingSession object.\nconst meetingSession = new DefaultMeetingSession(\n  configuration,\n  logger,\n  deviceController\n);\n```\n\n#### Getting responses from your server application\n\nYou can use an AWS SDK, the AWS Command Line Interface (AWS CLI), or the REST API\nto make API calls. In this section, you will use the AWS SDK for JavaScript in your server application, e.g. Node.js.\nSee [Amazon Chime SDK API Reference](https://docs.aws.amazon.com/chime-sdk/latest/APIReference/welcome.html) for more information.\n\n> \u26a0\ufe0f The server application does not require the Amazon Chime SDK for JavaScript.\n\n```js\nconst AWS = require('aws-sdk');\nconst { v4: uuid } = require('uuid');\n\n// You must use \"us-east-1\" as the region for Chime API and set the endpoint.\nconst chime = new AWS.ChimeSDKMeetings({ region: 'us-east-1' });\n\nconst meetingResponse = await chime\n  .createMeeting({\n    ClientRequestToken: uuid(),\n    MediaRegion: 'us-west-2', // Specify the region in which to create the meeting.\n  })\n  .promise();\n\nconst attendeeResponse = await chime\n  .createAttendee({\n    MeetingId: meetingResponse.Meeting.MeetingId,\n    ExternalUserId: uuid(), // Link the attendee to an identity managed by your application.\n  })\n  .promise();\n```\n\nNow securely transfer the `meetingResponse` and `attendeeResponse` objects to your client application.\nThese objects contain all the information needed for a client application using the Amazon Chime SDK for JavaScript to join the meeting.\n\nThe value of the MediaRegion parameter in the createMeeting() should ideally be set to the one of the media regions which is closest to the user creating a meeting. An implementation can be found under the topic 'Choosing the nearest media Region' in the [Amazon Chime SDK Media Regions documentation](https://docs.aws.amazon.com/chime-sdk/latest/dg/chime-sdk-meetings-regions.html).\n\n### Messaging session\n\nCreate a messaging session in your client application to receive messages from Amazon Chime SDK for Messaging.\n\n```js\nimport { ChimeSDKMessagingClient } from '@aws-sdk/client-chime-sdk-messaging';\n\nimport {\n  ConsoleLogger,\n  DefaultMessagingSession,\n  LogLevel,\n  MessagingSessionConfiguration,\n} from 'amazon-chime-sdk-js';\n\nconst logger = new ConsoleLogger('SDK', LogLevel.INFO);\n\n// You will need AWS credentials configured before calling AWS or Amazon Chime APIs.\nconst chime = new ChimeSDKMessagingClient({ region: 'us-east-1'});\n\nconst userArn = /* The userArn */;\nconst sessionId = /* The sessionId */;\nconst configuration = new MessagingSessionConfiguration(userArn, sessionId, undefined, chime);\nconst messagingSession = new DefaultMessagingSession(configuration, logger);\n```\n\nIf you would like to enable prefetch feature when connecting to a messaging session, you can follow the code below.\nPrefetch feature will send out CHANNEL_DETAILS event upon websocket connection, which includes information about channel,\nchannel messages, channel memberships etc. Prefetch sort order can be adjusted with `prefetchSortBy`, setting it to either\n`unread` (default value if not set) or `lastMessageTimestamp`\n\n```js\nconfiguration.prefetchOn = Prefetch.Connect;\nconfiguration.prefetchSortBy = PrefetchSortBy.Unread;\n```\n\n## Building and testing\n\n```\ngit fetch --tags https://github.com/aws/amazon-chime-sdk-js\nnpm run build\nnpm run test\n```\n\nAfter running `npm run test` the first time, you can use `npm run test:fast` to speed up the test suite.\n\nTags are fetched in order to correctly generate versioning metadata.\n\nTo view code coverage results open `coverage/index.html` in your browser after running `npm run test`.\n\nIf you run `npm run test` and the tests are running but the coverage report is not getting generated then you might have a resource clean up issue. In Mocha v4.0.0 or newer the implementation was changed so that the Mocha processes will not force exit when the test run is complete.\n\nFor example, if you have a `DefaultVideoTransformDevice` in your unit test then you must call `await device.stop();` to clean up the resources and not run into this issue. You can also look into the usage of `done();` in the [Mocha documentation](https://mochajs.org/#asynchronous-code).\n\n## Generating the documentation\n\nTo generate JavaScript API reference documentation run:\n\n```\nnpm run build\nnpm run doc\n```\n\nThen open `docs/index.html` in your browser.\n\n## Reporting a suspected vulnerability\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our\n[vulnerability reporting page](https://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public GitHub issue.\n\n## Usage\n\n- [Device](#device)\n- [Starting a session](#starting-a-session)\n- [Audio](#audio)\n- [Video](#video)\n- [Screen and content share](#screen-and-content-share)\n- [Attendees](#attendees)\n- [Monitoring and alerts](#monitoring-and-alerts)\n- [Stopping a session](#stopping-a-session)\n- [Meeting readiness checker](#meeting-readiness-checker)\n- [Selecting an Audio Profile](#selecting-an-audio-profile)\n- [Starting a Messaging Session](#starting-a-messaging-session)\n- [Providing application metadata](#providing-application-metadata)\n\n### Device\n\n> Note: Before starting a session, you need to choose your microphone, speaker, and camera.\n\n**Use case 1.** List audio input, audio output, and video input devices. The browser will ask for microphone\u00a0and camera permissions.\n\nWith the `forceUpdate` parameter set to true, cached device information is discarded and updated after the device label trigger is called. In some cases, builders need to delay the triggering of permission dialogs, e.g., when joining a meeting in view-only mode, and then later be able to trigger a permission prompt in order to show device labels; specifying `forceUpdate` allows this to occur.\n\n```js\nconst audioInputDevices = await meetingSession.audioVideo.listAudioInputDevices();\nconst audioOutputDevices = await meetingSession.audioVideo.listAudioOutputDevices();\nconst videoInputDevices = await meetingSession.audioVideo.listVideoInputDevices();\n\n// An array of MediaDeviceInfo objects\naudioInputDevices.forEach(mediaDeviceInfo => {\n  console.log(`Device ID: ${mediaDeviceInfo.deviceId} Microphone: ${mediaDeviceInfo.label}`);\n});\n```\n\n**Use case 2.** Choose audio input and audio output devices by passing the `deviceId` of a `MediaDeviceInfo` object.\nNote that you need to call `listAudioInputDevices` and `listAudioOutputDevices` first.\n\n```js\nconst audioInputDeviceInfo = /* An array item from meetingSession.audioVideo.listAudioInputDevices */;\nawait meetingSession.audioVideo.startAudioInput(audioInputDeviceInfo.deviceId);\n\nconst audioOutputDeviceInfo = /* An array item from meetingSession.audioVideo.listAudioOutputDevices */;\nawait meetingSession.audioVideo.chooseAudioOutput(audioOutputDeviceInfo.deviceId);\n```\n\n**Use case 3.** Choose a video input device by passing the `deviceId` of a `MediaDeviceInfo` object.\nNote that you need to call `listVideoInputDevices` first.\n\nIf there is an LED light next to the attendee's camera, it will be turned on indicating that it is now capturing from the camera.\nYou probably want to choose a video input device when you start sharing your video.\n\n```js\nconst videoInputDeviceInfo = /* An array item from meetingSession.audioVideo.listVideoInputDevices */;\nawait meetingSession.audioVideo.startVideoInput(videoInputDeviceInfo.deviceId);\n\n// Stop video input. If the previously chosen camera has an LED light on,\n// it will turn off indicating the camera is no longer capturing.\nawait meetingSession.audioVideo.stopVideoInput();\n```\n\n**Use case 4.** Add a device change observer to receive the updated device list.\nFor example, when you pair Bluetooth headsets with your computer, `audioInputsChanged` and `audioOutputsChanged` are called\nwith the device list including headsets.\n\nYou can use the `audioInputMuteStateChanged` callback to track the underlying\nhardware mute state on browsers and operating systems that support that.\n\n```js\nconst observer = {\n  audioInputsChanged: freshAudioInputDeviceList => {\n    // An array of MediaDeviceInfo objects\n    freshAudioInputDeviceList.forEach(mediaDeviceInfo => {\n      console.log(`Device ID: ${mediaDeviceInfo.deviceId} Microphone: ${mediaDeviceInfo.label}`);\n    });\n  },\n\n  audioOutputsChanged: freshAudioOutputDeviceList => {\n    console.log('Audio outputs updated: ', freshAudioOutputDeviceList);\n  },\n\n  videoInputsChanged: freshVideoInputDeviceList => {\n    console.log('Video inputs updated: ', freshVideoInputDeviceList);\n  },\n\n  audioInputMuteStateChanged: (device, muted) => {\n    console.log('Device', device, muted ? 'is muted in hardware' : 'is not muted');\n  },\n};\n\nmeetingSession.audioVideo.addDeviceChangeObserver(observer);\n```\n\n### Starting a session\n\n**Use case 5.** Start a session. To hear audio, you need to bind a device and stream to an `<audio>` element.\nOnce the session has started, you can talk and listen to attendees.\nMake sure you have chosen your microphone and speaker (See the \"Device\" section), and at least one other attendee has joined the session.\n\n```js\nconst audioElement = /* HTMLAudioElement object e.g. document.getElementById('audio-element-id') */;\nmeetingSession.audioVideo.bindAudioElement(audioElement);\n\nconst observer = {\n  audioVideoDidStart: () => {\n    console.log('Started');\n  }\n};\n\nmeetingSession.audioVideo.addObserver(observer);\n\nmeetingSession.audioVideo.start();\n```\n\n**Use case 6.** Add an observer to receive session lifecycle events: connecting, start, and stop.\n\n> Note: You can remove an observer by calling `meetingSession.audioVideo.removeObserver(observer)`.\n> In a component-based architecture (such as React, Vue, and Angular), you may need to add an observer\n> when a component is mounted, and remove it when unmounted.\n\n```js\nconst observer = {\n  audioVideoDidStart: () => {\n    console.log('Started');\n  },\n  audioVideoDidStop: sessionStatus => {\n    // See the \"Stopping a session\" section for details.\n    console.log('Stopped with a session status code: ', sessionStatus.statusCode());\n  },\n  audioVideoDidStartConnecting: reconnecting => {\n    if (reconnecting) {\n      // e.g. the WiFi connection is dropped.\n      console.log('Attempting to reconnect');\n    }\n  },\n};\n\nmeetingSession.audioVideo.addObserver(observer);\n```\n\n### Audio\n\n> Note: So far, you've added observers to receive device and session lifecycle events.\n> In the following use cases, you'll use the real-time API methods to send and receive volume indicators and control mute state.\n\n**Use case 7.** Mute and unmute an audio input.\n\n```js\n// Mute\nmeetingSession.audioVideo.realtimeMuteLocalAudio();\n\n// Unmute\nconst unmuted = meetingSession.audioVideo.realtimeUnmuteLocalAudio();\nif (unmuted) {\n  console.log('Other attendees can hear your audio');\n} else {\n  // See the realtimeSetCanUnmuteLocalAudio use case below.\n  console.log('You cannot unmute yourself');\n}\n```\n\n**Use case 8.** To check whether the local microphone is muted, use this method rather than keeping track of your own mute state.\n\n```js\nconst muted = meetingSession.audioVideo.realtimeIsLocalAudioMuted();\nif (muted) {\n  console.log('You are muted');\n} else {\n  console.log('Other attendees can hear your audio');\n}\n```\n\n**Use case 9.** Disable unmute. If you want to prevent users from unmuting themselves (for example during a presentation), use these methods rather than keeping track of your own can-unmute state.\n\n```js\nmeetingSession.audioVideo.realtimeSetCanUnmuteLocalAudio(false);\n\n// Optional: Force mute.\nmeetingSession.audioVideo.realtimeMuteLocalAudio();\n\nconst unmuted = meetingSession.audioVideo.realtimeUnmuteLocalAudio();\nconsole.log(`${unmuted} is false. You cannot unmute yourself`);\n```\n\n**Use case 10.** Subscribe to volume changes of a specific attendee. You can use this to build a real-time volume indicator UI.\n\n```js\nimport { DefaultModality } from 'amazon-chime-sdk-js';\n\n// This is your attendee ID. You can also subscribe to another attendee's ID.\n// See the \"Attendees\" section for an example on how to retrieve other attendee IDs\n// in a session.\nconst presentAttendeeId = meetingSession.configuration.credentials.attendeeId;\n\nmeetingSession.audioVideo.realtimeSubscribeToVolumeIndicator(\n  presentAttendeeId,\n  (attendeeId, volume, muted, signalStrength) => {\n    const baseAttendeeId = new DefaultModality(attendeeId).base();\n    if (baseAttendeeId !== attendeeId) {\n      // See the \"Screen and content share\" section for details.\n      console.log(`The volume of ${baseAttendeeId}'s content changes`);\n    }\n\n    // A null value for any field means that it has not changed.\n    console.log(`${attendeeId}'s volume data: `, {\n      volume, // a fraction between 0 and 1\n      muted, // a boolean\n      signalStrength, // 0 (no signal), 0.5 (weak), 1 (strong)\n    });\n  }\n);\n```\n\n**Use case 11.** Subscribe to mute or signal strength changes of a specific attendee. You can use this to build UI for only mute or only signal strength changes.\n\n```js\n// This is your attendee ID. You can also subscribe to another attendee's ID.\n// See the \"Attendees\" section for an example on how to retrieve other attendee IDs\n// in a session.\nconst presentAttendeeId = meetingSession.configuration.credentials.attendeeId;\n\n// To track mute changes\nmeetingSession.audioVideo.realtimeSubscribeToVolumeIndicator(\n  presentAttendeeId,\n  (attendeeId, volume, muted, signalStrength) => {\n    // A null value for volume, muted and signalStrength field means that it has not changed.\n    if (muted === null) {\n      // muted state has not changed, ignore volume and signalStrength changes\n      return;\n    }\n\n    // mute state changed\n    console.log(`${attendeeId}'s mute state changed: `, {\n      muted, // a boolean\n    });\n  }\n);\n\n// To track signal strength changes\nmeetingSession.audioVideo.realtimeSubscribeToVolumeIndicator(\n  presentAttendeeId,\n  (attendeeId, volume, muted, signalStrength) => {\n    // A null value for volume, muted and signalStrength field means that it has not changed.\n    if (signalStrength === null) {\n      // signalStrength has not changed, ignore volume and muted changes\n      return;\n    }\n\n    // signal strength changed\n    console.log(`${attendeeId}'s signal strength changed: `, {\n      signalStrength, // 0 (no signal), 0.5 (weak), 1 (strong)\n    });\n  }\n);\n```\n\n**Use case 12.** Detect the most active speaker. For example, you can enlarge the active speaker's video element if available.\n\n```js\nimport { DefaultActiveSpeakerPolicy } from 'amazon-chime-sdk-js';\n\nconst activeSpeakerCallback = attendeeIds => {\n  if (attendeeIds.length) {\n    console.log(`${attendeeIds[0]} is the most active speaker`);\n  }\n};\n\nmeetingSession.audioVideo.subscribeToActiveSpeakerDetector(\n  new DefaultActiveSpeakerPolicy(),\n  activeSpeakerCallback\n);\n```\n\n### Video\n\n> Note: In Chime SDK terms, a video tile is an object containing an attendee ID,\n> a video stream, etc. To view a video in your application, you must bind a tile to a `<video>` element.\n>\n> - Make sure you bind a tile to the same video element until the tile is removed.\n> - A local video tile can be identified using `localTile` property.\n> - A tile is created with a new tile ID when the same remote attendee restarts the video.\n> - Media Capture Pipeline relies on the meeting session to get the attendee info. After calling `this.meetingSession.audioVideo.start();`, wait for `audioVideoDidStart` event to be received before calling `startLocalVideoTile`.\n\n**Use case 13.** Start sharing your video. The local video element is flipped horizontally (mirrored mode).\n\n```js\nconst videoElement = /* HTMLVideoElement object e.g. document.getElementById('video-element-id') */;\n\n// Make sure you have chosen your camera. In this use case, you will choose the first device.\nconst videoInputDevices = await meetingSession.audioVideo.listVideoInputDevices();\n\n// The camera LED light will turn on indicating that it is now capturing.\n// See the \"Device\" section for details.\nawait meetingSession.audioVideo.startVideoInput(videoInputDevices[0].deviceId);\n\nconst observer = {\n  // videoTileDidUpdate is called whenever a new tile is created or tileState changes.\n  videoTileDidUpdate: tileState => {\n    // Ignore a tile without attendee ID and other attendee's tile.\n    if (!tileState.boundAttendeeId || !tileState.localTile) {\n      return;\n    }\n\n    meetingSession.audioVideo.bindVideoElement(tileState.tileId, videoElement);\n  }\n};\n\nmeetingSession.audioVideo.addObserver(observer);\n\nmeetingSession.audioVideo.startLocalVideoTile();\n```\n\n**Use case 14.** Stop sharing your video.\n\n```js\nconst videoElement = /* HTMLVideoElement object e.g. document.getElementById('video-element-id') */;\n\nlet localTileId = null;\nconst observer = {\n  videoTileDidUpdate: tileState => {\n    // Ignore a tile without attendee ID and other attendee's tile.\n    if (!tileState.boundAttendeeId || !tileState.localTile) {\n      return;\n    }\n\n    // videoTileDidUpdate is also invoked when you call startLocalVideoTile or tileState changes.\n    // The tileState.active can be false in poor Internet connection, when the user paused the video tile, or when the video tile first arrived.\n    console.log(`If you called stopLocalVideoTile, ${tileState.active} is false.`);\n    meetingSession.audioVideo.bindVideoElement(tileState.tileId, videoElement);\n    localTileId = tileState.tileId;\n  },\n  videoTileWasRemoved: tileId => {\n    if (localTileId === tileId) {\n      console.log(`You called removeLocalVideoTile. videoElement can be bound to another tile.`);\n      localTileId = null;\n    }\n  }\n};\n\nmeetingSession.audioVideo.addObserver(observer);\n\nmeetingSession.audioVideo.stopLocalVideoTile();\n\n// Stop video input. If the previously chosen camera has an LED light on,\n// it will turn off indicating the camera is no longer capturing.\nawait meetingSession.audioVideo.stopVideoInput();\n\n// Optional: You can remove the local tile from the session.\nmeetingSession.audioVideo.removeLocalVideoTile();\n```\n\n**Use case 15.** View one attendee video, e.g. in a 1-on-1 session.\n\n```js\nconst videoElement = /* HTMLVideoElement object e.g. document.getElementById('video-element-id') */;\n\nconst observer = {\n  // videoTileDidUpdate is called whenever a new tile is created or tileState changes.\n  videoTileDidUpdate: tileState => {\n    // Ignore a tile without attendee ID, a local tile (your video), and a content share.\n    if (!tileState.boundAttendeeId || tileState.localTile || tileState.isContent) {\n      return;\n    }\n\n    meetingSession.audioVideo.bindVideoElement(tileState.tileId, videoElement);\n  }\n};\n\nmeetingSession.audioVideo.addObserver(observer);\n```\n\n**Use case 16.** View up to 25 attendee videos. Assume that you have 25 video elements in your application,\nand that an empty cell means it's taken.\n\n```js\n/*\n  No one is sharing video                    e.g. 9 attendee videos (9 empty cells)\n\n  Next available:                            Next available:\n  videoElements[0]                           videoElements[7]\n  \u2554\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2557                 \u2554\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2557\n  \u2551  0 \u2551  1 \u2551  2 \u2551  3 \u2551  4 \u2551                 \u2551    \u2551    \u2551    \u2551    \u2551    \u2551\n  \u2560\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2563                 \u2560\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2563\n  \u2551  5 \u2551  6 \u2551  7 \u2551  8 \u2551  9 \u2551                 \u2551    \u2551    \u2551  7 \u2551  8 \u2551    \u2551\n  \u2560\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2563                 \u2560\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2563\n  \u2551 10 \u2551 11 \u2551 12 \u2551 13 \u2551 14 \u2551                 \u2551 10 \u2551    \u2551 12 \u2551 13 \u2551 14 \u2551\n  \u2560\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2563                 \u2560\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2563\n  \u2551 15 \u2551 16 \u2551 17 \u2551 18 \u2551 19 \u2551                 \u2551 15 \u2551 16 \u2551 17 \u2551 18 \u2551 19 \u2551\n  \u2560\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2563                 \u2560\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2563\n  \u2551 20 \u2551 21 \u2551 22 \u2551 23 \u2551 24 \u2551                 \u2551 20 \u2551 21 \u2551 22 \u2551 23 \u2551 24 \u2551\n  \u255a\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u255d                 \u255a\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u255d\n */\nconst videoElements = [\n  /* an array of 25 HTMLVideoElement objects in your application */\n];\n\n// index-tileId pairs\nconst indexMap = {};\n\nconst acquireVideoElement = tileId => {\n  // Return the same video element if already bound.\n  for (let i = 0; i < 25; i += 1) {\n    if (indexMap[i] === tileId) {\n      return videoElements[i];\n    }\n  }\n  // Return the next available video element.\n  for (let i = 0; i < 25; i += 1) {\n    if (!indexMap.hasOwnProperty(i)) {\n      indexMap[i] = tileId;\n      return videoElements[i];\n    }\n  }\n  throw new Error('no video element is available');\n};\n\nconst releaseVideoElement = tileId => {\n  for (let i = 0; i < 25; i += 1) {\n    if (indexMap[i] === tileId) {\n      delete indexMap[i];\n      return;\n    }\n  }\n};\n\nconst observer = {\n  // videoTileDidUpdate is called whenever a new tile is created or tileState changes.\n  videoTileDidUpdate: tileState => {\n    // Ignore a tile without attendee ID, a local tile (your video), and a content share.\n    if (!tileState.boundAttendeeId || tileState.localTile || tileState.isContent) {\n      return;\n    }\n\n    meetingSession.audioVideo.bindVideoElement(\n      tileState.tileId,\n      acquireVideoElement(tileState.tileId)\n    );\n  },\n  videoTileWasRemoved: tileId => {\n    releaseVideoElement(tileId);\n  },\n};\n\nmeetingSession.audioVideo.addObserver(observer);\n```\n\n**Use case 17.** Add an observer to know all the remote video sources when changed.\n\n```js\nconst observer = {\n  remoteVideoSourcesDidChange: videoSources => {\n    videoSources.forEach(videoSource => {\n      const { attendee } = videoSource;\n      console.log(\n        `An attendee (${attendee.attendeeId} ${attendee.externalUserId}) is sending video`\n      );\n    });\n  },\n};\n\nmeetingSession.audioVideo.addObserver(observer);\n```\n\nYou can also call below method to know all the remote video sources:\n\n> Note: `getRemoteVideoSources` method is different from `getAllRemoteVideoTiles`,\n> `getRemoteVideoSources` returns all the remote video sources that are available to be viewed,\n> while `getAllRemoteVideoTiles` returns the ones that are actually being seen.\n\n```js\nconst videoSources = meetingSession.audioVideo.getRemoteVideoSources();\nvideoSources.forEach(videoSource => {\n  const { attendee } = videoSource;\n  console.log(`An attendee (${attendee.attendeeId} ${attendee.externalUserId}) is sending video`);\n});\n```\n\n### Screen and content share\n\n> Note: When you or other attendees share content (a screen capture, a video file, or any other MediaStream object),\n> the content attendee (attendee-id#content) joins the session and shares content as if a regular attendee shares a video.\n>\n> For example, your attendee ID is \"my-id\". When you call `meetingSession.audioVideo.startContentShare`,\n> the content attendee \"my-id#content\" will join the session and share your content.\n\n**Use case 18.** Start sharing your screen.\n\n```js\nimport { DefaultModality } from 'amazon-chime-sdk-js';\n\nconst observer = {\n  videoTileDidUpdate: tileState => {\n    // Ignore a tile without attendee ID and videos.\n    if (!tileState.boundAttendeeId || !tileState.isContent) {\n      return;\n    }\n\n    const yourAttendeeId = meetingSession.configuration.credentials.attendeeId;\n\n    // tileState.boundAttendeeId is formatted as \"attendee-id#content\".\n    const boundAttendeeId = tileState.boundAttendeeId;\n\n    // Get the attendee ID from \"attendee-id#content\".\n    const baseAttendeeId = new DefaultModality(boundAttendeeId).base();\n    if (baseAttendeeId === yourAttendeeId) {\n      console.log('You called startContentShareFromScreenCapture');\n    }\n  },\n  contentShareDidStart: () => {\n    console.log('Screen share started');\n  },\n  contentShareDidStop: () => {\n    // Chime SDK allows 2 simultaneous content shares per meeting.\n    // This method will be invoked if two attendees are already sharing content\n    // when you call startContentShareFromScreenCapture or startContentShare.\n    console.log('Screen share stopped');\n  },\n};\n\nmeetingSession.audioVideo.addContentShareObserver(observer);\nmeetingSession.audioVideo.addObserver(observer);\n\n// A browser will prompt the user to choose the screen.\nconst contentShareStream = await meetingSession.audioVideo.startContentShareFromScreenCapture();\n```\n\nIf you want to display the content share stream for the sharer, you can bind the returned content share stream to a\nvideo element using `connectVideoStreamToVideoElement` from DefaultVideoTile.\n\n```js\nDefaultVideoTile.connectVideoStreamToVideoElement(contentShareStream, videoElement, false);\n```\n\n**Use case 19.** Start sharing your screen in an environment that does not support a screen picker dialog. e.g. Electron\n\n```js\nconst sourceId = /* Window or screen ID e.g. the ID of a DesktopCapturerSource object in Electron */;\n\nawait meetingSession.audioVideo.startContentShareFromScreenCapture(sourceId);\n```\n\n**Use case 20.** Start streaming your video file from an `<input>` element of type `file`.\n\n```js\nconst videoElement = /* HTMLVideoElement object e.g. document.getElementById('video-element-id') */;\nconst inputElement = /* HTMLInputElement object e.g. document.getElementById('input-element-id') */;\n\ninputElement.addEventListener('change', async () => {\n  const file = inputElement.files[0];\n  const url = URL.createObjectURL(file);\n  videoElement.src = url;\n  await videoElement.play();\n\n  const mediaStream = videoElement.captureStream(); /* use mozCaptureStream for Firefox e.g. videoElement\n.mozCaptureStream(); */\n  await meetingSession.audioVideo.startContentShare(mediaStream);\n  inputElement.value = '';\n});\n```\n\n**Use case 21.** Stop sharing your screen or content.\n\n```js\nconst observer = {\n  contentShareDidStop: () => {\n    console.log('Content share stopped');\n  },\n};\n\nmeetingSession.audioVideo.addContentShareObserver(observer);\n\nawait meetingSession.audioVideo.stopContentShare();\n```\n\n**Use case 22.** View up to 2 attendee content or screens. Chime SDK allows 2 simultaneous content shares per meeting.\n\n```js\nimport { DefaultModality } from 'amazon-chime-sdk-js';\n\nconst videoElementStack = [\n  /* an array of 2 HTMLVideoElement objects in your application */\n];\n\n// tileId-videoElement map\nconst tileMap = {};\n\nconst observer = {\n  videoTileDidUpdate: tileState => {\n    // Ignore a tile without attendee ID and videos.\n    if (!tileState.boundAttendeeId || !tileState.isContent) {\n      return;\n    }\n\n    const yourAttendeeId = meetingSession.configuration.credentials.attendeeId;\n\n    // tileState.boundAttendeeId is formatted as \"attendee-id#content\".\n    const boundAttendeeId = tileState.boundAttendeeId;\n\n    // Get the attendee ID from \"attendee-id#content\".\n    const baseAttendeeId = new DefaultModality(boundAttendeeId).base();\n    if (baseAttendeeId !== yourAttendeeId) {\n      console.log(`${baseAttendeeId} is sharing screen now`);\n\n      // Get the already bound video element if available, or use an unbound element.\n      const videoElement = tileMap[tileState.tileId] || videoElementStack.pop();\n      if (videoElement) {\n        tileMap[tileState.tileId] = videoElement;\n        meetingSession.audioVideo.bindVideoElement(tileState.tileId, videoElement);\n      } else {\n        console.log('No video element is available');\n      }\n    }\n  },\n  videoTileWasRemoved: tileId => {\n    // Release the unused video element.\n    const videoElement = tileMap[tileId];\n    if (videoElement) {\n      videoElementStack.push(videoElement);\n      delete tileMap[tileId];\n    }\n  },\n};\n\nmeetingSession.audioVideo.addObserver(observer);\n```\n\n### Attendees\n\n**Use case 23.** Subscribe to attendee presence changes. When an attendee joins or leaves a session,\nthe callback receives `presentAttendeeId` and `present` (a boolean).\n\n```js\nconst attendeePresenceSet = new Set();\nconst callback = (presentAttendeeId, present) => {\n  console.log(`Attendee ID: ${presentAttendeeId} Present: ${present}`);\n  if (present) {\n    attendeePresenceSet.add(presentAttendeeId);\n  } else {\n    attendeePresenceSet.delete(presentAttendeeId);\n  }\n};\n\nmeetingSession.audioVideo.realtimeSubscribeToAttendeeIdPresence(callback);\n```\n\n**Use case 24.** Create a simple roster by subscribing to attendee presence and volume changes.\n\n```js\nimport { DefaultModality } from 'amazon-chime-sdk-js';\n\nconst roster = {};\n\nmeetingSession.audioVideo.realtimeSubscribeToAttendeeIdPresence((presentAttendeeId, present) => {\n  if (!present) {\n    delete roster[presentAttendeeId];\n    return;\n  }\n\n  meetingSession.audioVideo.realtimeSubscribeToVolumeIndicator(\n    presentAttendeeId,\n    (attendeeId, volume, muted, signalStrength) => {\n      const baseAttendeeId = new DefaultModality(attendeeId).base();\n      if (baseAttendeeId !== attendeeId) {\n        // Optional: Do not include the content attendee (attendee-id#content) in the roster.\n        // See the \"Screen and content share\" section for details.\n        return;\n      }\n\n      if (roster.hasOwnProperty(attendeeId)) {\n        // A null value for any field means that it has not changed.\n        roster[attendeeId].volume = volume; // a fraction between 0 and 1\n        roster[attendeeId].muted = muted; // A boolean\n        roster[attendeeId].signalStrength = signalStrength; // 0 (no signal), 0.5 (weak), 1 (strong)\n      } else {\n        // Add an attendee.\n        // Optional: You can fetch more data, such as attendee name,\n        // from your server application and set them here.\n        roster[attendeeId] = {\n          attendeeId,\n          volume,\n          muted,\n          signalStrength,\n        };\n      }\n    }\n  );\n});\n```\n\n### Monitoring and alerts\n\n**Use case 25.** Add an observer to receive WebRTC metrics processed by Chime SDK such as bitrate, packet loss, and bandwidth. See `AudioVideoObserver` for more available metrics.\n\n```js\nconst observer = {\n  metricsDidReceive: clientMetricReport => {\n    const metricReport = clientMetricReport.getObservableMetrics();\n\n    const {\n      videoPacketSentPerSecond,\n      videoUpstreamBitrate,\n      availableOutgoingBitrate,\n      availableIncomingBitrate,\n      audioSpeakerDelayMs,\n    } = metricReport;\n\n    console.log(\n      `Sending video bitrate in kilobits per second: ${\n        videoUpstreamBitrate / 1000\n      } and sending packets per second: ${videoPacketSentPerSecond}`\n    );\n    console.log(\n      `Sending bandwidth is ${availableOutgoingBitrate / 1000}, and receiving bandwidth is ${\n        availableIncomingBitrate / 1000\n      }`\n    );\n    console.log(`Audio speaker delay is ${audioSpeakerDelayMs}`);\n  },\n};\n\nmeetingSession.audioVideo.addObserver(observer);\n```\n\n**Use case 26.** Add an observer to receive alerts. You can use these alerts to notify users of connection problems.\n\n```js\nconst observer = {\n  connectionDidBecomePoor: () => {\n    console.log('Your connection is poor');\n  },\n  connectionDidSuggestStopVideo: () => {\n    console.log('Recommend turning off your video');\n  },\n  videoSendDidBecomeUnavailable: () => {\n    // Chime SDK allows a total of 25 simultaneous videos per meeting.\n    // If you try to share more video, this method will be called.\n    // See videoAvailabilityDidChange below to find out when it becomes available.\n    console.log('You cannot share your video');\n  },\n  videoAvailabilityDidChange: videoAvailability => {\n    // canStartLocalVideo will also be true if you are already sharing your video.\n    if (videoAvailability.canStartLocalVideo) {\n      console.log('You can share your video');\n    } else {\n      console.log('You cannot share your video');\n    }\n  },\n};\n\nmeetingSession.audioVideo.addObserver(observer);\n```\n\n### Stopping a session\n\n**Use case 27.** Leave a session.\n\n```js\nimport { MeetingSessionStatusCode } from 'amazon-chime-sdk-js';\n\nconst observer = {\n  audioVideoDidStop: sessionStatus => {\n    const sessionStatusCode = sessionStatus.statusCode();\n    if (sessionStatusCode === MeetingSessionStatusCode.Left) {\n      /*\n       * You called meetingSession.audioVideo.stop().\n       */\n      console.log('You left the session');\n    } else {\n      console.log('Stopped with a session status code: ', sessionStatusCode);\n    }\n  },\n};\n\nmeetingSession.audioVideo.addObserver(observer);\n\nmeetingSession.audioVideo.stop();\n```\n\n**Use case 28.** Add an observer to get notified when a session has ended.\n\n```js\nimport { MeetingSessionStatusCode } from 'amazon-chime-sdk-js';\n\nconst observer = {\n  audioVideoDidStop: sessionStatus => {\n    const sessionStatusCode = sessionStatus.statusCode();\n    if (sessionStatusCode === MeetingSessionStatusCode.MeetingEnded) {\n      /*\n        - You (or someone else) have called the DeleteMeeting API action in your server application.\n        - You attempted to join a deleted meeting.\n        - No audio connections are present in the meeting for more than five minutes.\n        - The meeting time exceeds 24 hours.\n        See https://docs.aws.amazon.com/chime-sdk/latest/dg/mtgs-sdk-mtgs.html for details.\n      */\n      console.log('The session has ended');\n    } else {\n      console.log('Stopped with a session status code: ', sessionStatusCode);\n    }\n  },\n};\n\nmeetingSession.audioVideo.addObserver(observer);\n```\n\n### Meeting readiness checker\n\n**Use case 29.** Initialize the meeting readiness checker.\n\n```js\nimport { DefaultMeetingReadinessChecker } from 'amazon-chime-sdk-js';\n\n// In the usage examples below, you will use this meetingReadinessChecker object.\nconst meetingReadinessChecker = new DefaultMeetingReadinessChecker(logger, meetingSession);\n```\n\n**Use case 30.** Use the meeting readiness checker to perform local checks.\n\n```js\nimport { CheckAudioInputFeedback } from 'amazon-chime-sdk-js';\n\nconst audioInputDeviceInfo = /* An array item from meetingSession.audioVideo.listAudioInputDevices */;\nconst audioInputFeedback = await meetingReadinessChecker.checkAudioInput(audioInputDeviceInfo.deviceId);\n\nswitch (audioInputFeedback) {\n  case CheckAudioInputFeedback.Succeeded:\n    console.log('Succeeded');\n    break;\n  case CheckAudioInputFeedback.Failed:\n    console.log('Failed');\n    break;\n  case CheckAudioInputFeedback.PermissionDenied:\n    console.log('Permission denied');\n    break;\n}\n```\n\n**Use case 31.** Use the meeting readiness checker to perform end-to-end checks, e.g. audio, video, and content share.\n\n```js\nimport {\n  CheckAudioConnectivityFeedback,\n  CheckContentShareConnectivityFeedback,\n  CheckVideoConnectivityFeedback\n} from 'amazon-chime-sdk-js';\n\n// Tests audio connection\nconst audioDeviceInfo = /* An array item from meetingSession.audioVideo.listAudioInputDevices */;\nconst audioFeedback = await meetingReadinessChecker.checkAudioConnectivity(audioDeviceInfo.deviceId);\nconsole.log(`Feedback result: ${CheckAudioConnectivityFeedback[audioFeedback]}`);\n\n// Test video connection\nconst videoInputInfo = /* An array item from meetingSession.audioVideo.listVideoInputDevices */;\nconst videoFeedback = await meetingReadinessChecker.checkVideoConnectivity(videoInputInfo.deviceId);\nconsole.log(`Feedback result: ${CheckVideoConnectivityFeedback[videoFeedback]}`);\n\n// Tests content share connectivity\nconst contentShareFeedback = await meetingReadinessChecker.checkContentShareConnectivity();\nconsole.log(`Feedback result: ${CheckContentShareConnectivityFeedback[contentShareFeedback]}`);\n```\n\n**Use case 32.** Use the meeting readiness checker to perform network checks, e.g. TCP and UDP.\n\n```js\nimport {\n  CheckNetworkUDPConnectivityFeedback,\n  CheckNetworkTCPConnectivityFeedback,\n} from 'amazon-chime-sdk-js';\n\n// Tests for UDP network connectivity\nconst networkUDPFeedback = await meetingReadinessChecker.checkNetworkUDPConnectivity();\nconsole.log(`Feedback result: ${CheckNetworkUDPConnectivityFeedback[networkUDPFeedback]}`);\n\n// Tests for TCP network connectivity\nconst networkTCPFeedback = await meetingReadinessChecker.checkNetworkTCPConnectivity();\nconsole.log(`Feedback result: ${CheckNetworkTCPConnectivityFeedback[networkTCPFeedback]}`);\n```\n\n### Selecting an audio profile\n\n**Use case 32.** Set the audio quality of the main audio input to optimize for speech or music:\n\nUse the following setting to optimize the audio bitrate of the main audio input for fullband speech with a mono channel:\n\n```js\nmeetingSession.audioVideo.setAudioProfile(AudioProfile.fullbandSpeechMono());\n```\n\n**Use case 33.** Set the audio quality of content share audio to optimize for speech or music:\n\nUse the following setting to optimize the audio bitrate of content share audio for fullband music with a mono channel:\n\n```js\nmeetingSession.audioVideo.setContentAudioProfile(AudioProfile.fullbandMusicMono());\n```\n\n**Use case 34.** Sending and receiving stereo audio\n\nYou can send an audio stream with stereo channels either as content or through the main audio input.\n\nUse the following setting to optimize the main audio input and output for an audio stream with stereo channels:\n\n```js\nmeetingSession.audioVideo.setAudioProfile(AudioProfile.fullbandMusicStereo());\n```\n\nUse the following setting to optimize the content share audio for an audio stream with stereo channels:\n\n```js\nmeetingSession.audioVideo.setContentAudioProfile(AudioProfile.fullbandMusicStereo());\n```\n\n**Use case 35.** Redundant Audio\n\nStarting from version 3.18.2, the SDK starts sending redundant audio data to our servers on detecting packet loss\nto help reduce its effect on audio quality. Redundant audio packets are only sent out for packets containing active\naudio, ie, speech or music. This may increase the bandwidth consumed by audio to up to 3 times the normal amount\ndepending on the amount of packet loss detected. The SDK will automatically stop sending redundant data if it hasn't\ndetected any packet loss for 5 minutes.\n\nThis feature requires `blob:` to be in your content security policy under the `worker-src` directive.\nWithout this, we will not be able to send out redundant audio data.\n\nThis feature is not supported on Firefox at the moment.\nWe were able to successfully send redundant audio from safari 16.1 onwards. 15.6.1 advertises support as well but is untested.\nChrome advertises support for redundant audio from version M96.\n\nTo disable this feature for attendee audio, you can use the following:\n\n```js\nmeetingSession.audioVideo.setAudioProfile(new AudioProfile(null, false));\n```\n\nIf using bitrate optimization and you want to disable audio redundancy you can use the below line.\nIn the example below, we only use fullbandSpeechMono but you can use fullbandMusicMono and fullbandMusicStereo\ndepending on your use case.\n\n```js\nmeetingSession.audioVideo.setAudioProfile(AudioProfile.fullbandSpeechMono(false));\n```\n\nTo disable this feature for content share audio, you can use any one of the following:\n\n```js\nmeetingSession.audioVideo.setContentAudioProfile(new AudioProfile(null, false));\n```\n\nIf using bitrate optimization and you want to disable audio redundancy you can use the below line.\nIn the example below, we only use fullbandSpeechMono but you can use fullbandMusicMono and fullbandMusicStereo\ndepending on your use case.\n\n```js\nmeetingSession.audioVideo.setContentAudioProfile(AudioProfile.fullbandSpeechMono(false));\n```\n\nWhile there is an option to disable the feature, we recommend keeping it enabled for improved audio quality.\nOne possible reason to disable it might be if your customers have very strict bandwidth limitations.\n\n### Starting a messaging session\n\n**Use case 36.** Setup an observer to receive events: connecting, start, stop and receive message; and\nstart a messaging session.\n\n> Note: You can remove an observer by calling `messagingSession.removeObserver(observer)`.\n> In a component-based architecture (such as React, Vue, and Angular), you may need to add an observer\n> when a component is mounted, and remove it when unmounted.\n\n```js\nconst observer = {\n  messagingSessionDidStart: () => {\n    console.log('Session started');\n  },\n  messagingSessionDidStartConnecting: reconnecting => {\n    if (reconnecting) {\n      console.log('Start reconnecting');\n    } else {\n      console.log('Start connecting');\n    }\n  },\n  messagingSessionDidStop: event => {\n    console.log(`Closed: ${event.code} ${event.reason}`);\n  },\n  messagingSessionDidReceiveMessage: message => {\n    console.log(`Receive message type ${message.type}`);\n  },\n};\n\nmessagingSession.addObserver(observer);\nawait messagingSession.start();\n```\n\n### Providing application metadata\n\nAmazon Chime SDK for JavaScript allows builders to provide application metadata in the meeting session configuration. This field is optional. Amazon Chime uses application metadata to analyze meeting health trends or identify common failures to improve your meeting experience.\n\n> \u26a0\ufe0f Do not pass any Personal Identifiable Information (PII).\n\n**Use case 37.** Provide application metadata to the meeting session configuration.\n\n```js\nimport { MeetingSessionConfiguration, ApplicationMetadata } from 'amazon-chime-sdk-js';\n\nconst createMeetingResponse = // CreateMeeting API response.\nconst createAttendeeResponse = // CreateAttendee API response.\nconst meetingSessionConfiguration = new MeetingSessionConfiguration(\n  createMeetingResponse,\n  createAttendeeResponse\n);\n\nmeetingSessionConfiguration.applicationMetadata = ApplicationMetadata.create({\n  appName: 'AppName',\n  appVersion: '1.0.0'\n});\n```\n\n#### Accepted application metadata constraints\n\n```js\n// The appName must be between 1-32 characters.\n// The appName must satisfy following regular expression:\n// /^[a-zA-Z0-9]+[a-zA-Z0-9_-]*[a-zA-Z0-9]+$/g\nappName: string;\n\n// The appVersion must be between 1-32 characters.\n// The appVersion must follow the Semantic Versioning format.\n// https://semver.org/\nappVersion: string;\n```\n\n## Notice\n\nThe use of Amazon Voice Focus. background blur, and background replacement via this SDK involves the downloading and execution of code at runtime by end users.\n\nThe use of Amazon Voice Focus. background blur, and background replacement runtime code is subject to additional notices. See [this Amazon Voice Focus NOTICES file](https://static.sdkassets.chime.aws/workers/NOTICES.txt), [background blur and background replacement NOTICES file](https://static.sdkassets.chime.aws/bgblur/workers/NOTICES.txt), and [background blur 2.0 and background replacement 2.0 NOTICES file](https://static.sdkassets.chime.aws/ml_media_fx/otherassets/NOTICES.txt) for details. You agree to make these additional notices available to all end users who use Amazon Voice Focus, background blur and background replacement, background blur 2.0 and background replacement 2.0, runtime code via this SDK.\n\nThe browser demo applications in the [demos directory](https://github.com/aws/amazon-chime-sdk-js/tree/main/demos) use [TensorFlow.js](https://github.com/tensorflow/tfjs) and pre-trained [TensorFlow.js models](https://github.com/tensorflow/tfjs-models) for image segmentation. Use of these third party models involves downloading and execution of code at runtime from [jsDelivr](https://www.jsdelivr.com/) by end user browsers. For the jsDelivr Acceptable Use Policy, please visit this [link](https://www.jsdelivr.com/terms/acceptable-use-policy-jsdelivr-net).\n\nThe use of TensorFlow runtime code referenced above may be subject to additional license requirements. See the licenses page for TensorFlow.js [here](https://github.com/tensorflow/tfjs/blob/master/LICENSE) and TensorFlow.js models [here](https://github.com/tensorflow/tfjs-models/blob/master/LICENSE) for details.\n\nYou and your end users are responsible for all Content (including any images) uploaded for use with background replacement, and must ensure that such Content does not violate the law, infringe or misappropriate the rights of any third party, or otherwise violate a material term of your agreement with Amazon (including the documentation, the AWS Service Terms, or the Acceptable Use Policy).\n\nLive transcription using the Amazon Chime SDK for JavaScript is powered by Amazon Transcribe. Use of Amazon Transcribe is subject to the [AWS Service Terms](https://aws.amazon.com/service-terms/), including the terms specific to the AWS Machine Learning and Artificial Intelligence Services. Standard charges for Amazon Transcribe and Amazon Transcribe Medical will apply.\n\nYou and your end users understand that recording Amazon Chime SDK meetings may be subject to laws or regulations regarding the recording of electronic communications. It is your and your end users\u2019 responsibility to comply with all applicable laws regarding the recordings, including properly notifying all participants in a recorded session, or communication that the session or communication is being recorded, and obtain their consent.\n\n---\n\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n", "release_dates": ["2024-02-15T19:22:46Z", "2023-12-20T18:13:41Z", "2023-10-11T20:45:17Z", "2023-10-02T17:51:25Z", "2023-09-28T21:32:04Z", "2023-09-14T19:31:30Z", "2023-08-23T18:26:41Z", "2023-06-30T19:35:45Z", "2023-05-26T22:56:15Z", "2023-04-13T19:04:43Z", "2023-03-30T18:01:35Z", "2023-02-17T00:24:55Z", "2023-01-06T18:53:15Z", "2022-11-07T22:20:42Z", "2022-09-23T21:17:16Z", "2022-08-22T23:06:43Z", "2022-07-18T21:49:48Z", "2022-06-28T21:32:34Z", "2022-06-06T21:23:56Z", "2022-05-26T22:23:55Z", "2022-05-16T23:14:59Z", "2022-05-04T00:09:43Z", "2022-04-28T22:07:05Z", "2022-04-07T18:23:36Z", "2022-03-21T21:15:03Z", "2022-03-11T21:24:43Z", "2022-03-10T23:38:07Z", "2022-03-02T21:59:17Z", "2022-02-25T18:16:45Z", "2022-02-14T22:37:20Z"]}, {"name": "amazon-cloudfront-client-routing-library", "description": "A library to encode and decode dns labels", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon CloudFront Client Routing Library\n\nThe Amazon CloudFront Client Routing Library is an open-source library designed for CloudFront's Client Routing feature, which is used direct client devices to CloudFront Points of Presence (POPs) with greater precision. Client Routing feature utilizes information present within a specially formatted DNS label, and this library provides functions to encode and decode such DNS labels.\n\n\n### What is Client Routing?\n\nClient Routing is a new feature from CloudFront which utilizes client subnet information encoded in a DNS label to route traffic to CloudFront POPs. In addition to utilizing this library, this feature has associated prerequisites such as using Route53 and requiring certificate updates. Documentation for Client Routing will be released in the near future, but if you are interested in using this feature sooner, please reach out to AWS Support to know more.\n\n\n### How to Use the Amazon CloudFront Client Routing Library?\n\nThere are two main functions in the library: `encode_request_data` and `decode_request_data`.\n\n#### Encoding\n\n`encode_request_data` takes three parameters: `client_ip`, `content_group_id`, and `fqdn`. A Client Routing label is generated from this data and then that label is returned prepended as a subdomain to the `fqdn`.\n\nThe `content_group_id` is set aside for future use and must be set to an empty string for now.\n\n\n```\nlet encoded_label = amazon_cloudfront_client_routing_lib::encode_request_data(\"1.2.3.4\", \"\", \"example.com\"); // encoded_label is abacaqdaaaaaaaamaaaaaaaaaaaaa.example.com\n```\n\n#### Decoding\n\n`decode_request_data` takes one parameter: `domain`. A result containing either a `DecodedClientRoutingLabel` struct or a `DecodeLengthError` is returned with each field set according to the `domain`. The `domain` can be either a FQDN or just the Client Routing label.\n\n```\nlet decoded_label = amazon_cloudfront_client_routing_lib::decode_request_data(\"abacaqdaaaaaaaamaaaaaaaaaaaaa\").unwrap();\n// DecodedClientRoutingLabel {\n//     client_sdk_version: 1,\n//     is_ipv6: false,\n//     client_subnet: [1, 2, 3, 0, 0, 0, 0, 0],\n//     subnet_mask: 24,\n//     cgid: 0,\n// }\nlet decoded_label = amazon_cloudfront_client_routing_lib::decode_request_data(\"abacaqdaaaaaaaamaaaaaaaaaaaaa.example.com\").unwrap();\n// DecodedClientRoutingLabel {\n//     client_sdk_version: 1,\n//     is_ipv6: false,\n//     client_subnet: [1, 2, 3, 4, 0, 0, 0, 0],\n//     subnet_mask: 24,\n//     cgid: 0,\n// }\n```\n\nIf the first dns label of the domain is an invalid Client Routing label (eg. improper length) then the result will contain an error.\n\n```\nlet decoded_label = amazon_cloudfront_client_routing_lib::decode_request_data(\"abacaqdaaaaaaaamnjg3oubcyv\").unwrap();\n// DecodeLengthError {\n//     num_chars: 26,\n//     expected_num_chars: 29   \n// }\n```\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": []}, {"name": "amazon-cloudwatch-agent", "description": "CloudWatch Agent enables you to collect and export host-level metrics and logs on instances running Linux or Windows server.", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "[![codecov](https://codecov.io/gh/aws/amazon-cloudwatch-agent/branch/main/graph/badge.svg?token=79VYANUGOM)](https://codecov.io/gh/aws/amazon-cloudwatch-agent)\n\n# Amazon CloudWatch Agent\nThe Amazon CloudWatch Agent is software developed for the [CloudWatch Agent](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html)\n\n## Overview\nThe Amazon CloudWatch Agent enables you to do the following:\n\n- Collect more system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The additional metrics that can be collected are listed in [Metrics Collected by the CloudWatch Agent](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html).\n- Collect system-level metrics from on-premises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.\n- Retrieve custom metrics from your applications or services using the StatsD and collectd protocols. StatsD is supported on both Linux servers and servers running Windows Server. collectd is supported only on Linux servers.\n- Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.\n- Collect Open Telemetry and AWS X-Ray traces\n\nAmazon CloudWatch Agent uses open-source projects [telegraf](https://github.com/influxdata/telegraf) and [opentelemetry-collector](https://github.com/open-telemetry/opentelemetry-collector) as its dependencies. \nIt operates by starting an opentelemetry collector and is capable of operating pipelines consisting of both telegraf and opentemetry components in addition to customized components.\n\n### Setup\n* [Configuring IAM Roles](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/create-iam-roles-for-cloudwatch-agent.html)\n* [Installation](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html)\n* [Configuring the CloudWatch Agent](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/create-cloudwatch-agent-configuration-file.html)\n\n### Troubleshooting\n* [Troubleshooting CloudWatch Agent](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/troubleshooting-CloudWatch-Agent.html)\n\n## Building and Running from source\n\n* Install go. For more information, see [Getting started](https://golang.org/doc/install)\n* The agent uses go modules for dependency management. For more information, see [Go Modules](https://github.com/golang/go/wiki/Modules)\n\n* Install rpm-build\n```\nsudo yum install -y rpmdevtools rpm-build\n```\n* Run `make build` to build the CloudWatch Agent for Linux, Debian, Windows environment.\n\n* Run `make release` to build the agent. This also packages it into a RPM, DEB and ZIP package.\n\nThe following folders are generated when the build completes:\n```\nbuild/bin/linux/arm64/amazon-cloudwatch-agent.rpm\nbuild/bin/linux/amd64/amazon-cloudwatch-agent.rpm\nbuild/bin/linux/arm64/amazon-cloudwatch-agent.deb\nbuild/bin/linux/amd64/amazon-cloudwatch-agent.deb\nbuild/bin/windows/amd64/amazon-cloudwatch-agent.zip\nbuild/bin/darwin/amd64/amazon-cloudwatch-agent.tar.gz\n```\n\n* Install your own build of the agent\n\n    1. rpm package\n\n        * `rpm -Uvh amazon-cloudwatch-agent.rpm`\n\n    1. deb package\n\n        * `dpkg -i -E ./amazon-cloudwatch-agent.deb`\n\n    1. windows package\n\n        * unzip `amazon-cloudwatch-agent.zip`\n        * `./install.ps1`\n\n    1. darwin package\n        * `tar -xvf amazon-cloudwatch-agent.tar.gz`\n        * `cp -rf ./opt/aws /opt`\n        * `cp -rf ./Library/LaunchDaemons/com.amazon.cloudwatch.agent.plist /Library/LaunchDaemons/`\n\n### Building and running container\n\nSee [Dockerfiles](amazon-cloudwatch-container-insights/cloudwatch-agent-dockerfile).\n\n### Make Targets\nThe following targets are available. Each may be run with `make <target>`.\n\n| Make Target              | Description |\n|:-------------------------|:------------|\n| `build`                  | `build` builds the agent for Linux, Debian and Windows amd64 environment |\n| `release`                | *(Default)* `release` builds the agent and also packages it into a RPM, DEB and ZIP package |\n| `clean`                  | `clean` removes build artifacts |\n| `dockerized-build`       | build using docker container without local go environment |\n\n## Features\n### Log Filtering\nCloudWatch agent supports log filtering, where the agent processes each log message with the filters that you specify, and only published events that pass all filters to CloudWatch Logs. See [docs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-Configuration-File-Details.html#CloudWatch-Agent-Configuration-File-Logssection) for details.\n\nFor example, the following excerpt of the CloudWatch agent configuration file publishes logs that are PUT and POST requests to CloudWatch Logs, but excluding logs that come from Firefox:\n```json\n{\n  \"collect_list\": [\n    {\n      \"file_path\": \"/opt/aws/amazon-cloudwatch-agent/logs/test.log\",\n      \"log_group_name\": \"test.log\",\n      \"log_stream_name\": \"test.log\",\n      \"filters\": [\n        {\n          \"type\": \"exclude\",\n          \"expression\": \"Firefox\"\n        },\n        {\n          \"type\": \"include\",\n          \"expression\": \"P(UT|OST)\"\n        }\n      ]\n    }\n  ]\n}\n```\nExample with above config:\n```\n2021-09-27T19:36:35Z I! [logagent] Firefox Detected   // Agent excludes this \n2021-09-27T19:36:35Z POST (StatusCode: 200).  // Agent would push this to CloudWatch\n2021-09-27T19:36:35Z GET (StatusCode: 400). // doesn't match regex, will be excluded\n```\n## Versioning\nIt is using [Semantic versioning](https://semver.org/)\n\n## Distributions\nYou can download the official release from S3, refer to [link](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/download-cloudwatch-agent-commandline.html)\n\nNightly s3 release are not production ready and should be used at own risk\n1. Download Binaries\n    1. Linux\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/linux_{amd64/arm64}/amazon-cloudwatch-agent\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/linux_{amd64/arm64}/amazon-cloudwatch-agent-config-wizard\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/linux_{amd64/arm64}/config-downloader\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/linux_{amd64/arm64}/config-translator\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/linux_{amd64/arm64}/start-amazon-cloudwatch-agent\n    1. Windows\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/windows_amd64/amazon-cloudwatch-agent-config-wizard.exe\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/windows_amd64/amazon-cloudwatch-agent.exe\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/windows_amd64/config-downloader.exe\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/windows_amd64/config-translator.exe\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/windows_amd64/start-amazon-cloudwatch-agent.exe\n    1. Mac\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/darwin_amd64/amazon-cloudwatch-agent\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/darwin_amd64/amazon-cloudwatch-agent-config-wizard\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/darwin_amd64/config-downloader\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/darwin_amd64/config-translator\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/darwin_amd64/start-amazon-cloudwatch-agent\n2. Download Packages\n    1. Linux\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/linux/{amd64/arm64}/amazon-cloudwatch-agent.{deb/rpm}\n    2. Windows\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/windows/amd64/amazon-cloudwatch-agent.zip\n    3. Mac\n        * https://amazoncloudwatch-agent.s3.amazonaws.com/nightly-build/latest/darwin/amd64/amazon-cloudwatch-agent.tar.gz\n\n## Usage data\nBy default, the CloudWatch agent sends health and performance data about itself to CloudWatch whenever it publishes metrics or logs to CloudWatch. This data incurs no costs to you. You can prevent the agent from sending this data by specifying `false` for `usage_data` in the `agent` section of the configuration. If you omit this parameter, the default of true is used and the agent sends the health and performance data. Refer to [link](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-Configuration-File-Details.html#CloudWatch-Agent-Configuration-File-Agentsection).\n\n## Security disclosures\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n## License\n\nMIT License\n\nCopyright (c) 2015-2019 InfluxData Inc.\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including  without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to  the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN  NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE  SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", "release_dates": ["2024-01-04T16:33:20Z", "2024-01-16T18:00:20Z", "2023-12-07T19:41:38Z", "2023-12-05T19:36:15Z", "2023-12-05T19:36:28Z", "2023-12-05T19:36:04Z", "2023-12-05T19:35:50Z", "2023-12-05T19:35:37Z", "2023-12-05T19:35:23Z", "2023-10-04T17:37:36Z", "2023-10-03T21:07:02Z", "2023-09-15T21:31:21Z", "2023-09-11T16:32:30Z", "2023-08-22T05:56:42Z", "2023-08-22T05:36:16Z", "2023-08-22T05:03:22Z", "2023-08-22T04:58:23Z", "2023-08-22T04:43:50Z", "2023-06-23T21:08:42Z", "2023-05-16T19:45:35Z", "2023-04-20T14:01:04Z", "2023-03-29T17:41:42Z", "2022-12-19T20:39:30Z", "2022-12-19T20:38:23Z", "2022-10-17T20:40:40Z", "2022-08-19T22:29:41Z", "2022-07-15T18:28:46Z", "2022-06-08T23:05:29Z", "2022-02-28T18:26:17Z", "2021-11-04T23:52:32Z"]}, {"name": "amazon-cloudwatch-agent-operator", "description": "The Amazon CloudWatch Agent Operator is software developed to manage the CloudWatch Agent on kubernetes.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon CloudWatch Agent Operator\nThe Amazon CloudWatch Agent Operator is software developed to manage the [CloudWatch Agent](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html) on kubernetes.\n\nThis repo is based off of the [OpenTelemetry Operator](https://github.com/open-telemetry/opentelemetry-operator)\n\n## Build and Deployment\n- Image can be built using `make container`\n- Deploy kubernetes objects to your cluster `make deploy`\n\n## Pre requisites\n1. Have an existing kubernetes cluster, such as [minikube](https://minikube.sigs.k8s.io/docs/start/)\n\n2. Install cert-manager on your cluster\n```\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.0/cert-manager.yaml\n```\n\n## Getting started\n1. Set a shortcut for kubectl for the operator namespace\n\n```\nkubectl config set-context --current --namespace=amazon-cloudwatch\n```\n\n2. Look at all resources created\n\n```\nkubectl get all\n```\n\n3. Look at the manager pod logs to ensure the manager is functioning and waiting for workers\n\n```\nkubectl logs amazon-cloudwatch-agent-operator-controller-manager-66f67f47f78\n```\n\nYou should see logs that look similar to below\n\n```\n{\"level\":\"info\",\"ts\":\"2023-06-29T01:37:36Z\",\"msg\":\"Starting workers\",\"controller\":\"amazoncloudwatchagent\",\"controllerGroup\":\"cloudwatch.aws.amazon.com\",\"controllerKind\":\"AmazonCloudWatchAgent\",\"worker count\":1}\n```\n\n4. Create an AmazonCloudWatchAgent resource\n\n```\nkubectl apply -f - <<EOF\napiVersion: cloudwatch.aws.amazon.com/v1alpha1\nkind: AmazonCloudWatchAgent\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  mode: daemonset\n  serviceAccount: cloudwatch-agent\n  config: |\n    {\n        // insert cloudwatch agent config here\n    }\n  volumeMounts:\n  - mountPath: /rootfs\n    name: rootfs\n    readOnly: true\n  - mountPath: /var/run/docker.sock\n    name: dockersock\n    readOnly: true\n  - mountPath: /run/containerd/containerd.sock\n    name: containerdsock\n  - mountPath: /var/lib/docker\n    name: varlibdocker\n    readOnly: true\n  - mountPath: /sys\n    name: sys\n    readOnly: true\n  - mountPath: /dev/disk\n    name: devdisk\n    readOnly: true\n  volumes:\n  - name: rootfs\n    hostPath:\n      path: /\n  - hostPath:\n      path: /var/run/docker.sock\n    name: dockersock\n  - hostPath:\n      path: /var/lib/docker\n    name: varlibdocker\n  - hostPath:\n      path: /run/containerd/containerd.sock\n    name: containerdsock\n  - hostPath:\n      path: /sys\n    name: sys\n  - hostPath:\n      path: /dev/disk/\n    name: devdisk\n  env:\n    - name: K8S_NODE_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.nodeName\n    - name: HOST_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.hostIP\n    - name: HOST_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.nodeName\n    - name: K8S_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\nEOF\n```\n\n5. Create Instrumentation resource\n\n```\nkubectl apply -f - <<EOF\napiVersion: cloudwatch.aws.amazon.com/v1alpha1\nkind: Instrumentation\nmetadata:\n  name: java-instrumentation\n  namespace: default # use a namespace with pods you'd like to inject\nspec:\n  exporter:\n    endpoint: http://cloudwatch-agent.amazon-cloudwatch:4316/v1/metrics\n  propagators:\n    - tracecontext\n    - baggage\n    - b3\n    - xray\n  java:\n    env:\n      - name: OTEL_METRICS_EXPORTER\n        value: \"none\"\n      - name: OTEL_LOGS_EXPORTER\n        value: \"none\"\n      - name: OTEL_AWS_APP_SIGNALS_ENABLED\n        value: \"true\"\n      - name: OTEL_EXPORTER_OTLP_PROTOCOL\n        value: \"http/protobuf\"\n      - name: OTEL_AWS_APP_SIGNALS_EXPORTER_ENDPOINT\n        value: \"http://cloudwatch-agent.amazon-cloudwatch:4316/v1/metrics\"\nEOF\n```\n\n## Helpful tools\n1. This package uses [kubebuilder markers](https://book.kubebuilder.io/reference/markers.html) to generate kubernetes configs. Run `make manifests` to create crds and roles in `config/crd` and `config/rbac`\n2. Generate deepcopy.go by running `make generate`\n\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "amazon-cloudwatch-agent-test", "description": null, "language": "Go", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "## Amazon CloudWatch Agent Test\n\nBe sure to:\n\n* Edit your repository description on GitHub\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.\n\n\n## Dev Manuals\n\n1. [Learning Testing Workflow](docs/learning-testing-workflow.md)\n", "release_dates": []}, {"name": "amazon-cloudwatch-logs-for-fluent-bit", "description": "A Fluent Bit output plugin for CloudWatch Logs", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "[![Test Actions Status](https://github.com/aws/amazon-cloudwatch-logs-for-fluent-bit/workflows/Build/badge.svg)](https://github.com/aws/amazon-cloudwatch-logs-for-fluent-bit/actions)\n\n## Fluent Bit Plugin for CloudWatch Logs\n\n**NOTE: A new higher performance Fluent Bit CloudWatch Logs Plugin has been released.** Check out our [official guidance](#new-higher-performance-core-fluent-bit-plugin).\n\nA Fluent Bit output plugin for CloudWatch Logs\n\n#### Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or email AWS security directly at [aws-security@amazon.com](mailto:aws-security@amazon.com).\n\n### Usage\n\nRun `make` to build `./bin/cloudwatch.so`. Then use with Fluent Bit:\n```\n./fluent-bit -e ./cloudwatch.so -i cpu \\\n-o cloudwatch \\\n-p \"region=us-west-2\" \\\n-p \"log_group_name=fluent-bit-cloudwatch\" \\\n-p \"log_stream_name=testing\" \\\n-p \"auto_create_group=true\"\n```\n\nFor building Windows binaries, we need to install `mingw-w64` for cross-compilation. The same can be done using-\n```\nsudo apt-get install -y gcc-multilib gcc-mingw-w64\n```\nAfter this step, run `make windows-release`. Then use with Fluent Bit on Windows:\n```\n./fluent-bit.exe -e ./cloudwatch.dll -i dummy `\n-o cloudwatch `\n-p \"region=us-west-2\" `\n-p \"log_group_name=fluent-bit-cloudwatch\" `\n-p \"log_stream_name=testing\" `\n-p \"auto_create_group=true\"\n```\n\n### Plugin Options\n\n* `region`: The AWS region.\n* `log_group_name`: The name of the CloudWatch Log Group that you want log records sent to. This value allows a template in the form of `$(variable)`. See section [Templating Log Group and Stream Names](#templating-log-group-and-stream-names) for more. Fluent Bit will create missing log groups if `auto_create_group` is set, and will throw an error if it does not have permission.\n* `log_stream_name`: The name of the CloudWatch Log Stream that you want log records sent to. This value allows a template in the form of `$(variable)`. See section [Templating Log Group and Stream Names](#templating-log-group-and-stream-names) for more.\n* `default_log_group_name`: This required variable is the fallback in case any variables in `log_group_name` fails to parse. Defaults to `fluentbit-default`.\n* `default_log_stream_name`: This required variable is the fallback in case any variables in `log_stream_name` fails to parse. Defaults to `/fluentbit-default`.\n* `log_stream_prefix`: (deprecated) Prefix for the Log Stream name. Setting this to `prefix-` is the same as setting `log_stream_name = prefix-$(tag)`.\n* `log_key`: By default, the whole log record will be sent to CloudWatch. If you specify a key name with this option, then only the value of that key will be sent to CloudWatch. For example, if you are using the Fluentd Docker log driver, you can specify `log_key log` and only the log message will be sent to CloudWatch.\n* `log_format`: An optional parameter that can be used to tell CloudWatch the format of the data. A value of `json/emf` enables CloudWatch to extract custom metrics embedded in a JSON payload. See the [Embedded Metric Format](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Embedded_Metric_Format_Specification.html).\n* `role_arn`: ARN of an IAM role to assume (for cross account access).\n* `auto_create_group`: Automatically create log groups (and add tags). Valid values are \"true\" or \"false\" (case insensitive). Defaults to false. If you use dynamic variables in your log group name, you may need this to be `true`.\n* `auto_create_stream`: Automatically create log streams. Valid values are \"true\" or \"false\" (case insensitive). Defaults to true.\n* `new_log_group_tags`: Comma/equal delimited string of tags to include with _auto created_ log groups. Example: `\"tag=val,cooltag2=my other value\"`\n* `log_retention_days`: If set to a number greater than zero, and newly create log group's retention policy is set to this many days.\n* `endpoint`: Specify a custom endpoint for the CloudWatch Logs API.\n* `sts_endpoint`: Specify a custom endpoint for the STS API; used to assume your custom role provided with `role_arn`.\n* `credentials_endpoint`: Specify a custom HTTP endpoint to pull credentials from. The HTTP response body should look like the following:\n```\n{\n    \"AccessKeyId\": \"ACCESS_KEY_ID\",\n    \"Expiration\": \"EXPIRATION_DATE\",\n    \"SecretAccessKey\": \"SECRET_ACCESS_KEY\",\n    \"Token\": \"SECURITY_TOKEN_STRING\"\n}\n```\n\n**Note**: The plugin will always create the log stream, if it does not exist.\n\n### Permissions\n\nThis plugin requires the following permissions:\n* CreateLogGroup (useful when using dynamic groups)\n* CreateLogStream\n* DescribeLogStreams\n* PutLogEvents\n* PutRetentionPolicy (if `log_retention_days` is set > 0)\n\n### Credentials\n\nThis plugin uses the AWS SDK Go, and uses its [default credential provider chain](https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html). If you are using the plugin on Amazon EC2 or Amazon ECS or Amazon EKS, the plugin will use your EC2 instance role or [ECS Task role permissions](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html) or [EKS IAM Roles for Service Accounts for pods](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html). The plugin can also retrieve credentials from a [shared credentials file](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html), or from the standard `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_SESSION_TOKEN` environment variables.\n\n### Environment Variables\n\n* `FLB_LOG_LEVEL`: Set the log level for the plugin. Valid values are: `debug`, `info`, and `error` (case insensitive). Default is `info`. **Note**: Setting log level in the Fluent Bit Configuration file using the Service key will not affect the plugin log level (because the plugin is external).\n* `SEND_FAILURE_TIMEOUT`: Allows you to configure a timeout if the plugin can not send logs to CloudWatch. The timeout is specified as a [Golang duration](https://golang.org/pkg/time/#ParseDuration), for example: `5m30s`. If the plugin has failed to make any progress for the given period of time, then it will exit and kill Fluent Bit. This is useful in scenarios where you want your logging solution to fail fast if it has been misconfigured (i.e. network or credentials have not been set up to allow it to send to CloudWatch).\n\n### Retries and Buffering\n\nBuffering and retries are managed by the Fluent Bit core engine, not by the plugin. Whenever the plugin encounters any error, it returns a retry to the engine which schedules a retry. This means that log group creation, log stream creation or log retention policy calls can consume a retry if they fail.\n\n* [Fluent Bit upstream documentation on retries](https://docs.fluentbit.io/manual/administration/scheduling-and-retries)\n* [Fluent Bit upstream documentation on buffering](https://docs.fluentbit.io/manual/administration/buffering-and-storage)\n* [FireLens OOMKill prevent example for buffering](https://github.com/aws-samples/amazon-ecs-firelens-examples/tree/mainline/examples/fluent-bit/oomkill-prevention)\n\n### Templating Log Group and Stream Names\n\n A template in the form of `$(variable)` can be set in `log_group_name` or `log_stream_name`. `variable` can be a map key name in the log message. To access sub-values in the map use the form `$(variable['subkey'])`. Also, it can be replaced with special values to insert the tag, ECS metadata or a random string in the name.\n\n Special Values:\n *  `$(tag)` references the full tag name, `$(tag[0])` and `$(tag[1])` are the first and second values of log tag split on periods. You may access any member by index, 0 through 9.\n *  `$(uuid)` will insert a random string in the names. The random string is generated automatically with format: 4 bytes of time (seconds) + 16 random bytes. It is created when the plugin starts up and uniquely identifies the output - which means that until Fluent Bit is restarted, it will be the same. If you have multiple CloudWatch outputs, each one will get a unique UUID.\n * If your container is running in ECS, `$(variable)` can be set as `$(ecs_task_id)`, `$(ecs_cluster)` or `$(ecs_task_arn)`. It will set ECS metadata into `log_group_name` or `log_stream_name`.\n\n Here is an example for `fluent-bit.conf`:\n\n```\n[INPUT]\n    Name        dummy\n    Tag         dummy.data\n    Dummy {\"pam\": {\"item\": \"soup\", \"item2\":{\"subitem\": \"rice\"}}}\n\n[OUTPUT]\n    Name cloudwatch\n    Match   *\n    region us-east-1\n    log_group_name fluent-bit-cloudwatch-$(uuid)-$(tag)\n    log_stream_name from-fluent-bit-$(pam['item2']['subitem'])-$(ecs_task_id)-$(ecs_cluster)\n    auto_create_group true\n```\n\nAnd here is the resulting log stream name and log group name:\n\n```\nlog_group_name fluent-bit-cloudwatch-1jD7P6bbSRtbc9stkWjJZYerO6s-dummy.data\nlog_stream_name from-fluent-bit-rice-37e873f6-37b4-42a7-af47-eac7275c6152-ecs-local-cluster\n```\n\n#### Templating Log Group and Stream Names based on Kubernetes metadata\n\nIf you enable the kubernetes filter, then metadata like the following will be added to each log:\n\n```\nkubernetes: {\n    annotations: {\n        \"kubernetes.io/psp\": \"eks.privileged\"\n    },\n    container_hash: \"<some hash>\",\n    container_name: \"myapp\",\n    docker_id: \"<some id>\",\n    host: \"ip-10-1-128-166.us-east-2.compute.internal\",\n    labels: {\n        app: \"myapp\",\n        \"pod-template-hash\": \"<some hash>\"\n    },\n    namespace_name: \"default\",\n    pod_id: \"198f7dd2-2270-11ea-be47-0a5d932f5920\",\n    pod_name: \"myapp-5468c5d4d7-n2swr\"\n}\n```\n\nFor help setting up Fluent Bit with kubernetes please see [Kubernetes Logging Powered by AWS for Fluent Bit](https://aws.amazon.com/blogs/containers/kubernetes-logging-powered-by-aws-for-fluent-bit/) or [Set up Fluent Bit as a DaemonSet to send logs to CloudWatch Logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-logs-FluentBit.html).\n\nThe kubernetes metadata can be referenced just like any other keys using the templating feature, for example, the following will result in a log group name which is `/eks/{namespace_name}/{pod_name}`. \n\n```\n    [OUTPUT]\n      Name              cloudwatch\n      Match             kube.*\n      region            us-east-1\n      log_group_name    /eks/$(kubernetes['namespace_name'])/$(kubernetes['pod_name'])\n      log_stream_name   $(kubernetes['namespace_name'])/$(kubernetes['container_name'])\n      auto_create_group true\n```\n\n### New Higher Performance Core Fluent Bit Plugin\n\nIn the summer of 2020, we released a [new higher performance CloudWatch Logs plugin](https://docs.fluentbit.io/manual/pipeline/outputs/cloudwatch) named `cloudwatch_logs`.\n\nThat plugin has a core subset of the features of this older, lower performance and less efficient plugin. Check out its [documentation](https://docs.fluentbit.io/manual/pipeline/outputs/cloudwatch).\n\n#### Do you plan to deprecate this older plugin?\n\nAt this time, we do not. This plugin will continue to be supported. It contains features that have not been ported to the higher performance version. Specifically, the feature for [templating of log group name and streams with ECS Metadata or values in the logs](#templating-log-group-and-stream-names). While [simple templating support](https://docs.fluentbit.io/manual/pipeline/outputs/cloudwatch#log-stream-and-group-name-templating-using-record_accessor-syntax) now exists in the high performance plugin, it does not have all of the features of the plugin in this repo. Some users will continue to need the features in this repo. \n\n#### Which plugin should I use?\n\nIf the features of the higher performance plugin are sufficient for your use cases, please use it. It can achieve higher throughput and will consume less CPU and memory.\n\n#### How can I migrate to the higher performance plugin?\n\nIt supports a subset of the options of this plugin. For many users, you can simply replace the plugin name `cloudwatch` with the new name `cloudwatch_logs`. Check out its [documentation](https://docs.fluentbit.io/manual/pipeline/outputs/cloudwatch). \n\n#### Do you accept contributions to both plugins?\n\nYes. The high performance plugin is written in C, and this plugin is written in Golang. We understand that Go is an easier language for amateur contributors to write code in- that is a key reason why we are continuing to maintain it.\n\nHowever, if you can write code in C, please consider contributing new features to the [higher performance plugin](https://github.com/fluent/fluent-bit/tree/master/plugins/out_cloudwatch_logs).\n\n### Fluent Bit Versions\n\nThis plugin has been tested with Fluent Bit 1.2.0+. It may not work with older Fluent Bit versions. We recommend using the latest version of Fluent Bit as it will contain the newest features and bug fixes.\n\n### Example Fluent Bit Config File\n\n```\n[INPUT]\n    Name        forward\n    Listen      0.0.0.0\n    Port        24224\n\n[OUTPUT]\n    Name cloudwatch\n    Match   *\n    region us-east-1\n    log_group_name fluent-bit-cloudwatch\n    log_stream_prefix from-fluent-bit-\n    auto_create_group true\n```\n\n### AWS for Fluent Bit\n\nWe distribute a container image with Fluent Bit and these plugins.\n\n##### GitHub\n\n[github.com/aws/aws-for-fluent-bit](https://github.com/aws/aws-for-fluent-bit)\n\n##### Amazon ECR Public Gallery\n\n[aws-for-fluent-bit](https://gallery.ecr.aws/aws-observability/aws-for-fluent-bit)\n\nOur images are available in Amazon ECR Public Gallery. You can download images with different tags by following command:\n\n```\ndocker pull public.ecr.aws/aws-observability/aws-for-fluent-bit:<tag>\n```\n\nFor example, you can pull the image with latest version by:\n\n```\ndocker pull public.ecr.aws/aws-observability/aws-for-fluent-bit:latest\n```\n\nIf you see errors for image pull limits, try log into public ECR with your AWS credentials:\n\n```\naws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws\n```\n\nYou can check the [Amazon ECR Public official doc](https://docs.aws.amazon.com/AmazonECR/latest/public/get-set-up-for-amazon-ecr.html) for more details.\n\n##### Docker Hub\n\n[amazon/aws-for-fluent-bit](https://hub.docker.com/r/amazon/aws-for-fluent-bit/tags)\n\n##### Amazon ECR\n\nYou can use our SSM Public Parameters to find the Amazon ECR image URI in your region:\n\n```\naws ssm get-parameters-by-path --path /aws/service/aws-for-fluent-bit/\n```\n\nFor more see [our docs](https://github.com/aws/aws-for-fluent-bit#public-images).\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": ["2023-05-23T17:28:23Z", "2023-04-06T21:32:45Z", "2023-03-07T03:32:05Z", "2022-11-09T21:22:05Z", "2022-07-26T21:49:29Z", "2022-06-09T21:28:34Z", "2022-01-31T19:19:39Z", "2021-11-15T23:11:59Z", "2021-09-24T01:07:36Z", "2021-08-26T22:12:54Z", "2021-04-06T22:52:00Z", "2020-12-08T22:37:05Z", "2020-11-02T23:07:35Z", "2020-10-12T19:42:24Z", "2020-09-16T02:44:55Z", "2020-07-17T23:07:36Z", "2020-03-04T21:08:25Z", "2020-01-08T23:40:32Z", "2019-12-13T00:27:30Z", "2019-11-26T02:32:26Z"]}, {"name": "amazon-codeguru-jupyterlab-extension", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon CodeGuru JupyterLab Extension\n\nAmazon CodeGuru extension for [JupyterLab](https://jupyterlab.readthedocs.io/en/stable/) and [SageMaker Studio](https://aws.amazon.com/sagemaker/studio/).\nThis extension runs scans on your notebook files and provides security recommendations and quality improvements to your code.\n\n## Requirements\n\n- JupyterLab >= 3.0\n\n## Install\n\n```sh\npip install amazon_codeguru_jupyterlab_extension\n```\n\n## Uninstall\n\n```sh\npip uninstall amazon_codeguru_jupyterlab_extension\n```\n\n## Development\n\n### Prerequisites\n\nEnsure the following dependencies are available in your environment.\n\n- Python >= 3.8\n- JupyterLab >= 3.0\n- NodeJS >= 18\n\nAlternatively, you can create a conda virtual environment with the following commands:\n\n```sh\nconda env update --file binder/environment.yml\nconda activate amazon-codeguru-extension-demo\n```\n\n### Manual Setup\n\n1. Install the Python package in development mode.\n\n```sh\npip install -e .\n```\n\n2. Link the extension with JupyterLab.\n\n```sh\njupyter labextension develop . --overwrite\n```\n\n3. Build the Typescript source.\n\n```sh\njlpm build\n# or\njlpm watch # automatically rebuild changes\n```\n\n4. Start the JupyterLab server\n\n```sh\njupyter lab\n```\n\n### Quick Setup\n\nRun the following command to quickly build and install the extension.\n\n```sh\npython3 binder/postBuild\n```\n\n## Release\n\nThis extension can be distributed as a Python package.\nFirst, install build dependencies:\n\n```sh\npip install build twine hatch\n```\n\nBump the version using `hatch`. By default this will create a tag.\n\n```sh\nhatch version <new-version>\n```\n\nTo generate a new Python source package (`.tar.gz`) and the binary package (`.whl`) in the `dist/` directory, run the following command:\n\n```sh\npython -m build\n```\n\nThen to upload the package to PyPI, run the following command:\n\n```sh\ntwine upload dist/*\n```\n\n## Security\n\nSee [SECURITY](SECURITY.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2023-07-11T01:41:03Z", "2023-07-05T21:40:28Z", "2023-05-10T03:30:05Z"]}, {"name": "amazon-codeguru-profiler-python-agent", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon CodeGuru Profiler Python Agent\n\nFor more details, check the documentation: https://docs.aws.amazon.com/codeguru/latest/profiler-ug/what-is-codeguru-profiler.html\n\n## How to use it\n\nThis package is being released to PyPI as [codeguru-profiler-agent](https://pypi.org/project/codeguru-profiler-agent), so use it as any Python package from PyPI.\n\nFor a demo application that uses this agent, check our [demo application](https://github.com/aws-samples/aws-codeguru-profiler-python-demo-application).\n\n## How to contribute\n\nCheck the GitHub repository at [aws/amazon-codeguru-profiler-python-agent](https://github.com/aws/amazon-codeguru-profiler-python-agent).\n\nSee [CONTRIBUTING](CONTRIBUTING.md) for details.\n\n## How to release\n\nSee [DEVELOPMENT](DEVELOPMENT.md) for more information.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License. See [LICENSE](LICENSE) for details.\n\n", "release_dates": ["2024-02-15T20:12:47Z", "2021-09-14T09:36:36Z", "2021-07-27T11:44:09Z", "2021-07-12T17:18:24Z", "2021-06-30T09:33:36Z", "2021-06-28T21:57:36Z", "2021-03-29T12:42:07Z", "2021-03-22T15:10:58Z", "2021-03-08T13:16:52Z", "2021-02-17T14:46:12Z", "2021-02-16T14:01:51Z", "2021-01-07T16:25:46Z"]}, {"name": "amazon-cognito-dotnet", "description": "Official repository for Amazon Cognito Sync Manager SDK for Dotnet.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Sync Manager SDK for .NET (Amazon Cognito)\n\nOBSOLETE - This repository is now obsolete. Cognito Sync Manager is now included directly in the [AWS SDK for .NET](https://github.com/aws/aws-sdk-net).\n\n[Amazon Cognito](http://aws.amazon.com/cognito/) is a service that makes it easy to save user data, such as app preferences or game state, in the AWS Cloud without writing any backend code or managing any infrastructure. You can save data locally on users\u2019 devices allowing your applications to work even when the devices are offline. You can also synchronize data across a user\u2019s devices so that their app experience will be consistent regardless of the device they use.\n", "release_dates": []}, {"name": "amazon-cognito-sync-manager-net", "description": "The AWS SDK for .NET CognitoSync SyncManager enables customers to easily sync data with Amazon Cognito with non-constant network availability.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWSSDK.CognitoSync.SyncManager\n\n__Please use Amazon AppSync. This repository is provided for existing customers.__\n\nThe AWS SDK for .NET CognitoSync SyncManager enables customers to easily sync data with Amazon Cognito with non-constant network availability.\n\nIf you are looking for the Unity SyncManager, it is located in the Unity archive: https://github.com/aws/aws-sdk-unity-net\n", "release_dates": []}, {"name": "amazon-documentdb-jdbc-driver", "description": "Amazon DocumentDB JDBC driver to connect from BI tools and execute SQL Queries", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "\ufeff# Amazon DocumentDB JDBC Driver\n![Code Coverage Instructions](./.github/badges/jacoco.svg)\n![Code Coverage Branches](./.github/badges/branches.svg)\n\n## Overview\n\nThe JDBC driver for the Amazon DocumentDB managed document database provides an \nSQL-relational interface for developers and BI tool users.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n## Documentation\n\nSee the [product documentation](src/markdown/index.md) for more detailed information about this driver.\n\n## Setup and Usage\n\nTo setup and use the DocumentDB JDBC driver, follow [these directions](src/markdown/setup/setup.md).\n\n## Connection String Syntax\n\n```\njdbc:documentdb://[<user>[:<password>]@]<hostname>[:<port>]/<database-name>[?<option>=<value>[&<option>=<value>[...]]]\n```\n\nFor more information about connecting to an Amazon DocumentDB database using this JDBC driver, see\nthe [connection string documentation](src/markdown/setup/connection-string.md) for more details.\n\n## Schema Discovery\n\nThe Amazon DocumentDB JDBC driver can perform automatic schema discovery and generate an SQL to \nDocumentDB schema mapping. See the [schema discovery documentation](src/markdown/schema/schema-discovery.md) \nfor more details of this process.\n\n## Schema Management\n\nSchema can be managed in the following ways:\n\n- generated\n- removed\n- listed\n- exported\n- imported\n  \nSee the [schema management documentation](src/markdown/schema/manage-schema-cli.md) and \n[table schemas JSON format](src/markdown/schema/table-schemas-json-format.md) for further \ninformation.\n\n## SQL and JDBC Limitations\n\nThe Amazon DocumentDB JDBC driver has a number of important limitations. See the\n[SQL limitations documentation](src/markdown/sql/sql-limitations.md) and\n[JDBC limitations documentation](src/markdown/jdbc/jdbc-limitations.md) for more information.\n\n\n## Troubleshooting Guide\n\nIf you're having an issue using the Amazon DocumentDB JDBC driver, consult the \n[Troubleshooting Guide](src/markdown/support/troubleshooting-guide.md) to see if has a solution for \nyour issue.\n\n## Security Notice\n\nIf you discover a potential security issue in this project, please consult our [security guidance page](SECURITY.md).\n\n## Contributor's Getting Started Guide\n\nIf you're a developer and want to contribute to this project, ensure to read and follow the\n[Getting Started as a Developer](GETTING_STARTED.md) guide.\n", "release_dates": ["2023-01-06T00:56:20Z", "2022-12-21T18:23:17Z", "2022-12-12T23:54:49Z", "2022-11-03T23:01:25Z", "2022-10-21T23:17:46Z", "2022-10-03T23:35:49Z", "2022-08-19T18:19:16Z", "2022-07-22T22:23:33Z", "2022-06-30T21:47:00Z", "2022-05-28T00:56:16Z", "2022-05-06T17:25:53Z", "2022-04-08T00:58:49Z", "2022-04-01T21:29:35Z", "2022-03-05T00:19:48Z", "2022-02-04T18:23:33Z", "2021-12-16T23:52:53Z", "2021-11-26T23:45:08Z", "2021-11-11T01:32:43Z", "2021-10-25T20:27:39Z", "2021-09-30T01:32:54Z", "2021-09-24T23:11:36Z", "2021-09-04T00:27:46Z", "2021-08-24T22:44:23Z", "2021-08-23T23:47:01Z", "2021-08-16T23:25:17Z", "2021-07-20T18:01:40Z", "2021-07-19T18:55:39Z", "2021-06-28T21:38:41Z", "2021-06-10T21:00:22Z"]}, {"name": "amazon-documentdb-odbc-driver", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon DocumentDB ODBC Driver\n![Code Coverage Windows](./.github/badges/coverage_badge_windows.svg)\n![Code Coverage macOS](./.github/badges/coverage_badge_macOS.svg)\n![Code Coverage Linux](./.github/badges/coverage_badge_linux.svg)\n\n## Overview\n\nThe ODBC driver for the Amazon DocumentDB managed document database provides an \nSQL-relational interface for developers and BI tool users.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n## Documentation\n\nSee the [product documentation](src/markdown/index.md) for more detailed information about this driver, such as setup and configuration.\n\n## Setup and Usage\n\nTo setup and use the DocumentDB ODBC driver, follow [these directions](src/markdown/setup/setup.md).\n\n## Connection String Syntax\n\n```\nDRIVER={Amazon DocumentDB};HOSTNAME=<host>:<port>;DATABASE=<database>;USER=<user>;PASSWORD=<password>;<option>=<value>;\n```\n\nFor more information about connecting to an Amazon DocumentDB database using this ODBC driver, see\nthe [connection string documentation](src/markdown/setup/connection-string.md) for more details.\n\n## Security Notice\n\nIf you discover a potential security issue in this project, please consult our [security guidance page](SECURITY.md).\n", "release_dates": ["2023-02-01T19:34:27Z", "2022-10-14T00:11:20Z", "2022-09-17T00:10:56Z"]}, {"name": "amazon-ec2-hibinit-agent", "description": null, "language": "Shell", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "The Amazon Linux hibernation agent.\n\nThe purpose of this agent is to create a setup for an instance to support hibernation feature. \nThe setup is created only on supported instance types. \n\nThis agent does several things upon startup:\n1. It checks for sufficient swap space to allow hibernation and fails if not enough space\n2. If there's no swap file or the existing swap file isn't of a sufficient size, a swap file is created\n     1. If `touch-swap` is enabled, all the swap file's blocks will be touched\n        so that the root EBS volume is pre-warmed.\n3. It updates the offset of the swap file in the kernel using `snapshot_set_swap_area` ioctl.\n4. It updates the resume offset and resume device in grub file.\n\n## Building in Red hat\n\n1- Install Development Tools in Red Hat to build the RPM package\n```\nsudo dnf group install \"Development Tools\"\nmkdir -p ~/rpmbuild/{BUILD,BUILDROOT,RPMS,SOURCES,SPECS,SRPMS}\n```\n2- Install required build packages for ec2-hibernate-linux-agent.\n\n```\nsudo yum install python3-devel\nsudo yum install selinux-policy-devel\n```\n\n3- Download the package from github repository. You can replace `1.0.3` with any release version number.\n\n```\necho '%_topdir %(echo $HOME)/rpmbuild' > ~/.rpmmacros\nwget https://github.com/aws/amazon-ec2-hibinit-agent/archive/v1.0.3/ec2-hibinit-agent-1.0.3.tar.gz\ntar -xf ec2-hibinit-agent-1.0.3.tar.gz \n```\n\n4- Copy spec file to SPEC directory.\n\n```\ncd ~/rpmbuild//SPECS\ncp ~/rpmbuild/SOURCES/amazon-ec2-hibinit-agent-1.0.3/packaging/rhel/ec2-hibinit-agent.spec ~/rpmbuild/SPECS/\n\n```\n5- Build the Spec file \n\n```\nnohup rpmbuild -bb --target=noarch ec2-hibinit-agent.spec\n```\nYou will find the RPM package generated at `~/rpmbuild/RPMS/noarch/` directory\n\n\n\n## Building in SUSE Linux \n\n1- Install Development Tools in Suse Linux to build the RPM package\n```\nsudo zypper install rpm-build\nmkdir -p ~/rpmbuild/{BUILD,BUILDROOT,RPMS,SOURCES,SPECS,SRPMS}\n```\n2- Install required build packages for ec2-hibernate-linux-agent.\n\n```\nsudo zypper install python3-devel\nsudo zypper install tuned\nsudo zypper install python-rpm-generators\n```\n\n3- Download the package from github repository. You can replace `1.0.3` with any release version number.\n\n```\necho '%_topdir %(echo $HOME)/rpmbuild' > ~/.rpmmacros\ncd ~/rpmbuild/SOURCES\nwget https://github.com/aws/amazon-ec2-hibinit-agent/archive/v1.0.3/ec2-hibinit-agent-1.0.3.tar.gz\ntar -xf ec2-hibinit-agent-1.0.3.tar.gz \n```\n\n4- Copy spec file to SPEC directory. You can replace `1.0.3` with any release version number.\n\n```\ncd ~/rpmbuild//SPECS\ncp ~/rpmbuild/SOURCES/amazon-ec2-hibinit-agent-1.0.3/packaging/sles/ec2-hibinit-agent.spec ~/rpmbuild/SPECS/\n\n```\n5- Build the Spec file \n\n```\nnohup rpmbuild -bb --target=noarch ec2-hibernate-linux-agent.spec\n```\nYou will find the RPM package generated at `~/rpmbuild/RPMS/noarch/` directory\n\n", "release_dates": ["2023-12-28T19:57:49Z", "2023-10-18T21:05:13Z", "2023-09-28T20:54:31Z", "2022-05-10T20:18:00Z", "2021-05-25T03:58:37Z", "2021-01-15T00:14:09Z"]}, {"name": "amazon-ec2-instance-selector", "description": "A CLI tool and go library which recommends instance types based on resource criteria like vcpus and memory", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<h1>Amazon EC2 Instance Selector</h1>\n\n<h4>A CLI tool and go library which recommends instance types based on resource criteria like vcpus and memory.</h4>\n\n<p>\n  <a href=\"https://golang.org/doc/go1.17\">\n    <img src=\"https://img.shields.io/github/go-mod/go-version/aws/amazon-ec2-instance-selector?color=blueviolet\" alt=\"go-version\">\n  </a>\n  <a href=\"https://opensource.org/licenses/Apache-2.0\">\n    <img src=\"https://img.shields.io/badge/License-Apache%202.0-ff69b4.svg\" alt=\"license\">\n  </a>\n  <a href=\"https://goreportcard.com/report/github.com/aws/amazon-ec2-instance-selector\">\n    <img src=\"https://goreportcard.com/badge/github.com/aws/amazon-ec2-instance-selector\" alt=\"go-report-card\">\n  </a>\n  <a href=\"https://hub.docker.com/r/amazon/amazon-ec2-instance-selector\">\n    <img src=\"https://img.shields.io/docker/pulls/amazon/amazon-ec2-instance-selector\" alt=\"docker-pulls\">\n  </a>\n</p>\n\n![EC2 Instance Selector CI and Release](https://github.com/aws/amazon-ec2-instance-selector/workflows/EC2%20Instance%20Selector%20CI%20and%20Release/badge.svg)\n\n<div>\n<hr>\n</div>\n\n## Summary\n\nThere are over 270 different instance types available on EC2 which can make the process of selecting appropriate instance types difficult. Instance Selector helps you select compatible instance types for your application to run on. The command line interface can be passed resource criteria like vcpus, memory, network performance, and much more and then return the available, matching instance types. \n\nIf you are using spot instances to save on costs, it is a best practice to use multiple instances types within your auto-scaling group (ASG) to ensure your application doesn't experience downtime due to one instance type being interrupted. Instance Selector will help to find a set of instance types that your application can run on.\n\nInstance Selector can also be consumed as a go library for direct integration into your go code.\n\n## Major Features\n\n- Filter AWS Instance Types using declarative resource criteria like vcpus, memory, network performance, and much more!\n- Aggregate filters allow for more opinionated instance selections like `--base-instance-type` and `--flexible`\n- Consumable as a go library\n\n## Installation and Configuration\n\n#### Install w/ Homebrew\n\n```\nbrew tap aws/tap\nbrew install ec2-instance-selector\n```\n\n#### Install w/ Curl for Linux/Mac\n\n```\n$ curl -Lo ec2-instance-selector https://github.com/aws/amazon-ec2-instance-selector/releases/download/v2.4.1/ec2-instance-selector-`uname | tr '[:upper:]' '[:lower:]'`-amd64 && chmod +x ec2-instance-selector\n$ sudo mv ec2-instance-selector /usr/local/bin/\n$ ec2-instance-selector --version\n```\n\nTo execute the CLI, you will need AWS credentials configured. Take a look at the [AWS CLI configuration documentation](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#config-settings-and-precedence) for details on the various ways to configure credentials. An easy way to try out the ec2-instance-selector CLI is to populate the following environment variables with your AWS API credentials.\n\n```\nexport AWS_ACCESS_KEY_ID=\"...\"\nexport AWS_SECRET_ACCESS_KEY=\"...\"\n```\n\nIf you already have an AWS CLI profile setup, you can pass that directly into ec2-instance-selector:\n\n```\n$ ec2-instance-selector --profile my-aws-cli-profile --vcpus 2 --region us-east-1\n```\n\nYou can set the AWS_REGION environment variable if you don't want to pass in `--region` on each run.\n\n```\n$ export AWS_REGION=\"us-east-1\"\n```\n\n## Examples\n\n### CLI\n\n**Find Instance Types with 4 GiB of memory, 2 vcpus, and runs on the x86_64 CPU architecture**\n```\n$ ec2-instance-selector --memory 4 --vcpus 2 --cpu-architecture x86_64 -r us-east-1\nc5.large\nc5a.large\nc5ad.large\nc5d.large\nc6a.large\nc6i.large\nt2.medium\nt3.medium\nt3a.medium\n```\n\n**Find instance types that support 100GB/s networking that can be purchased as spot instances**\n```\n$ ec2-instance-selector --network-performance 100 --usage-class spot -r us-east-1\nc5n.18xlarge\nc5n.metal\nc6gn.16xlarge\ndl1.24xlarge\ng4dn.metal\ng5.48xlarge\ni3en.24xlarge\ni3en.metal\nim4gn.16xlarge\ninf1.24xlarge\nm5dn.24xlarge\nm5dn.metal\nm5n.24xlarge\nm5n.metal\nm5zn.12xlarge\nm5zn.metal\np3dn.24xlarge\np4d.24xlarge\nr5dn.24xlarge\nr5dn.metal\n```\n\n**Short Table Output**\n```\n$ ec2-instance-selector --memory 4 --vcpus 2 --cpu-architecture x86_64 -r us-east-1 -o table\nInstance Type        VCPUs        Mem (GiB)\n-------------        -----        ---------\nc5.large             2            4\nc5a.large            2            4\nc5ad.large           2            4\nc5d.large            2            4\nc6a.large            2            4\nc6i.large            2            4\nt2.medium            2            4\nt3.medium            2            4\nt3a.medium           2            4\n```\n\n**Wide Table Output**\n```\n$ ec2-instance-selector --memory 4 --vcpus 2 --cpu-architecture x86_64 -r us-east-1 -o table-wide\nInstance Type  VCPUs   Mem (GiB)  Hypervisor  Current Gen  Hibernation Support  CPU Arch      Network Performance  ENIs    GPUs    GPU Mem (GiB)  GPU Info  On-Demand Price/Hr  Spot Price/Hr (30d avg)  \n-------------  -----   ---------  ----------  -----------  -------------------  --------      -------------------  ----    ----    -------------  --------  ------------------  -----------------------  \nc5.large       2       4          nitro       true         true                 x86_64        Up to 10 Gigabit     3       0       0              none      -Not Fetched-       $0.03932                 \nc5a.large      2       4          nitro       true         false                x86_64        Up to 10 Gigabit     3       0       0              none      -Not Fetched-       $0.03822                 \nc5ad.large     2       4          nitro       true         false                x86_64        Up to 10 Gigabit     3       0       0              none      -Not Fetched-       $0.03449                 \nc5d.large      2       4          nitro       true         true                 x86_64        Up to 10 Gigabit     3       0       0              none      $0.096              $0.03983                 \nc6a.large      2       4          nitro       true         false                x86_64        Up to 12.5 Gigabit   3       0       0              none      $0.0765             $0.034                   \nc6i.large      2       4          nitro       true         false                x86_64        Up to 12.5 Gigabit   3       0       0              none      $0.085              $0.03605                 \nc6id.large     2       4          nitro       true         false                x86_64        Up to 12.5 Gigabit   3       0       0              none      -Not Fetched-       $0.034                   \nt2.medium      2       4          xen         true         true                 i386, x86_64  Low to Moderate      3       0       0              none      $0.0464             $0.0139                  \nt3.medium      2       4          nitro       true         true                 x86_64        Up to 5 Gigabit      3       0       0              none      $0.0416             $0.0125                  \nt3a.medium     2       4          nitro       true         true                 x86_64        Up to 5 Gigabit      3       0       0              none      -Not Fetched-       $0.01246\n```\n\n**Interactive Output**\n```\n$ ec2-instance-selector -o interactive\n```\nhttps://user-images.githubusercontent.com/68402662/184218343-6b236d4a-3fe6-42ae-9fe3-3fd3ee92a4b5.mov\n\n**Sort by memory in ascending order using shorthand**\n```\n$ ec2-instance-selector -r us-east-1 -o table-wide --max-results 10 --sort-by memory --sort-direction asc\nInstance Type  VCPUs   Mem (GiB)  Hypervisor  Current Gen  Hibernation Support  CPU Arch      Network Performance  ENIs    GPUs    GPU Mem (GiB)  GPU Info  On-Demand Price/Hr  Spot Price/Hr (30d avg)  \n-------------  -----   ---------  ----------  -----------  -------------------  --------      -------------------  ----    ----    -------------  --------  ------------------  -----------------------  \nt2.nano        1       0.5        xen         true         true                 i386, x86_64  Low to Moderate      2       0       0              none      $0.0058             -Not Fetched-            \nt4g.nano       2       0.5        nitro       true         false                arm64         Up to 5 Gigabit      2       0       0              none      $0.0042             $0.0013                  \nt3a.nano       2       0.5        nitro       true         true                 x86_64        Up to 5 Gigabit      2       0       0              none      -Not Fetched-       $0.00328                 \nt3.nano        2       0.5        nitro       true         true                 x86_64        Up to 5 Gigabit      2       0       0              none      $0.0052             $0.0016                  \nt1.micro       1       0.6123     xen         false        false                i386, x86_64  Very Low             2       0       0              none      -Not Fetched-       $0.00205                 \nt3a.micro      2       1          nitro       true         true                 x86_64        Up to 5 Gigabit      2       0       0              none      -Not Fetched-       $0.00284                 \nt3.micro       2       1          nitro       true         true                 x86_64        Up to 5 Gigabit      2       0       0              none      $0.0104             $0.0031                  \nt2.micro       1       1          xen         true         true                 i386, x86_64  Low to Moderate      2       0       0              none      -Not Fetched-       $0.0035                  \nt4g.micro      2       1          nitro       true         false                arm64         Up to 5 Gigabit      2       0       0              none      -Not Fetched-       $0.0025                  \nm1.small       1       1.69922    xen         false        false                i386, x86_64  Low                  2       0       0              none      -Not Fetched-       $0.01876\nNOTE: 547 entries were truncated, increase --max-results to see more\n```\nAvailable shorthand flags: vcpus, memory, gpu-memory-total, network-interfaces, spot-price, on-demand-price, instance-storage, ebs-optimized-baseline-bandwidth, ebs-optimized-baseline-throughput, ebs-optimized-baseline-iops, gpus, inference-accelerators\n\n**Sort by memory in descending order using JSON path**\n```\n$ ec2-instance-selector -r us-east-1 -o table-wide --max-results 10 --sort-by .MemoryInfo.SizeInMiB --sort-direction desc\nInstance Type      VCPUs   Mem (GiB)  Hypervisor  Current Gen  Hibernation Support  CPU Arch  Network Performance  ENIs    GPUs    GPU Mem (GiB)  GPU Info  On-Demand Price/Hr  Spot Price/Hr (30d avg)  \n-------------      -----   ---------  ----------  -----------  -------------------  --------  -------------------  ----    ----    -------------  --------  ------------------  -----------------------  \nu-12tb1.112xlarge  448     12,288     nitro       true         false                x86_64    100 Gigabit          15      0       0              none      $109.2              -Not Fetched-            \nu-9tb1.112xlarge   448     9,216      nitro       true         false                x86_64    100 Gigabit          15      0       0              none      -Not Fetched-       -Not Fetched-            \nu-6tb1.112xlarge   448     6,144      nitro       true         false                x86_64    100 Gigabit          15      0       0              none      $54.6               -Not Fetched-            \nu-6tb1.56xlarge    224     6,144      nitro       true         false                x86_64    100 Gigabit          15      0       0              none      $46.40391           -Not Fetched-            \nx2iedn.metal       128     4,096      none        true         false                x86_64    100 Gigabit          15      0       0              none      $26.676             $20.92296                \nx2iedn.32xlarge    128     4,096      nitro       true         false                x86_64    100 Gigabit          15      0       0              none      $26.676             $8.70294                 \nx1e.32xlarge       128     3,904      xen         true         false                x86_64    25 Gigabit           8       0       0              none      $26.688             $8.0064                  \nx2iedn.24xlarge    96      3,072      nitro       true         false                x86_64    75 Gigabit           15      0       0              none      $20.007             $6.0021                  \nu-3tb1.56xlarge    224     3,072      nitro       true         false                x86_64    50 Gigabit           8       0       0              none      $27.3               -Not Fetched-            \nx2idn.metal        128     2,048      none        true         false                x86_64    100 Gigabit          15      0       0              none      $13.338             $7.46603\nNOTE: 547 entries were truncated, increase --max-results to see more\n```\nJSON path must point to a field in the [instancetype.Details struct](https://github.com/aws/amazon-ec2-instance-selector/blob/5bffbf2750ee09f5f1308bdc8d4b635a2c6e2721/pkg/instancetypes/instancetypes.go#L37).\n\n**Example output of instance type object using Verbose output**\n```\n$ ec2-instance-selector --max-results 1 -v\n[\n    {\n        \"AutoRecoverySupported\": true,\n        \"BareMetal\": false,\n        \"BurstablePerformanceSupported\": false,\n        \"CurrentGeneration\": false,\n        \"DedicatedHostsSupported\": true,\n        \"EbsInfo\": {\n            \"EbsOptimizedInfo\": {\n                \"BaselineBandwidthInMbps\": 1750,\n                \"BaselineIops\": 10000,\n                \"BaselineThroughputInMBps\": 218.75,\n                \"MaximumBandwidthInMbps\": 3500,\n                \"MaximumIops\": 20000,\n                \"MaximumThroughputInMBps\": 437.5\n            },\n            \"EbsOptimizedSupport\": \"default\",\n            \"EncryptionSupport\": \"supported\",\n            \"NvmeSupport\": \"required\"\n        },\n        \"FpgaInfo\": null,\n        \"FreeTierEligible\": false,\n        \"GpuInfo\": null,\n        \"HibernationSupported\": false,\n        \"Hypervisor\": \"nitro\",\n        \"InferenceAcceleratorInfo\": null,\n        \"InstanceStorageInfo\": null,\n        \"InstanceStorageSupported\": false,\n        \"InstanceType\": \"a1.2xlarge\",\n        \"MemoryInfo\": {\n            \"SizeInMiB\": 16384\n        },\n        \"NetworkInfo\": {\n            \"DefaultNetworkCardIndex\": 0,\n            \"EfaInfo\": null,\n            \"EfaSupported\": false,\n            \"EnaSupport\": \"required\",\n            \"EncryptionInTransitSupported\": false,\n            \"Ipv4AddressesPerInterface\": 15,\n            \"Ipv6AddressesPerInterface\": 15,\n            \"Ipv6Supported\": true,\n            \"MaximumNetworkCards\": 1,\n            \"MaximumNetworkInterfaces\": 4,\n            \"NetworkCards\": [\n                {\n                    \"MaximumNetworkInterfaces\": 4,\n                    \"NetworkCardIndex\": 0,\n                    \"NetworkPerformance\": \"Up to 10 Gigabit\"\n                }\n            ],\n            \"NetworkPerformance\": \"Up to 10 Gigabit\"\n        },\n        \"PlacementGroupInfo\": {\n            \"SupportedStrategies\": [\n                \"cluster\",\n                \"partition\",\n                \"spread\"\n            ]\n        },\n        \"ProcessorInfo\": {\n            \"SupportedArchitectures\": [\n                \"arm64\"\n            ],\n            \"SustainedClockSpeedInGhz\": 2.3\n        },\n        \"SupportedBootModes\": [\n            \"uefi\"\n        ],\n        \"SupportedRootDeviceTypes\": [\n            \"ebs\"\n        ],\n        \"SupportedUsageClasses\": [\n            \"on-demand\",\n            \"spot\"\n        ],\n        \"SupportedVirtualizationTypes\": [\n            \"hvm\"\n        ],\n        \"VCpuInfo\": {\n            \"DefaultCores\": 8,\n            \"DefaultThreadsPerCore\": 1,\n            \"DefaultVCpus\": 8,\n            \"ValidCores\": null,\n            \"ValidThreadsPerCore\": null\n        },\n        \"OndemandPricePerHour\": 0.204,\n        \"SpotPrice\": 0.03939999999999999\n    }\n]\nNOTE: 497 entries were truncated, increase --max-results to see more\n```\nNOTE: Use this JSON format as reference when finding JSON paths for sorting\n\n**All CLI Options**\n\n```\n$ ec2-instance-selector --help\n```\n\n```bash#help\nec2-instance-selector is a CLI tool to filter EC2 instance types based on resource criteria.\nFiltering allows you to select all the instance types that match your application requirements.\nFull docs can be found at github.com/aws/amazon-ec2-instance-selector\n\nUsage:\n  ec2-instance-selector [flags]\n\nExamples:\nec2-instance-selector --vcpus 4 --region us-east-2 --availability-zones us-east-2b\nec2-instance-selector --memory-min 4 --memory-max 8 --vcpus-min 4 --vcpus-max 8 --region us-east-2\n\nFilter Flags:\n      --allow-list string                              List of allowed instance types to select from w/ regex syntax (Example: m[3-5]\\.*)\n      --auto-recovery                                  EC2 Auto-Recovery supported\n  -z, --availability-zones strings                     Availability zones or zone ids to check EC2 capacity offered in specific AZs\n      --baremetal                                      Bare Metal instance types (.metal instances)\n  -b, --burst-support                                  Burstable instance types\n  -a, --cpu-architecture string                        CPU architecture [x86_64/amd64, x86_64_mac, i386, or arm64]\n      --cpu-manufacturer string                        CPU manufacturer [amd, intel, aws]\n      --current-generation                             Current generation instance types (explicitly set this to false to not return current generation instance types)\n      --dedicated-hosts                                Dedicated Hosts supported\n      --deny-list string                               List of instance types which should be excluded w/ regex syntax (Example: m[1-2]\\.*)\n      --disk-encryption                                EBS or local instance storage where encryption is supported or required\n      --disk-type string                               Disk Type: [hdd or ssd]\n      --ebs-optimized                                  EBS Optimized is supported or default\n      --ebs-optimized-baseline-bandwidth string        EBS Optimized baseline bandwidth (Example: 4 GiB) (sets --ebs-optimized-baseline-bandwidth-min and -max to the same value)\n      --ebs-optimized-baseline-bandwidth-max string    Maximum EBS Optimized baseline bandwidth (Example: 4 GiB) If --ebs-optimized-baseline-bandwidth-min is not specified, the lower bound will be 0\n      --ebs-optimized-baseline-bandwidth-min string    Minimum EBS Optimized baseline bandwidth (Example: 4 GiB) If --ebs-optimized-baseline-bandwidth-max is not specified, the upper bound will be infinity\n      --ebs-optimized-baseline-iops int                EBS Optimized baseline IOPS per second (Example: 10000) (sets --ebs-optimized-baseline-iops-min and -max to the same value)\n      --ebs-optimized-baseline-iops-max int            Maximum EBS Optimized baseline IOPS per second (Example: 10000) If --ebs-optimized-baseline-iops-min is not specified, the lower bound will be 0\n      --ebs-optimized-baseline-iops-min int            Minimum EBS Optimized baseline IOPS per second (Example: 10000) If --ebs-optimized-baseline-iops-max is not specified, the upper bound will be infinity\n      --ebs-optimized-baseline-throughput string       EBS Optimized baseline throughput per second (Example: 4 GiB) (sets --ebs-optimized-baseline-throughput-min and -max to the same value)\n      --ebs-optimized-baseline-throughput-max string   Maximum EBS Optimized baseline throughput per second (Example: 4 GiB) If --ebs-optimized-baseline-throughput-min is not specified, the lower bound will be 0\n      --ebs-optimized-baseline-throughput-min string   Minimum EBS Optimized baseline throughput per second (Example: 4 GiB) If --ebs-optimized-baseline-throughput-max is not specified, the upper bound will be infinity\n      --efa-support                                    Instance types that support Elastic Fabric Adapters (EFA)\n  -e, --ena-support                                    Instance types where ENA is supported or required\n  -f, --fpga-support                                   FPGA instance types\n      --free-tier                                      Free Tier supported\n      --gpu-manufacturer string                        GPU Manufacturer name (Example: NVIDIA)\n      --gpu-memory-total string                        Number of GPUs' total memory (Example: 4 GiB) (sets --gpu-memory-total-min and -max to the same value)\n      --gpu-memory-total-max string                    Maximum Number of GPUs' total memory (Example: 4 GiB) If --gpu-memory-total-min is not specified, the lower bound will be 0\n      --gpu-memory-total-min string                    Minimum Number of GPUs' total memory (Example: 4 GiB) If --gpu-memory-total-max is not specified, the upper bound will be infinity\n      --gpu-model string                               GPU Model name (Example: K520)\n  -g, --gpus int32                                     Total Number of GPUs (Example: 4) (sets --gpus-min and -max to the same value)\n      --gpus-max int32                                 Maximum Total Number of GPUs (Example: 4) If --gpus-min is not specified, the lower bound will be 0\n      --gpus-min int32                                 Minimum Total Number of GPUs (Example: 4) If --gpus-max is not specified, the upper bound will be infinity\n      --hibernation-support                            Hibernation supported\n      --hypervisor string                              Hypervisor: [xen or nitro]\n      --inference-accelerator-manufacturer string      Inference Accelerator Manufacturer name (Example: AWS)\n      --inference-accelerator-model string             Inference Accelerator Model name (Example: Inferentia)\n      --inference-accelerators int                     Total Number of inference accelerators (Example: 4) (sets --inference-accelerators-min and -max to the same value)\n      --inference-accelerators-max int                 Maximum Total Number of inference accelerators (Example: 4) If --inference-accelerators-min is not specified, the lower bound will be 0\n      --inference-accelerators-min int                 Minimum Total Number of inference accelerators (Example: 4) If --inference-accelerators-max is not specified, the upper bound will be infinity\n      --instance-storage string                        Amount of local instance storage (Example: 4 GiB) (sets --instance-storage-min and -max to the same value)\n      --instance-storage-max string                    Maximum Amount of local instance storage (Example: 4 GiB) If --instance-storage-min is not specified, the lower bound will be 0\n      --instance-storage-min string                    Minimum Amount of local instance storage (Example: 4 GiB) If --instance-storage-max is not specified, the upper bound will be infinity\n      --ipv6                                           Instance Types that support IPv6\n  -m, --memory string                                  Amount of Memory available (Example: 4 GiB) (sets --memory-min and -max to the same value)\n      --memory-max string                              Maximum Amount of Memory available (Example: 4 GiB) If --memory-min is not specified, the lower bound will be 0\n      --memory-min string                              Minimum Amount of Memory available (Example: 4 GiB) If --memory-max is not specified, the upper bound will be infinity\n      --network-encryption                             Instance Types that support automatic network encryption in-transit\n      --network-interfaces int32                       Number of network interfaces (ENIs) that can be attached to the instance (sets --network-interfaces-min and -max to the same value)\n      --network-interfaces-max int32                   Maximum Number of network interfaces (ENIs) that can be attached to the instance If --network-interfaces-min is not specified, the lower bound will be 0\n      --network-interfaces-min int32                   Minimum Number of network interfaces (ENIs) that can be attached to the instance If --network-interfaces-max is not specified, the upper bound will be infinity\n      --network-performance int                        Bandwidth in Gib/s of network performance (Example: 100) (sets --network-performance-min and -max to the same value)\n      --network-performance-max int                    Maximum Bandwidth in Gib/s of network performance (Example: 100) If --network-performance-min is not specified, the lower bound will be 0\n      --network-performance-min int                    Minimum Bandwidth in Gib/s of network performance (Example: 100) If --network-performance-max is not specified, the upper bound will be infinity\n      --nvme                                           EBS or local instance storage where NVME is supported or required\n      --placement-group-strategy string                Placement group strategy: [cluster, partition, spread]\n      --price-per-hour float                           Price/hour in USD (Example: 0.09) (sets --price-per-hour-min and -max to the same value)\n      --price-per-hour-max float                       Maximum Price/hour in USD (Example: 0.09) If --price-per-hour-min is not specified, the lower bound will be 0\n      --price-per-hour-min float                       Minimum Price/hour in USD (Example: 0.09) If --price-per-hour-max is not specified, the upper bound will be infinity\n      --root-device-type string                        Supported root device types: [ebs or instance-store]\n  -u, --usage-class string                             Usage class: [spot or on-demand]\n  -c, --vcpus int32                                    Number of vcpus available to the instance type. (sets --vcpus-min and -max to the same value)\n      --vcpus-max int32                                Maximum Number of vcpus available to the instance type. If --vcpus-min is not specified, the lower bound will be 0\n      --vcpus-min int32                                Minimum Number of vcpus available to the instance type. If --vcpus-max is not specified, the upper bound will be infinity\n      --vcpus-to-memory-ratio string                   The ratio of vcpus to GiBs of memory. (Example: 1:2)\n      --virtualization-type string                     Virtualization Type supported: [hvm or pv]\n\n\nSuite Flags:\n      --base-instance-type string   Instance Type used to retrieve similarly spec'd instance types\n      --flexible                    Retrieves a group of instance types spanning multiple generations based on opinionated defaults and user overridden resource filters\n      --service string              Filter instance types based on service support (Example: emr-5.20.0)\n\n\nGlobal Flags:\n      --cache-dir string        Directory to save the pricing and instance type caches (default \"~/.ec2-instance-selector/\")\n      --cache-ttl int           Cache TTLs in hours for pricing and instance type caches. Setting the cache to 0 will turn off caching and cleanup any on-disk caches. (default 168)\n  -h, --help                    Help\n      --max-results int         The maximum number of instance types that match your criteria to return (default 20)\n  -o, --output string           Specify the output format (table, table-wide, one-line, interactive)\n      --profile string          AWS CLI profile to use for credentials and config\n  -r, --region string           AWS Region to use for API requests (NOTE: if not passed in, uses AWS SDK default precedence)\n      --sort-by string          Specify the field to sort by. Quantity flags present in this CLI (memory, gpus, etc.) or a JSON path to the appropriate instance type field (Ex: \".MemoryInfo.SizeInMiB\") is acceptable. (default \".InstanceType\")\n      --sort-direction string   Specify the direction to sort in (ascending, asc, descending, desc) (default \"ascending\")\n  -v, --verbose                 Verbose - will print out full instance specs\n      --version                 Prints CLI version\n```\n\n\n### Go Library\n\nThis is a minimal example of using the instance selector go package directly:\n\n**cmd/examples/example1.go**\n```go#cmd/examples/example1.go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/aws/amazon-ec2-instance-selector/v2/pkg/bytequantity\"\n\t\"github.com/aws/amazon-ec2-instance-selector/v2/pkg/selector\"\n\t\"github.com/aws/aws-sdk-go-v2/config\"\n\tec2types \"github.com/aws/aws-sdk-go-v2/service/ec2/types\"\n)\n\nfunc main() {\n\t// Initialize a context for the application\n\tctx := context.Background()\n\n\t// Load an AWS session by looking at shared credentials or environment variables\n\t// https://aws.github.io/aws-sdk-go-v2/docs/configuring-sdk\n\tcfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(\"us-east-2\"))\n\tif err != nil {\n\t\tfmt.Printf(\"Oh no, AWS session credentials cannot be found: %v\", err)\n\t\treturn\n\t}\n\n\t// Instantiate a new instance of a selector with the AWS session\n\tinstanceSelector, err := selector.New(ctx, cfg)\n\tif err != nil {\n\t\tfmt.Printf(\"Oh no, there was an error :( %v\", err)\n\t\treturn\n\t}\n\n\t// Instantiate an int range filter to specify min and max vcpus\n\tvcpusRange := selector.Int32RangeFilter{\n\t\tLowerBound: 2,\n\t\tUpperBound: 4,\n\t}\n\t// Instantiate a byte quantity range filter to specify min and max memory in GiB\n\tmemoryRange := selector.ByteQuantityRangeFilter{\n\t\tLowerBound: bytequantity.FromGiB(2),\n\t\tUpperBound: bytequantity.FromGiB(4),\n\t}\n\t// Create a variable for the CPU Architecture so that it can be passed as a pointer\n\t// when creating the Filter struct\n\tcpuArch := ec2types.ArchitectureTypeX8664\n\n\t// Create a Filter struct with criteria you would like to filter\n\t// The full struct definition can be found here for all of the supported filters:\n\t// https://github.com/aws/amazon-ec2-instance-selector/blob/main/pkg/selector/types.go\n\tfilters := selector.Filters{\n\t\tVCpusRange:      &vcpusRange,\n\t\tMemoryRange:     &memoryRange,\n\t\tCPUArchitecture: &cpuArch,\n\t}\n\n\t// Pass the Filter struct to the Filter function of your selector instance\n\tinstanceTypesSlice, err := instanceSelector.Filter(ctx, filters)\n\tif err != nil {\n\t\tfmt.Printf(\"Oh no, there was an error :( %v\", err)\n\t\treturn\n\t}\n\t// Print the returned instance types slice\n\tfmt.Println(instanceTypesSlice)\n}\n```\n\n**Execute the example:**\n\n*NOTE: Make sure you have [AWS credentials](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html#cli-configure-files-settings) setup*\n```bash#cmd/examples/example1.go\n$ git clone https://github.com/aws/amazon-ec2-instance-selector.git\n$ cd amazon-ec2-instance-selector/\n$ go run cmd/examples/example1.go\n[c4.large c5.large c5a.large c5ad.large c5d.large c6i.large t2.medium t3.medium t3.small t3a.medium t3a.small]\n```\n\n## Building\nFor build instructions please consult [BUILD.md](./BUILD.md).\n\n## Communication\nIf you've run into a bug or have a new feature request, please open an [issue](https://github.com/aws/amazon-ec2-instance-selector/issues/new).\n\nCheck out the open source [Amazon EC2 Spot Instances Integrations Roadmap](https://github.com/aws/ec2-spot-instances-integrations-roadmap) to see what we're working on and give us feedback! \n\n##  Contributing\nContributions are welcome! Please read our [guidelines](https://github.com/aws/amazon-ec2-instance-selector/blob/main/CONTRIBUTING.md) and our [Code of Conduct](https://github.com/aws/amazon-ec2-instance-selector/blob/main/CODE_OF_CONDUCT.md).\n\n## License\nThis project is licensed under the [Apache-2.0](LICENSE) License.\n", "release_dates": ["2023-01-27T19:44:06Z", "2022-08-12T20:43:05Z", "2022-07-21T19:53:47Z", "2022-07-12T15:43:29Z", "2022-06-15T22:48:43Z", "2022-04-11T19:50:46Z", "2022-04-04T20:25:38Z", "2022-03-19T21:29:22Z", "2021-08-19T17:38:43Z", "2020-12-17T23:01:21Z", "2020-07-30T17:43:45Z", "2020-07-30T12:58:46Z", "2020-07-20T21:18:14Z", "2020-07-16T17:51:08Z", "2020-07-09T18:35:39Z", "2020-06-24T17:26:36Z", "2020-06-22T21:00:16Z", "2020-06-11T21:25:42Z", "2020-06-02T17:04:33Z", "2020-05-19T22:00:57Z", "2020-05-19T21:43:44Z", "2020-05-19T17:46:59Z", "2020-05-15T18:11:27Z"]}, {"name": "amazon-ec2-metadata-mock", "description": "A tool to simulate Amazon EC2 instance metadata", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon EC2 Metadata Mock\n\n**Amazon EC2 Metadata Mock (AEMM)** is a tool to simulate [Amazon EC2 instance metadata service](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html) for local testing.\n\n<br/>\n<p>\n   <a href=\"https://gallery.ecr.aws/aws-ec2/amazon-ec2-metadata-mock\">\n   <img src=\"https://img.shields.io/github/v/release/aws/amazon-ec2-metadata-mock?color=yellowgreen&label=latest%20release&sort=semver\" alt=\"latest release\">\n   </a>\n   <a href=\"https://golang.org/doc/go1.17\">\n   <img src=\"https://img.shields.io/github/go-mod/go-version/aws/amazon-ec2-metadata-mock?color=blueviolet\" alt=\"go-version\">\n   </a>\n   <a href=\"https://opensource.org/licenses/Apache-2.0\">\n   <img src=\"https://img.shields.io/badge/License-Apache%202.0-ff69b4.svg?color=orange\" alt=\"license\">\n   </a>\n   <a href=\"https://gallery.ecr.aws/aws-ec2/amazon-ec2-metadata-mock\">\n   <img src=\"https://img.shields.io/docker/pulls/amazon/amazon-ec2-metadata-mock\" alt=\"docker-pulls\">\n   </a>\n</p>\n\n![EC2 Metadata Mock CI and Release](https://github.com/aws/amazon-ec2-metadata-mock/workflows/EC2%20Metadata%20Mock%20CI%20and%20Release/badge.svg)\n\n# Table of Contents\n\n   * [Project Summary](#project-summary)\n   * [Major Features](#major-features)\n   * [Supported Metadata Categories](#supported-metadata-categories)\n   * [Getting Started](#getting-started)\n      * [Installation](#installation)\n      * [Starting AEMM](#starting-aemm)\n      * [Making a Request](#making-a-request)\n   * [Configuration](#configuration)\n      * [Defaults](#defaults)\n      * [Overrides](#overrides)\n   * [Usage](#usage)\n      * [Spot Interruption](#spot-interruption)\n      * [Scheduled Events](#events)\n      * [Instance Metadata Service Versions](#instance-metadata-service-versions)\n      * [Static Metadata](#static-metadata)\n   * [Troubleshooting](#troubleshooting)\n      * [Warnings and Expected Outcome](#warnings-and-expected-outcome)\n   * [Integrations](#integrations)\n   * [Building](#building)\n   * [Communication](#communication)\n   * [Contributing](#contributing)\n   * [License](#license)\n\n# Summary\nAWS EC2 Instance metadata is data about your instance that you can use to configure or manage the running instance. Instance metadata is divided into categories like hostname, instance id, maintenance events, spot instance action. See the complete list of metadata categories [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-categories.html).\n\nThe instance metadata can be accessed from within the instance. Some instance metadata is available only when an instance is affected by the event. E.g. A Spot instance's metadata item `spot/instance-action` is available only when AWS decides to interrupt the Spot instance.\nThese bring forth some challenges like not being able to test one's application in the event of Spot interruption or other such events and requiring an EC2 instance for testing.\nThis project attempts to bridge these gaps by providing mocks for **most** of these metadata categories. The mock responses are designed to replicate those from the actual instance metadata service for accurate, local testing.\n\n# Major Features\n- Simulate Spot Instance Interruption (ITN) & EC2 Rebalance Recommendation events for Spot instances\n- Delay mock response from the mock serve start time\n- Configure metadata in mock responses via CLI flags, config file, env variables\n- IMDSv1 and v2 support (configurable for IMDSv2 support only)\n- Save processed configuration to a local file\n\n# Supported Metadata Categories\nAEMM supports most [metadata categories](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-categories.html) **except for:**\n* ancestor-ami-ids\n* events/maintenance/history\n\nPRs for any of the above paths are always welcome! Please see our [Contributing](#contributing) section for details.\n\n## Windows Metadata\nPaths specific to Windows instances such as `elastic-gpus/associations/elastic-gpu-id` will **not** be supported at this time. Feel free to open\nan issue to discuss if you would like AEMM to support these paths in the future.\n\n# Getting Started\nAEMM is simple to get up and running.\n\n## Installation\n\n### Install w/ Homebrew\n\n```\nbrew tap aws/tap\nbrew install ec2-metadata-mock\n```\n### Install w/ Curl\n\n#### MacOS/Linux\n```\ncurl -Lo ec2-metadata-mock https://github.com/aws/amazon-ec2-metadata-mock/releases/download/v1.11.2/ec2-metadata-mock-`uname | tr '[:upper:]' '[:lower:]'`-amd64\nchmod +x ec2-metadata-mock\n```\n\n#### ARM Linux\n```\ncurl -Lo ec2-metadata-mock https://github.com/aws/amazon-ec2-metadata-mock/releases/download/v1.11.2/ec2-metadata-mock-linux-arm\n```\n\n```\ncurl -Lo ec2-metadata-mock https://github.com/aws/amazon-ec2-metadata-mock/releases/download/v1.11.2/ec2-metadata-mock-linux-arm64\n```\n\n#### Windows\n```\ncurl -Lo ec2-metadata-mock https://github.com/aws/amazon-ec2-metadata-mock/releases/download/v1.11.2/ec2-metadata-mock-windows-amd64.exe\n```\n\n### Install w/ Docker\n```\ndocker pull public.ecr.aws/aws-ec2/amazon-ec2-metadata-mock:v1.11.2\ndocker run -it --rm -p 1338:1338 public.ecr.aws/aws-ec2/amazon-ec2-metadata-mock:v1.11.2\n```\n\n### On Kubernetes\n#### Supported versions\n* Kubernetes >= 1.14\n\n#### Helm\nWe are hosting helm-charts for Amazon EC2 Metadata Mock in ecr-public. The chart for this project is hosted in [helm/amazon-ec2-metadata-mock](https://gallery.ecr.aws/aws-ec2/helm/amazon-ec2-metadata-mock).\n \nDetailed instructions on installing Amazon EC2 Metadata Mock using Helm can be found here [Helm README](https://github.com/aws/amazon-ec2-metadata-mock/blob/main/helm/amazon-ec2-metadata-mock/README.md)\n\n#### kubectl\nkubectl apply -f https://github.com/aws/amazon-ec2-metadata-mock/releases/download/v1.11.2/all-resources.yaml\n\n## Starting AEMM\nUse `ec2-metadata-mock --help` to view examples and explanations of supported flags and commands:\n\n```\n$ ec2-metadata-mock --help\n\nec2-metadata-mock is a tool to mock Amazon EC2 instance metadata.\n\nUsage:\n  ec2-metadata-mock <command> [arguments] [flags]\n  ec2-metadata-mock [command]\n\nExamples:\n  ec2-metadata-mock --mock-delay-sec 10\tmocks all metadata paths\n  ec2-metadata-mock spot --action terminate\tmocks spot ITN only\n\nAvailable Commands:\n  events      Mock EC2 maintenance events\n  help        Help about any command\n  spot        Mock EC2 Spot interruption notice\n\nFlags:\n  -c, --config-file string              config file for cli input parameters in json format (default: $HOME/aemm-config.json)\n  -h, --help                            help for ec2-metadata-mock\n  -n, --hostname string                 the HTTP hostname for the mock url (default: 0.0.0.0)\n  -I, --imdsv2                          whether to enable IMDSv2 only, requiring a session token when submitting requests (default: false, meaning both IMDS v1 and v2 are enabled)\n  -d, --mock-delay-sec int              spot itn delay in seconds, relative to the application start time (default: 0 seconds)\n  -x, --mock-ip-count int               number of IPs in a cluster that can receive a Spot Interrupt Notice and/or Scheduled Event (default 2)\n      --mock-trigger-time string        spot itn trigger time in RFC3339 format. This takes priority over mock-delay-sec (default: none)\n  -p, --port string                     the HTTP port where the mock runs (default: 1338)\n      --rebalance-delay-sec int         rebalance rec delay in seconds, relative to the application start time (default: 0 seconds)\n      --rebalance-trigger-time string   rebalance rec trigger time in RFC3339 format. This takes priority over rebalance-delay-sec (default: none)\n  -s, --save-config-to-file             whether to save processed config from all input sources in .ec2-metadata-mock/.aemm-config-used.json in $HOME or working dir, if homedir is not found (default: false)\n      --version                         version for ec2-metadata-mock\n\nUse \"ec2-metadata-mock [command] --help\" for more information about a command.\n```\n\nStarting AEMM with default configurations using `ec2-metadata-mock` will start the server on the default host and port:\n\n```\n$ ec2-metadata-mock\n\nInitiating ec2-metadata-mock for all mocks on port 1338\nServing the following routes: /latest/meta-data/product-codes, /latest/meta-data/iam/info, /latest/meta-data/instance-type, ...(truncated for readability)\n```\n\n## Making a Request\nWith the server running, send a request to one of the supported routes above using `curl localhost:1338/<route>`. Example request to display all supported routes:\n\n```\n$ curl localhost:1338/latest/meta-data\n\nami-id\nami-launch-index\nami-manifest-path\nblock-device-mapping/\nelastic-inference/\nevents/\nhostname\niam/\ninstance-action\ninstance-id\ninstance-life-cycle\ninstance-type\nkernel-id\nlocal-hostname\nlocal-ipv4\nmac\nnetwork/\nplacement/\nproduct-codes\npublic-hostname\npublic-ipv4\npublic-keys/\nramdisk-id\nreservation-id\nsecurity-groups\nservices/\nspot/\ntags/\n```\n\n\n# Configuration\nAEMM's wide-range of configurability ranges from overriding port numbers to enabling IMDSv2-only to updating specific metadata values and paths.\nThese configurations can be loaded from various sources with a deterministic precedence.\n\n## Defaults\nDefaults for AEMM configuration are sourced throughout code. Examples below:\n* **CLI flags**\n  * [server config defaults](https://github.com/aws/amazon-ec2-metadata-mock/blob/master/pkg/config/server.go#L22)\n* **Metadata mock responses**\n  * [aemm-metadata-default-values.json](https://github.com/aws/amazon-ec2-metadata-mock/blob/master/pkg/config/defaults/aemm-metadata-default-values.json)\n* **Commands**\n  * [events](https://github.com/aws/amazon-ec2-metadata-mock/blob/master/pkg/cmd/events/events.go#L72)\n\n## Overrides\nAEMM supports configuration from various sources including: cli flags, env variables, and config files. Details regarding\nconfiguration steps, behavior, and precedence are outlined [here](https://github.com/aws/amazon-ec2-metadata-mock/blob/master/docs/configuration.md).\n\n# Usage\nAEMM is primarily used as a developer tool to help test behavior related to Metadata Service. Popular use cases include: emulating spot instance interrupts after a designated delay, mocking scheduled maintenance events, IMDSv2 migrations,\nand requesting static metadata. This section outlines the common use cases of AEMM; advanced usage and behavior are documented [here](https://github.com/aws/amazon-ec2-metadata-mock/blob/master/docs/usage.md).\n\n## Spot Interruption\nTo view the available flags for the Spot Interruption command use `spot --help`:\n```\n$ ec2-metadata-mock spot --help\nMock EC2 Spot interruption notice\n\nUsage:\n  ec2-metadata-mock spot [--action ACTION] [flags]\n\nAliases:\n  spot\n\nExamples:\n  ec2-metadata-mock spot -h \tspot help\n  ec2-metadata-mock spot -d 5 --action terminate\t\tmocks spot interruption only\n\nFlags:\n  -a, --action string                  action in the spot interruption notice (default: terminate)\n                                       action can be one of the following: terminate,hibernate,stop\n  -h, --help                           help for spot\n  -r, --rebalance-rec-time string      rebalance rec time specifies the approximate time when the rebalance recommendation notification will be emitted in RFC3339 format\n  -t, --time string                    termination time specifies the approximate time when the spot instance will receive the shutdown signal in RFC3339 format to execute instance action E.g. 2020-01-07T01:03:47Z (default: request time + 2 minutes in UTC)\n\nGlobal Flags:\n  -c, --config-file string              config file for cli input parameters in json format (default: $HOME/aemm-config.json)\n  -n, --hostname string                 the HTTP hostname for the mock url (default: 0.0.0.0)\n  -I, --imdsv2                          whether to enable IMDSv2 only, requiring a session token when submitting requests (default: false, meaning both IMDS v1 and v2 are enabled)\n  -d, --mock-delay-sec int              spot itn delay in seconds, relative to the application start time (default: 0 seconds)\n  -x, --mock-ip-count int               number of IPs in a cluster that can receive a Spot Interrupt Notice and/or Scheduled Event (default 2)\n      --mock-trigger-time string        spot itn trigger time in RFC3339 format. This takes priority over mock-delay-sec (default: none)\n  -p, --port string                     the HTTP port where the mock runs (default: 1338)\n      --rebalance-delay-sec int         rebalance rec delay in seconds, relative to the application start time (default: 0 seconds)\n      --rebalance-trigger-time string   rebalance rec trigger time in RFC3339 format. This takes priority over rebalance-delay-sec (default: none)\n  -s, --save-config-to-file             whether to save processed config from all input sources in .ec2-metadata-mock/.aemm-config-used.json in $HOME or working dir, if homedir is not found (default: false)\n```\n\n1.) **Starting AEMM with `spot`**:  `spot` routes available immediately:\n```\n$ ec2-metadata-mock spot\nInitiating ec2-metadata-mock for EC2 Spot interruption notice on port 1338\nServing the following routes: ... (truncated for readability)\n```\nSend the request:\n```\n$ curl localhost:1338/latest/meta-data/spot/instance-action\n{\n\t\"action\": \"terminate\",\n\t\"time\": \"2020-04-24T17:11:44Z\"\n}\n```\n\n\n2.) **Starting AEMM with `spot` after Delay**: Users can apply a *delay* duration in seconds for when the `spot` metadata will become available:\n\n```\n$ ec2-metadata-mock spot -d 10\nInitiating ec2-metadata-mock for EC2 Spot interruption notice on port 1338\n\nFlags:\nmock-delay-sec: 10\n\nServing the following routes: ... (truncated for readability)\n```\n\nSending a request to `spot` paths before the delay has passed will return **404 - Not Found:**\n```\n$ curl localhost:1338/latest/meta-data/spot/instance-action\n\n<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n\t\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n <head>\n  <title>404 - Not Found</title>\n </head>\n <body>\n  <h1>404 - Not Found</h1>\n </body>\n</html>\n\n\n// Server log\nDelaying the response by 10s as requested. The mock response will be available in 2s. Returning `notFoundResponse` for now\n```\n\nOnce the delay is complete, querying `spot` paths return expected results:\n```\n$ curl localhost:1338/latest/meta-data/spot/instance-action\n\n{\n\t\"action\": \"terminate\",\n\t\"time\": \"2020-04-24T17:19:32Z\"\n}\n\n```\n\nAlternatively a trigger time can be configured using `--mock-trigger-time` which can be useful to synchronize spot interruption simulation over multiple instances.\n\n### EC2 Instance Rebalance Recommendation\nThe Rebalance Recommendation notification is also available under the **spot** command as it is sent when EC2 emits this signal when Spot Instances are at an elevated risk of interruption:\n```\n$ ec2-metadata-mock spot\nInitiating ec2-metadata-mock for EC2 Spot interruption notice on port 1338\nServing the following routes: ... (truncated for readability)\n```\nSend the request:\n```\n$ curl localhost:1338/latest/meta-data/events/recommendations/rebalance\n{\n        \"noticeTime\": \"2020-10-16T19:18:24Z\"\n}\n```\n*Note: although Rebalance Recommendation path contains `events` it will **not be available** when starting AEMM with the `events` command*\n\n\n## Events\nSimilar to spot, the `events` command, view the local flags using `events --help`:\n\n```\n$ ec2-metadata-mock events --help\nMock EC2 Scheduled Events\n\nUsage:\n  ec2-metadata-mock events [--code CODE] [--state STATE] [--not-after] [--not-before-deadline] [flags]\n\nAliases:\n  events, se, scheduledevents\n\nExamples:\n  ec2-metadata-mock events -h \tevents help\n  ec2-metadata-mock events -o instance-stop --state active -d\t\tmocks an active and upcoming scheduled event for instance stop with a deadline for the event start time\n\nFlags:\n  -o, --code string                  event code in the scheduled event (default: system-reboot)\n                                     event-code can be one of the following: instance-reboot,system-reboot,system-maintenance,instance-retirement,instance-stop\n  -h, --help                         help for events\n  -a, --not-after string             the latest end time for the scheduled event in RFC3339 format E.g. 2020-01-07T01:03:47Z default: application start time + 7 days in UTC))\n  -b, --not-before string            the earliest start time for the scheduled event in RFC3339 format E.g. 2020-01-07T01:03:47Z (default: application start time in UTC)\n  -l, --not-before-deadline string   the deadline for starting the event in RFC3339 format E.g. 2020-01-07T01:03:47Z (default: application start time + 9 days in UTC)\n  -t, --state string                 state of the scheduled event (default: active)\n                                     state can be one of the following: active,completed,canceled\n\n(Truncated Global Flags for readability)\n```\n\n1.) **Starting AEMM with `events`**: `events` route available immediately and `spot` routes will no longer be available due to the implementation of Commands [detailed here](https://github.com/aws/amazon-ec2-metadata-mock/blob/master/docs/usage.md):\n\n```\n$ ec2-metadata-mock events --code instance-reboot -a 2020-01-07T01:03:47Z  -b 2020-01-01T01:03:47Z -l 2020-01-10T01:03:47Z --state completed\nInitiating ec2-metadata-mock for EC2 Events on port 1338\nServing the following routes: ... (truncated for readability)\n\n```\n\nSend the request:\n```\n$ curl localhost:1338/latest/meta-data/events/maintenance/scheduled\n{\n\t\"Code\": \"instance-reboot\",\n\t\"Description\": \"The instance is scheduled for instance-reboot\",\n\t\"State\": \"completed\",\n\t\"EventId\": \"instance-event-1234567890abcdef0\",\n\t\"NotBefore\": \"1 Jan 2020 01:03:47 GMT\",\n\t\"NotAfter\": \"7 Jan 2020 01:03:47 GMT\",\n\t\"NotBeforeDeadline\": \"10 Jan 2020 01:03:47 GMT\"\n}\n```\n\n## Instance Metadata Service Versions\nAEMM supports [both versions](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html) of Instance Metadata service. By default, AEMM starts with supporting v1 and v2; however, it is possible to enable **IMDSv2 only** via overrides.\n\n1.) **Starting AEMM with IMDSv2 only:** session tokens are required for all requests; v1 requests will return **401 - Unauthorized:**\n\n```\n$ ec2-metadata-mock --imdsv2\n```\nSend a v1 request:\n```\n$ curl localhost:1338/latest/meta-data/mac\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n   <head>\n      <title>401 - Unauthorized</title>\n   </head>\n   <body>\n      <h1>401 - Unauthorized</h1>\n   </body>\n</html>\n```\n\nSend a v2 request:\n```\nTOKEN=`curl -X PUT \"localhost:1338/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"` \\\n&& curl -H \"X-aws-ec2-metadata-token: $TOKEN\" localhost:1338/latest/meta-data/mac\n0e:49:61:0f:c3:11\n```\n\nRequesting a token outside the TTL bounds (between 1-2600 seconds) will return **400 - Bad Request:**\n```\n$ curl -X PUT \"localhost:1338/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 0\"\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n   <head>\n      <title>400 - Bad Request</title>\n   </head>\n   <body>\n      <h1>400 - Bad Request</h1>\n   </body>\n</html>\n```\n\nProviding an expired token is synonymous to no token at all resulting in **401 - Unauthorized**.\n\n## Static Metadata\nStatic metadata is classified as instance-specific metadata that is **always** available regardless of which command is used to start the tool.\n\nExamples of static metadata include *all* [metadata categories](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-categories.html) from the non-dynamic category table (ami-id, instance-id, mac) **except for events and spot categories (classified as commands in AEMM).**\n\n*Note that 'static' naming is used within the context of this tool ONLY*\n\n1.) **Requesting static metadata `instance-id`**:\n```\n$ ec2-metadata-mock\n```\n\nSend the request:\n```\n$ curl localhost:1338/latest/meta-data/instance-id\ni-1234567890abcdef0\n```\n\nDetails on overriding static metadata values and behavior can be found [here](https://github.com/aws/amazon-ec2-metadata-mock/blob/master/docs/usage.md#static-metadata)\n\n# Troubleshooting\n\n## Warnings and Expected Outcome\n|Warning Displayed|Cause|Effect|\n|---|---|---|\n|Warning: Config File _file name_ Not Found in _locations_ |input configuration file not found|other input sources are used (See above for details)|\n|Warning: Failed to save the final configuration to local file - Failed to create directory for final configuration at _path/to/dir_: _error string_|Failure to create the hidden directory `.amazon-ec2-metadata-mock` to store the final configuration file| configuration used by the tool is NOT saved to a local file. The tool continues with its primary job of mocking metadata paths |\n|Warning: Failed to save the final configuration to local file - The destination '_path/to/dir_' for saving the configuration already exists, but is not a directory |Failure to create the hidden directory `.amazon-ec2-metadata-mock` to store the final configuration file, because a resource by that name already exists| configuration used by the tool is NOT saved to a local file. The tool continues with its primary job of mocking metadata paths |\n|Warning: Failed to save the final configuration to local file _path/to/local/file_: _error string_  |Failure to save final configuration to a file |configuration used by the tool is NOT saved to a local file. The tool continues with its primary job of mocking metadata paths |\n|Warning: Failed to find home directory due to error: _error string_|Failure to get home directory| working directory is used instead|\n\n# Integrations\n[aws-node-termination-handler](https://github.com/aws/aws-node-termination-handler) uses AEMM in its [e2e test suite](https://github.com/aws/aws-node-termination-handler/tree/master/test/e2e)\nto mock the metadata service and interrupt events.\n\nIn the [NTH imds-v2-test](https://github.com/aws/aws-node-termination-handler/blob/master/test/e2e/imds-v2-test), Helm is used\nto download the latest AEMM release, install it onto the cluster, and start it with imdsv2-only access. NTH then acquires the v2 token from AEMM, consumes the interrupt event, then\ncordons the worker node and evicts the test pod, thus validating NTH functionality. For more details on NTH e2e tests refer to the documentation [here](https://github.com/aws/aws-node-termination-handler/blob/master/test/README.md).\n\n# Building\nFor build instructions, please consult [BUILD.md](https://github.com/aws/amazon-ec2-metadata-mock/blob/master/BUILD.md)\n\n# Communication\nIf you've run into a bug or have a new feature request, please open an [issue](https://github.com/aws/amazon-ec2-metadata-mock/issues/new).\n\nCheck out the open source [Amazon EC2 Spot Instances Integrations Roadmap](https://github.com/aws/ec2-spot-instances-integrations-roadmap) to see what we're working on and give us feedback!\n\n# Contributing\nContributions are welcome! Please read our [guidelines](https://github.com/aws/amazon-ec2-metadata-mock/blob/master/CONTRIBUTING.md) and our [Code of Conduct](https://github.com/aws/amazon-ec2-metadata-mock/blob/master/CODE_OF_CONDUCT.md)\n\n# License\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2022-08-03T16:33:36Z", "2022-07-05T16:11:08Z", "2022-06-16T15:40:53Z", "2022-02-07T20:25:28Z", "2022-01-31T20:24:04Z", "2021-08-26T23:29:56Z", "2021-08-20T15:54:09Z", "2021-05-13T16:13:41Z", "2021-01-07T17:33:54Z", "2020-11-13T17:24:08Z", "2020-11-04T21:31:56Z", "2020-09-08T19:41:08Z", "2020-08-17T20:11:40Z", "2020-08-12T21:05:43Z", "2020-08-11T17:07:17Z", "2020-08-10T18:32:16Z", "2020-07-23T20:35:39Z", "2020-07-20T19:11:55Z", "2020-07-16T22:22:20Z", "2020-07-10T15:47:04Z", "2020-07-01T22:40:07Z", "2020-06-26T16:54:30Z", "2020-06-17T18:31:52Z", "2020-06-17T17:23:28Z", "2020-06-16T22:18:43Z", "2020-06-09T20:37:09Z", "2020-05-22T14:34:08Z", "2020-05-21T18:28:03Z", "2020-05-21T17:20:40Z", "2020-05-08T16:56:01Z"]}, {"name": "amazon-ec2-spot-interrupter", "description": "The ec2-spot-interrupter is a simple CLI tool that triggers Amazon EC2 Spot Interruption Notifications and Rebalance Recommendations.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon EC2 Spot Interrupter\n\nThe `ec2-spot-interrupter` is a simple CLI tool that triggers Amazon EC2 Spot Interruption Notifications and Rebalance Recommendations.\n\n[![Action Status](https://github.com/aws/amazon-ec2-spot-interrupter/actions/workflows/release.yaml/badge.svg)](https://github.com/aws/amazon-ec2-spot-interrupter/actions/workflows/release.yaml)\n\n## Installation\n\n```bash\nbrew tap aws/tap\nbrew install ec2-spot-interrupter\n```\n\n## About\n\n[Amazon EC2 Spot](https://aws.amazon.com/ec2/spot/) Instances let you run flexible, fault-tolerant, or stateless applications in the AWS Cloud at up to a 90% discount from On-Demand prices. \nSpot instances are regular EC2 capacity that can be reclaimed by AWS with a 2-minute notification called the [Interruption Notification](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html).\nApplications that are able to gracefully handle this notification and respond by check pointing or draining work can leverage Spot for deeply discounted compute resources! In addition to Interruption Notifications, [Rebalance Recommendation Events](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/rebalance-recommendations.html) are sent to spot instances that are at higher risk of being interrupted. Handling Rebalance Recommendations can potentially give your application even more time to gracefully shutdown than the 2 minutes an Interruption Notification would give you.\n\nIt can be challenging to test your application's handling of Spot Interruption Notifications and Rebalance Recommendations. The [AWS Fault Injection Simulator](https://aws.amazon.com/fis/) (FIS) supports sending real Spot Interruptions and Rebalance Recommendations to your spot instances so that you can test how your application responds. However, since FIS is a general purpose fault injection simulation service, it can be cumbersome to setup the required fault injection experiment templates to execute experiments for Spot. The `ec2-spot-interrupter` CLI tool streamlines this process as it wraps FIS and allows you to simply pass a list of instance IDs which `ec2-spot-interrupter` will use to craft the required experiment templates and then execute those experiments.\n\nFor details on how to use the AWS Fault Injection Simulator directly to trigger Spot Interruption Notifications, checkout this [blog post](https://aws.amazon.com/blogs/compute/implementing-interruption-tolerance-in-amazon-ec2-spot-with-aws-fault-injection-simulator/).\n\nIf you are looking for a tool to test Spot Interruption Notifications and Rebalance Recommendations locally on your laptop (not EC2), then checkout the [EC2 Metadata Mock](https://github.com/aws/amazon-ec2-metadata-mock).\n\n## Usage\n\n```\n$ ec2-spot-interrupter is a simple CLI tool that triggers Amazon EC2 Spot Instance Interruption Notifications and Rebalance Recommendations.\n\nUsage:\n  ec2-spot-interrupter [flags]\n\nFlags:\n  -c, --clean                  clean up the underlying simulations (default true)\n  -d, --delay duration         duration until the interruption notification is sent (default 15s)\n  -h, --help                   help for ec2-spot-interrupter\n  -i, --instance-ids strings   instance IDs to interrupt\n      --interactive            interactive TUI\n  -p, --profile string         the AWS Profile\n  -r, --region string          the AWS Region\n  -v, --version                the version\n```\n\nTry the interactive TUI mode:\n\n```bash\n$ ec2-spot-interrupter --interactive\n```\n\nOr use the regular CLI options:\n\n```\n$ ec2-spot-interrupter --instance-ids i-0208a716009d70b36\n===================================================================\n\ud83d\udcd6 Experiment Summary:\n        ID: EXPBCcSv1NvRNTek58\n  Role ARN: arn:aws:iam::1234567890:role/aws-fis-itn\n    Action: aws:ec2:send-spot-instance-interruptions\n   Targets:\n    - i-0208a716009d70b36\n===================================================================\n2022-05-18T11:39:45: \u2705 Rebalance Recommendation sent\n2022-05-18T11:39:45: \u23f3 Interruption will be sent in 15 seconds\n2022-05-18T11:40:05: \u2705 Spot 2-minute Interruption Notification sent\n2022-05-18T11:42:05: \u2705 Spot Instance Shutdown sent\n```\n\n## Communication\n\nIf you've run into a bug or have a new feature request, please open an [issue](https://github.com/aws/amazon-ec2-spot-interrupter/issues/new).\n\nCheck out the open source [Amazon EC2 Spot Instances Integrations Roadmap](https://github.com/aws/ec2-spot-instances-integrations-roadmap) to see what we're working on and give us feedback! \n\n##  Contributing\n\nContributions are welcome! Please read our [guidelines](https://github.com/aws/amazon-ec2-spot-interrupter/blob/main/CONTRIBUTING.md) and our [Code of Conduct](https://github.com/aws/amazon-ec2-spot-interrupter/blob/main/CODE_OF_CONDUCT.md).\n\n## License\n\nThis project is licensed under the [Apache-2.0](LICENSE) License.\n", "release_dates": ["2023-02-27T15:26:34Z", "2022-09-23T19:41:47Z", "2022-06-15T16:38:01Z", "2022-05-18T12:07:45Z", "2022-05-18T11:48:05Z", "2022-05-18T10:37:56Z", "2022-05-18T10:26:02Z", "2022-05-18T09:57:01Z", "2022-05-18T09:48:44Z", "2022-05-18T09:33:06Z"]}, {"name": "amazon-ecs-agent", "description": "Amazon Elastic Container Service Agent", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon ECS Container Agent\n\n![Amazon ECS logo](doc/ecs.png \"Amazon ECS\")\n\nThe Amazon ECS Container Agent is a component of Amazon Elastic Container Service\n([Amazon ECS](http://aws.amazon.com/ecs/)) and is responsible for managing containers on behalf of Amazon ECS.\n\nThis repository comes with ECS-Init, which is a [systemd](http://www.freedesktop.org/wiki/Software/systemd/) based service to support the Amazon ECS Container Agent and keep it running. It is used for systems that utilize `systemd` as init systems and is packaged as deb or rpm. The source for ECS-Init is available in this repository at `./ecs-init` while the packaging is available at `./packaging`.\n\n## Usage\n\nThe best source of information on running this software is the\n[Amazon ECS documentation](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_agent.html).\n\n### On the Amazon Linux AMI\n\nOn the [Amazon Linux AMI](https://aws.amazon.com/amazon-linux-ami/), we provide an installable RPM which can be used via\n`sudo yum install ecs-init && sudo start ecs`. This is the recommended way to run it in this environment.\n\n### On Other Linux AMIs\n\n[Amazon ECS docs](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-install.html) provides deb and rpm packages and instructions to install ECS Container Agent on non-Amazon Linux instances.\n\nThe Amazon ECS Container Agent may also be run in a Docker container on an EC2 instance with a recent Docker version\ninstalled. Docker images are available in\n[Docker Hub Repository](https://hub.docker.com/r/amazon/amazon-ecs-agent) and [ECR Public Gallery](https://gallery.ecr.aws/ecs/amazon-ecs-agent).\n\n```bash\n$ # Set up directories the agent uses\n$ mkdir -p /var/log/ecs /etc/ecs /var/lib/ecs/data\n$ touch /etc/ecs/ecs.config\n$ # Set up necessary rules to enable IAM roles for tasks\n$ sysctl -w net.ipv4.conf.all.route_localnet=1\n$ iptables -t nat -A PREROUTING -p tcp -d 169.254.170.2 --dport 80 -j DNAT --to-destination 127.0.0.1:51679\n$ iptables -t nat -A OUTPUT -d 169.254.170.2 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 51679\n$ # Run the agent\n$ docker run --name ecs-agent \\\n    --detach=true \\\n    --restart=on-failure:10 \\\n    --volume=/var/run/docker.sock:/var/run/docker.sock \\\n    --volume=/var/log/ecs:/log \\\n    --volume=/var/lib/ecs/data:/data \\\n    --net=host \\\n    --env-file=/etc/ecs/ecs.config \\\n    --env=ECS_LOGFILE=/log/ecs-agent.log \\\n    --env=ECS_DATADIR=/data/ \\\n    --env=ECS_ENABLE_TASK_IAM_ROLE=true \\\n    --env=ECS_ENABLE_TASK_IAM_ROLE_NETWORK_HOST=true \\\n    amazon/amazon-ecs-agent:latest\n```\n\n### On Other Linux AMIs when awsvpc networking mode is enabled\n\nFor the AWS VPC networking mode, ECS agent requires CNI plugin and dhclient to be available. ECS also needs the ecs-init to run as part of its startup.\nThe following is an example of docker run configuration for running ecs-agent with Task ENI enabled. Note that ECS agent currently only supports cgroupfs for cgroup driver.\n```\n$ # Run the agent\n$ /usr/bin/docker run --name ecs-agent \\\n--init \\\n--restart=on-failure:10 \\\n--volume=/var/run:/var/run \\\n--volume=/var/log/ecs/:/log:Z \\\n--volume=/var/lib/ecs/data:/data:Z \\\n--volume=/etc/ecs:/etc/ecs \\\n--volume=/sbin:/host/sbin \\\n--volume=/lib:/lib \\\n--volume=/lib64:/lib64 \\\n--volume=/usr/lib:/usr/lib \\\n--volume=/usr/lib64:/usr/lib64 \\\n--volume=/proc:/host/proc \\\n--volume=/sys/fs/cgroup:/sys/fs/cgroup \\\n--net=host \\\n--env-file=/etc/ecs/ecs.config \\\n--cap-add=sys_admin \\\n--cap-add=net_admin \\\n--env ECS_ENABLE_TASK_ENI=true \\\n--env ECS_UPDATES_ENABLED=true \\\n--env ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION=1h \\\n--env ECS_DATADIR=/data \\\n--env ECS_ENABLE_TASK_IAM_ROLE=true \\\n--env ECS_ENABLE_TASK_IAM_ROLE_NETWORK_HOST=true \\\n--env ECS_LOGFILE=/log/ecs-agent.log \\\n--env ECS_AVAILABLE_LOGGING_DRIVERS='[\"json-file\",\"awslogs\",\"syslog\",\"none\"]' \\\n--env ECS_LOGLEVEL=info \\\n--detach \\\namazon/amazon-ecs-agent:latest\n```\n\nSee also the Advanced Usage section below.\n\n### On the ECS Optimized Windows AMI\n\nECS Optimized Windows AMI ships with a pre-installed PowerShell module called ECSTools to install, configure, and run the ECS Agent as a Windows service.\nTo install the service, you can run the following PowerShell commands on an EC2 instance. To launch into another cluster instead of windows, replace the 'windows' in the script below with the name of your cluster.\n\n```powershell\nPS C:\\> Import-Module ECSTools\nPS C:\\> # The -EnableTaskIAMRole option is required to enable IAM roles for tasks.\nPS C:\\> Initialize-ECSAgent -Cluster 'windows' -EnableTaskIAMRole\n```\n\n#### Downloading Different Version of ECS Agent\n\nTo download different version of ECS Agent, you can do the following:\n\n```powershell\nPS C:\\> # use agentVersion = \"latest\" for the latest available agent version\nPS C:\\> $agentVersion = \"v1.20.4\"\nPS C:\\> Initialize-ECSAgent -Cluster 'windows' -EnableTaskIAMRole -Version $agentVersion\n```\n\n## Build ECS Agent from source\n\n### Build ECS Agent Image (Linux)\n\nECS Agent can also be built locally from source on a linux machine. Use the following steps to build ECS Agent\n* Get ECS Agent source\n```\ngit clone https://github.com/aws/amazon-ecs-agent.git\n```\n* Build Agent image using ```release-agent``` make target\n```\nmake release-agent\n```\nThis installs the required build dependencies, builds ECS Agent image and saves it at a path ```ecs-agent-v${AGENT_VERSION}.tar```. Load this using\n```\ndocker load < ecs-agent-v${AGENT_VERSION}.tar\n```\nFollow the instructions [above](https://github.com/aws/amazon-ecs-agent#on-other-linux-amis) to continue with the installation\n\n### Build and run standalone (Linux)\n\nThe Amazon ECS Container Agent may also be run outside of a Docker container as a Go binary. At this time, this is not recommended\nfor production on Linux, but it can be useful for development or easier integration with your local Go tools.\n\nThe following commands run the agent outside of Docker:\n\n```\nmake gobuild\n./out/amazon-ecs-agent\n```\n\n### Standalone (Windows)\n\nThe Amazon ECS Container Agent may be built by invoking `scripts\\build_agent.ps1`\n\n### Scripts (Windows)\n\nThe following scripts are available to help develop the Amazon ECS Container Agent on Windows:\n\n* `scripts\\run-integ-tests.ps1` - Runs all integration tests in the `engine` and `stats` packages\n* `misc\\windows-deploy\\Install-ECSAgent.ps1` - Install the ECS agent as a Windows service\n* `misc\\windows-deploy\\amazon-ecs-agent.ps1` - Helper script to set up the host and run the agent as a process\n* `misc\\windows-deploy\\user-data.ps1` - Sample user-data that can be used with the Windows Server 2016 with Containers\n  AMI to run the agent as a process\n\n\n### Build ECS-Init Package (Linux)\n\nECS-Init package can also be built as a deb or rpm depending on the linux system you are running. Follow instructions at [generic-deb-integrated](https://github.com/aws/amazon-ecs-agent/tree/master/packaging/generic-deb-integrated/debian) or [generic-rpm-integrated](https://github.com/aws/amazon-ecs-agent/tree/master/packaging/generic-rpm-integrated) to build and install ECS Agent with Init using deb or rpm.\n\n## Advanced Usage\n\nThe Amazon ECS Container Agent supports a number of configuration options, most of which should be set through\nenvironment variables.\n\n### Environment Variables\n\nThe table below provides an overview of optional environment variables that can be used to configure the ECS agent. See\n[the Amazon ECS developer guide](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html) for\nadditional details on each available environment variable.\n\n| Environment Key | Example Value(s)            | Description | Default value on Linux | Default value on Windows |\n|:----------------|:----------------------------|:------------|:-----------------------|:-------------------------|\n| `ECS_CLUSTER`       | clusterName             | The cluster this agent should check into. | default | default |\n| `ECS_RESERVED_PORTS` | `[22, 80, 5000, 8080]` | An array of ports that should be marked as unavailable for scheduling on this container instance. | `[22, 2375, 2376, 51678, 51679]` | `[53, 135, 139, 445, 2375, 2376, 3389, 5985, 5986, 51678, 51679]`\n| `ECS_RESERVED_PORTS_UDP` | `[53, 123]` | An array of UDP ports that should be marked as unavailable for scheduling on this container instance. | `[]` | `[]` |\n| `ECS_ENGINE_AUTH_TYPE`     |  \"docker\" &#124; \"dockercfg\" | The type of auth data that is stored in the `ECS_ENGINE_AUTH_DATA` key. | | |\n| `ECS_ENGINE_AUTH_DATA`     | See the [dockerauth documentation](https://godoc.org/github.com/aws/amazon-ecs-agent/agent/dockerclient/dockerauth) | Docker [auth data](https://godoc.org/github.com/aws/amazon-ecs-agent/agent/dockerclient/dockerauth) formatted as defined by `ECS_ENGINE_AUTH_TYPE`. | | |\n| `AWS_DEFAULT_REGION` | &lt;us-west-2&gt;&#124;&lt;us-east-1&gt;&#124;&hellip; | The region to be used in API requests as well as to infer the correct backend host. | Taken from Amazon EC2 instance metadata. | Taken from Amazon EC2 instance metadata. |\n| `AWS_ACCESS_KEY_ID` | AKIDEXAMPLE             | The [access key](http://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html) used by the agent for all calls. | Taken from Amazon EC2 instance metadata. | Taken from Amazon EC2 instance metadata. |\n| `AWS_SECRET_ACCESS_KEY` | EXAMPLEKEY | The [secret key](http://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html) used by the agent for all calls. | Taken from Amazon EC2 instance metadata. | Taken from Amazon EC2 instance metadata. |\n| `AWS_SESSION_TOKEN` | | The [session token](http://docs.aws.amazon.com/STS/latest/UsingSTS/Welcome.html) used for temporary credentials. | Taken from Amazon EC2 instance metadata. | Taken from Amazon EC2 instance metadata. |\n| `DOCKER_HOST`   | `unix:///var/run/docker.sock` | Used to create a connection to the Docker daemon; behaves similarly to this environment variable as used by the Docker client. | `unix:///var/run/docker.sock` | `npipe:////./pipe/docker_engine` |\n| `ECS_LOGLEVEL`  | &lt;crit&gt; &#124; &lt;error&gt; &#124; &lt;warn&gt; &#124; &lt;info&gt; &#124; &lt;debug&gt; | The level of detail to be logged. | info | info |\n| `ECS_LOGLEVEL_ON_INSTANCE`  | &lt;none&gt; &#124; &lt;crit&gt; &#124; &lt;error&gt; &#124; &lt;warn&gt; &#124; &lt;info&gt; &#124; &lt;debug&gt; | Can be used to override `ECS_LOGLEVEL` and set a level of detail that should be logged in the on-instance log file, separate from the level that is logged in the logging driver. If a logging driver is explicitly set, on-instance logs are turned off by default, but can be turned back on with this variable. | none if `ECS_LOG_DRIVER` is explicitly set to a non-empty value; otherwise the same value as `ECS_LOGLEVEL` | none if `ECS_LOG_DRIVER` is explicitly set to a non-empty value; otherwise the same value as `ECS_LOGLEVEL` |\n| `ECS_LOGFILE`   | /ecs-agent.log              | The location where logs should be written. Log level is controlled by `ECS_LOGLEVEL`. | blank | blank |\n| `ECS_CHECKPOINT`   | &lt;true &#124; false&gt; | Whether to checkpoint state to the DATADIR specified below. | true if `ECS_DATADIR` is explicitly set to a non-empty value; false otherwise | true if `ECS_DATADIR` is explicitly set to a non-empty value; false otherwise |\n| `ECS_DATADIR`      |   /data/                  | The container path where state is checkpointed for use across agent restarts. Note that on Linux, when you specify this, you will need to make sure that the Agent container has a bind mount of `$ECS_HOST_DATA_DIR/data:$ECS_DATADIR` with the corresponding values of `ECS_HOST_DATA_DIR` and `ECS_DATADIR`. | /data/ | `C:\\ProgramData\\Amazon\\ECS\\data`\n| `ECS_UPDATES_ENABLED` | &lt;true &#124; false&gt; | Whether to exit for an updater to apply updates when requested. | false | false |\n| `ECS_DISABLE_METRICS`     | &lt;true &#124; false&gt;  | Whether to disable metrics gathering for tasks. | false | false |\n| `ECS_POLL_METRICS`     | &lt;true &#124; false&gt;  | Whether to poll or stream when gathering metrics for tasks. Setting this value to `true` can help reduce the CPU usage of dockerd and containerd on the ECS container instance. See also ECS_POLL_METRICS_WAIT_DURATION for setting the poll interval. | `false` | `false` |\n| `ECS_POLLING_METRICS_WAIT_DURATION` | 10s | Time to wait between polling for metrics for a task. Not used when ECS_POLL_METRICS is false. Maximum value is 20s and minimum value is 5s. If user sets above maximum it will be set to max, and if below minimum it will be set to min. As the number of tasks/containers increase, a higher `ECS_POLLING_METRICS_WAIT_DURATION` value can potentially cause a problem where memory reservation value of ECS cluster reported in metrics becomes unstable due to missing metrics sample at metric collection time. It is recommended to keep this value smaller than 18s. This behavior is only observed on certain OS and platforms. | 10s | 10s |\n| `ECS_PULL_DEPENDENT_CONTAINERS_UPFRONT` | &lt;true &#124; false&gt; | Whether to pull images for containers with dependencies before the dependsOn condition has been satisfied. | false | false |\n| `ECS_RESERVED_MEMORY` | 32 | Reduction, in MiB, of the memory capacity of the instance that is reported to Amazon ECS. Used by Amazon ECS when placing tasks on container instances. This doesn't reserve memory usage on the instance. | 0 | 0 |\n| `ECS_AVAILABLE_LOGGING_DRIVERS` | `[\"awslogs\",\"fluentd\",\"gelf\",\"json-file\",\"journald\",\"logentries\",\"splunk\",\"syslog\"]` | Which logging drivers are available on the container instance. | `[\"json-file\",\"none\"]` | `[\"json-file\",\"none\"]` |\n| `ECS_DISABLE_PRIVILEGED` | `true` | Whether launching privileged containers is disabled on the container instance. | `false` | `false` |\n| `ECS_SELINUX_CAPABLE` | `true` | Whether SELinux is available on the container instance. (Limited support; Z-mode mounts only.) | `false` | `false` |\n| `ECS_APPARMOR_CAPABLE` | `true` | Whether AppArmor is available on the container instance. | `false` | `false` |\n| `ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION` | 10m | Default time to wait to delete containers for a stopped task (see also `ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION_JITTER`). If set to less than 1 second, the value is ignored.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 3h | 3h |\n| `ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION_JITTER` | 1h | Jitter value for the task engine cleanup wait duration. When specified, the actual cleanup wait duration time for each task will be the duration specified in `ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION` plus a random duration between 0 and the jitter duration. | blank | blank |\n| `ECS_CONTAINER_STOP_TIMEOUT` | 10m | Instance scoped configuration for time to wait for the container to exit normally before being forcibly killed. | 30s | 30s |\n| `ECS_CONTAINER_START_TIMEOUT` | 10m | Timeout before giving up on starting a container. | 3m | 8m |\n| `ECS_CONTAINER_CREATE_TIMEOUT` | 10m | Timeout before giving up on creating a container. Minimum value is 1m. If user sets a value below minimum it will be set to min. | 4m | 4m |\n| `ECS_ENABLE_TASK_IAM_ROLE` | `true` | Whether to enable IAM Roles for Tasks on the Container Instance | `false` | `false` |\n| `ECS_ENABLE_TASK_IAM_ROLE_NETWORK_HOST` | `true` | Whether to enable IAM Roles for Tasks when launched with `host` network mode on the Container Instance | `false` | `false` |\n| `ECS_DISABLE_IMAGE_CLEANUP` | `true` | Whether to disable automated image cleanup for the ECS Agent. | `false` | `false` |\n| `ECS_IMAGE_CLEANUP_INTERVAL` | 30m | The time interval between automated image cleanup cycles. If set to less than 10 minutes, the value is ignored. | 30m | 30m |\n| `ECS_IMAGE_MINIMUM_CLEANUP_AGE` | 30m | The minimum time interval between when an image is pulled and when it can be considered for automated image cleanup. | 1h | 1h |\n| `NON_ECS_IMAGE_MINIMUM_CLEANUP_AGE` | 30m | The minimum time interval between when a non ECS image is created and when it can be considered for automated image cleanup. | 1h | 1h |\n| `ECS_NUM_IMAGES_DELETE_PER_CYCLE` | 5 | The maximum number of images to delete in a single automated image cleanup cycle. If set to less than 1, the value is ignored. | 5 | 5 |\n| `ECS_IMAGE_PULL_BEHAVIOR` | &lt;default &#124; always &#124; once &#124; prefer-cached &gt; | The behavior used to customize the pull image process. If `default` is specified, the image will be pulled remotely, if the pull fails then the cached image in the instance will be used. If `always` is specified, the image will be pulled remotely, if the pull fails then the task will fail. If `once` is specified, the image will be pulled remotely if it has not been pulled before or if the image was removed by image cleanup, otherwise the cached image in the instance will be used. If `prefer-cached` is specified, the image will be pulled remotely if there is no cached image, otherwise the cached image in the instance will be used. | default | default |\n| `ECS_IMAGE_PULL_INACTIVITY_TIMEOUT` | 1m | The time to wait after docker pulls complete waiting for extraction of a container. Useful for tuning large Windows containers. | 1m | 3m |\n| `ECS_IMAGE_PULL_TIMEOUT` | 1h | The time to wait for pulling docker image. | 2h | 2h |\n| `ECS_INSTANCE_ATTRIBUTES` | `{\"stack\": \"prod\"}` | These attributes take effect only during initial registration. After the agent has joined an ECS cluster, use the PutAttributes API action to add additional attributes. For more information, see [Amazon ECS Container Agent Configuration](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html) in the Amazon ECS Developer Guide.| `{}` | `{}` |\n| `ECS_ENABLE_TASK_ENI` | `false` | Whether to enable task networking for task to be launched with its own network interface | `false` | Not applicable |\n| `ECS_ENABLE_HIGH_DENSITY_ENI` | `false` | Whether to enable high density eni feature when using task networking | `true` | Not applicable |\n| `ECS_CNI_PLUGINS_PATH` | `/ecs/cni` | The path where the cni binary file is located | `/amazon-ecs-cni-plugins` | Not applicable |\n| `ECS_AWSVPC_BLOCK_IMDS` | `true` | Whether to block access to [Instance Metadata](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html) for Tasks started with `awsvpc` network mode | `false` | Not applicable |\n| `ECS_AWSVPC_ADDITIONAL_LOCAL_ROUTES` | `[\"10.0.15.0/24\"]` | In `awsvpc` network mode, traffic to these prefixes will be routed via the host bridge instead of the task ENI | `[]` | Not applicable |\n| `ECS_ENABLE_CONTAINER_METADATA` | `true` | When `true`, the agent will create a file describing the container's metadata and the file can be located and consumed by using the container enviornment variable `$ECS_CONTAINER_METADATA_FILE` | `false` | `false` |\n| `ECS_HOST_DATA_DIR` | `/var/lib/ecs` | The source directory on the host from which ECS_DATADIR is mounted. We use this to determine the source mount path for container metadata files in the case the ECS Agent is running as a container. We do not use this value in Windows because the ECS Agent is not running as container in Windows. On Linux, note that when you specify this, you will need to make sure that the Agent container has a bind mount of `$ECS_HOST_DATA_DIR/data:$ECS_DATADIR` with the corresponding values of `ECS_HOST_DATA_DIR` and `ECS_DATADIR`. | `/var/lib/ecs` | `Not used` |\n| `ECS_ENABLE_TASK_CPU_MEM_LIMIT` | `true` | Whether to enable task-level cpu and memory limits | `true` | `false` |\n| `ECS_CGROUP_PATH` | `/sys/fs/cgroup` | The root cgroup path that is expected by the ECS agent. This is the path that accessible from the agent mount. | `/sys/fs/cgroup` | Not applicable |\n| `ECS_CGROUP_CPU_PERIOD` | `10ms` | CGroups CPU period for task level limits. This value should be between 8ms to 100ms | `100ms` | Not applicable |\n| `ECS_AGENT_HEALTHCHECK_HOST` | `localhost` | Override for the ecs-agent container's healthcheck localhost ip address| `localhost` | `localhost` |\n| `ECS_ENABLE_CPU_UNBOUNDED_WINDOWS_WORKAROUND` | `true` | When `true`, ECS will allow CPU unbounded(CPU=`0`) tasks to run along with CPU bounded tasks in Windows. | Not applicable | `false` |\n| `ECS_ENABLE_MEMORY_UNBOUNDED_WINDOWS_WORKAROUND` | `true` | When `true`, ECS will ignore the memory reservation parameter (soft limit) to run along with memory bounded tasks in Windows. To run a memory unbounded task, omit the memory hard limit and set any memory reservation, it will be ignored. | Not applicable | `false` |\n| `ECS_TASK_METADATA_RPS_LIMIT` | `100,150` | Comma separated integer values for steady state and burst throttle limits for combined total traffic to task metadata endpoint and agent api endpoint. | `40,60` | `40,60` |\n| `ECS_SHARED_VOLUME_MATCH_FULL_CONFIG` | `true` | When `true`, ECS Agent will compare name, driver options, and labels to make sure volumes are identical. When `false`, Agent will short circuit shared volume comparison if the names match. This is the default Docker behavior. If a volume is shared across instances, this should be set to `false`. | `false` | `false`|\n| `ECS_CONTAINER_INSTANCE_PROPAGATE_TAGS_FROM` | `ec2_instance` | If `ec2_instance` is specified, existing tags defined on the container instance will be registered to Amazon ECS and will be discoverable using the `ListTagsForResource` API. Using this requires that the IAM role associated with the container instance have the `ec2:DescribeTags` action allowed. | `none` | `none` |\n| `ECS_CONTAINER_INSTANCE_TAGS` | `{\"tag_key\": \"tag_val\"}` | The metadata that you apply to the container instance to help you categorize and organize them. Each tag consists of a key and an optional value, both of which you define. Tag keys can have a maximum character length of 128 characters, and tag values can have a maximum length of 256 characters. If tags also exist on your container instance that are propagated using the `ECS_CONTAINER_INSTANCE_PROPAGATE_TAGS_FROM` parameter, those tags will be overwritten by the tags specified using `ECS_CONTAINER_INSTANCE_TAGS`. | `{}` | `{}` |\n| `ECS_ENABLE_UNTRACKED_IMAGE_CLEANUP` | `true` | Whether to allow the ECS agent to delete containers and images that are not part of ECS tasks. | `false` | `false` |\n| `ECS_EXCLUDE_UNTRACKED_IMAGE` | `alpine:latest` | Comma separated list of `imageName:tag` of images that should not be deleted by the ECS agent if `ECS_ENABLE_UNTRACKED_IMAGE_CLEANUP` is enabled. | | |\n| `ECS_DISABLE_DOCKER_HEALTH_CHECK` | `false` | Whether to disable the Docker Container health check for the ECS Agent. | `false` | `false` |\n| `ECS_NVIDIA_RUNTIME` | nvidia | The Nvidia Runtime to be used to pass Nvidia GPU devices to containers. | nvidia | Not Applicable |\n| `ECS_ALTERNATE_CREDENTIAL_PROFILE` | default | An alternate credential role/profile name. | default | default |\n| `ECS_ENABLE_SPOT_INSTANCE_DRAINING` | `true` | Whether to enable Spot Instance draining for the container instance. If true, if the container instance receives a [spot interruption notice](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html), agent will set the instance's status to [DRAINING](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-draining.html), which gracefully shuts down and replaces all tasks running on the instance that are part of a service. It is recommended that this be set to `true` when using spot instances. | `false` | `false` |\n| `ECS_LOG_ROLLOVER_TYPE` | `size` &#124; `hourly` | Determines whether the container agent logfile will be rotated based on size or hourly. By default, the agent logfile is rotated each hour. | `hourly` | `hourly` |\n| `ECS_LOG_OUTPUT_FORMAT` | `logfmt` &#124; `json` | Determines the log output format. When the json format is used, each line in the log would be a structured JSON map. | `logfmt` | `logfmt` |\n| `ECS_LOG_MAX_FILE_SIZE_MB` | `10` | When the ECS_LOG_ROLLOVER_TYPE variable is set to size, this variable determines the maximum size (in MB) the log file before it is rotated. If the rollover type is set to hourly then this variable is ignored. | `10` | `10` |\n| `ECS_LOG_MAX_ROLL_COUNT` | `24` | Determines the number of rotated log files to keep. Older log files are deleted once this limit is reached. | `24` | `24` |\n| `ECS_LOG_DRIVER` | `awslogs` &#124; `fluentd` &#124; `gelf` &#124; `json-file` &#124; `journald` &#124; `logentries` &#124; `syslog` &#124; `splunk` | The logging driver to be used by the Agent container. | `json-file` | Not applicable |\n| `ECS_LOG_OPTS` | `{\"option\":\"value\"}` | The options for configuring the logging driver set in `ECS_LOG_DRIVER`. | `{}` | Not applicable |\n| `ECS_ENABLE_AWSLOGS_EXECUTIONROLE_OVERRIDE` | `true` | Whether to enable awslogs log driver to authenticate via credentials of task execution IAM role. Needs to be true if you want to use awslogs log driver in a task that has task execution IAM role specified. When using the ecs-init RPM with version equal or later than V1.16.0-1, this env is set to true by default. | `false` | `false` |\n| `ECS_FSX_WINDOWS_FILE_SERVER_SUPPORTED` | `true` | Whether FSx for Windows File Server volume type is supported on the container instance. This variable is only supported on agent versions 1.47.0 and later. | `false` | `true` |\n| `ECS_ENABLE_RUNTIME_STATS` | `true` | Determines if [pprof](https://pkg.go.dev/net/http/pprof) is enabled for the agent. If enabled, the different profiles can be accessed through the agent's introspection port (e.g. `curl http://localhost:51678/debug/pprof/heap > heap.pprof`). In addition, agent's [runtime stats](https://pkg.go.dev/runtime#ReadMemStats) are logged to `/var/log/ecs/runtime-stats.log` file. | `false` | `false` |\n| `ECS_EXCLUDE_IPV6_PORTBINDING` | `true` | Determines if agent should exclude IPv6 port binding using default network mode. If enabled, IPv6 port binding will be filtered out, and the response of DescribeTasks API call will not show tasks' IPv6 port bindings, but it is still included in Task metadata endpoint. | `true` | `true` |\n| `ECS_WARM_POOLS_CHECK` | `true` | Whether to ensure instances going into an [EC2 Auto Scaling group warm pool](https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html) are prevented from being registered with the cluster. Set to true only if using EC2 Autoscaling | `false` | `false` |\n| `ECS_SKIP_LOCALHOST_TRAFFIC_FILTER` | `false` | By default, the ecs-init service adds an iptable rule to drop non-local packets to localhost if they're not part of an existing forwarded connection or DNAT, and removes the rule upon stop. If this is set to true, the rule will not be added or removed. | `false` | `false` |\n| `ECS_ALLOW_OFFHOST_INTROSPECTION_ACCESS` | `true` | By default, the ecs-init service adds an iptable rule to block access to the agent introspection port from off-host (or containers in awsvpc network mode), and removes the rule upon stop. If this is set to true, the rule will not be added or removed | `false` | `false` |\n| `ECS_OFFHOST_INTROSPECTION_INTERFACE_NAME` | `eth0` | The primary network interface name to be used for blocking offhost agent introspection port access | `eth0` | `eth0` |\n| `ECS_ENABLE_GPU_SUPPORT` | `true` | Whether you use container instances with GPU support. This parameter is specified for the agent. You must also configure your task definitions for GPU. For more information | `false` | `Not applicable` |\n| `HTTP_PROXY` | `10.0.0.131:3128` | The hostname (or IP address) and port number of an HTTP proxy to use for the Amazon ECS agent to connect to the internet. For example, this proxy will be used if your container instances do not have external network access through an Amazon VPC internet gateway or NAT gateway or instance. If this variable is set, you must also set the NO_PROXY variable to filter Amazon EC2 instance metadata and Docker daemon traffic from the proxy. | `null` | `null` |\n| `NO_PROXY` | <For Linux: 169.254.169.254,169.254.170.2,/var/run/docker.sock &#124; For Windows: 169.254.169.254,169.254.170.2,\\\\.\\pipe\\docker_engine> | The HTTP traffic that should not be forwarded to the specified HTTP_PROXY. You must specify 169.254.169.254,/var/run/docker.sock to filter Amazon EC2 instance metadata and Docker daemon traffic from the proxy. | `null` | `null` |\n| `ECS_GMSA_SUPPORTED` | `true` | Whether you use gMSA authentication to Active Directory in tasks. Each task must specify the location of a credential specification file in the `dockerSecurityOpts` parameter of a container definition. On Linux, this requires the [credentials-fetcher daemon](https://github.com/aws/credentials-fetcher). | `false` | `false` |\n| `CREDENTIALS_FETCHER_HOST`   | `unix:///var/credentials-fetcher/socket/credentials_fetcher.sock` | Used to create a connection to the [credentials-fetcher daemon](https://github.com/aws/credentials-fetcher); to support gMSA on Linux. The default is fine for most users, only needs to be modified if user is configuring a custom credentials-fetcher socket path, ie, [CF_UNIX_DOMAIN_SOCKET_DIR](https://github.com/aws/credentials-fetcher#default-environment-variables). | `unix:///var/credentials-fetcher/socket/credentials_fetcher.sock` | Not Applicable |\n| `CREDENTIALS_FETCHER_SECRET_NAME_FOR_DOMAINLESS_GMSA`   | `secretmanager-secretname` | Used to support scaling option for gMSA on Linux [credentials-fetcher daemon](https://github.com/aws/credentials-fetcher). If user is configuring gMSA on a non-domain joined instance, they need to create an Active Directory user with access to retrieve principals for the gMSA account and store it in secrets manager | `secretmanager-secretname` | Not Applicable |\n| `ECS_DYNAMIC_HOST_PORT_RANGE` | `100-200` | This specifies the dynamic host port range that the agent uses to assign host ports from, for container ports mapping. If there are no available ports in the range for containers, including customer containers and Service Connect Agent containers (if Service Connect is enabled), service deployments would fail. | Defined by `/proc/sys/net/ipv4/ip_local_port_range` | `49152-65535` |\n| `ECS_TASK_PIDS_LIMIT` | `100` | Specifies the per-task pids limit cgroup setting for each task launched on the container instance. This setting maps to the pids.max cgroup setting at the ECS task level. See https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#pid. If unset, pids will be unlimited. Min value is 1 and max value is 4194304 (4*1024*1024) | `unset` | Not Supported on Windows |\n| `ECS_EBSTA_SUPPORTED` | `true` | Whether to use the container instance with EBS Task Attach support. This variable is set properly by ecs-init. Its value indicates if correct environment to support EBS volumes by instance has been set up or not. ECS only schedules EBSTA tasks if this feature is supported by the platform type. Check [EBS Volume considerations](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ebs-volumes.html#ebs-volume-considerations) for other EBS support details | `true` | Not Supported on Windows |\n\nAdditionally, the following environment variable(s) can be used to configure the behavior of the ecs-init service. When using ECS-Init, all env variables, including the ECS Agent variables above, are read from path `/etc/ecs/ecs.config`:\n| Environment Variable Name | Example Value(s)            | Description | Default value |\n|:----------------|:----------------------------|:------------|:-----------------------|\n| `ECS_SKIP_LOCALHOST_TRAFFIC_FILTER` | &lt;true &#124; false&gt; | By default, the ecs-init service adds an iptable rule to drop non-local packets to localhost if they're not part of an existing forwarded connection or DNAT, and removes the rule upon stop. If `ECS_SKIP_LOCALHOST_TRAFFIC_FILTER` is set to true, this rule will not be added/removed. | false |\n| `ECS_ALLOW_OFFHOST_INTROSPECTION_ACCESS` | &lt;true &#124; false&gt; | By default, the ecs-init service adds an iptable rule to block access to ECS Agent's introspection port from off-host (or containers in awsvpc network mode), and removes the rule upon stop. If `ECS_ALLOW_OFFHOST_INTROSPECTION_ACCESS` is set to true, this rule will not be added/removed. | false |\n| `ECS_OFFHOST_INTROSPECTION_INTERFACE_NAME` | `eth0` | Primary network interface name to be used for blocking offhost agent introspection port access. By default, this value is `eth0` | `eth0` |\n| `ECS_AGENT_LABELS` | `{\"test.label.1\":\"value1\",\"test.label.2\":\"value2\"}` | The labels to add to the ECS Agent container. | |\n| `ECS_AGENT_APPARMOR_PROFILE` | `unconfined` | Specifies the name of the AppArmor profile to run the ecs-agent container under. This only applies to AppArmor-enabled systems, such as Ubuntu, Debian, and SUSE. If unset, defaults to the profile written out by ecs-init (ecs-agent-default). | `ecs-agent-default` |\n\n\n### Persistence\n\nWhen you run the Amazon ECS Container Agent in production, its `datadir` should be persisted between runs of the Docker\ncontainer. If this data is not persisted, the agent registers a new container instance ARN on each launch and is not\nable to update the state of tasks it previously ran.\n\n### Flags\n\nThe agent also supports the following flags:\n\n* `-k` &mdash; The agent will not require valid SSL certificates for the services that it communicates with. We\n  recommend against using this flag.\n* ` -loglevel` &mdash; Options: `[<crit>|<error>|<warn>|<info>|<debug>]`. The agent will output on stdout at the given\n  level. This is overridden by the `ECS_LOGLEVEL` environment variable, if present.\n\n\n### Make Targets (on Linux)\n\nThe following targets are available. Each may be run with `make <target>`.\n\n| Make Target            | Description |\n|:-----------------------|:------------|\n| `release-agent`        | *(Default Agent build)* Builds Agent fetching required dependencies and saves image .tar to disk|\n| `generic-rpm-integrated`| Builds init rpm package and saves .rpm package to disk |\n| `generic-deb-integrated`| Builds init deb package and saves .deb package to disk |\n| `release`              | *(Legacy Agent build)* Builds the agent within a Docker container and packages it into a scratch-based image |\n| `gobuild`              | Runs a normal `go build` of the agent and stores the binary in `./out/amazon-ecs-agent` |\n| `static`               | Runs `go build` to produce a static binary in `./out/amazon-ecs-agent` |\n| `test`                 | Runs all unit tests using `go test` |\n| `test-in-docker`       | Runs all tests inside a Docker container |\n| `run-integ-tests`      | Runs all integration tests in the `engine` and `stats` packages |\n| `clean`                | Removes build artifacts. *Note: this does not remove Docker images* |\n\n\n## Contributing\n\nContributions and feedback are welcome! Proposals and pull requests will be considered and responded to. For more\ninformation, see the [CONTRIBUTING.md](https://github.com/aws/amazon-ecs-agent/blob/master/CONTRIBUTING.md) file.\n\nIf you have a bug/and issue around the behavior of the ECS agent, please open it here.\n\nIf you have a feature request, please open it over at the [AWS Containers Roadmap](https://github.com/aws/containers-roadmap).\n\nAmazon Web Services does not currently provide support for modified copies of this software.\n\n## Security disclosures\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n## License\n\nThe Amazon ECS Container Agent is licensed under the Apache 2.0 License.\n", "release_dates": ["2024-02-27T22:00:07Z", "2024-02-03T01:58:10Z", "2024-01-16T21:50:48Z", "2023-12-14T19:06:39Z", "2023-11-17T17:30:08Z", "2023-11-07T18:13:54Z", "2023-10-27T23:25:05Z", "2023-10-25T19:46:23Z", "2023-10-04T18:00:25Z", "2023-09-21T16:29:17Z", "2023-09-12T15:34:47Z", "2023-08-16T17:44:24Z", "2023-07-25T20:27:56Z", "2023-07-13T19:53:15Z", "2023-06-28T16:41:27Z", "2023-06-13T18:28:52Z", "2023-06-01T19:36:09Z", "2023-05-16T18:16:36Z", "2023-04-28T20:54:37Z", "2023-04-18T01:31:49Z", "2023-04-04T21:30:26Z", "2023-03-24T16:36:47Z", "2023-03-03T16:58:20Z", "2023-02-11T03:04:13Z", "2023-01-30T17:32:03Z", "2023-01-09T17:35:18Z", "2022-12-13T20:51:07Z", "2022-12-08T19:07:38Z", "2022-12-08T19:07:27Z", "2022-11-12T22:32:08Z"]}, {"name": "amazon-ecs-ami", "description": "Packer recipes for building the official ECS-optimized Amazon Linux AMIs", "language": "Shell", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ECS-optimized AMI Build Recipes\n\nThis is a [packer](https://packer.io) recipe for creating an ECS-optimized AMI.\nIt will create a private AMI in whatever account you are running it in.\n\n## Instructions\n\n1. Setup AWS cli credentials.\n2. Make the recipe that you want, REGION must be specified. Options are: al1, al2, al2arm, al2gpu, al2keplergpu, al2inf, \nal2kernel5dot10, al2kernel5dot10arm, al2kernel5dot10gpu, al2kernel5dot10inf, al2023, al2023arm, al2023neu.\n```\nREGION=us-west-2 make al2\n```\n\n**NOTE**: `al2keplergpu` is a build recipe that this package supports to build ECS-Optimized GPU AMIs for instances with GPUs\nwith Kepler architecture (such as P2 type instances). ECS-Optimized GPU AMIs for this target are not officially built and published.\n\n## Configuration\n\nThis recipe allows for configuration of your AMI. All configuration variables are defined and documented\nin the file: `./variables.pkr.hcl`. This is also where some defaults are defined.\n\nVariables can be set in `./release.auto.pkrvars.hcl` or `./overrides.auto.pkrvars.hcl`.\n\n#### Overrides\n\nIf you would like to override any of the defaults provided here without committing any changes to git, you\ncan use the `overrides.auto.pkrvars.hcl` file, which is ignored by source control.\n\nFor example, if you want your AMI to have a smaller root block device, you can override the default value\nof 30 GB like this:\n\n```\nexport REGION=us-west-2\necho \"block_device_size_gb = 8\" > ./overrides.auto.pkrvars.hcl\nmake al2\n```\n\n## Additional Packages\n\nAny rpm package placed into the additional-packages/ directory will be uploaded to the instance and installed.\n\n**NOTE**: All packages must end with extension `\"$(uname -m).rpm\"`, ie `.x86_64.rpm` or `.aarch64.rpm`.\n\n## Cleanup\n\n1. Deregister the AMI from EC2 Images via cli or console.\n2. Delete the snapshot from EC2 EBS via cli or console.\n\n## IAM Permissions\n\nFor details on the minimum IAM permissions required to build the AMI, please see the\npacker docs: https://www.packer.io/docs/builders/amazon#iam-task-or-instance-role\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2024-02-29T18:45:57Z", "2024-02-23T01:30:40Z", "2024-02-13T01:08:01Z", "2024-02-06T18:07:07Z", "2024-01-31T23:38:25Z", "2024-01-17T18:02:21Z", "2023-12-20T22:58:45Z", "2023-12-14T18:33:22Z", "2023-12-12T23:12:48Z", "2023-12-05T18:30:40Z", "2023-12-01T19:29:00Z", "2023-11-08T21:47:00Z", "2023-10-26T20:31:02Z", "2023-10-03T16:59:09Z", "2023-09-18T17:52:32Z", "2023-09-12T16:24:53Z", "2023-08-16T18:49:29Z", "2023-08-02T20:33:07Z", "2023-07-25T22:46:57Z", "2023-07-10T20:28:20Z", "2023-06-28T17:01:10Z", "2023-06-13T19:27:56Z", "2023-06-02T20:21:11Z", "2023-05-15T19:58:42Z", "2023-05-08T20:57:39Z"]}, {"name": "amazon-ecs-cli", "description": "The Amazon ECS CLI enables users to run their applications on ECS/Fargate using the Docker Compose file format, quickly provision resources, push/pull images in ECR, and monitor running applications on ECS/Fargate.", "language": "Go", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "__\u2728 The [AWS Copilot CLI](https://github.com/aws/copilot-cli) is now in Generally Available! The CLI makes it easy to build, release and operate your container apps on Amazon ECS and AWS Fargate.__\n\n<details>\n<summary>Learn more about the AWS Copilot CLI</summary>\n\nThe [AWS Copilot CLI](https://github.com/aws/copilot-cli) is a CLI focused on the full developer experience of building, deploying and operating your containerized apps. From helping manage all of your infrastructure, to setting up CD Pipelines, `copilot` is here to help. To learn more about AWS Copilot, check out the [documentation](https://aws.github.io/copilot-cli/).\n</details>\n\n\n# Amazon ECS CLI\n\nThe Amazon ECS Command Line Interface (CLI) is a command line tool for Amazon Elastic Container\nService (Amazon ECS) that provides high-level commands to simplify creating, updating, and\nmonitoring clusters and tasks from a local development environment. The Amazon ECS CLI supports\n[Docker Compose](https://docs.docker.com/compose/), a popular open-source tool for defining and\nrunning multi-container applications. Use the CLI as part of your everyday development and testing\ncycle as an alternative to the AWS Management Console or the AWS CLI.\n\nFor more information about Amazon ECS, see the [Amazon ECS Developer\nGuide](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html).\n\nThe AWS Command Line Interface (AWS CLI) is a unified client for AWS services that provides commands\nfor all public API operations. These commands are lower level than those provided by the Amazon ECS\nCLI. For more information about supported services and to download the AWS CLI, see the [AWS Command\nLine Interface](http://aws.amazon.com/cli/) product detail page.\n\n- [Installing](#installing)\n\t- [Latest version](#latest-version)\n\t- [Download Links for within China](#download-links-for-within-china)\n\t- [Download specific version](#download-specific-version)\n\t- [Verifying Signatures](#verifying-signatures)\n- [Configuring the CLI](#configuring-the-cli)\n\t- [ECS Profiles](#ecs-profiles)\n\t- [Cluster Configurations](#cluster-configurations)\n\t- [Configuring Defaults](#configuring-defaults)\n- [Using the CLI](#using-the-cli)\n\t- [Creating an ECS Cluster](#creating-an-ecs-cluster)\n\t\t- [Creating a Fargate cluster](#creating-a-fargate-cluster)\n\t- [Starting/Running Tasks](#startingrunning-tasks)\n\t- [Creating a Service](#creating-a-service)\n\t- [Using ECS parameters](#using-ecs-parameters)\n\t\t- [Launching an AWS Fargate task](#launching-an-aws-fargate-task)\n\t\t- [Using Route53 Service Discovery](#using-route53-service-discovery)\n\t- [Viewing Running Tasks](#viewing-running-tasks)\n\t- [Viewing Container Logs](#viewing-container-logs)\n\t- [Using FIPS Endpoints](#using-fips-endpoints)\n\t- [Using Private Registry Authentication](#using-private-registry-authentication)\n\t- [Checking for Missing Attributes and Debugging Reason Attribute Errors](#checking-for-missing-attributes-and-debugging-reason-attribute-errors)\n\t- [Tagging Resources](#tagging-resources)\n\t\t- [ARN Formats](#arn-formats)\n\t- [Running Tasks Locally](#running-tasks-locally)\n- [Amazon ECS CLI Commands](#amazon-ecs-cli-commands)\n- [Contributing to the CLI](#contributing-to-the-cli)\n- [License](#license)\n\n#### Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or email AWS security directly at [aws-security@amazon.com](mailto:aws-security@amazon.com).\n\n## Installing\n\nDownload the binary archive for your platform, and install the binary on your `$PATH`.\nYou can use the provided `md5` hash to verify the integrity of your download.\n\nFor information about installing and using the Amazon ECS CLI, see the [ECS Command Line Interface](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI.html).\n\n### Latest version\n* Linux (amd64):\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-latest](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-latest)\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-latest.md5](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-latest.md5)\n* Linux (arm64):\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-arm64-latest](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-arm64-latest)\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-arm64-latest.md5](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-arm64-latest.md5)\n* Macintosh:\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-darwin-amd64-latest](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-darwin-amd64-latest)\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-darwin-amd64-latest.md5](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-darwin-amd64-latest.md5)\n* Windows:\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-windows-amd64-latest.exe](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-windows-amd64-latest.exe)\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-windows-amd64-latest.md5](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-windows-amd64-latest.md5)\n\n### Download Links for within China\n\nAs of v0.6.2 the ECS CLI supports the cn-north-1 region in China. The following links are the exact\nsame binaries, but they are localized within China to provide a faster download experience.\n\n* Linux (amd64):\n  * [https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-linux-amd64-latest](https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-linux-amd64-latest)\n  * [https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-linux-amd64-latest.md5](https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-linux-amd64-latest.md5)\n* Linux (arm64):\n  * [https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-linux-arm64-latest](https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-linux-arm64-latest)\n  * [https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-linux-arm64-latest.md5](https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-linux-arm64-latest.md5)\n* Macintosh:\n  * [https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-darwin-amd64-latest](https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-darwin-amd64-latest)\n  * [https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-darwin-amd64-latest.md5](https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-darwin-amd64-latest.md5)\n* Windows:\n  * [https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-windows-amd64-latest.exe](https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-windows-amd64-latest.exe)\n  * [https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-windows-amd64-latest.md5](https://amazon-ecs-cli.s3.cn-north-1.amazonaws.com.cn/ecs-cli-windows-amd64-latest.md5)\n\n### Download specific version\nUsing the URLs above, replace `latest` with the desired tag, for example `v1.0.0`. After\ndownloading, remember to rename the binary file to `ecs-cli`. \n\n***NOTE:*** Windows is only supported starting with version `v1.0.0`.\n\n***NOTE:*** ARM64 is only supported starting with version `v1.20.0`.\n\n* Linux (amd64):\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-v1.0.0](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-v1.0.0)\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-v1.0.0.md5](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-v1.0.0.md5)\n* Linux (arm64):\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-arm64-v1.20.0](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-arm64-v1.20.0)\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-arm64-v1.20.0.md5](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-arm64-v1.20.0.md5)\n* Macintosh:\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-darwin-amd64-v1.0.0](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-darwin-amd64-v1.0.0)\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-darwin-amd64-v1.0.0.md5](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-darwin-amd64-v1.0.0.md5)\n* Windows:\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-windows-amd64-v1.0.0.exe](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-windows-amd64-v1.0.0.exe)\n  * [https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-windows-amd64-v1.0.0.md5](https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-windows-amd64-v1.0.0.md5)\n\n### Verifying Signatures\n\nIf you wish to verify your ECS CLI download, you can use the PGP Signatures.\n\n#### 1. Install [GnuPG](https://www.gnupg.org/)\n\n###### Linux\n\nInstall `gpg` using the package manager on your flavor of linux.\n\n###### Mac\n\nOne easy way is to use Homebrew, a package manager for OS X. Install Homebrew using the [instructions on its site](https://brew.sh/).\n\n```bash\nbrew install gnupg\nbrew install amazon-ecs-cli\n```\n\n###### Windows\n\nGo to the GnuPG [download page](https://gnupg.org/download/) and download the simple installer for Windows. Use the installer to install the GPG tool.\n\n#### 2. Import the Amazon ECS PGP Public Key\n\nYou can find the Public Key in our GitHub Repo, in the file [amazon-ecs-public-key.gpg](amazon-ecs-public-key.gpg).\n\n```\ngpg --import amazon-ecs-public-key.gpg\n```\n\nKey Metadata:\n\n- Key ID: 0x2D51784F\n- Type: RSA\n- Size: 4096/4096\n- Expires: Never\n- User ID: Amazon ECS <ecs-security@amazon.com>\n- Key fingerprint: F34C 3DDA E729 26B0 79BE AEC6 BCE9 D9A4 2D51 784F\n\n#### 4. Downloading Signatures\n\nECS CLI signatures are ascii armored detached PGP signatures stored in files with the extension \".asc\". The signatures file will have the same name as its corresponding executable with \".asc\" appended. In the\n\n###### Mac\n```\ncurl -o ecs-cli.asc https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-darwin-amd64-latest.asc\n```\n\n###### Linux (amd64)\n```\ncurl -o ecs-cli.asc https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-latest.asc\n```\n\n###### Linux (arm64)\n```\ncurl -o ecs-cli.asc https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-arm64-latest.asc\n```\n\n###### Windows\n```\nPS C:\\> Invoke-WebRequest -OutFile ecs-cli.asc https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-windows-amd64-latest.exe.asc\n```\n#### 4. Verifying a Signature\n\nAssuming you installed the ECS CLI in the recommended location for your platform:\n\n###### Mac and Linux\n```\ngpg --verify ecs-cli.asc /usr/local/bin/ecs-cli\n```\n###### Windows\n```\ngpg --verify ecs-cli.asc C:\\Program Files\\Amazon\\ECSCLI\\ecs-cli.exe\n```\n\nExpected output:\n\n```\ngpg: Signature made Tue Apr  3 13:29:30 2018 PDT\ngpg:                using RSA key DE3CBD61ADAF8B8E\ngpg: Good signature from \"Amazon ECS <ecs-security@amazon.com>\" [unknown]\ngpg: WARNING: This key is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: F34C 3DDA E729 26B0 79BE  AEC6 BCE9 D9A4 2D51 784F\n     Subkey fingerprint: EB3D F841 E2C9 212A 2BD4  2232 DE3C BD61 ADAF 8B8E\n```\n\nThe warning in the output is expected and is not problematic; it occurs because there is not a chain of trust between your personal PGP key (if you have one) and the Amazon ECS PGP key. For more information, learn about the [Web of trust](https://en.wikipedia.org/wiki/Web_of_trust).\n\n\n## Configuring the CLI\n\nThe Amazon ECS CLI requires some basic configuration information before you can use it, such as your\nAWS credentials, the AWS region in which to create your cluster, and the name of the Amazon ECS\ncluster to use. Configuration information is stored in the `~/.ecs` directory on macOS and Linux\nsystems and in `C:\\Users\\<username>\\AppData\\local\\ecs` on Windows systems.\n\n### ECS Profiles\n\nThe Amazon ECS CLI supports configuring multiple sets of AWS credentials as named profiles using the\n`ecs-cli configure profile command`. These profiles can then be referenced when you run Amazon ECS\nCLI commands using the `--ecs-profile` flag; if a custom profile is not specified, the default\nprofile will be used.\n\nSet up a CLI profile with the following command, substituting `profile_name` with your desired\nprofile name, and `$AWS_ACCESS_KEY_ID`, `$AWS_SECRET_ACCESS_KEY`, and `AWS_SESSION_TOKEN` environment variables with your\nAWS credentials.\n\n`ecs-cli configure profile --profile-name profile_name --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY --session-token AWS_SESSION_TOKEN`\n\n### Cluster Configurations\n\nA cluster configuration is the set of fields that describes an Amazon ECS cluster, including the\nname of the cluster and the region. These configurations can then be referenced when you run Amazon\nECS CLI commands using the `--cluster-config` flag; otherwise, the default configuration is used.\n\nCreate a cluster configuration with the following command, substituting `region_name` with your\ndesired AWS region, `cluster_name` with the name of an existing Amazon ECS cluster or a new cluster\nto use, and `configuration_name` with the name you'd like to give this configuration.\n\n`ecs-cli configure --cluster cluster_name --region region_name --config-name configuration_name`\n\nYou can also optionally add `--default-launch-type` to your cluster configuration. This value will\nbe used as the launch type for tasks run in this cluster (see: [Launching an AWS Fargate\nTask](#launching-an-aws-fargate-task)) , and will also be used to determine which resources to\ncreate when you bring up a cluster (see: [Creating a Fargate Cluster](#creating-a-fargate-cluster)).\nValid values for this field are EC2 or FARGATE. If not specified, ECS will default to EC2 launch\ntype.\n\n### Configuring Defaults\n\nThe first Cluster Configuration or ECS Profile that you configure will be set as the default. The\ndefault ECS Profile can be changed using the `ecs-cli configure profile default` command; the\ndefault cluster configuration can be changed using the `ecs-cli configure default` command. Note\nthat unlike in the AWS CLI, the default ECS Profile does not need to be named \"default\".\n\n#### Using Credentials from `~/.aws/credentials`, Assuming a Role, and Multi-Factor Authentication\n\nThe `--aws-profile` flag and `$AWS_PROFILE` environment variable allow you to reference any named profile in `~/.aws/credentials`.\n\nHere is an example on how to assume a role: [amazon-ecs-cli/blob/master/ecs-cli/modules/config/aws_credentials_example.ini](https://github.com/aws/amazon-ecs-cli/blob/master/ecs-cli/modules/config/aws_credentials_example.ini)\n\nIf you are trying to use Multi-Factor Authentication, please see this comment and the associated issue: [#284 (comment)](https://github.com/aws/amazon-ecs-cli/issues/284#issuecomment-336310034).\n\n#### Order of Resolution for credentials\n\n1) ECS CLI Profile Flags\n  a) ECS Profile (--ecs-profile)\n  b) AWS Profile (--aws-profile)\n2) Environment Variables - attempts to fetch the credentials from environment variables:\n  a) ECS_PROFILE\n  b) AWS_PROFILE\n  c) AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, Optional: AWS_SESSION_TOKEN\n3) ECS Config - attempts to fetch the credentials from the default ECS Profile\n4) Default AWS Profile - attempts to use credentials (aws_access_key_id, aws_secret_access_key) or assume_role (role_arn, source_profile) from AWS profile name\n  a) AWS_DEFAULT_PROFILE environment variable (defaults to 'default')\n5) EC2 Instance role\n\n#### Order of Resolution for Region\n\n1) ECS CLI Flags\n   a) Region Flag --region\n   b) Cluster Config Flag (--cluster-config)\n2) ECS Config - attempts to fetch the region from the default ECS Profile\n3) Environment Variable - attempts to fetch the region from environment variables:\n   a) AWS_REGION (OR)\n   b) AWS_DEFAULT_REGION\n4)  AWS Profile - attempts to use region from AWS profile name\n   a) AWS_PROFILE environment variable (OR) \u2013aws-\n   b) AWS_DEFAULT_PROFILE environment variable (defaults to 'default')\n\n\nFor more information, see [ECS CLI Configuration](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_Configuration.html).\n\n## Using the CLI\n\nECS now offers two different launch types for tasks and services: EC2 and FARGATE. With the FARGATE\nlaunch type, customers no longer have to manage their own container-instances.\n\nIn the ECS-CLI, you can specify either launch type when you bring up a cluster using the\n`--launch-type` flag (see: [Creating an ECS Cluster](#creating-an-ecs-cluster)). You can also\nconfigure your cluster to use a particular launch type with the `--default-launch-type` flag (see:\n[Cluster Configurations](#cluster-configurations)).\n\nYou can also specify which launch type to use for a task or service in `compose up` or `compose\nservice up`, regardless of which launch type is configured for your cluster (see: [Starting/Running\nTasks](#startingrunning-tasks)).\n\n### Creating an ECS Cluster\nAfter installing the Amazon ECS CLI and configuring your credentials, you are ready to create an ECS cluster. The basic command for creating a cluster is:\n```\necs-cli up\n```\n\n(To see all available options, run `ecs-cli up --help`)\n\nFor example, to create an ECS cluster with two Amazon EC2 instances using the EC2 launch type, use\nthe following command:\n\n```\n$ ecs-cli up --keypair my-key --capability-iam --size 2\n```\n\nIt takes a few minutes to create the resources requested by `ecs-cli up`.  To see when the cluster\nis ready to run tasks, use the AWS CLI to confirm that the ECS instances are registered:\n\n```\n$ aws ecs list-container-instances --cluster your-cluster-name\n{\n    \"containerInstanceArns\": [\n        \"arn:aws:ecs:us-east-1:980116778723:container-instance/6a302e06-0aa6-4bbc-9428-59b17089b887\",\n        \"arn:aws:ecs:us-east-1:980116778723:container-instance/7db3c588-0ef4-49fa-be32-b1e1464f6eb5\",\n    ]\n}\n\n```\nIn addition to EC2 Instances, other resources created by default include:\n* Autoscaling Group\n* Autoscaling Launch Configuration\n* EC2 VPC\n* EC2 Internet Gateway\n* EC2 VPC Gateway Attachment\n* EC2 Route Table\n* EC2 Route\n* 2 Public EC2 Subnets\n* 2 EC2 SubnetRouteTableAssocitaions\n* EC2 Security Group\n\nYou can provide your own resources (such as subnets, VPC, or security groups) via their flag options.\n\n**Note:** The default security group created by `ecs-cli up` allows inbound traffic on port 80 by\ndefault. To allow inbound traffic from a different port, specify the port you wish to open with the\n`--port` option. To add more ports to the default security group, go to **EC2 Security Groups** in\nthe AWS Management Console and search for the security group containing \u201cecs-cli\u201d. Add a rule as\ndescribed in the [Adding Rules to a Security Group](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html#adding-security-group-rule)\ntopic.\n\nAlternatively, you may specify one or more existing security group IDs with the `--security-group` option.\n\nYou can also create an empty ECS cluster by using the `--empty` or `--e` flag:\n\n```\necs-cli up --cluster myCluster --empty\n```\n\nThis is equivalent to the [create-cluster command](https://docs.aws.amazon.com/cli/latest/reference/ecs/create-cluster.html), and will not create a CloudFormation stack associated with your cluster.\n\n#### AMI\n\nYou can specify the AMI to use with your EC2 instances using the `--image-id` flag. Alternatively, if you do not specify an image ID, the ECS CLI will use the [recommended Amazon Linux 2 ECS Optimized AMI](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/retrieve-ecs-optimized_AMI.html). By default, the x86 variant of this AMI is used. However, if you specify an instance in the A1 family using `--instance-type`, then the `arm64` version of the ECS Optimized AMI will be used. Note: `arm64` ECS Optimized AMIs are only supported in some regions; please see [Amazon ECS-Optimized Amazon Linux 2 AMI](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/al2ami.html).\n\n#### User Data\n\nFor the EC2 launch type, the ECS CLI always creates EC2 instances that include the following User Data:\n\n```\n#!/bin/bash\necho ECS_CLUSTER={ clusterName } >> /etc/ecs/ecs.config\n```\n\nThis user data directs the EC2 instance to join your ECS Cluster. You can optionally include extra user data with `--extra-user-data`; this flag takes a file name as its argument.\nThe flag can be used multiple times to specify multiple files. Extra user data can be shell scripts or cloud-init directives- see the [EC2 documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html) for more information.\nThe ECS CLI takes all the User Data, and packs it into a MIME Multipart archive which can be used by cloud-init on the EC2 instance. The ECS CLI even allows existing MIME Multipart archives to be passed in with `--extra-user-data`.\nThe CLI will unpack the existing archive, and then repack it into the final archive (preserving all header and content type information). Here is an example of specifying extra user data:\n\n```\necs-cli up \\\n  --capability-iam \\\n  --extra-user-data my-shellscript \\\n  --extra-user-data my-cloud-boot-hook \\\n  --extra-user-data my-mime-multipart-archive \\\n  --launch-type EC2\n```\n\n#### Creating a Fargate cluster\n\n```\n$ ecs-cli up --launch-type FARGATE\n```\n\nThis will create an ECS Cluster without container instances. By default, this will create the\nfollowing resources:\n\n* EC2 VPC\n* EC2 Internet Gateway\n* EC2 VPC Gateway Attachment\n* EC2 Route Table\n* EC2 Route\n* 2 Public EC2 Subnets\n* 2 EC2 SubnetRouteTableAssocitaions\n\nThe subnet and VPC ids will be printed to the terminal once the creation is complete. You can then\nuse the subnet IDs in your ECS Params file to launch Fargate tasks.\n\nFor more information on using AWS Fargate, see the [ECS CLI Fargate tutorial](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_tutorial_fargate.html).\n\n### Starting/Running Tasks\nAfter the cluster is created, you can run tasks \u2013 groups of containers \u2013 on the ECS cluster. First,\nauthor a [Docker Compose configuration file](https://docs.docker.com/compose).  You can run the\nconfiguration file locally using Docker Compose. Information about specific compose versions and fields supported by the ecs-cli can be found [here](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cmd-ecs-cli-compose-parameters.html).\n\nHere is an example Docker Compose configuration file that creates a web page:\n\n```\nversion: '2'\nservices:\n  web:\n    image: amazon/amazon-ecs-sample\n    ports:\n     - \"80:80\"\n```\n\nTo run the configuration file on Amazon ECS, use `ecs-cli compose up`. This creates an ECS task\ndefinition and starts an ECS task. You can see the task that is running with `ecs-cli compose ps`,\nfor example:\n\n```\n$ ecs-cli compose ps\nName                                      State    Ports                     TaskDefinition\nfd8d5a69-87c5-46a4-80b6-51918092e600/web  RUNNING  54.209.244.64:80->80/tcp  web:1\n```\n\nNavigate your web browser to the task\u2019s IP address to see the sample app running in the ECS cluster.\n\n### Creating a Service\nYou can also run tasks as services. The ECS service scheduler ensures that the specified number of\ntasks are constantly running and reschedules tasks when a task fails (for example, if the underlying\ncontainer instance fails for some reason).\n\n```\n$ ecs-cli compose --project-name wordpress-test service create\n\nINFO[0000] Using Task definition                         TaskDefinition=wordpress-test:1\nINFO[0000] Created an ECS Service                        serviceName=wordpress-test taskDefinition=wordpress-test:1\n\n```\n\nYou can then start the tasks in your service with the following command:\n`$ ecs-cli compose --project-name wordpress-test service start`\n\nIt may take a minute for the tasks to start. You can monitor the progress using\nthe following command:\n```\n$ ecs-cli compose --project-name wordpress-test service ps\nName                                            State    Ports                      TaskDefinition\n34333aa6-e976-4096-991a-0ec4cd5af5bd/wordpress  RUNNING  54.186.138.217:80->80/tcp  wordpress-test:1\n34333aa6-e976-4096-991a-0ec4cd5af5bd/mysql      RUNNING                             wordpress-test:1\n```\n\nSee the `$ ecs-cli compose service` [documentation page](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cmd-ecs-cli-compose-service.html) for more information about available service options, including load balancing.\n\n### Using ECS parameters\n\nSince there are certain fields in an ECS task definition that do not correspond to fields in a\nDocker Composefile, you can specify those values using the `--ecs-params` flag. Currently, the file\nsupports the follow schema:\n\n```\nversion: 1\ntask_definition:\n  ecs_network_mode: string               // Supported string values: none, bridge, host, or awsvpc\n  task_role_arn: string\n  task_execution_role: string            // Needed to use Cloudwatch Logs or ECR with your ECS tasks\n  task_size:                             // Required for running tasks with Fargate launch type\n    cpu_limit: string\n    mem_limit: string                    // Values specified without units default to MiB\n  pid_mode: string                       // Supported string values: task or host\n  ipc_mode: string                       // Supported string values: task, host, or none\n  services:\n    <service_name>:\n      essential: boolean\n      depends_on:\n        - container_name: string         // <service_name> of any other service in services\n          condition: string              // Valid values: START | COMPLETE | SUCCESS | HEALTHY\n      repository_credentials:\n        credentials_parameter: string\n      cpu_shares: integer\n      firelens_configuration:\n        type: string                     // Supported string values: fluentd or fluentbit\n        options: list of strings\n      mem_limit: string                  // Values specified without units default to bytes, as in docker run\n      mem_reservation: string\n      gpu: string\n      init_process_enabled: boolean\n      healthcheck:\n        test: string or list of strings\n        interval: string\n        timeout: string\n        retries: integer\n        start_period: string\n      logging:\n        secret_options:\n          - value_from: string\n            name: string\n      secrets:\n        - value_from: string\n          name: string\n  docker_volumes:\n    - name: string\n      scope: string                      // Valid values: \"shared\" | \"task\"\n      autoprovision: boolean             // only valid if scope = \"shared\"\n      driver: string\n      driver_opts:\n        string: string\n      labels:\n        string: string\n  efs_volumes:\n     - name: string\n       filesystem_id: string\n       root_directory: string\n       transit_encryption: string       // Valid values: \"ENABLED\" | \"DISABLED\" (default). Required if \n                                        //   IAM is enabled or an access point ID is  \n                                        //   specified\n       transit_encryption_port: int64   // required if transit_encryption is enabled\n       access_point: string\n       iam: string                      // Valid values: \"ENABLED\" | \"DISABLED\" (default). Enable IAM \n                                        //   authentication for FS access. \n  placement_constraints:\n    - type: string                      // Valid values: \"memberOf\"\n      expression: string\n\nrun_params:\n  network_configuration:\n    awsvpc_configuration:\n      subnets: array of strings          // These should be in the same VPC and Availability Zone as your instance\n      security_groups: list of strings   // These should be in the same VPC as your instance\n      assign_public_ip: string           // supported values: ENABLED or DISABLED\n  task_placement:\n    strategy:\n      - type: string                     // Valid values: \"spread\"|\"binpack\"|\"random\"\n        field: string                    // Not valid if type is \"random\"\n    constraints:\n      - type: string                     // Valid values: \"memberOf\"|\"distinctInstance\"\n        expression: string               // Not valid if type is \"distinctInstance\"\n  service_discovery:\n    container_name: string\n    container_port: integer\n    private_dns_namespace:\n      id: string\n      name: string\n      vpc: string\n      description: string\n    public_dns_namespace:\n      id: string\n      name: string\n    service_discovery_service:\n      name: string\n      description: string\n      dns_config:\n        type: string\n        ttl: integer\n      healthcheck_custom_config:\n        failure_threshold: integer\n```\n\n**Version**\nSchema version being used for the ecs-params.yml file. Currently, we only support version 1.\n\n**Task Definition**\nFields listed under `task_definition` correspond to fields that will be included in your ECS Task Definition.\n\n* `ecs_network_mode` corresponds to NetworkMode on an ECS Task Definition (Not to be confused with the network_mode field in Docker Compose). Supported values are none, bridge, host, or awsvpc. If not specified, this will default to bridge mode. If you wish to run tasks with Network Configuration, this field *must* be set to `awsvpc`.\n\n* `task_role_arn` should be the ARN of an IAM role. **NOTE**: If this role does not have the proper permissions/trust relationships on it, the `up` command will fail.\n\n* `services` correspond to the services listed in your docker compose file, with `service_name` matching the name of the container you wish to run. Its fields will be merged into an [ECS Container Definition](http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-containerdefinitions.html).\n  * If the [`essential`](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-containerdefinitions.html#cfn-ecs-taskdefinition-containerdefinition-essential) field is not specified, the value defaults to true.\n  * `depends_on` field maps to [`dependsOn`](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#container_definition_dependson) parameter in task definition. It allows you to specify a list of [`ContainerDependency`](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ecs-taskdefinition-containerdependency.html), which can be used for conditional startup of dependent containers or ensuring order of startup between containers. Refer [example](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/example_task_definitions.html#example_task_definition-containerdependency).\n  * If you are using Docker compose version 3, the `cpu_shares`, `mem_limit`, and `mem_reservation` fields are optional and must be specified in the ECS params file rather than the compose file.\n  * In Docker compose version 2, the `cpu_shares`, `mem_limit`, and `mem_reservation` fields can be specified in either the compose or ECS params file. If they are specified in the ECS params file, the values will override values present in the compose file.\n  * If you are using a private repository for pulling images, `repository_credentials` allows you to specify an AWS Secrets Manager secret ARN for the name of the secret containing your private repository credentials as a `credential_parameter`.\n  * `init_process_enabled` is a [Linux-specific option](https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_LinuxParameters.html) that can be be set to run an init process inside the container that forwards signals and reaps processes. This parameter maps to the `--init` option to [docker run](https://docs.docker.com/engine/reference/run/). This parameter requires version 1.25 of the Docker Remote API or greater on your container instance.\n  * `firelens_configuration` contains configuration parameters for [Firelens](https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_FirelensConfiguration.html).\n    * `type` Valid options are fluentbit or fluentd\n    * `options` Please see the [AWS docs for Firelens](https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_FirelensConfiguration.html)\n  * `gpu` is the number of physical GPUs the Amazon ECS container agent will reserve for the container. Maps to the GPU [resource requirement](https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_ResourceRequirement.html) field in the task definition. For example: \"1\", \"4\", \"8\", \"16\".\n  * `healthcheck` This parameter maps to `healthcheck` in the [Docker compose file reference](https://docs.docker.com/compose/compose-file/#healthcheck). This field can either be used here in the ECS Params file, or it can be used in Compose File version 3 with the ECS CLI.\n    * `test` can also be specified as `command` and must be either a string or a list or strings. If `test` is specified as a list of strings, the first item must be either NONE, CMD, or CMD-SHELL. If test or command is specified as a string, CMD-SHELL will be prepended and ECS will run the command in the container's default shell.\n    * `interval`, `timeout`, and `start_period` are specified as durations in a string format. For example: 2.5s, 10s, 1m30s, 2h23m, or 5h34m56s.\n  * `secrets` allows you to specify secrets which will be retrieved from SSM Parameter Store. See the [ECS Docs](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html) for more information, including how reference AWS Secrets Managers secrets from SSM Parameter Store.\n    * `value_from` is the SSM (or Secrets Manager) Parameter ARN or name (if the parameter is in the same region as your ECS Task).\n    * `name` is the name of the environment variable in which the secret will be stored.\n  * If you need to inject secrets into your logging configuration, you may set `secret_options` under `logging`. For more information, See the [logging secrets section](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html#secrets-logconfig) of the ECS docs.\n    * `value_from` is the SSM (or Secrets Manager) Parameter ARN or name (if the parameter is in the same region as your ECS Task).\n    * `name` is the name of the logging option in which the secret will be stored.\n\n* `docker_volumes` allows you to create docker volumes. The name key is required, and `scope`, `autoprovision`, `driver`, `driver_opts` and `labels` correspond with the fields under [dockerVolumeConfiguration](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-volumes.html) in an ECS Task Definition. Volumes defined with the `docker_volumes` key can be referenced in your compose file by name, even if they were not also specified in the compose file.\n\n* `efs_volumes` allows you to mount EFS volumes to your container. The name and EFS filesystem ID are required. EFS volumes can be referenced by name in your compose file like `docker_volumes`. \n\n* `task_execution_role` should be the ARN of an IAM role. **NOTE**: This field is required to enable ECS Tasks to be configured with Cloudwatch Logs, or to pull images from ECR for your tasks.\n\n* `task_size` Contains two fields, CPU and Memory. These fields are required for launching tasks with Fargate launch type. See [the documentation on ECS Task Definition Parameters](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html) for more information.\n\n* `placement_constraints` allows you to specify a list of constraints on task placement within the task definition. Not supported with the `FARGATE` launch type.\n\n* `pid_mode` allows you to control the process namespace in which your containers run. Valid values are `task` or `host`. See the [ECS documentation](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#task_definition_pidmode) for more information.\n\n* `ipc_mode` allows you to control the IPC resource namespace in which your containers run. Valid values are `task`, `host`, or `none`. See the [ECS documentation](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#task_definition_ipcmode) for more information.\n\n**Run Params**\nFields listed under `run_params` are for values needed as options to API calls not related to a Task Definition, such as `compose up` (RunTask) and `compose service up` (CreateService).\nCurrently, the only parameter supported under `run_params` is `network_configuration`. This is required to run tasks with [Task Networking](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html), as well as with Fargate launch type.\n\n* `network_configuration` is required if you specify `ecs_network_mode` as `awsvpc`. It takes one nested parameter, `awsvpc_configuration`, which has three subfields:\n  * `subnets`: list of subnet ids used to launch tasks. ***NOTE*** These should be in the same VPC and availability zone as the instances on which you wish to launch your tasks.\n  * `security_groups`: list of securtiy-group ids used to launch tasks. ***NOTE*** These should be in the same VPC as the instances on which you wish to launch your tasks.\n  * `assign_public_ip`: supported values for this field are either \"ENABLED\" or \"DISABLED\". This field is *only* used for tasks launched with Fargate launch type. If this field is present in tasks with network configuration launched with EC2 launch type, the request will fail.\n* `task_placement` is an optional field with `EC2` launch-type only (it is *not* valid for `FARGATE`). It has two subfields:\n  * `strategy`: A list of objects, with two keys. Valid keys are `type` and `field`.\n    * `type`: Valid values are `random`, `binpack`, or `spread`. If `random` is specified, the `field` key should not be provided.\n    * `field`: Valid values depend on the strategy type.\n      * For `spread`, valid values are `instanceId`, `host`, or attribute key/value pairs, e.g. `attribute:ecs.instance-type =~ t2.*`\n      * For \"binpack\", valid values are \"cpu\" or \"memory\".\n  * `constraint`: A list of objects, with two keys. Valid keys are `type` and `expression`.\n    * `type`: Valid values are `distinctInstance` and `memberOf`. If `distinctInstance` is specified, the `expression` key should not be provided.\n    * `expression`: When `type` is `memberOf`, valid values are key/value pairs for attributes or task groups, e.g. `task:group == databases` or `attribute:color =~ green`.\n* `service_discovery` allows the configuration of Service Discovery using Route53 auto naming. For an explanation of these fields, see [Using Route53 Service Discovery](#using-route53-service-discovery).\n\nFor more information on task placement, see [Amazon ECS TaskPlacement] (https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement.html).\n\nExample `ecs-params.yml` file:\n\n```\nversion: 1\ntask_definition:\n  ecs_network_mode: host\n  task_role_arn: myCustomRole\n  services:\n    logging:\n      essential: false\n    wordpress:\n      cpu_shares: 100\n      mem_limit: 500m\n    mysql:\n      cpu_shares: 105\n      mem_limit: 500m\n      mem_reservation: 450m\n  docker_volumes:\n    - name: database_volume\n      scope: shared\n      autoprovision: true\n      driver: local\n```\n\nExample `ecs-params.yml` with network configuration with **EC2** launch type:\n\n```\nversion: 1\ntask_definition:\n  ecs_network_mode: awsvpc\n  services:\n    my_service:\n      essential: false\n\nrun_params:\n  network_configuration:\n    awsvpc_configuration:\n      subnets:\n        - subnet-feedface\n        - subnet-deadbeef\n      security_groups:\n        - sg-bafff1ed\n        - sg-c0ffeefe\n```\nExample `ecs-params.yml` with network configuration with **FARGATE** launch type:\n\n```\nversion: 1\ntask_definition:\n  ecs_network_mode: awsvpc\n  task_execution_role: myFargateRole\n  task_size:\n    cpu_limit: 512\n    mem_limit: 2GB\n  services:\n    my_service:\n      essential: false\n\nrun_params:\n  network_configuration:\n    awsvpc_configuration:\n      subnets:\n        - subnet-feedface\n        - subnet-deadbeef\n      security_groups:\n        - sg-bafff1ed\n        - sg-c0ffeefe\n      assign_public_ip: ENABLED\n```\n\nExample `ecs-params.yml` with task placement:\n\n```\nversion: 1\nrun_params:\n  task_placement:\n    strategy:\n      - field: memory\n        type: binpack\n      - field: attribute:ecs.availability-zone\n        type: spread\n      - type: random\n    constraints:\n      - expression: attribute:ecs.instance-type =~ t2.*\n        type: memberOf\n      - type: distinctInstance`\n```\n\nExample `ecs-params.yml` with EFS volume:\n\n```\nversion: 1\ntask_definition:\n  task_execution_role: ecsTaskExecutionRole\n  ecs_network_mode: awsvpc\n  task_size:\n    mem_limit: 1.0GB\n    cpu_limit: 512\n  efs_volumes:\n    - name: \"myEFSVolume\"\n      filesystem_id: \"fs-fedc8554\"\nrun_params:\n  network_configuration:\n    awsvpc_configuration:\n      subnets:\n        - \"subnet-0b24acd73f534bb4f\"\n        - \"subnet-0f0e20022e2cccd67\"\n      security_groups:\n        - \"sg-0fb24ebc7dd5254b0\"\n      assign_public_ip: \"ENABLED\"\n```\n\nYou can then start a task by calling:\n```\necs-cli compose --ecs-params my-ecs-params.yml up\n```\n\nIf you have a file name `ecs-params.yml` in your current directory, `ecs-cli compose` will automatically read it without your having to set the `--ecs-params` flag value explicitly.\n\n```\necs-cli compose up\n```\n\n#### Launching an AWS Fargate task\n\nWith network configuration specified in your ecs-params.yml file, you can now launch a task with\nlaunch type FARGATE:\n\n```\necs-cli compose --ecs-params my-ecs-params.yml up --launch-type FARGATE\n```\n\nor\n\n```\necs-cli compose --ecs-params my-ecs-params.yml service up --launch-type FARGATE\n```\n\n#### Using Route53 Service Discovery\n\nWith the ECS CLI, you can create an ECS Service that uses [Route53 auto naming for service discovery](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-discovery.html). Service Discovery requires a Service Discovery Service and a DNS Namespace. Keep in mind that:\n* When you enable Service Discovery with the ECS CLI, a new Service Discovery Service is always created using CloudFormation.\n* For the DNS Namespace, you have the option of using an existing public or private DNS Namespace, or letting the ECS CLI create a private DNS Namespace for you using CloudFormation.\n* Creation of a Public DNS Namespaces is not supported with the ECS CLI.\n* Only a single DNS Namespace may be used with Service Discovery.\n\n##### Enabling Service Discovery\n\n###### Specifying Values\n\nThe ECS-CLI simplifies the use of Service Discovery by providing default values for most fields, while still allowing maximum configurability. Here are the default values and explanations listed with the ECS Params input schema:\n\n```\nversion: 1\nrun_params:\n  service_discovery:\n    container_name: string            // Required if using SRV records\n    container_port: string            // Required if using SRV records\n    private_dns_namespace:\n      id: string                      // Allows you to specify an existing namespace by ID\n      name: string                    // DNS name for private namespace. Either used to specify an existing namespace, or if one does not exist with this name, the ECS CLI will create it\n      vpc: string                     // Required if \"id\" is not specified\n      description: string             // Only used if the namespace does not yet exist. Default = \"Created by the Amazon ECS CLI\"\n    public_dns_namespace:\n      id: string                      // Specify an existing public namespace by ID\n      name: string                    // Or specify an existing public namespace by Name\n    service_discovery_service:\n      name: string                    // Default = Name of the your ECS Service\n      description: string             // Default = \"Created by the Amazon ECS CLI\"\n      dns_config:\n        type: string                  // Valid values: A or SRV. SRV is required/the default when using bridge or host network mode. A is the default for the awsvpc network mode.\n        ttl: integer                  // Default = 60\n      healthcheck_custom_config:\n        failure_threshold: integer    // Default = 1\n```\n\n###### Simple Workflow\n\nLet's walk through a simple scenario with Service Discovery to see how it works with the ECS CLI. Many of the Service Discovery configuration values can be specified with flags, which take precedence over the ECS Params if both are present. Remember that with the ECS CLI, the Compose Project Name (name of the directory containing your Docker Compose File, unless otherwise specified using the flag) is used as the name for your ECS Service.\n\nFirst, we create a Service named `backend` and create a Private DNS Namespace in our VPC. Assume that the network mode is `awsvpc`, so the `container_name` and `container_port` values are not needed.\n\n```\n$ ecs-cli compose --project-name backend service up --private-dns-namespace tutorial --vpc vpc-04deee8176dce7d7d --enable-service-discovery\nINFO[0001] Using ECS task definition                     TaskDefinition=\"backend:1\"\nINFO[0002] Waiting for the private DNS namespace to be created...\nINFO[0002] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nWARN[0033] Defaulting DNS Type to A because network mode was awsvpc\nINFO[0033] Waiting for the Service Discovery Service to be created...\nINFO[0034] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0065] Created an ECS service                        service=backend taskDefinition=\"backend:1\"\nINFO[0066] Updated ECS service successfully              desiredCount=1 serviceName=backend\nINFO[0081] (service backend) has started 1 tasks: (task 824b5a76-8f9c-4beb-a64b-6904e320630e).  timestamp=\"2018-09-12 00:00:26 +0000 UTC\"\nINFO[0157] Service status                                desiredCount=1 runningCount=1 serviceName=backend\nINFO[0157] ECS Service has reached a stable state        desiredCount=1 runningCount=1 serviceName=backend\n```\n\nNext, we create another service called `frontend` in the same Private DNS Namespace. Since the Namespace was already created, the ECS CLI knows to use the existing one.\n\n```\n$ ecs-cli compose --project-name frontend service up --private-dns-namespace tutorial --vpc vpc-04deee8176dce7d7d --enable-service-discovery\nINFO[0001] Using ECS task definition                     TaskDefinition=\"frontend:1\"\nINFO[0002] Using existing namespace ns-kvhnzhb5vxplfmls\nWARN[0033] Defaulting DNS Type to A because network mode was awsvpc\nINFO[0033] Waiting for the Service Discovery Service to be created...\nINFO[0034] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0065] Created an ECS service                        service=frontend taskDefinition=\"frontend:1\"\nINFO[0066] Updated ECS service successfully              desiredCount=1 serviceName=frontend\nINFO[0081] (service frontend) has started 1 tasks: (task 824b5a76-8f9c-4beb-a64b-6904e320630e).  timestamp=\"2018-09-12 00:00:26 +0000 UTC\"\nINFO[0157] Service status                                desiredCount=1 runningCount=1 serviceName=frontend\nINFO[0157] ECS Service has reached a stable state        desiredCount=1 runningCount=1 serviceName=frontend\n```\n\nNow, the two Services can find each other in the VPC using DNS. The DNS host name will be the name of the Service Discovery Service plus the name of the DNS Namespace. So the ECS Service `frontend` can be found at `frontend.tutorial`, and `backend` can be found at `backend.tutorial`. Remember that since this is a Private DNS Namespace, these domain names can only be resolved within your VPC.\n\nNow, let's update some of the Service Discovery settings for `frontend`; the only values that can be updated are `DNS TTL` and `Health Check Custom Config Failure Threshold` (the failure threshold for the health check administered by ECS, which determines when unhealthy containers will have their DNS records removed).\n\n```\n$ ecs-cli compose --project-name frontend service up --update-service-discovery --dns-type SRV --dns-ttl 120 --healthcheck-custom-config-failure-threshold 2\nINFO[0001] Using ECS task definition                     TaskDefinition=\"frontend:1\"\nINFO[0001] Updated ECS service successfully              desiredCount=1 serviceName=frontend\nINFO[0001] Service status                                desiredCount=1 runningCount=1 serviceName=frontend\nINFO[0001] ECS Service has reached a stable state        desiredCount=1 runningCount=1 serviceName=frontend\nINFO[0002] Waiting for your Service Discovery resources to be updated...\nINFO[0002] Cloudformation stack status                   stackStatus=UPDATE_IN_PROGRESS\n```\n\nNext, we delete the services and the Service Discovery resources. When we delete `frontend`, the CLI automatically removes its associated Service Discovery Service.\n\n```\n$ ecs-cli compose --project-name frontend service down\nINFO[0000] Updated ECS service successfully              desiredCount=0 serviceName=frontend\nINFO[0001] Service status                                desiredCount=0 runningCount=1 serviceName=frontend\nINFO[0016] Service status                                desiredCount=0 runningCount=0 serviceName=frontend\nINFO[0016] (service frontend) has stopped 1 running tasks: (task 824b5a76-8f9c-4beb-a64b-6904e320630e).  timestamp=\"2018-09-12 00:37:25 +0000 UTC\"\nINFO[0016] ECS Service has reached a stable state        desiredCount=0 runningCount=0 serviceName=frontend\nINFO[0016] Deleted ECS service                           service=frontend\nINFO[0016] ECS Service has reached a stable state        desiredCount=0 runningCount=0 serviceName=frontend\nINFO[0027] Waiting for your Service Discovery Service resource to be deleted...\nINFO[0027] Cloudformation stack status                   stackStatus=DELETE_IN_PROGRESS\n```\n\nFinally, we delete `backend` and the Private DNS Namespace which was created with it (the CLI associates the CloudFormation Stack for the Namespace with the ECS Service that it was originally created for, so the two should be deleted together).\n\n```\n$ ecs-cli compose --project-name backend service down --delete-namespace\nINFO[0000] Updated ECS service successfully              desiredCount=0 serviceName=backend\nINFO[0001] Service status                                desiredCount=0 runningCount=1 serviceName=backend\nINFO[0016] Service status                                desiredCount=0 runningCount=0 serviceName=backend\nINFO[0016] (service backend) has stopped 1 running tasks: (task 824b5a76-8f9c-4beb-a64b-6904e320630e).  timestamp=\"2018-09-12 00:37:25 +0000 UTC\"\nINFO[0016] ECS Service has reached a stable state        desiredCount=0 runningCount=0 serviceName=backend\nINFO[0016] Deleted ECS service                           service=backend\nINFO[0016] ECS Service has reached a stable state        desiredCount=0 runningCount=0 serviceName=backend\nINFO[0027] Waiting for your Service Discovery Service resource to be deleted...\nINFO[0027] Cloudformation stack status                   stackStatus=DELETE_IN_PROGRESS\nINFO[0059] Waiting for your Private DNS Namespace resource to be deleted...\nINFO[0059] Cloudformation stack status                   stackStatus=DELETE_IN_PROGRESS\n```\n\n\n### Viewing Running Tasks\n\nThe PS commands allow you to see running and recently stopped tasks. To see the Tasks running in your cluster:\n\n```\n$ ecs-cli ps\nName                                            State    Ports                     TaskDefinition\n37e873f6-37b4-42a7-af47-eac7275c6152/web        RUNNING  10.0.1.27:8080->8080/tcp  TaskNetworking:2\n37e873f6-37b4-42a7-af47-eac7275c6152/lb         RUNNING  10.0.1.27:80->80/tcp      TaskNetworking:2\n37e873f6-37b4-42a7-af47-eac7275c6152/redis      RUNNING                            TaskNetworking:2\n40bedf31-d707-446e-affc-766eac4cfb85/mysql      RUNNING                            fargate:1\n40bedf31-d707-446e-affc-766eac4cfb85/wordpress  RUNNING  54.16.93.6:80->80/tcp     fargate:1\n```\n\nThe IP address displayed by the ECS CLI depends on how your cluster is configured and which launch-type is used. If you are running tasks with launch type EC2 without task networking, then the IP address shown will be the public IP of the EC2 instance running your task. If no public IP was assigned, the instance's private IP will be displayed.\n\nFor tasks that use Task Networking with EC2 launch type, the ECS CLI will only show the private IP address of the ENI attached to the task.\n\nFor Fargate tasks, the ECS CLI will return the public IP assigned to the ENI attached to the Fargate task. The ENI for your Fargate task will be assigned a public IP if `assign_public_ip: ENABLED` is present in your ECS Params file. If the ENI lacks a public IP, then its private IP is shown.\n\nYou can use the `--desired-status` flag to filter for \"STOPPED\" or \"RUNNING\" containers.\n\n### Viewing Container Logs\n\nView the CloudWatch Logs for a given task and container:\n\n`ecs-cli logs --task-id 4c2df707-a160-475e-9c16-15dfb9df01cc --container-name mysql`\n\nFor Fargate tasks, it is recommended that you send your container logs to CloudWatch. *Note: For Fargate tasks you must specify a Task Execution IAM Role in your ECS Params file in order to use CloudWatch Logs.* You can specify the `awslogs` driver and logging options in your compose file like this:\n\n```\nservices:\n  <My Service>:\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: <Log Group Name>\n        awslogs-region: <Log Region>\n        awslogs-stream-prefix: <Prefix Name>\n```\n\nThe log stream prefix is technically optional; however, it is highly recommended that you specify it. If you do specify it, then you can use the `ecs-cli logs` command. The Logs command allows you to retrieve the Logs for a task. There are many options for the logs command:\n\n```\nOPTIONS:\n--task-id value            Print the logs for this ECS Task.\n--task-def value           [Optional] Specifies the name or full Amazon Resource Name (ARN) of the ECS Task Definition associated with the Task ID. This is only needed if the Task is using an inactive Task Definition.\n--follow                   [Optional] Specifies if the logs should be streamed.\n--filter-pattern value     [Optional] Substring to search for within the logs.\n--container-name value     [Optional] Prints the logs for the given container. Required if containers in the Task use different log groups\n--since value              [Optional] Returns logs newer than a relative duration in minutes. Cannot be used with --start-time (default: 0)\n--start-time value         [Optional] Returns logs after a specific date (format: RFC 3339. Example: 2006-01-02T15:04:05+07:00). Cannot be used with --since flag\n--end-time value           [Optional] Returns logs before a specific date (format: RFC 3339. Example: 2006-01-02T15:04:05+07:00). Cannot be used with --follow\n--timestamps, -t           [Optional] Shows timestamps on each line in the log output.\n```\n\n### Using FIPS Endpoints\nThe ECS-CLI supports using [FIPS endpoints](https://aws.amazon.com/compliance/fips/) for calls to ECR. To ensure you are accessing ECR using FIPS endpoints, use the `--use-fips` flag on the `push`, `pull`, or `images` command. FIPS endpoints are currently available in us-west-1, us-west-2, us-east-1, us-east-2, and in the [GovCloud partition](https://docs.aws.amazon.com/govcloud-us/latest/ug-west/using-govcloud-endpoints.html).\n\n```\n$ ecs-cli push myRepository:latest --use-fips --debug\nDEBU[0000] Using FIPS endpoint: https://ecr-fips.us-west-2.amazonaws.com\nINFO[0000] Getting AWS account ID...\nDEBU[0000] Getting authorization token...\nDEBU[0000] Checking file cache                           registry=xxxxxxxxxx123\nDEBU[0000] Calling ECR.GetAuthorizationToken             registry=xxxxxxxxxx123\nDEBU[0000] Saving credentials to file cache              registry=xxxxxxxxxx123\nDEBU[0000] Retrieved authorization token via endpoint: https://xxxxxxxxxxx123.dkr.ecr-fips.us-west-2.amazonaws.com\nINFO[0000] Tagging image                                 image=myRepository repository=xxxxxxxxxxx123.dkr.ecr-fips.us-west-2.amazonaws.com/myRepository tag=latest\nINFO[0000] Image tagged\nDEBU[0000] Check if repository exists                    repository=myRepository\nINFO[0000] Pushing image                                 repository=xxxxxxxxxxx123.dkr.ecr-fips.us-west-2.amazonaws.com/myRepository tag=latest\nINFO[0002] Image pushed\n```\n\n### Using Private Registry Authentication\n\nIf you want to use privately hosted container images with ECS, the ECS CLI can store your private registry credentials in AWS Secrets Manager and create an IAM role which ECS can use to access the credentials and private images. This allows you to:\n\n* Store private registry credentials within AWS for use with ECS\n* Add the permissions needed to use your registry secrets to a new or existing Task Execution Role\n* Automatically add your private registry credentials to your task definition when running a task or service\n\nUsing privately hosted images with the ECS CLI is done in two parts:\n\n1) Create new AWS Secrets Manager secrets and an IAM Task Execution Role with `ecs-cli registry-creds up`\n2) Run `ecs-cli compose` commands to create and run a task definition that includes the new resources\n\n#### Storing private registry credentials with `ecs-cli registry-creds up`\n\nTo get started, first create an input file that contains the name of your registry and the credentials needed to access it:\n\n```\n# file name: cred_input.yml\n# when using environment variables, only '${VAR_NAME}' format is supported\n\nversion: '1'\nregistry_credentials:\n  my-registry.example.com:\n    secrets_manager_arn:        # required when using (with no modification) or updating an existing secret\n    username: myUserName        # required when creating or updating a new secret\n    password: ${MY_PASSWORD}    # required when creating or updating a new secret\n    kms_key_id:                 # optional custom KMS Key ID to use to encrypt new secret\n    container_names:            # required to match credential resources with docker-compose services\n      - web\n      - log\n```\n\nIn this example, we're storing credentials for a registry called `my-registry.example.com` and passing in the password with an environment variable. `container_names` is a list of the `service_names` in your Docker Compose project which need access to images in this registry. If you don't plan to use the output of `registry-creds up` to launch a task or service with `compose`, then you can leave this field empty.\n\nOther options:\n* To store credentials for multiple private registries, add additional (up to 10 total) registry names and their required details as separate keys under `registry_credentials`.\n  * Existing registry secrets from other regions can be included by specifying their `secrets_manager_arn` and associated `kms_key_id`. Creating or updating secrets must be done from within that region.\n* If you want to encrypt the AWS Secrets Manager secret for your registry with a custom KMS Key, then add the ARN, ID or Alias of the Key in the `kms_key_id` field. Otherwise, AWS Secrets Manager will use the default key in your account.\n* If you don't want to create or update an IAM Task Execution Role for these secrets, use the `--no-role` flag instead of specifying a role name.\n* If you don't want to generate an output file for use with `compose` or for records purposes, use the `--no-output-file` flag.\n* If you want the output file to be created in a specific directory on your machine, you can specify it with the `--output-dir <value>` flag. Otherwise, the file will be created in your working directory.\n\nAfter creating the input file, run the `registry-creds up` command on the file and pass in the name of the new or existing Task Execution Role you want to use for the secrets:\n\n```\n$ ecs-cli registry-creds up ./cred_input.yml --role-name myTaskExecutionRole\n```\n\nThe command will output the names of the resources it creates, including the name of the output file which was generated:\n\n```\n$ ecs-cli registry-creds up ./cred_input.yml --role-name myTaskExecutionRole\nINFO[0000] Processing credentials for registry my-registry.example.com...\nINFO[0000] New credential secret created: arn:aws:secretsmanager:region:aws_account_id:secret:amazon-ecs-cli-setup-my-registry.example.com-VeDqXm\nINFO[0000] Creating resources for task execution role myTaskExecutionRole...\nINFO[0000] Created new task execution role arn:aws:iam::aws_account_id:role/myTaskExecutionRole\nINFO[0000] Created new task execution role policy arn:aws:iam::aws_account_id:policy/amazon-ecs-cli-setup-myTaskExecutionRole-policy-20181023T210805Z\nINFO[0000] Attached AWS managed policy arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy to role myTaskExecutionRole\nINFO[0001] Attached new policy arn:aws:iam::aws_account_id:policy/amazon-ecs-cli-setup-myTaskExecutionRole-policy-20181023T210805Z to role myTaskExecutionRole\nINFO[0001] Writing registry credential output to new file C:\\Users\\myuser\\regcreds\\regCredTest\\ecs-registry-creds_20181023T210805Z.yml\n```\n\nThe output file `ecs-registry-creds_20181023T210805Z.yml` should like like this:\n```\nversion: \"1\"\nregistry_credential_outputs:\n  task_execution_role: myTaskExecutionRole\n  container_credentials:\n    my-registry.example.com:\n      credentials_parameter: arn:aws:secretsmanager:region:aws_account_id:secret:amazon-ecs-cli-setup-my-registry.example.com-VeDqXm\n      container_names:\n      - web\n      - log\n```\n\nThis file contains:\n* the name of the IAM Task Execution Role with permissions for the new secrets\n* the ARN of the new `credentials_parameter` created for the registry\n* the list of containers the new `credentials_parameter` should be used for when running a task or service\n\nWe can now use this file with `ecs-cli compose` commands to start a task with images in our private registry.\n\n#### Using private registry credentials when launching tasks or services\n\nNow that we have an output file that identifies which resources we need to use our private registry, the ECS CLI will incorporate them into our Docker Compose project when we run `ecs-cli compose`.\n\nIn the same directory (let's call it \"privateImageApp\"), create a docker-compose.yml file for your application:\n\n```\nversion: \"3\"\nservices:\n  web:\n    environment:\n      - SERVICE_NAME=web\n    image: my-registry.example.com/httpd\n    ports:\n      - \"80:80\"\n  log:\n    environment:\n      - SERVICE_NAME=log\n    image: my-registry.example.com/logging\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: myApps\n        awslogs-region: us-west-2\n        awslogs-stream-prefix: privateImageApp\n```\n\nNow run the command `ecs-cli compose up` to launch a task. The ECS CLI will automatically detect and use the newest `ecs-registry-creds` file within the current directory:\n\n```\n$~\\privateImageApp> ecs-cli compose up\nINFO[0000] Found ecs-registry-creds file C:\\Users\\myuser\\regcreds\\regCredTest\\ecs-registry-creds_20181023T210805Z.yml\nINFO[0000] Using ecs-registry-creds value arn:aws:secretsmanager:region:aws_account_id:secret:amazon-ecs-cli-setup-my-registry.example.com-VeDqXm container name=web option name=credentials_parameter\nUsing ecs-registry-creds value arn:aws:secretsmanager:region:aws_account_id:secret:amazon-ecs-cli-setup-my-registry.example.com-VeDqXm container name=log option name=credentials_parameter\nINFO[0000] Using ecs-registry-creds value myTaskExecutionRole option name=task_execution_role\nINFO[0000] Using ECS task definition TaskDefinition=\"privateImageApp:1\"\nINFO[0000] Starting container... container=bf35a813-dd76-4fe0-b5a2-c1334c2331f4/web\nINFO[0000] Starting container... container=bf35a813-dd76-4fe0-b5a2-c1334c2331f4/log\nINFO[0012] Describe ECS container status container=bf35a813-dd76-4fe0-b5a2-c1334c2331f4/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=\"privateImageApp:1\"\nINFO[0013] Describe ECS container status container=bf35a813-dd76-4fe0-b5a2-c1334c2331f4/log desiredStatus=RUNNING lastStatus=PENDING taskDefinition=\"privateImageApp:1\"\nINFO[0018] Started container... container=bf35a813-dd76-4fe0-b5a2-c1334c2331f4/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=\"privateImageApp:1\"\nINFO[0018] Started container... container=bf35a813-dd76-4fe0-b5a2-c1334c2331f4/log desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=\"privateImageApp:1\"\n```\n\n The within your new task definition `privateImageApp:1`, the container definitions for both `web` and `log` should have your \"my-registry.example.com\" secret as a `credentialsParameter`. The `executionRoleArn` field will be the role we created in the previous step, \"myTaskExecutionRole\".\n\n Other options:\n * to use an ecs-registry-creds output file from outside the current directory, you can specify it in with the `--registry-creds <value>` flag\n\n For more information about using private registries with ECS, see [Private Registry Authentication for Tasks](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/private-auth.html).\n\n### Checking for Missing Attributes and Debugging Reason Attribute Errors\n\nSometimes, when you try to Run a Task, the API will return the error message `\"Reasons : [\"ATTRIBUTE\"]\"`. This occurs because your container instances are missing an attribute required by your Task Definition. You can debug these failures using the `ecs-cli check-attributes` command.\n\nHere's an example of the command in action:\n\n```\n$ ecs-cli check-attributes --container-instances 28c5abd2-360e-41a0-81d8-0afca2d08d9b,45510138-f24f-47c6-a418-71c46dd51f88,ae66e18e-1d46-47ff-81c5-647f0f1426ce,dffe7f91-8faa-4e00-983b-c58fd279cf6d --cluster practice-cluster --region us-east-2 --task-def fluentd-kinesis\nContainer Instance                    Missing Attributes\ndffe7f91-8faa-4e00-983b-c58fd279cf6d  None\n28c5abd2-360e-41a0-81d8-0afca2d08d9b  com.amazonaws.ecs.capability.logging-driver.fluentd\n45510138-f24f-47c6-a418-71c46dd51f88  None\nae66e18e-1d46-47ff-81c5-647f0f1426ce  com.amazonaws.ecs.capability.logging-driver.fluentd\n```\n\nThe command outputs a table of container instances and which attributes they are missing. In this case, the Task Definition requires the Fluentd log driver, but 2 container instances lack support for it.\n\n### Tagging Resources\n\nECS CLI Commmands support a `--tags` flag which allows you to specify AWS Resource Tags in the format `key=value,key2=value2,key3=value3`. Resource tags can be used for cost allocation, automation, access control, and more. See [AWS Tagging Strategies](https://aws.amazon.com/answers/account-management/aws-tagging-strategies/) for a discussion of use cases.\n\n#### ARN Formats\n\nECS has released [new longer ARN formats](https://aws.amazon.com/blogs/compute/migrating-your-amazon-ecs-deployment-to-the-new-arn-and-resource-id-format-2/). ***You must opt in to these new formats in order to tag Tasks, Services, and Container instances.*** We strongly recommend opting-in all IAM Identities in your account. You can use the [PutAccountSettingDefault](https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PutAccountSettingDefault.html) API to opt-in to the new format for all IAM Identities in your account.\n\n\n#### ecs-cli up command\n\n The ECS Cluster, and CloudFormation template with EC2 resources can be tagged. In addition, the ECS CLI will add tags to the following resources which are created by the CloudFormation template:\n * VPC\n * Subnets\n * Internet Gateway\n * Route Tables\n * Security Group\n * Autoscaling Group\n * ECS Container Instances (only if opted-in to [Container Instance Long ARN format](https://aws.amazon.com/blogs/compute/migrating-your-amazon-ecs-deployment-to-the-new-arn-and-resource-id-format-2/))\n\n For the autoscaling group, the ECS CLI will add a `Name` tag whose value will be `ECS Instance - <CloudFormation stack name>`, which will be propagated to your EC2 instances. You can override this behavior by specifying your own `Name` tag.\n\n#### ecs-cli compose create/up\n\nResource tags specified with `--tags` will be added to your Tasks and Task Definitions. In addition, [ECS Managed Tags](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html) are enabled by default for all tasks launched by the ECS CLI (if you are opted-in the the new Task Long ARN Format). ECS will automatically add a `aws:ecs:clusterName` tag to each of your tasks. You can disable this feature using `--disable-ecs-managed-tags`.\n\n#### ecs-cli compose service create/up\n\nResource tags specified with `--tags` will be added to your Service and Task Definitions. In addition, all Services created by the ECS CLI have [`propagateTags`](https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_CreateService.html#ECS-CreateService-request-propagateTags) set to `TASK_DEFINITION` which means that tags from the Task Definition will propagate to the tasks in the Service. If you add new tags, the ECS CLI will register a new Task Definition and these tags will be propagated by ECS to your tasks.\n\nSimilar to `compose up/create`, [ECS Managed Tags](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html) are enabled by default for all Services launched by the ECS CLI (if you are opted-in the the new Task Long ARN Format). ECS will automatically add `aws:ecs:clusterName` and `aws:ecs:serviceName` tags to each of the tasks launched by your service. You can disable this feature using `--disable-ecs-managed-tags`.\n\n#### ecs-cli push\n\nResource tags specified with `--tags` will be added to your ECR repository.\n\n#### ecs-cli registry-creds up\n\nResource tags specified with `--tags` will be added to new IAM Roles and new or existing AWS Secrets Manager Secrets. (Existing IAM Roles cannot be tagged.)\n\n### Running Tasks Locally\nThe ECS CLI supports creating, running, inspecting and stopping tasks defined by an ECS Task Definition through its `local` subcommands. You can run an ECS Task Definition specified in a local JSON file or pulled from a registered ECS Task Definition.\n\n#### ecs-cli local create\nIf you want to convert an ECS Task Definition to a Docker Compose file, you can run:\n\n```\n$ ecs-cli local create\n```\nWithout arguments, this will try to read an ECS Task Definition from local a file named `task-definition.json` located in the current directory and generate both a compose file, by default named `docker-compose.ecs-local.yml`, as well as a compose override file, by default named `docker-compose.ecs-local.override.yml`. This command is equivalent to a dry-run of `local up`.\n**NOTE** Using these Compose files as input to `ecs-cli compose` subcommands may not translate back to the same ECS Task Definition used as input to `local create`.\n\nTo run an ECS Task Definition specified in a different file, you can use the `--task-def-file` or `-f` flag with the name of the file.\nTo run an ECS Task Definition already registered with ECS, you can use the `--task-def-remote` or `-t` flag with the ARN or family name of the Task Definition.\nYou can also specify a different output file using the `--output` or `-o` flag.\nTo skip the overwrite confirmation prompt, use the `--force` flag.\n\n\n#### ecs-cli local up\nTo run an ECS Task Definition locally, you can run:\n\n```\n$ ecs-cli local up\n```\n\nThis command takes the same flags as `local create`. You can also specify compose override files using the `--override` flag.\n\nThis command will also create the local end [Amazon ECS Local Endpoints Container](https://github.com/awslabs/amazon-ecs-local-container-endpoints) and the network, `ecs-local-network` that your containers will be run in.\n\n\n#### ecs-cli local ps\nOnce you have your task running locally, the basic command to list your task's containers is:\n ```\n$ ecs-cli local ps\n```\nThis will search for containers created from the `./task-definition.json` file (to see all available options, run `ecs-cli local ps --help`).\n\nFor example, if you'd like to list containers created from a specific task definition file, use the following command:\n```\n$ ecs-cli local ps -f ./app-task-definition.json\nCONTAINER ID        IMAGE               STATUS              PORTS               NAMES                 TASKDEFINITION\n84ff8e68e613        nginx               Up 15 seconds                           /local-cmds_nginx_1   /path/to/app-task-definition.json\n```\n\n#### ecs-cli local down\nIf you want to stop and remove a task's containers, you can run:\n```\n$ ecs-cli local down\n```\nThis will stop and remove all the containers started from the `./task-definition.json` file  (to see all available options, run `ecs-cli local down --help`).\n\nFor example, you can stop and remove all tasks running locally using the `--all` flag:\n```\n$ ecs-cli local down --all\nINFO[0000] Searching for all running containers\nINFO[0000] Stop and remove 1 container(s)\nINFO[0000] Stopped container with id 84ff8e68e613\nINFO[0000] Removed container with id 84ff8e68e613\nINFO[0000] The network ecs-local-network has no more running tasks\nINFO[0001] Stopped container with name amazon-ecs-local-container-endpoints\nINFO[0001] Removed container with name amazon-ecs-local-container-endpoints\nINFO[0001] Removed network with name ecs-local-network\n```\n\nIf you have no more tasks running, then this command will also stop and remove the [Amazon ECS Local Container Endpoints](https://github.com/awslabs/amazon-ecs-local-container-endpoints)\nand finally remove the `ecs-local-network` as well.\n\n## Amazon ECS CLI Commands\n\nFor a complete list of commands, see the\n[Amazon ECS CLI documentation](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI.html).\n\n## Contributing to the CLI\nContributions and feedback are welcome! Proposals and pull requests will be considered and responded to.\nFor more information, see the [CONTRIBUTING.md](https://github.com/aws/amazon-ecs-cli/blob/master/CONTRIBUTING.md) file.\n\nAmazon Web Services does not currently provide support for modified copies of\nthis software.\n\n## License\n\nThe Amazon ECS CLI is distributed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).\nSee [LICENSE](https://github.com/aws/amazon-ecs-cli/blob/master/LICENSE) and [NOTICE](https://github.com/aws/amazon-ecs-cli/blob/master/NOTICE) for more information.\n", "release_dates": ["2020-12-21T22:48:37Z", "2020-07-07T22:31:25Z", "2020-05-27T23:34:13Z", "2020-05-26T23:27:31Z", "2020-02-28T19:48:46Z", "2019-11-19T00:34:49Z", "2019-09-03T22:35:28Z", "2019-07-10T23:15:03Z", "2019-07-10T00:13:22Z", "2019-04-24T20:31:48Z", "2019-04-04T19:03:41Z", "2019-03-12T20:10:24Z", "2019-03-07T19:12:20Z", "2018-12-13T19:13:54Z", "2018-11-20T22:38:12Z", "2018-11-20T01:08:16Z", "2018-11-20T00:12:42Z", "2018-10-25T23:19:53Z", "2018-10-18T21:42:08Z", "2018-09-07T18:33:55Z", "2018-07-18T19:11:44Z", "2018-06-05T23:15:43Z", "2018-04-19T18:14:41Z", "2018-03-20T21:11:07Z", "2018-03-15T22:51:20Z", "2018-03-09T18:33:28Z", "2018-01-19T21:14:40Z", "2018-01-10T01:32:49Z", "2018-01-05T23:55:45Z", "2017-12-19T23:52:53Z"]}, {"name": "amazon-ecs-cluster-state-service", "description": "Materialized local view of your ECS cluster state built on top of the Amazon ECS event stream.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# amazon-ecs-cluster-state-service\n\n### Description\n\nThe amazon-ecs-cluster-state-service consumes events from a stream of all changes to containers and instances across your Amazon ECS clusters, persists the events in a local data store, and provides APIs (e.g., search, filter, list, etc.) that enable you to query the state of your cluster so you can respond to changes in real-time. The amazon-ecs-cluster-state-service utilizes etcd as the data store to track your Amazon ECS cluster state locally, and it also manages any drift in state by periodically reconciling state with Amazon ECS.  \n\n### REST API\n\nThe amazon-ecs-cluster-state-service API operations:  \n*\tLists and describes container instances and tasks\n*\tFilters container instances and tasks by status or cluster\n*\tListens to streaming container instance and task state changes\n\n### Building amazon-ecs-cluster-state-service\n\nThe amazon-ecs-cluster-state-service depends on golang and go-swagger. Install and configure [golang](https://golang.org/doc/). For more information about installing go-swagger, see the [go-swagger documentation](https://github.com/go-swagger/go-swagger).\n\n```\n$ git clone https://github.com/aws/amazon-ecs-cluster-state-service.git\n$ cd aws/amazon-ecs-cluster-state-service\n$ make get-deps\n$ make\n\n# Find the cluster-state-service binary in 'out' folder\n$ ls out/\nLICENSE                 amazon-ecs-cluster-state-service\n\n```\n\n### Usage\n\nWe provide an AWS CloudFormation template to set up the necessary prerequisites for the amazon-ecs-cluster-state-service. After the prerequisites are ready, you can launch the amazon-ecs-cluster-state-service via the Docker compose file, if you prefer. For more information, see the the [Deployment Guide](deploy).\n\nTo launch the amazon-ecs-cluster-state-service manually, use the following steps.\n\n#### Prerequisites\n\nIn order to use the amazon-ecs-cluster-state-service, you need to set up an Amazon SQS queue, configure CloudWatch Events, and add the queue as a target for ECS events.\n\nThe amazon-ecs-cluster-state-service also depends on etcd to store the cluster state locally. To set up etcd manually, see the [etcd documentation](https://github.com/coreos/etcd).\n\n#### Quick Start - Launching the amazon-ecs-cluster-state-service\nThe amazon-ecs-cluster-state-service is provided as a Docker image for your convenience. You can launch it with the following code. Use appropriate values for AWS_REGION, etcd IP, and port and queue names.\n\n```\ndocker run -e AWS_REGION=us-west-2 \\\n    AWS_PROFILE=default \\\n    -v ~/.aws:/.aws \\\n    -v /tmp/css-logs:/var/output/logs \\\n    amazon-ecs-cluster-state-service:0.3.0 \\\n    --etcd-endpoint $ETCD_IP:$ETCD_PORT \\\n    --queue_name $SQS_QUEUE_NAME\n```\n\nYou can also override the logger configuration like the log file and log level.\n\n```\ndocker run -e AWS_REGION=us-west-2 \\\nAWS_PROFILE=default \\\n    CSS_LOG_FILE=/var/output/logs/css.log \\\n    CSS_LOG_LEVEL=info \\\n    -v ~/.aws:/.aws \\\n    -v /tmp/css-logs:/var/output/logs \\\n    amazon-ecs-cluster-state-service:0.3.0 \\\n    --etcd-endpoint $ETCD_IP:$ETCD_PORT \\\n    --queue event_stream\n```\n\n#### API endpoint\n\nAfter you launch the amazon-ecs-cluster-state-service, you can interact with and use the REST API by using the endpoint at port 3000. Identify the amazon-ecs-cluster-state-service container IP address and connect to port 3000. For more information about the API definitions, see the [swagger specification](swagger/v1/swagger.json).\n\n### Contributing\n\namazon-ecs-cluster-state-service is released under Apache 2.0 and the usual Apache Contributor Agreements apply for individual contributors. All projects are maintained in public on GitHub, issues and pull requests use GitHub. We look forward to collaborating with the community.\n", "release_dates": []}, {"name": "amazon-ecs-cni-plugins", "description": "Networking Plugins repository for ECS Task Networking", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon ECS CNI Plugins\n\n[![Build Status](https://travis-ci.org/aws/amazon-ecs-cni-plugins.svg?branch=master)](https://travis-ci.org/aws/amazon-ecs-cni-plugins)\n## Description\n\nAmazon ECS CNI Plugins is a collection of Container Network Interface([CNI](https://github.com/containernetworking/cni)) Plugins used by the [Amazon ECS Agent](https://github.com/aws/amazon-ecs-agent) to configure network namespace of containers with Elastic Network Interfaces ([ENIs](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html))\n\nFor more information about Amazon ECS, see the [Amazon ECS Developer Guide](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html).\n\nFor more information about Plugins in this project, see the individual READMEs.\n\n## Plugins\n* [ECS ENI Plugin](plugins/eni/README.md): configures the network namespace of the container with an ENI device\n* [ECS Bridge Plugin](plugins/ecs-bridge/README.md): configures the network namespace of the container to be able to communicate with the credentials endpoint of the ECS Agent\n* [ECS IPAM Plugin](plugins/ipam/README.md): allocates an IP address and constructs Gateway and Route structures used by the ECS Bridge plugin to configure the bridge and veth pair in the container network namespace\n\n## Security disclosures\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n", "release_dates": []}, {"name": "amazon-ecs-init", "description": "Amazon Elastic Container Service RPM", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon Elastic Container Service RPM\n\n[![Build Status](https://travis-ci.org/aws/amazon-ecs-init.svg?branch=master)](https://travis-ci.org/aws/amazon-ecs-init)\n\nThe Amazon Elastic Container Service RPM is software developed to support the [Amazon ECS Container\nAgent](http://github.com/aws/amazon-ecs-agent).  The Amazon ECS RPM is packaged for RPM-based systems that utilize\n[Upstart](http://upstart.ubuntu.com) as the init system.\n\n## Behavior\nThe upstart script installed by the Amazon ECS RPM runs at the completion of runlevel 3, 4, or 5 as the system starts.\nThe script will clean up any previous copies of the Amazon ECS Container Agent, and then start a new copy.  Logs from\nthe RPM are available at `/var/log/ecs/ecs-init.log`, while logs from the Amazon ECS Container Agent are available at\n`/var/log/ecs/ecs-agent.log`.  The Amazon ECS RPM makes the Amazon ECS Container Agent introspection endpoint available\nat `http://127.0.0.1:51678/v1`.  Configuration for the Amazon ECS Container Agent is read from `/etc/ecs/ecs.config`.\nAll of the configurations in this file are used as environment variables of the ECS Agent container. Additionally, some\nconfigurations can be used to configure other properties of the ECS Agent container, as described below.\n\n| Configuration Key | Example Value(s)            | Description | Default value |\n|:----------------|:----------------------------|:------------|:-----------------------|\n| `ECS_AGENT_LABELS` | `{\"test.label.1\":\"value1\",\"test.label.2\":\"value2\"}` | The labels to add to the ECS Agent container. | |\n\nAdditionally, the following environment variable(s) can be used to configure the behavior of the RPM:\n\n| Environment Variable Name | Example Value(s)            | Description | Default value |\n|:----------------|:----------------------------|:------------|:-----------------------|\n| `ECS_SKIP_LOCALHOST_TRAFFIC_FILTER` | &lt;true &#124; false&gt; | By default, the ecs-init service adds an iptable rule to drop non-local packets to localhost if they're not part of an existing forwarded connection or DNAT, and removes the rule upon stop. If `ECS_SKIP_LOCALHOST_TRAFFIC_FILTER` is set to true, this rule will not be added/removed. | false |\n| `ECS_ALLOW_OFFHOST_INTROSPECTION_ACCESS` | &lt;true &#124; false&gt; | By default, the ecs-init service adds an iptable rule to block access to ECS Agent's introspection port from off-host (or containers in awsvpc network mode), and removes the rule upon stop. If `ECS_ALLOW_OFFHOST_INTROSPECTION_ACCESS` is set to true, this rule will not be added/removed. | false |\n| `ECS_OFFHOST_INTROSPECTION_INTERFACE_NAME` | `eth0` | Primary network interface name to be used for blocking offhost agent introspection port access. By default, this value is the interface that handles the default route (`0.0.0.0/0`) in kernel routing table (`/proc/net/route`). If none could be found, we fall back to `eth0` | - (Resolved at runtime) |\n\nThe above environment variable(s) can be used in the following way\n- On Amazon Linux 1, the flag `ECS_SKIP_LOCALHOST_TRAFFIC_FILTER` can be turned on by adding `env ECS_SKIP_LOCALHOST_TRAFFIC_FILTER=true` to /etc/init/ecs.conf.\n- On Amazon Linux 2, the flag `ECS_SKIP_LOCALHOST_TRAFFIC_FILTER` can be turned on by adding `ECS_SKIP_LOCALHOST_TRAFFIC_FILTER=true` to /etc/ecs/ecs.config.\n\n## Usage\nThe upstart script installed by the Amazon Elastic Container Service RPM can be started or stopped with the following commands respectively:\n\n* `sudo start ecs`\n* `sudo stop ecs`\n\n### Updates\nUpdates to the Amazon ECS Container Agent should be performed through the Amazon ECS Container Agent.  In the case where\nan update failed and the Amazon ECS Container Agent is no longer functional, a rollback can be initiated as follows:\n\n1. `sudo stop ecs`\n2. `sudo /usr/libexec/amazon-ecs-init reload-cache`\n3. `sudo start ecs`\n\n## Security disclosures\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n## Development\n\n#### Building the RPM for test\n\nOn your local machine, you can use the docker target to generate an rpm:\n\n```\nmake rpm-in-docker\n```\n\nThis rpm can then be installed in an amazon linux ami:\n\n```\n# send rpm either through s3 or scp\nrpm -i rpm-that-you-built.rpm\nsudo systemctl enable ecs\nsudo systemctl start ecs\n```\n\n#### Dev dependencies\n\nRun `make get-deps` to get dependencies for running tests and generating mocks.\n\n#### Generating mocks\n\nMocks can be generated using the `make generate` Makefile target. **NOTE** that this must be run on a linux machine.\n\n## License\n\nThe Amazon Elastic Container Service RPM is licensed under the Apache 2.0 License.\n", "release_dates": ["2022-10-07T16:56:09Z", "2022-09-14T19:20:05Z", "2022-09-14T19:20:32Z", "2022-08-23T14:10:53Z", "2022-08-04T17:00:19Z", "2022-07-29T17:13:22Z", "2022-06-24T18:48:52Z", "2022-06-03T22:46:39Z", "2022-05-05T17:54:29Z", "2022-04-08T01:13:03Z", "2022-03-28T16:24:58Z", "2022-03-04T19:17:49Z", "2022-02-09T18:58:02Z", "2022-01-20T23:35:26Z", "2021-12-09T18:56:48Z", "2021-11-05T17:37:56Z", "2021-10-25T23:37:53Z", "2021-10-15T19:30:15Z", "2021-10-07T00:35:20Z", "2021-09-22T21:45:59Z", "2021-09-08T15:04:10Z", "2021-08-24T03:34:47Z", "2021-08-12T04:49:19Z", "2021-07-27T18:29:31Z", "2021-07-12T17:52:35Z", "2021-06-28T23:46:37Z", "2021-06-11T22:02:12Z", "2021-05-26T03:57:50Z", "2021-05-21T22:40:45Z", "2021-05-15T19:14:34Z"]}, {"name": "amazon-ecs-logs-collector", "description": "The script will be used to collect general os logs as well as Docker and ecs-agent logs, it also support to enable debug mode for docker and ecs-agent in Amazon Linux. ", "language": "Shell", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ECS Logs Collector\n\nThis project was created to collect [Amazon ECS](https://aws.amazon.com/ecs) log files and Operating System log files for troubleshooting Amazon ECS customer support cases.\n\nThe following functions are supported:\n\n* Collect Operating System logs\n* Collect Operating System settings\n* Collect Docker logs\n* Collect Amazon ECS agent Logs\n* Enable debug mode for Docker and the Amazon ECS agent (only available for Systemd init systems and Amazon Linux)\n* Create a tar zip file in the same folder as the script\n\n## Usage\nRun this project as the root user:\n\n```\ncurl -O https://raw.githubusercontent.com/aws/amazon-ecs-logs-collector/master/ecs-logs-collector.sh\nbash ecs-logs-collector.sh\n```\n\nConfirm if the tarball file was successfully created (it can be .tgz or .tar.gz)\n\n```\n# ls collect*\ncollect-i-ffffffffffffffffff-YYYYMMDDHHmm.tgz\n\ncollect:\ni-ffffffffffffffffff\n```\n### Retrieving the logs\n\nDownload the tarball using your favourite Secure Copy tool.\n\n## Example output\nThe project can be used in normal or enable-debug mode. Enable debug is only available for Systemd init systems and Amazon Linux.\n\n```\n# bash ecs-logs-collector.sh --help\nUSAGE: ./ecs-logs-collector.sh [--mode=[brief|enable-debug]]\n       ./ecs-logs-collector.sh --help\n\nOPTIONS:\n     --mode  Sets the desired mode of the script. For more information,\n             see the MODES section.\n     --help  Show this help message.\n\nMODES:\n     brief         Gathers basic operating system, Docker daemon, and Amazon\n                   ECS Container Agent logs. This is the default mode.\n     enable-debug  Enables debug mode for the Docker daemon and the Amazon\n                   ECS Container Agent. Only supported on Systemd init systems\n                   and Amazon Linux.\n```\n\n### Example output in normal mode\nThe following output shows this project running in normal mode.\n\n```\n# bash ecs-logs-collector.sh\nTrying to check if the script is running as root ... ok\nTrying to resolve instance-id ... ok\nTrying to collect system information ... ok\nTrying to check disk space usage ... ok\nTrying to collect common operating system logs ... ok\nTrying to collect kernel logs ... ok\nTrying to get mount points and volume information ... ok\nTrying to check SELinux status ... ok\nTrying to get iptables list ... ok\nTrying to detect installed packages ... ok\nTrying to detect active system services list ... ok\nTrying to gather Docker daemon information ... ok\nTrying to inspect all Docker containers ... ok\nTrying to collect Docker and containerd daemon logs ... ok\nTrying to collect Docker systemd unit file ... ok\nTrying to collect containerd systemd unit file ... ok\nTrying to collect Docker sysconfig ... ok\nTrying to collect Docker storage sysconfig ... ok\nTrying to collect Docker daemon.json ... ok\nTrying to collect Amazon ECS Container Agent logs ... ok\nTrying to collect Amazon ECS Container Agent state and config ... ok\nTrying to collect Amazon ECS Container Agent engine data ... ok\nTrying to get open files list ... ok\nTrying to collect /etc/os-release ... ok\nTrying to get uname kernel info ... ok\nTrying to get dmidecode info ... ok\nTrying to get lsmod info ... ok\nTrying to collect systemd slice info ... ok\nTrying to get veth info ... ok\nTrying to get gpu info ... ok\nTrying to archive gathered log information ... ok\n```\n\n### Example output in enable-debug mode\nThe following output shows this project enabling debug mode for the Docker daemon and the Amazon ECS Container Agent. This mode only works on Amazon Linux OS and Systemd init systems such as RHEL 7 and Ubuntu 16.04. Note that enable-debug mode restarts Docker and the Amazon ECS agent.\n\n```\n# bash ecs-logs-collector.sh --mode=enable-debug\nTrying to check if the script is running as root ... ok\nTrying to collect system information ... ok\nTrying to enable debug mode for the Docker daemon ... ok\nTrying to restart Docker daemon to enable debug mode ... ok\nTrying to enable debug mode for the Amazon ECS Container Agent ... ok\nTrying to restart the Amazon ECS Container Agent to enable debug mode ... ok\n```\n\n## Contributing\n\nPlease [create a new GitHub issue](https://github.com/awslabs/ecs-logs-collector/issues/new) for any feature requests, bugs, or documentation improvements.\n\nWhere possible, [submit a pull request](https://help.github.com/articles/creating-a-pull-request-from-a-fork/) for the change.\n\n## License\n\nCopyright 2011-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at\n\n[http://aws.amazon.com/apache2.0/](http://aws.amazon.com/apache2.0/)\n\nor in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n", "release_dates": []}, {"name": "amazon-ecs-service-connect-agent", "description": "Amazon ECS Service Connect Agent", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon ECS Service Connect Agent\n\n![Amazon ECS logo](doc/ecs.png \"Amazon AWS ECS\")\n\nThe Amazon ECS Service Connect Agent is a primary component of [Amazon ECS Service Connect](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-connect.html) and [AWS App Mesh](https://aws.amazon.com/app-mesh/). It monitors the Envoy proxy and provides a management interface. This management interface serves as a safe endpoint to interact with the Envoy proxy and provides several APIs for health checks, telemetry data and summarizes the operating condition of the proxy. It is used in both ECS Service Connect proxy and App Mesh [Envoy Docker Image](https://docs.aws.amazon.com/app-mesh/latest/userguide/envoy.html).\n\n\n## Building the Agent\n\nOn an [Amazon Linux AMI](https://aws.amazon.com/amazon-linux-ami/). Download Go at https://go.dev/doc/install. In the project's `agent` directory, issue the `make` command to compile the agent binary:\n\n```\n$ yum -y install docker\n$ yum -y groupinstall \"Development Tools\"\n$ make\ngo fmt ./...\ngo test -mod=vendor -count=1 -v ./...\n...\n$ ls -laF agent\n-rwxrwxr-x 1 ec2-user ec2-user 21192704 Feb  1 18:40 agent*\n```\n\n\nTo create an Envoy image, you could use the following example files:\n\nExample `Dockerfile.agent`:\n\n```\nCOPY agent /usr/bin/agent\n\nCMD /usr/bin/agent\n```\n\nExample `Makefile`:\n\n```\nAWS_REGION ?= us-west-2\nSSM_PARAMETER = \"/aws/service/appmesh/envoy\"\nIMAGE_STRING = $(shell aws --region $(AWS_REGION) ssm get-parameter --name $(SSM_PARAMETER) --query Parameter.Value)\nECR = $(shell echo $(IMAGE_STRING) | cut -d / -f 1)\nIMAGE_NAME ?= \"ecs-service-connect-agent:latest\"\n\n.PHONY: docker-build\ndocker-build:\n        ECR=$(shell echo $(IMAGE_STRING) | sed 's/\\(.*\\):\\(.*\\)/\\1/g')\n        echo \"FROM $(IMAGE_STRING)\" > source\n        cat source Dockerfile.agent > Dockerfile\n\n        aws ecr get-login-password | docker login --password-stdin --username AWS $(ECR)\n        docker build -t $(IMAGE_NAME) .\n        rm source Dockerfile\n```\n\nPlace these files along with the built `agent` binary in a single directory and issue the `make docker-build` command. The resulting `ecs-service-connect:latest` can be used in ECS Service Connect or App Mesh as a sidecar.\n\n## Advanced Usage\n\nThe Amazon ECS Service Connect Agent supports using a few environment variables to alter some aspects of the Envoy's behavior. These variables are outlined below, and documented in the AWS App Mesh [User Guide](https://docs.aws.amazon.com/app-mesh/latest/userguide/envoy-config.html). These environment variables can be configured when used with AWS App Mesh, and they are not configurable when used with ECS Service Connect.\n\n\n\n\n**Required Variables**\n\n|Environment Key\t|Example Value(s)\t|Description\t|Default Value\t|\n|---\t|---\t|---\t|---\t|\n|`APPMESH_RESOURCE_ARN`\t|\t|When you add the Envoy container to a task group, set this environment variable to the ARN of the virtual node or the virtual gateway that the task group represents\t|\t|\n\n\n**Envoy Bootstrap Environment Variables**\n\nThese environment variables offer controls for the bootstrap config generation for Envoy when it's started.\n\n|Environment Key\t|Example Value(s)\t|Description\t|Default Value\t|\n|---\t|---\t|---\t|---\t|\n|`ENVOY_ADMIN_MODE`\t| <tcp &#124; uds>\t| Specify whether to bind Envoy's admin interface to a tcp address or a unix socket.\t| tcp\t|\n|`ENVOY_ADMIN_ACCESS_LOG_FILE`\t|/path/to/access.log\t|Log file for the Envoy admin access service\t|/var/log/envoy_admin_access.log\t|\n|`ENVOY_ADMIN_ACCESS_PORT`\t|1234\t|Port where Envoy admin access is reachable and also to override the default port through which Amazon ECS Service Connect Agent is connecting to envoy \t|9901\t|\n|`ENVOY_ADMIN_ACCESS_ENABLE_IPV6`\t|<true &#124; false>\t|Determines if the Envoy will listen for IPv6 traffic on the admin interface \t|false\t|\n|`ENVOY_LOG_LEVEL`\t|<info &#124; warn &#124; error &#124; debug &#124; trace>\t|Envoy Log Level\t|info\t|\n|`ENVOY_INITIAL_FETCH_TIMEOUT`\t|\t|Length of time Envoy will wait for an initial config response\t|0\t|\n|`ENVOY_CONCURRENCY`  | 2 | number of concurrent processes for Envoy |-1 |\n|`ENABLE_ENVOY_STATS_TAGS`\t|<0 &#124; 1>\t|Enables the use of App Mesh defined tags `appmesh.mesh` and `appmesh.virtual_node`. For more information, see [config.metrics.v3.TagSpecifier](https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/metrics/v3/stats.proto#config-metrics-v3-tagspecifier) in the Envoy documentation. To enable, set the value to 1. |   |\n|`ENVOY_STATS_FLUSH_INTERVAL`  | 5000ms | Sets optional duration between flushes to configured stats sinks. (unit: Duration) | 5000ms |\n|`ENVOY_STATS_CONFIG_FILE`\t|\t|Stats config file (see: https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/observability/statistics).\t|\t|\n|`ENVOY_STATS_SINKS_CFG_FILE`\t|\t|Specify a file path in the Envoy container file system to override the default configuration with your own. For more information, see [config.metrics.v3.StatsSink](https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/metrics/v3/stats.proto#config-metrics-v3-statssink) in the Envoy documentation.\t|\t|\n|`ENABLE_ENVOY_DOG_STATSD`\t|<0 &#124; 1>\t|Enables DogStatsD stats using `127.0.0.1:8125` as the default daemon endpoint\t|0\t|\n|`STATSD_PORT`\t|1234\t|Specify a port value to override the default DogStatsD daemon port\t|8125\t|\n|`STATSD_ADDRESS`\t|127.0.0.1\t|Specify an IP address value to override the default DogStatsD daemon IP address.  This variable can only be used with version `1.15.0` or later of the Envoy image.\t|127.0.0.1\t|\n|`STATSD_SOCKET_PATH`\t|/path/to/socket\t|Specify a unix domain socket for the DogStatsD daemon. If this variable is not specified, and if DogStatsD is enabled, then this value defaults to the DogStatsD daemon IP address port of `127.0.0.1:8125`. If the `ENVOY_STATS_SINKS_CFG_FILE` variable is specified containing a stats sinks configuration, it will override all of the DogStatsD variables. This variable is supported with Envoy image version `v1.19.1.0-prod` or late\t|\t|\n|`APPMESH_METRIC_EXTENSION_VERSION`\t|<0 &#124; 1>\t|Enables the App Mesh metrics extension\t|\t|\n|`ENABLE_ENVOY_XRAY_TRACING`\t|<0 &#124; 1>\t|Enables X-Ray tracing using 127.0.0.1:2000 as the default daemon endpoint\t|\t|\n|`XRAY_DAEMON_PORT`\t|1234\t|Overrides the X-Ray daemon port\t|2000\t|\n|`XRAY_SAMPLING_RATE`\t|0.0 - 1.00\t|Override the default sampling rate of 0.05 (5%) for AWS X-Ray tracer. The value should be specified as a decimal between 0 and 1.00 (100%). This will be overridden if `XRAY_SAMPLING_RULE_MANIFEST` is specified\t|\t|\n|`XRAY_SAMPLING_RULE_MANIFEST`\t|/path/to/ruleset\t|Specify a file path in the Envoy container file system to configure the localized custom sampling rules for the X-Ray tracer. For more information, see [Sampling rules](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-go-configuration.html#xray-sdk-go-configuration-sampling) in the *AWS X-Ray Developer Guide*\t|\t|\n|`XRAY_SEGMENT_NAME`\t|\u201cmesh/resourceName\u201d\t|Specify a segment name for traces to override the default X-Ray segment name. This variable is supported with Envoy image version `v1.23.0.0-prod` or later.\t|`meshName`/`virtualNodeName`\t|\n|`AWS_XRAY_DAEMON_ADDRESS`  | **Same port** \u2013 `address:port`; **Different ports** \u2013 `tcp:address:port udp:address:port` | Set the host and port of the X-Ray daemon listener. Use this variable if you have configured the daemon to [listen on a different port](https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html) or if it is running on a different host. | By default, the SDK uses `127.0.0.1:2000` for both trace data (UDP) and sampling (TCP) |\n|`ENABLE_ENVOY_DATADOG_TRACING`\t|<0 &#124; 1>\t|Enables Datadog trace collection using `127.0.0.1:8126` as the default Datadog agent endpoint. To enable, set the value to `1`\t|0\t|\n|`DATADOG_TRACER_PORT`\t|1234\t|Specify a port value to override the default Datadog agent port\t|8126\t|\n|`DATADOG_TRACER_ADDRESS`\t|127.0.0.1\t|Specify an IP address to override the default Datadog agent address\t|127.0.0.1\t|\n|`DD_SERVICE`\t|\u201cmesh/resourceName\u201d\t|Specify a service name for traces to override the default Datadog service name. This variable is supported with Envoy image version `v1.18.3.0-prod` or later.\t|`envoy-meshName`/`virtualNodeName`\t|\n|`ENABLE_ENVOY_JAEGER_TRACING`\t|<0 &#124; 1>\t|Enables Jaeger trace collection using `127.0.0.1:9411` as the default Jaeger endpoint\t|0\t|\n|`JAEGER_TRACER_PORT`\t|1234\t|Specify a port value to override the default Jaeger port\t|9411\t|\n|`JAEGER_TRACER_ADDRESS`\t|127.0.0.1\t|Specify an IP address to override the default Jaeger address\t|127.0.0.1\t|\n|`JAEGER_TRACER_VERSION`\t|<PROTO &#124; JSON>\t|Specify whether the collector needs traces in `JSON` or `PROTO` endoded format\t|PROTO\t|\n|`ENVOY_TRACING_CFG_FILE`\t|\t|Tracing configuration file (see: https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/observability/tracing).\t|\t|\n|`ENVOY_CONFIG_FILE`\t|/usr/local/etc/envoy.yaml\t|Location of an alternative Envoy configuration file. If it is provided, a full and valid config file must be provided to the container. If this is not provided the Agent generates the config file.\t|\t|\n|`ENVOY_RESOURCES_CONFIG_FILE`\t|/usr/local/etc/resources.yaml\t|Location for providing additional resources to be applied on the bootstrap configuration file.  If this is specified the Agent will concatenate the resources with the default resources that are generated.\t|\t|\n|`APPMESH_RESOURCE_CLUSTER`\t|\t|By default App Mesh uses the name of the resource you specified in `APPMESH_RESOURCE_ARN` when Envoy is referring to itself in metrics and traces. You can override this behavior by setting the `APPMESH_RESOURCE_CLUSTER` environment variable with your own name. This variable can only be used with version `1.15.0` or later of the Envoy image.\t|\t|\n|`APPMESH_XDS_ENDPOINT`\t|hostname.aws:1234\t|Envoy's configuration endpoint with port\t| `appmesh-envoy-management.$AWS_REGION.amazonaws.com:443`\t|\n|`APPMESH_SIGNING_NAME`  | appmesh | The service signing name for Aws request signing filter. | appmesh |\n|`APPMESH_SET_TRACING_DECISION`\t|<true &#124; false>\t|Controls whether Envoy modifies the `x-request-id` header appearing in a request from a client\t|TRUE\t|\n|`ENVOY_NO_EXTENSION_LOOKUP_BY_NAME`\t|<true &#124; false>\t|Controls whether Envoy needs type URL to lookup extensions regardless of the name field. If the type URL is missing it will reject (NACK) the configuration\t|true\t|\n|`ENVOY_ENABLE_TCP_POOL_IDLE_TIMEOUT`\t|<true &#124; false>\t|Controls whether the `idle_timeout` protocol options feature is enabled for TCP upstreams. If not configured the default `idle_timeout` is 10 minutes. Set this environment variable to `false` to disable `idle_timeout` option.\t|true\t|\n|`ENVOY_USE_HTTP_CLIENT_TO_FETCH_AWS_CREDENTIALS`\t|<true &#124; false>\t|Controls whether to use http async client to fetch AWS credentials in Envoy from metadata credentials providers instead of libcurl. The usage of libcurl is deprecated in Envoy\t|false\t|\n|`MAX_REQUESTS_PER_IO_CYCLE`\t|1\t|For setting the limit on the number of HTTP requests processed from a single connection in a single I/O cycle. Requests over this limit are processed in subsequent I/O cycles. This mitigates CPU starvation by connections that simultaneously send high number of requests by allowing requests from other connections to make progress. This runtime value can be set to 1 in the presence of abusive HTTP/2 or HTTP/3 connections. By default this is not set.\t|\t|\n|`APPMESH_SDS_SOCKET_PATH`\t|/path/to/socket\t|Unix Domain Socket for SDS Based TLS.\t|\t|\n|`APPMESH_PREVIEW`\t|<0 &#124; 1>\t|Enables the App Mesh Preview Endpoint\t|\t|\n|`APPMESH_DUALSTACK_ENDPOINT`\t|<0 &#124; 1>\t|Enables the App Mesh Dual-Stack Endpoint\t|\t|\n|`APPMESH_PLATFORM_K8S_VERSION`\t|\u201cv1.21.2\u201d\t|For Envoy running on K8s, K8s platform version injected by App Mesh Controller\t|\t|\n|`APPMESH_PLATFORM_APP_MESH_CONTROLLER_VERSION`\t|\"v1.4.1\"\t|For Envoy running on K8s, app mesh controller version injected by App Mesh Controller\t|\t|\n\n**Agent Sidecar Operation Environment Variables**\n\nThese environment variables offer controls to alter Amazon ECS Service Connect Agent functionality acting as a process manager for Envoy and serving useful APIs via a management interface.\n\n|Environment Key\t|Example Value(s)\t|Description\t|Default Value\t|\n|---\t|---\t|---\t|---\t|\n|`APPNET_ENVOY_RESTART_COUNT`\t|10\t|The number of times the Agent will restart Envoy within a running task\t|0\t|\n|`PID_POLL_INTERVAL_MS`\t|25\t|The interval at which the Envoy process\u2019 state is checked\t|100\t|\n|`LISTENER_DRAIN_WAIT_TIME_S`\t|1\t|Controls the time Envoy waits for active connections to gracefully close before the process exits\t|20\t|\n|`APPNET_AGENT_ADMIN_MODE`  | <tcp &#124; uds> | Starts Agent's management interface server and binds it to either a tcp address or a unix socket. |  |\n|`APPNET_AGENT_HTTP_PORT`  |1234  |Specify a port to be used for binding Agent's management interface in tcp mode. Ensure port value is > 1024 if uid != 0.  Ensure port is less than 65535 | 9902  |\n|`APPNET_AGENT_HTTP_BIND_ADDRESS` |127.0.0.1\t|Specify an IP address to override the default Amazon ECS Service Connect Agent Management interface address in tcp mode.\t|[::]\t|\n|`APPNET_AGENT_ADMIN_UDS_PATH` |/path/to/socket |Specify unix domain socket path for Amazon ECS Service Connect agent management interface in uds mode. |/var/run/ecs/appnet_admin.sock   |\n|`APPNET_AGENT_LOGGING_RESET_TIMEOUT` |300 |Length of time agent will wait for log level to be reset after it is updated via `/enableLogging` endpoint (unit: s)  | 300  |\n|`APPNET_ENVOY_LOG_DESTINATION` |stdout/stderr |Location of log file of the agent. If this variable is set to a file, the log won't be printed to stdour/stderr. If this variable is not set or is set to an empty string, the default value will be applied.   |stdout/stderr   |\n|`APPNET_ENVOY_LOG_NAME` |appnet_envoy.log |The name of Log file of the agent  |appnet_envoy.log   |\n|`APPNET_AGENT_MAX_LOG_FILE_SIZE` |1.0 |The max size of log file of the agent (unit: MB)  |1.0   |\n|`APPNET_AGENT_MAX_RETENTION_COUNT` |3 |The max number of log files of the agent  |5   |\n|`HC_POLL_INTERVAL_MS` |2000 |The interval at which the agent health checks envoy (unit: ms)  | 10000   |\n|`HC_DISCONNECTED_TIMEOUT_S`  | 3600 | Timeout after which a continued disconnection from management server would result in failing orchestrator health checks (unit: s) | 604800 |\n|`APPNET_AGENT_POLL_ENVOY_READINESS_INTERVAL_S` |5 | Specified by the controller when running on EKS, this specifies the interval of non-daemon envoy health checks by agent. (second) |5   |\n|`APPNET_AGENT_POLL_ENVOY_READINESS_TIMEOUT_S` |180 |The timeout of non-daemon envoy health check (second) |180   |\n|`ENABLE_STATS_SNAPSHOT`  | <true &#124; false> | Specify whether the agent should take periodic snapshot of emitted stats and compute a timed delta. | false |\n\n**Agent Relay Mode Operation Environment Variables**\n\nThese environment variables offer controls to alter the agent functionality when running in the Relay mode. The relay runs one per container instance and proxies xDS connections/requests from all the Amazon ECS Service Connect Agent containers running on the host to the control plane management server. It uses a static bootstrap config file stored in the `agent/resources/bootstrap_configs` directory.\n\n|Environment Key\t|Example Value(s)\t|Description\t|Default Value\t|\n|---\t|---\t|---\t|---\t|\n|`APPNET_ENABLE_RELAY_MODE_FOR_XDS` |<0 &#124; 1> |Enables relay mode for the agent to be run on the container instance. If set as 1, Envoy would be bootstrapped with the static config present in the image and act as a relay for all communication between the agent containers on the instance and the management server. |0   |\n|`APPNET_MANAGEMENT_DOMAIN_NAME` |hostname.aws.api | Management service endpoint domain name for relay bootstrap config generation. |ecs-sc.$AWS_REGION.aws.api  |\n|`APPNET_MANAGEMENT_PORT` |1234 | Management service endpoint port for relay bootstrap config generation. |443  |\n|`APPNET_RELAY_LISTENER_UDS_PATH` |/path/to/socket |Specify unix domain socket path for xDS Relay listener to serve control plane requests from the Amazon ECS Service Connect Agent. | `/tmp/relay_xds.sock` |\n|`RELAY_STREAM_IDLE_TIMEOUT`  | 2000s | Timeout value for connection between the agent in relay mode and the management server. | 2400s |\n|`RELAY_BUFFER_LIMIT_BYTES`  | 10485760 | Allows for configurable connection buffer limit for agent in relay mode. | 10485760 |\n\n**Management Server Operating Environment Variables**\n\nThese environment variables are used to pass operating platform/environment information to the management server for control plane operations and dynamic configuration generation.\n\n|Environment Key\t|Example Value(s)\t|Description\t|Default Value\t|\n|---\t|---\t|---\t|---\t|\n|`ECS_CONTAINER_INSTANCE_ARN`  | `arn:aws:ecs:region:aws_account_id:container-instance/cluster-name/container-instance-id` | When set, used to send ECS container instance Arn information to management server for authorization purposes. |  |\n|`APPMESH_PLATFORM_K8S_POD_UID`  | `arn:aws:ecs:region:aws_account_id:container-instance/cluster-name/container-instance-id` | For Envoy running on K8s, Pod UID injected by App Mesh Controller. |  |\n|`APPNET_CONTAINER_IP_MAPPING`  | `{\"App1\":\"172.10.1.1\",\"App2\":\"172.10.1.2\"}` | Specifies address mapping of application container as set by ECS agent in ECS Service Connect. |  |\n|`APPNET_LISTENER_PORT_MAPPING`  | `{\"Listener1\":15000,\"Listener2\":15001}` | Specifies port mapping for each application port as set by ECS agent in ECS Service Connect. |  |\n\n### Deprecated\n\n* `APPMESH_RESOURCE_NAME`\n* `APPMESH_VIRTUAL_NODE_NAME`\n\n### Management APIs\n\nThe Amazon ECS Service Connect Agent offers a local management interface when `APPNET_AGENT_ADMIN_MODE` is set. Following are the supported queries:\n\n* `GET /status`: Returns Envoy operating information such as its connectivity state, restarts count, connection with control plane, health check, etc.\n* `POST /drain_listeners`: Drains all inbound Envoy listeners.\n* `POST /enableLogging?level=<desired_level>`: Change Envoy logging level across all loggers. The change is automatically reset after a duration configurable using `APPNET_AGENT_LOGGING_RESET_TIMEOUT` variable.\n* `GET /stats/prometheus`: Returns Envoy statistics in Prometheus format.\n* `GET /stats/prometheus?usedonly`: Only returns statistics that Envoy has updated.\n* `GET /stats/prometheus?filter=metrics_extension`: Filters and returns only the statistics generated by [Metrics Extension](https://docs.aws.amazon.com/app-mesh/latest/userguide/metrics.html#metrics-extension). Can be used in conjunction with `usedonly` parameter.\n* `GET /stats/prometheus?usedonly&filter=metrics_extension&delta`: Returns a delta of the statistics computed using the latest snapshot retrieved from Envoy. Requires enabling the snapshotter using `ENABLE_STATS_SNAPSHOT` variable.\n\n\n## Contributing\n\nContributions and feedback are welcome! Proposals and pull requests will be considered and responded to. For more information, see the [CONTRIBUTING](CONTRIBUTING.md) file.\n\nIf you have a bug/and issue around the behavior of the Amazon ECS Service Connect Agent, please open it here.\n\nIf you have a feature request, please open it over at the [AWS Containers Roadmap](https://github.com/aws/containers-roadmap).\n\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2024-02-27T21:38:33Z", "2023-10-26T20:30:13Z", "2023-10-21T00:04:19Z", "2023-08-18T02:39:58Z", "2023-04-11T20:34:31Z", "2023-03-30T17:24:26Z"]}, {"name": "amazon-eks-connector", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# EKS Connector\n\nEKS Connector is a client-side agent that connects any Kubernetes cluster to AWS. \n\n## How it works\n\nEKS Connector runs in Kubernetes as a Pod that consists of below containers:\n\n![./doc/eks-connector-diagram.png](doc/eks-connector-diagram.png)\n\n### init container\n\nThe init container is responsible for initiating the state of EKS Connector.\n\n### proxy container\n\nThe proxy container is responsible for proxying Kubernetes API Server traffic and applying appropriate\nuser-impersonation flow.\n\n### agent container\n\nThe agent container runs\nthe [AWS System Manager Agent](https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html). It\nmaintains a persistent, secure connection between the Kubernetes cluster and AWS.\n\nAWS SSM agent is published at [ECR Public](https://gallery.ecr.aws/amazon-ssm-agent/amazon-ssm-agent) \n\n## Development\n\n### Install from repo\n\nRun the following command against the cluster after retrieving your activation code and id. See the guide \n[here](https://docs.aws.amazon.com/eks/latest/userguide/connecting-cluster.html).\n\n```shell\n$ helm -n eks-connector install eks-connector \\\n    oci://public.ecr.aws/eks-connector/eks-connector-chart \\\n    --set eks.activationCode=\"<your-activation-code>\" \\\n    --set eks.activationId=\"<your-activation-id>\" \\\n    --set eks.agentRegion=\"<your-region>\"\n```\n\n### Updating dependencies\n\n- `GOPROXY=direct go get -t <module>`\n- `go mod vendor`\n- development with new dependency\n- `go mod tidy`\n- commit vendor folder changes in a dedicated CR for easier review\n- commit code changes in follow-up CR\n\n## Release\n\nAmazon EKS Connector build is released at [ECR Public](https://gallery.ecr.aws/eks-connector/eks-connector). \n\n## Test\n\nTo deploy it we need to create an SSM hybrid activation first.\n__For testing, put a high number of activation instance__ so that we don't need to create activation often when SSM\nagent restarts.\n\n```bash\n# Fill in the activation ID and activation code.\nexport EKS_ACTIVATION_ID=\"\"\nexport EKS_ACTIVATION_CODE=\"\"\nexport EKS_AWS_REGION=\"\"\n# Replace with your custom built images if needed\nexport EKS_CONNECTOR_IMAGE=\"public.ecr.aws/eks-connector/eks-connector:0.0.3\"\nexport SSM_AGENT_IMAGE=\"public.ecr.aws/amazon-ssm-agent/amazon-ssm-agent:3.1.1927.0\"\n\n# Apply the manifest\nsed \"s~%AWS_REGION%~$EKS_AWS_REGION~g; s~%EKS_CONNECTOR_IMAGE%~$EKS_CONNECTOR_IMAGE~g; s~%SSM_AGENT_IMAGE%~$SSM_AGENT_IMAGE~g; s~%EKS_ACTIVATION_ID%~$EKS_ACTIVATION_ID~g; s~%EKS_ACTIVATION_CODE%~$(echo -n $EKS_ACTIVATION_CODE | base64)~g\" \\\n    ./manifests/eks-connector.yaml | kubectl apply -f -\n# After a few seconds the connector pod should be healthy in kubernetes.\n\n# Now get the managed instance at SSM.\naws ssm describe-instance-information --filters Key=ActivationIds,Values=$EKS_ACTIVATION_ID\n# If you are lucky you should see exactly one managed instance.\n# Alternatively, grep the logs at init container, which should print out the instance id.\n\n# Now execute non interactive command\n# NOTE: fill in TARGET with your own managed instance id like `mi-069f7e4b6ce64c0ce`\naws ssm start-session \\\n    --target TARGET \\\n    --document-name AWS-StartNonInteractiveCommand \\\n    --parameters '{\"command\": [\"curl --unix-socket /var/eks/shared/connector.sock -H \\\"x-aws-eks-identity-arn: arn:aws:iam::123456789012:user/test-user\\\" http://localhost/api/v1/pods\"]}'\n```\n\n### Cleanup\n\nJust delete with the manifest\n\n```bash\nsed \"s~%AWS_REGION%~$EKS_AWS_REGION~g; s~%EKS_CONNECTOR_IMAGE%~$EKS_CONNECTOR_IMAGE~g; s~%SSM_AGENT_IMAGE%~$SSM_AGENT_IMAGE~g; s~%EKS_ACTIVATION_ID%~$EKS_ACTIVATION_ID~g; s~%EKS_ACTIVATION_CODE%~$(echo -n $EKS_ACTIVATION_CODE | base64)~g\" \\\n    ./manifests/eks-connector.yaml  | kubectl delete -f -\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "amazon-eks-diag", "description": null, "language": "PowerShell", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# amazon-eks-diag\n\n`amazon-eks-diag` is a PowerShell module to help Amazon EKS users gather diagnostic information from their Amazon EC2 Windows worker nodes. The module is executed in a local PowerShell session on each worker node. The module will gather the local diagnostic information for this node only, and compresses the information into an archive. For security, the archive is left on the local file system for the system administrator to choose an apropriate mechanism for retrieving the archive.\n\n## Usage\nThe module needs to be run on an EC2 Windows worker node. There are many ways of distributing the module to a node. The most straight forward is using the AWS.Tools.S3 SDK to upload the module to S3, and then download it on the node. Example:\n1. Compress the `amazon-eks-diag` module directory, and write the archive to S3\n```powershell\nCompress-Archive -Path .\\amazon-eks-diag\\ -DestinationPath .\\amazon-eks-diag.zip\nWrite-S3Object -BucketName my-bucket -Key modules/amazon-eks-diag.zip -File .\\amazon-eks-diag.zip\n```\n2. On your node, download the archive and extract it\n```powershell\nRead-S3Object -BucketName my-bucket -Key modules/amazon-eks-diag.zip -File .\\amazon-eks-diag.zip\nExpand-Archive -Path .\\amazon-eks-diag.zip -DestinationPath .\\\n```\n3. Open a powershell session with the appropriate elevation, import the module, and execute the diagnostic tool\n```powershell\nImport-Module .\\amazon-eks-diag\n$outputZip = Start-EKSDiag\n```\n## Diagnostic Information Gathered\nFollowing is a categorical overview of the diagnostic information gathered by the tool in the way the data is gathered by the tool:\n1. EKS & EC2 Windows related log and configuration files\n    * $ENV:ProgramData\\Amazon\\EKS\\logs\\\\**\\\\*.log\n    * $ENV:ProgramData\\Amazon\\EKS\\cni\\\\**\\\\*.config\n    * $ENV:ProgramData\\Amazon\\EC2-Windows\\\\**\\\\*.log\n    * $ENV:ProgramData\\Amazon\\SSM\\\\**\\\\*.log `(Excluding ipcTempFeil.log from SSM Session Manager)`\n2. Related PowerShell objects serialized into JSON\n    * Get-NetAdapter\n    * Get-NetRoute\n    * Get-HNSNetwork\n    * Get-HNSEndpoint\n    * Get-HNSPolicyList\n    * Get-ScheduledTask -TaskName '\\*EKS\\*'\n    * Get-EventLog -LogName 'EKS'\n    * Get-Service -Name 'kubelet'\n    * Get-Service -Name 'kube-proxy'\n    * Get-Service -Name 'docker'\n    * Get-Service -Name 'AmazonSSMAgent'\n3. Related executable output\n    * docker ps -a\n    * docker images -a\n    * docker network ls\n    * aws-iam-authenticator version\n    * kubelet --version\n    * kube-proxy --version\n4. Pester test output from the tests stored within the local module\n    * amazon-eks-diag.hns.tests.ps1\n    * amazon-eks-diag.kubeproxy.tests.ps1\n    * amazon-eks-diag.kubelet.tests.ps1\n5. A PowerShell transcript of the entire tool execution for transparency\n\n## Help\n\n```powershell\nGet-Help Start-EKSDiag -Full\n```\n\n## Unit Testing\n\n```powershell\nInstall-Module Pester -Repository PSGallery -SkipPublisherCheck -Force\n.\\Start-UnitTests.ps1\n```\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": []}, {"name": "amazon-eks-pod-identity-webhook", "description": "Amazon EKS Pod Identity Webhook", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![build](https://github.com/aws/amazon-eks-pod-identity-webhook/workflows/build/badge.svg)\n\n# Amazon EKS Pod Identity Webhook\n\nThis webhook is for mutating pods that will require AWS IAM access.\n\n## Note\nAfter version v0.3.0, `--in-cluster=true` no longer works and is deprecated.  Please use `--in-cluster=false`\nand manage the cluster certificate with cert-manager or some other external certificate provisioning system.\nThis is because certificates using the `legacy-unknown` signer are no longer signed when using the v1\ncertificates API.\n\n## EKS Walkthrough\n\n1. [Create an OIDC provider][1] in IAM for your cluster. You can find the OIDC\n   discovery endpoint by describing your EKS cluster.\n    ```bash\n    aws eks describe-cluster --name $CLUSTER_NAME --query cluster.identity.oidc\n    ```\n    And enter \"sts.amazonaws.com\" as the client-id\n2. Create an IAM role for your pods and [modify the trust policy][2] to allow\n   your pod's service account to use the role:\n    ```json\n    {\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n      {\n       \"Effect\": \"Allow\",\n       \"Principal\": {\n        \"Federated\": \"arn:aws:iam::111122223333:oidc-provider/oidc.REGION.eks.amazonaws.com/CLUSTER_ID\"\n       },\n       \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n       \"Condition\": {\n        \"__doc_comment\": \"scope the role to the service account (optional)\",\n        \"StringEquals\": {\n         \"oidc.REGION.eks.amazonaws.com/CLUSTER_ID:sub\": \"system:serviceaccount:default:my-serviceaccount\"\n        },\n        \"__doc_comment\": \"scope the role to a namespace (optional)\",\n        \"StringLike\": {\n         \"oidc.REGION.eks.amazonaws.com/CLUSTER_ID:sub\": \"system:serviceaccount:default:*\"\n        }\n       }\n      }\n     ]\n    }\n    ```\n3. Modify your pod's service account to be annotated with the ARN of the role\n   you want the pod to use\n    ```yaml\n    apiVersion: v1\n    kind: ServiceAccount\n    metadata:\n      name: my-serviceaccount\n      namespace: default\n      annotations:\n        eks.amazonaws.com/role-arn: \"arn:aws:iam::111122223333:role/s3-reader\"\n        # optional: Defaults to \"sts.amazonaws.com\" if not set\n        eks.amazonaws.com/audience: \"sts.amazonaws.com\"\n        # optional: When set to \"true\", adds AWS_STS_REGIONAL_ENDPOINTS env var\n        #   to containers\n        eks.amazonaws.com/sts-regional-endpoints: \"true\"\n        # optional: Defaults to 86400 for expirationSeconds if not set\n        #   Note: This value can be overwritten if specified in the pod \n        #         annotation as shown in the next step.\n        eks.amazonaws.com/token-expiration: \"86400\"\n    ```\n4. All new pod pods launched using this Service Account will be modified to use\n   IAM for pods. Below is an example pod spec with the environment variables and\n   volume fields added by the webhook.\n    ```yaml\n    apiVersion: v1\n    kind: Pod\n    metadata:\n      name: my-pod\n      namespace: default\n      annotations:\n        # optional: A comma-separated list of initContainers and container names\n        #   to skip adding volumes and environment variables\n        eks.amazonaws.com/skip-containers: \"init-first,sidecar\"\n        # optional: Defaults to 86400, or value specified in ServiceAccount\n        #   annotation as shown in previous step, for expirationSeconds if not set\n        eks.amazonaws.com/token-expiration: \"86400\"\n    spec:\n      serviceAccountName: my-serviceaccount\n      initContainers:\n      - name: init-first\n        image: container-image:version\n      containers:\n      - name: sidecar\n        image: container-image:version\n      - name: container-name\n        image: container-image:version\n    ### Everything below is added by the webhook ###\n        env:\n        - name: AWS_DEFAULT_REGION\n          value: us-west-2\n        - name: AWS_REGION\n          value: us-west-2\n        - name: AWS_ROLE_ARN\n          value: \"arn:aws:iam::111122223333:role/s3-reader\"\n        - name: AWS_WEB_IDENTITY_TOKEN_FILE\n          value: \"/var/run/secrets/eks.amazonaws.com/serviceaccount/token\"\n        - name: AWS_STS_REGIONAL_ENDPOINTS\n          value: \"regional\"\n        volumeMounts:\n        - mountPath: \"/var/run/secrets/eks.amazonaws.com/serviceaccount/\"\n          name: aws-token\n      volumes:\n      - name: aws-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              audience: \"sts.amazonaws.com\"\n              expirationSeconds: 86400\n              path: token\n    ```\n\n### Usage with Windows container workloads\n\nTo ensure workloads are scheduled on windows nodes have the right environment variables, they must have a `nodeSelector` targeting windows it must run on.  Workloads targeting windows nodes using `nodeAffinity` are currently not supported.\n```yaml\n  nodeSelector:\n    beta.kubernetes.io/os: windows\n```\n\nOr for Kubernetes 1.14+\n\n```yaml\n  nodeSelector:\n    kubernetes.io/os: windows\n```\n\n\n\n[1]: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html\n[2]: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-idp_oidc.html\n\n### Usage with non-root container user\n\nWhen running a container with a non-root user, you need to give the container access to the token file by setting the `fsGroup` field in the `securityContext` object.\n\n## Usage\n\n```\nUsage of amazon-eks-pod-identity-webhook:\n      --add_dir_header                   If true, adds the file directory to the header\n      --alsologtostderr                  log to standard error as well as files\n      --annotation-prefix string         The Service Account annotation to look for (default \"eks.amazonaws.com\")\n      --aws-default-region string        If set, AWS_DEFAULT_REGION and AWS_REGION will be set to this value in mutated containers\n      --enable-debugging-handlers        Enable debugging handlers. Currently /debug/alpha/cache is supported\n      --in-cluster                       Use in-cluster authentication and certificate request API (default true)\n      --kube-api string                  (out-of-cluster) The url to the API server\n      --kubeconfig string                (out-of-cluster) Absolute path to the API server kubeconfig file\n      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)\n      --log_dir string                   If non-empty, write log files in this directory\n      --log_file string                  If non-empty, use this log file\n      --log_file_max_size uint           Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)\n      --logtostderr                      log to standard error instead of files (default true)\n      --metrics-port int                 Port to listen on for metrics and healthz (http) (default 9999)\n      --namespace string                 (in-cluster) The namespace name this webhook, the TLS secret, and configmap resides in (default \"eks\")\n      --port int                         Port to listen on (default 443)\n      --service-name string              (in-cluster) The service name fronting this webhook (default \"pod-identity-webhook\")\n      --skip_headers                     If true, avoid header prefixes in the log messages\n      --skip_log_headers                 If true, avoid headers when opening log files\n      --stderrthreshold severity         logs at or above this threshold go to stderr (default 2)\n      --sts-regional-endpoint false      Whether to inject the AWS_STS_REGIONAL_ENDPOINTS=regional env var in mutated pods. Defaults to false.\n      --tls-cert string                  (out-of-cluster) TLS certificate file path (default \"/etc/webhook/certs/tls.crt\")\n      --tls-key string                   (out-of-cluster) TLS key file path (default \"/etc/webhook/certs/tls.key\")\n      --tls-secret string                (in-cluster) The secret name for storing the TLS serving cert (default \"pod-identity-webhook\")\n      --token-audience string            The default audience for tokens. Can be overridden by annotation (default \"sts.amazonaws.com\")\n      --token-expiration int             The token expiration (default 86400)\n      --token-mount-path string          The path to mount tokens (default \"/var/run/secrets/eks.amazonaws.com/serviceaccount\")\n  -v, --v Level                          number for the log level verbosity\n      --version                          Display the version and exit\n      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging\n      --watch-config-map                 Enables watching serviceaccounts that are configured through the pod-identity-webhook configmap instead of using annotations\n```\n\n### AWS_DEFAULT_REGION Injection\n\nWhen the `aws-default-region` flag is set this webhook will inject `AWS_DEFAULT_REGION` and `AWS_REGION` in mutated containers if `AWS_DEFAULT_REGION` and `AWS_REGION` are not already set.\n\n### AWS_STS_REGIONAL_ENDPOINTS Injection\n\nWhen the `sts-regional-endpoint` flag is set to `true`, the webhook will\ninject the environment variable `AWS_STS_REGIONAL_ENDPOINTS` with the value set\nto `regional`. This environment variable will configure the AWS SDKs to perform\nthe `sts:AssumeRoleWithWebIdentity` call to get credentials from the regional\nendpoint, instead of the global endpoint in `us-east-1`. This is desirable in\nalmost all cases, unless the STS regional endpoint is [disabled in your\naccount](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_enable-regions.html).\n\nYou can also enable this per-service account with the annotation\n`eks.amazonaws.com/sts-regional-endpoints` set to `\"true\"`.\n\n### pod-identity-webhook ConfigMap\n\nThe purpose of the `pod-identity-webhook` ConfigMap is to simplify the mapping of IAM roles and ServiceAccount\nwhen using tools/installers like [kOps](https://kops.sigs.k8s.io/) that directly manage IAM roles and trust policies. When using these tools,\nusers do not need to configure annotations on the ServiceAccounts as the tools already know the relationship can relay it to the webhook.\n\nWhen the `watch-config-map` flag is set to `true`, the webhook will watch the\n`pod-identity-webhook` ConfigMap in the namespace configured by the `--namespace` flag\nfor additional ServiceAccounts. The webhook will mutate Pods configured to use these\nServiceAccounts even if they have no annotations.\n\nShould the same ServiceAccount both be referenced both in the ConfigMap and have annotations, the annotations takes presedence. \n\nHere is an example ConfigMap:\n\n```\napiVersion: v1\ndata:\n  config: '{\"default/myserviceaccount\":{\"RoleARN\":\"arn:aws-test:iam::123456789012:role/myserviceaccount.default.sa.minimal.example.com\",\"Audience\":\"amazonaws.com\",\"UseRegionalSTS\":true,\"TokenExpiration\":0},\"myapp/myotherserviceaccount\":{\"RoleARN\":\"arn:aws-test:iam::123456789012:role/myotherserviceaccount.myapp.sa.minimal.example.com\",\"Audience\":\"amazonaws.com\",\"UseRegionalSTS\":true,\"TokenExpiration\":0},\"test-*/myserviceaccount\":{\"RoleARN\":\"arn:aws-test:iam::123456789012:role/myserviceaccount.test-wildcard.sa.minimal.example.com\",\"Audience\":\"amazonaws.com\",\"UseRegionalSTS\":true,\"TokenExpiration\":0}}'\nkind: ConfigMap\nmetadata:\n  annotations:\n    prometheus.io/port: \"443\"\n    prometheus.io/scheme: https\n    prometheus.io/scrape: \"true\"\n  creationTimestamp: null\n  name: pod-identity-webhook\n  namespace: kube-system\n```\n\n\n## Container Images\n\nContainer images for amazon-eks-pod-identity-webhook can be found on [Docker Hub](https://hub.docker.com/r/amazon/amazon-eks-pod-identity-webhook).\n\n## Installation\n\n### Pre-requisites\n\nYou must install cert-manager as it is a pre-requisite for below deployments. (See [cert-manager installation](https://cert-manager.io/docs/installation/))\n\n### In-cluster\n\nYou can use the provided configuration files in the `deploy` directory, along with the provided `Makefile`.\n\n```\nmake cluster-up IMAGE=amazon/amazon-eks-pod-identity-webhook:latest\n```\n\nThis will:\n* Create a service account, role, cluster-role, role-binding, and cluster-role-binding that the deployment requires\n* Create the deployment, service, ClusterIssuer, certificate, and mutating webhook in the cluster\n* Use `in-cluster=false` so that the webhook reloads certificates from the filesystem rather than creating CSRs to request certificates (using CSRs is now deprecated and will not work versions later than v0.3.0).\n\nFor self-hosted API server configuration, see see [SELF_HOSTED_SETUP.md](/SELF_HOSTED_SETUP.md)\n\n### On API server\nTODO\n\n### Notes\nWith the upgrade to client-go 1.18, certificate_manager_server_expiration_seconds metric has been removed by an upstream commit kubernetes/kubernetes#85874.\nA new metric certificate_manager_server_rotation_seconds is added which tracks the time a certificate was valid before getting rotated.\n\n## Code of Conduct\nSee [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md)\n\n## License\nApache 2.0 - Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\nSee [LICENSE](LICENSE)\n\n", "release_dates": ["2023-10-26T17:39:53Z", "2023-09-15T20:42:23Z", "2023-08-18T00:06:07Z", "2022-03-04T06:32:09Z", "2022-01-22T02:20:50Z", "2020-01-23T19:58:17Z", "2020-01-23T19:57:17Z"]}, {"name": "amazon-elastic-inference-tools", "description": "Amazon Elastic Inference tools and utilities.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon Elastic Inference Tools\n\nAmazon Elastic Inference tools and utilities.\n\n## License\n\nAmazon Elastic Inference setup tool for EC2 is licensed under the Apache 2.0 License. \n\n## Amazon Elastic Inference Setup Tool for EC2\n\nFor more information on how to use the Amazon Elastic Inference setup tool, please refer to [Launch EI accelerators in minutes with the Amazon Elastic Inference setup tool for EC2.](https://aws.amazon.com/blogs/machine-learning/launch-ei-accelerators-in-minutes-with-the-amazon-elastic-inference-setup-tool-for-ec2/)\n", "release_dates": []}, {"name": "amazon-finspace-examples", "description": "This repo contains sample code and sample notebooks to illustrate how to work with Amazon FinSpace", "language": "Jupyter Notebook", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# Amazon FinSpace Examples\nThis repository contains examples that show how to work with [Amazon FinSpace](https://aws.amazon.com/finspace/). Amazon FinSpace is comprised of [Managed kdb Insights](https://aws.amazon.com/finspace/features/managed-kdb-insights/) and the [Dataset Browser](https://aws.amazon.com/finspace/features/dataset-browser/)\n\n## Managed kdb Insights\nThese are example projects using FinSpace with Managed kdb Insights. Refer to each project README.\n\n- [AWS Boto Examples](ManagedkdbInsights/boto)\n- [q Examples](ManagedkdbInsights/q)\n- [Reference Architecture](ManagedkdbInsights/basic_tick)\n- [HDB Backup](ManagedkdbInsights/hdb_backup)\n\n## Dataset Browser\nExample notebooks using the dataset browser.\n\n- [Dataset Browser Notebooks](DatasetBrowser)\n\n## Blogs\n[Analyze daily trading activity using transaction data from Amazon Redshift in Amazon FinSpace](blogs/finspace_redshift-2021-09)   \nHow to connect Amazon FinSpace to a Redshift cluster, import tables into FinSpace datasets, and pull data in tables from\nRedshift directly into Spark DataFrames.\n\n## Webinars\n[Making Financial Data More Accessible in the Cloud](webinars/snowflake_2021-09)  \nNotebooks used to demonstrate integration of Snowflake tables with Amazon FinSpace. Presented at Snowflake Financial\nServices Summit Sept 14, 2021: [Making Financial Data More Accessible in the Cloud](https://www.snowflake.com/financial-services-data-summit/americas/agenda/?agendaPath=session/615483)\n\n## FAQ\n*How do I contribute my own examples?*  \n\n- Although we're extremely excited to receive contributions from the community, we're still working on the best mechanism to take in examples from external sources.  Please bear with us in the short-term if pull requests take longer than expected or are closed.\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.\n\n", "release_dates": []}, {"name": "amazon-freertos", "description": "DEPRECATED - See README.md", "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# ![image](https://user-images.githubusercontent.com/56273942/202568467-0ee721bb-1424-4efd-88fc-31b4f2a59dc6.png) DEPRECATED\n\n## Announcement:\nAs of November 18th, 2022, this repository is deprecated. The contents of this repository will remain available but we will no longer provide updates or accept new contributions and pull requests. We recommend you start [here](https://docs.aws.amazon.com/freertos/latest/userguide/freertos-getting-started-modular.html) for creating a new project. If you have an existing FreeRTOS project based on this repository, see the [migration guide](https://docs.aws.amazon.com/freertos/latest/userguide/github-repo-migration.html). \n\nOver the years, AWS has improved the modularity of the FreeRTOS libraries and repository structure to make it easier for you to build and update FreeRTOS-based projects. This repository deprecation aligns with some of these significant initiatives:\n*   We decomposed libraries to include them in their individual repositories and removed interdependencies between each library giving you the flexibility to choose the FreeRTOS libraries and project structure that suits your project and toolchain.\n*   We split libraries that are AWS dependent and FreeRTOS dependent into separate repositories giving you the option to mix and match libraries that are specific to your board and use case.\n*   We provided  feature stability, security patches, and critical bug fixes through the Long Term Support (LTS) libraries. \n\nHave more questions? Post them in the [FreeRTOS forum](https://forums.freertos.org/).\n\n# FreeRTOS AWS Reference Integrations\n\n## Cloning\nThis repo uses [Git Submodules](https://git-scm.com/book/en/v2/Git-Tools-Submodules) to bring in dependent components.\n\nNote: If you download the ZIP file provided by GitHub UI, you will not get the contents of the submodules. (The ZIP file is also not a valid git repository)\n\nIf using Windows, because this repository and its submodules contain symbolic links, set `core.symlinks` to true with the following command:\n```\ngit config --global core.symlinks true\n```\nIn addition to this, either enable [Developer Mode](https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development) or, whenever using a git command that writes to the system (e.g. `git pull`, `git clone`, and `git submodule update --init --recursive`), use a console elevated as administrator so that git can properly create symbolic links for this repository. Otherwise, symbolic links will be written as normal files with the symbolic links' paths in them as text. [This](https://blogs.windows.com/windowsdeveloper/2016/12/02/symlinks-windows-10/) gives more explanation.\n\nTo clone using HTTPS:\n```\ngit clone https://github.com/aws/amazon-freertos.git --recurse-submodules\n```\nUsing SSH:\n```\ngit clone git@github.com:aws/amazon-freertos.git --recurse-submodules\n```\n\nIf you have downloaded the repo without using the `--recurse-submodules` argument, you need to run:\n```\ngit submodule update --init --recursive\n```\n\n## Important branches to know\nmaster            --> Development is done continuously on this branch  \nrelease           --> Fully tested released source code  \nrelease-candidate --> Preview of upcoming release  \nfeature/*         --> Alpha/beta of an upcoming feature  \n\n## Getting Started\n\nFor more information on FreeRTOS, refer to the [Getting Started section of FreeRTOS webpage](https://aws.amazon.com/freertos).\n\nTo directly access the **Getting Started Guide** for supported hardware platforms, click the corresponding link in the Supported Hardware section below.\n\nFor detailed documentation on FreeRTOS, refer to the [FreeRTOS User Guide](https://aws.amazon.com/documentation/freertos).\n\n### AWS Collection of Metrics\n\nThe demos that connect to AWS IoT report metrics to AWS about the operating system, and the MQTT client library used by sending a specially formatted string in the username field of the MQTT CONNECT packet. These metrics help AWS IoT improve security and provide better technical support. Providing these metrics is optional for users, and these can be disabled by updating the following configuration macros in the `demos/include/aws_iot_metrics.h` file:\n\n```\n#define AWS_IOT_METRICS_STRING           NULL\n#define AWS_IOT_METRICS_STRING_LENGTH    0U\n```\n\n#### Format\n\nThe format of the username string with metrics is:\n\n```\n<Actual_Username>?SDK=<OS_Name>&Version=<OS_Version>MQTTLib=<MQTT_Library_name>@<MQTT_Library_version>\n```\n\nwhere\n\n* **Actual_Username** is the actual username used for authentication (if a username/password is used for authentication).\n* **OS_Name** is the Operating System the application is running on.\n* **OS_Version** is the version number of the Operating System.\n* **MQTT_Library_name** is the MQTT Client library being used.\n* **MQTT_Library_version** is the version of the MQTT Client library being used.\n\n## FreeRTOS Qualified Boards\n\nFor a complete list of boards that have been qualified for FreeRTOS by AWS Partners, please visit the [AWS Partner Device Catalog](https://devices.amazonaws.com/search?page=1&sv=freertos)\n\nIn addition, AWS supports the following boards with FreeRTOS Build Integration and maintains them with each release:\n\n1. **Texas Instruments** - [CC3220SF-LAUNCHXL](https://devices.amazonaws.com/detail/a3G0L00000AANtaUAH/SimpleLink-Wi-Fi%C2%AE-CC3220SF-Wireless-Microcontroller-LaunchPad-Development-Kit).\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_ti.html)\n    * IDEs: [Code Composer Studio](http://www.ti.com/tools-software/ccs.html), [IAR Embedded Workbench](https://www.iar.com/iar-embedded-workbench/partners/texas-instruments)\n2. **STMicroelectronics** - [STM32L4 Discovery kit IoT node](https://devices.amazonaws.com/detail/a3G0L00000AANsWUAX/STM32L4-Discovery-Kit-IoT-Node).\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_st.html)\n    * IDE: [STM32 System Workbench](http://openstm32.org/HomePage)\n3. **NXP** - [LPC54018 IoT Module](https://devices.amazonaws.com/detail/a3G0L00000AANtAUAX/LPC54018-IoT-Solution), \n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_nxp.html)\n    * IDEs: [IAR Embedded Workbench](https://www.iar.com/iar-embedded-workbench/partners/nxp), [MCUXpresso IDE](https://www.nxp.com/mcuxpresso/ide/download)\n4. **Espressif** - [ESP32-DevKitC](https://devices.amazonaws.com/detail/a3G0L00000AANtjUAH/ESP32-WROOM-32-DevKitC), [ESP-WROVER-KIT](https://devices.amazonaws.com/detail/a3G0L00000AANtlUAH/ESP-WROVER-KIT), [ESP32-WROOM-32SE](https://devices.amazonaws.com/detail/a3G0L00000AANtjUAH/ESP32-WROOM-32-DevKitC), [ESP32-S2-SAOLA-1](https://devices.amazonaws.com/detail/a3G0h00000AkFngEAF/ESP32-S2-Saola-1)\n    * [Getting Started Guide - ESP32-DevKitC, ESP-WROVER-KIT](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_espressif.html)\n    * [Getting Started Guide - ESP32-WROOM-32SE](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_esp32wroom-32se.html)\n5. **Infineon** - [Infineon XMC4800 IoT Connectivity Kit](https://devices.amazonaws.com/detail/a3G0L00000AANsbUAH/XMC4800-IoT-FreeRTOS-Connectivity-Kit-WiFi), [Optiga TrustX](https://devices.amazonaws.com/detail/a3G0h000007712QEAQ/OPTIGA%E2%84%A2-Trust-X-Security-Solution)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_infineon.html)\n    * IDE: [DAVE](https://infineoncommunity.com/dave-download_ID645)\n6. **Xilinx** - [Xilinx Zynq-7000 based MicroZed Industrial IoT Bundle](https://devices.amazonaws.com/detail/a3G0L00000AANtqUAH/MicroZed-IIoT-Bundle-with-FreeRTOS)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_xilinx.html)\n    * IDE: [Xilinx SDK](https://www.xilinx.com/products/design-tools/embedded-software/sdk.html)\n7. **MediaTek** - [MediaTek MT7697Hx Development Kit](https://devices.amazonaws.com/detail/a3G0L00000AAOmPUAX/MT7697Hx-Development-Kit)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_mediatek.html)\n    * IDE: [Keil uVision](http://www2.keil.com/mdk5/install/)\n8. **Renesas** - [Renesas Starter Kit+ for RX65N-2MB](https://devices.amazonaws.com/detail/a3G0L00000AAOkeUAH/Renesas-Starter-Kit+-for-RX65N-2MB)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_renesas.html)\n    * IDE: [e2 studio](https://www.renesas.com/us/en/products/software-tools/tools/ide/e2studio.html)\n9. **Cypress CYW54907** - [Cypress CYW954907AEVAL1F Evaluation Kit](https://devices.amazonaws.com/detail/a3G0L00000AAPg5UAH/CYW954907AEVAL1F)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_cypress_54.html)\n    * IDE: [WICED Studio](https://community.cypress.com/community/wiced-wifi)\n10. **Cypress CYW43907** - [Cypress CYW943907AEVAL1F Evaluation Kit](https://devices.amazonaws.com/detail/a3G0L00000AAPg0UAH/CYW943907AEVAL1F)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_cypress_43.html)\n    * IDE: [WICED Studio](https://community.cypress.com/community/wiced-wifi)\n11. **Cypress PSoC 64** - [PSoC 64 Standard Secure AWS Wi-Fi Bluetooth Pioneer Kit](https://devices.amazonaws.com/detail/a3G0h0000088AgXEAU/PSoC%C2%AE-64-Standard-Secure-AWS-Wi-Fi-Bluetooth-Pioneer-Kit)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_cypress_psoc64.html)\n    * IDE: [ModusToolbox](https://www.cypress.com/products/modustoolbox-software-environment)\n12. **NXP MW320** - [MW320 AWS IoT Starter Kit](https://devices.amazonaws.com/detail/a3G0h000000OaRnEAK/MW320-AWS-IoT-Starter-Kit)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_mw32x.html)\n13. **NXP MW322** - [MW322 AWS IoT Starter Kit](https://devices.amazonaws.com/detail/a3G0h000000OblKEAS/MW322-AWS-IoT-Starter-Kit)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_mw32x.html)\n14. **Nordic nRF52840 DK** - [nRF52840 DK Development kit](https://devices.amazonaws.com/detail/a3G0L00000AANtrUAH/nRF52840-Development-Kit)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_nordic.html)  \n15. **Nuvoton** - [NuMaker-IoT-M487](https://devices.amazonaws.com/detail/a3G0h000000Tg9cEAC/NuMaker-IoT-M487)\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting-started-nuvoton-m487.html)\n16. **Windows Simulator** - To evaluate FreeRTOS without using MCU-based hardware, you can use the Windows Simulator.\n    * Requirements: Microsoft Windows 7 or newer, with at least a dual core and a hard-wired Ethernet connection\n    * [Getting Started Guide](https://docs.aws.amazon.com/freertos/latest/userguide/getting_started_windows.html)\n    * IDE: [Visual Studio Community Edition](https://www.visualstudio.com/downloads/)\n\n\n## amazon-freeRTOS/projects\nThe ```./projects``` folder contains the IDE test and demo projects for each vendor and their boards. The majority of boards can be built with both IDE and cmake (there are some exceptions!). Please refer to the Getting Started Guides above for board specific instructions.\n\n## Mbed TLS License\nThis repository uses Mbed TLS under Apache 2.0.\n\n## amazon-freerTOS/vendors License\nThe `./vendors` directory contains content that may be subject to different license terms. For vendor licensing information, see the LICENSE files or source header documentation for each vendor directory.\n\n## CBMC\n\nThe `tools/cbmc/proofs` directory contains CBMC proofs.\n\nTo learn more about CBMC and proofs specifically, review the training material [here](https://model-checking.github.io/cbmc-training).\n\nIn order to run these proofs you will need to install CBMC and other tools by following the instructions [here](https://model-checking.github.io/cbmc-training/installation.html).\n\n", "release_dates": ["2022-03-16T23:40:57Z", "2021-07-15T02:04:57Z", "2020-12-21T20:12:58Z", "2020-12-21T19:20:37Z", "2020-11-10T02:09:01Z", "2020-07-18T01:06:01Z", "2020-02-19T00:39:47Z", "2020-02-07T21:54:42Z", "2020-02-07T21:51:24Z"]}, {"name": "amazon-freertos-ble-android-sdk", "description": "Android SDK for FreeRTOS Bluetooth Devices.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ![image](https://user-images.githubusercontent.com/56273942/202568467-0ee721bb-1424-4efd-88fc-31b4f2a59dc6.png) DEPRECATED\n\n## Announcement:\nAs of July 31st, 2023, this repository is deprecated. The contents of this repository will remain available but we will no longer provide updates nor accept new contributions and pull requests. We recommend instead that you start [here](https://docs.amplify.aws/start/q/integration/android/) for creating interactions between an Android device and AWS. For Bluetooth Low Energy APIs we recommend you start [here](https://developer.android.com/guide/topics/connectivity/bluetooth/ble-overview).\n\nSince the initial release of this package, several new Android versions have been released bringing with them enhanced security and usability. AWS Amplify has also [released dozens of later versions](https://github.com/aws-amplify/aws-sdk-android/releases) of the [AWS SDK](https://github.com/aws-amplify/aws-sdk-android) with many new features and bug fixes.\n\nThe amazon-freertos-ble-android-sdk is being deprecated to remove an example built on outdated code.\n\nHave more questions? Post them in the [FreeRTOS forum](https://forums.freertos.org/).\n\n# FreeRTOS BLE Mobile SDK for Android\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/software.amazon.freertos/amazonfreertossdk/badge.svg?cacheSeconds=10)](https://maven-badges.herokuapp.com/maven-central/software.amazon.freertos/amazonfreertossdk/)\n## Introduction\n\nUsing the Android SDK for FreeRTOS Bluetooth Devices, you can create mobile applications that do the following:\n\n- Scan for and connect to nearby BLE devices running FreeRTOS\n\n- Perform WiFi provisioning of the FreeRTOS BLE devices after connecting to them ( Only supported for Espressif ESP32-DevKitC )\n\n- Act as a proxy for transmitting MQTT messages between a device running FreeRTOS and the AWS IoT cloud\n\n## System requirements\n\n- Android 6.0 (API level 23) or higher\n\n- Bluetooth 4.2 or higher\n\n- Android Studio\n\n## Setting Up the SDK\n\n1. Set the SDK as a dependency for the application.\n\n**Option 1**: install from maven\nIn your app's `build.gradle` file, add the following into dependencies block:\n(replace x.y.z with [![Maven Central](https://maven-badges.herokuapp.com/maven-central/software.amazon.freertos/amazonfreertossdk/badge.svg?cacheSeconds=30)](https://maven-badges.herokuapp.com/maven-central/software.amazon.freertos/amazonfreertossdk/))\n```\n    implementation('software.amazon.freertos:amazonfreertossdk:x.y.z')\n```\n\n**Option 2**: Build the sdk locally.\nIn your app's `build.gradle` file, add the following into dependencies block:\n```\n    implementation project(':amazonfreertossdk')\n```\nIn project's `settings.gradle` file, add ':amazonfreertossdk'\n```\n    include ':app', ':amazonfreertossdk'\n```\n\n2. In your app's `AndroidManifest.xml` file, add following permissions:\n\n```\n<uses-permission android:name=\"android.permission.BLUETOOTH\"/>\n    <!-- initiate device discovery and manipulate bluetooth settings -->\n<uses-permission android:name=\"android.permission.BLUETOOTH_ADMIN\"/>\n    <!-- allow scan BLE -->\n<uses-permission android:name=\"android.permission.ACCESS_FINE_LOCATION\" />\n\n    <!-- AWS Mobile SDK -->\n<uses-permission android:name=\"android.permission.INTERNET\" />\n<uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\" />\n```\n\n3. Turn on phone's **Location Services**.\n\nIn addition to adding location permissions, this is required by android to perform a successful BLE scan.\n\n## Contents\n\n### BLE Helper Functions\n\nThe SDK includes some functions that help you perform BLE operations with FreeRTOS devices:\n\n```\n    startScanDevices\n    stopScanDevices\n    connectToDevice\n    disconnectFromDevice\n```\n\nOnce the connection to the device is established, you get an AmazonFreeRTOSDevice object, and you can\nuse this object to do WiFi provisioning or Mqtt proxy.\n\n### WiFi Provisioning Service\n\nProvision the WiFi credential on the FreeRTOS device through the app. It provides 4 functions:\n\n```\n    ListNetwork\n    SaveNetwork\n    EditNetwork\n    DeleteNetwork\n````\n\n### MQTT Proxy Service\n\nThe MQTT proxy service controls the MQTT proxy. It allows the device to send and receive MQTT messages\nfrom the AWS IoT cloud through the phone, when this feature is enabled.\n\n\nYou can find the [API documentation](https://aws.github.io/amazon-freertos-ble-android-sdk/) for these functions in the docs directory of this repository and [on github pages](https://aws.github.io/amazon-freertos-ble-android-sdk/).\n\n\n## Demo Application\n\nThe SDK includes a demo application that demonstrates some of the main features of the SDK. You can find the demo in [app](app).\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": ["2022-05-13T04:16:09Z", "2022-01-05T23:03:55Z", "2020-07-08T18:55:38Z", "2020-02-14T00:36:09Z", "2019-06-17T21:02:56Z", "2019-06-08T01:10:17Z", "2019-05-09T18:19:48Z", "2019-04-17T21:50:17Z", "2019-03-04T21:47:56Z", "2019-01-30T19:08:20Z", "2018-11-27T18:18:53Z"]}, {"name": "amazon-freertos-ble-ios-sdk", "description": "iOS SDK for FreeRTOS Bluetooth Devices", "language": "Swift", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ![image](https://user-images.githubusercontent.com/56273942/202568467-0ee721bb-1424-4efd-88fc-31b4f2a59dc6.png) DEPRECATED\n\n## Announcement:\nAs of July 31st, 2023, this repository is deprecated. The contents of this repository will remain available but we will no longer provide updates nor accept new contributions and pull requests. We recommend instead that you start [here](https://docs.amplify.aws/start/q/integration/ios/) for creating interactions between an iOS device and AWS. For Bluetooth Low Energy APIs we recommend you start [here](https://developer.apple.com/bluetooth/).\n\nSince the initial release of this package, several new iOS versions have been released bringing with them enhanced security and usability. AWS Amplify has also [released dozens of later versions](https://github.com/aws-amplify/aws-sdk-ios/releases) of the [AWS SDK](https://github.com/aws-amplify/aws-sdk-ios) with many new features and bug fixes.\n\nThe amazon-freertos-ble-ios-sdk is being deprecated to remove an example built on outdated code.\n\nHave more questions? Post them in the [FreeRTOS forum](https://forums.freertos.org/).\n\n# iOS SDK for FreeRTOS Bluetooth Devices\n\n[![CocoaPods Version](https://img.shields.io/cocoapods/v/AmazonFreeRTOS.svg?style=flat)](https://cocoapods.org/pods/AmazonFreeRTOS)\n[![License](https://img.shields.io/cocoapods/l/AmazonFreeRTOS.svg?style=flat)](https://cocoapods.org/pods/AmazonFreeRTOS)\n[![Platform](https://img.shields.io/cocoapods/p/AmazonFreeRTOS.svg?style=flat)](https://cocoapods.org/pods/AmazonFreeRTOS)\n[![Build Status](https://travis-ci.org/aws/amazon-freertos-ble-ios-sdk.svg?branch=master)](https://travis-ci.org/aws/amazon-freertos-ble-ios-sdk)\n\n## Introduction\n\nUsing the iOS SDK for FreeRTOS Bluetooth Devices, you can create mobile applications that do the following:\n\n- Scan for and connect to nearby BLE devices running FreeRTOS\n\n- Provision Wi-Fi networks for a BLE device running FreeRTOS\n\n- Act as a proxy for transmitting MQTT messages between a device running FreeRTOS and the AWS IoT cloud\n\n## Setting Up the SDK\n\n**To install the iOS SDK for FreeRTOS Bluetooth Devices**\n\n1. Install CocoaPods:\n```ruby\n$ gem install cocoapods\n$ pod setup\n```\n\n**Note** \n\nYou might need to use sudo to install CocoaPods.\n\n2. Install the SDK with CocoaPods (In Podfile):\n\n**Newer version of the freertos firmware use CBOR encoding, please use:**\n\n```ruby\npod 'AmazonFreeRTOS'\n```\n\n**Older version of the freertos firmware use JSON encoding, please use:**\n\n```ruby\npod 'AmazonFreeRTOS', :git => 'https://github.com/aws/amazon-freertos-ble-ios-sdk.git', :tag => '0.9.4'\n```\n\n**For FreeRTOS**\n\nhttps://github.com/aws/amazon-freertos release 201906.00_Major and after\n\n\n## Contents\n\nAll main functions are defined in \n\n[AmazonFreeRTOSManager.swift](AmazonFreeRTOS/AmazonFreeRTOSManager.swift)\n\n[AmazonFreeRTOSDevice.swift](AmazonFreeRTOS/AmazonFreeRTOSDevice.swift)\n\nThese functions include:\n\n## AmazonFreeRTOSManager\n\n### BLE Helper Functions\n\nThe SDK includes some functions that help you perform BLE operations with Amazon FreeRTOS devices:\n\n```swift\n// Start scan for FreeRTOS devices.\nstartScanForDevices()\n\n// Stop scan for FreeRTOS devices.\nstopScanForDevices()\n\n//  Disconnect. Clear all contexts. Scan for FreeRTOS devices.\nrescanForDevices()\n```\n\n## AmazonFreeRTOSDevice\n\n### BLE Helper Functions\n\nThe device ble options:\n\n```swift\n// Connect to the FreeRTOS device.\nconnect(reconnect: Bool, certificateId: String? = nil, credentialsProvider: AWSCredentialsProvider? = nil)\n\n// Disconnect from the FreeRTOS device.\ndisconnect()\n```\n\n### MQTT Proxy Service\n\nMQTT proxy service start automatically\n\n### Network Config Service\n\nThe network configuration service configures the Wi-Fi network of the FreeRTOS Device. Its functions include:\n\n```swift\nlistNetwork(_ listNetworkReq: ListNetworkReq)\nsaveNetwork(_ saveNetworkReq: SaveNetworkReq)\neditNetwork(_ editNetworkReq: EditNetworkReq)\ndeleteNetwork(_ deleteNetworkReq: DeleteNetworkReq)\n```\n## Documentation\n\nhttps://aws.github.io/amazon-freertos-ble-ios-sdk/\n\n## Demo Application\n\nThe SDK includes a demo application that demonstrates some of the main features of the SDK. You can find the demo in [Example/AmazonFreeRTOSDemo](Example/AmazonFreeRTOSDemo).\n\n## License\n\nThis library is licensed under the Apache 2.0 License. \n", "release_dates": ["2022-05-12T18:56:59Z", "2022-01-15T01:18:01Z", "2021-02-16T08:39:14Z", "2020-02-14T22:49:35Z", "2019-06-17T20:00:28Z", "2019-06-17T18:43:26Z", "2019-05-09T19:07:24Z", "2019-03-06T23:08:22Z", "2019-02-20T03:18:53Z", "2019-02-17T19:33:04Z", "2019-02-14T23:00:50Z", "2019-01-31T00:31:23Z", "2019-01-09T00:09:29Z", "2019-06-17T21:07:17Z"]}, {"name": "amazon-gamelift-plugin-unity", "description": "The Amazon GameLift Plugin for Unity contains libraries and native UI that makes it easier to access GameLift resources and integrate GameLift into your Unity game. You can use the GameLift Unity Plugin to access GameLift APIs and deploy AWS CloudFormation templates for common gaming scenarios.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon GameLift Plugin for Unity Engine\r\n\r\n![GitHub license](https://img.shields.io/github/license/aws/amazon-gamelift-plugin-unity)\r\n![GitHub latest release version (by date)](https://img.shields.io/github/v/release/aws/amazon-gamelift-plugin-unity)\r\n![GitHub downloads all releases](https://img.shields.io/github/downloads/aws/amazon-gamelift-plugin-unity/total)\r\n![GitHub downloads latest release (by date)](https://img.shields.io/github/downloads/aws/amazon-gamelift-plugin-unity/latest/total)\r\n\r\nCompatible with Unity 2021.3 LTS and 2022.3 LTS.\r\n\r\n# Overview\r\n\r\nAmazon GameLift is a fully managed service that lets game developers to manage and scale dedicated game servers for session-based multiplayer games. The Amazon GameLift plugin for Unity provides tools that makes it quicker and easier to set up your Unity project for hosting on Amazon GameLift. Once the plugin is installed, you can access the plugin from within the Unity editor and start using it to integrate Amazon GameLift functionality into your client and server code. The plugin contains functionality to automatically bootstrap your game runtime environment to the AWS Cloud, fully test your game server integration with Amazon GameLift locally, and deploy your game servers on Amazon GameLift. For more information about using the plugin for Unity, see the [Amazon GameLift plugin for Unity guide](https://docs.aws.amazon.com/gamelift/latest/developerguide/unity-plug-in.html).\r\n\r\nYou can use built-in templates to deploy your game for the following common scenarios. \r\n* Single-region fleet: Deploy your game server to one fleet in a single AWS Region. Use this scenario to experiment with your install scripts and runtime deployment, as well as your integration.\r\n* FlexMatch fleet: Deploy your game server for hosting with a FlexMatch matchmaking solution. [Amazon GameLift FlexMatch](https://docs.aws.amazon.com/gamelift/latest/flexmatchguide/match-intro.html) is a highly scalable and customizable matchmaking service for multiplayer games. Use this scenario to set up basic matchmaking components (including a rule set) that you can customize.\r\n\r\nEach scenario uses an AWS CloudFormation template to deploy a resource stack for your game server solution. You can view and manage your resource stacks in the AWS Management Console for CloudFormation.\r\n\r\n- [Prerequisites](#prerequisites)\r\n- [Install the plugin](#install-the-plugin)\r\n- [Contributing to this plugin](#contributing-to-this-plugin)\r\n- [FAQ](#faq)\r\n- [Amazon GameLift resources](#amazon-gamelift-resources)\r\n\r\n## Prerequisites\r\n\r\n* Amazon GameLift plugin for Unity download package. Download a zip file from [the GitHub Releases page](https://github.com/aws/amazon-gamelift-plugin-unity/releases). Or clone the plugin from the [Github repo](https://github.com/aws/amazon-gamelift-plugin-unity).\r\n* A compatible Unity editor (2021.3 LTS, 2022.3 LTS) with Dedicated Server Build Support module for Windows (and Linux if desired).\r\n* (Optional) A C# multiplayer game project with game code.\r\n* An AWS account with access permissions to use Amazon GameLift, Amazon S3, and AWS CloudFormation. See [Set up programmatic access](https://docs.aws.amazon.com/gamelift/latest/developerguide/setting-up-aws-login.html) with long-term credentials.\r\n\r\n## Install the plugin\r\n\r\nComplete the following steps to install and enable the plugin for your multiplayer game project. For more details, see the [Amazon GameLift documentation](https://docs.aws.amazon.com/gamelift/latest/developerguide/unity-plug-in-install.html).\r\n\r\n1. Install the Amazon GameLift Plugin for Unity.\r\n    1. Find the `com.amazonaws.gamelift-<version>.tgz` file within the downloaded release zip or follow the [contribution guide](CONTRIBUTING.md) to build the tarball yourself.\r\n    1. In your Unity project, open `Window > Package Manager`.\r\n    1. Click `+ > Add package from tarball...` and select the above tarball.\r\n\r\n1. Install the Amazon GameLift C# Server SDK for Unity plugin (aka. lightweight Unity plugin).\r\n    1. Find and unzip the `GameLift-CSharp-ServerSDK-UnityPlugin-<version>.zip` file within the downloaded release zip or download it from [Amazon GameLift's Getting Started](https://aws.amazon.com/gamelift/getting-started/).\r\n    1. In your Unity project, open `Edit > Project Settings > Package Manager`.\r\n    1. Under `Scoped Registries`, click on the `+` button and enter the values for the [UnityNuGet](https://github.com/xoofx/UnityNuGet) scoped registry:\r\n        ```\r\n        Name: Unity NuGet\r\n        Url: https://unitynuget-registry.azurewebsites.net\r\n        Scope(s): org.nuget\r\n        ```\r\n    1. In your Unity project, open `Window > Package Manager`.\r\n    1. Click `+ > Add package from tarball...` and select the tarball within the unzipped folder, `com.amazonaws.gameliftserver.sdk-<version>.tgz`.\r\n\r\n1. (Optional) Import the sample project and configure the build settings.\r\n    1. In your Unity project, select `Amazon GameLift > Sample Game > Import Sample Game` and import all assets.\r\n    1. In your Unity project, select `Amazon GameLift > Sample Game > Initialize Settings`.\r\n\r\n## Contributing to this plugin\r\n\r\n### Prerequisites\r\n\r\n* Administrator rights on a Microsoft Windows OS\r\n* A supported Unity version\r\n    * You also need to add the Unity editor folder (e.g. `C:\\Program Files\\Unity\\Hub\\Editor\\<version>\\Editor\\ `) to the Windows PATH environment variable.\r\n* Visual Studio 2019 (can be installed with Unity)\r\n* .NET Core 6 to build the core plugin source.\r\n* NodeJS/npm: https://nodejs.org/en/download/ to package the plugin.\r\n\r\n### Modifying the plugin code\r\n\r\n1. Clone the [`amazon-gamelift-plugin-unity`](https://github.com/aws/amazon-gamelift-plugin-unity) repository from GitHub.\r\n1. Run `Scripts~\\windows\\release.ps1 -Sdk <version>` in PowerShell to build the plugin and dependent libraries (only needed once).\r\n1. In Unity Hub, create a new project.\r\n1. Open Unity Package Manager, import project from disk, and select the `package.json` located in the plugin's root folder.\r\n1. Setup code debugging in Unity: https://docs.unity3d.com/Manual/ManagedCodeDebugging.html, and change Unity project to Debug Mode.\r\n1. A .sln file should be created in the Unity project root, you can open that with Visual Studio.\r\n1. Make changes to the plugin code, and Unity should recompile after each change.\r\n1. Once changes are made, run the unit tests via `Window > General > Test Runner`.\r\n\r\n### Packaging the plugin\r\n\r\nRun `Scripts~\\windows\\release.ps1 -Sdk <version>` to clean, build, export, and package the plugin with the server SDK in a single command.\r\n\r\nAlternatively:\r\n1. Run `Scripts~\\windows\\clean.ps1` to delete all dlls and temp files (If you want to build faster, you can comment out `.clean-download-files` execution).\r\n1. Run `Scripts~\\windows\\build.ps1` to build dlls and sample game.\r\n1. Run `Scripts~\\windows\\export.ps1 -Sdk <version>` to export the plugin into a tarball (.tgz) and package it with the server SDK in the project root folder.\r\n\r\n### Testing the plugin\r\n\r\nFollow instructions in [Unity Docs](https://docs.unity3d.com/Manual/cus-tests.html#tests) to enable your project for testing:\r\n1. Open the Project manifest (located at `<project>/Packages/manifest.json`).\r\n1. Verify `com.amazonaws.gamelift` is present as a dependency.\r\n1. Add to the bottom of the file:\r\n\r\n````\r\n    \"testables\": [ \"com.amazonaws.gamelift\" ]\r\n````\r\n\r\nAfter enabling testing, the project tests can be run via [Unity Test Runner](https://docs.unity3d.com/2017.4/Documentation/Manual/testing-editortestsrunner.html).\r\n\r\n## FAQ\r\n\r\n### What Unity versions are supported?\r\n\r\nThe Amazon GameLift Plug-in for Unity is compatible only with officially supported versions of Unity 2021.3 LTS and 2022.3 LTS for Windows and Mac OS.\r\n\r\n### Where are the logs?\r\n\r\nAn additional error log file related to the Unity game project can be found in the following location: `\r\nlogs/amazon-gamelift-plugin-logs[YYYYMMDD].txt`. Note that the log file is created once a day.\r\n\r\n## Amazon GameLift Resources\r\n\r\n* [About Amazon GameLift](https://aws.amazon.com/gamelift/)\r\n* [Amazon GameLift documentation](https://docs.aws.amazon.com/gamelift/)\r\n* [AWS Game Tech forum](https://repost.aws/topics/TAo6ggvxz6QQizjo9YIMD35A/game-tech/c/amazon-gamelift)\r\n* [AWS for Games blog](https://aws.amazon.com/blogs/gametech/)\r\n* [AWS Support Center](https://console.aws.amazon.com/support/home)\r\n* [GitHub issues](https://github.com/aws/amazon-gamelift-plugin-unity/issues)\r\n* [Contributing guidelines](CONTRIBUTING.md)\r\n\r\n", "release_dates": ["2024-02-13T19:58:31Z", "2023-11-16T20:11:59Z", "2023-08-04T17:33:51Z", "2023-04-21T21:08:27Z", "2022-10-31T19:25:25Z", "2021-12-03T20:44:19Z", "2021-10-15T18:29:16Z", "2021-10-01T22:26:57Z", "2021-09-18T00:11:39Z", "2021-09-17T06:10:34Z", "2021-09-16T05:48:46Z", "2021-09-13T16:34:44Z", "2021-09-10T09:20:13Z", "2021-08-27T01:33:54Z"]}, {"name": "amazon-gamelift-plugin-unreal", "description": "The Amazon GameLift Plugin for Unreal contains libraries and native UI that makes it easier to access GameLift resources and integrate GameLift into your Unreal game. You can use the GameLift Unreal Plugin to access GameLift APIs and deploy AWS CloudFormation templates for common gaming scenarios.", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon GameLift Plugin for Unreal Engine\n\n![GitHub](https://img.shields.io/github/license/aws/amazon-gamelift-plugin-unreal)\n![GitHub release (latest by date)](https://img.shields.io/github/v/release/aws/amazon-gamelift-plugin-unreal)\n![GitHub all releases](https://img.shields.io/github/downloads/aws/amazon-gamelift-plugin-unreal/total)\n![GitHub release (latest by date)](https://img.shields.io/github/downloads/aws/amazon-gamelift-plugin-unreal/latest/total)\n\nCompatible with Unreal Engine 5 (versions 5.0, 5.1, 5.2 and 5.3).\n\n# Overview\n\nAmazon GameLift is a fully managed service that allows game developers to manage and scale dedicated game servers for session-based multiplayer games. The Amazon GameLift plugin for Unreal Engine provides tools that makes setting up your Unreal project, compatible with UE 5.0, 5.1, 5.2 and 5.3, for hosting on Amazon GameLift quicker and easier. Once installed, you will be able to search the plugin from within the Unreal Engine editor and start using it to integrate Amazon GameLift functionality into your client and server code. The plugin contains functionality to automatically bootstrap your game runtime environment to the AWS Cloud, fully test your game server integration with Amazon GameLift locally, and deploy your game servers on Amazon GameLift. \n\nYou can use the following built-in templates to deploy your game for some of the common scenarios. \n* Single-region fleet: Deploy your game server to one fleet in a single AWS Region. Use this scenario to experiment with your install scripts and runtime deployment, as well as your integration.\n* FlexMatch fleet: Deploy your game server for hosting with a FlexMatch matchmaking solution. [Amazon GameLift's FlexMatch](https://docs.aws.amazon.com/gamelift/latest/flexmatchguide/match-intro.html) is a highly scalable and customizable matchmaking service for multiplayer games. Use this scenario to set up basic matchmaking components (including a rule set) that you can customize.\n\nEach scenario uses an AWS CloudFormation template to  deploy your game, creating a stack with the necessary resources. You can view and manage your resource stacks in the AWS Management Console for CloudFormation. \n\n- [Prerequisites](#prerequisites)\n- [Install the plugin](#install-the-plugin)\n- [Amazon GameLift resources](#amazon-gamelift-resources)\n\n## Prerequisites\n\n* Amazon GameLift plugin for Unreal download package. Download a zip file from [the GitHub Releases page](https://github.com/aws/amazon-gamelift-plugin-unreal/releases). Or clone the plugin from the [Github repo](https://github.com/aws/amazon-gamelift-plugin-unreal). \n* If you cloned the repo you will also need to download the following items from [Amazon GameLift's Getting Started](https://aws.amazon.com/gamelift/getting-started/). Otherwise, if you downloaded the zip file from [the GitHub Releases page](https://github.com/aws/amazon-gamelift-plugin-unreal/releases), you can skip this step.\n    * C++ Server SDK Plugin for Unreal\n    * C++ Server SDK\n* Microsoft Visual Studio 2019 or newer.\n* A source-built version of the Unreal Engine editor. Required to develop server build components for a multiplayer game. See the Unreal Engine documentation: \n    * [Accessing Unreal Engine source code on GitHub](https://www.unrealengine.com/ue-on-github). Requires  GitHub and Epic Games accounts.\n    * [Building Unreal Engine from Source](https://docs.unrealengine.com/5.3/en-US/building-unreal-engine-from-source/) \n* (Optional) A C++ multiplayer game project with game code. Projects that use Blueprints only are not compatible with this plugin, at this time.\n* An AWS account with access permissions to use AWS GameLift. See [Set up programmatic access](https://docs.aws.amazon.com/gamelift/latest/developerguide/setting-up-aws-login.html) with long-term credentials.\n\n## Install the plugin\n\nComplete the following steps to install and enable the plugin for your multiplayer game project. For more details, see the [AWS GameLift documentation](https://docs.aws.amazon.com/gamelift/latest/developerguide/unreal-plugin.html).\n\n1. Build the Amazon GameLift C++ Server SDK. See section below for details.\n\n1. Install and enable the plugin.\n    1. In your game project root folder, create a folder called \"Plugins\" and copy the \"GameLiftPlugin\" folder located inside the downloaded Amazon GameLift plugin into this new folder.\n    1. In the `.uproject` file, add the following to the `Plugins` section: \n        \n        ```\n        {\n            \"Name\": \"GameLiftPlugin\",\n            \"Enabled\": true\n        },\n        {\n            \"Name\": \"WebBrowserWidget\",\n            \"Enabled\": true\n        }\n        ```\n1. Set your project to use the source-built UE editor. Do this step if your game project was created with a non-source-built version of UE. In your game project folder, select the `.uproject` file and choose the option **Switch Unreal Engine Version**.\n\n1. Rebuild the project solution. After completing the previous steps to update your project files, rebuild the solution. \n\n## Build the Amazon GameLift C++ Server SDK\n\nBefore you can use the plugin inside an Unreal game, you need to build the Amazon GameLift server C++ SDK.  \n\nTo build the Amazon GameLift server SDK:\n1. Open a terminal/command prompt.\n1. Navigate to the `GameLift-Cpp-ServerSDK-<version>` folder that was included with the Amazon GameLift SDK Release download.\n1. Follow the below instructions for your platform.  \n\n### Linux\n\n1. Run the following commands\n    ```sh\n    mkdir out\n    cd out\n    cmake -DBUILD_FOR_UNREAL=1 ..\n    make\n    ```\n1. Once complete, the following file should have been built\n    ```\n    prefix/lib/aws-cpp-sdk-gamelift-server.so\n    ```\n1. Copy the file over to the following location in the Unreal plugin folder: \n    ```\n    GameLiftPlugin/Source/GameLiftServer/ThirdParty/GameLiftServerSDK/Linux/x86_64-unknown-linux-gnu/\n    ```  \n    Once complete you should have a filepath similar to this example\n    ```\n    GameLiftPlugin/Source/GameLiftServer/ThirdParty/GameLiftServerSDK/Linux/x86_64-unknown-linux-gnu/aws-cpp-sdk-gamelift-server.so \n    ```\n\n### Windows\n\n1. Run the following commands\n    ```sh\n    mkdir out\n    cd out\n    cmake -G \"Visual Studio 17 2022\" -DBUILD_FOR_UNREAL=1 ..\n    msbuild ALL_BUILD.vcxproj /p:Configuration=Release\n    ```\n1. The above step produces the following binary files required by the plugin.  \n    ```\n    prefix\\bin\\aws-cpp-sdk-gamelift-server.dll  \n    prefix\\lib\\aws-cpp-sdk-gamelift-server.lib\n    ```\n1. Copy the files over to this location in the Unreal plugin folder:\n    ```\n    GameLiftPlugin\\Source\\GameLiftServer\\ThirdParty\\GameLiftServerSDK\\Win64\\\n    ```  \n    Once complete you should have two filepaths similar to this example  \n    ```\n    GameLiftPlugin\\Source\\GameLiftServer\\ThirdParty\\GameLiftServerSDK\\Win64\\aws-cpp-sdk-gamelift-server.dll  \n    GameLiftPlugin\\Source\\GameLiftServer\\ThirdParty\\GameLiftServerSDK\\Win64\\aws-cpp-sdk-gamelift-server.lib \n    ```\n\nFor more detailed instructions on how to build the C++ SDK,, please refer to the README.md file located in the C++ SDK directory.\n\n## Amazon GameLift Resources \n\n* [About Amazon GameLift](https://aws.amazon.com/gamelift/)\n* [Amazon GameLift documentation](https://docs.aws.amazon.com/gamelift/)\n* [AWS Game Tech forum](https://repost.aws/topics/TAo6ggvxz6QQizjo9YIMD35A/game-tech/c/amazon-gamelift)\n* [AWS for Games blog](https://aws.amazon.com/blogs/gametech/)\n* [Contributing guidelines](CONTRIBUTING.md)\n", "release_dates": ["2024-02-13T22:47:14Z", "2023-09-28T20:10:08Z"]}, {"name": "amazon-genomics-cli", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon Genomics CLI\n\n[![Join the chat at https://gitter.im/aws/amazon-genomics-cli](https://badges.gitter.im/aws/amazon-genomics-cli.svg)](https://gitter.im/aws/amazon-genomics-cli?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## Overview\n\nThe Amazon Genomics CLI is a tool to simplify the processes of deploying the AWS infrastructure required to\nrun genomics workflows in the cloud, to submit those workflows to run, and to monitor the logs and outputs of those workflows.\n\n## Quick Start\n\nTo get an introduction to Amazon Genomics CLI refer to the [Quick Start Guide](https://aws.github.io/amazon-genomics-cli/docs/getting-started/)\nin our wiki.\n\n## Further Reading\n\nFor full documentation, please refer to our [docs](https://aws.github.io/amazon-genomics-cli/docs/).\n\n## Releases\n\nAll releases can be accessed on our releases [page](https://github.com/aws/amazon-genomics-cli/releases).\n\nThe latest nightly build can be accessed here: `s3://healthai-public-assets-us-east-1/amazon-genomics-cli/nightly-build/amazon-genomics-cli.zip`\n\n## Development\n\nTo build from source you will need to ensure the following prerequisites are met.\n\n### One-time setup\n\nThere are a few prerequisites you'll need to install on your machine before you can start developing.\n\nOnce you've installed all the dependencies listed here, run `make init` to install the rest.\n\n#### Go\n\nThe Amazon Genomics CLI is written in Go.\n\nTo manage and install Go versions, we use [goenv](https://github.com/syndbg/goenv). Follow the installation\ninstructions [here](https://github.com/syndbg/goenv/blob/master/INSTALL.md).\n\nOnce goenv is installed, use it to install the version of Go required by the\nAmazon Genomics CLI build process, so that it will be available when the build\nprocess invokes goenv's `go` shim:\n\n```bash\ngoenv install\n```\n\nYou will need to do this step again whenever the required version of Go is\nchanged.\n\n#### Node\n\nAmazon Genomics CLI makes use of the AWS CDK to deploy infrastructure into an AWS account. Our CDK code is written in TypeScript.\nYou'll need Node to ensure the appropriate dependencies are installed at build time.\n\nTo manage and install Node versions, we use [nvm](https://github.com/nvm-sh/nvm).\n\n```bash\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash\necho 'export NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] && printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"' >> ~/.bashrc\necho '[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\" # This loads nvm' >> ~/.bashrc\nsource ~/.bashrc\nnvm install\n```\n\nNote: If you are using Zsh, replace `~/.bashrc` with `~/.zshrc`.\n\n#### Sed (OSX)\n\nOSX uses an outdated version of [sed](https://www.gnu.org/software/sed/manual/sed.html). If you are on a Mac, you will\nneed to use a newer version of sed to ensure script compatibility.\n\n```bash\nbrew install gnu-sed\necho 'export PATH=\"$(brew --prefix gnu-sed)/libexec/gnubin:$PATH\"' >> ~/.bashrc\nsource ~/.bashrc\n```\n\nNote: If you are using Zsh, replace `~/.bashrc` with `~/.zshrc`.\n\n#### One time setup\n\nOnce you've installed all the dependencies listed here, run `make init` to automatically install all remaining dependencies.\n\n\n#### Make\n\nWe use `make` to build, test and deploy artifacts. To build and test issue the `make` command from the project root.\n\nIf you're experiencing build issues, try running `go clean --cache` in the project root to clean up your local go build cache. Then try to run `make init` then `make` again. This should ideally resolve it.\n\n### Running Development Code\n\n#### Option 1. Running with local release\nUnlike running 'run-dev.sh' script, this option will build and install a new version of Amazon Genomics CLI, replacing\nthe one installed. To run a release version of Amazon Genomics CLI from your local build, first build your changes and then run `make release`.\nThis will create a release bundle `dist/` at this package root directory. Run the `install.sh` script in the `dist` folder\nto install your local release version of Amazon Genomics CLI. After installing, you should be able to run `agc` on the terminal.\n\n#### Option 2. Running with development script\nTo run against a development version of Amazon Genomics CLI, first build your relevant changes and then run `./scripts/run-dev.sh`. This will\nset the required environment variables and then enter into an Amazon Genomics CLI command shell.\n\n#### Option 3.  Running from development code manually + Custom images \n* Update dependencies and build code with `make init && make`. At this point the compiled binary will be found at `packages/cli/bin/local/agc`.\n* Optionally, you may build the install package and install the binary and CDK libraries. `make release && (cd dist/amazon-genomics-cli && ./install.sh)`\n* Before creating any contexts, ensure you have the relevant environment variables set  to point to the ECR repository holding the images of the engines you wish to test. Leave these values unset to test against production images.\n\n```shell\nexport ECR_CROMWELL_ACCOUNT_ID=<some-value>\nexport ECR_CROMWELL_REGION=<some-value>\nexport ECR_CROMWELL_TAG=<some-value>\nexport ECR_CROMWELL_REPOSITORY=<some-value>\n\nexport ECR_NEXTFLOW_ACCOUNT_ID=<some-value>\nexport ECR_NEXTFLOW_REGION=<some-value>\nexport ECR_NEXTFLOW_TAG=<some-value>\nexport ECR_NEXTFLOW_REPOSITORY=<some-value>\n\nexport ECR_MINIWDL_ACCOUNT_ID=<some-value>\nexport ECR_MINIWDL_REGION=<some-value>\nexport ECR_MINIWDL_TAG=<some-value>\nexport ECR_MINIWDL_REPOSITORY=<some-value>\n\nexport ECR_TOIL_ACCOUNT_ID=<some-value>\nexport ECR_TOIL_REGION=<some-value>\nexport ECR_TOIL_TAG=<some-value>\nexport ECR_TOIL_REPOSITORY=<some-value>\n```\n\nThese environment variables point to the ECR account, region, repository and tags of the Cromwell, Nextflow, MiniWDL, and Toil engine respectively\nthat will be deployed for your contexts. They are used when you create a context using the corresponding engine types.\n\nThe `./scripts/run-dev.sh` contains logic to determine the current\ndev versions of the images which you would typically use. You may also use production images, the current values of which will\nbe written when you activate an account with the production version of Amazon Genomics CLI. If you have customized containers that you\nwant to develop against you can specify these however you will need to make these available if you wish to make pull requests\nwith code that depends on them.\n\n### Building locally with CodeBuild\n\nThis package is buildable with AWS CodeBuild. You can use the AWS CodeBuild agent to run CodeBuild builds on a local\nmachine.\n\nYou only need to set up the build image the first time you run the agent, or when the image has changed. To set up the\nbuild image, use the following commands:\n\n```bash\ngit clone https://github.com/aws/aws-codebuild-docker-images.git\ncd aws-codebuild-docker-images/ubuntu/standard/5.0\ndocker build -t aws/codebuild/standard:5.0 .\ndocker pull amazon/aws-codebuild-local:latest --disable-content-trust=false\n```\n\nCreate an environment file (e.g. `env.txt`) with the appropriate entries depending on which image tags you want to use.\n\n```shell\nCROMWELL_ECR_TAG=2021-06-17T23-48-54Z\nECR_NEXTFLOW_TAG=2021-06-17T23-48-54Z\nWES_ECR_TAG=2021-06-17T23-48-54Z\n```\n\nIn the root directory for this package, download and run the CodeBuild build script:\n\n```bash\nwget https://raw.githubusercontent.com/aws/aws-codebuild-docker-images/master/local_builds/codebuild_build.sh\nchmod +x codebuild_build.sh\n./codebuild_build.sh -i aws/codebuild/standard:5.0 -a ./output -c -e env.txt\n```\n\n### Configuring docker image location\n\nThe default values for all variables are placeholders (e.g. 'WES_ECR_TAG_PLACEHOLDER'). It is replaces by the actual\nvalue during a build process.\n\n#### WES adapter for Cromwell\n\nLocal environment variables:\n\n- `ECR_WES_ACCOUNT_ID`\n- `ECR_WES_REGION`\n- `ECR_WES_TAG`\n\nThe corresponding AWS Systems Manager Parameter Store property names:\n\n- /agc/_common/wes/ecr-repo/account\n- /agc/_common/wes/ecr-repo/region\n- /agc/_common/wes/ecr-repo/tag\n\n#### Cromwell engine\n\nLocal environment variables:\n\n- `ECR_CROMWELL_ACCOUNT_ID`\n- `ECR_CROMWELL_REGION`\n- `ECR_CROMWELL_TAG`\n\nThe corresponding AWS Systems Manager Parameter Store property names:\n\n- /agc/_common/cromwell/ecr-repo/account\n- /agc/_common/cromwell/ecr-repo/region\n- /agc/_common/cromwell/ecr-repo/tag\n\n#### Nextflow engine\n\nLocal environment variables:\n\n- `ECR_NEXTFLOW_ACCOUNT_ID`\n- `ECR_NEXTFLOW_REGION`\n- `ECR_NEXTFLOW_TAG`\n\nThe corresponding AWS Systems Manager Parameter Store property names:\n\n- /agc/_common/nextflow/ecr-repo/account\n- /agc/_common/nextflow/ecr-repo/region\n- /agc/_common/nextflow/ecr-repo/tag\n\n## Contributing\n\n### Issues\n\nSee [Reporting Bugs/Feature Requests](CONTRIBUTING.md#reporting-bugsfeature-requests) for more information. For a list of\nopen bugs and feature requests, please refer to our [issues](https://github.com/aws/amazon-genomics-cli/issues?q=is%3Aopen+is%3Aissue) page.\n\n### Pull Requests\n\nSee [Contributing via Pull Requests](CONTRIBUTING.md#contributing-via-pull-requests)\n\n## Security\n\nSee [Security Issue Notification](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2022-12-29T21:35:25Z", "2022-10-31T18:54:45Z", "2022-09-20T19:54:06Z", "2022-05-25T20:21:02Z", "2022-04-20T17:02:37Z", "2022-03-07T19:26:39Z", "2022-02-17T17:46:23Z", "2022-01-19T03:39:43Z", "2021-11-29T18:25:04Z", "2021-11-17T07:18:28Z", "2021-11-11T03:58:51Z", "2021-10-02T02:36:14Z", "2021-09-27T04:03:57Z"]}, {"name": "amazon-inspector-container-image-scanner-jenkins-plugin", "description": null, "language": "HTML", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "This plugin gives you the ability to add Amazon Inspector vulnerability scans to your pipeline. These scans leverage the Inspector SBOM generator binary and Amazon Inspector Scan API to produce detailed reports at the end of your build so you can investigate and remediate risk before deployment. The scans can also be configured to pass or fail pipeline executions based on the number and severity of vulnerabilities detected.\n\nAmazon Inspector is a vulnerability management service offered by AWS that scans container images for both operating system and programming language package vulnerabilities based on CVEs. For more information on Amazon Inspector's CI/CD integration see [Integrating Amazon Inspector scans into your CI/CD pipeline](https://docs.aws.amazon.com/inspector/latest/user/scanning-cicd.html).\n\nFor a list of packages and container image formats the Inspector plugin supports see, [Supported packages and image formats](https://docs.aws.amazon.com/inspector/latest/user/sbom-generator.html#sbomgen-supported).\n\nFollow the steps in each section of this document to use the Inspector Jenkins plugin:\n\n#### 1. Set up an AWS account\n* Configure an AWS account with an IAM role that allows access to the Inspector SBOM scanning API. For instructions, see [Setting up an AWS account to use the Amazon Inspector CI/CD integration](https://docs.aws.amazon.com/inspector/latest/user/configure-cicd-account.html)\n\n#### 2. Install the Inspector Jenkins Plugin\n1. From your Jenkins dashboard, go to **Manage Jenkins > Manage Plugins** and select the **Available** tab.\n2. Search for **Amazon Inspector Scans**.\n3. Install the plugin.\n\n#### 3. Install the Inspector SBOM Generator\n* Install and configure the Amazon Inspector SBOM Generator. For instructions, see [Installing Amazon Inspector SBOM Generator (Sbomgen)](https://docs.aws.amazon.com/inspector/latest/user/sbom-generator.html)\n\n#### 4. Add your Docker credentials to Jenkins\n1. Go to **Dashboard > Manage Jenkins > Credentials > System > Global credentials > Add credentials**.\n2. Fill in details and select **Create**.\n\n#### 5. Add an Amazon Inspector Scan build step to your project\n1. On the configuration page, scroll down to **Build Steps**, select **Add build step** and select **Amazon Inspector Scan**.\n2. Configure the Amazon Inspector Scan build step by filling in following details:\n    * Select one of the following Inspector-SBOMGen installation methods:\n        * Automatic: Download the most recent version of inspector-sbomgen, based on operating system and CPU architecture.\n        * Manual: Provide an absolute path to a downloaded version of inspector-sbomgen.\n          * Download: https://docs.aws.amazon.com/inspector/latest/user/sbom-generator.html#install-sbomgen\n    * For **Path to inspector-sbomgen** add the installation path to your Amazon Inspector SBOM Generator generator.\n    * For **Image Id** input the path to your image. Your image can be local, remote, or archived. Image names should follow the Docker naming convention. If analyzing an exported image, provide the path to the expected tar file. See the following example Image Id paths:\n        * For local or remote containers: `NAME[:TAG|@DIGEST]`\n        * For a tar file: `/path/to/image.tar`\n    * For private registry and container images, please refer to https://docs.aws.amazon.com/inspector/latest/user/sbom-generator.html\n    * Select an **AWS Region** to send the scan request through.\n    * For **IAM Role** enter the ARN for the role you configured in step 1.\n    * For **Docker credentials** select your Docker username.\n    * (Optional) Specify the **Vulnerability thresholds** per severity. If the number you specify is exceeded during a scan the image build will fail. If the values are all 0 the build will succeed regardless of the number of vulnerabilities found.\n3. Select **Save**.\n\n#### 6. View your Amazon Inspector vulnerability report\n1. Complete a new build of your project.\n2. When the build completes select an output format from the results.\n3. (Optional) Enable CSS support in Jenkins script console to allow HTML report links to open: https://www.jenkins.io/doc/book/security/user-content/\n\n### Troubleshooting\n\nIssue #1: If you receive the following error:\n\nInstanceProfileCredentialsProvider(): Failed to load credentials from IMDS.\n\nResolution : Set up aws_access_key_id and aws_secret_access_key in ~/.aws/credential\n\n### Known Limitations and Issues\n\n* Support for Windows OS and macOS is not provided at this time.\n* Sbomgen was load tested against container images spanning 5 GB in size, 60 layers, and 2,000 installed packages. Sbomgen should be able to inventory images of this size within 5 minutes; however, this may vary depending on the configuration of your image and available hardware resources.\n* Sbomgen prioritizes accuracy and low false positive rates, which often comes at the expense of speed.\n* Sbomgen only generates SBOMs - it does not perform vulnerability identification at this time.\n* Sbomgen only generates SBOMs in CycloneDX + JSON format at this time.\n", "release_dates": []}, {"name": "amazon-inspector-container-image-scanner-teamcity-plugin", "description": null, "language": "HTML", "license": null, "readme": "This plugin gives you the ability to add Amazon Inspector vulnerability scans to your pipeline. These scans produce detailed reports at the end of your build so you can investigate and remediate risk before deployment. These scans can also be configured to pass or fail pipeline executions based on the number and severity of vulnerabilities detected.\n\nAmazon Inspector is a vulnerability management service offered by AWS that scans container images for both operating system and programming language package vulnerabilities based on CVEs.  For more information on Amazon Inspector\u2019s CI/CD integration see [Integrating Amazon Inspector scans into your CI/CD pipeline](https://docs.aws.amazon.com/inspector/latest/user/scanning-cicd.html).\n\nFor a list of packages and container image formats the Inspector plugin supports see, [Supported packages and image formats](https://docs.aws.amazon.com/inspector/latest/user/sbom-generator.html#sbomgen-supported).\n\nFollow the steps in each section of this document to use the Inspector TeamCity plugin:\n\n1. Set up an AWS account\n\n* Configure an AWS account with an IAM role that allows access to the Inspector SBOM scanning API. For instructions, see [Setting up an AWS account to use the Amazon Inspector CI/CD integration](https://docs.aws.amazon.com/inspector/latest/user/configure-cicd-account.html)\n\n3. Install the Inspector TeamCity Plugin\n\n1. From your dashboard, go to Administration > Plugins.\n2. Search for Amazon Inspector Scans.\n3. Install the plugin.\n\n2. Install and configure the Inspector SBOM Generator\n\n* Install and configure the Amazon Inspector SBOM Generator. For instructions, see [Installing Amazon Inspector SBOM Generator (Sbomgen)](https://docs.aws.amazon.com/inspector/latest/user/sbom-generator.html#install-sbomgen)\n\n4. Add Inspector scan to your project\n\n2. On the configuration page, choose Build Steps, click Add build step and select Amazon Inspector Scan.\n3. Configure the Amazon inspector Scan build step by filling in following details:\n    1. Add a Step name.\n    2. For Image Id input the path to your image.\n    3. For Path to SBOM generator Binary add the installation path to your Amazon Inspector SBOM Generator.\n    4. For Role Arn the ARN for the role you configured in step 1.\n    5. Select a Region to send the scan request through.\n    6. For Docker Authentication add your Docker Username and Docker Password.\n    7. [Optional] Specify the Maximum Vulnerabilities Allowed based on severity. If the maximum is exceeded during a scan the image build will fail. If the values are all 0 the build will succeed regardless of the number of vulnerabilities found.\n4. Select Save.\n\n5. View your Inspector scan report\n\n1. Complete a new build of your project.\n2. When the build completes select an output format from the results. When you select HTML you have the option to download a JSON SBOM or CSV version of the report.\n\n### Known Limitations and Issues\n\n* Support for Windows OS and macOS is not provided at this time.\n* Sbomgen was load tested against container images spanning 5 GB in size, 60 layers, and 2,000 installed packages. Sbomgen should be able to inventory images of this size within 5 minutes; however, this may vary depending on the configuration of your image and available hardware resources.\n* Sbomgen prioritizes accuracy and low false positive rates, which often comes at the expense of speed.\n* Sbomgen ONLY generates SBOMs - it does not perform vulnerability identification at this time.\n* Sbomgen only generates SBOMs in CycloneDX + JSON format at this time.\n\n", "release_dates": []}, {"name": "amazon-ivs-react-native-player", "description": "A React Native wrapper for the Amazon IVS iOS and Android player SDKs.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon IVS React Native Player\n\nThis package implements native binding for Amazon IVS Player for iOS and Android.\n\n## Installation\n\n- install `amazon-ivs-react-native-player` dependency using yarn or npm\n\n```sh\nnpm install amazon-ivs-react-native-player\n```\n\n- install pods for your ios project. Go to `ios` directory and run\n\n```sh\npod install\n```\n\n## Usage\n\n```tsx\nimport IVSPlayer from 'amazon-ivs-react-native-player';\n\nconst URL =\n  'https://fcc3ddae59ed.us-west-2.playback.live-video.net/api/video/v1/us-west-2.893648527354.channel.DmumNckWFTqz.m3u8';\n\nfunction App() {\n  return <IVSPlayer streamUrl={URL} />;\n}\n```\n\nA more detailed guide about usage can be found [here](./docs/usage-guide.md)\n\n## IVSPlayer component\n\n`IVSPlayer` is a component that can render a video stream based on the passed URL.\n\n- [Props documentation](./docs/ivs-player-reference.md#props)\n- [Ref methods documentation](./docs/ivs-player-reference.md#ref-methods)\n\n## Contributing\n\nSee the [contributing guide](CONTRIBUTING.md) to learn how to contribute to the repository and the development workflow.\n\n## Troubleshooting\n\nTo hide Home Indicator on iOS when video is in full screen, use this library:\nhttps://github.com/flowkey/react-native-home-indicator\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2024-02-20T21:02:52Z", "2023-08-09T21:39:24Z"]}, {"name": "amazon-keyspaces-cql-to-cfn-converter", "description": "Command-line tool for converting Apache Cassandra Query Language (CQL) scripts to AWS CloudFormation (CFN) templates, so Amazon Keyspaces (for Apache Cassandra) schema can be easily managed in AWS CloudFormation stacks.", "language": "Kotlin", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# IMPORTANT: Latest Version\n\nThe current version is 1.0.0. Please see the [changelog](./CHANGELOG.md) for details on version history.\n\n# Overview\n\nThis package implements a command-line tool for converting Apache Cassandra Query Language (CQL) scripts to AWS CloudFormation (CFN) templates, which allows Amazon Keyspaces (for Apache Cassandra) schema to be easily managed in AWS CloudFormation stacks.\n\nThe tool currently supports the following statements:\n\n* `CREATE KEYSPACE`\n* `CREATE TABLE`\n* `USE`\n\n# Example Usage\n\nGiven a CQL script named `my_cql_script.cql`\n\n```sql\nCREATE KEYSPACE my_keyspace WITH replication = {'class': 'SingleRegionStrategy'};\nUSE my_keyspace;\nCREATE TABLE my_table (pk int PRIMARY KEY);\n```\n\nrun the tool by executing\n\n```sh\n$ cql2cfn my_cql_script.cql my_cfn_template.json\n```\n\nthis will produce a CFN template named `my_cfn_template.json`\n\n\n```json\n{\n  \"Resources\": {\n    \"Keyspace1\": {\n      \"Type\": \"AWS::Cassandra::Keyspace\",\n      \"Properties\": {\n        \"KeyspaceName\": \"my_keyspace\"\n      }\n    },\n    \"Table1\": {\n      \"Type\": \"AWS::Cassandra::Table\",\n      \"Properties\": {\n        \"KeyspaceName\": {\n          \"Ref\": \"Keyspace1\"\n        },\n        \"PartitionKeyColumns\": [\n          {\n            \"ColumnName\": \"pk\",\n            \"ColumnType\": \"int\"\n          }\n        ],\n        \"ClusteringKeyColumns\": [],\n        \"RegularColumns\": [],\n        \"TableName\": \"my_table\"\n      }\n    }\n  }\n}\n```\n\nThe tool requires one argument, the path to the CQL script to be converted. An optional second argument can be supplied to specify the path to the generated CFN template; otherwise, the generated CFN template is printed out to the standard output.\n\nBy default, the tool operates in relaxed mode, in which case it only gives warnings about the following issues:\n\n* A property specified for a keyspace/table is not applicable to or supported by Keyspaces\n* A keyspace/table is created a second time without `IF NOT EXISTS`\n* A keyspace is referenced without being created before\n\nThese warnings can be turned into errors by supplying the `--strict` option to the tool.\n\nA help message can be printed out by using the `--help` option.\n\n# How to build the tool\n\nThe tool needs to be built from source by running the following on Linux/MacOS...\n\n```sh\n./gradlew installDist\n```\n\non Microsoft Windows, run... \n\n```bat\ngradlew.bat installDist\n```\n\nJDK 11 or later is required for building the tool.\n\nAfter it is built successfully, the executable can be found in `./build/install/cql2cfn/bin/cql2cfn`.\n\n\n## Testing\nTo execute tests on Linux/MacOS use...\n\n```sh\n./gradlew test\n```\n\nOn Microsoft Windows use... \n\n```bat\ngradlew.bat test\n```\n\n\n", "release_dates": []}, {"name": "amazon-kinesis-firehose-for-fluent-bit", "description": "A Fluent Bit output plugin for Amazon Kinesis Data Firehose", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "[![Test Actions Status](https://github.com/aws/amazon-kinesis-firehose-for-fluent-bit/workflows/Build/badge.svg)](https://github.com/aws/amazon-kinesis-firehose-for-fluent-bit/actions)\n## Fluent Bit Plugin for Amazon Kinesis Firehose\n\n**NOTE: A new higher performance Fluent Bit Firehose Plugin has been released.** Check out our [official guidance](#new-higher-performance-core-fluent-bit-plugin).\n\nA Fluent Bit output plugin for Amazon Kinesis Data Firehose.\n\n#### Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or email AWS security directly at [aws-security@amazon.com](mailto:aws-security@amazon.com).\n\n### Usage\n\nRun `make` to build `./bin/firehose.so`. Then use with Fluent Bit:\n```\n./fluent-bit -e ./firehose.so -i cpu \\\n-o firehose \\\n-p \"region=us-west-2\" \\\n-p \"delivery_stream=example-stream\"\n```\n\nFor building Windows binaries, we need to install `mingw-64w` for cross-compilation. The same can be done using-\n```\nsudo apt-get install -y gcc-multilib gcc-mingw-w64\n```\nAfter this step, run `make windows-release` to build `./bin/firehose.dll`. Then use with Fluent Bit on Windows:\n```\n./fluent-bit.exe -e ./firehose.dll -i dummy `\n-o firehose `\n-p \"region=us-west-2\" `\n-p \"delivery_stream=example-stream\"\n```\n### Plugin Options\n\n* `region`: The region which your Firehose delivery stream(s) is/are in.\n* `delivery_stream`: The name of the delivery stream that you want log records sent to.\n* `data_keys`: By default, the whole log record will be sent to Kinesis. If you specify a key name(s) with this option, then only those keys and values will be sent to Kinesis. For example, if you are using the Fluentd Docker log driver, you can specify `data_keys log` and only the log message will be sent to Kinesis. If you specify multiple keys, they should be comma delimited.\n* `log_key`: By default, the whole log record will be sent to Firehose. If you specify a key name with this option, then only the value of that key will be sent to Firehose. For example, if you are using the Fluentd Docker log driver, you can specify `log_key log` and only the log message will be sent to Firehose.\n* `role_arn`: ARN of an IAM role to assume (for cross account access).\n* `endpoint`: Specify a custom endpoint for the Kinesis Firehose API.\n* `sts_endpoint`: Specify a custom endpoint for the STS API; used to assume your custom role provided with `role_arn`.\n* `time_key`: Add the timestamp to the record under this key. By default the timestamp from Fluent Bit will not be added to records sent to Kinesis. The timestamp inserted comes from the timestamp that Fluent Bit associates with the log record, which is set by the input that collected it. For example, if you are reading a log file with the [tail input](https://docs.fluentbit.io/manual/pipeline/inputs/tail), then the timestamp for each log line/record can be obtained/parsed by using a Fluent Bit parser on the log line.\n* `time_key_format`: [strftime](http://man7.org/linux/man-pages/man3/strftime.3.html) compliant format string for the timestamp; for example, `%Y-%m-%dT%H:%M:%S%z`. This option is used with `time_key`. You can also use `%L` for milliseconds and `%f` for microseconds. Remember that the `time_key` option only inserts the timestamp Fluent Bit has for each record into the record. So the record must have been collected with a timestamp with precision in order to use sub-second precision formatters. If you are using ECS FireLens, make sure you are running Amazon ECS Container Agent v1.42.0 or later, otherwise the timestamps associated with your stdout & stderr container logs will only have second precision.\n* `replace_dots`: Replace dot characters in key names with the value of this option. For example, if you add `replace_dots _` in your config then all occurrences of `.` will be replaced with an underscore. By default, dots will not be replaced.\n* `simple_aggregation`: Option to allow plugin send multiple log events in the same record if current record not exceed the maximumRecordSize (1 MiB). It joins together as many log records as possible into a single Firehose record and delimits them with newline. It's good to enable if your destination supports aggregation like S3. Default to be `false`, set to `true` to enable this option.\n### Permissions\n\nThe plugin requires `firehose:PutRecordBatch` permissions.\n\n### Credentials\n\nThis plugin uses the AWS SDK Go, and uses its [default credential provider chain](https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html). If you are using the plugin on Amazon EC2 or Amazon ECS or Amazon EKS, the plugin will use your EC2 instance role or [ECS Task role permissions](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html) or [EKS IAM Roles for Service Accounts for pods](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html). The plugin can also retrieve credentials from a [shared credentials file](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html), or from the standard `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_SESSION_TOKEN` environment variables.\n\n### Environment Variables\n\n* `FLB_LOG_LEVEL`: Set the log level for the plugin. Valid values are: `debug`, `info`, and `error` (case insensitive). Default is `info`. **Note**: Setting log level in the Fluent Bit Configuration file using the Service key will not affect the plugin log level (because the plugin is external).\n* `SEND_FAILURE_TIMEOUT`: Allows you to configure a timeout if the plugin can not send logs to Firehose. The timeout is specified as a [Golang duration](https://golang.org/pkg/time/#ParseDuration), for example: `5m30s`. If the plugin has failed to make any progress for the given period of time, then it will exit and kill Fluent Bit. This is useful in scenarios where you want your logging solution to fail fast if it has been misconfigured (i.e. network or credentials have not been set up to allow it to send to Firehose).\n\n\n### New Higher Performance Core Fluent Bit Plugin\n\nIn the summer of 2020, we released a [new higher performance Kinesis Firehose plugin](https://docs.fluentbit.io/manual/pipeline/outputs/firehose) named `kinesis_firehose`.\n\nThat plugin has almost all of the features of this older, lower performance and less efficient plugin. Check out its [documentation](https://docs.fluentbit.io/manual/pipeline/outputs/firehose).\n\n#### Do you plan to deprecate this older plugin?\n\nThis plugin will continue to be supported. However, we are pausing development on it and will focus on the high performance version instead.\n\n#### Which plugin should I use?\n\nIf the features of the higher performance plugin are sufficient for your use cases, please use it. It can achieve higher throughput and will consume less CPU and memory.\n\nAs time goes on we expect new features to be added to the C plugin only, however, this is determined on a case by case basis. There is a small feature gap between the two plugins. Please consult the [C plugin documentation](https://docs.fluentbit.io/manual/pipeline/outputs/firehose) and this document for the features offered by each plugin. \n\n#### How can I migrate to the higher performance plugin?\n\nFor many users, you can simply replace the plugin name `firehose` with the new name `kinesis_firehose`. At the time of writing, the only feature missing from the high performance version is the `replace_dots` option. Check out its [documentation](https://docs.fluentbit.io/manual/pipeline/outputs/cloudwatch).\n\n#### Do you accept contributions to both plugins?\n\nYes. The high performance plugin is written in C, and this plugin is written in Golang. We understand that Go is an easier language for amateur contributors to write code in- that is the primary reason we are continuing to maintain this repo.\n\nHowever, if you can write code in C, please consider contributing new features to the [higher performance plugin](https://github.com/fluent/fluent-bit/tree/master/plugins/out_kinesis_firehose).\n\n### Fluent Bit Versions\n\nThis plugin has been tested with Fluent Bit 1.2.0+. It may not work with older Fluent Bit versions. We recommend using the latest version of Fluent Bit as it will contain the newest features and bug fixes.\n\n### Example Fluent Bit Config File\n\n```\n[INPUT]\n    Name        forward\n    Listen      0.0.0.0\n    Port        24224\n\n[OUTPUT]\n    Name   firehose\n    Match  *\n    region us-west-2\n    delivery_stream my-stream\n    replace_dots _\n```\n\n### AWS for Fluent Bit\n\nWe distribute a container image with Fluent Bit and these plugins.\n\n##### GitHub\n\n[github.com/aws/aws-for-fluent-bit](https://github.com/aws/aws-for-fluent-bit)\n\n##### Amazon ECR Public Gallery\n\n[aws-for-fluent-bit](https://gallery.ecr.aws/aws-observability/aws-for-fluent-bit)\n\nOur images are available in Amazon ECR Public Gallery. You can download images with different tags by following command:\n\n```\ndocker pull public.ecr.aws/aws-observability/aws-for-fluent-bit:<tag>\n```\n\nFor example, you can pull the image with latest version by:\n\n```\ndocker pull public.ecr.aws/aws-observability/aws-for-fluent-bit:latest\n```\n\nIf you see errors for image pull limits, try log into public ECR with your AWS credentials:\n\n```\naws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws\n```\n\nYou can check the [Amazon ECR Public official doc](https://docs.aws.amazon.com/AmazonECR/latest/public/get-set-up-for-amazon-ecr.html) for more details.\n\n##### Docker Hub\n\n[amazon/aws-for-fluent-bit](https://hub.docker.com/r/amazon/aws-for-fluent-bit/tags)\n\n##### Amazon ECR\n\nYou can use our SSM Public Parameters to find the Amazon ECR image URI in your region:\n\n```\naws ssm get-parameters-by-path --path /aws/service/aws-for-fluent-bit/\n```\n\nFor more see [our docs](https://github.com/aws/aws-for-fluent-bit#public-images).\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": ["2023-04-06T21:29:45Z", "2022-11-09T21:12:35Z", "2022-07-26T21:51:23Z", "2021-09-24T01:12:13Z", "2021-03-08T20:33:21Z", "2020-11-02T23:08:22Z", "2020-10-12T19:44:34Z", "2020-07-27T23:44:06Z", "2020-07-17T23:08:42Z", "2020-06-02T06:27:54Z", "2020-03-04T21:11:37Z", "2019-12-13T00:28:19Z", "2019-11-26T02:30:48Z"]}, {"name": "amazon-kinesis-streams-for-fluent-bit", "description": "A Fluent Bit output plugin for Kinesis Streams", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "[![Test Actions Status](https://github.com/aws/amazon-kinesis-streams-for-fluent-bit/workflows/Build/badge.svg)](https://github.com/aws/amazon-kinesis-streams-for-fluent-bit/actions)\n## Fluent Bit Plugin for Amazon Kinesis Data Streams\n\n**NOTE: A new higher performance Fluent Bit Kinesis Plugin has been released.** Check out our [official guidance](#new-higher-performance-core-fluent-bit-plugin).\n\nA Fluent Bit output plugin for Amazon Kinesis Data Streams.\n\n#### Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or email AWS security directly at [aws-security@amazon.com](mailto:aws-security@amazon.com).\n\n### Usage\n\nRun `make` to build `./bin/kinesis.so`. Then use with Fluent Bit:\n```\n./fluent-bit -e ./kinesis.so -i cpu \\\n-o kinesis \\\n-p \"region=us-west-2\" \\\n-p \"stream=test-stream\"\n```\n\nFor building Windows binaries, we need to install `mingw-64w` for cross-compilation. The same can be done using-\n```\nsudo apt-get install -y gcc-multilib gcc-mingw-w64\n```\nAfter this step, run `make windows-release`. Then use with Fluent Bit on Windows:\n```\n./fluent-bit.exe -e ./kinesis.dll -i dummy `\n-o kinesis `\n-p \"region=us-west-2\" `\n-p \"stream=test-stream\"\n```\n\n### Plugin Options\n\n* `region`: The region which your Kinesis Data Stream is in.\n* `stream`: The name of the Kinesis Data Stream that you want log records sent to.\n* `partition_key`: A partition key is used to group data by shard within a stream. A Kinesis Data Stream uses the partition key that is associated with each data record to determine which shard a given data record belongs to. For example, if your logs come from Docker containers, you can use container_id as the partition key, and the logs will be grouped and stored on different shards depending upon the id of the container they were generated from. As the data within a shard are coarsely ordered, you will get all your logs from one container in one shard roughly in order. Nested partition key is supported and you can use `->` to point to your target key which is nested under another key. For example, your `partition_key` could be `kubernetes->pod_name`. If you don't set a partition key or put an invalid one, a random key will be generated, and the logs will be directed to random shards. If the partition key is invalid, the plugin will print an warning message.\n* `data_keys`: By default, the whole log record will be sent to Kinesis. If you specify key name(s) with this option, then only those keys and values will be sent to Kinesis. For example, if you are using the Fluentd Docker log driver, you can specify `data_keys log` and only the log message will be sent to Kinesis. If you specify multiple keys, they should be comma delimited.\n* `log_key`: By default, the whole log record will be sent to Kinesis. If you specify a key name with this option, then only the value of that key will be sent to Kinesis. For example, if you are using the Fluentd Docker log driver, you can specify `log_key log` and only the log message will be sent to Kinesis.\n* `role_arn`: ARN of an IAM role to assume (for cross account access).\n* `endpoint`: Specify a custom endpoint for the Kinesis Streams API.\n* `sts_endpoint`: Specify a custom endpoint for the STS API; used to assume your custom role provided with `role_arn`.\n* `append_newline`: If you set append_newline as true, a newline will be addded after each log record.\n* `time_key`: Add the timestamp to the record under this key. By default the timestamp from Fluent Bit will not be added to records sent to Kinesis. The timestamp inserted comes from the timestamp that Fluent Bit associates with the log record, which is set by the input that collected it. For example, if you are reading a log file with the [tail input](https://docs.fluentbit.io/manual/pipeline/inputs/tail), then the timestamp for each log line/record can be obtained/parsed by using a Fluent Bit parser on the log line.\n* `time_key_format`: [strftime](http://man7.org/linux/man-pages/man3/strftime.3.html) compliant format string for the timestamp; for example, `%Y-%m-%dT%H:%M:%S%z`. This option is used with `time_key`. You can also use `%L` for milliseconds and `%f` for microseconds. Remember that the `time_key` option only inserts the timestamp Fluent Bit has for each record into the record. So the record must have been collected with a timestamp with precision in order to use sub-second precision formatters. If you are using ECS FireLens, make sure you are running Amazon ECS Container Agent v1.42.0 or later, otherwise the timestamps associated with your stdout & stderr container logs will only have second precision.\n* `experimental_concurrency`: Specify a limit of concurrent go routines for flushing records to kinesis.  By default `experimental_concurrency` is set to 0 and records are flushed in Fluent Bit's single thread. This means that requests to Kinesis will block the execution of Fluent Bit.  If this value is set to `4` for example then calls to Flush records from fluentbit will spawn concurrent go routines until the limit of `4` concurrent go routines are running.  Once the `experimental_concurrency` limit is reached calls to Flush will return a retry code.  The upper limit of the `experimental_concurrency` option is `10`.  WARNING:  Enabling `experimental_concurrency` can lead to data loss if the retry count is reached.  Enabling concurrency will increase resource usage (memory and CPU).\n* `experimental_concurrency_retries`: Specify a limit to the number of retries concurrent goroutines will attempt.  By default `4` retries will be attempted before records are dropped.\n* `aggregation`: Setting `aggregation` to `true` will enable KPL aggregation of records sent to Kinesis.  This feature changes the behavior of the `partition_key` feature.  See the KPL aggregation section below for more details.\n* `compression`: Specify an algorithm for compression of each record. Supported compression algorithms are `zlib` and `gzip`. By default this feature is disabled and records are not compressed.\n* `replace_dots`: Replace dot characters in key names with the value of this option. For example, if you add `replace_dots _` in your config then all occurrences of `.` will be replaced with an underscore. By default, dots will not be replaced.\n* `http_request_timeout`: Specify a timeout (in seconds) for the underlying AWS SDK Go HTTP call when sending records to Kinesis. By default, a timeout of `0` is used, indicating no timeout. Note that even with no timeout, the default behavior of the AWS SDK Go library may still lead to an eventual timeout.\n\n### Permissions\n\nThe plugin requires `kinesis:PutRecords` permissions.\n\n### Credentials\n\nThis plugin uses the AWS SDK Go, and uses its [default credential provider chain](https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html). If you are using the plugin on Amazon EC2 or Amazon ECS or Amazon EKS, the plugin will use your EC2 instance role or [ECS Task role permissions](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html) or [EKS IAM Roles for Service Accounts for pods](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html). The plugin can also retrieve credentials from a [shared credentials file](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html), or from the standard `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_SESSION_TOKEN` environment variables.\n\n### Environment Variables\n\n* `FLB_LOG_LEVEL`: Set the log level for the plugin. Valid values are: `debug`, `info`, and `error` (case insensitive). Default is `info`. **Note**: Setting log level in the Fluent Bit Configuration file using the Service key will not affect the plugin log level (because the plugin is external).\n* `SEND_FAILURE_TIMEOUT`: Allows you to configure a timeout if the plugin can not send logs to Kinesis Streams. The timeout is specified as a [Golang duration](https://golang.org/pkg/time/#ParseDuration), for example: `5m30s`. If the plugin has failed to make any progress for the given period of time, then it will exit and kill Fluent Bit. This is useful in scenarios where you want your logging solution to fail fast if it has been misconfigured (i.e. network or credentials have not been set up to allow it to send to Kinesis Streams).\n\n### Fluent Bit Versions\n\nThis plugin has been tested with Fluent Bit 1.2.0+. It may not work with older Fluent Bit versions. We recommend using the latest version of Fluent Bit as it will contain the newest features and bug fixes.\n\n### Example Fluent Bit Config File\n\n```\n[INPUT]\n    Name        forward\n    Listen      0.0.0.0\n    Port        24224\n\n[OUTPUT]\n    Name            kinesis\n    Match           *\n    region          us-west-2\n    stream          my-kinesis-stream-name\n    partition_key   container_id\n    append_newline  true\n    replace_dots    _\n```\n\n### AWS for Fluent Bit\n\nWe distribute a container image with Fluent Bit and this plugin.\n\n##### GitHub\n\n[github.com/aws/aws-for-fluent-bit](https://github.com/aws/aws-for-fluent-bit)\n\n##### Amazon ECR Public Gallery\n\n[aws-for-fluent-bit](https://gallery.ecr.aws/aws-observability/aws-for-fluent-bit)\n\nOur images are available in Amazon ECR Public Gallery. You can download images with different tags by following command:\n\n```\ndocker pull public.ecr.aws/aws-observability/aws-for-fluent-bit:<tag>\n```\n\nFor example, you can pull the image with latest version by:\n\n```\ndocker pull public.ecr.aws/aws-observability/aws-for-fluent-bit:latest\n```\n\nIf you see errors for image pull limits, try log into public ECR with your AWS credentials:\n\n```\naws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws\n```\n\nYou can check the [Amazon ECR Public official doc](https://docs.aws.amazon.com/AmazonECR/latest/public/get-set-up-for-amazon-ecr.html) for more details.\n\n##### Docker Hub\n\n[amazon/aws-for-fluent-bit](https://hub.docker.com/r/amazon/aws-for-fluent-bit/tags)\n\n##### Amazon ECR\n\nYou can use our SSM Public Parameters to find the Amazon ECR image URI in your region:\n\n```\naws ssm get-parameters-by-path --path /aws/service/aws-for-fluent-bit/\n```\n\nFor more see [our docs](https://github.com/aws/aws-for-fluent-bit#public-images).\n\n\n### KPL aggregation\n\nKPL aggregation can be enabled by setting the `aggregation` parameter to `true` (default is false).  With aggregation enabled each Record in the PutRecords request can contain multiple serialized records in the KCL protobuf structure.  This batch of records will only count as a single record towards the Kinesis records per second limit (currently 1000 records/sec per shard).\n\nThe advantages of enabling KPL aggregation are:\n\n - Increased throughput, and decreased Kinesis costs for smaller records (records less than 1K).\n - Less overhead in error checking PutRecords results (fewer PutRecords results to verify).\n - Firehose will de-aggregate the records automatically (free de-aggregation if Firehose is leveraged).\n\nThe disadvantages are:\n - The flush time (or buffer size) will need to be tuned to take advantage of aggregation (more on that below).\n - You must use the KCL library to read data from kinesis to de-aggregate the protobuf serialization (if Firehose isn't the consumer).\n - The `partition_key` feature isn't fully compatible with aggregation given multiple records are in each PutRecord structure.  The `partition_key` value of the first record in the batch will be used to route the entire batch to a given shard.  Given this limitation, using both `partition_key` and `aggregation` simultaneously requires careful consideration. In most container log use cases, all logs from a single container/pod are sent in the same stream, thus if you use the pod/container as the partition key, it should still work as expected since all records in an aggregated batch can use the same partition key. In other use cases, aggregation will cause records that should have had different partition keys to have the same partition key.\n\nKPL Aggregated Record Reference:  https://github.com/awslabs/amazon-kinesis-producer/blob/master/aggregation-format.md\n\n#### Tuning for aggregation\n\nWhen using `aggregation` the buffers and flush time may need to be tuned.  For low volume use cases a longer flush time maybe preferable to take full advantage of the aggregation cost savings.\n\nMore specifically, increasing the flush value will ensure the most records are aggregated taking full advantage of the cost savings.\n\n```\n[SERVICE]\n     Flush 20\n```\n\n\n### Example Fluent Bit Aggregation Config File\n\n```\n[SERVICE]\n     Flush 20\n\n[INPUT]\n    Name        forward\n    Listen      0.0.0.0\n    Port        24224\n\n[OUTPUT]\n    Name            kinesis\n    Match           *\n    region          us-west-2\n    stream          my-kinesis-stream-name\n    aggregation     true\n    append_newline  true\n```\n\n### ZLIB Compression\n\nEnabling `zlib` compression will compress each record individually reducing the network bandwidth required to send logs.  Using this feature in conjunction with `aggregation` can greatly reduce the number of Kinesis shards required.\n\nCompression Advantages:\n\n   - Reduces network bandwidth required\n   - Reduces Kinesis shard count in some scenarios\n\nCompression Disadvantages:\n\n   - Fluentbit will require more CPU and memory to send records\n   - A consumer must decompress the records\n\n\nExample config:\n\n```\n[SERVICE]\n     Flush 20\n\n[INPUT]\n    Name        forward\n    Listen      0.0.0.0\n    Port        24224\n\n[OUTPUT]\n    Name            kinesis\n    Match           *\n    region          us-west-2\n    stream          my-kinesis-stream-name\n    compression     zlib\n    append_newline  true\n```\n\n### New Higher Performance Core Fluent Bit Plugin\n\nWe have released a [new higher performance Kinesis Streams plugin](https://docs.fluentbit.io/manual/pipeline/outputs/kinesis) named `kinesis_streams`.\n\nThat plugin has many of the features of this older, lower performance and less efficient plugin. Please compare this document with its [documentation](https://docs.fluentbit.io/manual/pipeline/outputs/kinesis) for an up to date feature set comparison between the two plugins.\n\n#### Do you plan to deprecate this older plugin?\n\nThis plugin will continue to be supported. However, we are pausing development on it and will focus on the high performance version instead.\n\n#### Which plugin should I use?\n\nIf the features of the higher performance plugin are sufficient for your use cases, please use it. It can achieve higher throughput and will consume less CPU and memory.\n\nAs time goes on we expect new features to be added to the C plugin only, however, this is determined on a case by case basis. There is some feature gap between the two plugins. Please consult the [C plugin documentation](https://docs.fluentbit.io/manual/pipeline/outputs/firehose) and this document for the features offered by each plugin. \n\n#### How can I migrate to the higher performance plugin?\n\nFor many users, you can simply replace the plugin name `kinesis` with the new name `kinesis_streams`.\n\n#### Do you accept contributions to both plugins?\n\nYes. The high performance plugin is written in C, and this plugin is written in Golang. We understand that Go is an easier language for amateur contributors to write code in- that is one of the primary reasons we are continuing to maintain this repo.\n\nHowever, if you can write code in C, please consider contributing new features to the [higher performance plugin](https://github.com/fluent/fluent-bit/tree/master/plugins/out_kinesis_firehose).\n", "release_dates": ["2023-04-06T21:31:38Z", "2022-11-09T23:08:53Z", "2022-07-26T21:52:13Z", "2022-03-08T23:52:28Z", "2022-01-31T19:21:28Z", "2021-09-30T23:44:14Z", "2021-09-24T01:10:54Z", "2021-08-26T22:21:01Z", "2020-11-02T23:09:38Z", "2020-10-12T19:51:21Z", "2020-09-16T02:46:26Z", "2020-07-27T23:50:43Z", "2020-07-17T23:09:39Z", "2020-07-14T02:23:13Z", "2020-06-02T06:25:15Z", "2020-06-02T06:24:20Z", "2020-03-04T21:16:05Z", "2019-12-13T00:31:40Z", "2019-11-26T02:34:10Z"]}, {"name": "amazon-kinesis-video-streams-parser-library", "description": "Amazon Kinesis Video Streams parser library is for developers to include in their applications that makes it easy to work with the output of video streams such as retrieving frame-level objects, metadata for fragments, and more.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon Kinesis Video Streams Parser Library \n\n## License\n\nThis library is licensed under the Apache 2.0 License. \n\n## Introduction\nThe Amazon Kinesis Video Streams Parser Library for Java enables Java developers to parse the streams returned by `GetMedia` calls to Amazon Kinesis Video. \nIt contains:\n* A streaming Mkv Parser called `StreamingMkvReader` that provides an iterative interface to read the `MkvElement`s in a stream.\n* Applications such as `OutputSegmentMerger` and `FragmentMetadataVisitor` built using the `StreamingMkvReader` .\n* A callback based parser called `EBMLParser` that minimizes data buffering and copying. `StreamingMkvReader` is built on top of `EBMLParser`\n* Unit tests for the applications and parsers that demonstrate how the applications work.\n\n## Building from Source\nAfter you've downloaded the code from GitHub, you can build it using Maven. Use this command: `mvn clean install`\n\n\n## Details\n### StreamingMkvReader\n`StreamingMkvReader` which provides an iterative interface to read `MkvElement`s from a stream.\nA caller calls `nextIfAvailable` to get the next `MkvElement`. An `MkvElement` wrapped in an `Optional` is returned if a complete element is available.\nIt buffers an individual `MkvElement` until it can return a complete `MkvElement`.  \nThe `mightHaveNext` method returns true if there is a chance that additional `MkvElements` can be returned. \nIt returns false when the end of the input stream has been reached.\n \n### MkvElement\nThere are three types of `MkvElement` vended by a `MkvStreamReader`:\n* `MkvDataElement`: This encapsulates Mkv Elements that are not master elements and contain data. \n* `MkvStartMasterElement` : This represents the start of a *master* Mkv element that contains child elements. Child elements can be other master elements or data elements.\n* `MkvEndMasterElement` : This represents the end of *master* element that contains child elements.\n\n### MkvElementVisitor\nThe `MkvElementVisitor` is a visitor pattern that helps process the events in the `MkvElement` hierarchy. It has a visit \nmethod for each type of `MkvElement`.\n\n## Visitors\nA `GetMedia` call to Kinesis Video vends a stream of fragments where each fragment is encapsulated in a Mkv stream containing *EBML* and *Segment* elements.\n`OutputSegmentMerger` can be used to merge consecutive fragments that share the same *EBML* and *Track* data into a single Mkv stream with \na shared *EBML* and *Segment*. This is useful for passing the output of `GetMedia` to any downstream processor that expects a single Mkv stream\nwith one *Segment*. Its use can be seen in `OutputSegmentMergerTest`\n\n`FragmentMetadataVisitor` is a `MkvElementVisitor` that collects the Kinesis Video specific meta-data (such as *FragmentNumber* and *Server Side Timestamp* )\n for the current fragment being processed. The `getCurrentFragmentMetadata` method can be used to get the current fragment's metadata. Similarly \n`getPreviousFragmentMetadata` can be used get the previous fragment's metadata. The `getMkvTrackMetadata` method can be used to get\nthe details of a particular track.\n\n`ElementSizeAndOffsetVisitor` is a visitor that writes out the metadata of the Mkv elements in a stream. For each element\n the name, offset, header size and data size is written out. The output uses indentation to indicate the hierarchy of master elements\n and their child elements. `ElementSizeAndOffsetVisitor` is useful for looking into Mkv streams, where mkvinfo fails.\n\n`CountVisitor` is a visitor that can be used to count the number of Mkv elements of different types in a Mkv stream.\n\n`CompositeMkvElementVisitor` is a visitor that is made up of a number of constituent visitors. It calls accept on the \nvisited `MkvElement` for each constituent visitor in the order in which the visitors are specified.\n\n`FrameVisitor` is a visitor used to process the frames in the output of a GetMedia call. It invokes an implementation of the\n `FrameVisitor.FrameProcessor` and provides it with a `Frame` object and the metadata of the track to which the `Frame` belongs.\n\n`CopyVisitor` is a visitor used to copy the raw bytes of the Mkv elements in a stream to an output stream.\n\n## ResponseStreamConsumers\nThe `GetMediaResponseStreamConsumer` is an abstract class used to consume the output of a GetMedia* call to Kinesis Video in a streaming fashion.\nIt supports a single abstract method called process that is invoked to process the streaming payload of a GetMedia response.\nThe first parameter for process method is the payload inputStream in a GetMediaResult returned by a call to GetMedia.\nImplementations of the process method of this interface should block until all the data in the inputStream has been\n processed or the process method decides to stop for some other reason. The second argument is a FragmentMetadataCallback \n which is invoked at the end of every processed fragment. The `GetMediaResponseStreamConsumer` provides a utility method \n `processWithFragmentEndCallbacks` that can be used by child classes to  implement the end of fragment callbacks.\n The process method can be implemented using a combination of the visitors described earlier.\n \n### MergedOutputPiper\nThe `MergedOutputPiper` extends `GetMediaResponseStreamConsumer` to merge consecutive mkv streams in the output of GetMedia\n and pipes the merged stream to the stdin of a child process. It is meant to be used to pipe the output of a GetMedia* call to a processing application that can not deal\nwith having multiple consecutive mkv streams. Gstreamer is one such application that requires a merged stream.\n\n\n## Example\n### KinesisVideoExample\n`KinesisVideoExample` is an example that shows how the `StreamingMkvReader` and the different visitors can be integrated\nwith the AWS SDK for the Kinesis Video. This example provides examples for\n* Create a stream, deleting and recreating if the stream of the same name already exists.\n* Call PutMedia to stream video fragments into the stream.\n* Simultaneously call GetMedia to stream video fragments out of the stream.\n* It uses the StreamingMkvReader to parse the returned the stream and apply the `OutputSegmentMerger`, `FragmentMetadataVisitor` visitors\nalong with a local one as part of the same `CompositeMkvElementVisitor` visitor.\n\n### KinesisVideoRendererExample\n`KinesisVideoRendererExample` shows parsing and rendering of KVS video stream fragments using JCodec(http://jcodec.org/) that were ingested using Producer SDK GStreamer sample application.\n* To run the example:\n  Run the Unit test `testExample` in `KinesisVideoRendererExampleTest`. After starting the unitTest you should be able to view the frames in a JFrame.\n* If you want to store it as image files you could do it by adding (in KinesisVideoRendererExample after AWTUtil.toBufferedImage(rgb, renderImage); )\n\n```\ntry {\n    ImageIO.write(renderImage, \"png\", new File(String.format(\"frame-capture-%s.png\", UUID.randomUUID())));\n } catch (IOException e) {\n    log.warn(\"Couldn't convert to a PNG\", e);\n}\n```\n* It has been tested not only for streams ingested by `PutMediaWorker` but also streams sent to Kinesis Video Streams using GStreamer Demo application (https://github.com/awslabs/amazon-kinesis-video-streams-producer-sdk-cpp)\n\n### KinesisVideoGStreamerPiperExample\n`KinesisVideoGStreamerPiperExample` is an example for continuously piping the output of GetMedia calls from a Kinesis Video stream to GStreamer.\n The test `KinesisVideoGStreamerPiperExampleTest` provides an example that pipes the output of a KVS GetMedia call to a Gstreamer pipeline.\n The Gstreamer pipeline is a toy example that demonstrates that Gstreamer can parse the mkv passed into it.\n\n### KinesisVideo - Rekognition Examples\nKinesis Video - Rekognition examples demonstrate how to combine Rekognition outputs (JSON) with KinesisVideo Streams output (H264 fragments) and\n render the frames with overlapping bounding boxes.\n\n#### KinesisVideoRekognitionIntegrationExample\n`KinesisVideoRekognitionIntegrationExample` decodes H264 frames and renders them with bounding boxes locally using JFrame. To run the sample, run `KinesisVideoRekognitionIntegrationExampleTest` with\nappropriate inputs.\n\n#### KinesisVideoRekognitionLambdaExample\n`KinesisVideoRekognitionLambdaExample` decodes H264 frames, overlaps bounding boxes, encodes to H264 frames again and ingests them into a new Kinesis Video streams using Kinesis Video Producer SDK.\nTo run the sample follow the below steps:\n* Run `mvn package` from 'amazon-kinesis-video-streams-parser-library' folder. Upload './target/amazon-kinesis-video-streams-parser-library-$VERSION-shaded.jar' file to a S3 bucket.\n* Note the S3 bucket and key name.\n* Find the file `LambdaExampleCFnTemplate.yml` in the github package.\n* Goto AWS CloudFormation console and create stack using above template, follow the description to the input parameters.\n* Now start the producer to ingest data into Kinesis Video Streams for which the Rekognition stream processor is configured.\n* Lambda will be triggered as soon as the Rekognition stream processor starts emitting records in Kinesis Data Streams. Lambda will also create a new Kinesis Video streams\n with the input stream name + 'Rekognized' suffix and ingest frames overlapped with bounding boxes which should be viewable in Kinesis Video Streams console.\n* To troubleshoot any issues, use Monitoring tab in lambda and click 'View logs in Cloudwatch'.\n* NOTE: As this lambda executes resource intense decoding and encoding (using Jcodec which is not optimal https://github.com/jcodec/jcodec#performance--quality-considerations),\n the new Kinesis Video stream might be delayed significantly.\n\n## Release Notes\n\n### Release 1.2.4 (Mar 2022)\n* Update amazon-kinesis-client from 1.14.7 to 1.14.8\n\n### Release 1.2.3 (Feb 2022)\n* Update slf4j-reload4j and slf4j-api from 1.7.35 to 1.7.36\n* Update aws-lambda-java-events from 1.2.0 to 2.2.9\n* Update amazon-kinesis-video-streams-producer-sdk-java from 1.8.0 to 1.12.0\n* Update aws-java-sdk-bom from 1.11.487 to 1.12.162\n\n### Release 1.2.2 (Jan 2022)\n* Update slf4j-reload4j(slf4j-log4j12) and slf4j-api from 1.7.33 to 1.7.35\n* Update amazon-kinesis-client from 1.9.3 to 1.14.7\n* Update aws-lambda-java-core from 1.2.0 to 1.2.1\n* Update junit from 4.13.1 to 4.13.2\n* Update lombok from 1.18.16 to 1.18.22\n* Update commons-lang3 from 3.6 to 3.12.0\n* Update powermock-mockito-release-full from 1.6.3 to 1.6.4\n* Update maven-compiler-plugin from 3.2 to 3.9.0\n* Update lombok-maven-plugin from 1.18.16.0 to 1.18.20.0\n* Update maven-javadoc-plugin from 3.1.1 to 3.3.1\n* Update maven-source-plugin from 3.0.1 to 3.2.1\n* Update maven-shade-plugin from 2.3 to 3.2.4\n\n### Release 1.2.1 (Jan 2022)\n* Update slf4j-log4j12 and slf4j-api from 1.7.25 to 1.7.33\n* Update log4j-slf4j-impl from 2.8.2 to 2.17.1\n\n### Release 1.2.0 (Jan 2022)\n* Move from aws-lambda-java-log4j 1.1.0 to aws-lambda-java-log4j2 1.5.1 to address CVE\n* Update log4j to 2.17.1 to address CVE\n\n### Release 1.1.0 (Dec 2021)\n* Add ListFragment worker and update GetMediaForFragmentListWorker\n* Upgrade Log4j to 2.16 to address CVE\n\n\n### Release 1.0.15 (Aug 2020)\n* Added new cluster packing option to the `OputputSegmentMerger` to enable creation of a playable MKV file from a sparse KVS stream.\n* Added parsing of audio specific fields from the MKV track header.\n* Bump some dependency versions.\n* Modify the log level on some log messages.\n\n### Release 1.0.14 (Aug 2019)\n* Fixed frame timecode during re-encoding in KinesisVideoRekognitionLambdaExample\n* Fixed region for derived KVS Stream\n* Using default FaceType for external image ids that doesn't follow specified format\n* Upgraded JCodec version to 0.2.3 which provides scaling list support\n* Log improvements\n\n### Release 1.0.13 (Apr 2019)\n* Fix: Make process method in H264FrameProcessor and H264FrameDecoder throw FrameProcessException.\n\n### Release 1.0.12 (Mar 2019)\n* Bugfix: Fix KinesisVideoExampleTest example issue that was using non-exist test file.\n* Improve KinesisVideoRekognitionLambdaExample to use AWS CloudFormation Template to create resources.\n\n### Release 1.0.11 (Mar 2019)\n* Bugfix: KinesisVideoRekognitionIntegrationExample broken because the frame process callback is not invoked.\n\n### Release 1.0.10 (Feb 2019)\n* Bugfix: Close GetMedia connection to fix the connection leak issue.\n\n### Release 1.0.9 (Jan 2019)\n* Added KinesisVideo Rekgonition Lamba example which combines Rekognition output with KVS fragments to draw bounding boxes\nfor detected faces and ingest into new KVS Stream.\n* Added Optional track number parameter in the FrameVisitor to process only frames with that track number.\n\n### Release 1.0.8 (Dec 2018)\n* Add close method for derived classes to cleanup resources.\n* Add exception type which could be used in downstream frame processing logic.\n* Make boolean value thread-safe in ContinuousGetMediaWorker.\n* Remove extra exception wrapping in CompositeMkvElementVisitor.\n* Declare exception throwing for some methods.\n* Enabled stack trace in ContinuousGetMediaWorker when there is an exception.\n\n### Release 1.0.7 (Sep 2018)\n* Add flag in KinesisVideoRendererExample and KinesisVideoExample to use the existing stream (and not doing PutMedia again if it exists already).\n* Added support to retrieve the information from FragmentMetadata and display in the image panel during rendering.\n\n### Release 1.0.6 (Sep 2018)\n* Introduce handling for empty fragment metadata\n* Added a new SimpleFrame Visitor to handle video with no tags\n* Refactored H264FrameDecoder, so that base class method can be reused by child class\n\n### Release 1.0.5 (May 2018)\n* Introduce `GetMediaResponseStreamConsumer` as an abstract class used to consume the output of a GetMedia* call\nto Kinesis Video in a streaming fashion. Child classes will use visitors to implement different consumers.\n* The `MergedOutputPiper` extends `GetMediaResponseStreamConsumer` to merge consecutive mkv streams in the output of GetMedia\n   and pipes the merged stream to the stdin of a child process.\n* Add the capability and example to pipe the output of GetMedia calls to GStreamer using `MergedOutputPiper`.\n\n### Release 1.0.4 (April 2018)\n* Add example for KinesisVideo Streams integration with Rekognition and draw Bounding Boxes for every sampled frame.\n* Fix for stream ending before reporting tags visited.\n* Same test data file for parsing and rendering example.\n* Known Issues:  In `KinesisVideoRekognitionIntegrationExample`, the decode/renderer sample using JCodec may not be able to decode all mkv files.\n\n### Release 1.0.3 (Februrary 2018)\n*  In OutputSegmentMerger, make sure that the lastClusterTimecode is updated for the first fragment.\nIf timecode is equal to that of a previous cluster, stop merging\n* FrameVisitor to process the frames in the output of a GetMedia call.\n* CopyVisitor to copy the raw bytes of the stream being parsed to an output stream.\n* Add example that shows parsing and rendering Kinesis Video Streams.\n* Known Issues:  In `KinesisVideoRendererExample`, the decode/renderer sample using JCodec may not be able to decode all mkv files.\n   \n### Release 1.0.2 (December 2017)\n* Add example that shows integration with Kinesis Video Streams.\n* Remove unnecessary import.\n\n### Release 1.0.1 (November 2017)\n* Update to include the url for Amazon Kinesis Video Streams in the pom.xml\n\n### Release 1.0.0 (November 2017)\n* First release of the Amazon Kinesis Video Parser Library.\n* Supports Mkv elements up to version 4. \n* Known issues:\n    * EBMLMaxIDLength and EBMLMaxSizeLength are hardcoded as 4 and 8 bytes respectively\n    * Unknown EBML elements not specified in `MkvTypeInfos` are not readable by the user using `StreamingMkvReader`.\n    * Unknown EBML elements not specified in `MkvTypeInfos` of unknown length lead to an exception.\n    * Does not do any CRC validation for any Mkv elements with the `CRC-32` element. \n", "release_dates": ["2023-02-22T20:01:01Z", "2022-03-10T20:24:36Z", "2022-02-21T21:07:07Z", "2022-01-28T22:47:52Z", "2022-01-21T18:57:35Z", "2022-01-13T18:23:09Z", "2021-12-16T15:43:12Z", "2021-12-14T22:42:21Z", "2020-09-23T19:21:18Z", "2019-08-17T01:18:17Z", "2019-04-29T20:13:42Z", "2019-03-06T18:43:36Z", "2019-03-01T23:00:47Z", "2019-02-26T01:41:09Z", "2019-01-29T23:45:15Z", "2018-12-19T00:53:05Z", "2018-10-15T20:47:23Z", "2018-09-21T17:12:01Z", "2018-05-18T21:04:08Z", "2018-04-19T03:45:19Z", "2018-03-05T18:50:40Z", "2017-12-18T20:14:21Z", "2017-11-29T18:31:35Z", "2017-11-28T19:26:58Z"]}, {"name": "amazon-managed-grafana-roadmap", "description": "Amazon Managed Grafana Roadmap", "language": null, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## Amazon Managed Grafana Public Roadmap\n\nThis is the public roadmap for [Amazon Managed Grafana](https://aws.amazon.com/grafana/). We share information here about what we are working on and equally, you can use this repo to provide feedback and suggest features. Note that this roadmap presents features that we are considering developing, however it does remain subject to change from time to time. Equally, there is no guarantee that we will release these features or for the exact timeline in which we may release them.\n\n[Go to the roadmap now \u00bb](https://github.com/aws/amazon-managed-grafana-roadmap/issues)\n\n\n## Security\n\nIf you think you\u2019ve found a potential security issue with Amazon Managed Grafana, please DO NOT create an issue for it here but rather follow the instructions in the [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) document.\n\n## License \n\nThe issue content is made available under the Creative Commons Attribution-ShareAlike 4.0 International License, see also the [LICENSE-SUMMARY](/LICENSE-SUMMARY) file.\nThe sample code is made available under the MIT-0 license, see also the [LICENSE-SAMPLECODE](/LICENSE-SAMPLECODE) file.\n", "release_dates": []}, {"name": "amazon-managed-service-for-prometheus-roadmap", "description": "Amazon Managed Service for Prometheus Public Roadmap", "language": null, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## Amazon Managed Service for Prometheus Product Feature Requests\n\nThis is the public space where you can add and influence feature requests\nfor the [Amazon Managed Service for Prometheus](https://aws.amazon.com/prometheus/)\nroadmap and allows all AWS customers to give direct feedback. Knowing about our upcoming products and priorities helps our customers plan. \n\n[Go to the feature list now \u00bb](https://github.com/aws/amazon-managed-service-for-prometheus-roadmap/issues)\n\n## FAQs\n\n#### Q: Why did you build this?\nA: We know that our customers are making decisions and plans based on what \nwe are developing, and we want to provide our customers the insights they need to plan.\n\n#### Q: Why are there no dates on the feature requests?\nA: Because job zero is security and operational stability, we can't provide\nspecific target dates for features.\n\n#### Q: Is everything from the feature list on the roadmap?\nA: Feature requests will be considered for the\n[Amazon Managed Service for Prometheus](https://aws.amazon.com/prometheus/) roadmap. \n\n#### Q: How can I provide feedback or ask for more information?\nA: Please open an issue!\n\n#### Q: How can I request a feature?\nA: Please open an issue! You can read about how to contribute here. \nCommunity submitted issues will be tagged with `proposed` and will be\nreviewed by the service team.\n\n## Security\n\nIf you think you\u2019ve found a potential security issue with Amazon Managed Service \nfor Prometheus, please DO NOT create an issue for it here but rather follow the \ninstructions in the [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) \ndocument or [email AWS security directly](mailto:aws-security@amazon.com).\n\n## License \n\nThe issue content is made available under the Creative Commons \nAttribution-ShareAlike 4.0 International License, see also the\n[LICENSE-SUMMARY](/LICENSE-SUMMARY) file.\nThe sample code is made available under the MIT-0 license, see\nalso the [LICENSE-SAMPLECODE](/LICENSE-SAMPLECODE) file.\n\n\n\n", "release_dates": []}, {"name": "amazon-neptune-csv-to-rdf-converter", "description": "Amazon Neptune CSV to RDF Converter is a tool for Amazon Neptune that converts property graphs stored as comma separated values into RDF graphs.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon Neptune CSV to RDF Converter\n\nA tool for [Amazon Neptune](https://aws.amazon.com/neptune/) that converts property graphs stored as comma separated values into RDF graphs.\n\n\n## Usage\n\nAmazon Neptune CSV to RDF Converter is a Java library for converting a property graph stored in\nCSV files to RDF. It expects an input directory containing the CSV files, an output directory, and an\noptional configuration file. The output directory will be created if it does not exist.\nSee [Gremlin Load Data Format](https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-format-gremlin.html)\nabout the input and [RDF 1.1 N-Quads](https://www.w3.org/TR/n-quads/) about the output format.\n\nThe input files need to be UTF-8 encoded. The same encoding is used for the output files.\n\nThe library is available as executable Jar file and can be run from the command line by `java -jar amazon-neptune-csv2rdf.jar -i <input directory> -o <output directory>`. Use `java -jar amazon-neptune-csv2rdf.jar -h` to see all options:\n\n\tUsage: java -jar amazon-neptune-csv2rdf.jar [-hV] [-c=<configuration file>]\n\t       -i=<input directory> -o=<output directory>\n\t  -c, --config=<configuration file>\n\t                  Propery file containing the configuration.\n\t  -h, --help      Show this help message and exit.\n\t  -i, --input=<input directory>\n\t                  Directory containing the CSV files (UTF-8 encoded).\n\t  -o, --output=<output directory>\n\t                  Directory for writing the RDF files (UTF-8 encoded); will be\n\t                    created if it does not exist.\n\t  -V, --version   Print version information and exit.\n\nThe conversion is based on two steps. First, a **general mapping** from property graph vertices and edges\nto RDF statements is applied to the input files. The optional second step **transforms RDF resource IRIs**\naccording to user defined rules for replacing artificial ids with more natural ones. However, this\ntransformation needs to load all triples into main memory, so the JVM memory must be set accordingly with\n`-Xmx`, e.g., `java -Xmx2g`.\n\nLet's start with a small example to see how both steps work.\n\n**General mapping**\n\nLet vertices and edges be given as\n\n\t~id,~label,name,code,country\n\t1,city,Seattle,S,USA\n\t2,city,Vancouver,V,CA\n\nand\n\n\t~id,~label,~from,~to,distance,type\n\ta,route,1,2,166,highway\n\nUsing some simplified namespaces (see Configuration below for the details), the mapping results in:\n\n\t<vertex:1> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <type:City> <dng:/> .\n\t<vertex:1> <vproperty:name> \"Seattle\" <dng:/> .\n\t<vertex:1> <vproperty:code> \"S\" <dng:/> .\n\t<vertex:1> <vproperty:country> \"USA\" <dng:/> .\n\t<vertex:2> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <type:City> <dng:/> .\n\t<vertex:2> <vproperty:name> \"Vancouver\" <dng:/> .\n\t<vertex:2> <vproperty:code> \"V\" <dng:/> .\n\t<vertex:2> <vproperty:country> \"CA\" <dng:/> .\n\n\t<vertex:1> <edge:route> <vertex:2> <econtext:a> .\n\t<econtext:a> <eproperty:distance> \"166\" <dng:/> .\n\t<econtext:a> <eproperty:type> \"highway\" <dng:/> .\n\nThe result shows that **edge identifiers are stored as context** of the corresponding RDF statement, and\nthe edge properties are statements about that context. The edge identifiers can be queried in SPARQL using\nthe [GRAPH keyword](https://www.w3.org/TR/sparql11-query/#accessByLabel).\n\nVertex labels are mapped to **RDF types**. The first letter of the label will be capitalized for this step:\nThe label `city` becomes the RDF type `<type:City>`.\n\nAdditionally, the mapping can add **RDFS labels** to the vertices. For example, the configuration\n\n\tmapper.mapping.pgVertexType2PropertyForRdfsLabel.city=name\n\n creates two additional RDF statements:\n\n\t<vertex:1> <http://www.w3.org/2000/01/rdf-schema#label> \"Seattle\" <dng:/> .\n\t<vertex:2> <http://www.w3.org/2000/01/rdf-schema#label> \"Vancouver\" <dng:/> .\n\nThe mapping can also map **property values to resources**. In the example, the value for *country* becomes\nan URI with\n\n\tmapper.mapping.pgProperty2RdfResourcePattern.country=country:{{VALUE}}\n\nand the two statements with the literal value \"USA\" and \"CA\" are replaced by:\n\n\t<vertex:1> <edge:country> <country:USA> <dng:/> .\n\t<vertex:2> <edge:country> <country:CA> <dng:/> .\n\n**URI transformations**\n\nA URI transformation rule replaces parts of a resource URI with the value of a property. In the previous\nexample, the code could be used to create the resource URIs. This can be achieved using:\n\n\ttransformer.uriPostTransformations.1.srcPattern=vertex:([0-9]+)\n\ttransformer.uriPostTransformations.1.typeUri=type:City\n\ttransformer.uriPostTransformations.1.propertyUri=vproperty:code\n\ttransformer.uriPostTransformations.1.dstPattern=city:{{VALUE}}\n\nThe resulting statements are now:\n\n\t<city:S> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <type:City> <dng:/> .\n\t<city:S> <http://www.w3.org/2000/01/rdf-schema#label> \"Seattle\" <dng:/> .\n\t<city:S> <vproperty:name> \"Seattle\" <dng:/> .\n\t<city:S> <vproperty:code> \"S\" <dng:/> .\n\t<city:S> <edge:country> <country:USA> <dng:/> .\n\t<city:V> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <type:City> <dng:/> .\n\t<city:V> <http://www.w3.org/2000/01/rdf-schema#label> \"Vancouver\" <dng:/> .\n\t<city:V> <vproperty:name> \"Vancouver\" <dng:/> .\n\t<city:V> <vproperty:code> \"V\" <dng:/> .\n\t<city:V> <edge:country> <country:CA> <dng:/> .\n\t<city:S> <edge:route> <city:V> <econtext:a> .\n\t<econtext:a> <eproperty:distance> \"166\" <dng:/> .\n\t<econtext:a> <eproperty:type> \"highway\" <dng:/> .\n\n\n## Configuration\n\nThe configuration of the converter is a property file. It contains a default type, a default named graph,\nand namespaces for building vertex URIs, edge URIs, type URIs, vertex property URIs, and edge property URIs.\nThe rules for adding RDFS labels, creating resources from property values, and the URI transformations are\noptional. It's also possible to set the file extension of the input files.\n\nIf no configuration file is given, the following default values are used:\n\n\tinputFileExtension=csv\n\n\tmapper.alwaysAddPropertyStatements=true\n\n\tmapper.mapping.typeNamespace=http://aws.amazon.com/neptune/csv2rdf/class/\n\tmapper.mapping.vertexNamespace=http://aws.amazon.com/neptune/csv2rdf/resource/\n\tmapper.mapping.edgeNamespace=http://aws.amazon.com/neptune/csv2rdf/objectProperty/\n\tmapper.mapping.edgeContextNamespace=http://aws.amazon.com/neptune/csv2rdf/resource/\n\tmapper.mapping.vertexPropertyNamespace=http://aws.amazon.com/neptune/csv2rdf/datatypeProperty/\n\tmapper.mapping.edgePropertyNamespace=http://aws.amazon.com/neptune/csv2rdf/datatypeProperty/\n\tmapper.mapping.defaultNamedGraph=http://aws.amazon.com/neptune/vocab/v01/DefaultNamedGraph\n\tmapper.mapping.defaultType=http://www.w3.org/2002/07/owl#Thing\n\nThe setting `mapper.alwaysAddPropertyStatements` has only effect if a rule for adding RDFS labels is used. In\nthat case it decides whether or not to add the property that is being used for the RDFS label additionally\nas RDF literal statement with that property. For the small example above, if the setting was chosen to be\n`false`, two statements would not be generated:\n\n\t<city:S> <vproperty:name> \"Seattle\" <dng:/> .\n\t<city:V> <vproperty:name> \"Vancouver\" <dng:/> .\n\nThe setting `mapper.mapping.edgeContextNamespace` takes effect only when explicitly set. Otherwise, it uses\nthe value set for `mapper.mapping.vertexNamespace`.\n\n**Vertex type to RDFS label mapping**\n\nVertex types are defined by vertex labels. The option `mapper.mapping.pgVertexType2PropertyForRdfsLabel.<vertex type>.<vertex property>` is used to specify a mapping from a vertex type to to a vertex property, whose value is then used\nto create RDFS labels for any vertex belonging to this vertex type. Multiple such mappings are allowed.\n\n**Property to RDF resource mapping**\n\nThe option `pgProperty2RdfResourcePattern.<vertex property>=<namespace>{{VALUE}}` is used to create RDF resources\ninstead of literal values for vertices where the specified property is found. The variable `{{value}}` will\nbe replaced with the value of the property and prefixed with the given namespace. Multiple such mappings\nare allowed.\n\n**URI Post Transformations**\n\nURI Post Transformations are used to transform RDF resource IRIs into more readable ones.\n\nAn URI Post Transformation consists of four elements:\n\n\turiPostTransformation.<ID>.srcPattern=<URI regex patten>\n\turiPostTransformation.<ID>.typeUri=<URI>\n\turiPostTransformation.<ID>.propertyUri=<URI>\n\turiPostTransformation.<ID>.dstPattern=<URI pattern>\n\nA positive integer `<ID>` is required to group the elements. The grouping numbers of several transformation\nconfigurations do not need to be consecutive. The transformation rules will be executed in ascending order\naccording to the grouping numbers. All four configuration items are required:\n\n* `srcPattern` is a URI with a single regular expression group, e.g.\n `<http://aws.amazon.com/neptune/csv2rdf/resource/([0-9]+)>`, defining\n the URI patterns of RDF resources to which the post transformation applies.\n * `typeUri` filters out all matched source URIs that do not belong to\n the specified RDF type.\n* `propertyUri` is the RDF predicate pointing to the replacement\n value.\n* `dstPattern` is the new URI, which must contain a\n `{{VALUE}}` substring which is then substituted with the value of\n `propertyUri`.\n\n*Example:*\n\n\turiPostTransformation.1.srcPattern=http://example.org/resource/([0-9]+)\n\turiPostTransformation.1.typeUri=http://example.org/class/Country\n\turiPostTransformation.1.propertyUri=http://example.org/datatypeProperty/code\n\turiPostTransformation.1.dstPattern=http://example.org/resource/{{VALUE}}\n\nThis configuration transforms the URI `http://example.org/resource/123` into  `http://example.org/resource/FR`,\ngiven that there are the statements:\n\n\thttp://example.org/resource/123 a http://example.org/class/Country.\n\thttp://example.org/resource/123 http://example.org/datatypeProperty/code \"FR\".\n\nNote that we assume that the property `propertyUri` is unique for each resource, otherwise a runtime exception\nwill be thrown. Also note that the post transformation is applied using a two-pass algorithm over the\ngenerated data, and the translation mapping is kept fully in memory. This means the property is suitable\nonly in cases where the number of mappings is small or if the amount of main memory is large.\n\n\n**Complete Configuration**\n\nThe complete configuration for the small example above is:\n\n\tmapper.alwaysAddPropertyStatements=false\n\n\tmapper.mapping.typeNamespace=type:\n\tmapper.mapping.vertexNamespace=vertex:\n\tmapper.mapping.edgeNamespace=edge:\n\tmapper.mapping.edgeContextNamespace=econtext:\n\tmapper.mapping.vertexPropertyNamespace=vproperty:\n\tmapper.mapping.edgePropertyNamespace=eproperty:\n\tmapper.mapping.defaultNamedGraph=dng:/\n\tmapper.mapping.defaultType=dt:/\n\tmapper.mapping.defaultPredicate=dp:/\n\tmapper.mapping.pgVertexType2PropertyForRdfsLabel.city=name\n\n\tmapper.mapping.pgProperty2RdfResourcePattern.country=country:{{VALUE}}\n\n\ttransformer.uriPostTransformations.1.srcPattern=vertex:([0-9]+)\n\ttransformer.uriPostTransformations.1.typeUri=type:City\n\ttransformer.uriPostTransformations.1.propertyUri=vproperty:code\n\ttransformer.uriPostTransformations.1.dstPattern=city:{{VALUE}}\n\n\n## Examples\n\nThe small example above is contained in `src/test/example` and can be tested with:\n\n\tjava -jar amazon-neptune-csv2rdf.jar -i src/test/example/ -o . -c src/test/example/city.properties\n\nAdditionally, the directory `src/test/air-routes` contains a Zip archive of the\n[Air Routes data set](https://github.com/krlawrence/graph/tree/master/sample-data) and a sample configuration.\nAfter unzipping the archive into `air-routes`, it can be converted with:\n\n\tjava -jar amazon-neptune-csv2rdf.jar -i air-routes/ -o . -c src/test/air-routes/air-routes.properties\n\n\n## Known Limitations\n\nThe general mapping from property graph vertices and edges is done individually for each CSV line in order to avoid\nloading the whole CSV file into memory. However, that means that properties being defined on different lines are not joined\nand cardinality constraints cannot be checked. For example, the RDF mapping (using the simplified namespaces from the small\nexample above) of the following property graph\n * should reject the statement `<vertex:1> <eproperty:since> \"tomorrow\" <dng:/>` because edge properties have *single*\n   cardinality,\n * should contain only one `<vertex:2> <edge:knows> <vertex:3> <vertex:1>` statement (however, RDF joins multiple equal\n   statements into one), and\n * should not generate the statement `<vertex:3> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <dt:/> <dng:/>` because\n   vertex 3 has a label.\n\n**Property Graph:**\n\n\t~id,~label,name\n\t2,person,Alice\n\t3,person,Bob\n\t3,,Robert\n\n\t~id,~label,~from,~to,since,personally\n\t1,knows,2,3,yesterday,\n\t1,knows,2,3,tomorrow,\n\t1,knows,2,3,,true\n\n**RDF mapping:**\n\n\t<vertex:2> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <type:Person> <dng:/> .\n\t<vertex:2> <vproperty:name> \"Alice\" <dng:/> .\n\t<vertex:3> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <type:Person> <dng:/> .\n\t<vertex:3> <vproperty:name> \"Bob\" <dng:/> .\n\t<vertex:3> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <dt:/> <dng:/> .\n\t<vertex:3> <vproperty:name> \"Robert\" <dng:/> .\n\n\t<vertex:2> <edge:knows> <vertex:3> <econtext:1> .\n\t<econtext:1> <eproperty:since> \"yesterday\" <dng:/> .\n\t<vertex:2> <edge:knows> <vertex:3> <econtext:1> .\n\t<econtext:1> <eproperty:since> \"tomorrow\" <dng:/> .\n\t<vertex:2> <edge:knows> <vertex:3> <econtext:1> .\n\t<econtext:1> <eproperty:personally> \"true\" <dng:/> .\n\n\n## Building from source\n\nAmazon Neptune CSV to RDF Converter is a Java Maven project and requires JDK 8 and Maven 3 to build from source. Change\ninto the source folder containing the file `pom.xml` and run `mvn clean install`. The directory `target/` contains the\nexecutable Jar library `amazon-neptune-csv2rdf.jar` after a successful build. The executable Jar is not attached to the\nbuild artifacts.\n\nActivate the profile *integration* for running the integration tests during the build by using\n`mvn -Pintegration clean install`. Integration tests are distinguished from other tests by adding\nthe annotation `@Tag(\"IntegrationTest\")`.\n\n\n## Adding the library to your build\n\nThe group id of Amazon Neptune CSV to RDF Converter \\[[javadoc](https://www.javadoc.io/doc/software.amazon.neptune/amazon-neptune-csv2rdf)\\]\nis `software.amazon.neptune`, its artifact id is `amazon-neptune-csv2rdf`. In case you want to use\nthe library as part of another project, use the following to add a dependency in Maven:\n\n\t<dependency>\n\t\t<groupId>software.amazon.neptune</groupId>\n\t\t<artifactId>amazon-neptune-csv2rdf</artifactId>\n\t\t<version>1.0.0</version>\n\t</dependency>\n\n\n## License\n\nAmazon Neptune CSV to RDF Converter is available under [Apache License, Version 2.0](https://aws.amazon.com/apache2.0).\n\n\n----\n\nCopyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n", "release_dates": []}, {"name": "amazon-neptune-for-graphql", "description": "Amazon Neptune utility for GraphQL\u2122 schemas and resolvers", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<br>\n<img src=\"https://github.com/aws/amazon-neptune-for-graphql/blob/main/doc/images/utilityRunning.gif\" width=\"180\" height=\"100\">\n\n# **Amazon Neptune utility for GraphQL&trade; schemas and resolvers**\n\nThe Amazon Neptune utility for GraphQL&trade; is a Node.js command-line utility to help with the creation and maintenance of a GraphQL API for the Amazon Neptune database. It is a no-code solution for a GraphQL resolver when GraphQL queries have a variable number of input parameters and return a variable number of nested fields.\n\nIf you **start from a Neptune database with data**, the utility discover the graph database schema including nodes, edges, properties and edges cardinality, generate the GraphQL schema with the directives required to map the GraphQL types to the graph databases nodes and edges, and auto-generate the resolver code. We optimized the resolver code to reduce the latency of querying Amazon Neptune by returning only the data requested by the GraphQL query. *(Note: the utility works only for Property Graph databases, not RDF yet)*\n\nYou can also **start with a GraphQL schema with your types and an empty Neptune database**. The utility will process your starting GraphQL schema and inference the directives required to map it to the Neptune database graph nodes and edges. You can also **start with GraphQL schema with the directives**, that you have modified or created.\n\nThe utility has the option to **generate the AWS resources** of the entire pipeline, including the AWS AppSync API, configuring the roles, data source, schema and resolver, and the AWS Lambda that queries Amazon Neptune.\n\nIf you have a **few queries** with a static number of input parameters and return fields, and you are willing to code your GraphQL resolver, look at these blogs:\n\n- [Building Serverless Calorie tracker application with AWS AppSync and Amazon Neptune](https://github.com/aws-samples/aws-appsync-calorie-tracker-workshop)\n- [Integrating alternative data sources with AWS AppSync: Amazon Neptune and Amazon ElastiCache](https://aws.amazon.com/blogs/mobile/integrating-aws-appsync-neptune-elasticache/)\n\n<br>\n\nIndex:\n- [Install and Setup](#install-and-setup)\n- [Starting from a Neptune database with data](#starting-from-a-neptune-database-with-data)\n- [Starting from a Neptune database with data: Air Routes Example](https://github.com/aws/amazon-neptune-for-graphql/blob/main/doc/routesExample.md)\n- [Starting from a GraphQL schema and an empty Neptune database](#starting-from-a-graphql-schema-and-an-empty-neptune-database)\n- [Starting from a GraphQL schema and an empty Neptune database: Todo Example](https://github.com/aws/amazon-neptune-for-graphql/blob/main/doc/todoExample.md)\n- [Starting from GraphQL schema with directives](#starting-from-a-graphql-schema-with-directives)\n- [Customize the GraphQL schema with directives](#customize-the-graphql-schema-with-directives)\n- [AWS resources for the GraphQL API](#aws-resources-for-the-graphql-api)\n- [Commands reference: utility CLI options](https://github.com/aws/amazon-neptune-for-graphql/blob/main/doc/cliReference.md)\n- [Known Limitations](#known-limitations)\n- [Roadmap](#roadmap)\n- [License](#license)\n- [Contributing](#contributing)\n\n<br>\n\n# Install and Setup \nThe utility requires Node.js to be executed. Some features require reaching to a Neptune database, and having access to the AWS CLI with permissions to create AWS resources. You can also run the utility just to process input files without the need to connect it directly to a Neptune database or to create AWS resources. In this case you will find the GraphQL schemas and the resolver code in the local *output* directory.\n\n### Node.js is required in any scenario\nNode.js is required to run the utility, v18 or above. \n- To install it on macOS or Windows go to the [Node.js website](https://nodejs.org/) to download the installer.\n- To install on an EC2-Instance follow the [AWS documentation](https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-up-node-on-ec2-instance.html).\n\n### Install the Amazon Neptune utility for GraphQL\nThe utility is available as NPM package. To install it:\n\n`npm install @aws/neptune-for-graphql -g`\n\nAfter the installation run the utility `--help`` command to check if runs:\n\n`neptune-for-graphql --help` \n\n### Reach to a Neptune database endpoint\nIf you are starting from a Neptune database with data, you need to enable the utility to reach the database endpoint. By default Neptune databases are accessible only within a VPC.\n\nThe easiet way is to run the utility from an EC2 instance which network is configured within your Neptune database VPC. The minimum size instance to run the utility is ***t2.micro*** (free of charge). During the creation of the instance select the Neptune database VPC using the *Common Security Groups* pulldown menu.\n\n\n#### Next Step\n- Install Node.js\n- [Install AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) if you need to create the AWS resources, or you need an IAM role to access the Neptune database.\n\n<br>\n\n\n# Starting from a Neptune database with data\nIndependently if you are familiar with GraphQL or not, the command below is the fastest way to create a GraphQL API. Starting from an existing Neptune database endpoint, the utility scans the Neptune database discovering the schema of the existing nodes, edges and properties. Based on the graph database schema, it inferences a GraphQL schema, queries and mutations. Then, it creates an AppSync GraphQL API, and the required AWS resources, like a pair of IAM roles and a Lambda function with the GraphQL resolver code. As soon as the utility complete the execution, you will find in the AppSync console your new GraphQL API called *your-new-GraphQL-API-name*API. To test it, use the AppSync \u201cQueries\u201d from the menu. (*Note: follow the setup instructions below to enable your environment to reach the Neptune database and create AWS resources.*)\n\n`neptune-for-graphql --input-graphdb-schema-neptune-endpoint `<*your-neptune-database-endpoint:port*>` --create-update-aws-pipeline --create-update-aws-pipeline-name` <*your-new-GraphQL-API-name*>` --output-resolver-query-https`\n\nIf you run the command above a second time, it will look again at the Neptune database data and update the AppSync API and the Lambda code.\n\nTo rollback, removing all the AWS resources run:\n\n`neptune-for-graphql --remove-aws-pipeline-name` <*your-new-GraphQL-API-name*>\n\n#### References:\n- [here](/doc/routesExample.md) an example using the Air Routes data on Amazon Neptune, showing the outputs of the utility.\n- If you are wondering which AWS resources the utility is creating, look at the section below.\n- To customize the GraphQL schema, look at the section below.\n\n<br>\n\n# Starting from a GraphQL schema and an empty Neptune database\nYou can start from an empty Neptune database and use a GraphQL API to create the data and query it. Running the command below, the utility will automate the creation of the AWS resources. Your *your-graphql-schema-file* must include the GraphQL schema types, like in the TODO example [here](/doc/todoExample.md). The utility will analyze your schema and create an extended version based on your types. It will add queries and mutations for the nodes stored in the graph database, and in case your schema have nested types, it will add relationships between the types stored as edges in the graph database, again see the TODO example [here](/doc/todoExample.md). The utility creates an AppSync GraphQL API, and the required AWS resources, like a pair of IAM roles and a Lambda function with the GraphQL resolver code. As soon as the utility complete the execution, you will find in the AppSync console your new GraphQL API called *your-new-GraphQL-API-name*API. To test it, use the AppSync \u201cQueries\u201d from the menu. (*Note: follow the setup instructions below to enable your environment to reach the Neptune database and create AWS resources.*)\n\n`neptune-for-graphql --input-schema-file `<*your-graphql-schema-file*>` --create-update-aws-pipeline --create-update-aws-pipeline-name` <*your-new-GraphQL-API-name*>` --create-update-aws-pipeline-neptune-endpoint` <*your-neptune-database-endpoint:port*>  ` --output-resolver-query-https`\n\n#### References:\n- [here](https://github.com/aws/amazon-neptune-for-graphql/blob/main/doc/todoExample.md) an example using a TODO GraphQL schema, showing the outputs of the utility.\n- If you are wondering which AWS resources the utility is creating, look at the section below.\n- To customize the GraphQL schema, look at the section below.\n\n<br>\n\n# Starting from a GraphQL schema with directives\nYou can start from a GraphQL schema with directives for a graph database. For the list of supported directives see the section below [Customize the GraphQL schema with directives](#customize-the-graphql-schema-with-directives). \n\n\n`neptune-for-graphql --input-schema-file `<*your-graphql-schema-file-with-directives*>` --create-update-aws-pipeline --create-update-aws-pipeline-name` <*your-new-GraphQL-API-name*> `--create-update-aws-pipeline-neptune-endpoint` <*your-neptune-database-endpoint:port*>` --output-resolver-query-https`\n\n\n<br>\n\n# Customize the GraphQL schema with directives\nThe utility generate a GraphQL schema with directives, queries and mutations. You might want to use it as is, or change it. Here below ways to change it.\n\n### Please no mutations in my schema\nIn case you don't want your Graph API having the option of updating your database data, add the the CLI option `--output-schema-no-mutations`.\n\n\n\n### @alias\nThis directive can be applied to GraphQL schema types or fields. It maps different names between the graph database and the GraphQL schema. The syntax is *@alias(property: your-name)*. In the example below *airport* is the graph database node label mapped to the *Airport* GraphQL type, and *desc* is the property of the graph database node mapped to the field *description*. See the [Air Routes Example](https://github.com/aws/amazon-neptune-for-graphql/blob/main/doc/routesExample.md). The GraphQL guidence is pascal case for types and camel case for fields.\n\n```graphql\ntype Airport @alias(property: \"airport\") {  \n  city: String\n  description: String @alias(property: \"desc\")  \n}\n```\n\n### @relationship\nThis directive maps nested GraphQL types to a graph databases edges. The syntax is *@relationship(edgeType: graphdb-edge-name, direction: IN|OUT)*. \n<br>\nSee the [Todo Example](https://github.com/aws/amazon-neptune-for-graphql/blob/main/doc/todoExample.md) and the [Air Routes Example](https://github.com/aws/amazon-neptune-for-graphql/blob/main/doc/routesExample.md).\n\n```graphql\ntype Airport @alias(property: \"airport\") {\n  ...\n  continentContainsIn: Continent @relationship(edgeType: \"contains\", direction: IN)\n  countryContainsIn: Country @relationship(edgeType: \"contains\", direction: IN)\n  airportRoutesOut(filter: AirportInput, options: Options): [Airport] @relationship(edgeType: \"route\", direction: OUT)\n  airportRoutesIn(filter: AirportInput, options: Options): [Airport] @relationship(edgeType: \"route\", direction: IN)\n}\n```\n\n### @graphQuery, @cypher\nYou can define your openCypher queries to resolve a field value, add queries or mutations. \n\nHere new a field *outboundRoutesCount* is added to the type *Airport* to count the outboud routes:\n\n```graphql\ntype Airport @alias(property: \"airport\") {\n  ...\n  outboundRoutesCount: Int @graphQuery(statement: \"MATCH (this)-[r:route]->(a) RETURN count(r)\")\n}\n```\nHere an example of new queries and mutations. Note that if you omit the *RETURN*, the resolver will assume the keyword *this* is the returning scope.\n```graphql\ntype Query {\n  getAirportConnection(fromCode: String!, toCode: String!): Airport @cypher(statement: \"MATCH (:airport{code: '$fromCode'})-[:route]->(this:airport)-[:route]->(:airport{code:'$toCode'})\")   \n}\n\ntype Mutation {\n  createAirport(input: AirportInput!): Airport @graphQuery(statement: \"CREATE (this:airport {$input}) RETURN this\")\n  addRoute(fromAirportCode:String, toAirportCode:String, dist:Int): Route @graphQuery(statement: \"MATCH (from:airport{code:'$fromAirportCode'}), (to:airport{code:'$toAirportCode'}) CREATE (from)-[this:route{dist:$dist}]->(to) RETURN this\")\n}\n```\n\nYou can add query or mututation using a Gremlin query. At this time Gremlin queries are limited to return *scalar* values, or *elementMap()* for a single node, or *elementMap().fold()* for a list of nodes.\n```graphql\ntype Query {\n  getAirportWithGremlin(code:String): Airport @graphQuery(statement: \"g.V().has('airport', 'code', '$code').elementMap()\") # single node\n  getAirportsWithGremlin: [Airport] @graphQuery(statement: \"g.V().hasLabel('airport').elementMap().fold()\") # list of nodes\n  getCountriesCount: Int @graphQuery(statement: \"g.V().hasLabel('country').count()\")  # scalar example\n}\n```\n\n### @id\nThe directive @id identify the field mapped to the graph database entity id. Graph databases like Amazon Neptune always have a unique id for nodes and edges assigned during bulk imports or autogenerated. \nIn the example below _id \n```graphql\ntype Airport {\n  _id: ID! @id\n  city: String\n  code: String  \n}\n```\n\n### Reserved types, queries and mutations names\nThe utility autogenerates queries and mutations to enable you to have a working GraphQL API just after having ran the utility. The pattern of these names are recognized by the resolver and are reserved. Here an example for the type *Airport* and the connecting type *Route*:\n\nThe type *Options* is reserved. \n```graphql\ninput Options {\n  limit: Int\n}\n```\n\nThe function parameters *filter*, and *options* are reserved.\n```graphql\ntype Query {  \n    getNodeAirports(filter: AirportInput, options: Options): [Airport]\n}\n```\n\nThe beginning of query names *getNode...*, and the beginning of the mutations names like *createNode...*, *updateNode...*, *deleteNode...*, *connectNode...*, *updateEdge...*, and *deleteEdge...*.\n```graphql\ntype Query {  \n  getNodeAirport(id: ID, filter: AirportInput): Airport\n  getNodeAirports(filter: AirportInput): [Airport]\n}\n\ntype Mutation {  \n  createNodeAirport(input: AirportInput!): Airport\n  updateNodeAirport(id: ID!, input: AirportInput!): Airport\n  deleteNodeAirport(id: ID!): Boolean  \n  connectNodeAirportToNodeAirportEdgeRoute(from: ID!, to: ID!, edge: RouteInput!): Route\n  updateEdgeRouteFromAirportToAirport(from: ID!, to: ID!, edge: RouteInput!): Route\n  deleteEdgeRouteFromAirportToAirport(from: ID!, to: ID!): Boolean\n}\n```\n\n### Re-apply your changes with --input-schema-changes-file\nYou might want to modify the GraphQL source schema and run the utility again getting the latest schema from your Neptune database. Every time the utility discover the a new graphdb schema it generates a new GraphQL schema. To inject your changes, you can manually edit the GraphQL source schema, and run the utility again using it as input instead of the Neptune database endpoint, or write your changes the file format below. As you run the utility with the option `--input-schema-changes-file <value>`, your changes will be applied at once.\n```json\n[\n     { \"type\": \"graphQLTypeName\",\n       \"field\": \"graphQLFieldName\",\n       \"action\": \"remove|add\",\n       \"value\": \"value\"\n    }\n]\n```\nFor Example:\n\n```json\n[\n    { \"type\": \"Airport\", \"field\": \"outboundRoutesCountAdd\", \"action\": \"add\", \"value\":\"outboundRoutesCountAdd: Int @graphQuery(statement: \\\"MATCH (this)-[r:route]->(a) RETURN count(r)\\\")\"},    \n    { \"type\": \"Mutation\", \"field\": \"deleteNodeVersion\", \"action\": \"remove\", \"value\": \"\" },\n    { \"type\": \"Mutation\", \"field\": \"createNodeVersion\", \"action\": \"remove\", \"value\": \"\" }\n]\n```\n\n\n<br>\n\n# AWS resources for the GraphQL API\nYou have three option to created the GraphQL API pipeline:\n- Let the utility create the AWS resources\n- Let the utility create a CDK file, then you run it\n- You manually create the AWS resources \n\nIndependently of the method you or the utility will need to create the following resources:\n- Create a IAM role for Lambda called LambdaExecutionRole\n- Attach to the LambdaExecutionRole the IAM policy AWSLambdaBasicExecutionRole\n- For VPC (default) attach to the LambdaExecutionRole the IAM policy AWSLambdaVPCAccessExecutionRole\n- For IAM create and attach to the LambdaExecutionRole a new IAM policy that only allow to connect and query Neptune\n- Create a Lambda function with the LambdaExecutionRole\n- Create a IAM for AppSync API to call the Lambda called LambdaInvocationRole\n- Attach to the LambdaInvocationRole the policy LambdaInvokePolicy\n- Create the AppSync GraphQL API \n- Add to the AppSync API a Function with the LambdaInvocationRole to call the Lambda\n\n\n### Let the utility create the resources\nWith the CLI option `--create-update-aws-pipeline`, and its accesory options ([see the commands reference](https://github.com/aws/amazon-neptune-for-graphql/blob/main/doc/cliReference.md)), the utility automatically creates the resources.<br>\nYou need to run the utility from a shell in which you installed the AWS CLI, and it is configured for a user with the permission of creating AWS resources. <br>\nThe utility creates a file with the list of resources named *pipeline-name.resources.json*. Then it uses the file to modify the existing resources when the user runs the command again, or when the user wants to delete the AWS resources with the command option `--remove-aws-pipeline-name <value>`.\nThe code of the utiliy uses the JavaScript AWS sdk v3, if you'd like to review the code is only in [this file](https://github.com/aws/amazon-neptune-for-graphql/blob/main/src/pipelineResources.js).\n\n### I prefer a CDK file\nTo option to trigger the creation of the CDK file starts with `--output-aws-pipeline-cdk`, and its accessory options ([see the commands reference](https://github.com/aws/amazon-neptune-for-graphql/blob/main/doc/cliReference.md)). <br> \nAfter you ran it, you will find in the *output* folder the file *CDK-pipeline-name-cdk.js* and *CDK-pipeline-name.zip*. The ZIP file is the code for the Lambda function.\nSee CDK end to end example [here](https://github.com/aws/amazon-neptune-for-graphql/blob/main/doc/cdk.md).\n\n### Let me setup the resources manually or with my favorite DevOps toolchain\n[Here](https://github.com/aws/amazon-neptune-for-graphql/blob/main/doc/resources.md) the detailed list of resorces needed to configure the GraphQL API pipeline.\n<br>\n\n# Known limitations\n- @graphQuery using Gremlin works only if the query returns a scalar value, one elementMap(), or list as elementMap().fold(), this feature is under development.\n- Neptune RDF database and SPARQL language is not supported.\n<br>\n\n# Roadmap\n- Gremlin resolver.\n- SPARQL resolver for RDF database.\n- Generate GraphQL resolver and configurations for Apollo Server.\n<br>\n\n# License\nCopyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\").\nYou may not use this file except in compliance with the License.\nA copy of the License is located at http://www.apache.org/licenses/LICENSE-2.0\nor in the \"license\" file accompanying this file. This file is distributed\non an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\nexpress or implied. See the License for the specific language governing\npermissions and limitations under the License. [Full licence page](/LICENSE.txt).\n<br>\n\n# Contributing\nFollow AWS open source practices.\n<br>\n[Contributing page.](/CONTRIBUTING.md)\n<br>\n[Code of conduct page.](/CODE_OF_CONDUCT.md)\n<br>\n", "release_dates": ["2023-10-21T21:25:42Z"]}, {"name": "amazon-neptune-gremlin-dotnet-sigv4", "description": "A custom library for Amazon Neptune that enables AWS Signature Version 4 signing by extending the Apache TinkerPop Gremlin .NET client.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon Neptune Gremlin .NET SigV4\n\nThis project provides a custom library that extends the [Apache TinkerPop Gremlin.NET client](https://github.com/apache/tinkerpop/tree/master/gremlin-dotnet) to enable AWS IAM Signature Version 4 signing for establishing authenticated connections to [Amazon Neptune](https://aws.amazon.com/neptune/).\n\nFor example usage refer to: [NeptuneGremlinNETSigV4Example.cs](example/NeptuneGremlinNETSigV4Example.cs). This example shows how to leverage this library for establishing an authenticated connection to Neptune.\n\nFor general information on how to connect to Amazon Neptune using Gremlin and best practices, refer to the [documentation](https://docs.aws.amazon.com/neptune/latest/userguide/best-practices-gremlin.html).\n\n## Usage\n\nA snippet of the code from [NeptuneGremlinNETSigV4Example.cs](example/NeptuneGremlinNETSigV4Example.cs):\n\n```\nvar neptune_host = \"neptune-endpoint\"; // ex: mycluster.cluster.us-east-1.neptune.amazonaws.com\nvar neptune_port = 8182;\n\nvar gremlinServer = new GremlinServer(neptune_host, neptune_port);\nvar gremlinClient = new GremlinClient(gremlinServer, \n    webSocketConfiguration: new SigV4RequestSigner().signRequest(neptune_host, neptune_port));\nvar remoteConnection = new DriverRemoteConnection(gremlinClient);\nvar g = Traversal().WithRemote(remoteConnection);\n```\n\nThe `GremlinClient` library accepts both a `GremlinServer` object as well as a `webSocketConfiguration` object that contains a custom configuration set for establishing the WebSocket connection to Amazon Neptune.  The [SigV4RequestSigner](src/SigV4RequestSigner.cs) library fetchs IAM credentials using the `FallbackCredentialsFactory` API (which works similarly to the [Java Default Credential Provider Chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html)), performs the proper Signature Version 4 signing of an http request, and creates the proper WebSocket configuration based on this signed http request.  One can then pass this `webSocketConfiguration` to the `GremlinClient` to create the connection to Neptune.\n\n### Using within Amazon EC2\n\nTo use this library in an application hosted on EC2, be sure to assign a role to the EC2 instance with the [proper permissions](https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-policy.html) to access Amazon Neptune.  This library will fetch the IAM role credentials from the [EC2 metadata store](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html).  If an IAM role is not assigned to the instance, the library will look for the `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `SESSION_TOKEN` environment variables or look for an AWS CLI credentials file at `~/.aws/credentials`.\n\n### Using within AWS Lambda\n\nTo use this library in an application hosted in a Lambda function, be sure to assign a role to the EC2 instance with the [proper permissions](https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-policy.html) to access Amazon Neptune.  Upon invocation, the Lambda function will import the IAM role's credentials into the following environment variables: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_SESSION_TOKEN`.  This library will use those environment variables to import the credentials and perform the request signing.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2021-11-17T20:29:15Z"]}, {"name": "amazon-neptune-gremlin-java-sigv4", "description": "A Gremlin client for Amazon Neptune that includes AWS Signature Version 4 signing.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon Neptune Gremlin Java Sigv4\n\nAn extension to GremlinDriver with a custom channelizer that enables AWS Signature Version 4 signed requests to [Amazon Neptune](https://aws.amazon.com/neptune). \n \nFor example usage refer to:\n \n-\t[NeptuneGremlinSigV4Example.java](https://github.com/aws/amazon-neptune-gremlin-java-sigv4/blob/master/src/main/java/com/amazon/neptune/gremlin/driver/example/NeptuneGremlinSigV4Example.java): This package can also be used to enable Gremlin Console to send signed requests to Neptune, refer to [Connecting to Neptune Using the Gremlin Console with Signature Version 4 Signing](https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-connecting-gremlin-console.html).\n\nFor the official Amazon Neptune page refer to: https://aws.amazon.com/neptune\n\n## Version\n\n1.x - This series uses TinkerPop 3.3.x client. Note that active maintenance on TinkerPop 3.3.x has stopped and hence, this version is not recommended.\n\n2.x - This series uses TinkerPop 3.4.x client. This major version tracks the latest stable release for this package. Note that a minor version (y in 2.x.y) is bumped whenever a new version of Apache TinkerPop is added as a dependency or a major feature is introduced. All minor versions in 2.x series are backward compatible.\n\nFor more information on compatibility with Amazon Neptune engine releases, see [Use the Latest Version of the Gremlin Java Client](https://docs.aws.amazon.com/neptune/latest/userguide/best-practices-gremlin-java-latest.html). \n\n## License\n\nThis library is licensed under the Apache 2.0 License. \n", "release_dates": ["2021-06-04T14:08:16Z", "2021-01-04T21:53:45Z", "2020-10-24T00:28:51Z", "2020-03-21T03:44:24Z", "2020-03-11T22:34:03Z", "2019-06-26T21:21:26Z", "2019-03-01T21:49:22Z", "2018-09-18T22:40:24Z", "2018-08-31T23:07:47Z", "2018-07-16T23:06:21Z", "2018-06-05T19:08:37Z", "2018-05-30T00:14:20Z"]}, {"name": "amazon-neptune-jdbc-driver", "description": "Amazon Neptune JDBC Driver by Amazon Web Services", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# JDBC Driver for Amazon Neptune\n\nThis driver provides read-only JDBC connectivity for the Amazon Neptune service using SQL, Gremlin, openCypher and SPARQL queries.\n\n## Using the Driver\n\nThe driver is available on [Maven Central](https://search.maven.org/search?q=g:software.amazon.neptune%20AND%20a:amazon-neptune-jdbc-driver).\nIn addition to the regular jar, a shadow/uber/fat jar is also published with the classifier `all`. \n\nTo use the Driver in BI tools, please refer to the documentation below. \n\nTo connect to Amazon Neptune using the JDBC driver, the Neptune instance must be available through an SSH tunnel, load balancer, or the JDBC driver must be deployed in an EC2 instance.\n\n**SSH Tunnel and host file must be configured before using the drive to connect to Neptune, please see [SSH configuration](markdown/setup/configuration.md).**\n\n### Specifications\n\nThis driver is compatible with JDBC 4.2 and requires a minimum of Java 8.\n\n### Compatibility with AWS Neptune\n\n| Engine Release             | Driver Version |\n|----------------------------|----------------|\n| < 1.1.1.0                  | 1.1.0          |\n| &ge; 1.1.1.0 and < 1.2.0.0 | 2.0.0+         |\n| &ge; 1.2.0.0 and < 1.2.1.0 | 3.0.0+         |\n| &ge; 1.2.1.0               | 3.0.2+         |\n\n### Connection URL and Settings\n\nTo set up a connection, the driver requires a JDBC connection URL. The connection URL is generally of the form:\n\n```\njdbc:neptune:[connectionType]://[host];[propertyKey1=value1];[propertyKey2=value2]..;[propertyKeyN=valueN]\n```\n\nA basic example of a connection string is:\n\n```jdbc:neptune:sqlgremlin://neptune-example.com;port=8182```\n\nSpecific requirements for the string can be found [below](#graph-query-language-support) in the specific query language documentation.\n\n### Connecting using the DriverManager Interface\n\nIf the jar is in the application's classpath, no other configuration is required. The driver can be connected to using the JDBC DriverManager by connecting using an Amazon Neptune connection string.\n\nBelow is an example where Neptune is accessible through the endpoint neptune-example.com on port 8182.\n\n**Reminder: The Neptune endpoint is only accessible if a SSH tunnel is established to a EC2 instance in the same Amazon VPC as the Neptune cluster.**\n\nIn this example, the SSH tunnel would have been established by running something similar to the following in a shell:\n\n```ssh -i \"ec2Access.pem\" -L 8182:neptune-example.com:8182 ubuntu@ec2-34-229-221-164.compute-1.amazonaws.com -N ```\n\nThe full documentation for how to establish the SSH tunnel can once again be found [here](markdown/setup/configuration.md).\n\n```\nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.Statement;\n\nvoid example() {\n    String url = \"jdbc:neptune:sqlgremlin://neptune-example.com;port=8182\";\n\n    Connection connection = DriverManager.getConnection(url);\n    Statement statement = connection.createStatement();\n    \n    connection.close();\n}\n```\n\nRefer to the connection string options in the specific query language documentation below for more information about configuring the connection.\n\nFor more example applications, see the [sample applications](./src/test/java/sample/applications).\n\n## Graph Query Language Support\n\n### SQL\nThe driver supports a subset of SQL-92 and some common extensions. \n\nTo connection to Amazon Neptune using SQL, please see the [SQL connection configurations](markdown/sql.md) for details about connection string configurations.\n\n### Gremlin\n\nGremlin is a graph traversal language supported by Neptune. To issue Gremlin queries to Neptune though the driver, please see\n[Gremlin connection configurations](markdown/gremlin.md).\n\n### openCypher\n\nopenCypher is an open query language for property graph database supported by Neptune. To issue openCypher queries to Neptune though the driver, please see\n[openCypher connection configurations](markdown/opencypher.md).\n\n### SPARQL\n\nSPARQL is an RDF query language supported by Neptune. To issue SPARQL queries to Neptune though the driver, please see\n[SPARQL connection configurations](markdown/sparql.md).\n\n## Driver Setup in BI Applications\n\nTo learn how to set up the driver in various BI tools, instructions are outlined here for:\n* [Tableau Desktop](markdown/bi-tools/tableau.md)\n* [DbVisualizer](markdown/bi-tools/DbVisualizer.md)\n* [DBeaver](markdown/bi-tools/DBeaver.md)\n\n## Troubleshooting\n\nTo troubleshoot or debug issues with the JDBC driver, please see the [troubleshooting instructions](markdown/troubleshooting.md).\n\n## Contributing\n\nBecause the JDBC driver is available as open source, contribution from the community is encouraged. If you are interested in improving performance, adding new features, or fixing bugs, please see our [contributing guidelines](./CONTRIBUTING.md).\n\n## Building from Source\n\nIf you wish to contribute, you will need to build the driver. The requirements to build the driver are very simple, you only need a Java 8 compiler with a runtime environment and you can build and run the driver.\n\n## Security Issue Notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public GitHub issue.\n\n## Licensing\n\nSee the [LICENSE](./LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\n## Copyright\n\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n", "release_dates": ["2024-03-01T03:38:04Z", "2023-05-29T23:35:27Z", "2022-12-22T22:44:47Z", "2022-10-20T00:11:36Z", "2022-10-03T21:15:42Z", "2021-12-16T22:22:30Z", "2021-12-02T21:21:53Z", "2021-11-12T23:31:37Z"]}, {"name": "amazon-neptune-sigv4-signer", "description": "A library for Amazon Neptune that enables AWS Signature Version 4 signing for HTTP using Netty.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon Neptune SigV4 Signer\n\nA library for sending AWS Signature Version 4 signed requests over HTTP to [Amazon Neptune](https://aws.amazon.com/neptune). This package provides signers that can be used with various implementations of HttpRequests:\n\n1. [NeptuneApacheHttpSigV4Signer.java](https://github.com/aws/amazon-neptune-sigv4-signer/blob/master/src/main/java/com/amazonaws/neptune/auth/NeptuneApacheHttpSigV4Signer.java) - provides an implementation for signing Apache Http Requests.\n2. [NeptuneNettyHttpSigV4Signer.java](https://github.com/aws/amazon-neptune-sigv4-signer/blob/master/src/main/java/com/amazonaws/neptune/auth/NeptuneNettyHttpSigV4Signer.java) - provides an implementation for signing Netty Http requests\n3. [NeptuneRequestMetadataSigV4Signer.java](https://github.com/aws/amazon-neptune-sigv4-signer/blob/master/src/main/java/com/amazonaws/neptune/auth/NeptuneRequestMetadataSigV4Signer.java) - provides an implementation for a generic Request object RequestMetadata. A user of this class can convert their native HttpRequest into a RequestMetadata object and pass it to this class to create the signature.\n \nFor examples of usage of this library refer to:\n\n1. [Connecting to Neptune Using Java and Gremlin with Signature Version 4 Signing](https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-connecting-gremlin-java.html)\n2. [amazon-neptune-sparql-java-sigv4](https://github.com/aws/amazon-neptune-sparql-java-sigv4)  - Contains examples for sending SigV4 signed requests with Apache HttpUriRequest objects.\n \nFor more documentation around IAM database authentication for Neptune refer to [Identity and Access Management in Amazon Neptune](https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth.html)\n\nFor the official Amazon Neptune page refer to: https://aws.amazon.com/neptune\n\n## License\n\nThis library is licensed under the Apache 2.0 License. \n", "release_dates": ["2021-06-04T13:59:29Z", "2020-10-24T00:23:47Z", "2020-03-21T03:40:54Z", "2020-03-11T22:33:39Z", "2019-06-26T21:18:34Z", "2018-09-18T21:49:58Z", "2018-08-31T23:05:48Z", "2018-07-16T23:06:15Z", "2018-06-05T19:08:34Z", "2018-05-29T23:35:26Z"]}, {"name": "amazon-neptune-sparql-java-sigv4", "description": "A SPARQL client for Amazon Neptune that includes AWS Signature Version 4 signing. Implemented as an RDF4J repository. ", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon Neptune Sparql Java Sigv4\n\nA SPARQL client for [Amazon Neptune](https://aws.amazon.com/neptune) that includes AWS Signature Version 4 signing. Implemented as an RDF4J repository and Jena HTTP Client.\n \nFor example usage refer to:\n \n-\t[NeptuneJenaSigV4Example.java](https://github.com/aws/amazon-neptune-sparql-java-sigv4/blob/master/src/main/java/com/amazonaws/neptune/client/jena/NeptuneJenaSigV4Example.java) \n-\t[NeptuneRdf4JSigV4Example.java](https://github.com/aws/amazon-neptune-sparql-java-sigv4/blob/master/src/main/java/com/amazonaws/neptune/client/rdf4j/NeptuneRdf4JSigV4Example.java) \n\nFor the official Amazon Neptune page refer to: https://aws.amazon.com/neptune\n\n## License\n\nThis library is licensed under the Apache 2.0 License. \n", "release_dates": ["2021-06-04T14:03:31Z", "2020-10-24T00:32:10Z", "2020-03-21T03:47:50Z", "2020-03-11T22:34:18Z", "2019-06-26T21:23:26Z", "2018-09-18T22:43:01Z", "2018-08-31T23:08:52Z", "2018-07-16T23:06:18Z", "2018-06-05T19:08:32Z", "2018-05-30T00:13:31Z"]}, {"name": "amazon-network-policy-controller-k8s", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon Network Policy Controller for Kubernetes\n\nController for Kubernetes NetworkPolicy resources.\n\nNetwork Policy Controller resolves the configured network policies and publishes the resolved endpoints via Custom CRD (`PolicyEndpoints`) resource.\n\n## Getting Started\n\nWhen you create a new Amazon EKS cluster, the network policy controller is automatically installed on the EKS control plane. It actively monitors the creation of network policies within your cluster and reconciles policy endpoints. Subsequently, the controller instructs the node agent to create or update eBPF programs on the node by publishing pod information through the policy endpoints. Network policy controller configures policies for pods in parallel to pod provisioning, until then new pods will come up with default allow policy. All ingress and egress traffic is allowed to and from the new pods until they are reconciled against the existing policies. To effectively manage network policies on self-managed Kubernetes clusters, you need to deploy a network policy controller on a node.\n\nStay tuned for additional instructions for installing Network Policy Controller on nodes. The controller image is published to AWS ECR.\n\nThe controller does not require any IAM policies. It does not make AWS API calls. \n\n### Prerequisites\n\n- Kubernetes Version - 1.25+\n- Amazon VPC CNI version - 1.14.0+\n\n## Security Disclosures \n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues. Instead, please follow the\ninstructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n## Contributing\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for further details.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2024-02-19T02:05:49Z", "2024-01-09T22:40:00Z", "2024-01-03T18:55:47Z", "2023-12-21T18:12:22Z", "2023-11-16T01:09:50Z", "2023-08-29T16:39:18Z", "2023-07-12T01:15:20Z", "2023-07-11T00:19:45Z"]}, {"name": "amazon-q-connectjs", "description": null, "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# amazon-q-connectjs\n\nAmazon Q in Connect, an LLM-enhanced evolution of Amazon Connect Wisdom. The Amazon Q Connect JavaScript library (QConnectJS) gives you the power to build your own Amazon Q Connect widget.\n\nThe library uses an Amazon Connect authentication token to make API calls to Amazon Q Connect and supports all Amazon Q Connect `Agent Assistant` functionality. For example, you can manually query knowledge documents, get knowledge content, or start generating automated suggestions.\n\nQConnectJS supports the following APIs:\n* [QueryAssistant](#QueryAssistant)\n* [GetContact](#GetContact)\n* [GetRecommendations](#GetRecommendations)\n* [NotifyRecommendationsReceived](#NotifyRecommendationsReceived)\n* [GetContent](#GetContent)\n* [SearchSessions](#SearchSessions)\n* [ListIntegrationAssociations](#ListIntegrationAssociations)\n\n\nNote that this library must be used in conjunction with [amazon-connect-streams](https://github.com/amazon-connect/amazon-connect-streams).\n\n## Learn More\n\nFor more advanced features, all Amazon Q Connect functionality is accessible using the public API. For example, you can create an assistant and a knowledge base. Check out [Amazon Q Connect](https://docs.aws.amazon.com/cli/latest/reference/amazon-q-connect/index.html) available via the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html).\n\nTo learn more about Amazon Q Connect and its capabilities, please check out the [Amazon Q Connect User Guide](https://docs.aws.amazon.com/connect/latest/adminguide/amazon-q-connect.html).\n\nTo learn more about Amazon Connect and its capabilities, please check out the [Amazon Connect User Guide](https://docs.aws.amazon.com/connect/latest/adminguide/what-is-amazon-connect.html).\n\n# Getting Started\n\n## Prerequisites\n\n### Create an Amazon Connect Instance\n\nThe first step in setting up your Amazon Connect contact center is to create a virtual contact center instance. Each instance contains all the resources and settings related to your contact center. Follow the [Get started with Amazon Connect](https://docs.aws.amazon.com/connect/latest/adminguide/amazon-connect-get-started.html) user guide to get started.\n\n### Enable Amazon Q Connect For Your Instance\n\nTo utilize QConnectJS you should start by enabling Amazon Q Connect for your Amazon Connect instance. Follow the [Enable Amazon Q Connect](https://docs.aws.amazon.com/connect/latest/adminguide/enable-q-connect.html) user guide to get started.\n\n### Set Up Integrated Applications\n\nAll domains looking to integrate with Amazon Connect and Amazon Q Connect must be explicitly allowed for cross-domain access to the instance. For example, to integrate with your custom agent application, you must place your agent application domain in an allow list. To allow list a domain URL follow the [app integration guide](https://docs.aws.amazon.com/connect/latest/adminguide/app-integration.html).\n\n### A few things to note:\n* Allowlisted domains must be HTTPS.\n* All of the pages that attempt to initialize the QConnectJS library must be hosted on domains that are allowlisted.\n\n# Usage\n\n## Install from NPM\n\n```bash\nnpm install amazon-q-connectjs\n```\n\n## Build with NPM\n\n```bash\n$ git clone https://github.com/aws/amazon-q-connectjs\ncd amazon-q-connectjs\nnpm install\nnpm run bundle\n```\n\nFind build artifacts in the `release` directory. This will generate a file called `amazon-q-connectjs.js` and a minified version `amazon-q-connectjs-min.js`. This is the full QConnectJS client which you will want to include in your page.\n\n## Download from Github\n\n`amazon-q-connectjs` is available on [NPM](https://www.npmjs.com/package/amazon-q-connectjs) but if you'd like to download it here, you can find build artificacts in the [release](/release) directory.\n\n## Load from CDN\n\n`amazon-q-connectjs` is also available on open source CDNs. If you'd like to load build artifacts from a CDN, you can use either of the script tags below.\n\n```html\n<script src=\"https://cdn.jsdelivr.net/npm/amazon-q-connectjs@1/release/amazon-q-connectjs.js\"></script>\n```\n\n```html\n<script src=\"https://cdn.jsdelivr.net/npm/amazon-q-connectjs@1/release/amazon-q-connectjs-min.js\"></script>\n```\n\n# Initialization\n\nInitializing the QConnectJS client is the fist step to verify that you have everything setup correctly.\n\n```html\n<!doctype html>\n<html>\n  <head>\n    <meta charset=\"UTF-8\">\n    <script type=\"text/javascript\" src=\"connect-streams-min.js\"></script>\n    <script type=\"text/javascript\" src=\"amazon-q-connectjs-min.js\"></script>\n  </head>\n  <!-- Add the call to init() as an onload so it will only run once the page is loaded -->\n  <body onload=\"init()\">\n    <div id='ccp-container' style=\"width: 400px; height: 800px;\"></div>\n    <script type=\"text/javascript\">\n      const instanceUrl = 'https://my-instance-domain.awsapps.com/connect';\n\n      function init() {\n        // Initialize StreamsJS API\n        connect.agentApp.initApp(\n          'ccp',\n          'ccp-container',\n          `${instanceUrl}/ccp-v2/`,\n          {\n            ccpParams: {\n              style: 'width:400px; height:600px;',\n            }\n          }\n        );\n  \n        // Initialize QConnectJS client with either \"QConnectClient\" or \"Client\"\n        const qConnectClient = new connect.qconnectjs.QConnectClient({\n          instanceUrl,\n        });\n\n        const qConnectClient = new connect.qconnectjs.Client({\n          instanceUrl: instanceUrl,                                        // REQUIRED\n          endpoint: \"https://my-instance-domain.awsapps.com/connect/api\",  // optional, defaults to '<instanceUrl>'\n          callSource: \"agent-app\",                                         // optional, defaults to 'agent-app'\n          serviceId: 'AmazonQConnect',                                     // optional, defaults to 'AmazonQConnect'\n          maxAttempts: 3,                                                  // optional, defaults to 3\n          logger: {},                                                      // optional, if provided overrides default logger\n          headers: {},                                                     // optional, if provided overrides request headers\n          requestHandler: {},                                              // optional, if provided overrides the default request handler\n        });\n      }\n    </script>\n  </body>\n</html>\n```\n\nThe QConnectJS client integrates with Connect by loading the pre-built Amazon Q Connect widget located at `<instanceUrl>/wisdom-v2` into an iframe and placing it into a container div. API requests are funneled through this widget and made available to your JS client code.\n\n* `instanceUrl`: The Connect instance url.\n* `endpoint`: Optional, set to override the Connect endpoint to use.\n* `callSource`: Optional, set to override the call source identifier on requests.\n* `headers`: This object is optional and allows overriding the headers provided to the HTTP handler.\n* `logger`: This object is optional and allows overriding the default Logger for logging debug/info/warn/error messages.\n* `maxAttempts`: Optional, set to specify how many times a request will be made at most in case of retry.\n* `requestHandler`: This object is optional and allows overriding the default request handler.\n* `serviceId`: Optional, set to override the unique service identifier on requests.\n\n\n# ES Modules\n\n## Imports\n\nQConnectJS is modularized by client and commands.\nTo send a request, you only need to import the `QConnectClient` and\nthe commands you need, for example `GetRecommendations`:\n\n```js\n// ES5 example\nconst { Client, GetRecommendations } = require(\"amazon-q-connectjs\");\n```\n\n```ts\n// ES6+ example\nimport { Client, GetRecommendations } from \"amazon-q-connectjs\";\n```\n\n## Convenience Methods\n\nThe QConnectJS client can also send requests using convenience methods. However, it results in a bigger bundle size.\n\n```ts\nimport { QConnectClient } from \"amazon-q-connectjs\";\n\nconst qConnectClient = new QConnectClient({\n  instanceUrl: \"https://your-connect-instance.my.connect.aws\",\n});\n\n// async/await.\ntry {\n  const response = await qConnectClient.getRecommendations(params);\n  // process response.\n} catch (error) {\n  // error handling.\n}\n\n// Promises.\nqConnectClient\n  .getRecommendations(params)\n  .then((response) => {\n    // process response.\n  })\n  .catch((error) => {\n    // error handling.\n  });\n```\n\n# Sending Requests\n\nTo send a request, you:\n\n- Initiate the client with the desired configuration (e.g. `instanceUrl`, `endpoint`).\n- call the desired API\n\n```js\nconst qConnectClient = new QConnectClient({\n  instanceUrl: \"https://your-connect-instance.my.connect.aws\",\n});\n\nqConnectClient.getRecommendations({\n  // input parameters\n});\n```\n\n# Reading Responses\n\nAll API calls through QConnectJS return a promise. The promise resolves/rejects to provide the response from the API call.\n\n## Async/await\n\nWe recommend using the [await](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/await)\noperator.\n\n```js\n// async/await.\ntry {\n  const response = await qConnectClient.getRecommendations({\n    // input parameters\n  });\n  // process response.\n} catch (error) {\n  // error handling.\n} finally {\n  // finally.\n}\n```\n\n`async`-`await` is clean, concise, intuitive, easy to debug and has better error handling\nas compared to using Promise chains.\n\n## Promises\n\nYou can also use [Promise chaining](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Using_promises#chaining).\n\n```js\nqConnectClient.getRecommendations({\n  // input parameters\n}).then((response) => {\n  // process response.\n}).catch((error) => {\n  // error handling.\n}).finally(() => {\n  // finally.\n});\n```\n\n# APIs\n\n## QueryAssistant\n\nPerforms a manual search against the specified assistant. To retrieve recommendations for an assistant, use GetRecommendations. For more information check out the [QueryAssistant](https://docs.aws.amazon.com/amazon-q-connect/latest/APIReference/API_QueryAssistant.html) API reference.\n\n### URI Request Parameters\n\n* `assistantId`: The identifier of the Amazon Q Connect assistant. Can be either the ID or the ARN. URLs cannot contain the ARN.\n* `queryText`: The text to search for.\n* `maxResults`: The maximum number of results to return per page.\n* `nextToken`: The token for the next set of results. Use the value returned in the previous response in the next request to retrieve the next set of results.\n\n#### A few things to note:\n\n* The `assistantId` can be retrieved by using the `ListIntegrationAssociations` API provided by QConnectJS to look up the `assistant` and `knowledge base` that has been configured for Amazon Q Connect. See [ListIntegrationAssociations](#ListIntegrationAssociations) for more information.\n\n### Response Syntax\n\nIf the action is successful, the service sends back an HTTP 200 response.\n\n```json\n{\n  \"nextToken\": \"string\",\n  \"results\": [\n    {\n      \"document\": {\n        \"contentReference\": {\n          \"contentArn\": \"string\",\n          \"contentId\": \"string\",\n          \"knowledgeBaseArn\": \"string\",\n          \"knowledgeBaseId\": \"string\"\n        },\n        \"excerpt\": {\n          \"highlights\": [\n            {\n              \"beginOffsetInclusive\": \"number\",\n              \"endOffsetExclusive\": \"number\"\n            }\n          ],\n          \"text\": \"string\"\n        },\n        \"title\": {\n          \"highlights\": [\n            {\n              \"beginOffsetInclusive\": \"number\",\n              \"endOffsetExclusive\": \"number\"\n            }\n          ],\n          \"text\": \"string\"\n        }\n      },\n      \"relevanceScore\": \"number\",\n      \"resultId\": \"string\"\n    }\n  ]\n}\n```\n\n### Sample Query\n\n```ts\nconst queryAssistantCommand = new QueryAssistant({\n  assistantId: 'b5b0e4af-026e-4472-9371-d171a9fdf75a',\n  maxResults: 10,\n  queryText: 'cancel order',\n});\n\ntry {\n  const response = await qConnectClient.call(queryAssistantCommand);\n    // process response.\n} catch (error) {\n  // error handling.\n}\n```\n\n## GetContact\n\nRetrieves contact details, including the Amazon Q Connect `session ARN`, for a specified contact.\n\n### URI Request Parameters\n\n* `awsAccountId`: The identifier of the AWS account. You can find the awsAccountId in the ARN of the instance.\n* `InstanceId`: The identifier of the Amazon Connect instance. You can find the instanceId in the ARN of the instance.\n* `contactId`: The identifier of the Connect contact. Can be either the ID or the ARN. URLs cannot contain the ARN.\n\n#### A few things to note:\n\n* One of the request parameters of the `GetContact` API is the Amazon Connect `contactId`. The StreamsJS Contact API provides event subscription methods and action methods which can be called on behalf of a Contact and used to retrieve the Amazon Connect `contactId`. See [StreamsJS Integration](#StreamsJS-Integration) below for more information.\n\n### Response Syntax\n\nIf the action is successful, the service sends back an HTTP 200 response.\n\n```json\n{\n  \"contactId\": \"string\",\n  \"contactState\": \"string\",\n  \"contactSchemaVersion\": \"number\",\n  \"channel\": \"string\",\n  \"targetQueueResourceId\": \"string\",\n  \"agentResourceId\": \"string\",\n  \"targetAgentResourceId\": \"string\",\n  \"attributes\": {},\n  \"participants\": [],\n  \"contactFeature\": {\n    \"loggingEnabled\": \"boolean\",\n    \"textToSpeechFeatures\": {},\n    \"voiceIdFeatures\": {},\n    \"wisdomFeatures\": {\n      \"wisdomConfig\": {\n        \"sessionArn\": \"string\"\n      }\n    }\n  },\n  \"routingAttributes\": {},\n  \"languageCode\": \"string\",\n  \"channelContext\": {},\n}\n```\n\n### Sample Query\n\n```ts\nconst getContactCommand = new GetContact({\n  awsAccountId: 'b5b0e4af-026e-4472-9371-d171a9fdf75a',\n  instanceId: 'b5b0e4af-026e-4472-9371-d171a9fdf75a',\n  contactId: 'b5b0e4af-026e-4472-9371-d171a9fdf75a',\n});\n\ntry {\n  const response = await qConnectClient.call(getContactCommand);\n    // process response.\n} catch (error) {\n  // error handling.\n}\n```\n\n## GetRecommendations\n\nRetrieves recommendations for the specified session. To avoid retrieving the same recommendations in subsequent calls, use NotifyRecommendationsReceived. This API supports long-polling behavior with the `waitTimeSeconds` parameter. Short poll is the default behavior and only returns recommendations already available. To perform a manual query against an assistant, use the QueryAssistant API. For more information check out the [GetRecommendations](https://docs.aws.amazon.com/amazon-q-connect/latest/APIReference/API_GetRecommendations.html) API reference.\n\n### URI Request Parameters\n\n* `assistantId`: The identifier of the Amazon Q Connect assistant. Can be either the ID or the ARN. URLs cannot contain the ARN.\n* `maxResult`: The maximum number of results to return per page.\n* `sessionId`: The identifier of the session. Can be either the ID or the ARN. URLs cannot contain the ARN.\n* `waitTimeSeconds`: The duration (in seconds) for which the call waits for a recommendation to be made available before returning. If a recommendation is available, the call returns sooner than WaitTimeSeconds. If no messages are available and the wait time expires, the call returns successfully with an empty list.\n\n#### A few things to note:\n\n* The `assistantId` can be retrieved by using the `ListIntegrationAssociations` API provided by QConnectJS to look up the `assistant` and `knowledge base` that has been configured for Amazon Q Connect. See [ListIntegrationAssociations](#ListIntegrationAssociations) for more information.\n* The `session ARN` can be retrieved by used the `GetContact` API provided by QConnectJS to look up the `session` associated with a given active `contact`. See [GetContact](#GetContact) for more information.\n* To avoid retrieving the same recommendations on subsequent calls, the `NotifyRecommendationsReceived` API should be called after each response. See [NotifyRecommendationsReceived]() for more information.\n\n### Response Syntax\n\n```json\n{\n  \"recommendations\": [\n    {\n      \"document\": {\n        \"contentReference\": {\n          \"contentArn\": \"string\",\n          \"contentId\": \"string\",\n          \"knowledgeBaseArn\": \"string\",\n          \"knowledgeBaseId\": \"string\"\n        },\n        \"excerpt\": {\n          \"highlights\": [\n            {\n              \"beginOffsetInclusive\": \"number\",\n              \"endOffsetExclusive\": \"number\"\n            }\n          ],\n          \"text\": \"string\"\n        },\n        \"title\": {\n          \"highlights\": [\n            {\n              \"beginOffsetInclusive\": \"number\",\n              \"endOffsetExclusive\": \"number\"\n            }\n          ],\n          \"text\": \"string\"\n        }\n      },\n      \"recommendationId\": \"string\",\n      \"relevanceLevel\": \"string\",\n      \"relevanceScore\": \"number\"\n    }\n  ]\n}\n```\n\n### Sample Query\n\n```ts\nconst getRecommendationsCommand = new GetRecommendations({\n  assistantId: 'b5b0e4af-026e-4472-9371-d171a9fdf75a',\n  sessionId: '9f2b6fab-9200-46d8-977a-c89be3b34639',\n  maxResults: 10,\n  waitTimeSeconds: 5,\n});\n\ntry {\n  const response = await qConnectClient.call(getRecommendationsCommand);\n    // process response.\n} catch (error) {\n  // error handling.\n}\n```\n\n## NotifyRecommendationsReceived\n\nRemoves the specified recommendations from the specified assistant's queue of newly available recommendations. You can use this API in conjunction with GetRecommendations and a `waitTimeSeconds` input for long-polling behavior and avoiding duplicate recommendations. For more information check out the [NotifyRecommendationsReceived](https://docs.aws.amazon.com/amazon-q-connect/latest/APIReference/API_NotifyRecommendationsReceived.html) API reference.\n\n### URI Request Parameters\n\n* `assistantId`: The identifier of the Amazon Q Connect assistant. Can be either the ID or the ARN. URLs cannot contain the ARN.\n* `sessionId`: The identifier of the session. Can be either the ID or the ARN. URLs cannot contain the ARN.\n* `recommendationIds`: The identifier of the recommendations.\n\n#### A few things to note:\n\n* The `assistantId` can be retrieved by using the `ListIntegrationAssociations` API provided by QConnectJS to look up the `assistant` and `knowledge base` that has been configured for Amazon Q Connect. See [ListIntegrationAssociations](#ListIntegrationAssociations) for more information.\n* The `session ARN` can be retrieved by used the `GetContact` API provided by QConnectJS to look up the `session` associated with a given active `contact`. See [GetContact](#GetContact) for more information.\n\n### Response Syntax\n\n```json\n{\n  \"errors\": [\n    {\n      \"message\": \"string\",\n      \"recommendationId\": \"string\"\n    }\n  ],\n  \"recommendationIds\": [ \"string\" ]\n}\n```\n\n### Sample Query\n\n```ts\nconst notifyRecommendationsReceivedCommand = new NotifyRecommendationsReceived({\n  assistantId: 'b5b0e4af-026e-4472-9371-d171a9fdf75a',\n  sessionId: '9f2b6fab-9200-46d8-977a-c89be3b34639',\n  recommendationIds: [\n    'f9b5fa90-b3ce-45c9-9967-582c87074864',\n  ],\n});\n\ntry {\n  const response = await qConnectClient.call(notifyRecommendationsReceivedCommand);\n    // process response.\n} catch (error) {\n  // error handling.\n}\n```\n\n## GetContent\n\nRetrieves content, including a pre-signed URL to download the content. The `contentId` and `knowledgeBaseId` request parameters are part of search results response syntax when calling `QueryAssistant` or recommendations response syntax when calling `GetRecommendations`. For more information check out the [GetContent](https://docs.aws.amazon.com/amazon-q-connect/latest/APIReference/API_GetContent.html) API reference.\n\n### URI Request Parameters\n\n* `contentId`: The identifier of the content. Can be either the ID or the ARN. URLs cannot contain the ARN.\n* `knowledgeBaseId`: The identifier of the knowledge base. Can be either the ID or the ARN. URLs cannot contain the ARN.\n\n#### A few things to note:\n\n* The `contentId` and `knowledgeBaseId` can be found by using either the `QueryAssistant` or `GetRecommendations` APIs to retrieve knowledge documents. Each of the documents from either response will include a `contentReference`. See [QueryAssistant](#QueryAssistant) and [GetRecommendations]() for more information.\n\n### Sample Query\n\n```ts\nconst getContentCommand = new GetContent({\n  contentId: '9f2b6fab-9200-46d8-977a-c89be3b34639',\n  knowledgeBaseId: 'f9b5fa90-b3ce-45c9-9967-582c87074864',\n});\n\ntry {\n  const response = await qConnectClient.call(getContentCommand);\n    // process response.\n} catch (error) {\n  // error handling.\n}\n```\n\n## SearchSessions\n\nSearches for sessions. For more information check out the [SearchSessions](https://docs.aws.amazon.com/amazon-q-connect/latest/APIReference/API_SearchSessions.html) API reference.\n\n### URI Request Parameters\n\n* `assistantId`: The identifier of the Amazon Q Connect assistant. Can be either the ID or the ARN. URLs cannot contain the ARN.\n* `searchExpression`: The search expression to filter results.\n* `maxResults`: The maximum number of results to return per page.\n* `nextToken`: The token for the next set of results. Use the value returned in the previous response in the next request to retrieve the next set of results.\n\n#### A few things to note:\n\n* The `assistantId` can be retrieved by using the `ListIntegrationAssociations` API provided by QConnectJS to look up the `assistant` and `knowledge base` that has been configured for Amazon Q Connect. See [ListIntegrationAssociations](#ListIntegrationAssociations) for more information.\n\n### Response Syntax\n\n```json\n{\n  \"nextToken\": \"string\",\n  \"sessionSummaries\": [\n    {\n      \"assistantArn\": \"string\",\n      \"assistantId\": \"string\",\n      \"sessionArn\": \"string\",\n      \"sessionId\": \"string\"\n    }\n  ]\n}\n```\n\n### Sample Query\n\n```ts\nconst searchSessionsCommand = new SearchSessions({\n  assistantId: 'b5b0e4af-026e-4472-9371-d171a9fdf75a',\n  searchExpression: {\n    filters: [\n      {\n        operator: 'equals',\n        field: 'name',\n        value: '249bbb30-aede-42a8-be85-d8483c317686',\n      }\n    ]\n  }\n});\n\ntry {\n  const response = await qConnectClient.call(searchSessionsCommand);\n    // process response.\n} catch (error) {\n  // error handling.\n}\n```\n\n## ListIntegrationAssociations\n\nRetrieves Connect integrations, including assistant and knowledge base integrations. For more information check out the [ListIntegrationAssociations](https://docs.aws.amazon.com/connect/latest/APIReference/API_ListIntegrationAssociations.html) API reference.\n\n### URI Request Parameters\n\n* `InstanceId`: The identifier of the Amazon Connect instance. You can find the instanceId in the ARN of the instance.\n* `IntegrationType`: The integration type.\n* `MaxResults`: The maximum number of results to return per page.\n* `nextToken`: The token for the next set of results. Use the value returned in the previous response in the next request to retrieve the next set of results.\n\n#### A few things to note:\n* One of the request parameters of the `ListIntegrationAssociations` API is the Amazon Connect `instanceId`. The StreamsJS Agent API provides event subscription methods and action methods which can be called on behalf of the agent and used to retrieve the Amazon Connect `instanceId`. See [StreamsJS Integration](#StreamsJS-Integration) below for more information.\n\n### Response Syntax\n\nIf the action is successful, the service sends back an HTTP 200 response.\n\n\n```json\n{\n  \"IntegrationAssociationSummaryList\": [\n    {\n      \"InstanceId\": \"string\",\n      \"IntegrationArn\": \"string\",\n      \"IntegrationAssociationArn\": \"string\",\n      \"IntegrationAssociationId\": \"string\",\n      \"IntegrationType\": \"string\",\n      \"SourceApplicationName\": \"string\",\n      \"SourceApplicationUrl\": \"string\",\n      \"SourceType\": \"string\"\n    }\n  ],\n  \"NextToken\": \"string\"\n}\n```\n\n### Sample Query\n\n```ts\nconst listIntegrationAssociationsCommand = new ListIntegrationAssociations({\n  InstanceId: 'b5b0e4af-026e-4472-9371-d171a9fdf75a',\n  IntegrationType: 'WISDOM_ASSISTANT',\n  MaxResults: 10,\n});\n\ntry {\n  const response = await qConnectClient.call(listIntegrationAssociations);\n    // process response.\n} catch (error) {\n  // error handling.\n}\n```\n\n# StreamsJS Integration\n\nIn order to use QConnectJS, the library must be used in conjunction with [amazon-connect-streams](https://github.com/amazon-connect/amazon-connect-streams). Integrating with Amazon\nConnect Streams provides enables you to handle agent and contact state events directly through an object oriented event driven interface.\n\n## Agent\n\nThe StreamsJS Agent API provides event subscription methods and action methods which can be called on behalf of the agent. For more information check out the StreamsJS [Agent API](https://github.com/amazon-connect/amazon-connect-streams/blob/master/Documentation.md#agent-api) reference.\n### InstanceId\n\nThe StreamsJS Agent API can be used to retrieve the Amazon Connect `instanceId` using the Agent `routingProfileId`. The routing profile contains the following fields:\n\n* `channelConcurrencyMap`: See `agent.getChannelConcurrency()` for more info.\n* `defaultOutboundQueue`: The default queue which should be associated with outbound contacts. See `queues` for details on properties.\n* `name`: The name of th routing profile.\n* `queues`: The queues contained in the routing profile. Each queue object has the following properties:\n  * `name`: The name of the queue.\n  * `queueARN`: The ARN of the queue.\n  * `queueId`: Alias for the `queueARN`.\n* `routingProfileARN`: The routing profile ARN.\n* `routingProfileId`: Alias for the `routingProfileARN`.\n\n```js\nconst routingProfile = agent.getRoutingProfile();\n\nconst instanceId = routingProfile.routingProfileId.match(\n  /instance\\/([0-9a-fA-F|-]+)\\//\n)[1];\n```\n\n## Contact\n\nThe StreamsJS Contact API provides event subscription methods and action methods which can be called on behalf of a contact. For more information check out the StreamsJS [Contact API](https://github.com/amazon-connect/amazon-connect-streams/blob/master/Documentation.md#contact-api) reference.\n### ContactId\n\nThe StreamsJS Contact API can be used to retrieve the Amazon Connect `contactId` using the Contact `getContactId` method.\n\n```js\nconst contactId = contact.getContactId();\n```\n\n# Troubleshooting\n\nWhen the service returns an exception, the error will include the exception information.\n\n```js\ntry {\n  const data = await client.call(command);\n  // process data.\n} catch (error) {\n  console.log(error);\n  // error handling.\n}\n```\n\n# Contributing\n\nSee [CONTRIBUTING](CONTRIBUTING.md) for more information.\n\n# License\n\nQConnectJS is distributed under the\n[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0),\nsee LICENSE for more information.\n", "release_dates": ["2023-11-28T12:48:49Z", "2023-05-01T16:35:26Z", "2022-09-01T13:47:52Z", "2022-07-21T16:42:08Z", "2022-06-22T20:35:25Z"]}, {"name": "amazon-redshift-jdbc-driver", "description": "Redshift JDBC Driver. It supports JDBC 4.2 specification.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Redshift JDBC Driver\n\nThe Amazon JDBC Driver for Redshift is a Type 4 JDBC driver that provides database connectivity through the standard JDBC application program interfaces (APIs) available in the Java Platform, Enterprise Editions. The Driver provides access to Redshift from any Java application, application server, or Java-enabled applet.\n\nThe driver has many Redshift specific features such as,\n\n* IAM authentication\n* IDP authentication\n* Redshift specific datatypes support\n* External schema support as part of getTables() and getColumns() JDBC API\n\nThe driver supports JDBC 4.2 specification.\n\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.amazon.redshift/redshift-jdbc42/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.amazon.redshift/redshift-jdbc42)\n[![javadoc](https://javadoc.io/badge2/com.amazon.redshift/redshift-jdbc42/javadoc.svg)](https://javadoc.io/doc/com.amazon.redshift/redshift-jdbc42)\n\n## Build Driver\n### Prerequisites\n* JDK8\n* Maven 3.x\n* Redshift instance connect to.\n\n### Build Artifacts\nOn Unix system run:\n```\nbuild.sh\n```\nIt builds **redshift-jdbc42-{version}.jar** and **redshift-jdbc42-{version}.zip** files under **target** directory. \nThe jar file is the Redshift JDBC driver.The zip file contains the driver jar file and all required dependencies files to use AWS SDK for the IDP/IAM features.\n\n### Installation and Configuration of Driver\n\nSee [Amazon Redshift JDBC Driver Installation and Configuration Guide](https://docs.aws.amazon.com/redshift/latest/mgmt/jdbc20-install.html) for more information.\n\nHere are download links for the latest release:\n\n* https://redshift-downloads.s3.amazonaws.com/drivers/jdbc/2.1.0.25/redshift-jdbc42-2.1.0.26.jar\n* https://redshift-downloads.s3.amazonaws.com/drivers/jdbc/2.1.0.25/redshift-jdbc42-2.1.0.26.zip\n\nIt also available on Maven Central, groupId: com.amazon.redshift and artifactId: redshift-jdbc42.\n\n## Report Bugs\n\nSee [CONTRIBUTING](CONTRIBUTING.md#Reporting-Bugs/Feature-Requests) for more information.\n\n## Contributing Code Development\n\nSee [CONTRIBUTING](CONTRIBUTING.md#Contributing-via-Pull-Requests) for more information.\n\n ## Changelog Generation\n An entry in the changelog is generated upon release using `gitchangelog <https://github.com/vaab/gitchangelog>`.\n Please use the configuration file, ``.gitchangelog.rc`` when generating the changelog.\n\t \n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2024-02-12T23:03:03Z", "2024-01-17T16:58:03Z", "2023-12-14T14:14:28Z", "2023-11-20T18:58:21Z", "2023-11-09T18:41:31Z", "2023-10-23T18:00:03Z", "2023-10-16T16:17:07Z", "2023-09-11T14:22:25Z", "2023-08-01T17:36:25Z", "2023-07-05T14:45:04Z", "2023-06-09T14:18:50Z", "2023-04-13T16:38:07Z", "2023-03-31T12:49:38Z", "2023-02-24T15:10:30Z", "2023-02-02T21:29:16Z", "2022-11-30T16:47:06Z", "2022-07-01T20:49:28Z", "2022-06-09T02:21:53Z", "2022-05-05T01:33:37Z", "2022-04-14T23:01:11Z", "2022-03-12T00:36:19Z", "2022-01-30T17:58:12Z", "2021-12-03T23:48:25Z", "2021-11-10T23:30:49Z", "2021-09-04T23:54:34Z", "2021-07-23T23:38:54Z", "2021-06-29T18:31:06Z", "2021-06-08T23:08:20Z", "2021-03-28T19:33:06Z", "2021-02-25T23:42:03Z"]}, {"name": "amazon-redshift-odbc-driver", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Redshift ODBC Driver\n\nThe Amazon ODBC Driver for Redshift database connectivity through the standard ODBC application program interfaces (APIs). The Driver provides access to Redshift from any C/C++ application.\n\nThe driver has many Redshift specific features such as,\n\n* IAM authentication\n* IDP authentication\n* Redshift specific datatypes support\n* External schema support as part of SQLTables() and SQLColumns() ODBC API\n\nAmazon Redshift provides 64-bit ODBC drivers for Linux, and Windows operating systems. \n\n## Build Driver\n### Prerequisites\n* https://git-lfs.com/ (for correctly cloning this repository)\n* Visual Stuido 2022 Community Edition (For Windows)\n* gcc (For Linux)\n* cmake >= 3.12 (For Linux)\n* Dependencies: To see a list of Linux dependencies, please look into src/odbc/rsodbc/CMakeLists.txt (windows: Dependencies are already included).\n\n### Build Artifacts\nOn Windows system run:\n```\nbuild64.bat n.n.n n \ne.g. build64.bat 2.0.1 0\n\n```\nIt builds **rsodbc.dll** file under **src\\odbc\\rsodbc\\x64\\Release** directory. \n\n\nexport DEPENDENCY_DIR=\nEnsure proper dependencies are provided on Unix systems by configuring the dependency variable: Set the `DEPENDENCY_DIR` variable in the `exports_basic.sh`` file. For further details, consult the `BUILD.CMAKE.md` file.\nThen run:\n```\nbuild64.sh n.n.n n\ne.g. build64.sh 2.0.1 0\n```\n\nIt builds **librsodbc64.so** file under **src/odbc/rsodbc/Release** directory. \n\n### Installation and Configuration of Driver\n\nDriver Name: Amazon Redshift ODBC Driver (x64)\n\nDefault Installation Directory:\n* C:\\Program Files\\Amazon Redshift ODBC Driver x64\\ (For Windows)\n* /opt/amazon/redshiftodbcx64/ (For Linux)\n\nSee [Amazon Redshift ODBC Driver Installation and Configuration Guide](https://docs.aws.amazon.com/redshift/latest/mgmt/odbc20-install.html) for more information.\n\nHere are download links for the latest release:\n* https://s3.amazonaws.com/redshift-downloads/drivers/odbc/2.1.0.0/AmazonRedshiftODBC64-2.1.0.0.msi (For Windows)\n* https://s3.amazonaws.com/redshift-downloads/drivers/odbc/2.1.0.0/AmazonRedshiftODBC-64-bit-2.1.0.0.x86_64.rpm (For Linux)\n\n## Report Bugs\n\nSee [CONTRIBUTING](CONTRIBUTING.md#Reporting-Bugs/Feature-Requests) for more information.\n\n## Contributing Code Development\n\nSee [CONTRIBUTING](CONTRIBUTING.md#Contributing-via-Pull-Requests) for more information.\n\n## Changelog Generation\nAn entry in the changelog is generated upon release using `gitchangelog <https://github.com/vaab/gitchangelog>`.\nPlease use the configuration file, ``.gitchangelog.rc`` when generating the changelog.\n\t \n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2024-01-25T21:55:21Z", "2023-11-21T21:11:49Z", "2023-09-13T14:12:30Z", "2023-08-16T08:51:53Z", "2023-07-07T08:07:46Z", "2023-05-24T22:28:40Z", "2023-03-28T17:21:59Z"]}, {"name": "amazon-redshift-python-driver", "description": "Redshift Python Connector. It supports  Python Database API Specification v2.0.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2024-02-20T19:16:09Z", "2023-12-14T22:08:42Z", "2023-11-20T18:44:22Z", "2023-11-13T21:05:23Z", "2023-10-16T17:27:11Z", "2023-09-12T00:00:53Z", "2023-07-12T22:32:46Z", "2023-07-05T17:06:16Z", "2023-06-06T00:00:37Z", "2023-01-25T20:43:02Z", "2022-09-22T21:48:25Z", "2022-07-01T16:33:56Z", "2022-05-06T19:16:28Z", "2022-04-15T18:04:07Z", "2022-03-07T20:53:38Z", "2022-02-07T18:39:44Z", "2022-01-10T18:14:39Z", "2021-12-14T19:01:03Z", "2021-12-06T17:47:46Z", "2021-11-19T00:50:22Z", "2021-10-25T19:46:36Z", "2021-09-27T18:15:18Z", "2021-09-14T18:10:06Z", "2021-08-30T20:10:52Z", "2021-08-16T17:39:40Z", "2021-08-02T17:38:54Z", "2021-07-19T17:13:15Z", "2021-06-23T17:47:55Z", "2021-05-19T17:38:45Z", "2021-05-10T22:10:45Z"]}, {"name": "amazon-s3-encryption-client-dotnet", "description": "An encryption client that allows you to secure your sensitive data before you send it to Amazon S3.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![.NET on AWS Banner](./logo.png \".NET on AWS\")\n\n## Amazon S3 Encryption Client for .NET\n\n[![nuget](https://img.shields.io/nuget/v/Amazon.Extensions.S3.Encryption.svg)](https://www.nuget.org/packages/Amazon.Extensions.S3.Encryption/)\n\n[The Amazon S3 Encryption Client for .NET](https://www.nuget.org/packages/Amazon.Extensions.S3.Encryption/) provides an easy-to-use Amazon S3 encryption client that allows you to secure your sensitive data before you send it to Amazon S3. The AmazonS3EncryptionClientV2 client automatically encrypts data on the client when uploading to Amazon S3, and automatically decrypts it when data is retrieved. You can use the client just like the regular S3 client, working with things like multipart uploads and the Transfer Utility with no additional code changes required besides swapping out the client used.\n\nThe AmazonS3EncryptionClientV2 supports the following encryption methods for encrypting DEKs (Data encryption keys):\n\n* AWS supplied KEK (key encryption key):\n  * AWS KMS + Context\n* User supplied KEK:\n  * RSA-OAEP-SHA1\n  * AES-GCM\n  \nObject content is encrypted using AES-GCM with generated DEKs which are stored in the S3 object metadata or in a separate instruction file (as configured).\n\n# Code examples and API Documentation\n \nFor more information, including code samples and API documentation, please visit: https://aws.github.io/amazon-s3-encryption-client-dotnet/index.html\n \n# Getting Help\n\nWe use the [GitHub issues](https://github.com/aws/amazon-s3-encryption-client-dotnet/issues) for tracking bugs and feature requests and have limited bandwidth to address them.\n\nIf you think you may have found a bug, please open an [issue](https://github.com/aws/amazon-s3-encryption-client-dotnet/issues/new)\n\n# Contributing\n\nWe welcome community contributions and pull requests. See\n[CONTRIBUTING](./CONTRIBUTING.md) for information on how to set up a development\nenvironment and submit code.\n\n# Additional Resources\n\n[AWS .NET GitHub Home Page](https://github.com/aws/dotnet)  \nGitHub home for .NET development on AWS. You'll find libraries, tools, and resources to help you build .NET applications and services on AWS.\n\n[AWS Developer Center - Explore .NET on AWS](https://aws.amazon.com/developer/language/net/)  \nFind .NET code samples, step-by-step guides, videos, blog content, tools, and information about live events all in one place. \n\n[AWS Developer Blog - .NET](https://aws.amazon.com/blogs/developer/category/programing-language/dot-net/)  \nCome and see what .NET developers at AWS are up to! Learn about new .NET software announcements, guides, and how-to's.\n\n[@dotnetonaws](https://twitter.com/dotnetonaws) \nFollow us on twitter!\n\n# License\n\nLibraries in this repository are licensed under the Apache 2.0 License. \n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.\n", "release_dates": []}, {"name": "amazon-s3-encryption-client-go", "description": "The Amazon S3 Encryption Client is a client-side encryption library that enables you to encrypt an object locally to ensure its security before passing it to Amazon Simple Storage Service (Amazon S3).", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon S3 Encryption Client for Go V3\n\n[![Go Build status](https://github.com/aws/amazon-s3-encryption-client-go/actions/workflows/go-test.yml/badge.svg?branch=main)](https://github.com/aws/amazon-s3-encryption-client-go/actions/workflows/go-test.yml)  [![Apache V2 License](https://img.shields.io/badge/license-Apache%20V2-blue.svg)](https://github.com/aws/amazon-s3-encryption-client-go/blob/main/LICENSE)\n\nThis library provides an S3 client that supports client-side encryption.\n`amazon-s3-encryption-client-go` is the v3 of the Amazon S3 Encryption Client for the Go programming language.\n\nThe v3 encryption client requires a minimum version of `Go 1.20`.\n\nCheck out the [release notes](https://github.com/aws/amazon-s3-encryption-client-go/blob/main/CHANGELOG.md) for information about the latest bug\nfixes, updates, and features added to the encryption client.\n\nJump To:\n* [Getting Started](#getting-started)\n* [Migration](#migration)\n\n## Maintenance and support for SDK major versions\n\nFor information about maintenance and support for SDK major versions and their underlying dependencies, see the\nfollowing in the AWS SDKs and Tools Shared Configuration and Credentials Reference Guide:\n\n* [AWS SDKs and Tools Maintenance Policy](https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html)\n* [AWS SDKs and Tools Version Support Matrix](https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html)\n\n### Go version support policy\n\nThe v3 Encryption Client follows the upstream [release policy](https://go.dev/doc/devel/release#policy)\nwith an additional six months of support for the most recently deprecated\nlanguage version.\n\n**AWS reserves the right to drop support for unsupported Go versions earlier to\naddress critical security issues.**\n\n## Getting started\nTo get started working with the S3 Encryption Client set up your project for Go modules, and retrieve the client's dependencies with `go get`.\nThis example shows how you can use the v3 encryption client to make a `PutItem` request using a KmsKeyring.\n\n###### Initialize Project\n```sh\n$ mkdir ~/encryptionclient\n$ cd ~/encryptionclient\n$ go mod init encryptionclient\n```\n###### Add SDK Dependencies\n```sh\n$ go get github.com/aws/amazon-s3-encryption-client-go/v3\n```\n\n###### Write Code\nIn your preferred editor add the following content to `main.go`\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"log\"\n\t\"github.com/aws/aws-sdk-go-v2/aws\"\n\t\"github.com/aws/aws-sdk-go-v2/config\"\n\t\"github.com/aws/aws-sdk-go-v2/service/kms\"\n\t\"github.com/aws/aws-sdk-go-v2/service/s3\"\n\t\n\t// Import the materials and client package \n\t\"github.com/aws/amazon-s3-encryption-client-go/v3/client\"\n\t\"github.com/aws/amazon-s3-encryption-client-go/v3/materials\"\n)\n\nfunc main() {\n\tctx := context.Background()\n    // Using the SDK's default configuration, loading additional config\n    // and credentials values from the environment variables, shared\n    // credentials, and shared configuration files\n    cfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(\"us-west-2\"))\n    if err != nil {\n        log.Fatalf(\"unable to load SDK config, %v\", err)\n    }\n\tkey := \"testObjectWithNewEncryptionClient\"\n\tplaintext := \"This is a test.\\n\"\n\t\n\ts3Client := s3.NewFromConfig(cfg)\n\tkmsClient := kms.NewFromConfig(cfg)\n\n\t// Create the keyring and &CMM-long; (&CMM-short;)\n\tcmm, err := materials.NewCryptographicMaterialsManager(materials.NewKmsKeyring(kmsClient, kmsKeyArn, func(options *materials.KeyringOptions) {\n\t\toptions.EnableLegacyWrappingAlgorithms = false\n\t}))\n\tif err != nil {\n\t\tlog.Fatalf(\"error while creating new CMM\")\n\t}\n\n\ts3EncryptionClient, err := client.New(s3Client, cmm)\n\t\n\t_, err = s3EncryptionClient.PutObject(ctx, &s3Client.PutObjectInput{\n\t\tBucket: aws.String(bucket),\n\t\tKey:    aws.String(key),\n\t\tBody:   bytes.NewReader([]byte(plaintext)),\n\t})\n\tif err != nil {\n\t\tlog.Fatalf(\"error calling putObject: %v\", err)\n\t}\n}\n```\n\n## Migration\n\nThis version of the library supports reading encrypted objects from previous versions.\nIt also supports writing objects with non-legacy algorithms.\nThe list of legacy modes and operations will be provided below.\n\n### Examples\n#### V2 KMS to V3\n\nThe following example demonstrates how to migrate a version v2 application that uses\nthe `NewKMSContextKeyGenerator` kms-key provider with a material\ndescription and `AESGCMContentCipherBuilderV2` content cipher to\nversion v3 of the S3 Encryption Client for Go.\n\n```go\nfunc KmsContextV2toV3GCMExample() error {\n \tbucket := LoadBucket()\n \tkmsKeyAlias := LoadAwsKmsAlias()\n \n \tobjectKey := \"my-object-key\"\n \tregion := \"us-west-2\"\n \tplaintext := \"This is an example.\\n\"\n \n \t// Create an S3EC Go v2 encryption client\n \t// using the KMS client from AWS SDK for Go v1\n\tsessKms, err := sessionV1.NewSession(&awsV1.Config{\n        Region: aws.String(region),\n    })\n \n \tkmsSvc := kmsV1.New(sessKms)\n \thandler := s3cryptoV2.NewKMSContextKeyGenerator(kmsSvc, kmsKeyAlias, s3cryptoV2.MaterialDescription{})\n \tbuilder := s3cryptoV2.AESGCMContentCipherBuilderV2(handler)\n \tencClient, err := s3cryptoV2.NewEncryptionClientV2(sessKms, builder)\n \tif err != nil {\n \t\tlog.Fatalf(\"error creating new v2 client: %v\", err)\n \t}\n \n \t// Encrypt using KMS+Context and AES-GCM content cipher\n \t_, err = encClient.PutObject(s3V1.PutObjectInput{\n \t\tBucket: aws.String(bucket),\n \t\tKey:    aws.String(objectKey),\n \t\tBody:   bytes.NewReader([]byte(plaintext)),\n \t})\n \tif err != nil {\n \t\tlog.Fatalf(\"error calling putObject: %v\", err)\n \t}\n \tfmt.Printf(\"successfully uploaded file to %s/%s\\n\", bucket, key)\n \n \t// Create an S3EC Go v3 client\n \t// using the KMS client from AWS SDK for Go v2\n \tctx := context.Background()\n \tcfg, err := config.LoadDefaultConfig(ctx,\n \t\tconfig.WithRegion(region),\n \t)\n \n \tkmsV2 := kms.NewFromConfig(cfg)\n \tcmm, err := materials.NewCryptographicMaterialsManager(materials.NewKmsKeyring(kmsV2, kmsKeyAlias))\n \tif err != nil {\n \t\tt.Fatalf(\"error while creating new CMM\")\n \t}\n \n \ts3V2 := s3.NewFromConfig(cfg)\n \ts3ecV3, err := client.New(s3V2, cmm)\n \n \tresult, err := s3ecV3.GetObject(ctx, s3.GetObjectInput{\n \t\tBucket: aws.String(bucket),\n \t\tKey:    aws.String(objectKey),\n \t})\n \tif err != nil {\n \t\tt.Fatalf(\"error while decrypting: %v\", err)\n \t}\n```\n\n#### Enable legacy decryption modes\nThe `enableLegacyUnauthenticatedModes` flag enables the S3 Encryption Client to decrypt\nencrypted objects with a fully supported or legacy encryption algorithm.\nVersion V3 of the S3 Encryption Client uses one of the fully supported wrapping algorithms and the\nwrapping key you specify to encrypt and decrypt the data keys. The\n`enableLegacyWrappingAlgorithms` flag enables the S3 Encryption Client to decrypt\nencrypted data keys with a fully supported or legacy wrapping algorithm.\n\n```go\ncmm, err := materials.NewCryptographicMaterialsManager(materials.NewKmsKeyring(kmsClient, kmsKeyArn, func(options *materials.KeyringOptions) {\n     options.EnableLegacyWrappingAlgorithms = true\n })\n \n if err != nil {\n \tt.Fatalf(\"error while creating new CMM\")\n }\n \n client, err := client.New(s3Client, cmm, func(clientOptions *client.EncryptionClientOptions) {\n \t\tclientOptions.EnableLegacyUnauthenticatedModes = true\n })\n \n if err != nil {\n \t// handle error\n }\n```\n\n### Legacy Algorithms and Modes\n#### Content Encryption\n* AES/CBC\n#### Key Wrap Encryption\n* KMS (without context)\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2023-11-16T23:00:08Z"]}, {"name": "amazon-s3-encryption-client-java", "description": "The Amazon S3 Encryption Client is a client-side encryption library that enables you to encrypt an object locally to ensure its security before passing it to Amazon Simple Storage Service (Amazon S3).", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon S3 Encryption Client\n\nThis library provides an S3 client that supports client-side encryption. For more information and detailed instructions\nfor how to use this library, refer to the \n[Amazon S3 Encryption Client Developer Guide](https://docs.aws.amazon.com/amazon-s3-encryption-client/latest/developerguide/what-is-s3-encryption-client.html).\n\n## Testing\nIntegration tests are included. To test them, certain environment variables need to be set:\n\n* `AWS_S3EC_TEST_BUCKET` - The bucket to write test values to\n* `AWS_S3EC_TEST_KMS_KEY_ID` - The key ID for the KMS key used for KMS tests\n* `AWS_S3EC_TEST_KMS_KEY_ALIAS` - An alias for the KMS key used for KMS tests. The alias must reference the key ID above.\n* `AWS_REGION` - The region the AWS resources (KMS key, S3 bucket) resides e.g. \"us-east-1\"\n\nTo create these resources, refer to the included CloudFormation template (cfn/S3EC-GitHub-CF-Template).\nThe IAM Role `S3ECGithubTestRole` SHOULD BE manually customized by you.\nMake sure that the repo in the trust policy of the IAM role refers to your fork instead of the `aws` organization.\nAlso, remove the `ToolsDevelopment` clause of the `S3ECGithubTestRole`'s `AssumeRolePolicyDocument`.\n**NOTE**: Your account may incur charges based on the usage of any resources beyond the AWS Free Tier.\n\nIf you have forked this repo, there are additional steps required.\nYou will need to configure your fork's Github Actions settings to be able to run CI:\n\nUnder Settings -> Actions -> General -> Workflow permissions, ensure \"Read and write permissions\" is selected.\nUnder Settings -> Security -> Secrets and variables -> Actions -> Repository secrets, add new secret:\n\n* `CI_AWS_ACCOUNT_ID` - the AWS account ID which contains the required resources, e.g. 111122223333.\n\nThe other values are added as variables (by clicking the \"New repository variable\" button):\n\n* `CI_AWS_ROLE` - the IAM role to assume during CI, e.g. S3EC-GitHub-test-role. It must exist in the above account and have permission to call S3 and KMS.\n* `CI_AWS_REGION` - the AWS region which contains the required resources, e.g. us-west-2.\n* `CI_S3_BUCKET` - the S3 bucket to use, e.g. s3ec-github-test-bucket.\n* `CI_KMS_KEY_ID` - the short KMS key ID to use, e.g. c3eafb5f-e87d-4584-9400-cf419ce5d782.\n* `CI_KMS_KEY_ALIAS` - the KMS key alias to use, e.g. S3EC-Github-KMS-Key. Note that the alias must reference the key ID above.\n\n## Migration\n\nThis version of the library supports reading encrypted objects from previous versions.\nIt also supports writing objects with non-legacy algorithms.\nThe list of legacy modes and operations is provided below.\n\nHowever, this version does not support V2's Unencrypted Object Passthrough.\nThis library can only read encrypted objects from S3,\nunencrypted objects MUST be read with the base S3 Client.\n\n### Examples\n#### V2 KMS Materials Provider to V3\n```java\nclass Example {\n    public static void main(String[] args) {\n        // V2\n        EncryptionMaterialsProvider materialsProvider = new KMSEncryptionMaterialsProvider(KMS_WRAPPING_KEY_ID);\n        AmazonS3EncryptionV2 v2Client = AmazonS3EncryptionClientV2.encryptionBuilder()\n                .withEncryptionMaterialsProvider(materialsProvider)\n                .build();\n        \n        // V3\n        S3Client v3Client = S3EncryptionClient.builder()\n                .kmsKeyId(KMS_WRAPPING_KEY_ID)\n                .build();\n    }\n}\n```\n\n#### V2 AES Key Materials Provider to V3\n```java\nclass Example {\n    public static void main(String[] args) {\n        KeyGenerator keyGen = KeyGenerator.getInstance(\"AES\");\n        keyGen.init(256);\n        SecretKey aesKey = keyGen.generateKey();\n        \n        // V2\n        EncryptionMaterialsProvider materialsProvider = new StaticEncryptionMaterialsProvider(new EncryptionMaterials(aesKey));\n        AmazonS3EncryptionV2 v2Client = AmazonS3EncryptionClientV2.encryptionBuilder()\n                .withEncryptionMaterialsProvider(materialsProvider)\n                .build();\n\n        // V3\n        S3Client v3Client = S3EncryptionClient.builder()\n                .aesKey(aesKey)\n                .build();\n    }\n}\n```\n\n#### V2 RSA Key Materials Provider to V3\n```java\nclass Example {\n    public static void main(String[] args) {\n        KeyPairGenerator keyPairGen = KeyPairGenerator.getInstance(\"RSA\");\n        keyPairGen.initialize(2048);\n        KeyPair rsaKey = keyPairGen.generateKeyPair();\n        \n        // V2\n        EncryptionMaterialsProvider materialsProvider = new StaticEncryptionMaterialsProvider(new EncryptionMaterials(rsaKey));\n        AmazonS3EncryptionV2 v2Client = AmazonS3EncryptionClientV2.encryptionBuilder()\n                .withEncryptionMaterialsProvider(materialsProvider)\n                .build();\n\n        // V3\n        S3Client v3Client = S3EncryptionClient.builder()\n                .rsaKeyPair(rsaKey)\n                .build();\n    }\n}\n```\n\n#### V1 Key Materials Provider to V3\nTo allow legacy modes (for decryption only), you must explicitly allow them\n```java\nclass Example {\n    public static void main(String[] args) {\n        KeyGenerator keyGen = KeyGenerator.getInstance(\"AES\");\n        keyGen.init(256);\n        SecretKey aesKey = keyGen.generateKey();\n        \n        // V1\n        EncryptionMaterialsProvider materialsProvider = new StaticEncryptionMaterialsProvider(new EncryptionMaterials(aesKey));\n        AmazonS3Encryption v1Client = AmazonS3EncryptionClient.encryptionBuilder()\n                .withEncryptionMaterials(materialsProvider)\n                .build();\n\n        // V3\n        S3Client v3Client = S3EncryptionClient.builder()\n                .aesKey(aesKey)\n                .enableLegacyUnauthenticatedModes(true) // for enabling legacy content decryption modes\n                .enableLegacyWrappingAlgorithms(true) // for enabling legacy key wrapping modes \n                .build();\n    }\n}\n```\n\n### Legacy Algorithms and Modes\n#### Content Encryption\n* AES/CBC\n#### Key Wrap Encryption\n* AES\n* AESWrap\n* RSA-OAEP w/MGF-1 and SHA-256\n* RSA\n* KMS (without context)\n#### Encryption Metadata Storage\n* Instruction File\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2024-01-24T22:20:38Z", "2023-09-01T18:39:03Z", "2023-06-01T22:47:45Z"]}, {"name": "amazon-s3-plugin-for-pytorch", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# S3 Plugin\n\n**Note: As of April 5th, 2022, this plugin is in maintenance mode. [The S3 IO is in the process of being upstreamed into `torchdata` package](https://github.com/pytorch/data/tree/main/torchdata/datapipes/iter/load#readme). In the future, we will support the new `torchdata` package, and be continuously improving the user experience and performance of the S3 IO datapipes. Please support and comment for the new S3 IO datapipes. Raise issues and create PRs if necessary.**\n\nS3-plugin is a high performance PyTorch dataset library to efficiently access datasets stored in S3 buckets. It provides streaming data access to datasets of any size and thus eliminates the need to provision local storage capacity. The library is designed to leverage the high throughput that S3 offers to access objects with minimal latency.\n\nThe users have the flexibility to use either map-style or iterable-style dataset interfaces based on their needs. The library itself is file-format agnostic and presents objects in S3 as a binary buffer(blob). Users are free to apply any additional transformation on the data received from S3.\n\n## Compatible Images\n\nOnly the following images are compatible with the Amazon S3 plugin for PyTorch:\n\n**Ubuntu 20.04**\n- **CPU**: 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.9.0-cpu-py38-ubuntu20.04-v1.1\n- **GPU**: 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.9.0-gpu-py38-cu111-ubuntu20.04-v1.1\n\n**Ubuntu 18.04**\n- **CPU**: 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.8.1-cpu-py36-ubuntu18.04-v1.6\n- **GPU**: 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.8.1-gpu-py36-cu111-ubuntu18.04-v1.7\n\n## Installation\n\nYou can install this package by following the below instructions.\n\n#### Prerequisite\n\n- Python 3.6 (or Python 3.7) is required for this installation.\n\n- [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html) for configuring S3 access.\n\n- Pytorch >= 1.5 (If not available, S3-plugin installs latest Torch)\n\n- *Note:* To run on Mac, [AWS_SDK_CPP](https://github.com/aws/aws-sdk-cpp) must be installed.\n\n\n#### Installing S3-Plugin via Wheel\n\n```shell script\n# List of wheels on Linux:\n# python 3.7: https://aws-s3-plugin.s3.us-west-2.amazonaws.com/binaries/0.0.1/bd37e27/awsio-0.0.1%2Bbd37e27-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# python 3.8: https://aws-s3-plugin.s3.us-west-2.amazonaws.com/binaries/0.0.1/bd37e27/awsio-0.0.1%2Bbd37e27-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# python 3.9: https://aws-s3-plugin.s3.us-west-2.amazonaws.com/binaries/0.0.1/bd37e27/awsio-0.0.1%2Bbd37e27-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\naws s3 cp <S3 URI> .\npip install <whl name awsio-0.0.1-cp...whl>\n```\n\n#### Installing S3-Plugin from source\n\n```shell\n# install [aws-sdk-cpp](https://github.com/aws/aws-sdk-cpp). example installation guide\ngit clone --recurse-submodules https://github.com/aws/aws-sdk-cpp\ncd aws-sdk-cpp/\nmkdir sdk-build\ncd sdk-build\ncmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_ONLY=\"s3;transfer\"\nmake\nmake install # may need sudo\n\n# install pybind11. example:\nconda install pybind11\nexport CMAKE_PREFIX_PATH=$CMAKE_PREFIX_PATH:/usr/local/lib/python3.7/site-packages/pybind11\n\n# install from source\npython setup.py install\n```\n\n### Configuration\n\nBefore reading data from S3 bucket, you need to provide bucket region parameter:\n\n* `AWS_REGION`: By default, regional endpoint is used for S3, with region controlled by `AWS_REGION`. If `AWS_REGION` is not specified, then `us-west-2` is used by default.\n\nTo read objects in a bucket that is not publicly accessible, AWS credentials must be provided through one of the following methods:\n\n* Install and configure [awscli](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html) by `aws configure`. \n* Set credentials in the AWS credentials profile file on the local system, located at: `~/.aws/credentials` on Linux, macOS, or Unix\n* Set the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables.\n* If you are using this library on an EC2 instance, specify an IAM role and then give the EC2 instance access to that role.\n\n#### Smoke Test\nTo test your setup, run:\n```\nbash tests/smoke_tests/import_awsio.sh\n```\n\nThe test will first make sure that the package imports correctly by printing the commit hash related to the build.\nThen, it will prompt the user for a S3 url to a file and return whether or not the file exists.\n\nFor example:\n```\n$ bash tests/smoke_tests/import_awsio.sh \nTesting: import awsio\n0.0.1+b119a6d\nimport awsio succeeded\nS3 URL : 's3://path/to/bucket/test_0.JPEG'\nTesting: checking setup by quering whether or not 's3://path/to/bucket/test_0.JPEG' is an existing file\nfile_exists: True\nSmoke test was successful.\n```\n\n### Usage\n\nOnce the above setup is complete, you can interact with S3 bucket in following ways:\n\nAccepted input S3 url formats:\n\n* Single url \n\n* `url = 's3://path/to/bucket/abc.tfrecord'`\n\n* List of urls as follows:\n\n```urls = ['s3://path/to/bucket/abc.tfrecord','s3://path/to/bucket/def.tfrecord']```\n\n* Prefix to S3 bucket to include all files under 's3_prefix' folder starting with '0'\n\n```urls = 's3://path/to/s3_prefix/0'```\n  \n* Using `list_files()` function, which can be used to manipulate input list of urls to fetch as follows:\n```shell\nfrom awsio.python.lib.io.s3.s3dataset import list_files\nurls = list_files('s3://path/to/s3_prefix/0')\n```\n\n#### Map-Style Dataset\n\nIf each object in S3 contains a single training sample, then map-style dataset i.e. S3Dataset can be used. To partition data across nodes and to shuffle data, this dataset can be used with PyTorch distributed sampler. Additionally, pre-processing can be applied to the data in S3 by extending the S3Dataset class. Following example illustrates use of map-style S3Dataset for image datasets: \n\n```python\nfrom awsio.python.lib.io.s3.s3dataset import S3Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport io\n\nclass S3ImageSet(S3Dataset):\n    def __init__(self, urls, transform=None):\n        super().__init__(urls)\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        img_name, img = super(S3ImageSet, self).__getitem__(idx)\n        # Convert bytes object to image\n        img = Image.open(io.BytesIO(img)).convert('RGB')\n        \n        # Apply preprocessing functions on data\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\nbatch_size = 32\n\npreproc = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    transforms.Resize((100, 100))\n])\n\n# urls can be S3 prefix containing images or list of all individual S3 images\nurls = 's3://path/to/s3_prefix/'\n\ndataset = S3ImageSet(urls, transform=preproc)\ndataloader = DataLoader(dataset,\n        batch_size=batch_size,\n        num_workers=64)\n\n```\n\n\n#### Iterable-style dataset\n\nIf each object in S3 contains multiple training samples e.g. archive files containing multiple small images or TF record files/shards containing multiple records, then it is advisable to use the Iterable-style dataset implementation i.e. S3IterableDataset. For the specific case of zip/tar archival files, each file contained in the archival is returned during each iteration in a streaming fashion. For all other file formats, binary blob for the whole shard is returned and users need to implement the appropriate parsing logic. Besides, S3IterableDataset takes care of partitioning the data across nodes and workers in a distributed setting.\n\n`Note:` For datasets consisting of a large number of smaller objects, accessing each object individually can be inefficient. For such datasets, it is recommended to create shards of the training data and use S3IterableDataset for better performance.\n```shell\n# tar file containing label and image files as below\n tar --list --file=file1.tar |  sed 4q\n\n1234.cls\n1234.jpg\n5678.cls\n5678.jpg\n```\n\nConsider tar file for image classification. It can be easily loaded by writing a custom python generator function using the iterator returned by S3IterableDataset. (Note: To create shards from a file dataset refer this [link](https://github.com/tmbdev/pytorch-imagenet-wds).)\n\n\n```python\nfrom torch.utils.data import IterableDataset\nfrom awsio.python.lib.io.s3.s3dataset import S3IterableDataset\nfrom PIL import Image\nimport io\nimport numpy as np\nfrom torchvision import transforms\n\nclass ImageS3(IterableDataset):\n    def __init__(self, urls, shuffle_urls=False, transform=None):\n        self.s3_iter_dataset = S3IterableDataset(urls,\n                                                 shuffle_urls)\n        self.transform = transform\n\n    def data_generator(self):\n        try:\n            while True:\n                # Based on alphabetical order of files, sequence of label and image may change.\n                label_fname, label_fobj = next(self.s3_iter_dataset_iterator)\n                image_fname, image_fobj = next(self.s3_iter_dataset_iterator)\n                \n                label = int(label_fobj)\n                image_np = Image.open(io.BytesIO(image_fobj)).convert('RGB')\n                \n                # Apply torch vision transforms if provided\n                if self.transform is not None:\n                    image_np = self.transform(image_np)\n                yield image_np, label\n\n        except StopIteration:\n            return\n            \n    def __iter__(self):\n        self.s3_iter_dataset_iterator = iter(self.s3_iter_dataset)\n        return self.data_generator()\n        \n    def set_epoch(self, epoch):\n        self.s3_iter_dataset.set_epoch(epoch)\n\n# urls can be a S3 prefix containing all the shards or a list of S3 paths for all the shards \n urls = [\"s3://path/to/file1.tar\", \"s3://path/to/file2.tar\"]\n\n# Example Torchvision transforms to apply on data    \npreproc = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    transforms.Resize((100, 100))\n])\n\ndataset = ImageS3(urls, transform=preproc)\n\n```\n \nThis dataset can be easily used with dataloader for parallel data loading and preprocessing:\n\n```python\ndataloader = torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=32)\n```\n\nWe can shuffle the sequence of fetching shards by setting shuffle_urls=True and calling set_epoch method at the beginning of every epochs as:\n```python\ndataset = ImageS3(urls, transform=preproc, shuffle_urls=True)\nfor epoch in range(epochs):\n    dataset.set_epoch(epoch)\n    # training code ...\n```\n\nNote that the above code will only shuffle sequence of shards, the individual training samples within shards will be fetched in the same order. To shuffle the order of training samples across shards, use ShuffleDataset. ShuffleDataset maintains a buffer of data samples read from multiple shards and returns a random sample from it. The count of samples to be buffered is specified by buffer_size. To use ShuffleDataset, update the above example as follows:\n\n```python\ndataset = ShuffleDataset(ImageS3(urls), buffer_size=4000)\n```\n\n#### Iterable-style dataset (NLP)\nThe data set can be similarly used for NLP tasks. Following example demonstrates use for S3IterableDataset for BERT data loading. \n\n```shell script\n# Consider S3 prefix containing hdf5 files.\n# Each hdf5 file contains numpy arrays for different variables required for BERT \n# training such as next sentence labels, masks etc.\naws s3 ls --human-readable s3://path/to/s3_prefix |  sed 3q\n\n\nfile_1.hdf5\nfile_2.hdf5\nfile_3.hdf5\n\n```\n\n```python\n\nimport torch\nfrom torch.utils.data import IterableDataset, DataLoader\nfrom itertools import islice\nimport h5py\nimport numpy as np\nimport io\nfrom awsio.python.lib.io.s3.s3dataset import S3IterableDataset\n\ndef create_data_samples_from_file(fileobj):\n    # Converts bytes data to numpy arrays\n    keys = ['input_ids', 'input_mask', 'segment_ids', \\\n        'masked_lm_positions', 'masked_lm_ids', 'next_sentence_labels']\n    dataset = io.BytesIO(fileobj)\n    with h5py.File(dataset, \"r\") as f:\n        data_file = [np.asarray(f[key][:]) for key in keys]\n    return data_file\n\nclass s3_dataset(IterableDataset):\n\n    def __init__(self, urls):\n        self.urls = urls\n        self.dataset = S3IterableDataset(self.urls, shuffle_urls=True)\n\n    def data_generator(self):\n        try:\n            while True:\n                filename, fileobj = next(self.dataset_iter)\n                # data_samples: list of six numpy arrays \n                data_samples = create_data_samples_from_file(fileobj)\n                \n                for sample in list(zip(*data_samples)):\n                    # Preprocess sample if required and then yield\n                    yield sample\n\n        except StopIteration as e:\n            return\n\n    def __iter__(self):\n        self.dataset_iter = iter(self.dataset)\n        return self.data_generator()\n\nurls = \"s3://path/to/s3_prefix\"\ntrain_dataset = s3_dataset(urls)\n\n```\n\n### Test Coverage\n\nTo check python test coverage, install [`coverage.py`](https://coverage.readthedocs.io/en/latest/index.html) as follows:\n\n```\npip install coverage\n```\n\nTo make sure that all tests are run, please also install `pytest`, `boto3`, and `pandas` as follows:\n```\npip install pytest boto3 pandas\n``` \n\nTo run tests and calculate coverage:\n\n```asm\ncoverage erase\ncoverage run -p --source=awsio -m pytest -v tests/py-tests/test_regions.py \\\ntests/py-tests/test_utils.py \\\ntests/py-tests/test_s3dataset.py \\\ntests/py-tests/test_s3iterabledataset.py \\\ntests/py-tests/test_read_datasets.py \\\ntests/py-tests/test_integration.py\ncoverage combine\ncoverage report -m\n```\n", "release_dates": []}, {"name": "amazon-sagemaker-clarify", "description": "Fairness Aware Machine Learning. Bias detection and mitigation for datasets and models.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![Python package](https://github.com/aws/amazon-sagemaker-clarify/workflows/Python%20package/badge.svg)\n![Pypi](https://img.shields.io/pypi/v/smclarify.svg?maxAge=60)\n![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg?style=flat)\n\n# smclarify\n\nAmazon Sagemaker Clarify\n\nBias detection and mitigation for datasets and models.\n\n\n# Installation\n\nTo install the package from PIP you can simply do:\n\n```\npip install smclarify\n```\n\nYou can see examples on running the Bias metrics on the notebooks in the [examples folder](https://github.com/aws/amazon-sagemaker-clarify/tree/master/examples).\n\n\n# Terminology\n\n### Facet\nA facet is column or feature that will be used to measure bias against. A facet can have value(s) that designates that sample as \"***sensitive***\".\n\n### Label\nThe label is a column or feature which is the target for training a machine learning model. The label can have value(s) that designates that sample as having a \"***positive***\" outcome.\n\n### Bias measure\nA bias measure is a function that returns a bias metric.\n\n### Bias metric\nA bias metric is a numerical value indicating the level of bias detected as determined by a particular bias measure.\n\n### Bias report\nA collection of bias metrics for a given dataset or a combination of a dataset and model.\n\n# Development\n\nIt's recommended that you setup a virtualenv.\n\n```\nvirtualenv -p(which python3) venv\nsource venv/bin/activate.fish\npip install -e .[test]\ncd src/\n../devtool all\n```\n\nFor running unit tests, do `pytest --pspec`. If you are using PyCharm, and cannot see the green run button next to the tests, open `Preferences` -> `Tools` -> `Python Integrated tools`, and set default test runner to `pytest`.\n\nFor Internal contributors, run ```../devtool integ_tests``` after creating virtualenv with the above steps to run the integration tests.\n", "release_dates": ["2023-04-04T17:16:24Z", "2023-03-17T19:45:47Z", "2022-12-06T03:42:38Z", "2021-03-02T20:24:32Z", "2020-12-08T15:43:18Z"]}, {"name": "amazon-sagemaker-examples", "description": "Example \ud83d\udcd3 Jupyter notebooks that demonstrate how to build, train, and deploy machine learning models using \ud83e\udde0 Amazon SageMaker. ", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![SageMaker](https://github.com/aws/amazon-sagemaker-examples/raw/main/_static/sagemaker-banner.png)\n\n# Amazon SageMaker Examples\n\nExample Jupyter notebooks that demonstrate how to build, train, and deploy machine learning models using Amazon SageMaker.\n\n## :books: Background\n\n[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service for data science and machine learning (ML) workflows.\nYou can use Amazon SageMaker to simplify the process of building, training, and deploying ML models.\n\nThe [SageMaker example notebooks](https://sagemaker-examples.readthedocs.io/en/latest/) are Jupyter notebooks that demonstrate the usage of Amazon SageMaker.\n\nThe [Sagemaker Example Community repository](https://github.com/aws/amazon-sagemaker-examples-community) are additional notebooks, beyond those critical for showcasing key SageMaker functionality, can be shared and explored by the commmunity.\n\n\n## :hammer_and_wrench: Setup\n\nThe quickest setup to run example notebooks includes:\n- An [AWS account](http://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html)\n- Proper [IAM User and Role](http://docs.aws.amazon.com/sagemaker/latest/dg/authentication-and-access-control.html) setup\n- An [Amazon SageMaker Notebook Instance](http://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n- An [S3 bucket](http://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html)\n\n## :computer: Usage\n\nThese example notebooks are automatically loaded into SageMaker Notebook Instances.\nThey can be accessed by clicking on the `SageMaker Examples` tab in Jupyter or the SageMaker logo in JupyterLab.\n\nAlthough most examples utilize key Amazon SageMaker functionality like distributed, managed training or real-time hosted endpoints, these notebooks can be run outside of Amazon SageMaker Notebook Instances with minimal modification (updating IAM role definition and installing the necessary libraries).\n\nAs of February 7, 2022, the default branch is named \"main\". See our [announcement](https://github.com/aws/amazon-sagemaker-examples/discussions/3131) for details and how to update your existing clone.\n\n## :notebook: Examples\n\n### Introduction to geospatial capabilities\n\nThese examples introduce SageMaker geospatial capabilities which makes it easy to build, train, and deploy ML models using geospatial data.\n\n- [How to use SageMaker Processing with geospatial image](sagemaker-geospatial/processing-geospatial-ndvi/geospatial-processing-ndvi-intro.ipynb) shows how to compute the normalized difference vegetation index (NDVI) which indicates health and density of vegetation using SageMaker Processing and satellite imagery\n- [Monitoring Lake Drought with SageMaker Geospatial Capabilities](sagemaker-geospatial/lake-mead-drought-monitoring) shows how to monitor Lake Mead drought using SageMaker geospatial capabilities.\n- [Digital Farming with Amazon SageMaker Geospatial Capabilities](sagemaker-geospatial/digital-farming-pipelines) shows how geospatial capabilities can help accelerating, optimizing, and easing the processing of the geospatial data for the Digital Farming use cases.\n- [Assess wildfire damage with Amazon SageMaker Geospatial Capabilities](sagemaker-geospatial/dixie-wildfire-damage-assessment/dixie-wildfire-damage-assessment.ipynb) demonstrates how Amazon SageMaker geospatial capabilities can be used to identify and assess vegetation loss caused by the Dixie wildfire in Northern California.\n- [Monitoring Glacier Melting with SageMaker Geospatial Capabilities](sagemaker-geospatial/mount-shasta-glacier-melting-monitoring) shows how to monitor glacier melting at Mount Shasta using SageMaker geospatial capabilities.\n- [Monitoring of methane (CH4) emission point sources using Amazon SageMaker Geospatial Capabilities](sagemaker-geospatial/methane-emission-monitoring/monitor_methane_ch4_emission_point_sources.ipynb) demonstrates how methane emissions can be detected by using open data Satellite imagery (Sentinel-2).\n- [Segmenting aerial imagery using geospatial GPU notebook](sagemaker-geospatial/segment-aerial-naip/segment_naip_geospatial_notebook.ipynb) shows how to use the geospatial GPU notebook with open-source libraries to perform segmentation on aerial imagery.\n- [Perform Sentinel-1 InSAR using ESA SNAP Toolkit](sagemaker-geospatial/sentinel1-insar-snap/sentinel1_insar_kumamoto.ipynb) shows how the SNAP toolkit can be used within Amazon SageMaker geospatial capabilities to create interferograms on Sentinel-1 SAR data.\n- [How to use Vector Enrichment Jobs for Map Matching](sagemaker-geospatial/vector-enrichment-map-matching/vector-enrichment-map-matching.ipynb) shows how to use vector enrichtment operations with Amazon SageMaker Geospatial capabilities to snap GPS coordinates to road segments.\n- [How to use Vector Enrichment Jobs for Reverse Geocoding](sagemaker-geospatial/vector-enrichment-reverse-geocoding/vector-enrichment-reverse-geocoding.ipynb) shows how to use Amazon SageMaker Geospatial capabilities for reverse geocoding to obtain human readable addresses from data with latitude/longitude information.\n- [Building geospatial pipelines with SageMaker Pipelines](sagemaker-geospatial/geospatial-processing-pipeline/geospatial_pipeline_processing.ipynb) shows how a geospatial data processing workflow can be automated by using Amazon SageMaker Pipelines.\n\n### Introduction to Ground Truth Labeling Jobs\n\nThese examples provide quick walkthroughs to get you up and running with the labeling job workflow for Amazon SageMaker Ground Truth.\n\n- [Bring your own model for SageMaker labeling workflows with active learning](ground_truth_labeling_jobs/bring_your_own_model_for_sagemaker_labeling_workflows_with_active_learning) is an end-to-end example that shows how to bring your custom training, inference logic and active learning to the Amazon SageMaker ecosystem.\n- [From Unlabeled Data to a Deployed Machine Learning Model: A SageMaker Ground Truth Demonstration for Image Classification](ground_truth_labeling_jobs/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification) is an end-to-end example that starts with an unlabeled dataset, labels it using the Ground Truth API, analyzes the results, trains an image classification neural net using the annotated dataset, and finally uses the trained model to perform batch and online inference.\n- [Ground Truth Object Detection Tutorial](ground_truth_labeling_jobs/ground_truth_object_detection_tutorial) is a similar end-to-end example but for an object detection task.\n- [Basic Data Analysis of an Image Classification Output Manifest](ground_truth_labeling_jobs/data_analysis_of_ground_truth_image_classification_output) presents charts to visualize the number of annotations for each class, differentiating between human annotations and automatic labels (if your job used auto-labeling). It also displays sample images in each class, and creates a pdf which concisely displays the full results.\n- [Training a Machine Learning Model Using an Output Manifest](ground_truth_labeling_jobs/object_detection_augmented_manifest_training) introduces the concept of an \"augmented manifest\" and demonstrates that the output file of a labeling job can be immediately used as the input file to train a SageMaker machine learning model.\n- [Annotation Consolidation](ground_truth_labeling_jobs/annotation_consolidation) demonstrates Amazon SageMaker Ground Truth annotation consolidation techniques for image classification for a completed labeling job.\n\n### Introduction to Applying Machine Learning\n\nThese examples provide a gentle introduction to machine learning concepts as they are applied in practical use cases across a variety of sectors.\n\n- [Predicting Customer Churn](introduction_to_applying_machine_learning/xgboost_customer_churn) uses customer interaction and service usage data to find those most likely to churn, and then walks through the cost/benefit trade-offs of providing retention incentives. This uses Amazon SageMaker's implementation of [XGBoost](https://github.com/dmlc/xgboost) to create a highly predictive model.\n- [Cancer Prediction](introduction_to_applying_machine_learning/breast_cancer_prediction) predicts Breast Cancer based on features derived from images, using SageMaker's Linear Learner.\n- [Ensembling](introduction_to_applying_machine_learning/ensemble_modeling) predicts income using two Amazon SageMaker models to show the advantages in ensembling.\n- [Video Game Sales](introduction_to_applying_machine_learning/video_game_sales) develops a binary prediction model for the success of video games based on review scores.\n- [MXNet Gluon Recommender System](introduction_to_applying_machine_learning/gluon_recommender_system) uses neural network embeddings for non-linear matrix factorization to predict user movie ratings on Amazon digital reviews.\n- [Fair Linear Learner](introduction_to_applying_machine_learning/fair_linear_learner) is an example of an effective way to create fair linear models with respect to sensitive features.\n- [Population Segmentation of US Census Data using PCA and Kmeans](introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans) analyzes US census data and reduces dimensionality using PCA then clusters US counties using KMeans to identify segments of similar counties.\n- [Document Embedding using Object2Vec](introduction_to_applying_machine_learning/object2vec_document_embedding) is an example to embed a large collection of documents in a common low-dimensional space, so that the semantic distances between these documents are preserved.\n- [Traffic violations forecasting using DeepAR](introduction_to_applying_machine_learning/deepar_chicago_traffic_violations) is an example to use daily traffic violation data to predict pattern and seasonality to use Amazon DeepAR alogorithm.\n- [Visual Inspection Automation with Pre-trained Amazon SageMaker Models](introduction_to_applying_machine_learning/visual_object_detection) is an example for fine-tuning pre-trained Amazon Sagemaker models on a target dataset.\n- [Create SageMaker Models Using the PyTorch Model Zoo](introduction_to_applying_machine_learning/sagemaker_pytorch_model_zoo) contains an example notebook to create a SageMaker model leveraging the PyTorch Model Zoo and visualize the results.\n- [Deep Demand Forecasting](introduction_to_applying_machine_learning/deep_demand_forecasting) provides an end-to-end solution for Demand Forecasting task using three state-of-the-art time series algorithms LSTNet, Prophet, and SageMaker DeepAR, which are available in GluonTS and Amazon SageMaker.\n- [Fraud Detection Using Graph Neural Networks](introduction_to_applying_machine_learning/fraud_detection_using_graph_neural_networks) is an example to identify fraudulent transactions from transaction and user identity datasets.\n- [Identify key insights from textual document](introduction_to_applying_machine_learning/identify_key_insights_from_textual_document) contains comphrensive notebooks for five natural language processing tasks Document Summarization, Text Classification, Question and Answering, Name Entity Recognition, and Semantic Relation Extracion.\n- [Synthetic Churn Prediction with Text](introduction_to_applying_machine_learning/synthetic_churn_prediction_with_text) contains an example notebook to train, deploy and use a churn prediction model that processed numerical, categorical and textual features to make its prediction.\n- [Credit Card Fraud Detector](introduction_to_applying_machine_learning/credit_card_fraud_detector) is an example of the core of a credit card fraud detection system using SageMaker with Random Cut Forest and XGBoost.\n- [Churn Prediction Multimodality of Text and Tabular](introduction_to_applying_machine_learning/churn_prediction_multimodality_of_text_and_tabular) is an example notebook to train and deploy a churn prediction model that uses state-of-the-art natural language processing model to find useful signals in text. In addition to textual inputs, this model uses traditional structured data inputs such as numerical and categorical fields.\n\n### SageMaker Automatic Model Tuning\n\nThese examples introduce SageMaker's hyperparameter tuning functionality which helps deliver the best possible predictions by running a large number of training jobs to determine which hyperparameter values are the most impactful.\n\n- [XGBoost Tuning](hyperparameter_tuning/xgboost_direct_marketing) shows how to use SageMaker hyperparameter tuning to improve your model fit.\n- [BlazingText Tuning](hyperparameter_tuning/blazingtext_text_classification_20_newsgroups) shows how to use SageMaker hyperparameter tuning with the BlazingText built-in algorithm and 20_newsgroups dataset..\n- [TensorFlow Tuning](hyperparameter_tuning/tensorflow_mnist) shows how to use SageMaker hyperparameter tuning with the pre-built TensorFlow container and MNIST dataset.\n- [MXNet Tuning](hyperparameter_tuning/mxnet_mnist) shows how to use SageMaker hyperparameter tuning with the pre-built MXNet container and MNIST dataset.\n- [HuggingFace Tuning](hyperparameter_tuning/huggingface_multiclass_text_classification_20_newsgroups) shows how to use SageMaker hyperparameter tuning with the pre-built HuggingFace container and 20_newsgroups dataset.\n- [Keras BYO Tuning](hyperparameter_tuning/keras_bring_your_own) shows how to use SageMaker hyperparameter tuning with a custom container running a Keras convolutional network on CIFAR-10 data.\n- [R BYO Tuning](hyperparameter_tuning/r_bring_your_own) shows how to use SageMaker hyperparameter tuning with the custom container from the [Bring Your Own R Algorithm](advanced_functionality/r_bring_your_own) example.\n- [Analyzing Results](hyperparameter_tuning/analyze_results) is a shared notebook that can be used after each of the above notebooks to provide analysis on how training jobs with different hyperparameters performed.\n- [Model tuning for distributed training](hyperparameter_tuning/model_tuning_for_distributed_training) shows how to use SageMaker hyperparameter tuning with Hyperband strategy for optimizing model in distributed training.\n- [Neural Architecture Search for Large Language Models](hyperparameter_tuning/neural_architecture_search_llm) shows how to prune fine-tuned large language models via neural architecture search.\n\n### SageMaker Autopilot\n\nThese examples introduce SageMaker Autopilot. Autopilot automatically performs feature engineering, model selection, model tuning (hyperparameter optimization) and allows you to directly deploy the best model to an endpoint to serve inference requests.\n\n- [Customer Churn AutoML](autopilot/) shows how to use SageMaker Autopilot to automatically train a model for the [Predicting Customer Churn](introduction_to_applying_machine_learning/xgboost_customer_churn) task.\n- [Targeted Direct Marketing AutoML](autopilot/) shows how to use SageMaker Autopilot to automatically train a model.\n- [Housing Prices AutoML](sagemaker-autopilot/housing_prices) shows how to use SageMaker Autopilot for a linear regression problem (predict housing prices).\n- [Portfolio Churn Prediction with Amazon SageMaker Autopilot and Neo4j](autopilot/sagemaker_autopilot_neo4j_portfolio_churn.ipynb) shows how to use SageMaker Autopilot with graph embeddings to predict investment portfolio churn.\n- [Move Amazon SageMaker Autopilot ML models from experimentation to production using Amazon SageMaker Pipelines](autopilot/sagemaker-autopilot-pipelines) shows how to use SageMaker Autopilot in combination with SageMaker Pipelines for end-to-end AutoML training automation.\n- [Amazon SageMaker Autopilot models to serverless endpoints](autopilot/autopilot-serverless-inference) shows how to deploy Autopilot generated models to serverless endpoints.\n\n### Introduction to Amazon Algorithms\n\nThese examples provide quick walkthroughs to get you up and running with Amazon SageMaker's custom developed algorithms. Most of these algorithms can train on distributed hardware, scale incredibly well, and are faster and cheaper than popular alternatives.\n\n- [k-means](sagemaker-python-sdk/1P_kmeans_highlevel) is our introductory example for Amazon SageMaker. It walks through the process of clustering MNIST images of handwritten digits using Amazon SageMaker k-means.\n- [Factorization Machines](introduction_to_amazon_algorithms/factorization_machines_mnist) showcases Amazon SageMaker's implementation of the algorithm to predict whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier.\n- [Latent Dirichlet Allocation (LDA)](introduction_to_amazon_algorithms/lda_topic_modeling) introduces topic modeling using Amazon SageMaker Latent Dirichlet Allocation (LDA) on a synthetic dataset.\n- [Linear Learner](introduction_to_amazon_algorithms/linear_learner_mnist) predicts whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier from Amazon SageMaker Linear Learner.\n- [Neural Topic Model (NTM)](introduction_to_amazon_algorithms/ntm_synthetic) uses Amazon SageMaker Neural Topic Model (NTM) to uncover topics in documents from a synthetic data source, where topic distributions are known.\n- [Principal Components Analysis (PCA)](introduction_to_amazon_algorithms/pca_mnist) uses Amazon SageMaker PCA to calculate eigendigits from MNIST.\n- [Seq2Seq](introduction_to_amazon_algorithms/seq2seq_translation_en-de) uses the Amazon SageMaker Seq2Seq algorithm that's built on top of [Sockeye](https://github.com/awslabs/sockeye), which is a sequence-to-sequence framework for Neural Machine Translation based on MXNet. Seq2Seq implements state-of-the-art encoder-decoder architectures which can also be used for tasks like Abstractive Summarization in addition to Machine Translation. This notebook shows translation from English to German text.\n- [Image Classification](introduction_to_amazon_algorithms/imageclassification_caltech) includes full training and transfer learning examples of Amazon SageMaker's Image Classification algorithm. This uses a ResNet deep convolutional neural network to classify images from the caltech dataset.\n- [XGBoost for regression](introduction_to_amazon_algorithms/xgboost_abalone) predicts the age of abalone ([Abalone dataset](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html)) using regression from Amazon SageMaker's implementation of [XGBoost](https://github.com/dmlc/xgboost).\n- [XGBoost for multi-class classification](introduction_to_amazon_algorithms/xgboost_mnist) uses Amazon SageMaker's implementation of [XGBoost](https://github.com/dmlc/xgboost) to classify handwritten digits from the MNIST dataset as one of the ten digits using a multi-class classifier. Both single machine and distributed use-cases are presented.\n- [DeepAR for time series forecasting](introduction_to_amazon_algorithms/deepar_synthetic) illustrates how to use the Amazon SageMaker DeepAR algorithm for time series forecasting on a synthetically generated data set.\n- [BlazingText Word2Vec](introduction_to_amazon_algorithms/blazingtext_word2vec_text8) generates Word2Vec embeddings from a cleaned text dump of Wikipedia articles using SageMaker's fast and scalable BlazingText implementation.\n- [Object detection for bird images](introduction_to_amazon_algorithms/object_detection_birds) demonstrates how to use the Amazon SageMaker Object Detection algorithm with a public dataset of Bird images.\n- [Object2Vec for movie recommendation](introduction_to_amazon_algorithms/object2vec_movie_recommendation) demonstrates how Object2Vec can be used to model data consisting of pairs of singleton tokens using movie recommendation as a running example.\n- [Object2Vec for multi-label classification](introduction_to_amazon_algorithms/object2vec_multilabel_genre_classification) shows how ObjectToVec algorithm can train on data consisting of pairs of sequences and singleton tokens using the setting of genre prediction of movies based on their plot descriptions.\n- [Object2Vec for sentence similarity](introduction_to_amazon_algorithms/object2vec_sentence_similarity) explains how to train Object2Vec using sequence pairs as input using sentence similarity analysis as the application.\n- [IP Insights for suspicious logins](introduction_to_amazon_algorithms/ipinsights_login) shows how to train IP Insights on a login events for a web server to identify suspicious login attempts.\n- [Semantic Segmentation](introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc) shows how to train a semantic segmentation algorithm using the Amazon SageMaker Semantic Segmentation algorithm. It also demonstrates how to host the model and produce segmentation masks and probability of segmentation.\n- [JumpStart Instance Segmentation](introduction_to_amazon_algorithms/jumpstart_instance_segmentation) demonstrates how to use a pre-trained Instance Segmentation model available in JumpStart for inference.\n- [JumpStart Semantic Segmentation](introduction_to_amazon_algorithms/jumpstart_semantic_segmentation) demonstrates how to use a pre-trained Semantic Segmentation model available in JumpStart for inference, how to finetune the pre-trained model on a custom dataset using JumpStart transfer learning algorithm, and how to use fine-tuned model for inference.\n- [JumpStart Text Generation](introduction_to_amazon_algorithms/jumpstart_text_generation) shows how to use JumpStart to generate text that appears indistinguishable from the hand-written text.\n- [JumpStart Text Summarization](introduction_to_amazon_algorithms/jumpstart_text_summarization) shows how to use JumpStart to summarize the text to contain only the important information.\n- [JumpStart Image Embedding](introduction_to_amazon_algorithms/jumpstart_image_embedding) demonstrates how to use a pre-trained model available in JumpStart for image embedding.\n- [JumpStart Text Embedding](introduction_to_amazon_algorithms/jumpstart_text_embedding) demonstrates how to use a pre-trained model available in JumpStart for text embedding.\n- [JumpStart Object Detection](introduction_to_amazon_algorithms/jumpstart_object_detection) demonstrates how to use a pre-trained Object Detection model available in JumpStart for inference, how to finetune the pre-trained model on a custom dataset using JumpStart transfer learning algorithm, and how to use fine-tuned model for inference.\n- [JumpStart Machine Translation](introduction_to_amazon_algorithms/jumpstart_machine_translation) demonstrates how to translate text from one language to another language in JumpStart.\n- [JumpStart Named Entity Recognition](introduction_to_amazon_algorithms/jumpstart_named_entity_recognition) demonstrates how to identify named entities such as names, locations etc. in the text in JumpStart.\n- [JumpStart Text to Image](introduction_to_amazon_algorithms/jumpstart_text_to_image) demonstrates how to generate image conditioned on text in JumpStart.\n- [JumpStart Upscaling](introduction_to_amazon_algorithms/jumpstart_upscaling) demonstrates how to enhance image quality with Stable Diffusion models in JumpStart.\n- [JumpStart Inpainting](introduction_to_amazon_algorithms/jumpstart_inpainting) demonstrates how to inpaint an image with Stable Diffusion models in JumpStart.\n- [In-context learning with AlexaTM 20B](introduction_to_amazon_algorithms/jumpstart_alexatm20b) demonstrates how to use AlexaTM 20B for in-context-learning in JumpStart.\n### Amazon SageMaker RL\n\nThe following provide examples demonstrating different capabilities of Amazon SageMaker RL.\n\n- [Cartpole using Coach](reinforcement_learning/rl_cartpole_coach) demonstrates the simplest usecase of Amazon SageMaker RL using Intel's RL Coach.\n- [AWS DeepRacer](reinforcement_learning/rl_deepracer_robomaker_coach_gazebo) demonstrates AWS DeepRacer trainig using RL Coach in the Gazebo environment.\n- [HVAC using EnergyPlus](reinforcement_learning/rl_hvac_coach_energyplus) demonstrates the training of HVAC systems using the EnergyPlus environment.\n- [Knapsack Problem](reinforcement_learning/rl_knapsack_coach_custom) demonstrates how to solve the knapsack problem using a custom environment.\n- [Mountain Car](reinforcement_learning/rl_mountain_car_coach_gymEnv) Mountain car is a classic RL problem. This notebook explains how to solve this using the OpenAI Gym environment.\n- [Distributed Neural Network Compression](reinforcement_learning/rl_network_compression_ray_custom) This notebook explains how to compress ResNets using RL, using a custom environment and the RLLib toolkit.\n- [Portfolio Management](reinforcement_learning/rl_portfolio_management_coach_customEnv) This notebook uses a custom Gym environment to manage multiple financial investments.\n- [Autoscaling](reinforcement_learning/rl_predictive_autoscaling_coach_customEnv) demonstrates how to adjust load depending on demand. This uses RL Coach and a custom environment.\n- [Roboschool](reinforcement_learning/rl_roboschool_ray) is an open source physics simulator that is commonly used to train RL policies for robotic systems. This notebook demonstrates training a few agents using it.\n- [Stable Baselines](reinforcement_learning/rl_roboschool_stable_baselines) In this notebook example, we will make the HalfCheetah agent learn to walk using the stable-baselines, which are a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines.\n- [Travelling Salesman](reinforcement_learning/rl_traveling_salesman_vehicle_routing_coach) is a classic NP hard problem, which this notebook solves with AWS SageMaker RL.\n- [Tic-tac-toe](reinforcement_learning/rl_tic_tac_toe_coach_customEnv) is a simple implementation of a custom Gym environment to train and deploy an RL agent in Coach that then plays tic-tac-toe interactively in a Jupyter Notebook.\n- [Unity Game Agent](reinforcement_learning/rl_unity_ray) shows how to use RL algorithms to train an agent to play Unity3D game.\n\n### Scientific Details of Algorithms\n\nThese examples provide more thorough mathematical treatment on a select group of algorithms.\n\n- [Streaming Median](scientific_details_of_algorithms/streaming_median) sequentially introduces concepts used in streaming algorithms, which many SageMaker algorithms rely on to deliver speed and scalability.\n- [Latent Dirichlet Allocation (LDA)](scientific_details_of_algorithms/lda_topic_modeling) dives into Amazon SageMaker's spectral decomposition approach to LDA.\n- [Linear Learner features](scientific_details_of_algorithms/linear_learner_class_weights_loss_functions) shows how to use the class weights and loss functions features of the SageMaker Linear Learner algorithm to improve performance on a credit card fraud prediction task\n\n### Amazon SageMaker Debugger\n\nThese examples provide and introduction to SageMaker Debugger which allows debugging and monitoring capabilities for training of machine learning and deep learning algorithms. Note that although these notebooks focus on a specific framework, the same approach works with all the frameworks that Amazon SageMaker Debugger supports. The notebooks below are listed in the order in which we recommend you review them.\n\n- [Using a built-in rule with TensorFlow](sagemaker-debugger/tensorflow_builtin_rule/)\n- [Using a custom rule with TensorFlow Keras](sagemaker-debugger/tensorflow_keras_custom_rule/)\n- [Interactive tensor analysis in notebook with MXNet](sagemaker-debugger/mnist_tensor_analysis/)\n- [Visualizing Debugging Tensors of MXNet training](sagemaker-debugger/mnist_tensor_plot/)\n- [Real-time analysis in notebook with MXNet](sagemaker-debugger/mxnet_realtime_analysis/)\n- [Using a built in rule with XGBoost](sagemaker-debugger/xgboost_builtin_rules/)\n- [Real-time analysis in notebook with XGBoost](sagemaker-debugger/xgboost_realtime_analysis/)\n- [Using SageMaker Debugger with Managed Spot Training and MXNet](sagemaker-debugger/mxnet_spot_training/)\n- [Reacting to CloudWatch Events from Rules to take an action based on status with TensorFlow](sagemaker-debugger/tensorflow_action_on_rule/)\n- [Using SageMaker Debugger with a custom PyTorch container](sagemaker-debugger/pytorch_custom_container/)\n\n### Amazon SageMaker Distributed Training\n\nThese examples provide an introduction to SageMaker Distributed Training Libraries for data parallelism and model parallelism. The libraries are optimized for the SageMaker training environment, help adapt your distributed training jobs to SageMaker, and improve training speed and throughput.\nMore examples for models such as BERT and YOLOv5 can be found in [distributed_training/](https://github.com/aws/amazon-sagemaker-examples/tree/main/training/distributed_training).\n\n- [Train GPT-2 with Sharded Data Parallel](https://github.com/aws/amazon-sagemaker-examples/tree/main/training/distributed_training/pytorch/model_parallel/gpt2/smp-train-gpt-simple-sharded-data-parallel.ipynb) shows how to train GPT-2 with near-linear scaling using Sharded Data Parallelism technique in SageMaker Model Parallelism Library.\n- [Train EleutherAI GPT-J with Model Parallel](https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt-j/11_train_gptj_smp_tensor_parallel_notebook.ipynb) shows how to train EleutherAI GPT-J with PyTorch and Tensor Parallelism technique in the SageMaker Model Parallelism Library.\n- [Train MaskRCNN with Data Parallel](https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/data_parallel/maskrcnn/pytorch_smdataparallel_maskrcnn_demo.ipynb) shows how to train MaskRCNN with PyTorch and SageMaker Data Parallelism Library.\n\n### Amazon SageMaker Smart Sifting\n\nThese examples provide an Introduction to Smart Sifting library. Smart Sifting is a framework to speed up training of PyTorch models. The framework implements a set of algorithms that filter out inconsequential training examples during training, reducing the computational cost and accelerating the training process. It is configuration-driven and extensible, allowing users to add custom logic to transform their training examples into a filterable format. Smart sifting provides a generic utility for any DNN model, and can reduce the training cost by up to 35% in infrastructure cost.\n  \n- [Train Image Classification using Vision Transformer with Smart Sifting](https://github.com/aws/amazon-sagemaker-examples/tree/main/training/smart_sifting/Image_Classification_VIT/Train_Image_classification.ipynb): This Example shows how to use Smart sifting to fine tune Vision Transformers for Image Classification.\n- [Train Text Classification using BERT with Smart Sifting](https://github.com/aws/amazon-sagemaker-examples/tree/main/training/smart_sifting/Text_Classification_BERT/Train_text_classification.ipynb): This Example shows how to use Smart Sifting to fine tune BERT for Text Classification.\n\n### Amazon SageMaker Clarify\n\nThese examples provide an introduction to SageMaker Clarify which provides machine learning developers with greater visibility into their training data and models so they can identify and limit bias and explain predictions.\n\n* [Fairness and Explainability with SageMaker Clarify](sagemaker-clarify/fairness_and_explainability) shows how to use SageMaker Clarify Processor API to measure the pre-training bias of a dataset and post-training bias of a model, and explain the importance of the input features on the model's decision.\n* [Amazon SageMaker Clarify Model Monitors](sagemaker_model_monitor/fairness_and_explainability) shows how to use SageMaker Clarify Model Monitor API to schedule bias monitor to monitor predictions for bias drift on a regular basis, and schedule explainability monitor to monitor predictions for feature attribution drift on a regular basis.\n\n### Publishing content from RStudio on Amazon SageMaker to RStudio Connect\n\nThese examples show you how to run R examples, and publish applications in RStudio on Amazon SageMaker to RStudio Connect.\n\n- [Publishing R Markdown](r_examples/rsconnect_rmarkdown/) shows how you can author an R Markdown document (.Rmd, .Rpres) within RStudio on Amazon SageMaker and publish to RStudio Connect for wide consumption.\n- [Publishing R Shiny Apps](r_examples/rsconnect_shiny/) shows how you can author an R Shiny application within RStudio on Amazon SageMaker and publish to RStudio Connect for wide consumption.\n- [Publishing Streamlit Apps](r_examples/rsconnect_streamlit/) shows how you can author a streamlit application withing Amazon SageMaker Studio and publish to RStudio Connect for wide consumption.\n\n### Advanced Amazon SageMaker Functionality\n\nThese examples showcase unique functionality available in Amazon SageMaker. They cover a broad range of topics and utilize a variety of methods, but aim to provide the user with sufficient insight or inspiration to develop within Amazon SageMaker.\n\n- [Data Distribution Types](advanced_functionality/data_distribution_types) showcases the difference between two methods for sending data from S3 to Amazon SageMaker Training instances. This has particular implication for scalability and accuracy of distributed training.\n- [Distributed Training and Batch Transform with Sentiment Classification](advanced_functionality/sentiment_parallel_batch) shows how to use SageMaker Distributed Data Parallelism, SageMaker Debugger, and distrubted SageMaker Batch Transform on a HuggingFace Estimator, in a sentiment classification use case.\n- [Encrypting Your Data](advanced_functionality/handling_kms_encrypted_data) shows how to use Server Side KMS encrypted data with Amazon SageMaker training. The IAM role used for S3 access needs to have permissions to encrypt and decrypt data with the KMS key.\n- [Using Parquet Data](advanced_functionality/parquet_to_recordio_protobuf) shows how to bring [Parquet](https://parquet.apache.org/) data sitting in S3 into an Amazon SageMaker Notebook and convert it into the recordIO-protobuf format that many SageMaker algorithms consume.\n- [Connecting to Redshift](advanced_functionality/working_with_redshift_data) demonstrates how to copy data from Redshift to S3 and vice-versa without leaving Amazon SageMaker Notebooks.\n- [Bring Your Own XGBoost Model](advanced_functionality/xgboost_bring_your_own_model) shows how to use Amazon SageMaker Algorithms containers to bring a pre-trained model to a realtime hosted endpoint without ever needing to think about REST APIs.\n- [Bring Your Own k-means Model](advanced_functionality/kmeans_bring_your_own_model) shows how to take a model that's been fit elsewhere and use Amazon SageMaker Algorithms containers to host it.\n- [Bring Your Own R Algorithm](advanced_functionality/r_bring_your_own) shows how to bring your own algorithm container to Amazon SageMaker using the R language.\n- [Installing the R Kernel](advanced_functionality/install_r_kernel) shows how to install the R kernel into an Amazon SageMaker Notebook Instance.\n- [Bring Your Own scikit Algorithm](advanced_functionality/scikit_bring_your_own) provides a detailed walkthrough on how to package a scikit learn algorithm for training and production-ready hosting.\n- [Bring Your Own MXNet Model](advanced_functionality/mxnet_mnist_byom) shows how to bring a model trained anywhere using MXNet into Amazon SageMaker.\n- [Bring Your Own TensorFlow Model](advanced_functionality/tensorflow_iris_byom) shows how to bring a model trained anywhere using TensorFlow into Amazon SageMaker.\n- [Bring Your Own Model train and deploy BERTopic](advanced_functionality/pytorch_extend_container_train_deploy_bertopic) shows how to bring a model through an external library, how to train it and deploy it into Amazon SageMaker by extending the pytorch base containers.\n- [Experiment Management Capabilities with Search](advanced_functionality/search) shows how to organize Training Jobs into projects, and track relationships between Models, Endpoints, and Training Jobs.\n- [Host Multiple Models with Your Own Algorithm](advanced_functionality/multi_model_bring_your_own) shows how to deploy multiple models to a realtime hosted endpoint with your own custom algorithm.\n- [Host Multiple Models with XGBoost](advanced_functionality/multi_model_xgboost_home_value) shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled XGBoost container.\n- [Host Multiple Models with SKLearn](advanced_functionality/multi_model_sklearn_home_value) shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled SKLearn container.\n- [Host Multimodal HuggingFace Model](advanced_functionality/huggingface_deploy_instructpix2pix) shows how to host an instruction based image editing model from HuggingFace as a SageMaker endpoint using single core or multi-core GPU based instances. Inference Recommender is used to run load tests and compare the performance of instances.\n- [SageMaker Training and Inference with Script Mode](sagemaker-script-mode) shows how to use custom training and inference scripts, similar to those you would use outside of SageMaker, with SageMaker's prebuilt containers for various frameworks like Scikit-learn, PyTorch, and XGBoost.\n- [Host Models with NVidia Triton Server](sagemaker-triton) shows how to deploy models to a realtime hosted endpoint using [Triton](https://developer.nvidia.com/nvidia-triton-inference-server) as the model inference server.\n- [Heterogenous Clusters Training in TensorFlow or PyTorch ](training/heterogeneous-clusters/README.md) shows how to train using TensorFlow tf.data.service (distributed data pipeline) or Pytorch (with gRPC) on top of Amazon SageMaker Heterogenous clusters to overcome CPU bottlenecks by including different instance types (GPU/CPU) in the same training job.\n\n### Amazon SageMaker Neo Compilation Jobs\n\nThese examples provide an introduction to how to use Neo to compile and optimize deep learning models.\n\n- [GluonCV SSD Mobilenet](sagemaker_neo_compilation_jobs/gluoncv_ssd_mobilenet) shows how to train GluonCV SSD MobileNet and use Amazon SageMaker Neo to compile and optimize the trained model.\n- [Image Classification](sagemaker_neo_compilation_jobs/imageclassification_caltech) Adapts from [image classification](introduction_to_amazon_algorithms/imageclassification_caltech) including Neo API and comparison against the uncompiled baseline.\n- [MNIST with MXNet](sagemaker_neo_compilation_jobs/mxnet_mnist) Adapts from [MXNet MNIST](sagemaker-python-sdk/mxnet_mnist) including Neo API and comparison against the uncompiled baseline.\n- [Deploying pre-trained PyTorch vision models](sagemaker_neo_compilation_jobs/pytorch_torchvision) shows how to use Amazon SageMaker Neo to compile and optimize pre-trained PyTorch models from TorchVision.\n- [Distributed TensorFlow](sagemaker_neo_compilation_jobs/tensorflow_distributed_mnist) includes Neo API and comparison against the uncompiled baseline.\n- [Predicting Customer Churn](sagemaker_neo_compilation_jobs/xgboost_customer_churn) Adapts from [XGBoost customer churn](introduction_to_applying_machine_learning/xgboost_customer_churn) including Neo API and comparison against the uncompiled baseline.\n\n### Amazon SageMaker Processing\n\nThese examples show you how to use SageMaker Processing jobs to run data processing workloads.\n\n- [Scikit-Learn Data Processing and Model Evaluation](sagemaker_processing/scikit_learn_data_processing_and_model_evaluation) shows how to use SageMaker Processing and the Scikit-Learn container to run data preprocessing and model evaluation workloads.\n- [Feature transformation with Amazon SageMaker Processing and SparkML](sagemaker_processing/feature_transformation_with_sagemaker_processing) shows how to use SageMaker Processing to run data processing workloads using SparkML prior to training.\n- [Feature transformation with Amazon SageMaker Processing and Dask](sagemaker_processing/feature_transformation_with_sagemaker_processing_dask) shows how to use SageMaker Processing to transform data using Dask distributed clusters\n- [Distributed Data Processing using Apache Spark and SageMaker Processing](sagemaker_processing/spark_distributed_data_processing) shows how to use the built-in Spark container on SageMaker Processing using the SageMaker Python SDK.\n\n### Amazon SageMaker Pipelines\n\nThese examples show you how to use [SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines) to create, automate and manage end-to-end Machine Learning workflows.\n\n- [Amazon Comprehend with SageMaker Pipelines](sagemaker-pipelines/nlp/amazon_comprehend_sagemaker_pipeline) shows how to deploy a custom text classification using Amazon Comprehend and SageMaker Pipelines.\n- [Amazon Forecast with SageMaker Pipelines](sagemaker-pipelines/time_series_forecasting/amazon_forecast_pipeline) shows how you can create a dataset, dataset group and predictor with Amazon Forecast and SageMaker Pipelines.\n- [Multi-model SageMaker Pipeline with Hyperparamater Tuning and Experiments](sagemaker-pipeline-multi-model) shows how you can generate a regression model by training real estate data from Athena using Data Wrangler, and uses multiple algorithms both from a custom container and a SageMaker container in a single pipeline.\n- [SageMaker Pipeline Local Mode with FrameworkProcessor and BYOC for PyTorch with sagemaker-training-toolkig](sagemaker-pipelines/tabular/local-mode/framework-processor-byoc)\n- [SageMaker Pipeline Step Caching](sagemaker-pipelines/tabular/caching) shows how you can leverage pipeline step caching while building pipelines and shows expected cache hit / cache miss behavior.\n- [Native AutoML step in SageMaker Pipelines](sagemaker-pipelines/tabular/automl-step/sagemaker_autopilot_pipelines_native_auto_ml_step.ipynb) shows how you can use SageMaker Autopilot with a native AutoML step in SageMaker Pipelines for end-to-end AutoML training automation.\n- [Computer Vision Pipeline using step decorator](sagemaker-pipelines/step-decorator/computer-vision-examples/computer-vision-pipeline.ipynb) shows how you can augment a dataset, train a computer vision model, and evaluate the model using a combination of built-in steps and the step decorator.\n\n### Amazon SageMaker Pre-Built Framework Containers and the Python SDK\n\n#### Pre-Built Deep Learning Framework Containers\n\nThese examples show you how to train and host in pre-built deep learning framework containers using the SageMaker Python SDK.\n\n- [Chainer CIFAR-10](sagemaker-python-sdk/chainer_cifar10) trains a VGG image classification network on CIFAR-10 using Chainer (both single machine and multi-machine versions are included)\n- [Chainer MNIST](sagemaker-python-sdk/chainer_mnist) trains a basic neural network on MNIST using Chainer (shows how to use local mode)\n- [Chainer sentiment analysis](sagemaker-python-sdk/chainer_sentiment_analysis) trains a LSTM network with embeddings to predict text sentiment using Chainer\n- [IRIS with Scikit-learn](sagemaker-python-sdk/scikit_learn_iris) trains a Scikit-learn classifier on IRIS data\n- [Model Registry and Batch Transform with Scikit-learn](sagemaker-python-sdk/scikit_learn_model_registry_batch_transform) trains a Scikit-learn Random Forest model, registers it in Model Registry, and runs a Batch Transform Job.\n- [MNIST with MXNet Gluon](sagemaker-python-sdk/mxnet_gluon_mnist) trains a basic neural network on the MNIST handwritten digit dataset using MXNet Gluon\n- [MNIST with MXNet](sagemaker-python-sdk/mxnet_mnist) trains a basic neural network on the MNIST handwritten digit data using MXNet's symbolic syntax\n- [Sentiment Analysis with MXNet Gluon](sagemaker-python-sdk/mxnet_gluon_sentiment) trains a text classifier using embeddings with MXNet Gluon\n- [TensorFlow training and serving](sagemaker-python-sdk/tensorflow_script_mode_training_and_serving) trains a basic neural network on MNIST\n- [TensorFlow with Horovod](sagemaker-python-sdk/tensorflow_script_mode_horovod) trains on MNIST using Horovod for distributed training\n- [TensorFlow using shell commands](sagemaker-python-sdk/tensorflow_script_mode_using_shell_commands) shows how to use a shell script for the container's entry point\n\n#### Pre-Built Machine Learning Framework Containers\n\nThese examples show you how to build Machine Learning models with frameworks like Apache Spark or Scikit-learn using SageMaker Python SDK.\n\n- [Inference with SparkML Serving](sagemaker-python-sdk/sparkml_serving_emr_mleap_abalone) shows how to build an ML model with Apache Spark using Amazon EMR on Abalone dataset and deploy in SageMaker with SageMaker SparkML Serving.\n- [Pipeline Inference with Scikit-learn and LinearLearner](sagemaker-python-sdk/scikit_learn_inference_pipeline) builds a ML pipeline using Scikit-learn preprocessing and LinearLearner algorithm in single endpoint\n\n### Using Amazon SageMaker with Apache Spark\n\nThese examples show how to use Amazon SageMaker for model training, hosting, and inference through Apache Spark using [SageMaker Spark](https://github.com/aws/sagemaker-spark). SageMaker Spark allows you to interleave Spark Pipeline stages with Pipeline stages that interact with Amazon SageMaker.\n\n- [MNIST with SageMaker PySpark](sagemaker-spark/pyspark_mnist)\n- [Parameterize spark configuration in pipeline PySparkProcessor execution](sagemaker-spark/parameterize-spark-config-pysparkprocessor-pipeline) shows how you can define spark-configuration in different pipeline PysparkProcessor executions\n\n### Using Amazon SageMaker with Amazon Keyspaces (for Apache Cassandra)\n\nThese examples show how to use Amazon SageMaker to read data from [Amazon Keyspaces](https://docs.aws.amazon.com/keyspaces/).\n- [Train Machine Learning Models using Amazon Keyspaces as a Data Source](ingest_data/sagemaker-keyspaces)\n\n\n### AWS Marketplace\n\n#### Create algorithms/model packages for listing in AWS Marketplace for machine learning.\n\nThese example notebooks show you how to package a model or algorithm for listing in AWS Marketplace for machine learning.\n\n- [Creating Marketplace Products](aws_marketplace/creating_marketplace_products)\n  - [Creating a Model Package - Listing on AWS Marketplace](aws_marketplace/creating_marketplace_products/models) provides a detailed walkthrough on how to package a pre-trained model as a SageMaker Model Package that can be listed on AWS Marketplace.\n  - [Creating Algorithm and Model Package - Listing on AWS Marketplace](aws_marketplace/creating_marketplace_products/algorithms) provides a detailed walkthrough on how to package a scikit learn algorithm to create SageMaker Algorithm and SageMaker Model Package entities that can be used with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs and listed on AWS Marketplace.\n\nOnce you have created an algorithm or a model package to be listed in the AWS Marketplace, the next step is to list it in AWS Marketplace, and provide a sample notebook that customers can use to try your algorithm or model package.\n\n- [Curate your AWS Marketplace model package listing and sample notebook](aws_marketplace/curating_aws_marketplace_listing_and_sample_notebook/ModelPackage) provides instructions on how to craft a sample notebook to be associated with your listing and how to curate a good AWS Marketplace listing that makes it easy for AWS customers to consume your model package.\n- [Curate your AWS Marketplace algorithm listing and sample notebook](aws_marketplace/curating_aws_marketplace_listing_and_sample_notebook/Algorithm) provides instructions on how to craft a sample notebook to be associated with your listing and how to curate a good AWS Marketplace listing that makes it easy for your customers to consume your algorithm.\n\n#### Use algorithms, data, and model packages from AWS Marketplace.\n\nThese examples show you how to use model-packages and algorithms from AWS Marketplace and dataset products from AWS Data Exchange, for machine learning.\n\n- [Using Algorithms](aws_marketplace/using_algorithms)\n  - [Using Algorithm From AWS Marketplace](aws_marketplace/using_algorithms/amazon_demo_product) provides a detailed walkthrough on how to use Algorithm with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs by choosing a canonical product listed on AWS Marketplace.\n  - [Using AutoML algorithm](aws_marketplace/using_algorithms/automl) provides a detailed walkthrough on how to use AutoML algorithm from AWS Marketplace.\n- [Using Model Packages](aws_marketplace/using_model_packages)\n  - [Using Model Packages From AWS Marketplace](aws_marketplace/using_model_packages/generic_sample_notebook) is a generic notebook which provides sample code snippets you can modify and use for performing inference on Model Packages from AWS Marketplace, using Amazon SageMaker.\n  - [Using Amazon Demo product From AWS Marketplace](aws_marketplace/using_model_packages/amazon_demo_product) provides a detailed walkthrough on how to use Model Package entities with the enhanced SageMaker Transform/Hosting APIs by choosing a canonical product listed on AWS Marketplace.\n  - [Using models for extracting vehicle metadata](aws_marketplace/using_model_packages/auto_insurance) provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of auto-insurance claim processing.\n  - [Using models for identifying non-compliance at a workplace](aws_marketplace/using_model_packages/improving_industrial_workplace_safety) provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of generating summary reports for identifying non-compliance at a construction/industrial workplace.\n  - [Creative writing using GPT-2 Text Generation](aws_marketplace/using_model_packages/creative-writing-using-gpt-2-text-generation) will show you how to use AWS Marketplace GPT-2-XL pre-trained model on Amazon SageMaker to generate text based on your prompt to help you author prose and poetry.\n  - [Amazon Augmented AI with AWS Marketplace ML models](aws_marketplace/using_model_packages/amazon_augmented_ai_with_aws_marketplace_ml_models) will show you how to use AWS Marketplace pre-trained ML models with Amazon Augmented AI to implement human-in-loop workflow reviews with your ML model predictions.\n  - [Monitoring data quality in third-party models from AWS Marketplace](aws_marketplace/using_model_packages/data_quality_monitoring) will show you how to perform Data Quality monitoring on a pre-trained third-party model from AWS Marketplace.\n  - [Evaluating ML models from AWS Marketplace for person counting use case](aws_marketplace/using_model_packages/evaluating_aws_marketplace_models_for_person_counting_use_case) will show you how to use two AWS Marketplace GluonCV pre-trained ML models for person counting use case and evaluate each model for performance in different types of crowd images.\n  - [Preprocessing audio data using a pre-trained machine learning model](using_model_packages/preprocessing-audio-data-using-a-machine-learning-model) demonstrates the usage of a pre-trained audio track separation model to create synthetic features and improve an acoustic classification model.\n- [Using Dataset Products](aws_marketplace/using_data)\n  - [Using Dataset Product from AWS Data Exchange with ML model from AWS Marketplace](aws_marketplace/using_data/using_data_with_ml_model) is a sample notebook which shows how a dataset from AWS Data Exchange can be used with an ML Model Package from AWS Marketplace.\n  - [Using Shutterstock Image Datasets to train Image Classification Models](aws_marketplace/using_data/image_classification_with_shutterstock_image_datasets) provides a detailed walkthrough on how to use the [Free Sample: Images & Metadata of \u201cWhole Foods\u201d Shoppers](https://aws.amazon.com/marketplace/pp/prodview-y6xuddt42fmbu?qid=1623195111604&sr=0-1&ref_=srh_res_product_title#offers) from Shutterstock's Image Datasets to train a multi-label image classification model using Shutterstock's pre-labeled image assets. You can learn more about this implementation [from this blog post](https://aws.amazon.com/blogs/awsmarketplace/using-shutterstocks-image-datasets-to-train-your-computer-vision-models/).\n\n## :balance_scale: License\n\nThis library is licensed under the [Apache 2.0 License](http://aws.amazon.com/apache2.0/).\nFor more details, please take a look at the [LICENSE](https://github.com/aws/amazon-sagemaker-examples/blob/master/LICENSE.txt) file.\n\n## :handshake: Contributing\n\nAlthough we're extremely excited to receive contributions from the community, we're still working on the best mechanism to take in examples from external sources. Please bear with us in the short-term if pull requests take longer than expected or are closed.\nPlease read our [contributing guidelines](https://github.com/aws/amazon-sagemaker-examples/blob/master/CONTRIBUTING.md)\nif you'd like to open an issue or submit a pull request.\n", "release_dates": ["2020-10-19T20:39:08Z"]}, {"name": "amazon-sagemaker-examples-community", "description": null, "language": "Jupyter Notebook", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "![SageMaker](https://github.com/aws/amazon-sagemaker-examples/raw/main/_static/sagemaker-banner.png)\n\nAmazon SageMaker Community Notebooks\nWelcome to the Amazon SageMaker Community Notebooks repository! This repository is designed to be a space where additional notebooks, beyond those critical for showcasing key SageMaker functionality, can be shared and explored by the community. While the primary SageMaker Example Notebooks repository focuses on demonstrating the core capabilities of Amazon SageMaker, this community repository aims to provide supplementary resources, tips, and innovative use cases.\n\nThe Main Notebook repository showcasing critical sagemaker functionality is found [here](https://github.com/aws/amazon-sagemaker-examples)\n\n\ud83d\udcda Purpose\n\nAmazon SageMaker is a powerful tool for simplifying machine learning workflows, from data preprocessing to model deployment. However, the journey of mastering SageMaker often involves experimentation, creative problem-solving, and the exploration of unique approaches that might not fit the standard showcase format. This community repository is here to accommodate such scenarios by hosting a collection of notebooks that might not fit the conventional mold but still provide valuable insights, alternative techniques, and unconventional applications of SageMaker.\n\n\ud83d\ude80 How to Contribute\n\nWe welcome contributions from the community! If you have a notebook that you believe would benefit others, whether it's an innovative workaround, a specialized use case, or a creative visualization, feel free to share it here. To contribute:\n\nFork this repository to your own GitHub account.\nCreate a new branch from the main branch for your changes.\nAdd your notebook to an appropriate directory or create a new directory if needed.\nCraft a comprehensive README for your notebook, explaining its purpose, contents, and any necessary setup instructions.\nSubmit a pull request back to this repository's main branch for review.\nRemember, the focus here is not just on showcasing SageMaker's core features, but on fostering a community-driven space for learning, sharing, and discovering innovative applications of machine learning with SageMaker.\n\n\ud83d\udcbb Exploration\n\nTo explore the notebooks in this repository:\n\nClone or Download: You can clone this repository to your local machine or download specific notebooks as needed.\n\nNotebook Instances: If you have an Amazon SageMaker Notebook Instance, you can upload these community notebooks and experiment with them directly in the SageMaker environment.\n\nLocal Setup: For notebooks that don't require extensive SageMaker-specific functionality, you can often run them on your local machine with minimal modifications. Just ensure you have the necessary libraries installed and, if needed, update IAM role definitions accordingly.\n\n\ud83d\udcdd Note\n\nWhile the primary SageMaker Example Notebooks repository is carefully curated to demonstrate key functionality, the notebooks in this community repository are more open-ended and diverse. They may not undergo the same level of scrutiny, and it's important to exercise caution and understand the code you're using.\n\nAs of now, the default branch is named main. Please refer to our announcements for any updates or changes.\n\nThank you for being part of the Amazon SageMaker community! Together, we can explore the full spectrum of machine learning possibilities this platform has to offer.\n", "release_dates": []}, {"name": "amazon-sagemaker-feedback", "description": "Amazon SageMaker Public Feedback Dashboard", "language": null, "license": {"key": "cc-by-sa-4.0", "name": "Creative Commons Attribution Share Alike 4.0 International", "spdx_id": "CC-BY-SA-4.0", "url": "https://api.github.com/licenses/cc-by-sa-4.0", "node_id": "MDc6TGljZW5zZTI2"}, "readme": "## Amazon SageMaker Public Feedback Dashboard\n\nThis is a public dashboard for Amazon SageMaker where you can raise issues, request features and ask questions. Amazon SageMaker is a fully managed machine learning service provided by Amazon Web Services. With SageMaker, data scientists and developers can quickly and easily build, train, tune, and deploy machine learning models at scale. To get started, visit [aws.amazon.com/sagemaker/](https://aws.amazon.com/sagemaker/).\n\n## Introduction\nThis is an experimental public dashboard for SageMaker where you can raise issues, request features and ask questions.  We want to work closely with you to shape the future of our product and your feedback will be invaluable as we plan upcoming features and map out the product roadmap. By working together, we can ensure we are focused on solving the most pressing issues that will have the greatest impact. We will keep you updated on the progress and you can track our work here - [amazon-sagemaker-feedback-tracker](https://github.com/orgs/aws/projects/160). We will continue to improve how we manage and organize this dashboard over time, based on input from customers that engage in it. \n\n## Security\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues. Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security](mailto:aws-security@amazon.com) directly. See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## FAQs\n**1) Why did you build this?**\n\nWe want our users to play an active role in shaping the future of our product. Gaining insights into the key challenges you face will be invaluable as we work to prioritize upcoming features and plan our roadmap. Through collaboration, we can ensure our development efforts are focused on addressing issues that truly matter. \n\n**2) Why are there no dates on your roadmap?**\n\nBecause our main priority is security and operational stability, we cannot provide specific target dates for releases. \n\n**3) Is everything on the roadmap?**\n\nThis is an experimental public roadmap for SageMaker, which currently focuses only on customer feedback. It is not intended to include everything we are working on. There will always be features and products that we are very excited about that we are going to launch without notice to surprise and delight our customers. We will continue to improve how we manage and organize this roadmap over time, based on input from customers that engage in it.\n\n**4) Will you build everything here?**\n\nWe intend to investigate all issues and feature requests, however we can't commit to delivering any feature or in any specific timeline. Roadmap issues in this repository do not guarantee a feature will be launched as proposed.\n\n**5) What do roadmap categories mean?**\n\n* *Shipped* - currently usable in SageMaker.\n*\t*Coming soon* - nearing completion. Think a couple of months out.\n*\t*We're working on it* - in progress, but further out. We might still be working through the implementation details, or scoping stuff out.\n* *Researching* - We're thinking about it. This might mean we're still evaluating designs and options, or thinking through how this should work.\n\n**6) How can I provide feedback or ask for more information?**\n\nPlease create an issue! All submitted issues will get reviewed and/or forwarded appropriately. When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can.\n\n**7) How can I request a feature be added to the roadmap?**\n\nWe encourage you to open an issue, even if you\u2019ve requested it before via other channels. All community-submitted issues will be reviewed by the roadmap maintainers. We\u2019ve created a template to make it easy to open new issues. When filing a request, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can.\n\n**8) Can I \"+1\" existing issues?**\n\nIf you find that someone has already reported a similar issue or requested a feature, we strongly encourage you to vote for it. This helps us prioritize issues with the widest impact. Please don't leave comments saying \"+1\" as such comments generate noise for people receiving notifications, and issues can't be sorted by number of \"+1\" comments. Instead, leave a \ud83d\udc4d reaction that can be used to sort issues for prioritization. To leave a reaction, navigate to the issue details page and add a reaction with the \ud83d\ude00 button on the post. We want you to help us decide which items will benefit you the most.\n\n**9) Will you accept a pull request?**\n\nWe haven't worked out how pull requests should work for a public feedback page, but we will take all PRs very seriously and review for inclusion. \n\n\n\n## License Summary\n\nThe documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.\n", "release_dates": []}, {"name": "amazon-sagemaker-operator-for-k8s", "description": "Amazon SageMaker operator for Kubernetes", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon SageMaker Operators for Kubernetes\n![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/aws/amazon-sagemaker-operator-for-k8s?sort=semver&logo=amazon-aws&color=232F3E)\n[![License](https://img.shields.io/badge/license-Apache--2.0-blue.svg?color=success)](http://www.apache.org/licenses/LICENSE-2.0)\n![GitHub go.mod Go version](https://img.shields.io/github/go-mod/go-version/aws/amazon-sagemaker-operator-for-k8s?color=69D7E5)\n\n## Introduction\nAmazon SageMaker Operators for Kubernetes are operators that can be used to train machine learning models, optimize hyperparameters for a given model, run batch transform jobs over existing models, and set up inference endpoints. With these operators, users can manage their jobs in Amazon SageMaker from their Kubernetes cluster in Amazon Elastic Kubernetes Service [EKS](http://aws.amazon.com/eks).\n\n## Migrate resources to the new SageMaker Operators for Kubernetes\n> :warning: This project will reach its end-of-life (EOL) on **Feb 15, 2023** along with [Amazon Elastic Kubernetes Service Kubernetes 1.21](https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar). If you are currently using version *v1.2.2* or below of this SageMaker Operators for Kubernetes, we recommend migrating your resources to the latest SageMaker Operators for Kubernetes, the **[ACK service controller for Amazon SageMaker](https://github.com/aws-controllers-k8s/sagemaker-controller)** based on [AWS Controllers for Kubernetes (ACK)](https://aws-controllers-k8s.github.io/community/).   \n\n- For information on the migration steps, see [Migrate resources to the latest Operators.](https://docs.aws.amazon.com/sagemaker/latest/dg/kubernetes-sagemaker-operators-migrate.html)\n- For answers to frequently asked questions (FAQ) regarding the end of support of this version of SageMaker Operators for Kubernetes, see [Announcing the End of Support of the Original Version of SageMaker Operator for Kubernetes.](https://docs.aws.amazon.com/sagemaker/latest/dg/kubernetes-sagemaker-operators-eos-announcement.html)\n\n\n## Usage\n> :note: The following steps do not install the latest version of SageMaker Operators for Kubernetes. Find the link to the new ACK-based SageMaker Operators for Kubernetes project in the warning message above.\n\nFirst, you must [install the operators](https://docs.aws.amazon.com/sagemaker/latest/dg/kubernetes-sagemaker-operators.html). After installation is complete, create a TrainingJob YAML specification by following one of the samples, like [samples/xgboost-mnist-trainingjob.yaml](./samples/xgboost-mnist-trainingjob.yaml). Then, use `kubectl` to create and monitor the progress of your job:\n\n```bash\n$ kubectl apply -f xgboost-mnist-trainingjob.yaml\ntrainingjob.sagemaker.aws.amazon.com/xgboost-mnist created\n\n$ kubectl get trainingjob\nNAME            STATUS       SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\nxgboost-mnist   InProgress   Starting           2019-11-26T23:38:11Z   xgboost-mnist-cf1e16fb10a511eaaa450a350733ba06\n```\n\nOnce the job starts training, you can use a `kubectl` plugin to stream training logs:\n\n```bash\n$ kubectl get trainingjob\nNAME            STATUS       SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\nxgboost-mnist   InProgress   Training           2019-11-26T23:38:11Z   xgboost-mnist-cf1e16fb10a511eaaa450a350733ba06\n\n$ kubectl smlogs trainingjob xgboost-mnist | head -n 5\n\"xgboost-mnist\" has SageMaker TrainingJobName \"xgboost-mnist-cf1e16fb10a511eaaa450a350733ba06\" in region \"us-east-2\", status \"InProgress\" and secondary status \"Training\"\nxgboost-mnist-cf1e16fb10a511eaaa450a350733ba06/algo-1-1574811611 2019-11-26 15:41:13.449 -0800 PST Arguments: train\nxgboost-mnist-cf1e16fb10a511eaaa450a350733ba06/algo-1-1574811611 2019-11-26 15:41:13.449 -0800 PST [2019-11-26:23:41:10:INFO] Running standalone xgboost training.\nxgboost-mnist-cf1e16fb10a511eaaa450a350733ba06/algo-1-1574811611 2019-11-26 15:41:13.45 -0800 PST [2019-11-26:23:41:10:INFO] File size need to be processed in the node: 1122.95mb. Available memory size in the node: 8501.08mb\nxgboost-mnist-cf1e16fb10a511eaaa450a350733ba06/algo-1-1574811611 2019-11-26 15:41:13.45 -0800 PST [2019-11-26:23:41:10:INFO] Determined delimiter of CSV input is ','\nxgboost-mnist-cf1e16fb10a511eaaa450a350733ba06/algo-1-1574811611 2019-11-26 15:41:13.45 -0800 PST [23:41:10] S3DistributionType set as FullyReplicated\n```\n\nThe Amazon SageMaker Operators for Kubernetes enable management of SageMaker TrainingJobs, HyperParameterTuningJobs, BatchTransformJobs and HostingDeployments (Endpoints). Create and monitor them using the same `kubectl` tool as above.\n\nTo install the operators onto your Kubernetes cluster, follow our [User Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/kubernetes-sagemaker-operators.html).\n\n### YAML Examples\n\nTo make a YAML spec, follow one of the below examples as a guide. Replace values like RoleARN, S3 input buckets and S3 output buckets with values that correspond to your account.\n\n* [BatchTransformJob](./samples/xgboost-mnist-batchtransform.yaml)\n* [HostingDeployment (Endpoint)](./samples/xgboost-mnist-hostingdeployment.yaml)\n* [HyperParameterTuningJob](./samples/xgboost-mnist-hpo.yaml)\n* [TrainingJob](./samples/xgboost-mnist-trainingjob.yaml)\n\n## Releases\n\nAmazon SageMaker Operator for Kubernetes adheres to the [SemVer](https://semver.org/) specification. Each release updates the major version tag (eg. `vX`), a major/minor version tag (eg. `vX.Y`) and a major/minor/patch version tag (eg. `vX.Y.Z`), as well as new versions of the `smlogs` binary with URLs of the same versioning formats. To see a full list of all releases, refer to our [Github releases page](https://github.com/aws/amazon-sagemaker-operator-for-k8s/releases).\n\nWe also maintain a `latest` tag, which is updated to stay in line with the `master` branch. We **do not** recommend installing this on any production cluster, as any new major versions updated on the `master` branch will introduce breaking changes.\n\n## Contributing\n`amazon-sagemaker-operator-for-k8s` is an open source project. See [CONTRIBUTING](https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/CONTRIBUTING.md) for details.\n\n## License\n\nThis project is distributed under the\n[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0),\nsee [LICENSE](https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/LICENSE) and [NOTICE](https://github.com/aws/amazon-sagemaker-operator-for-k8s/blob/master/NOTICE) for more information.\n", "release_dates": ["2022-09-16T04:31:26Z", "2021-05-26T01:39:05Z", "2021-04-29T21:29:45Z", "2020-04-02T22:50:12Z", "2019-12-09T23:22:04Z", "2019-12-01T00:52:53Z"]}, {"name": "amazon-ssm-agent", "description": "An agent to enable remote management of your EC2 instances, on-premises servers, or virtual machines (VMs).", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "[![ReportCard][ReportCard-Image]][ReportCard-URL]\n[![Build Status](https://travis-ci.org/aws/amazon-ssm-agent.svg?branch=mainline)](https://travis-ci.org/aws/amazon-ssm-agent)\n\n# Amazon SSM Agent\n\nThe Amazon EC2 Simple Systems Manager (SSM) Agent is software developed for the [Simple Systems Manager Service](http://docs.aws.amazon.com/systems-manager/latest/APIReference/Welcome.html). The SSM Agent is the primary component of a feature called Run Command.\n\n## Overview\n\nThe SSM Agent runs on EC2 instances and enables you to quickly and easily execute remote commands or scripts against one or more instances. The agent uses SSM [documents](https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-ssm-docs.html). When you execute a command, the agent on the instance processes the document and configures the instance as specified.\nCurrently, the agent and Run Command enable you to quickly run Shell scripts on an instance using the AWS-RunShellScript SSM document. \nSSM Agent also enables the Session Manager capability that lets you manage your Amazon EC2 instance through an interactive one-click browser-based shell or through the AWS CLI. The first time a Session Manager session is started on an instance, the agent will create a user called \"ssm-user\" with sudo or administrator privilege. Session Manager sessions will be launched in context of this user.\n\n### Verify Requirements\n\n* [SSM Run Command Prerequisites](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/remote-commands-prereq.html)\n* [SSM Session Manager Prerequisites and supported Operating Systems](http://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-prerequisites.html)\n* Linux systems require kernel version 3.2 or above\n\n### Setup\n\n* [Configuring IAM Roles and Users for SSM Run Command](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ssm-iam.html)\n* [Configuring the SSM Agent](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-ssm-agent.html)\n* [Configuring IAM Roles for Session Manager](http://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-instance-profile.html)\n* [Configuring Users for Session Manager](http://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-restrict-access.html)\n\n### Executing Commands\n\n[SSM Run Command Walkthrough Using the AWS CLI](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/walkthrough-cli.html)\n\n### Starting Sessions\n\n[Session Manager Walkthrough Using the AWS Console and CLI](http://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-sessions-start.html)\n\n### Troubleshooting\n\n[Troubleshooting SSM Run Command](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-remote-commands.html)\n[Troubleshooting SSM Session Manager](http://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-troubleshooting.html)\n\n## Feedback\n\nThank you for helping us to improve Systems Manager, Run Command and Session Manager. Please send your questions or comments to [Systems Manager Forums](https://forums.aws.amazon.com/forum.jspa?forumID=185&start=0)\n\n### Building inside docker container (Recommended)\n* Install docker: [Install CentOS](https://docs.docker.com/engine/install/centos/)\n\n* Build image\n```\ndocker build -t ssm-agent-build-image .\n```\n* Build the agent\n```\ndocker run -it --rm --name ssm-agent-build-container -v `pwd`:/amazon-ssm-agent ssm-agent-build-image make build-release\n```\n\n### Building on Linux\n\n* Install go [Getting started](https://golang.org/doc/install)\n\n* Install rpm-build and rpmdevtools\n\n* [Cross Compile SSM Agent](https://www.ardanlabs.com/blog/2013/10/cross-compile-your-go-programs.html)\n\n* Run `make build` to build the SSM Agent for Linux, Debian, Windows environment.\n\n* Run `make build-release` to build the agent and also packages it into a RPM, DEB and ZIP package.\n\nThe following folders are generated when the build completes:\n```\nbin/debian_386\nbin/debian_amd64\nbin/linux_386\nbin/linux_amd64\nbin/linux_arm\nbin/linux_arm64\nbin/windows_386\nbin/windows_amd64\n```\n* To enable the Agent for Session Manager scenario on Windows instances\n    * Clone the repo from https://github.com/masatma/winpty.git\n    * Follow instructions on https://github.com/rprichard/winpty to build winpty 64-bit binaries\n    * Copy the winpty.dll and winpty-agent.exe to the bin/SessionManagerShell folder\nFor the Windows Operating System, Session Manager is only supported on Windows Server 2008 R2 through Windows Server 2019 64-bit versions.\n\nPlease follow the user guide to [copy and install the SSM Agent](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-ssm-agent.html)\n\n### Code Layout\n\n* Source code\n    * Core functionality such as worker management is under core/\n    * Agent worker code is under agent/\n    * Other functionality such as IPC is under common/\n* Vendor package source code is under vendor/src\n* rpm and dpkg artifacts are under packaging\n* build scripts are under Tools/src\n\n### Linting\n\nTo lint the entire module call the `lint-all` target. This executes golangci-lint on all packages in the module.\nYou can configure golangci-lint with different linters using the `.golangci.yml` file.\n\nFor golangci-lint installation instructions see https://golangci-lint.run/usage/install/\nFor more information on the golangci-lint configuration file see https://golangci-lint.run/usage/configuration/\nFor more information on the linters used see https://golangci-lint.run/usage/linters/\n\n### GOPATH\n\nTo use vendor dependencies, the suggested GOPATH format is `:<packagesource>/vendor:<packagesource>`\n\n### Make Targets\n\nThe following targets are available. Each may be run with `make <target>`.\n\n| Make Target              | Description |\n|:-------------------------|:------------|\n| `build`                  | *(Default)* `build` builds the agent for Linux, Debian, Darwin and Windows amd64 and 386 environment |\n| `build-release`          | `build-release` checks code style and coverage, builds the agent and also packages it into a RPM, DEB and ZIP package |\n| `release`                | `release` checks code style and coverage, runs tests, packages all dependencies to the bin folder. |\n| `package`                | `package` packages build result into a RPM, DEB and ZIP package |\n| `pre-build`              | `pre-build` goes through Tools/src folder to make sure all the script files are executable |\n| `checkstyle`             | `checkstyle` runs the checkstyle script |\n| `analyze-install`        | `analyze-install` install static analysis dependencies for local use |\n| `analyze`                | `analyze` runs static analysis script to find possible vulnerabilities |\n| `quick-integtest`        | `quick-integtest` runs all tests tagged with integration using `go test` |\n| `quick-test`             | `quick-test` runs all the tests including integration and unit tests using `go test` |\n| `coverage`               | `coverage` runs all tests and calculate code coverage |\n| `build-linux`            | `build-linux` builds the agent for execution in the Linux amd64 environment |\n| `build-windows`          | `build-windows` builds the agent for execution in the Windows amd64 environment |\n| `build-darwin`           | `build-darwin` builds the agent for execution in the Darwin amd64 environment |\n| `build-linux-386`        | `build-linux-386` builds the agent for execution in the Linux 386 environment |\n| `build-windows-386`      | `build-windows-386` builds the agent for execution in the Windows 386 environment |\n| `build-darwin-386`       | `build-darwin-386` builds the agent for execution in the Darwin 386 environment |\n| `build-arm`              | `build-arm` builds the agent for execution in the arm environment |\n| `build-arm64`            | `build-arm64` builds the agent for execution in the arm64 environment |\n| `lint-all`               | `lint-all` runs golangci-lint on all packages. golangci-lint is configured by .golangci.yml |\n| `package-rpm`            | `package-rpm` builds the agent and packages it into a RPM package for Linux amd64 based distributions |\n| `package-deb`            | `package-deb` builds the agent and packages it into a DEB package Debian amd64 based distributions |\n| `package-win`            | `package-win` builds the agent and packages it into a ZIP package Windows amd64 based distributions |\n| `package-rpm-386`        | `package-rpm-386` builds the agent and packages it into a RPM package for Linux 386 based distributions |\n| `package-deb-386`        | `package-deb-386` builds the agent and packages it into a DEB package Debian 386 based distributions |\n| `package-win-386`        | `package-win-386` builds the agent and packages it into a ZIP package Windows 386 based distributions |\n| `package-rpm-arm64`      | `package-rpm-arm64` builds the agent and packages it into a RPM package Linux arm64 based distributions |\n| `package-deb-arm`        | `package-deb-arm` builds the agent and packages it into a DEB package Debian arm based distributions |\n| `package-deb-arm64`      | `package-deb-arm64` builds the agent and packages it into a DEB package Debian arm64 based distributions |\n| `package-linux`          | `package-linux` create update packages for Linux and Debian based distributions |\n| `package-windows`        | `package-windows` create update packages for Windows based distributions |\n| `package-darwin`         | `package-darwin` create update packages for Darwin based distributions |\n| `get-tools`              | `get-tools` gets gocode and oracle using `go get` |\n| `clean`                  | `clean` removes build artifacts |\n\n### Contributing\n\nContributions and feedback are welcome! Proposals and Pull Requests will be considered and responded to. Please see the [CONTRIBUTING.md](https://github.com/aws/amazon-ssm-agent/blob/mainline/CONTRIBUTING.md) file for more information.\n\nAmazon Web Services does not currently provide support for modified copies of this software.\n\n## Runtime Configuration\n\nTo set up your own custom configuration for the agent:\n* Navigate to /etc/amazon/ssm/ (or C:\\Program Files\\Amazon\\SSM for windows)\n* Copy the contents of amazon-ssm-agent.json.template to a new file amazon-ssm-agent.json\n* Restart agent\n\n### Config Property Definitions:\n* Profile - represents configurations for aws credential profile used to get managed instance role and credentials\n    * ShareCreds (boolean)\n        * Default: true\n    * ShareProfile (string)\n    * ForceUpdateCreds (boolean) - overwrite shared credentials file if existing one cannot be parsed\n        * Default: false\n    * KeyAutoRotateDays (int) - defines the maximum age in days for on-prem private key, default value might change to 30 in the close future\n        * Default: 0 (never rotate)\n* Mds - represents configuration for Message delivery service (MDS) where agent listens for incoming messages\n    * CommandWorkersLimit (int)\n        * Default: 5\n    * StopTimeoutMillis (int64)\n        * Default: 20000\n    * Endpoint (string)\n    * CommandRetryLimit (int)\n        * Default: 15\n* Ssm - represents configuration for Simple Systems Manager (SSM)\n    * Endpoint (string)\n    * HealthFrequencyMinutes (int)\n        * Default: 5\n    * CustomInventoryDefaultLocation (string)\n    * AssociationLogsRetentionDurationHours (int)\n        * Default: 24\n    * RunCommandLogsRetentionDurationHours (int)\n        * Default: 336\n    * SessionLogsRetentionDurationHours (int)\n        * Default: 336\n    * SessionLogsDestination (string) - Configure where you want Session Manager to write session data.\n        * Default: \"disk\" - Write session data to disk.\n        * OptionalValue: \"none\" - Don't write session data anywhere when CloudWatch and S3 logging are disabled.\n    * PluginLocalOutputCleanup (string) - Configure when after execution it is safe to delete local plugin output logs in orchestration folder\n        * Default: \"\" - Don't delete logs immediately after execution. Fall back to AssociationLogsRetentionDurationHours, RunCommandLogsRetentionDurationHours, and SessionLogsRetentionDurationHours \n        * OptionalValue: \"after-execution\" - Delete plugin output file locally after plugin execution\n        * OptionalValue: \"after-upload\" - Delete plugin output locally after successful s3 or cloudWatch upload\n    * OrchestrationDirectoryCleanup (string) - Configure only when it is safe to delete orchestration folder after document execution. This config overrides PluginLocalOutputCleanup when set.\n        * Default: \"\" - Don't delete orchestration folder after execution\n        * OptionalValue: \"clean-success\" - Deletes the orchestration folder only for successful document executions.\n        * OptionalValue: \"clean-success-failed\" - Deletes the orchestration folder for successful and failed document executions.\n* Mgs - represents configuration for Message Gateway service\n    * Region (string)\n    * Endpoint (string)\n    * StopTimeoutMillis (int64)\n        * Default: 20000\n    * SessionWorkersLimit (int)\n        * Default: 1000\n    * DeniedPortForwardingRemoteIPs ([]string)\n        * Default: [\"169.254.169.254\", \"fd00:ec2::254\", \"169.254.169.253\", \"fd00:ec2::253\"]\n* Agent - represents metadata for amazon-ssm-agent\n    * Region (string)\n    * OrchestrationRootDir (string)\n        * Default: \"orchestration\"\n    * SelfUpdate (boolean)\n        * Default: false\n    * TelemetryMetricsToCloudWatch (boolean)\n        * Default: false\n    * TelemetryMetricsToSSM (boolean)\n        * Default: true\n    * AuditExpirationDay (int)\n        * Default: 7\n    * LongRunningWorkerMonitorIntervalSeconds (int)\n        * Default: 60\n    * GoMaxProcForAgentWorker (int)\n        * Default: 0\n* Os - represents os related information, will be logged in reply messages\n    * Lang (string)\n        * Default: \"en-US\"\n    * Name (string)\n    * Version (string)\n        * Default: 1\n* S3 - represents configurations related to S3 bucket and key for SSM. Endpoint and region are typically determined automatically, and should only be set if a custom endpoint is required.  LogBucket and LogKey are currently unused.\n    * Endpoint (string)\n        * Default: \"\"\n    * Region (string) - Ignored\n    * LogBucket (string) - Ignored\n    * LogKey (string) - Ignored\n* Kms - represents configuration for Key Management Service if encryption is enabled for this session (i.e. kmsKeyId is set or using \"Port\" plugin) \n    * Endpoint (string)\n\n## Release\n\nAfter the SSM Agent source code has been released to github, it can take up to 2 weeks for the install packages to propagate to all AWS regions.\n\nThe following commands can be used to pull the `VERSION` file and check the latest agent available in a region.\n* Regional Bucket *(Non-CN*) - `curl https://s3.{region}.amazonaws.com/amazon-ssm-{region}/latest/VERSION`\n  * Replace `{region}` with region code like `us-east-1`.\n* Regional Bucket *(CN)* - `curl https://s3.{region}.amazonaws.com.cn/amazon-ssm-{region}/latest/VERSION`\n  * Replace `{region}` with region code `cn-north-1`, `cn-northwest-1`.\n* Global Bucket - `curl https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/VERSION`\n\n## License\n\nThe Amazon SSM Agent is licensed under the Apache 2.0 License.\n\n[ReportCard-URL]: https://goreportcard.com/report/aws/amazon-ssm-agent\n[ReportCard-Image]: https://goreportcard.com/badge/aws/amazon-ssm-agent\n", "release_dates": ["2024-02-07T22:46:48Z", "2024-01-31T22:12:03Z", "2024-01-18T22:13:56Z", "2023-12-26T20:18:28Z", "2023-12-12T22:12:25Z", "2023-11-13T15:28:47Z", "2023-10-11T18:46:35Z", "2023-09-18T23:33:40Z", "2023-09-18T23:31:59Z", "2023-08-14T21:49:28Z", "2023-07-19T18:22:02Z", "2023-07-03T20:04:30Z", "2023-06-27T17:25:40Z", "2023-06-05T18:29:35Z", "2023-05-23T16:39:51Z", "2023-05-05T21:10:37Z", "2023-04-05T20:28:38Z", "2023-02-07T19:17:25Z", "2023-02-07T19:16:22Z", "2023-01-18T16:56:19Z", "2022-12-23T22:14:40Z", "2022-12-22T23:24:29Z", "2022-11-18T20:20:04Z", "2022-11-01T18:23:59Z", "2022-11-01T18:21:34Z", "2022-09-28T16:23:21Z", "2022-09-09T21:45:02Z", "2022-08-24T19:12:05Z", "2022-07-14T22:06:15Z", "2022-06-16T21:30:19Z"]}, {"name": "amazon-ssm-agent-selinux", "description": "SELinux policy to confine Amazon SSM agent", "language": "Roff", "license": {"key": "gpl-2.0", "name": "GNU General Public License v2.0", "spdx_id": "GPL-2.0", "url": "https://api.github.com/licenses/gpl-2.0", "node_id": "MDc6TGljZW5zZTg="}, "readme": "## AMAZON-SSM-AGENT-SELINUX POLICY\n\n###### Note - This policy has been tested to work on AL2 only for now. Please make sure that the OS you are using is AL2.\n\nThis is the SELinux policy for AWS SSM agent. Install this policy to confine your SSM agent processes.\n\n## Installation instructions\n\n###### Note - It is recommended to start SELinux in permissive mode before enabling it in enforcement mode.\n\nMake sure Amazon SSM agent service is installed and running before compiling and installing the SELinux policy:\n```\nsudo yum install amazon-ssm-agent\nsudo systemctl start amazon-ssm-agent\n```\n\nRun the following commands:\n```\nsudo yum update\nsudo yum install policycoreutils-devel rpm-build git\n```\n\nTo build and install the SELinux policy, make sure that SELinux config file is in `permissive` or `enforcing` mode in `/etc/selinux/config` file:\n```\n# This file controls the state of SELinux on the system.\n# SELINUX= can take one of these three values:\n#     enforcing - SELinux security policy is enforced.\n#     permissive - SELinux prints warnings instead of enforcing.\n#     disabled - No SELinux policy is loaded.\nSELINUX=enforcing\n# SELINUXTYPE= can take one of three two values:\n#     targeted - Targeted processes are protected,\n#     minimum - Modification of targeted policy. Only selected processes are protected. \n#     mls - Multi Level Security protection.\nSELINUXTYPE=targeted\n```\nClone the repository, build and install using the commands below:\n\n```\ngit clone https://github.com/aws/amazon-ssm-agent-selinux.git\ncd amazon-ssm-agent-selinux\nchmod +x amazon_ssm_agent.sh\nsudo ./amazon_ssm_agent.sh\n```\nReboot the instance and verify that your instance is in `enforcing` mode or `permissive` mode as chosen:\n\n```\nsudo sestatus\n\nSELinux status:                 enabled\nSELinuxfs mount:                /sys/fs/selinux\nSELinux root directory:         /etc/selinux\nLoaded policy name:             targeted\nCurrent mode:                   enforcing\nMode from config file:          enforcing\nPolicy MLS status:              enabled\nPolicy deny_unknown status:     allowed\nMax kernel policy version:      31\n\n```\nList the SSM processes to check that they are confined:\n\n```\n\nps -efZ | grep -i amazon\n\nsystem_u:system_r:amazon_ssm_agent_t:s0 root 5665  1  0 00:15 ?        00:00:02 /usr/bin/amazon-ssm-agent\nsystem_u:system_r:amazon_ssm_agent_t:s0 root 5746 5665  0 00:15 ?      00:00:02 /usr/bin/ssm-agent-worker\n\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the GNU GPL v2 License.\n\n", "release_dates": []}, {"name": "amazon-ssm-document-language-service", "description": null, "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS Systems Manager Document Language Service\n\nAWS Systems Manager Document Language Service is a wrapper around [vscode-json-languageservice](https://github.com/microsoft/vscode-json-languageservice), and [yaml-language-server](https://github.com/redhat-developer/yaml-language-server#readme). It extends its functionality with validation, completions and snippets specific to AWS Systems Manager (SSM) documents.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "amazon-states-language-service", "description": null, "language": "TypeScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "## Amazon States Language Service\n\nAmazon States Language Service is a wrapper around [vscode-json-languageservice](https://github.com/microsoft/vscode-json-languageservice). It extends its functionality with validation, completions and snippets specific to amazon states language. It preserves its interface and can be used as a drop-in replacement for it. \n\n## License\n\nThis library is licensed under the MIT License. See the LICENSE file.\n\n\n\n![Build Status](https://codebuild.us-west-2.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoidVBIMnJWT0tzQ1E3ZmhGZGdyWXdQNWdVc01OVEp1bXBrUlhIOXdROUdsMkM0dVFNcVJzc242anFkdFg2WFdDNTh2OTdnbkNCSGYwdmJ3cVdCZ1gzNjQ4PSIsIml2UGFyYW1ldGVyU3BlYyI6IkV4UVlhZU9EWW5WWkhsNDUiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=master)\n\n", "release_dates": []}, {"name": "amazon-vpc-cni-k8s", "description": "Networking plugin repository for pod networking in Kubernetes using Elastic Network Interfaces on AWS", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "\n# amazon-vpc-cni-k8s\n\nNetworking plugin for pod networking in [Kubernetes](https://kubernetes.io/) using [Elastic Network Interfaces](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html) on AWS.\n\n[![Nightly Tests](https://github.com/aws/amazon-vpc-cni-k8s/workflows/Nightly%20Cron%20tests/badge.svg)](https://github.com/aws/amazon-vpc-cni-k8s/actions)\n[![GoReport Widget]][GoReport Status] [![codecov](https://codecov.io/gh/aws/amazon-vpc-cni-k8s/branch/master/graph/badge.svg)](https://codecov.io/gh/aws/amazon-vpc-cni-k8s)\n\n[GoReport Status]: https://goreportcard.com/report/github.com/aws/amazon-vpc-cni-k8s\n[GoReport Widget]: https://goreportcard.com/badge/github.com/aws/amazon-vpc-cni-k8s?\n\n## Setup\n\nDownload the latest version of the [yaml](./config/) and apply it to the cluster.\n\n```\nkubectl apply -f aws-k8s-cni.yaml\n```\n\nLaunch kubelet with network plugins set to cni (`--network-plugin=cni`), the cni directories configured (`--cni-config-dir`\nand `--cni-bin-dir`) and node ip set to the primary IPv4 address of the primary ENI for the instance\n(`--node-ip=$(curl http://169.254.169.254/latest/meta-data/local-ipv4)`).\nIt is also recommended that you set `--max-pods` equal to _(the number of ENIs for the instance type \u00d7\n(the number of IPs per ENI - 1)) + 2_; for details, see [vpc_ip_resource_limit.go][]. Setting `--max-pods` will prevent\nscheduling that exceeds the IP address resources available to the kubelet.\n\n[vpc_ip_resource_limit.go]: ./pkg/awsutils/vpc_ip_resource_limit.go\n\nThe default manifest expects `--cni-conf-dir=/etc/cni/net.d` and `--cni-bin-dir=/opt/cni/bin`.\n\nAlternatively there is also a [Helm](https://helm.sh/) chart: [eks/aws-vpc-cni](https://github.com/aws/eks-charts/tree/master/stable/aws-vpc-cni)\n\n## IAM Policy\n\nSee [here](./docs/iam-policy.md) for required IAM policies.\n\n## Building\n\n* `make` defaults to `make build-linux` that builds the Linux binaries.\n* `unit-test`, `format`,`lint` and `vet` provide ways to run the respective tests/tools and should be run before submitting a PR.\n* `make docker` will create a docker container using `docker buildx` that contains the finished binaries, with a tag of `amazon/amazon-k8s-cni:latest`\n* `make docker-unit-tests` uses a docker container to run all unit tests.\n* builds for all build and test actions run in docker containers based on `golang:1.21.5-6-gcc-al2` unless a different `GOLANG_IMAGE` tag is passed in.\n\n## Components\n\n  There are 2 components:\n\n* [CNI Plugin](https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/#cni), which will wire up the host's and pod's network stack when called.\n* `ipamd`, a long-running node-Local IP Address Management (IPAM) daemon, is responsible for:\n  * maintaining a warm-pool of available IP addresses, and\n  * assigning an IP address to a Pod.\n\nThe details can be found in [Proposal: CNI plugin for Kubernetes networking over AWS VPC](https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/cni-proposal.md).\n\n## Help & Feedback\n\nFor help, please consider the following venues (in order):\n\n* [Amazon EKS Best Practices Guide for Networking](https://aws.github.io/aws-eks-best-practices/networking/index/)\n* [Troubleshooting Guide](https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/troubleshooting.md) provides tips on how to debug and troubleshoot this CNI.\n* [Search open issues](https://github.com/aws/amazon-vpc-cni-k8s/issues)\n* [File an issue](https://github.com/aws/amazon-vpc-cni-k8s/issues/new/choose)\n* Chat with us on the `#aws-vpc-cni` channel in the [Kubernetes Slack](https://kubernetes.slack.com/) community.\n\n## Recommended Version\n\nFor all Kubernetes releases, *we recommend installing the latest VPC CNI release*. The following table denotes our *oldest* recommended\nVPC CNI version for each actively supported Kubernetes release.\n\n| Kubernetes Release | 1.29     | 1.28     | 1.27     | 1.26     | 1.25     | 1.24    |\n| ------------------ | -------- | -------- | -------- | -------- | -------- | ------- |\n| VPC CNI Version    | v1.14.1+ | v1.13.4+ | v1.12.5+ | v1.12.0+ | v1.11.4+ | v1.9.3+ |\n\n## Version Upgrade\n\nUpgrading (or downgrading) the VPC CNI version should result in no downtime. Existing pods should not be affected and will not lose network connectivity.\nNew pods will be in pending state until the VPC CNI is fully initialized and can assign pod IP addresses. In v1.12.0+, VPC CNI state is\nrestored via an on-disk file: `/var/run/aws-node/ipam.json`. In lower versions, state is restored via calls to container runtime.\n\n## ENI Allocation\n\nWhen a worker node first joins the cluster, there is only 1 ENI along with all of the addresses on the ENI. Without any\nconfiguration, ipamd always tries to keep one extra ENI.\n\nWhen the number of pods running on the node exceeds the number of addresses on a single ENI, the CNI backend starts allocating\na new ENI using the following allocation scheme:\n\n* If the number of current running Pods is between 0 and 29, ipamd will allocate one more eni. And Warm-Pool size is 2 eni * (30 -1) = 58\n* If the number of current running Pods is between 30 and 58, ipamd will allocate 2 more eni. And Warm-Pool size is 3 eni * (30 -1) = 87\n\nFor example, a m4.4xlarge node can have up to 8 ENIs, and each ENI can have up to 30 IP addresses. See\n[Elastic Network Interfaces documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html) for details.\n\nFor a detailed explanation, see [`WARM_ENI_TARGET`, `WARM_IP_TARGET` and `MINIMUM_IP_TARGET`](https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/eni-and-ip-target.md).\n\n## Privileged mode\n\nVPC CNI makes use of privileged mode (`privileged: true`) in the manifest for its `aws-vpc-cni-init` and `aws-eks-nodeagent` containers. `aws-vpc-cni-init` container requires elevated privilege to set the networking kernel parameters while `aws-eks-nodeagent` container requires these privileges for attaching BPF probes to enforce network policy\n\n## Network Policies\n\nIn Kubernetes, by default, all pod-to-pod communication is allowed. Communication can be restricted with Kubernetes NetworkPolicy objects.\n\nVPC CNI versions v1.14, and greater support [Kubernetes Network Policies.](https://kubernetes.io/docs/concepts/services-networking/network-policies/). Network Policies specify how pods can communicate over the network, at the IP address or port level. The VPC CNI implements the Kubernetes [NetworkPolicy](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#networkpolicy-v1-networking-k8s-io) API. Network policies generally include a pod selector, and Ingress/Egress rules.\n\nFor EKS clusters, review [Configure your cluster for Kubernetes network policies](https://docs.aws.amazon.com/eks/latest/userguide/cni-network-policy.html) in the Amazon EKS User Guide.\n\nThe AWS VPC CNI implementation of network policies may be enabled in self-managed clusters. This requires the VPC CNI agent, the [Network Policy Controller](https://github.com/aws/amazon-network-policy-controller-k8s), and [Network Policy Node Agent](https://github.com/aws/aws-network-policy-agent).\n\nReview the [Network Policy FAQ](./docs/network-policy-faq.md) for more information.\n\n### Network Policy Related Components\n\n* [Network Policy Controller](https://github.com/aws/amazon-network-policy-controller-k8s) watches for NetworkPolicy objects and instructs the node agent.\n  * Network policy controller configures policies for pods in parallel to pod provisioning, until then new pods will come up with default allow policy. All ingress and egress traffic is allowed to and from the new pods until they are resolved against the existing policies.\n  * This controller is automatically installed on the EKS Control Plane.\n* [Network Policy Node Agent](https://github.com/aws/aws-network-policy-agent) implements Network Policies on nodes by creating eBPF programs.\n* [AWS eBPF SDK for Go](https://github.com/aws/aws-ebpf-sdk-go) provides an interface to interact with eBPF programs on the node. This SDK allows for runtime introspection, tracing, and analysis of eBPF execution, aiding in identifying and resolving connectivity issues.\n* [VPC Resource Controller](https://github.com/aws/amazon-vpc-resource-controller-k8s) manages Branch & Trunk Network Interfaces for Kubernetes Pods. \n\n## ConfigMap\n\nIf the VPC CNI is installed as an Amazon EKS add-ons (also known as a managed add-on), configure it using [AWS APIs as described in the EKS User Guide](https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html).\n\nIf the VPC CNI is installed with a Helm Chart, the ConfigMap is installed in your cluster. Review the [Helm Chart information.](https://github.com/aws/eks-charts/tree/master/stable/aws-vpc-cni)\n\nOtherwise, the VPC CNI may be configured with a ConfigMap, as shown below:\n\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: amazon-vpc-cni\n  namespace: kube-system\ndata:\n  enable-network-policy-controller: \"true\"\n```\n\n* `enable-network-policy-controller`\n  * Default: False\n  * If enabled, the VPC CNI will enforce NetworkPolicies. Pods that are selected by at least one NetworkPolicy will have their traffic restricted.\n\n## Helm Charts\n\nAWS publishes a Helm chart to install the VPC CNI. [Review how to install the helm chart, and the configuration parameters for the chart.](https://github.com/aws/eks-charts/tree/master/stable/aws-vpc-cni)\n\n## CNI Configuration Variables<a name=\"cni-env-vars\"></a>\n\nThe Amazon VPC CNI plugin for Kubernetes supports a number of configuration options, which are set through environment variables.\nThe following environment variables are available, and all of them are optional.\n\n#### `AWS_MANAGE_ENIS_NON_SCHEDULABLE` (v1.12.6+)\n\nType: Boolean as a String\n\nDefault: `false`\n\nSpecifies whether IPAMD should allocate or deallocate ENIs on a non-schedulable node.\n\n#### `AWS_VPC_CNI_NODE_PORT_SUPPORT`\n\nType: Boolean as a String\n\nDefault: `true`\n\nSpecifies whether `NodePort` services are enabled on a worker node's primary network interface\\. This requires additional\n`iptables` rules, and the kernel's reverse path filter on the primary interface is set to `loose`.\n\n#### `AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG`\n\nType: Boolean as a String\n\nDefault: `false`\n\nSpecifies that your pods may use subnets and security groups that are independent of your worker node's VPC configuration.\nBy default, pods share the same subnet and security groups as the worker node's primary interface\\. Setting this variable\nto `true` causes `ipamd` to use the security groups and VPC subnet in a worker node's `ENIConfig` for elastic network interface\nallocation\\. You must create an `ENIConfig` custom resource for each subnet that your pods will reside in, and then annotate or\nlabel each worker node to use a specific `ENIConfig`. Multiple worker nodes can be annotated or labelled with the same `ENIConfig`, but\neach Worker node can be annotated with a single `ENIConfig` at a time.  Further, the subnet in the `ENIConfig` must belong to the\nsame Availability Zone that the worker node resides in.\nFor more information, see [*CNI Custom Networking*](https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network.html)\nin the Amazon EKS User Guide.\n\n#### `ENI_CONFIG_ANNOTATION_DEF`\n\nType: String\n\nDefault: `k8s.amazonaws.com/eniConfig`\n\nSpecifies node annotation key name. This should be used when `AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true`. Annotation value\nwill be used to set `ENIConfig` name. Note that annotations take precedence over labels.\n\n#### `ENI_CONFIG_LABEL_DEF`\n\nType: String\n\nDefault: `k8s.amazonaws.com/eniConfig`\n\nSpecifies node label key name\\. This should be used when `AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true`. Label value will be used\nto set `ENIConfig` name\\. Note that annotations will take precedence over labels. To use labels, ensure there is no annotation with key\n`k8s.amazonaws.com/eniConfig` or defined key (in `ENI_CONFIG_ANNOTATION_DEF`) set on the node.\nTo select an `ENIConfig` based upon availability zone set this to `topology.kubernetes.io/zone` and create an\n`ENIConfig` custom resource for each availability zone (e.g. `us-east-1a`). Note that tag `failure-domain.beta.kubernetes.io/zone` is deprecated and replaced with the tag `topology.kubernetes.io/zone`.\n\n#### `HOST_CNI_BIN_PATH`\n\nType: String\n\nDefault: `/host/opt/cni/bin`\n\nSpecifies the location to install CNI binaries. Note that the `aws-node` daemonset mounts `/opt/cni/bin` to `/host/opt/cni/bin`. The value you choose must be a location that the `aws-node` pod can write to.\n\n#### `HOST_CNI_CONFDIR_PATH`\n\nType: String\n\nDefault: `/host/etc/cni/net.d`\n\nSpecifies the location to install the VPC CNI conflist. Note that the `aws-node` daemonset mounts `/etc/cni/net.d` to `/host/etc/cni/net.d`. The value you choose must be a location that the `aws-node` pod can write to.\n\n#### `AWS_VPC_ENI_MTU` (v1.6.0+)\n\nType: Integer as a String\n\nDefault: 9001\n\nUsed to configure the MTU size for attached ENIs. The valid range for IPv4 is from `576` to `9001`, while the valid range for IPv6 is from `1280` to `9001`.\n\n#### `AWS_VPC_K8S_CNI_EXTERNALSNAT`\n\nType: Boolean as a String\n\nDefault: `false`\n\nSpecifies whether an external NAT gateway should be used to provide SNAT of secondary ENI IP addresses. If set to `true`, the\nSNAT `iptables` rule and off\\-VPC IP rule are not applied, and these rules are removed if they have already been applied.\nDisable SNAT if you need to allow inbound communication to your pods from external VPNs, direct connections, and external VPCs,\nand your pods do not need to access the Internet directly via an Internet Gateway. However, your nodes must be running in a\nprivate subnet and connected to the internet through an AWS NAT Gateway or another external NAT device.\n\n#### `AWS_VPC_K8S_CNI_RANDOMIZESNAT`\n\nType: String\n\nDefault: `prng`\n\nValid Values: `hashrandom`, `prng`, `none`\n\nSpecifies whether the SNAT `iptables` rule should randomize the outgoing ports for connections\\. This setting takes effect when\n`AWS_VPC_K8S_CNI_EXTERNALSNAT=false`, which is the default setting. The default setting for `AWS_VPC_K8S_CNI_RANDOMIZESNAT` is\n`prng`, meaning that `--random-fully` will be added to the SNAT `iptables` rule\\. For old versions of `iptables` that do not\nsupport `--random-fully` this option will fall back to `--random`. To disable random port allocation, if you for example\nrely on sequential port allocation for outgoing connections set it to `none`.\n\n*Note*: Any options other than `none` will cause outbound connections to be assigned a source port that is not necessarily\npart of the ephemeral port range set at the OS level (`/proc/sys/net/ipv4/ip_local_port_range`). This is relevant for any\ncustomers that might have NACLs restricting traffic based on the port range found in `ip_local_port_range`.\n\n#### `AWS_VPC_K8S_CNI_EXCLUDE_SNAT_CIDRS` (v1.6.0+)\n\nType: String\n\nDefault: empty\n\nSpecify a comma-separated list of IPv4 CIDRs to exclude from SNAT. For every item in the list an `iptables` rule and off\\-VPC\nIP rule will be applied. If an item is not a valid ipv4 range it will be skipped. This should be used when `AWS_VPC_K8S_CNI_EXTERNALSNAT=false`.\n\n#### `POD_MTU` (v1.16.4+)\n\nType: Integer as a String\n\n*Note*: If unset, the default value is derived from `AWS_VPC_ENI_MTU`, which defaults to `9001`.\nDefault: 9001\n\nUsed to configure the MTU size for pod virtual interfaces. The valid range for IPv4 is from `576` to `9001`, while the valid range for IPv6 is from `1280` to `9001`.\n\n#### `WARM_ENI_TARGET`\n\nType: Integer as a String\n\nDefault: `1`\n\nSpecifies the number of free elastic network interfaces \\(and all of their available IP addresses\\) that the `ipamd` daemon should\nattempt to keep available for pod assignment on the node\\. By default, `ipamd` attempts to keep 1 elastic network interface and all\nof its IP addresses available for pod assignment. The number of IP addresses per network interface varies by instance type. For more\ninformation, see [IP Addresses Per Network Interface Per Instance Type](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI)\nin the *Amazon EC2 User Guide for Linux Instances*.\n\nFor example, an `m4.4xlarge` launches with 1 network interface and 30 IP addresses\\. If 5 pods are placed on the node and 5 free IP\naddresses are removed from the IP address warm pool, then `ipamd` attempts to allocate more interfaces until `WARM_ENI_TARGET` free\ninterfaces are available on the node.\n\n**NOTE!** If `WARM_IP_TARGET` is set, then this environment variable is ignored and the `WARM_IP_TARGET` behavior is used instead.\n\n#### `WARM_IP_TARGET`\n\nType: Integer\n\nDefault: None\n\nSpecifies the number of free IP addresses that the `ipamd` daemon should attempt to keep available for pod assignment on the node. Setting this to a non-positive value is the same as setting this to 0 or not setting the variable.\nWith `ENABLE_PREFIX_DELEGATION` set to `true` then `ipamd` daemon will check if the existing (/28) prefixes are enough to maintain the\n`WARM_IP_TARGET` if it is not sufficient then more prefixes will be attached.\n\nFor example,\n\n1. if `WARM_IP_TARGET` is set to 5, then `ipamd` attempts to keep 5 free IP addresses available at all times. If the\nelastic network interfaces on the node are unable to provide these free addresses, `ipamd` attempts to allocate more interfaces\nuntil `WARM_IP_TARGET` free IP addresses are available.\n2. `ENABLE_PREFIX_DELEGATION` set to `true` and `WARM_IP_TARGET` is 16. Initially, 1 (/28) prefix is sufficient but once a single pod is assigned IP then\nremaining free IPs are 15 hence IPAMD will allocate 1 more prefix to achieve 16 `WARM_IP_TARGET`\n\n**NOTE!** Avoid this setting for large clusters, or if the cluster has high pod churn. Setting it will cause additional calls to the\nEC2 API and that might cause throttling of the requests. It is strongly suggested to set `MINIMUM_IP_TARGET` when using `WARM_IP_TARGET`.\n\nIf both `WARM_IP_TARGET` and `MINIMUM_IP_TARGET` are set, `ipamd` will attempt to meet both constraints.\nThis environment variable overrides `WARM_ENI_TARGET` behavior. For a detailed explanation, see\n[`WARM_ENI_TARGET`, `WARM_IP_TARGET` and `MINIMUM_IP_TARGET`](https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/eni-and-ip-target.md).\n\nIf `ENABLE_PREFIX_DELEGATION` set to `true` and `WARM_IP_TARGET` overrides `WARM_PREFIX_TARGET` behavior. For a detailed explanation, see\n[`WARM_PREFIX_TARGET`, `WARM_IP_TARGET` and `MINIMUM_IP_TARGET`](https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/prefix-and-ip-target.md).\n\n#### `MINIMUM_IP_TARGET` (v1.6.0+)\n\nType: Integer\n\nDefault: None\n\nSpecifies the number of total IP addresses that the `ipamd` daemon should attempt to allocate for pod assignment on the node.\n`MINIMUM_IP_TARGET` behaves identically to `WARM_IP_TARGET` except that instead of setting a target number of free IP\naddresses to keep available at all times, it sets a target number for a floor on how many total IP addresses are allocated. Setting to a\nnon-positive value is same as setting this to 0 or not setting the variable.\n\n`MINIMUM_IP_TARGET` is for pre-scaling, `WARM_IP_TARGET` is for dynamic scaling. For example, suppose a cluster has an\nexpected pod density of approximately 30 pods per node. If `WARM_IP_TARGET` is set to 30 to ensure there are enough IPs\nallocated up front by the CNI, then 30 pods are deployed to the node, the CNI will allocate an additional 30 IPs, for\na total of 60, accelerating IP exhaustion in the relevant subnets. If instead `MINIMUM_IP_TARGET` is set to 30 and\n`WARM_IP_TARGET` to 2, after the 30 pods are deployed the CNI would allocate an additional 2 IPs. This still provides\nelasticity, but uses roughly half as many IPs as using WARM_IP_TARGET alone (32 IPs vs 60 IPs).\n\nThis also improves the reliability of the EKS cluster by reducing the number of calls necessary to allocate or deallocate\nprivate IPs, which may be throttled, especially at scaling-related times.\n\n**NOTE!** \n1. If `MINIMUM_IP_TARGET` is set, `WARM_ENI_TARGET` will be ignored. Please utilize `WARM_IP_TARGET` instead.\n2. If `MINIMUM_IP_TARGET` is set and `WARM_IP_TARGET` is not set, `WARM_IP_TARGET` is assumed to be 0, which leads to the number of IPs attached to the node will be the value of `MINIMUM_IP_TARGET`. This configuration will prevent future ENIs/IPs from being allocated. It is strongly recommended that `WARM_IP_TARGET` should be set greater than 0 when `MINIMUM_IP_TARGET` is set.\n\n#### `MAX_ENI`\n\nType: Integer\n\nDefault: None\n\nSpecifies the maximum number of ENIs that will be attached to the node. When `MAX_ENI` is unset or 0 (or lower), the setting\nis not used, and the maximum number of ENIs is always equal to the maximum number for the instance type in question. Even when\n`MAX_ENI` is a positive number, it is limited by the maximum number for the instance type.\n\n#### `AWS_VPC_K8S_CNI_LOGLEVEL`\n\nType: String\n\nDefault: `DEBUG`\n\nValid Values: `DEBUG`, `INFO`, `WARN`, `ERROR`, `FATAL`. (Not case sensitive)\n\nSpecifies the log level for `ipamd` and `cni-metric-helper`.\n\n#### `AWS_VPC_K8S_CNI_LOG_FILE`\n\nType: String\n\nDefault: `/host/var/log/aws-routed-eni/ipamd.log`\n\nValid Values: `stdout`, `stderr`, or a file path\n\nSpecifies where to write the logging output of `ipamd`: `stdout`, `stderr`, or a file path other than the default (`/var/log/aws-routed-eni/ipamd.log`).\n\nNote: `/host/var/log/...` is the container file-system path, which maps to `/var/log/...` on the node.\n\nNote: The IPAMD process runs within the `aws-node` pod, so writing to `stdout` or `stderr` will write to `aws-node` pod logs.\n\n#### `AWS_VPC_K8S_PLUGIN_LOG_FILE`\n\nType: String\n\nDefault: `/var/log/aws-routed-eni/plugin.log`\n\nValid Values: `stderr` or a file path. Note that setting to the empty string is an alias for `stderr`, and this comes from upstream kubernetes best practices.\n\nSpecifies where to write the logging output for `aws-cni` plugin: `stderr` or a file path other than the default (`/var/log/aws-routed-eni/plugin.log`).\n\nNote: `stdout` cannot be supported for plugin log. Please refer to [#1248](https://github.com/aws/amazon-vpc-cni-k8s/issues/1248) for more details.\n\nNote: In EKS 1.24+, the CNI plugin is exec'ed by the container runtime, so `stderr` is for the container-runtime process, NOT the `aws-node` pod. In older versions, the CNI plugin was exec'ed by kubelet, so `stderr` is for the kubelet process.\n\nNote: If chaining an external plugin (i.e. Cilium) that does not provide a `pluginLogFile` in its config file, the CNI plugin will by default write to `os.Stderr`.\n\n#### `AWS_VPC_K8S_PLUGIN_LOG_LEVEL`\n\nType: String\n\nDefault: `DEBUG`\n\nValid Values: `DEBUG`, `INFO`, `WARN`, `ERROR`, `FATAL`. (Not case sensitive)\n\nSpecifies the loglevel for `aws-cni` plugin.\n\n#### `INTROSPECTION_BIND_ADDRESS`\n\nType: String\n\nDefault: `127.0.0.1:61679`\n\nSpecifies the bind address for the introspection endpoint.\n\nA Unix Domain Socket can be specified with the `unix:` prefix before the socket path.\n\n#### `DISABLE_INTROSPECTION`\n\nType: Boolean as a String\n\nDefault: `false`\n\nSpecifies whether introspection endpoints are disabled on a worker node. Setting this to `true` will reduce the debugging\ninformation we can get from the node when running the `aws-cni-support.sh` script.\n\n#### `DISABLE_METRICS`\n\nType: Boolean as a String\n\nDefault: `false`\n\nSpecifies whether the prometheus metrics endpoint is disabled or not for ipamd. By default metrics are published\non `:61678/metrics`.\n\n#### `AWS_VPC_K8S_CNI_VETHPREFIX`\n\nType: String\n\nDefault: `eni`\n\nSpecifies the veth prefix used to generate the host-side veth device name for the CNI. The prefix can be at most 4 characters long. The prefixes `eth`, `vlan`, and `lo` are reserved by the CNI plugin and cannot be specified. We recommend using prefix name not shared by any other network interfaces on the worker node instance.\n\n#### `ADDITIONAL_ENI_TAGS` (v1.6.0+)\n\nType: String\n\nDefault: `{}`\n\nExample values: `{\"tag_key\": \"tag_val\"}`\n\nMetadata applied to ENI helps you categorize and organize your resources for billing or other purposes. Each tag consists of a\ncustom-defined key and an optional value. Tag keys can have a maximum character length of 128 characters. Tag values can have\na maximum length of 256 characters. These tags will be added to all ENIs on the host.\n\nImportant: Custom tags should not contain `k8s.amazonaws.com` prefix as it is reserved. If the tag has `k8s.amazonaws.com`\nstring, tag addition will be ignored.\n\n#### `AWS_VPC_K8S_CNI_CONFIGURE_RPFILTER` (deprecated v1.12.1+)\n\nType: Boolean as a String\n\nDefault: `true`\n\nSpecifies whether ipamd should configure rp filter for primary interface. Setting this to `false` will require rp filter to be configured through init container.\n\n**NOTE!** `AWS_VPC_K8S_CNI_CONFIGURE_RPFILTER` has been deprecated, so setting this environment variable results in a no-op. The init container unconditionally configures the rp filter for the primary interface.\n\n#### `CLUSTER_NAME`\n\nType: String\n\nDefault: `\"\"`\n\nSpecifies the cluster name to tag allocated ENIs with. See the \"Cluster Name tag\" section below.\n\n#### `CLUSTER_ENDPOINT` (v1.12.1+)\n\nType: String\n\nDefault: `\"\"`\n\nSpecifies the cluster endpoint to use for connecting to the api-server without relying on kube-proxy.\nThis is an optional configuration parameter that can improve the initialization time of the AWS VPC CNI.\n\n**NOTE!** When setting CLUSTER_ENDPOINT, it is *STRONGLY RECOMMENDED* that you enable private endpoint access for your API server, otherwise VPC CNI requests can traverse the public NAT gateway and may result in additional charges.\n\n#### `ENABLE_POD_ENI` (v1.7.0+)\n\nType: Boolean as a String\n\nDefault: `false`\n\nTo enable security groups for pods you need to have at least an EKS 1.17 eks.3 cluster.\n\nSetting `ENABLE_POD_ENI` to `true` will allow IPAMD to add the `vpc.amazonaws.com/has-trunk-attached` label to the node if the instance has the capacity to attach an additional ENI.\n\nThe label notifies [vpc-resource-controller](https://github.com/aws/amazon-vpc-resource-controller-k8s) to attach a Trunk ENI to the instance. The label value is initially set to `false` and is marked to `true` by IPAMD when vpc-resource-controller attaches a Trunk ENI to the instance. However, there might be cases where the label value will remain `false` if the instance doesn't support ENI Trunking.\n\nOnce enabled the VPC resource controller will then advertise branch network interfaces as extended resources on these nodes in your cluster. Branch interface capacity is additive to existing instance type limits for secondary IP addresses and prefixes. For example, a c5.4xlarge can continue to have up to 234 secondary IP addresses or 234 /28 prefixes assigned to standard network interfaces and up to 54 branch network interfaces. Each branch network interface only receives a single primary IP address and this IP address will be allocated to pods with a security group(branch ENI pods).\n\nAny of the WARM targets do not impact the scale of the branch ENI pods so you will have to set the WARM_{ENI/IP/PREFIX}_TARGET based on the number of non-branch ENI pods. If you are having the cluster mostly using pods with a security group consider setting WARM_IP_TARGET to a very low value instead of default WARM_ENI_TARGET or WARM_PREFIX_TARGET to reduce wastage of IPs/ENIs.\n\n**NOTE!** Toggling `ENABLE_POD_ENI` from `true` to `false` will not detach the Trunk ENI from an instance. To delete/detach the Trunk ENI from an instance, you need to recycle the instance.\n\n#### `POD_SECURITY_GROUP_ENFORCING_MODE` (v1.11.0+)\n\nType: String\n\nDefault: `strict`\n\nValid Values: `strict`, `standard`\n\nOnce `ENABLE_POD_ENI` is set to `true`, this value controls how the traffic of pods with the security group behaves.\n\n* `strict` mode: all inbound/outbound traffic from pod with security group will be enforced by security group rules. This is the **default** mode if POD_SECURITY_GROUP_ENFORCING_MODE is not set.\n\n* `standard` mode: the traffic of pod with security group behaves same as pods without a security group, except that each pod occupies a dedicated branch ENI.\n  * inbound traffic to pod with security group from another host will be enforced by security group rules.\n  * outbound traffic from pod with security group to another host in the same VPC will be enforced by security group rules.\n  * inbound/outbound traffic from another pod on the same host or another service on the same host(such as kubelet/nodeLocalDNS) won't be enforced by security group rules.\n  * outbound traffic from pod with security group to IP address outside VPC\n    * if externalSNAT enabled, traffic won't be SNATed, thus will be enforced by security group rules.\n    * if externalSNAT disabled, traffic will be SNATed via eth0, thus will only be enforced by the security group associated with eth0.\n\n**NOTE!**: To make new behavior be in effect after switching the mode, existing pods with security group must be recycled. Alternatively, you can restart the nodes as well.\n\n#### `DISABLE_TCP_EARLY_DEMUX` (v1.7.3+)\n\nType: Boolean as a String\n\nDefault: `false`\n\nIf `ENABLE_POD_ENI` is set to `true`, for the kubelet to connect via TCP (for liveness or readiness probes)\nto pods that are using per pod security groups, `DISABLE_TCP_EARLY_DEMUX` should be set to `true` for `amazon-k8s-cni-init`\nthe container under `initcontainers`. This will increase the local TCP connection latency slightly.\nDetails on why this is needed can be found in this [#1212 comment](https://github.com/aws/amazon-vpc-cni-k8s/pull/1212#issuecomment-693540666).\nTo use this setting, a Linux kernel version of at least 4.6 is needed on the worker node.\n\nYou can use the below command to enable `DISABLE_TCP_EARLY_DEMUX` to `true` -\n\n```\nkubectl patch daemonset aws-node -n kube-system -p '{\"spec\": {\"template\": {\"spec\": {\"initContainers\": [{\"env\":[{\"name\":\"DISABLE_TCP_EARLY_DEMUX\",\"value\":\"true\"}],\"name\":\"aws-vpc-cni-init\"}]}}}}'\n```\n\n#### `ENABLE_PREFIX_DELEGATION` (v1.9.0+)\n\nType: Boolean as a String\n\nDefault: `false`\n\nTo enable prefix delegation on nitro instances. Setting `ENABLE_PREFIX_DELEGATION` to `true` will start allocating a prefix (/28 for IPv4\nand /80 for IPv6) instead of a secondary IP in the ENIs subnet. The total number of prefixes and private IP addresses will be less than the\nlimit on private IPs allowed by your instance. Setting or resetting of `ENABLE_PREFIX_DELEGATION` while pods are running or if ENIs are attached is supported and the new pods allocated will get IPs based on the mode of IPAMD but the max pods of kubelet should be updated which would need either kubelet restart or node recycle.\n\nSetting ENABLE_PREFIX_DELEGATION to true will not increase the density of branch ENI pods. The limit on the number of [branch network interfaces per instance type will remain the same.](https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html#supported-instance-types) Each branch network will be allocated a primary IP and this IP will be allocated for the branch ENI pods.\n\nPlease refer to [VPC CNI Feature Matrix](https://github.com/aws/amazon-vpc-cni-k8s#vpc-cni-feature-matrix) section below for additional information around using Prefix delegation with Custom Networking and Security Groups Per Pod features.\n\n**Note:** `ENABLE_PREFIX_DELEGATION` needs to be set to `true` when VPC CNI is configured to operate in IPv6 mode (supported in v1.10.0+). Prefix Delegation in IPv4 and IPv6 modes is supported on Nitro based Bare Metal instances as well from v1.11+. If you're using Prefix Delegation feature on Bare Metal instances, downgrading to an earlier version of VPC CNI from v1.11+ will be disruptive and not supported.\n\n#### `WARM_PREFIX_TARGET` (v1.9.0+)\n\nType: Integer\n\nDefault: None\n\nSpecifies the number of free IPv4(/28) prefixes that the `ipamd` daemon should attempt to keep available for pod assignment on the node. Setting to a non-positive value is same as setting this to 0 or not setting the variable.\nThis environment variable works when `ENABLE_PREFIX_DELEGATION` is set to `true` and is overridden when `WARM_IP_TARGET` and `MINIMUM_IP_TARGET` are configured.\n\n#### `DISABLE_NETWORK_RESOURCE_PROVISIONING` (v1.9.1+)\n\nType: Boolean as a String\n\nDefault: `false`\n\nSetting `DISABLE_NETWORK_RESOURCE_PROVISIONING` to `true` will make IPAMD depend only on IMDS to get attached ENIs and IPs/prefixes.\n\n#### `ENABLE_BANDWIDTH_PLUGIN` (v1.10.0+)\n\nType: Boolean as a String\n\nDefault: `false`\n\nSetting `ENABLE_BANDWIDTH_PLUGIN` to `true` will update `10-aws.conflist` to include upstream [bandwidth plugin](https://www.cni.dev/plugins/current/meta/bandwidth/) as a chained plugin.\n\nNOTE: Kubernetes Network Policy is supported in Amazon VPC CNI starting with version v1.14.0. Note that bandwidth plugin is not compatible with Amazon VPC CNI based Network policy. Network Policy agent uses TC (traffic classifier) system to enforce configured network policies for the pods. The policy enforcement will fail if bandwidth plugin is enabled due to conflict between TC configuration of bandwidth plugin and Network policy agent. We're exploring options to support bandwidth plugin along with Network policy feature and the issue is tracked [here](https://github.com/aws/aws-network-policy-agent/issues/68)\n\n#### `ANNOTATE_POD_IP` (v1.9.3+)\n\nType: Boolean as a String\n\nDefault: `false`\n\nSetting `ANNOTATE_POD_IP` to `true` will allow IPAMD to add an annotation `vpc.amazonaws.com/pod-ips` to the pod with pod IP.\n\nThere is a known [issue](https://github.com/kubernetes/kubernetes/issues/39113) with kubelet taking time to update `Pod.Status.PodIP` leading to calico being blocked on programming the policy. Setting `ANNOTATE_POD_IP` to `true` will enable AWS VPC CNI plugin to add Pod IP as an annotation to the pod spec to address this race condition.\n\nTo annotate the pod with pod IP, you will have to add `patch` permission for pods resource in aws-node clusterrole. You can use the below command -\n\n```\ncat << EOF > append.yaml\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  verbs:\n  - patch\nEOF\n```\n\n```\nkubectl apply -f <(cat <(kubectl get clusterrole aws-node -o yaml) append.yaml)\n```\n\nNOTE: Adding `patch` permissions to the `aws-node` Daemonset increases the security scope for the plugin, so add this permission only after performing a proper security assessment of the tradeoffs.\n\n#### `ENABLE_IPv4` (v1.10.0+)\n\nType: Boolean as a String\n\nDefault: `true`\n\nVPC CNI can operate in either IPv4 or IPv6 mode. Setting `ENABLE_IPv4` to `true` will configure it in IPv4 mode (default mode).\n\n**Note:** Dual-stack mode isn't yet supported. So, enabling both IPv4 and IPv6 will be treated as an invalid configuration.\n\n#### `ENABLE_IPv6` (v1.10.0+)\n\nType: Boolean as a String\n\nDefault: `false`\n\nVPC CNI can operate in either IPv4 or IPv6 mode. Setting `ENABLE_IPv6` to `true` (both under `aws-node` and `aws-vpc-cni-init` containers in the manifest)\nwill configure it in IPv6 mode. IPv6 is only supported in Prefix Delegation mode, so `ENABLE_PREFIX_DELEGATION` needs to be set to `true` if VPC CNI is\nconfigured to operate in IPv6 mode. Prefix delegation is only supported on nitro instances.\n\n**Note:** Please make sure that the required IPv6 IAM policy is applied (Refer to [IAM Policy](https://github.com/aws/amazon-vpc-cni-k8s#iam-policy) section above). Dual stack mode isn't yet supported. So, enabling both IPv4 and IPv6 will be treated as invalid configuration. Please refer to the [VPC CNI Feature Matrix](https://github.com/aws/amazon-vpc-cni-k8s#vpc-cni-feature-matrix) section below for additional information.\n\n#### `ENABLE_NFTABLES` (introduced in v1.12.1, deprecated in v1.13.2+)\n\nType: Boolean as a String\n\nDefault: `false`\n\nVPC CNI uses `iptables-legacy` by default. Setting `ENABLE_NFTABLES` to `true` will update VPC CNI to use `iptables-nft`.\n\n**Note:** VPC CNI image contains `iptables-legacy` and `iptables-nft`. Switching between them is done via `update-alternatives`. It is *strongly* recommended that the iptables mode matches that which is used by the base OS and `kube-proxy`.\nSwitching modes while pods are running or rules are installed will not trigger reconciliation. It is recommended that rules are manually updated or nodes are drained and cordoned before updating. If reloading node, ensure that previous rules are not set to be persisted.\n\n#### `AWS_EXTERNAL_SERVICE_CIDRS` (v1.12.6+)\n\nType: String\n\nDefault: empty\n\nSpecify a comma-separated list of IPv4 CIDRs that *must* be routed via main routing table. This is required for secondary ENIs to reach endpoints outside of VPC that are backed by a service.\nFor every item in the list, an `ip rule` will be created with a priority greater than the `ip rule` capturing egress traffic from the container. If an item is not a valid IPv4 CIDR, it will be skipped.\n\n#### `AWS_EC2_ENDPOINT` (v1.13.0+)\n\nType: String\n\nDefault: empty\n\nSpecify the EC2 endpoint to use. This is useful if you are using a custom endpoint for EC2. For example, if you are using a proxy for EC2, you can set this to the proxy endpoint. Any kind of URL or IP address is valid such as `https://localhost:8080` or `http://ec2.us-west-2.customaws.com`. If this is not set, the default EC2 endpoint will be used.\n\n#### `DISABLE_LEAKED_ENI_CLEANUP` (v1.13.0+)\n\nType: Boolean as a String\n\nDefault: `false`\n\nOn IPv4 clusters, IPAMD schedules an hourly background task per node that cleans up leaked ENIs. Setting this environment variable to `true` disables that job. The primary motivation to disable this task is to decrease the amount of EC2 API calls made from each node.\nNote that disabling this task should be considered carefully, as it requires users to manually cleanup ENIs leaked in their account. See [#1223](https://github.com/aws/amazon-vpc-cni-k8s/issues/1223) for a related discussion.\n\n#### `ENABLE_V6_EGRESS` (v1.13.0+)\n\nType: Boolean as a String\n\nDefault: `false`\n\nSpecifies whether PODs in an IPv4 cluster support IPv6 egress. If env is set to `true`, range `fd00::ac:00/118` is reserved for IPv6 egress.\n\nThis environment variable must be set for both the `aws-vpc-cni-init` and `aws-node` containers in order for this feature to work properly. This feature also requires that the node has an IPv6 address assigned to its primary ENI, as this address is used for SNAT to IPv6 endpoints outside of the cluster. If the configuration prerequisites are not met, the `egress-cni` plugin is not enabled and an error log is printed in the `aws-node` container.\n\nNote that enabling/disabling this feature only affects whether newly created pods have an IPv6 interface created. Therefore, it is recommended that you reboot existing nodes after enabling/disabling this feature.\n\n#### `ENABLE_V4_EGRESS` (v1.15.1+)\n\nType: Boolean as a String\n\nDefault: `true`\n\nSpecifies whether PODs in an IPv6 cluster support IPv4 egress. If env is set to `true`, range `169.254.172.0/22` is reserved for IPv4 egress. When enabled, traffic egressing an IPv6 pod destined to an IPv4 endpoint will be SNAT'ed via the node IPv4 address.\n\nNote that enabling/disabling this feature only affects whether newly created pods have an IPv4 interface created. Therefore, it is recommended that you reboot existing nodes after enabling/disabling this feature.\n\n#### `IP_COOLDOWN_PERIOD` (v1.15.0+)\n\nType: Integer as a String\n\nDefault: `30`\n\nSpecifies the number of seconds an IP address is in cooldown after pod deletion. The cooldown period gives network proxies, such as kube-proxy, time to update node iptables rules when the IP was registered as a valid endpoint, such as for a service. Modify this value with caution, as kube-proxy update time scales with the number of nodes and services.\n\n**Note:** 0 is a supported value, however it is highly discouraged.\n**Note:** Higher cooldown periods may lead to a higher number of EC2 API calls as IPs are in cooldown cache.\n\n#### `DISABLE_POD_V6` (v1.15.0+)\n\nType: Boolean as a String\n\nDefault: `false`\n\nWhen `DISABLE_POD_V6` is set, the [tuning plugin](https://www.cni.dev/plugins/current/meta/tuning/) is chained and configured to disable IPv6 networking in each newly created pod network namespace. Set this variable when you have an IPv4 cluster and containerized applications that cannot tolerate IPv6 being enabled.\nContainer runtimes such as `containerd` will enable IPv6 in newly created container network namespaces regardless of host settings.\n\nNote that if you set this while using Multus, you must ensure that any chained plugins do not depend on IPv6 networking. You must also ensure that chained plugins do not also modify these sysctls.\n\n### VPC CNI Feature Matrix\n\n\n| IP Mode | Secondary IP Mode | Prefix Delegation | Security Groups Per Pod | WARM & MIN IP/Prefix Targets | External SNAT | Network Policies |\n|---------|-------------------|-------------------|-------------------------|------------------------------|---------------|------------------|\n| `IPv4`  | Yes               | Yes               | Yes                     | Yes                          | Yes           | Yes              |\n| `IPv6`  | No                | Yes               | No                      | No                           | No            | Yes              |\n\n## ENI tags related to Allocation\n\nThis plugin interacts with the following tags on ENIs:\n\n* `cluster.k8s.amazonaws.com/name`\n* `node.k8s.amazonaws.com/instance_id`\n* `node.k8s.amazonaws.com/no_manage`\n\n#### Cluster Name tag\n\nThe tag `cluster.k8s.amazonaws.com/name` will be set to the cluster name of the\naws-node daemonset which created the ENI.\n\n#### Instance ID tag\n\nThe tag `node.k8s.amazonaws.com/instance_id` will be set to the instance ID of\nthe aws-node instance that allocated this ENI.\n\n#### No Manage tag\n\nThe tag `node.k8s.amazonaws.com/no_manage` is read by the aws-node daemonset to\ndetermine whether an ENI attached to the machine should not be configured or\nused for private IPs.\n\nThis tag is not set by the cni plugin itself, but rather may be set by a user\nto indicate that an ENI is intended for host networking pods, or for some other\nprocess unrelated to Kubernetes.\n\n*Note*: Attaching an ENI with the `no_manage` tag will result in an incorrect\nvalue for the Kubelet's `--max-pods` configuration option. Consider also\nupdating the `MAX_ENI` and `--max-pods` configuration options on this plugin\nand the kubelet respectively if you are making use of this tag.\n\n## Container Runtime\n\nFor VPC CNI >=v1.12.0, IPAMD have switched to use an on-disk file `/var/run/aws-node/ipam.json` to track IP allocations, thus became container runtime agnostic and no longer requires access to Container Runtime Interface(CRI) socket.\n\n* **Note**:\n  * Helm chart >=v1.2.0 is released with VPC CNI v1.12.0, thus no longer supports the `cri.hostPath.path`. If you need to install a VPC CNI <v1.12.0 with helm chart, a Helm chart version that <v1.2.0 should be used.\n\nFor VPC CNI <v1.12.0, IPAMD still depends on CRI to track IP allocations using pod sandboxes information upon its starting.\n\n* By default the dockershim CRI socket was mounted but can be customized to use other CRI:\n  * The mountPath should be changed to `/var/run/cri.sock` and hostPath should be pointed to CRI used by kubelet, such as `/var/run/containerd/containerd.sock` for containerd.\n  * With Helm chart <v1.2.0, the flag `--set cri.hostPath.path=/var/run/containerd/containerd.sock` can set above for you.\n* **Note**:\n  * When using a different container runtime instead of the default dockershim in VPC CNI, make sure kubelet is also configured to use the same CRI.\n  * If you want to enable containerd runtime with the support provided by Amazon AMI, please follow the instructions in our documentation, [Enable the containerd runtime bootstrap flag](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html#containerd-bootstrap)\n\n## Notes\n\n`L-IPAMD`(aws-node daemonSet) running on every worker node requires access to the Kubernetes API server. If it can **not** reach\nthe Kubernetes API server, ipamd will exit and CNI will not be able to get any IP address for Pods. Here is a way to confirm if\n`aws-node` has access to the Kubernetes API server.\n\n```\n# find out Kubernetes service IP, e.g. 10.0.0.1\nkubectl get svc Kubernetes\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.0.0.1   <none>        443/TCP   29d\n\n# ssh into worker node, check if worker node can reach API server\ntelnet 10.0.0.1 443\nTrying 10.0.0.1...\nConnected to 10.0.0.1.\nEscape character is '^]'.  <-------- Kubernetes API server is reachable\n```\n\n## Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues. Instead, please follow the\ninstructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n## Contributing\n\n[See CONTRIBUTING.md](./CONTRIBUTING.md)\n", "release_dates": ["2024-02-20T17:43:44Z", "2024-01-26T17:41:30Z", "2023-12-22T22:06:01Z", "2023-12-19T22:43:44Z", "2023-11-22T16:25:28Z", "2023-11-03T15:10:32Z", "2023-10-13T15:42:45Z", "2023-09-13T15:59:13Z", "2023-09-08T16:59:40Z", "2023-08-31T17:23:44Z", "2023-07-28T19:25:14Z", "2023-07-12T20:11:37Z", "2023-06-19T00:40:29Z", "2023-06-02T20:21:15Z", "2023-04-03T20:46:09Z", "2023-03-20T15:44:55Z", "2023-02-23T22:49:23Z", "2023-02-03T20:07:34Z", "2022-11-28T23:43:22Z", "2022-04-11T18:03:27Z", "2023-01-05T20:42:40Z", "2023-01-05T15:03:11Z", "2022-12-21T18:31:41Z", "2022-10-27T23:52:44Z", "2022-09-12T23:48:40Z", "2022-08-16T21:50:46Z", "2022-08-04T18:49:05Z", "2022-06-03T21:19:08Z", "2022-04-14T00:42:10Z", "2022-02-04T18:48:28Z"]}, {"name": "amazon-vpc-cni-plugins", "description": "VPC CNI plugins for Amazon ECS and EKS.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon VPC CNI Plugins\n\nVPC CNI plugins for Amazon ECS and Amazon EKS.\n\n## Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues. Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": []}, {"name": "amazon-vpc-resource-controller-k8s", "description": "Controller for managing Trunk & Branch Network Interfaces on EKS Cluster using Security Group For Pod feature and IPv4 Addresses for Windows Node.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# amazon-vpc-resource-controller-k8s\n\n![GitHub go.mod Go version](https://img.shields.io/github/go-mod/go-version/aws/amazon-vpc-resource-controller-k8s)\n[![Go Report Card](https://goreportcard.com/badge/github.com/aws/amazon-vpc-resource-controller-k8s)](https://goreportcard.com/report/github.com/aws/amazon-vpc-resource-controller-k8s)\n![GitHub](https://img.shields.io/github/license/aws/amazon-vpc-resource-controller-k8s?style=flat)\n\n## Usage\n\nController running on EKS Control Plane for managing Branch & Trunk Network Interface for [Kubernetes Pod](https://kubernetes.io/docs/concepts/workloads/pods/) using the [Security Group for Pod](https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html) feature and IPv4 Address Management(IPAM) of [Windows Nodes](https://docs.aws.amazon.com/eks/latest/userguide/windows-support.html).\n\nThe controller broadcasts its version to nodes. Describing any node will provide the version information in node `Events`. The mapping between the controller's version and the cluster's platform version is also available in release notes.\n\n## Security Group for Pods\n\nThe controller only manages the Trunk/Branch Network Interface for EKS Cluster using the Security Group for Pods feature. The Networking on the host is setup by [amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s) plugin.\n\nENI Trunking is a private feature even though the APIs are publicly accessible using AWS SDK. Hence, attempting to run the controller on your worker node for enabling [Security Group for Pod](https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html) for managing Trunk and Branch Network Interface will result in failure of the API calls.\n\nPlease follow the [guide](https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html) for enabling Security Group for Pods on your EKS Cluster. \n\nNote: The SecurityGroupPolicy CRD only supports up to 5 security groups per custom resource. If you need more than 5 security groups for a pod, please consider to use more than one custom resources. For example, you can have two custom resources to associate up to 10 security groups to a pod. Please be aware when you are doing so: \n\n1, you need to request increasing the limit since the default limit is 5 security groups per interface and there is a hard limit of 16 currently.\n\n2, currently Fargate only allows up to 5 security groups. If you are using Fargate, you can only use up to 5 security groups per pod.\n\n## Windows IPv4 Address Management\n\nThe controller manages the IPv4 Addresses for all the Windows Node in EKS Cluster and allocates IPv4 Address to Windows Pods. The Networking on the host is setup by [amazon-vpc-cni-plugins](https://github.com/aws/amazon-vpc-cni-plugins).\n\nThe controller supports the following modes for IPv4 address management on Windows-\n- **Secondary IPv4 address mode** &rarr; Secondary private IPv4 addresses are assigned to the primary instance ENI and the same are allocated to the Windows pods.\n  <br/><br/>\n  For more details about the high level workflow, please visit our documentation [here](docs/windows/secondary_ip_mode_workflow.md).\n\n\n- **Prefix delegation mode** &rarr; /28 IPv4 prefixes are assigned to the primary instance ENI and the IP addresses from the prefix are allocated to the Windows pods.\n  <br/><br/>\n  For more details about the configuration options with *prefix delegation*, please visit our documentation [here](docs/windows/prefix_delegation_config_options.md).\n  \n  For more details about the high level workflow, please visit our documentation [here](docs/windows/prefix_delegation_hld_workflow.md).\n\nPlease follow this [guide](https://docs.aws.amazon.com/eks/latest/userguide/windows-support.html) for enabling Windows Support on your EKS cluster.\n\n## Configuring the controller via amazon-vpc-cni configmap\n\nThe controller supports various configuration options for managing security groups for pods and Windows nodes which can be set via the EKS-managed configmap `amazon-vpc-cni`. For more details, refer to the security group for pods configuration options [here](docs/sgp/sgp_config_options.md) and Windows IPAM/PD related configuration options [here](docs/windows/prefix_delegation_config_options.md)\n\n## Troubleshooting\nFor troubleshooting issues related to Security group for pods or Windows IPv4 address management, please visit our troubleshooting guide [here](docs/troubleshooting.md).\n\n## License\n\nThis library is licensed under the Apache 2.0 License. \n\n## Contributing\n\nSee [CONTRIBUTING.md](./CONTRIBUTING.md)\n\nWe would appreciate your feedback and suggestions to improve the project and your experience with EKS and Kubernetes.\n", "release_dates": ["2024-02-15T23:50:28Z", "2024-01-12T00:52:22Z", "2024-01-05T01:39:02Z", "2024-01-05T01:49:39Z", "2023-12-28T20:46:42Z", "2023-12-28T20:46:56Z", "2023-11-08T00:49:38Z", "2023-09-19T00:19:10Z", "2023-08-22T00:46:30Z", "2023-07-24T23:42:06Z", "2023-07-07T00:32:55Z", "2023-06-07T19:30:01Z", "2023-04-10T19:23:06Z", "2022-11-03T17:07:10Z", "2022-06-02T21:23:30Z", "2021-11-10T01:37:01Z", "2021-11-01T21:58:06Z", "2021-06-04T13:40:48Z", "2021-03-12T00:32:30Z"]}, {"name": "aperf", "description": "A CLI tool to gather performance data and visualize using HTML graphs. Data from multiple collection runs can be viewed side-by-side, allowing for easy comparison of the same workload across different system configurations.", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# APerf\n## What is APerf?\nA CLI tool to gather many pieces of performance data in one go. APerf includes a recorder and a reporter sub tool. The recorder gathers performance metrics, stores them in a set of local files that can then be analyzed via the reporter sub tool.\n\n## Why does APerf exist?\nPerformance issues in applications are investigated by recreating them locally and collecting data/metrics using monitoring tools like sysstat, perf, sysctl, ebpf, etc... or by running these tools remotely. Installing and executing various performance monitoring tools is a manual process and prone to errors. Even with the [Graviton Performance Runbook](https://github.com/aws/aws-graviton-getting-started/blob/main/perfrunbook/README.md), understanding the output of these tools requires deep domain specific knowledge.\n\nThe aim of APerf is to enable anyone to collect performance data in their environment while providing tools to analyze and visualize application performance. APerf will hopefully enable faster troubleshooting by analyzing and highlighting deviations in performance between two application environments automatically. \n\n## What data does APerf collect?\nAPerf collects the following metadata:\n- System Info\n- When run on EC2 instances this includes basic EC2 metadata\n- Kernel Configuration (/boot/config)\n- Sysctl variable configuration settings\n\nAPerf collects the following performance data:\n- CPU Utilization, both per CPU and aggregate CPU utilization\n- Virtual Memory Utilization\n- Disk Utilization per Disk\n- Interrupt Data per Interrupt Line per CPU\n- CPU Performance Counters\n- Network stats\n- Meminfo\n- Profile data (if enabled with `--profile` and `perf` binary present)\n\n## Requirements\n* [Rust toolchain (v1.61.0+)](https://www.rust-lang.org/tools/install)\n* [Node.js (v16.16.0+)](https://nodejs.org/en/download/)\n\n## Installation\nDownload the binary from the [Releases](https://github.com/aws/APerf/releases) page.\n\n`aperf` only supports running on Linux.\n\n### Building from source\n1. Download the source code from the [Releases](https://github.com/aws/APerf/releases) page.\n2. Run the following commands:\n\n```\ncargo build\ncargo test\n```\n\n## Usage\n`aperf record` records performance data and stores them in a series of files. A report is then generated with `aperf report` and can be viewed in any system with a web browser.\n\n**KNOWN LIMITATION**\n\nThe default configuration of 10ms for `perf_event_mux_interval_ms` is known to cause serious performance overhead for systems with large core counts. We recommend setting this value to 100ms by doing the following:\n\n```\necho 100 | sudo tee /sys/devices/*/perf_event_mux_interval_ms\n```\n\n**aperf record**\n1. Download the `aperf` binary.\n2. Start `aperf record`:\n```\n./aperf record -r <RUN_NAME> -i <INTERVAL_NUMBER> -p <COLLECTION_PERIOD>\n```\n\n**aperf report**\n1. Download the `aperf` binary.\n2. Download the directory created by `aperf record`.\n3. Start `aperf report`:\n```\n./aperf report -r <COLLECTOR_DIRECTORY> -n <REPORT_NAME>\n```\n\nTo compare the results of two different performance runs, use the following command:\n```\n./aperf report -r <COLLECTOR_DIRECTORY_1> -r <COLLECTOR_DIRECTORY_2> -n <REPORT_NAME>\n```\n\n### Example\nTo see a step-by-step example, please see our example [here](./EXAMPLE.md)\n\n### Configuration\n\n`aperf record` has the following flags available for use:\n\n**Recorder Flags:**\n\n`-V, --version` version of APerf\n\n`-i, --interval` interval collection rate (default 1)\n\n`-p, --period` period (how long you want the data collection to run, default is 10s)\n\n`-r, --run-name` run name (name of the run for organization purposes, creates directory of the same name, default of aperf_[timestamp])\n\n`-v, --verbose` verbose messages\n\n`-vv, --verbose --verbose` more verbose messages\n\n`--profile` gather profiling data using the 'perf' binary\n\n\n`./aperf report -h`\n\n**Reporter Flags:**\n\n`-V, --version` version of APerf visualizer\n\n`-r, --run` run data to be visualized. Can be a directory or a tarball.\n\n`-n, --name` report name (name of the report for origanization purposes, creates directory of the same name, default of aperf_report_<run>\n\n`-v, --verbose` verbose messages\n\n`-vv, --verbose --verbose` more verbose messages\n\n## APerf Issues?\nBelow are some prerequisites for profiling with APerf:\n1. Select the [appropriate instance size](https://github.com/aws/aws-graviton-getting-started/blob/main/perfrunbook/debug_hw_perf.md) if you need PMU stats.\n2. For collecting PMU counter metrics w/o `root` or `sudo` permissions, set the `perf_event_paranoid` to `0`.\n3. To collect PMU counter metrics, APerf needs to open up to 50 file descriptors per vCPU. So, increase `ulimit` settings accordingly.\n4. APerf needs access to `/proc/kallsyms`, so we need to relax `kptr_restrict` by setting it to `0` (on Ubuntu OS).\n5. To enable function-level profiling, install the `perf` binary on your instances.\n6. Download to the instance the right [APerf binary](https://github.com/aws/aperf/releases), based on the instance type (x86/Intel/AMD or aarch64/Graviton).\n\n## Logging\n* `env_logger` is used to log information about the tool run to stdout.\n* To see it, use `./aperf <command> -v`.\n* To see more detail, use `./aperf <command> -vv`.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License. See [LICENSE](LICENSE) for more information.\n\n", "release_dates": ["2023-10-12T22:34:58Z", "2023-08-30T16:49:10Z", "2023-08-14T21:54:25Z", "2023-05-15T20:28:30Z", "2023-03-17T20:27:18Z", "2023-03-07T19:12:39Z", "2023-02-17T22:43:02Z", "2023-02-15T17:29:11Z", "2022-12-22T21:56:18Z", "2022-10-18T03:05:50Z"]}, {"name": "apprunner-roadmap", "description": "This is the public roadmap for AWS App Runner.", "language": null, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## AWS App Runner Roadmap\n\nThis is a public roadmap for AWS App Runner. We learnt from customers that knowing about our upcoming features and priorities helps them plan. This repository contains information about what we are working on and allows customers to give direct feedback.\n\n\n\n\n## License Summary\n\nThe documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.\n\n", "release_dates": []}, {"name": "audit-plugin-for-mysql", "description": "Audit Plugin for MySQL Server", "language": "C++", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Audit Plugin for MySQL Server\n\nThe Audit Plugin for MySQL Server is used by [Amazon RDS for\nMySQL](https://aws.amazon.com/rds/mysql/) to enable logging of server activity,\ntypically for security and compliance purposes.\n\nThe plugin source code is published openly on Github as a service to the\ncommunity of MySQL and MariaDB users. Amazon Web Services does not offer any\nwarranties or support for the plugin. The intended audience is software\ndevelopers with C++ skills who are able to independently compile and use the\nplugin.\n\nThis plugin is not official and not associated with\n[MySQL](https://www.mysql.com/). MySQL is a registered trademark of\n[Oracle](https://www.oracle.com/legal/trademarks.html) and/or its affiliates.\n\n## Based on the MariaDB Audit Plugin\n\nThis plugin is based on the [MariaDB Audit\nPlugin](https://github.com/MariaDB/server/tree/10.8/plugin/server_audit).\n\nIn MySQL 5.6, the plugin API was compatible with the MariaDB Audit Plugin.\nIn MySQL 5.7 and 8.0, the plugin APIs have however diverged significantly.\nSo, Amazon RDS for MariaDB created a [modified version that is compatible\nwith MySQL 5.7 and 8.0](https://aws.amazon.com/about-aws/whats-new/2021/06/amazon-rds-supports-mariadb-audit-plugin-for-mysql-version-8-0/).\n\nThis version maintains the functionality of the original audit plugin where\npossible, but adapts the plugin to use the MySQL 8.0 plugin API.\n\nTo view the MySQL 5.7 compatible version, see the branch `mysql-5.7` in this\nrepository.\n\n## Compatibility\n\nThis plugin has been tested and verified on *Amazon Linux OS*\nhttps://aws.amazon.com/amazon-linux-2. It has not been tested on other operating\nsystems and does not support Windows OS.\n\n## Compilation\n\nCopy the plugin source code on top of the sources of MySQL using the following\ncommand so it compiles as part of the MySQL build:\n\n```\nrsync -av plugin/ <mysqlSourceFolder>/plugin/\n```\n\nmysql-test is optional unless you want to run MTR tests:\n```\nrsync -av mysql-test/ <mysqlSourceFolder>/mysql-test/\n```\n\n## Installation\n\n1. Place the file `server_audit.so` in the plugin directory. You can run `SHOW VARIABLES LIKE 'plugin_dir';` to find the correct path.\n2. Activate the plugin by adding `plugin_load_add = server_audit` to `my.cnf` or by running `INSTALL PLUGIN server_audit SONAME \u2018server_audit.so\u2019;` once.\n\n## Usage\n\nFor details on usage, refer to the MariaDB Server documentation:\n\n* [system variables](https://mariadb.com/kb/en/mariadb-audit-plugin-options-and-system-variables/)\n* [status variables](https://mariadb.com/kb/en/mariadb-audit-plugin-status-variables/)\n* [logging settings](https://mariadb.com/kb/en/mariadb-audit-plugin-log-settings/)\n\n## Contributing\n\nThere are no plans to add any major features to this plugin. If you have modifications to improve the auditing in MySQL/MariaDB, [we recommend contributing to the MariaDB Server project](https://mariadb.org/contribute/), where the whole server and the original audit plugin is developed in a large and active open source community.\n\nThis Audit Plugin for MySQL does accept contributions, but the goal of the project is mainly to be stable and bug free. Notes for developers can be found in [DEVELOP.md](DEVELOP.md).\n", "release_dates": []}, {"name": "autopkg-recipes", "description": "Official recipes for AWS products for use by the Autopkg Community", "language": "Python", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "\n## ARCHIVED\n\nThis repo is set to be deleted. Go [here](https://github.com/autopkg/aws-recipes) for these recipes!\n\n# AWS AutoPkg Recipes\n\nOfficial recipes for AWS products for the AutoPkg Community\n\nTo add this repository to your AutoPkg setup, run:\n\n    autopkg repo-add autopkg/aws-recipes\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.\n", "release_dates": []}, {"name": "awesome-redshift", "description": null, "language": null, "license": null, "readme": "# Awesome Redshift [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\nA curated list of awesome Amazon Redshift libraries, utilities, and resources.\n\n- [Awesome Redshift](#awesome-redshift)\n    - [General Workshops](#general-workshops)\n    - [Key Features](#key-features)\n        - [Amazon Redshift Serverless](#amazon-redshift-serverless)\n        - [Data Sharing](#data-sharing)\n        - [Data APIs](#data-apis)\n        - [Federated Queries](#federated-queries)\n        - [Streaming Ingestion](#streaming-ingestion)\n        - [Redshift Spectrum](#redshift-spectrum)\n        - [User Defined Functions (UDFs)](#user-defined-functions-udfs)\n        - [Machine Learning (ML)](#machine-learning-ml)\n    - [Performance Tuning](#performance-tuning)\n    - [Connectors and Drivers](#connectors-and-drivers)\n    - [Operations](#operations)\n    - [Integrations](#integrations)\n        - [Pandas](#pandas)\n        - [dbt](#dbt)\n        - [Grafana](#grafana)\n        - [Apache Airflow](#apache-airflow)\n        - [SQLAlchemy](#sqlalchemy)\n        - [Querybook](#querybook)\n        - [AWS Glue](#aws-glue)\n        - [Amazon Location Service](#amazon-location-service)\n    - [Security and Authentication](#security-and-authentication)\n        - [General Security](#general-security)\n        - [Single Sign On (SSO)](#single-sign-on-sso)\n        - [Role Based Access Control (RBAC)](#role-based-access-control-rbac)\n        - [Row Level Security (RLS)](#row-level-security-rls)\n        - [Encryption](#encryption)\n    - [Cost Optimization](#cost-optimization)\n    - [CI/CD](#cicd)\n    - [Redshift Internals](#redshift-internals)\n- [General Resources](#general-resources)\n- [Contributing](#contributing)\n\n---\n\n## General Workshops\n\n*Hands-on workshops to learn Redshift.*\n\n* [Amazon Redshift Deep Dive Workshop](https://catalog.us-east-1.prod.workshops.aws/workshops/380e0b8a-5d4c-46e3-95a8-82d68cf5789a/en-US) - A hands-on workshop covering topics such as: Data API, Spectrum,  Redshift ML, Lambda UDF, Query federation, SageMaker, Apache Hudi, QuickSight, PowerBI, Oracle/SQL Server migrations.\n* [Redshift Immersion Labs Workshop](https://catalog.us-east-1.prod.workshops.aws/workshops/9f29cdba-66c0-445e-8cbb-28a092cb5ba7/en-US) - A hands-on workshop covering topics such as: ELT, Materialized Views, Data Sharing, and Redshift ML.\n\n## Key Features\n\n### Amazon Redshift Serverless\n*[Amazon Redshift Serverless](#https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-whatis.html) resources*\n* [Amazon Redshift Serverless RSQL ETL Framework](https://github.com/aws-samples/amazon-redshift-serverless-rsql-etl-framework) - A Serverless ETL framework.\n* [Self-service analytics with Amazon Redshift Serverless](https://www.youtube.com/watch?v=zGTxxOk4cBk) - A video session on getting started and best practices with Redshift Serverelss. \n\n### Data Sharing\n\n*[Data Sharing](https://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html) for sharing data between Redshift clusters*\n\n* [Seamless Data Sharing Using Amazon Redshift](https://catalog.us-east-1.prod.workshops.aws/workshops/4b2fb166-b467-461b-bd30-782dd2a2265c/en-US) - A hands-on workshop to share live data across Amazon Redshift clusters.\n* [Optimize Data Pattern using Data Sharing](https://www.wellarchitectedlabs.com/sustainability/300_labs/300_optimize_data_pattern_using_redshift_data_sharing/) - A hands-on workshop using data sharing to reduce the provisioned storage required to support your workload.\n\n### Data APIs\n\n*Resources related to [Data APIs](https://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html) for accessing Redshift from web services\u2013based applications*\n\n* [Getting Started with Redshift Data API](https://github.com/aws-samples/getting-started-with-amazon-redshift-data-api) - A sample project to access Redshift Data API from AWS Lambda.\n\n### Federated Queries \n\n*Resources related to [Federated Queries](https://docs.aws.amazon.com/redshift/latest/dg/federated-overview.html) querying live data from external databases*\n\n* [Best Practices for Amazon Redshift Federated Queries](https://aws.amazon.com/blogs/big-data/amazon-redshift-federated-query-best-practices-and-performance-considerations/) - A blog post listing  best practices to apply when using Redshift Federated Queries. \n\n### Streaming Ingestion\n\n*Resources related to [Streaming Ingestion](https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html) querying stream data from Amazon Kinesis Data Streams and Amazon Managed Streaming for Apache Kafka*\n\n* [Real-time Analytics with Amazon Redshift Streaming Ingestion](https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/) - A blog post describing how to query stream data in the same account.\n* [Near Real-time Fraud Detection using Amazon Redshift Streaming Ingestion](https://aws.amazon.com/blogs/big-data/near-real-time-fraud-detection-using-amazon-redshift-streaming-ingestion-with-amazon-kinesis-data-streams-and-amazon-redshift-ml/) - A blog post describing how to use Amazon Redshift Streaming Ingestion, Amazon Kinesis Data Streams, and Amazon Redshift ML to detect fraud near real-time.\n* [Cross-Account Streaming Ingestion for Amazon Redshift](https://aws.amazon.com/blogs/big-data/cross-account-streaming-ingestion-for-amazon-redshift/) - A blog post describing how to query stream data across accounts.\n* [Amazon Redshift Streaming Workshop](https://github.com/aws-samples/amazon-redshift-streaming-workshop) - A hands-on workshop and sample library to build a near-realtime logistics dashboard using Amazon Redshift and Amazon Managed Grafana.\n\n### Redshift Spectrum\n\n*Resources related to [Redshift Spectrum](https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html) for querying S3 data*\n\n* [Redshift Spectrum Row and Cell Level Security](https://aws.amazon.com/blogs/big-data/use-amazon-redshift-spectrum-with-row-level-and-cell-level-security-policies-defined-in-aws-lake-formation/) - A blog post describing how to use row and cell level security defined in AWS Lake Formation.\n\n### User Defined Functions (UDFs)\n\n*Collections of [User defined functions (UDFs)](https://docs.aws.amazon.com/redshift/latest/dg/user-defined-functions.html)*\n\n* [UDFs Collection](https://github.com/aws-samples/amazon-redshift-udfs) - A collection of useful UDFs, such as bitwise ops, url parsing, masking, kms encryption, dynamodb lookups, and converting json to upper case.\n* [Text UDFs](https://github.com/aws-samples/aws-redshift-udfs-textanalytics) - UDFs to analyze text, such as translating, detecting language, detecting sentiment, detecting and redacting entities, detecting and redacting PII.\n\n### Machine Learning (ML)\n*Resources related to [Amazon Redshift ML](https://aws.amazon.com/redshift/features/redshift-ml/)*\n\n* [Create and train ML Models using Amazon Redshift ML](https://catalog.us-east-1.prod.workshops.aws/workshops/4efa7f96-66a8-4b39-b7ea-c34595b2352b/en-US) - A hands-on workshop using Redshift ML to predict customer churn.\n* [Streaming Ingestion and ML Predictions with Amazon Redshift](https://catalog.us-east-1.prod.workshops.aws/workshops/03a761e3-8c64-42b6-8cc4-2123ffd1ec24/en-US) - A hands-on workshop using Streaming Ingestion and Redshift ML to detect fraud near real-time.\n\n## Performance Tuning\n\n*Tools and tips to measure and tune Redshift's performance.*\n\n* [Top 10 Redshift Performance Tuning Techniques](https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-techniques-for-amazon-redshift/) - A blog post outlining performance tuning techniques.\n* [Test Drive](https://github.com/aws/redshift-test-drive) - A collection of utilities and automation to compare performance of different Redshift configurations for a given workload.\n* [Simple Replay](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/SimpleReplay) - A library to record your queries and replay them on a different cluster to test performance. (DEPRECATED: Use [Test Drive](https://github.com/aws/redshift-test-drive))\n* [Node Configuration Compare](https://github.com/aws-samples/amazon-redshift-config-compare) - A library to compare performance of different cluster sizes and configurations by recording and replaying your queries (uses Simple Replay under the hood). (DEPRECATED: Use [Test Drive](https://github.com/aws/redshift-test-drive))\n* [Admin Scripts](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/AdminScripts) - A collection of queries and scripts to inspect performance and other administrative tasks.\n* [Admin Views](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/AdminViews) - A collection of views to inspect performance and other adminstrative tasks.\n* [Benchmark Redshift Using TPC-DS and TPC-H](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark) - A collection of commands and queries to setup and run TPC-DS/TPC-H on Redshift.\n* [The adx-tpc-ds Benchmark Scripts](https://github.com/aws-samples/redshift-benchmarks/tree/main/adx-tpc-ds) - A library to benchmark Redshift without having to generate and load data \n* [ClickBench - Compare Analytical DBMS](https://benchmark.clickhouse.com/) - A comparison of performance of various data warehouses and analytical DBMS.\n\n## Connectors and Drivers\n*Redshift connectors and drivers*\n* [Amazon Redshift Python Driver](https://github.com/aws/amazon-redshift-python-driver) - Amazon Redshift's connector for Python.\n* [Amazon Redshift JDBC driver](https://github.com/aws/amazon-redshift-jdbc-driver) - Amazon Redshift JDBC driver.\n* [Amazon Redshift ODBC driver](https://github.com/aws/amazon-redshift-odbc-driver) - Amazon Redshift ODBC driver.\n* [Amazon Redshift Integration with Apache Spark on EMR and Glue](https://aws.amazon.com/blogs/aws/new-amazon-redshift-integration-with-apache-spark/) - Connecting to Redshift from Amazon EMR 6.9, EMR Serverless, and AWS Glue 4.0.\n* [Redshift Data Source for Apache Spark - Community Edition](https://github.com/spark-redshift-community/spark-redshift) - Connecting to Redshift from Apache Spark - community edition.\n* [Query Amazon Redshift with Databricks](https://docs.databricks.com/external-data/amazon-redshift.html) - Connecting to Redshift from Databricks Runtime.\n\n## Operations\n\n*Tools and scripts to automate management and operations of Redshift.*\n\n* [Visualize Redshift Operational Metrics Using Grafana](https://aws.amazon.com/blogs/big-data/query-and-visualize-amazon-redshift-operational-metrics-using-the-amazon-redshift-plugin-for-grafana/) - A blog post how to use Amazon Redshift plugin for Grafana to query and visualize Redshift operational metrics.\n* [Redshift Stored Procedures](https://github.com/aws-samples/amazon-redshift-udfs/tree/master/stored-procedures) - A collection of stored procedures to perform common data tasks, such as integrity checks, permissions, and changing your data model.\n* [Redshift Automation](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/RedshiftAutomation) - A library to automate common tasks using AWS CloudWatch events and AWS Lambda.\n* [QMR Notifications Utility](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/QMRNotificationUtility) - A library to set SNS notifications for changes in WLM Query Monitoring Rules (QMR).\n\n## Integrations\n\n*Libraries and resources to help integrate Redshift with other frameworks and AWS services* \n### [Pandas](https://pandas.pydata.org/)\n* [AWS SDK for Pandas](https://github.com/aws/aws-sdk-pandas) - A library to transfer data between [Pandas](https://pandas.pydata.org/), Redshift, and other AWS services.\n\n### [dbt](https://docs.getdbt.com/reference/resource-configs/redshift-configs)\n* [Best Practices for Leveraging Amazon Redshift and dbt](https://d1.awsstatic.com/products/Redshift/Amazon-Redshift-dBT-Best-Practice_paper.pdf) - A white-paper covering best practices and performance tuning when using dbt and Amazon Redshift.\n* [Using DBT with Amazon Redshift Workshop](https://catalog.workshops.aws/dbt-cli-and-amazon-redshift/en-US) - A hands on workshop on integrating [DBT](https://github.com/dbt-labs/dbt-redshift) and Redshift.\n\n### [Grafana](https://grafana.com)\n* [Amazon Redshift Plugin for Grafana](https://grafana.com/grafana/plugins/grafana-redshift-datasource/) - Redshift plugin for Grafana.\n\n### [Apache Airflow](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/redshift.html)\n* [Amazon Redshift SQL Operator](https://airflow.apache.org/docs/apache-airflow-providers-amazon/2.4.0/operators/redshift.html) - An operator allowing Apache Airflow users to execute statements against Redshift in workflows.\n\n### [SQLAlchemy](https://www.sqlalchemy.org/)\n* [Use the Amazon Redshift SQLAlchemy dialect to interact with Amazon Redshift](https://aws.amazon.com/blogs/big-data/use-the-amazon-redshift-sqlalchemy-dialect-to-interact-with-amazon-redshift/) - A blog covering how to use the `sqlalchemy-redshift` dialect with SQLAlchemy.\n\n### [Querybook](https://www.querybook.org/)\n* [Adding Amazon Redshift Query engine to Querybook](https://www.querybook.org/docs/setup_guide/connect_to_query_engines/#step-by-step-guide) - A step by step guide showing how to add a Amazon Redshift query engine to Querybook.\n\n### [AWS Glue](https://aws.amazon.com/glue/)\n* [Execute Amazon Redshift Commands using AWS Glue](https://github.com/aws-samples/amazon-redshift-commands-using-aws-glue) - A library to use a AWS Glue Python Shell Job to execute SQL scripts on Amazon Redshift.\n\n### [Amazon Location Service](https://aws.amazon.com/location/)\n* [Amazon Redshift User Defined Functions to Call Amazon Location Service APIs](https://github.com/aws-samples/amazon-redshift-location-user-defined-functions) - A library using Lambda-based User Defined Functions (UDF) to call Amazon Location Service APIs.\n\n## Security and Authentication\n### General Security\n*General resources for Redshift's security*\n* [AWS Summit NY 2022 - Amazon Redshift Security Enhancements](https://www.youtube.com/watch?v=PPbmGLmivOA) - A video session covering authentication, access control, audit, and encryption.\n* [AWS Config Rules for Redshift Security](https://github.com/awslabs/aws-config-rules/blob/master/aws-config-conformance-packs/Security-Best-Practices-for-Redshift.yaml) - An [AWS Config Rules](https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html) [conformance pack](https://docs.aws.amazon.com/config/latest/developerguide/conformance-packs.html) to apply Amazon Redshift's security best practices.\n\n### Single Sign On (SSO)\n*Integration with SSO providers*\n\n* [Integrate Amazon Redshift with Microsoft Azure AD](https://aws.amazon.com/blogs/big-data/integrate-amazon-redshift-native-idp-federation-with-microsoft-azure-ad-using-a-sql-client/) - A blog post describing how to integrate Amazon Redshift native IdP federation with Microsoft Azure AD using a SQL client.\n* [Federate Amazon Redshift Access with Microsoft Azure AD SSO](https://aws.amazon.com/blogs/big-data/federate-amazon-redshift-access-with-microsoft-azure-ad-single-sign-on/) - A blog post describing how to federate Amazon Redshift access with Microsoft Azure AD single sign-on.\n* [Federate SSO Access to Amazon Redshit with Okta](https://aws.amazon.com/blogs/big-data/federate-single-sign-on-access-to-amazon-redshift-query-editor-v2-with-okta/) - A blog post describing how to federate single sign-on access to Amazon Redshift query editor v2 with Okta.\n* [Federate Access to Your Amazon Redshift cluster with Active Directory Federation Services](https://aws.amazon.com/blogs/big-data/federate-access-to-your-amazon-redshift-cluster-with-active-directory-federation-services-ad-fs-part-1/) - A 3-part blog post describing how to federate access to Amazon Redshift cluster with Active Directory Federation Services (AD FS).\n\n### Role Based Access Control (RBAC)\n*Using role-based access control (RBAC) to manage database permissions*\n* [Simplify Management of Database Privileges in Amazon Redshift](https://aws.amazon.com/blogs/big-data/simplify-management-of-database-privileges-in-amazon-redshift-using-role-based-access-control/) - A blog post providing a step-by-step guide to setting up role based access control.\n* [Introducing Role Based Access Control (RBAC) in Amazon Redshift](https://www.youtube.com/watch?v=IhHQ7mZ-tp4) - A video providing an overview and step-by-step guide to setting up role based access control.\n\n### Row Level Security (RLS)\n*Using row-level security (RLS) to gain granular access control*\n* [Achieve Fine-Grained Data Security with Row-Level Access Control](https://aws.amazon.com/blogs/big-data/achieve-fine-grained-data-security-with-row-level-access-control-in-amazon-redshift/) - A blog post providing a step-by-step guide to setting up row level security.\n* [AWS Summit NY 2022 - Amazon Redshift Security Enhancements](https://www.youtube.com/watch?v=PPbmGLmivOA) - A video session explaining how to protect data with role-based access controls, row-level security, and other AWS security features.\n\n### Encryption\n*Protect your data using encryption*\n* [Encrypt Amazon Redshift Data Loads with Amazon S3 and AWS KMS](https://aws.amazon.com/blogs/big-data/encrypt-your-amazon-redshift-loads-with-amazon-s3-and-aws-kms/) - A blog post describing how to encrypted data loads end-to-end.\n* [Accelerate Resize and Encryption of Amazon Redshift Clusters with Asynchronous Resize](https://aws.amazon.com/blogs/big-data/accelerate-resize-and-encryption-of-amazon-redshift-clusters-with-asynchronous-classic-resize/) - A blog post how to asynchronously resize and encrypt an existing cluster.\n\n## Cost Optimization\n*Tools and resources to help reduce Redshift cost*\n* [Cost Optimization Guidelines for Amazon Redshift](https://d1.awsstatic.com/whitepapers/amazon-redshift-cost-optimization.pdf) - A white paper of best-practices to optimize Redshift's cost.\n* [Query to Analyze Redshift's Cost and Usage Report (CUR)](https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/database/#amazon-redshift) - SQL query to analyze analyze Redshift's cost and usage using Amazon Athena.\n* [How to Attribute Amazon Redshift Costs to your End-Users](https://aws.amazon.com/blogs/big-data/how-to-attribute-amazon-redshift-costs-to-your-end-users/) - A blog detailing step-by-step instructions on how to attribute redshift costs to end users\n\n## CI/CD\n\n*Libraries and resources to help provision Redshift using CI/CD tools*\n\n* [Apply CI/CD DevOps Principles to Amazon Redshift Development](https://aws.amazon.com/blogs/big-data/apply-ci-cd-devops-principles-to-amazon-redshift-development/) - A blog post and [accompanying repo](https://github.com/aws-samples/amazon-redshift-devops-blog) step by step guide to provision Redshift as a part of a deployment pipeline, using [AWS CodeCommit](https://aws.amazon.com/codecommit/), [AWS CodeBuild](http://aws.amazon.com/codebuild), and [AWS CodePipeline](http://aws.amazon.com/codepipeline).\n* [Amazon Redshift Infrastructure Automation](https://github.com/aws-samples/amazon-redshift-infrastructure-automation) - A library to help automate provisoning of Redshift including data migration.\n* [Terraform Redshift Example Module](https://github.com/cloudposse/terraform-aws-redshift-cluster) - A template repository to deploy Redshift using [Terraform](https://www.terraform.io/).\n* [CDK Redshift Project](https://constructs.dev/packages/cdk-stepfunctions-redshift/) - An AWS Cloud Development Kit (CDK) [construct](https://docs.aws.amazon.com/cdk/v2/guide/constructs.html) to run SQL in Redshift using [AWS Step Functions](https://aws.amazon.com/step-functions).\n\n## Redshift Internals\n*Redshift's internal architecture and design*\n* [Amazon Redshift Re-invented](https://www.amazon.science/publications/amazon-redshift-re-invented) - A paper outlining Redshift's internal system architecture, data organization, and query processing flow.\n\n# General Resources\n\n*Blogs, forums, and other online Redshift resources*\n\n* [AWS re:Post](https://repost.aws/tags/TAByF7MpfSQUCX_lAeDTvODw/amazon-redshift) - A Q&A forum and knowledge sharing community.\n* [AWS Big Data Blog](https://aws.amazon.com/blogs/big-data/category/database/amazon-redshift/) - AWS official data blog.\n* [AWS Event YouTube Channel](https://www.youtube.com/@AWSEventsChannel/search?query=redshift) - Recorded presentations and talks from AWS events, such as re:Invent and AWS summits.\n\n# Contributing\n\nYour contributions are always welcome! Please take a look at the [contribution guidelines](https://github.com/aws/awesome-redshift/blob/main/CONTRIBUTING.md) first.\n\nWe will keep some pull requests open if we aren't sure whether those resources are awesome, you could [vote for them](https://github.com/aws/awesome-redshift/pulls) by adding :+1: to them.\n", "release_dates": []}, {"name": "aws-app-mesh-controller-for-k8s", "description": "A controller to help manage App Mesh resources for a Kubernetes cluster.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2023-12-14T22:43:53Z", "2023-11-22T20:46:21Z", "2023-08-01T17:11:30Z", "2023-06-05T20:38:28Z", "2023-02-24T19:14:58Z", "2023-01-05T00:15:38Z", "2022-11-15T00:24:02Z", "2022-09-07T21:43:03Z", "2022-08-19T01:31:31Z", "2022-08-10T19:55:32Z", "2022-05-20T21:42:08Z", "2022-03-24T21:20:10Z", "2021-12-07T00:35:42Z", "2021-08-25T18:59:49Z", "2021-06-15T18:11:44Z", "2021-02-04T20:51:14Z", "2020-12-14T15:38:46Z", "2020-11-09T20:06:00Z", "2020-08-14T00:01:57Z", "2020-07-10T01:05:46Z", "2020-06-18T19:11:41Z", "2020-04-24T06:15:56Z", "2020-03-13T17:56:25Z", "2019-11-17T04:41:06Z", "2019-10-18T02:04:18Z"]}, {"name": "aws-app-mesh-examples", "description": "AWS App Mesh is a service mesh that you can use with your microservices to manage service to service communication.", "language": "Shell", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# AWS App Mesh\n\n## Introduction\n\nApp Mesh makes it easy to run microservices by providing consistent visibility and network traffic controls for every microservice in an application. App Mesh separates the logic needed for monitoring and controlling communications into a proxy that runs next to every microservice. App Mesh removes the need to coordinate across teams or update application code to change how monitoring data is collected or traffic is routed. This allows you to quickly pinpoint the exact location of errors and automatically re-route network traffic when there are failures or when code changes need to be deployed.\n\nYou can use App Mesh with AWS Fargate, Amazon Elastic Container Service (ECS), Amazon Elastic Container Service for Kubernetes (EKS), and Kubernetes on EC2 to better run containerized microservices at scale. App Mesh uses [Envoy](https://www.envoyproxy.io/), an open source proxy, making it compatible with a wide range of AWS partner and open source tools for monitoring microservices.\n\nLearn more at https://aws.amazon.com/app-mesh\n\n## Availability\n\nToday, AWS App Mesh is generally available for production use. You can use App Mesh with AWS Fargate, Amazon Elastic Container Service (ECS), Amazon Elastic Container Service for Kubernetes (EKS), applications running on Amazon EC2, and Kubernetes on EC2 to better run containerized microservices at scale. App Mesh uses Envoy, an open source proxy, making it compatible with a wide range of AWS partner and open source tools for monitoring microservices.\n\nLearn more at https://aws.amazon.com/app-mesh\n\n## Getting started\nFor help getting started with App Mesh, take a look at the [examples](https://github.com/aws/aws-app-mesh-examples/tree/master/examples) in this repo.  \n\n### ARM64 support    \nAll the [walkthrough](https://github.com/aws/aws-app-mesh-examples/tree/main/walkthroughs) examples in this repo are compatible only\nwith amd64 linux instances. arm64 is only supported from version v1.20.0.1 or later of [aws-appmesh-envoy](https://gallery.ecr.aws/appmesh/aws-appmesh-envoy) and [v1.4.2](https://github.com/aws/aws-app-mesh-controller-for-k8s/releases/tag/v1.4.2) and later for\nAppmesh-controller. We are working on updating these walkthroughs to be arm64 compatible as well. See https://github.com/aws/aws-app-mesh-examples/issues/473 for more up-to-date information.  \n\n### China Regions\nAll the examples and walkthrough are written for commercial regions. You need to make few changes to make them work for China regions, below are some changes that will be needed:\n\n* Change ARN:\n  For China regions include aws-cn in all arns. So instead of 'arn:aws:' it starts with 'arn:aws-cn:'.\n  Replace 'arn:aws:' with 'arn:${AWS::Partition}:' to make it work for all partitions.\n* Change Endpoints:\n  The endpoint domain for China regions is amazonaws.com.cn. Replace the endpoints from amazonaws.com to amazonaws.com.cn Refer [this](https://docs.amazonaws.cn/en_us/aws/latest/userguide/endpoints-Beijing.html) doc for a list of endpoints for cn-north-1.\n  Do not change the Service Principal like ecs-tasks.amazonaws.com, it is a Service Principal not an endpoint.\n* Change TCP ports 80/8080/443\n  By default all AWS China accounts are blocked for TCP ports 80/8080/443 with EC2 and S3 services. These ports will be unlocked when an ICP license has been provided by customers. As a workaround you can use some other port for ex: 9090. The url that you curl for, needs to explicitly mention the port now.\n  For example: http://appme-.....us-west-2.elb.amazonaws.com.cn:9090/color\n\n### Roadmap\n\nThe AWS App Mesh team maintains a [public roadmap](https://github.com/aws/aws-app-mesh-roadmap).\n\n### Participate\n\nIf you have a suggestion, request, submission, or bug fix for the examples in this repo, please open it as an [Issue](https://github.com/aws/aws-app-mesh-examples/issues).  \n\nIf you have a feature request for AWS App Mesh, please open an Issue on the [public roadmap](https://github.com/aws/aws-app-mesh-roadmap).\n\n## Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n### Why use  App Mesh?\n\n1. Streamline operations by offloading communication management logic from application code and libraries into configurable infrastructure.\n2. Reduce troubleshooting time required by having end-to-end visibility into service-level logs, metrics and traces across your application.\n3. Easily roll out of new code by dynamically configuring routes to new application versions.\n4. Ensure high-availability with custom routing rules that help ensure every service is highly available during deployments, after failures, and as your application scales.\n5. Manage all service to service traffic using one set of APIs regardless of how the services are implemented.\n\n### What makes AWS App Mesh unique?\n\nAWS App Mesh is built in direct response to our customers needs implementing a 'service mesh' for their applications. Our customers asked us to:\n\n* Make it easy to manage microservices deployed across accounts, clusters, container orchestration tools, and compute services with simple and consistent abstractions.\n* Minimize the cognitive and operational overhead in running a microservices application and handling its monitoring and traffic control.\n* Remove the need to build or operate a control plane for service mesh.\n* Use open source software to allow extension to new tools and different use cases.\n\nIn order to best meet the needs of our customers, we have invested into building a service that includes a control plane and API that follows the AWS best practices. Specifically, App Mesh:\n\n* Is an AWS managed service that works across container services with a design that allows us to add support for other computer services in the future.\n* Works with the open source Envoy proxy\n* Is designed to pluggable and will support bringing your own Envoy images and Istio Mixer in the future.\n* Implemented as a multi-tenant control plane to be scalable, robust, cost-effective, and efficient.\n* Built to work independently of any particular container orchestration system. Today, App Mesh works with both Kubernetes and Amazon ECS.\n", "release_dates": []}, {"name": "aws-app-mesh-roadmap", "description": "AWS App Mesh is a service mesh that you can use with your microservices to manage service to service communication", "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS App Mesh Roadmap\n\nThis is the public roadmap for AWS App Mesh. \n\nAWS App Mesh makes it easy to monitor and control microservices running on AWS. App Mesh standardizes how your microservices communicate, giving you end-to-end visibility and helping to ensure high-availability for your applications.\nYou can use App Mesh with [AWS Fargate](https://aws.amazon.com/fargate), [Amazon Elastic Container Service (ECS)](https://aws.amazon.com/ecs/), [Amazon Elastic Container Service for Kubernetes (EKS)](https://aws.amazon.com/eks/), and Kubernetes on EC2 to better run containerized microservices at scale. App Mesh uses Envoy, an open source proxy, making it compatible with a wide range of AWS partner and open source tools for monitoring microservices.\n\nFor help getting started with App Mesh, head to https://aws.amazon.com/app-mesh.  Or, to see examples, you can visit our examples repository: https://github.com/aws/aws-app-mesh-examples\n\n---\n\n## Introduction\nThis is the experimental public roadmap for AWS App Mesh.\nKnowing about our upcoming products and priorities helps our customers plan. This repository contains information about what we are working on and allows all AWS customers to give direct feedback.\n\n[See the roadmap \u00bb](https://github.com/aws/aws-app-mesh-roadmap/projects/1)\n\n## Beta Channel\nAWS App Mesh offers a Beta Channel, a public service endpoint which allows every customer to try out and provide feedback on beta service features before they are generally available. The service endpoint is separate from the standard production endpoint, and is coupled with preview releases of the AWS CLI for App Mesh, allowing customers to test beta features without impacting their current production infrastructure.\n\nFor more information on the Beta Channel, see [AWS App Mesh Beta Channel](/BETA-CHANNEL.md).\n\n## Slack Community\nAWS App Mesh participates in an open Slack community in which participants can share tips and tricks, get help troubleshooting a problem, or discuss new and potential future features of App Mesh.\n\nThe slack community can be found at https://awsappmesh.slack.com, and you can join the slack community with [this invite](https://join.slack.com/t/awsappmesh/shared_invite/zt-dwgbt85c-Sj_md92__quV8YADKfsQSA).\n\n## Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n\n## FAQs\n**Q: Why did you build this?**\n\nA: We know that our customers are making decisions and plans based on what we are developing, and we want to provide our customers the insights they need to plan.\n\n**Q: Why are there no dates on your roadmap?**\n\nA: Because job zero is security and operational stability, we can't provide specific target dates for features.\n\n**Q: What do the roadmap categories mean?**\n* *Just shipped* - obvious, right?\n* *Coming soon* - coming up.  Think a couple of months out, give or take.\n* *We're working on it* - in progress, but further out.  We might still be working through the implementation details, or scoping stuff out.\n* *Researching* - We're thinking about it. This might mean we're still designing, or thinking through how this might work. This is a great phase to send how you want to see something implemented!  We'd love to see your usecase or design ideas here. \n\n**Q: Is everything on the roadmap?**\n\nA: The majority of our development work for AWS App Mesh and other AWS-sponsored OSS projects are included on this roadmap. Of course, there will be technologies we are very excited about that we are going to launch without notice to surprise and delight our customers.\n\n**Q: How can I provide feedback or ask for more information?**\n\nA: Please open an issue!\n\n**Q: How can I request a feature be added to the roadmap?**\n\nA: Please open an issue!  You can read about how to contribute [here](/CONTRIBUTING.md). Community submitted issues will be tagged \"Proposed\" and will be reviewed by the team.\n\n**Q: Will you accept a pull request?**\n\nA: We haven't worked out how pull requests should work for a public roadmap page, but we will take all PRs very seriously and review for inclusion. Read about [contributing](/CONTRIBUTING.md).\n\n## License\n\nThis library is licensed under the Apache 2.0 License. \n\nTo learn more about the services, head here: https://aws.amazon.com/app-mesh\n", "release_dates": []}, {"name": "aws-application-networking-k8s", "description": "A Kubernetes controller for Amazon VPC Lattice", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Gateway API Controller for VPC Lattice\n\n<p align=\"center\">\n    <img src=\"docs/images/kubernetes_icon.svg\" alt=\"Kubernetes logo\" width=\"100\" /> \n    <img src=\"docs/images/controller.png\" alt=\"AWS Load Balancer logo\" width=\"100\" />\n</p>\n\nAWS Application Networking is an implementation of the Kubernetes [Gateway API](https://gateway-api.sigs.k8s.io/). This project is designed to run in a Kubernetes cluster and orchestrates AWS VPC Lattice resources using Kubernetes Custom Resource Definitions like Gateway and HTTPRoute.\n\n## Documentation\n\n### Website\n\nThe API specification and detailed documentation is available on the project\nwebsite: [https://www.gateway-api-controller.eks.aws.dev/][ghp].\n\n### Concepts\n\nTo get started, please read through [API concepts][concepts]. These documents give the necessary background to understand the API and the use-cases it targets.\n\n### Getting started\n\nOnce you have a good understanding of the API at a higher-level, check out\n[getting started][getting-started] to install your first Gateway controller and try out\none of the guides.\n\n### References\n\nA complete API reference, please refer to:\n\n- [API reference][spec]\n- [Go docs for the package][godoc]\n\n## Contributing\n\nDeveloper guide can be found on the [developer guide page][dev].\nOur Kubernetes Slack channel is [#aws-gateway-api-controller][slack].\n\n### Code of conduct\n\nParticipation in the Kubernetes community is governed by the\n[Kubernetes Code of Conduct](code-of-conduct.md).\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n[ghp]: https://www.gateway-api-controller.eks.aws.dev/\n[dev]: https://www.gateway-api-controller.eks.aws.dev/contributing/developer/\n[slack]: https://kubernetes.slack.com/messages/aws-gateway-api-controller\n[getting-started]: https://www.gateway-api-controller.eks.aws.dev/guides/getstarted/\n[spec]: https://www.gateway-api-controller.eks.aws.dev/api-reference/\n[concepts]: https://www.gateway-api-controller.eks.aws.dev/concepts/\n[gh_release]: https://github.com/aws/aws-application-networking-k8s/releases/tag/v1.0.3\n[godoc]: https://www.gateway-api-controller.eks.aws.dev/\n", "release_dates": ["2024-01-19T19:48:21Z", "2023-12-15T23:38:50Z", "2023-11-22T00:49:30Z", "2023-11-18T03:30:57Z", "2023-11-14T20:12:46Z", "2023-10-06T16:33:18Z", "2023-08-31T22:09:56Z", "2023-07-27T19:40:20Z", "2023-07-05T20:27:15Z", "2023-06-29T05:43:02Z", "2023-05-25T16:27:38Z", "2023-05-03T16:09:54Z", "2023-04-18T23:48:57Z", "2023-03-30T06:20:41Z", "2023-03-30T06:20:18Z", "2023-03-30T06:18:58Z", "2023-03-30T06:18:19Z", "2023-03-30T06:17:32Z", "2023-03-30T06:16:42Z", "2022-11-29T00:37:23Z"]}, {"name": "aws-appsync-community", "description": "The AWS AppSync community", "language": "HTML", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS AppSync\n\n[AWS AppSync](https://aws.amazon.com/appsync/) is a managed GraphQL service for application data and a back-end for mobile, web, and enterprise applications.\n\n![Awesome AWS AppSync](https://s3.amazonaws.com/aws-mobile-hub-images/awesomeappsync.jpg)\n\n[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)\n\nAwesome AWS AppSync curates the best AWS AppSync, tools, tutorials, articles and more. PRs are welcome!\nAlso check out [Awesome AWS Amplify](https://github.com/dabit3/awesome-aws-amplify)\n\n## AppSync GraphQL Clients\n- [AWS AppSync JavaScript SDK](https://github.com/awslabs/aws-mobile-appsync-sdk-js)\n- [AWS Amplify Client](https://github.com/aws/aws-amplify)\n- [AWS AppSync iOS SDK](https://github.com/awslabs/aws-mobile-appsync-sdk-ios)\n- [AWS AppSync Android SDK](https://github.com/awslabs/aws-mobile-appsync-sdk-android)\n- [AWS AppSync Go client](https://github.com/sony/appsync-client-go)\n\n## Documentation\n- [Developer Guide](https://docs.aws.amazon.com/appsync/latest/devguide/what-is-appsync.html)\n- [Building a Client App](https://docs.aws.amazon.com/appsync/latest/devguide/building-a-client-app.html)\n- [Building a NodeJS Client App](https://docs.aws.amazon.com/appsync/latest/devguide/building-a-client-app-node.html)\n- [Working with Real Time Data](https://docs.aws.amazon.com/appsync/latest/devguide/aws-appsync-real-time-data.html)\n- [Security & Authorization](https://docs.aws.amazon.com/appsync/latest/devguide/security.html)\n- [Resolver Mapping Template Reference](https://docs.aws.amazon.com/appsync/latest/devguide/resolver-mapping-template-reference.html)\n- [AWS Amplify GraphQL Client Documentation](https://docs.amplify.aws/lib/graphqlapi/getting-started/q/platform/js)\n- [AWS AppSync Apollo Client Documentation](https://github.com/awslabs/aws-mobile-appsync-sdk-js)\n\n## Tutorials - Blogs\n\n#### General\n- [Invoke AWS services directly from AWS AppSync](https://aws.amazon.com/blogs/mobile/invoke-aws-services-directly-from-aws-appsync/)\n- [Getting Started with Amazon Aurora Serverless Database with Data API](https://read.acloud.guru/getting-started-with-the-amazon-aurora-serverless-data-api-6b84e466b109)\n- [Creating GraphQL Batch Operations for AWS Amplify with AppSync and Cognito](https://medium.com/@jan.hesters/creating-graphql-batch-operations-for-aws-amplify-with-appsync-and-cognito-ecee6938e8ee)\n- [Building a Fully Serverless Realtime CMS using AWS Appsync and Aurora Serverless](https://medium.com/the-apps-team/building-a-fully-serverless-realtime-cms-using-aws-appsync-and-aurora-serverless-7258fe1925f7)\n- [Visualizing time series data in Vue.js](https://medium.com/js-dojo/visualizing-time-series-data-in-vue-js-6a917ea5d869)\n- [Invoking AWS Services from AppSync HTTP Resolvers](https://hackernoon.com/invoking-aws-services-from-appsync-http-resolvers-4e2c54784204)\n- [The three ways to execute a GraphQL query from React with AWS AppSync](https://medium.com/open-graphql/the-three-ways-to-execute-a-graphql-query-from-react-with-aws-appsync-and-how-to-choose-49450fb9f98)\n- [Intro to AWS AppSync Pipeline Functions](https://medium.com/open-graphql/intro-to-aws-appsync-pipeline-functions-3df87ceddac1)\n- [Using AWS AppSync Pipeline Resolvers for GraphQL Authorization](https://medium.com/@crhuber/using-aws-appsync-pipeline-resolvers-for-graphql-authorization-d04bb7a8dc44)\n- [How to deploy a GraphQL API on AWS using the Serverless Framework](https://read.acloud.guru/deploy-a-graphql-service-on-aws-with-the-serverless-framework-7af8fc22a01d)\n- [Serverless GraphQL with AWS AppSync and Lambda](https://sbstjn.com/serverless-graphql-with-appsync-and-lambda.html)\n- [GraphQL authorization with multiple data sources using AWS AppSync](https://hackernoon.com/graphql-authorization-with-multiple-data-sources-using-aws-appsync-dfae2e350bf2)\n- [Build a Multi-user GraphQL Table with AWS AppSync](https://medium.com/@FizzyInTheHall/build-a-multi-user-graphql-table-with-aws-appsync-e2c8a9486b2a)\n- [Rethinking REST Practices](https://itnext.io/rethinking-rest-practices-1efc56418980)\n- [Go Forth & AppSync](https://medium.com/@mwarger/go-forth-and-appsync-34450c277075)\n- [Top 9 AWS AppSync Features You Didn\u2019t Know About](https://medium.com/open-graphql/top-9-aws-appsync-features-you-didnt-know-about-57224075ffb1)\n- [AWS AppSync code-centric development using CloudFormation](https://blissful.cc/articles/aws-appsync-code-centric-development-using-cloudformation/)\n- [Merge AppSync and Github API into One GraphQL API (on AWS Lambda)](https://medium.com/@hiroyuki.osaki/merge-appsync-and-github-api-into-one-graphql-api-on-aws-lambda-8ea4c42c2db2)\n- [Dead-simple AWS GraphQL API](https://medium.com/@mim3dot/dead-simple-aws-graphql-api-59db32510bfb)\n- [Calling AWS AppSync, or any GraphQL API, from AWS Lambda, part 1](https://www.fullsapps.com/2019/02/calling-aws-appsync-or-any-graphql-api.html)\n\n#### VTL\n- [AWS AppSync Velocity Templates Guide](https://medium.com/@gerard.sans/aws-appsync-velocity-templates-guide-55b9d2bff053)\n- [AWS AppSync and the GraphQL Info Object](https://aws.amazon.com/blogs/mobile/appsync-and-the-graphql-info-object/)\n\n#### AWS Amplify\n- [8 steps to building your own serverless GraphQL API using AWS Amplify](https://read.acloud.guru/8-steps-to-building-your-own-serverless-graphql-api-using-aws-amplify-42c21770424d)\n\n#### React\n- [Getting into GraphQL with AWS AppSync](https://css-tricks.com/getting-into-graphql-with-aws-appsync/)\n- [Create a Multiuser GraphQL CRUD(L) Elasticsearch App in 10 minutes with the new AWS Amplify CLI](https://medium.com/open-graphql/create-a-multiuser-graphql-crud-l-app-in-10-minutes-with-the-new-aws-amplify-cli-and-in-a-few-73aef3d49545)\n- [Implementing Search in GraphQL](https://medium.com/open-graphql/implementing-search-in-graphql-11d5f71f179)\n- [Building Serverless React GraphQL Applications with AWS AppSync](https://tylermcginnis.com/building-serverless-react-graphql-apps-with-aws-appsync/)\n- [SSR GraphQL with Next.js & AWS AppSync](https://medium.com/open-graphql/ssr-graphql-apps-with-next-js-aws-appsync-eaf7fbeb1bde)\n- [AWS AppSync App with React and Apollo](https://gyandeeps.com/aws-appsync-graphql/)\n- [GraphQL made easy by AWS AppSync](https://medium.com/@jorgenlybeck94/graphql-made-easy-by-aws-appsync-21dfae586d51)\n- [Building a chat application using AWS AppSync and Serverless](https://serverless.com/blog/building-chat-appliation-aws-appsync-serverless/)\n- [Building Chatt - A Real-time Multi-user GraphQL Chat App](https://dev.to/dabit3/building-chatt---a-real-time-multi-user-graphql-chat-app-3jik)\n- [React in a Serverless World](https://dev.to/exodevhub/react-in-a-serverless-world-2m3d)\n\n#### React Native\n- [Building AI Enabled GraphQL Applications](https://medium.com/open-graphql/building-ai-enabled-graphql-applications-d7fde3305062)\n- [Code an App With GraphQL, React Native, and AWS AppSync: The Back-End](https://code.tutsplus.com/tutorials/code-an-app-with-graphql-and-react-native--cms-30511)\n- [Code an App With GraphQL, React Native and AWS AppSync: The App](https://code.tutsplus.com/tutorials/code-an-app-with-graphql-react-native-and-aws-appsync-the-app--cms-30569)\n- [Integrating AWS AppSync with React-Native-Navigation](https://medium.com/@ryanu_81365/integrating-aws-appsync-with-react-native-navigation-466e103e7c15)\n- [Authentication \ud83d\udd10 FULL SETUP](https://itnext.io/aws-amplify-react-native-authentication-full-setup-7764b452a138)\n\n#### Native Android\n- [Building Android Landmark App with AWS Amplify, Google Sign-In and GraphQL](https://medium.com/step-by-step-building-mobile-with-mobile-backend/building-landmark-app-with-aws-amplify-google-sign-in-with-graphql-61f52fb115d7)\n\n#### Vue\n- [Using Appsync and Amplify with Vue](https://medium.com/@andrew.s.trigg/using-appsync-and-amplify-with-vue-f45ebef7276e)\n\n#### Angular\n- [Ionic 4 + AppSync: Build a mobile app with a GraphQL backend - 4 Part Series](https://gonehybrid.com/ionic-4-appsync-build-a-mobile-app-with-a-graphql-backend-part-1/)\n\n#### Serverless Framework\n- [GraphQL APIs with AWS AppSync (3 part series)](https://medium.com/@cbartling/graphql-apis-with-aws-appsync-part-one-bb441a5c2d9b)\n\n#### SST\n- [AWS AppSync with SST and Live Lambda Dev](https://sst.dev/examples/how-to-create-a-serverless-graphql-api-with-aws-appsync.html)\n\n#### Infrastructure as Code\n- [AWS AppSync + Pulumi + GraphQL Modules](https://github.com/bjerkio/pulumi-appsync-modules)\n\n#### Performance and Monitoring\n- [Getting more visibility into GraphQL performance with AWS AppSync logs](https://aws.amazon.com/blogs/mobile/getting-more-visibility-into-graphql-performance-with-aws-appsync-logs)\n- [Tracing with AWS X-Ray](https://docs.aws.amazon.com/appsync/latest/devguide/x-ray-tracing.html)\n\n## Tutorials - Videos\n- [Scalable Offline-Ready GraphQL Applications with AWS AppSync & React](https://egghead.io/courses/scalable-offline-ready-graphql-applications-with-aws-appsync-react)\n- [How to AWS AppSync (YouTube)](https://www.youtube.com/playlist?list=PLrHsRbHuA_HVl5q2mQ6q7wjTitskiUuu6)\n- [How to AWS AppSync (GitHub)](https://github.com/ifelsebranch/appsync-react-webapp)\n- [AWS AppSync - User Authorization & Fine Grained Access Control](https://www.youtube.com/watch?v=p0mfjz6wZng)\n- [AWS AppSync Authorization with Amazon Cognito User Pools](https://www.youtube.com/watch?v=VKhLqBG7sdI)\n- [Building an e-Commerce Loyalty App with GraphQL](https://www.youtube.com/watch?v=WOQIqRVzkas)\n\n## Miscellaneous Videos\n- [AWS re:Invent 2018: Authentication & Authorization in GraphQL with AWS AppSync](https://www.youtube.com/watch?v=2U4RsbFO4bA)\n- [AWS re:Invent 2018: Ten Tips And Tricks for Improving Your GraphQL API with AWS AppSync](https://www.youtube.com/watch?v=CwLB0BRwIqE)\n- [AWS re:Invent 2018: Develop Cross-Platform Mobile Apps with React Native, GraphQL, & AWS](https://www.youtube.com/watch?v=38Y-XvMYpfA)\n\n## News\n- [AWS joins the GraphQL Foundation](https://aws.amazon.com/blogs/mobile/aws-joins-the-graphql-foundation/)\n\n## Example Projects\n\n#### Relay Modern\n- [A sample Relay app using AWS AppSync](https://github.com/aws-samples/aws-appsync-relay)\n\n#### React Native\n- [React Native Starter App - Serverless Pet Tracker](https://github.com/aws-samples/aws-mobile-react-native-starter)\n- [A real time React Native application built using GraphQL & AWS AppSync](https://github.com/dabit3/appsync-graphql-cities)\n- [Enterprise Twitter Clone](https://github.com/dabit3/enterprise-twitter)\n- [GraphQL events app with realtime and offline functionality using AWS AppSync](https://github.com/aws-samples/aws-mobile-appsync-events-starter-react-native)\n- [React Native Movies App: AWS AppSync, AWS Amplify, AWS Cognito, GraphQL, DynamoDB](https://github.com/pjay79/MoviesApp)\n- [Basic React Native Implementation](https://github.com/dabit3/basic-react-native-appsync)\n- [AI Enabled GraphQL with React Native](https://github.com/dabit3/appsync-lambda-ai)\n- [React Native example with user authentication & authorization](https://github.com/dabit3/appsync-react-native-with-user-authorization)\n\n#### React\n- [SpeakerChat - Real-time Event Q&A Platform with Markdown Support](https://github.com/dabit3/speakerchat)\n- [Write with Me - Real-time Collaborative Markdown Editor](https://github.com/dabit3/write-with-me)\n- [Hype Beats](https://github.com/dabit3/hype-beats)\n- [Appsync Graphql Real-time Canvas](https://github.com/dabit3/appsync-graphql-real-time-canvas)\n- [AWS AppSync Chat](https://github.com/aws-samples/aws-appsync-chat)\n- [AWS AppSync Chat with AI features - Chatbots, Image Recognition, Sentiment Analysis, Translation, Text2Speech](https://github.com/aws-samples/aws-appsync-chat-starter-react)\n- [GraphQL events app with realtime and offline functionality using AWS AppSync](https://github.com/aws-samples/aws-mobile-appsync-events-starter-react)\n- [AWS AppSync Recipe App](https://github.com/dabit3/react-appsync-graphql-recipe-app)\n- [AWS AppSync GraphQL Photo Sample](https://github.com/aws-samples/aws-amplify-graphql)\n- [Unicorn Loyalty: E-Commerce Serverless GraphQL Loyalty Sample App](https://github.com/aws-samples/aws-serverless-appsync-loyalty)\n- [AWS AppSync With Apollo 3.0 using Links, React Hooks and Typescript](https://github.com/wolfeidau/appsync-apollo-links)\n\n#### Angular\n- [GraphQL PWA chat app w/ realtime and offline functionality using AWS AppSync](https://github.com/aws-samples/aws-mobile-appsync-chat-starter-angular)\n\n#### Vue\n- [Vue example using GraphQL with AWS AppSync](https://github.com/dabit3/vue-graphql-appsync)\n\n#### Native iOS\n- [GraphQL events application with realtime and offline functionality using AWS AppSync](https://github.com/aws-samples/aws-mobile-appsync-events-starter-ios)\n\n#### Native Android\n- [GraphQL events application using AWS AppSync](https://github.com/aws-samples/aws-mobile-appsync-events-starter-android)\n\n#### Other\n- [Serverless Application Model and custom resolver using Lambda with Go](https://github.com/sbstjn/appsync-resolvers-example)\n- [Serverless GraphQL Examples for AWS AppSync and Apollo](https://github.com/serverless/serverless-graphql)\n- [React State Museum](https://github.com/GantMan/ReactStateMuseum)\n- [AWS AppSync Calculator](https://medium.com/@ryanjones_io/aws-appsync-calculator-a2ad9f65a5e3)\n- [Beginner-friendly mobile backend based on AWS AppSync](https://cloudonaut.io/beginner-friendly-mobile-backend-based-on-aws-appsync/)\n- [Create an AWS AppSync API using SAM - SnappyFeatures #3](https://www.youtube.com/watch?v=J0qfMYRmQcc)\n- [AWS Chatt with ReasonML and Bucklescript](https://medium.com/@idkjs/aws-chatt-with-reasonml-and-bucklescript-895c353c3690?_branch_match_id=561306255803447569)\n\n## Tooling\n\n#### Go\n\n- [appsync-resolvers](https://github.com/sbstjn/appsync-resolvers)\n\n## Books\n\n- [Serverless GraphQL APIs with Amazon's AWS AppSync](https://www.amazon.com/Serverless-GraphQL-Amazons-AppSync-API-University-ebook/dp/B07DDD5NHF)\n", "release_dates": []}, {"name": "aws-aspnet-cognito-identity-provider", "description": "ASP.NET Core Identity Provider for Amazon Cognito", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![.NET on AWS Banner](./logo.png \".NET on AWS\")\n\n## ASP.NET Core Identity Provider for Amazon Cognito\n\n[![nuget](https://img.shields.io/nuget/v/Amazon.AspNetCore.Identity.Cognito.svg)](https://www.nuget.org/packages/Amazon.AspNetCore.Identity.Cognito/)\n\nASP.NET Core Identity Provider for [Amazon Cognito](https://aws.amazon.com/cognito/) simplifies using [Amazon Cognito](https://aws.amazon.com/cognito/) as a membership storage solution for building ASP.NET Core web applications using [ASP.NET Core Identity](https://github.com/aspnet/Identity/).\n\nThis library is not compatible with older versions of Identity such as the ones for ASP.NET MVC5 and lower. It only supports ASP.NET Core Identity and targets the .NET Standard 2.0.\n\nThe library introduces the following dependencies:\n\n* [Amazon.Extensions.CognitoAuthentication](https://www.nuget.org/packages/Amazon.Extensions.CognitoAuthentication/)\n* [AWSSDK.CognitoIdentity](https://www.nuget.org/packages/AWSSDK.CognitoIdentity/)\n* [AWSSDK.CognitoIdentityProvider](https://www.nuget.org/packages/AWSSDK.CognitoIdentityProvider/)\n* [AWSSDK.Extensions.NETCore.Setup](https://www.nuget.org/packages/AWSSDK.Extensions.NETCore.Setup/)\n* [Microsoft.AspNetCore.Identity](https://www.nuget.org/packages/Microsoft.AspNetCore.Identity/)\n* [Microsoft.Extensions.Configuration](https://www.nuget.org/packages/Microsoft.Extensions.Configuration/)\n* [Microsoft.Extensions.DependencyInjection](https://www.nuget.org/packages/Microsoft.Extensions.DependencyInjection/)\n\n\n# Getting Started\n\nFollow the examples below to see how the library can be integrated into your web application.  \n\nThis library extends the ASP.NET Core Identity membership system by using Amazon Cognito as a [Custom Storage Provider for ASP.NET Identity](https://docs.microsoft.com/en-us/aspnet/identity/overview/extensibility/overview-of-custom-storage-providers-for-aspnet-identity).\n\n## Referencing the library\n\nSimply add the following NuGet dependencies to your ASP.NET Core application:\n\n* [Amazon.AspNetCore.Identity.Cognito](https://www.nuget.org/packages/Amazon.AspNetCore.Identity.Cognito/)\n* [Amazon.Extensions.CognitoAuthentication](https://www.nuget.org/packages/Amazon.Extensions.CognitoAuthentication/)\n\n\n## Adding Amazon Cognito as an Identity Provider\n\nTo add Amazon Cognito as an Identity Provider, make the following change to your code:\n\nStartup.cs:\n\n```csharp\npublic void ConfigureServices(IServiceCollection services)\n{\n    // Adds Amazon Cognito as Identity Provider\n    services.AddCognitoIdentity();\n    ...\n}\n\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env)\n{\n    // If not already enabled, you will need to enable ASP.NET Core authentication\n    app.UseAuthentication();\n    ...\n}\n```\n\nNext the user pool and user pool client need to be configured as part of the IConfiguration of the ASP.NET Core application. For a development user pool edit either the `appsettings.Development.json` file or the projects [secrets.json](https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets) file. Below is an example of the JSON snippet to go into the file.\n\n```csharp\n\"AWS\": {\n    \"Region\": \"<your region id goes here>\",\n    \"UserPoolClientId\": \"<your user pool client id goes here>\",\n    \"UserPoolClientSecret\": \"<your user pool client secret goes here>\",\n    \"UserPoolId\": \"<your user pool id goes here>\"\n}\n```\n\n**Note:** If using `appsettings.Development.json` or some other file in your project structure be careful checking in secrets to source control.\n\nFor a production user pool it is recommend to configure the same settings as above either through IConfiguration's [environment variable support](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration/#environment-variables) or with the AWS System Manager's parameter store which can be integrated with IConfiguration using the [Amazon.Extensions.Configuration.SystemsManager](https://aws.amazon.com/blogs/developer/net-core-configuration-provider-for-aws-systems-manager/) NuGet package.\n\n\nAlternatively, instead of relying on a configuration file, you can inject your own instances of IAmazonCognitoIdentityProvider and CognitoUserPool in your Startup.cs file.\n\n```csharp\npublic void ConfigureServices(IServiceCollection services)\n{\n    ...\n    // Adds your own instance of Amazon Cognito clients \n    // cognitoIdentityProvider and cognitoUserPool are variables you would have instanciated yourself\n    services.AddSingleton<IAmazonCognitoIdentityProvider>(cognitoIdentityProvider);\n    services.AddSingleton<CognitoUserPool>(cognitoUserPool);\n\n    // Adds Amazon Cognito as Identity Provider\n    services.AddCognitoIdentity();\n    ...\n}\n```\n\n## Using the CognitoUser class as your web application user class\n\nOnce Amazon Cognito is added as the default ASP.NET Core Identity Provider, you need to use the newly introduced CognitoUser class instead of the default ApplicationUser class.\n\nThese changes will be required in existing Razor views and controllers. Here is an example with a Razor view:\n\n```csharp\n@using Microsoft.AspNetCore.Identity\n@using Amazon.Extensions.CognitoAuthentication\n\n@inject SignInManager<CognitoUser> SignInManager\n@inject UserManager<CognitoUser> UserManager\n```\n\nIn addition, this library introduces two child classes of SigninManager and UserManager designed for Amazon Cognito authentication and user management workflow: CognitoSigninManager and CognitoUserManager classes.\n\nThese two classes expose additional methods designed to support Amazon Cognito features, such as sending validation data to pre-signup [AWS Lambda triggers](https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-pre-sign-up.html) when registering a new user:\n\n```csharp\n/// <summary>\n/// Creates the specified <paramref name=\"user\"/> in Cognito with the given password and validation data,\n/// as an asynchronous operation.\n/// </summary>\n/// <param name=\"user\">The user to create.</param>\n/// <param name=\"password\">The password for the user</param>\n/// <param name=\"validationData\">The validation data to be sent to the pre sign-up lambda triggers.</param>\n/// <returns>\n/// The <see cref=\"Task\"/> that represents the asynchronous operation, containing the <see cref=\"IdentityResult\"/>\n/// of the operation.\n/// </returns>\npublic async Task<IdentityResult> CreateAsync(TUser user, string password, IDictionary<string, string> validationData)\n```\n\n# Explore the documentation and sample application\n\nFeel free to explore the [documentation folder](https://github.com/aws/aws-aspnet-cognito-identity-provider/tree/master/docs) and the [sample application](https://github.com/aws/aws-aspnet-cognito-identity-provider/tree/master/samples). These two resources provide additionnal examples on how to use the library with your ASP.NET Core web application.\n\n# Getting Help\n\nWe use the [GitHub issues](https://github.com/aws/aws-aspnet-cognito-identity-provider/issues) for tracking bugs and feature requests and have limited bandwidth to address them.\n\nIf you think you may have found a bug, please open an [issue](https://github.com/aws/aws-aspnet-cognito-identity-provider/issues/new)\n\n# Contributing\n\nWe welcome community contributions and pull requests. See\n[CONTRIBUTING](./CONTRIBUTING.md) for information on how to set up a development\nenvironment and submit code.\n\n# Additional Resources\n\n[AWS .NET GitHub Home Page](https://github.com/aws/dotnet)  \nGitHub home for .NET development on AWS. You'll find libraries, tools, and resources to help you build .NET applications and services on AWS.\n\n[AWS Developer Center - Explore .NET on AWS](https://aws.amazon.com/developer/language/net/)  \nFind all the .NET code samples, step-by-step guides, videos, blog content, tools, and information about live events that you need in one place. \n\n[AWS Developer Blog - .NET](https://aws.amazon.com/blogs/developer/category/programing-language/dot-net/)  \nCome see what .NET developers at AWS are up to!  Learn about new .NET software announcements, guides, and how-to's.\n\n[@dotnetonaws](https://twitter.com/dotnetonaws)  \nFollow us on twitter!\n\n# License\n\nLibraries in this repository are licensed under the Apache 2.0 License. \n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.\n", "release_dates": []}, {"name": "aws-auto-scaling-custom-resource", "description": "Libraries, samples, and tools to help AWS customers onboard with custom resource auto scaling.", "language": "Python", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# Getting Started with Application Auto Scaling Custom Resources\n\nThis *aws-auto-scaling-custom-resource* repository contains AWS CloudFormation templates and instructions to build, test, and remove automatic scaling for custom resources by using AWS serverless functions. In this context, a *custom resource* is an object that allows you to use the automatic scaling features of Application Auto Scaling with your own application or service.\n\nAn AWS CloudFormation template is a JSON or YAML-formatted text file that describes the AWS infrastructure needed to run an application or service along with any interconnections among infrastructure components. The included AWS CloudFormation template launches a collection of AWS resources, including a new Amazon API Gateway REST API, as a stack. The API Gateway REST API allows secure access to scalable resources in your application or service.\n\nWhen everything is deployed and configured, you'll have the following environment in your AWS account.\n\n![Image of Application Auto Scaling Custom Resource Environment](https://github.com/aws/aws-auto-scaling-custom-resource/blob/master/DESIGN.PNG)\n\nYou can use the *aws-auto-scaling-custom-resource* repository and the following procedures as the starting point for your customizations. More information about this approach to custom resource auto scaling is detailed in this [Auto Scaling Production Services on Titus](https://medium.com/netflix-techblog/auto-scaling-production-services-on-titus-1f3cd49f5cd7) blog post.\n\nIf you encounter an issue with the CloudFormation template, or if you are unsure how to solve a problem, we want to hear about it. Please create a GitHub issue. We welcome all feedback, pull requests, and other contributions.\n\n# Audience\n\n* Recommended for a technical audience looking to use Application Auto Scaling to configure automatic scaling for in-house applications and services.\n* Assumes experience with AWS, including configuring automatic scaling with target tracking scaling policies and custom metrics.\n* Assumes knowledge of Amazon API Gateway, CloudWatch, Lambda, and Open API Specification (aka Swagger 2.0 specs).\n\n# AWS Services Used\n\nThe AWS components used to create this serverless architecture include the following AWS services.\n\n* [Amazon API Gateway](https://aws.amazon.com/api-gateway/) \n* [Application Auto Scaling](https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html) \n* [AWS CloudFormation](https://aws.amazon.com/cloudformation/)\n* [AWS CloudTrail](https://aws.amazon.com/cloudtrail/) \n* [Amazon CloudWatch](https://aws.amazon.com/cloudwatch/details/) \n* [AWS Lambda](https://aws.amazon.com/lambda/) \n* [Amazon Simple Notification Service (SNS)](https://aws.amazon.com/sns/)\n\n# Regional Availability\n\nCustom resource auto scaling is available in Canada (Central), US West (N. California), US East (N. Virginia), US East (Ohio), US West (Oregon), South America (Sao Paulo), EU (Frankfurt), EU (Ireland), EU (London), EU (Paris), Asia Pacific (Mumbai), Asia Pacific (Seoul), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), and China (Beijing).\n\n# First Steps\n\n* AWS Command Line Interface [installed](https://docs.aws.amazon.com/cli/latest/userguide/installing.html) and [configured](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html).  \n* [Postman](https://www.getpostman.com/) installed.\u200a This tool allows you to test your API endpoints and observe the responses. Postman is a convenient testing tool because it provides fields for adding your [Signature Version 4](https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html) signing information to an HTTPS request.\n* Make sure that you have permissions to create a stack using a CloudFormation template, plus full access permissions to resources within the stack.\n\n# Set Up the AWS Serverless Functions\n\nThis section describes how to set up and test the required serverless functions in your AWS account. \n\n## Step 1: Test Your REST Endpoint URL \n\nThe provided CloudFormation template defines the serverless functions that will be created. Before you launch the stack, you must verify that your backend system conforms to the API specification that's supported by the template. (Alternatively, if you need a test environment and are familiar with Docker, a sample REST endpoint is provided as a Dockerized Apache Python CGI. For more information, see [sample-api-server](./sample-api-server/).)\n\n1. Configure the GET and PATCH request methods to connect with the Amazon API Gateway. For more information on these methods, see the `CustomResourceEndpoint` section of the [custom-resource-stack.yaml](https://github.com/aws/aws-auto-scaling-custom-resource/blob/master/cloudformation/templates/custom-resource-stack.yaml) template.\n1. Verify that your backend system's REST endpoint URL works by using the following command to issue GET and PATCH requests to it.\n\n```\n$ curl -i -X GET --header 'Accept: application/json' 'http://api.example.com/v1/scalableTargetDimensions/myservice'\n```\n\nIf the endpoint is set up properly, it should return a standard `200 OK` response message and a payload that represents the requested resource and its status.\n\nThe following is an example GET and PATCH response.\n\n```json\n{\n  \"actualCapacity\": 2.0,\n  \"desiredCapacity\": 2.0,\n  \"dimensionName\": \"MyDimension\",\n  \"resourceName\": \"MyService\",\n  \"scalableTargetDimensionId\": \"1-23456789\",\n  \"scalingStatus\": \"Successful\",\n  \"version\": \"MyVersion\"\n}\n```\n\n## Step 2: Launch the CloudFormation Stack\n\nNOTE: You may incur AWS charges as part of this deployment. Please monitor your Free Tier usage and make sure that you understand the AWS charges involved.\n\n1. Download the [custom-resource-stack.yaml](https://github.com/aws/aws-auto-scaling-custom-resource/blob/master/cloudformation/templates/custom-resource-stack.yaml) CloudFormation template from GitHub.\n1. Run the following [create-stack](https://docs.aws.amazon.com/cli/latest/reference/cloudformation/create-stack.html) command, adding your details as follows:\n- For `SNSSubscriptionEmail`, replace `email-address` with the email address where you want certificate expiry notifications to be sent.\n- For `IntegrationHttpEndpoint`, replace `endpoint-url` with your REST endpoint URL. For example, `http://api.example.com/v1/scalableTargetDimensions/{scalableTargetDimensionId}` where *{scalableTargetDimensionId}*  is replaced with the dimension in your backend API. The resulting URL would look something like: `http://api.example.com/v1/scalableTargetDimensions/1-23456789`.\n- (Optional) Change the [AWS Region](https://docs.aws.amazon.com/general/latest/gr/rande.html) by updating the `--region` value. The examples in this repository use `us-west-2`, but the steps are the same if you deploy into a different region. \n\nThe following example shows a sample create-stack command.\n\n```\n$ aws cloudformation create-stack \\\n    --stack-name CustomResourceAPIGatewayStack \\\n    --template-body file://~/custom-resource-stack.yaml \\\n    --region us-west-2 \\\n    --capabilities CAPABILITY_NAMED_IAM CAPABILITY_IAM \\\n    --parameters \\         \n        ParameterKey=SNSSubscriptionEmail,ParameterValue=\"email-address\" \\\n        ParameterKey=IntegrationHttpEndpoint,ParameterValue='\"endpoint-url\"'\n```\nThe stack takes only a few minutes to deploy. It creates a new REST API in API Gateway with two stages: \u201cPreProd\u201d and \u201cProd\u201d. Each stage is deployed with its own client-side certificate. With the serverless framework, you can deploy to different environments using stages. As your API evolves, these stages can be helpful for testing and development.\n\nWhen the deployment has completed successfully, you receive an email to confirm a subscription to the Amazon SNS topic created by the template. Choose the **Confirm subscription** link in the message to subscribe to emails that are sent whenever there is an expiring certificate. A Lambda function checks once a day to see if the client certificate is expiring in 7, 3, or 1 days.\n\nThe following is the full list of created resources:\n\n- AWS::ApiGateway::RestApi\n- AWS::ApiGateway::Stage \n- AWS::ApiGateway::ClientCertificate\n- AWS::ApiGateway::Deployment\n- AWS::ApiGateway::Account\n- AWS::Lambda::Function\n- AWS::SNS::Topic\n- AWS::IAM::Role (A service role to grant Lambda permission to call API Gateway and SNS)\n- AWS::IAM::Role (A CloudWatch role required for certificate expiry checks)\n- AWS::Events::Rule (A CloudWatch rule required for certificate expiry checks)\n- AWS::Lambda::Permission (Permissions for CloudWatch to invoke the Lambda function)\n- AWS::CloudTrail::Trail\n- AWS::S3::Bucket (An S3 bucket to receive the log files for CloudTrail)\n- AWS::S3::BucketPolicy (An S3 bucket policy that grants access only to the created CloudTrail)\n\n## Step 3: Gather the Stack's Output\n\nTo continue with these steps, you need the HTTPS prefixes of your new REST API in API Gateway and, optionally, the IDs of the client certificates from the stack. \n\nRun the following [describe-stacks](https://docs.aws.amazon.com/cli/latest/reference/cloudformation/describe-stacks.html) command and copy the output. \n\n```\n$ aws cloudformation describe-stacks --region us-west-2 --stack-name CustomResourceAPIGatewayStack  | jq '.Stacks[0][\"Outputs\"]'\n```\n\nThis returns a response similar to the following example.\n\n```json\n[\n  {\n    \"Description\": \"Application Auto Scaling Resource ID prefix for Preprod\",\n    \"OutputValue\": \"https://example.execute-api.us-west-2.amazonaws.com/preprod/scalableTargetDimensions/\",\n    \"OutputKey\": \"PreProdResourceIdPrefix\"\n  },\n  {\n    \"OutputValue\": \"customresourceapigatewaystack-s3bucket-ha8id2l1wpo6\",\n    \"OutputKey\": \"S3BucketName\"\n  },\n  {\n    \"Description\": \"Application Auto Scaling Resource ID prefix for Prod\",\n    \"OutputValue\": \"https://example.execute-api.us-west-2.amazonaws.com/prod/scalableTargetDimensions/\",\n    \"OutputKey\": \"ProdResourceIdPrefix\"\n  },\n{\n   \"Description\": \"API Gateway Client Cert\",\n    \"OutputKey\": \"PreProdClientCertificate\",\n    \"OutputValue\": \"MIIDoTCCAwqgAwIBAgIMCRkox...tt3rdw\"\n  },\n  {\n    \"Description\": \"API Gateway Client Cert\",\n    \"OutputKey\": \"ProdClientCertificate\",\n    \"OutputValue\": \"MIIDVDCCAr0CAQAweTEeMBwG...frw3tnx\"\n  }\n]\n```\n\n## Step 4: (Optional) Configure Backend Authentication\n\nBy default, client-side SSL certificates are used to authenticate the API Gateway API when it connects to your backend system. You can also use the certificates' public keys in the backend to verify that HTTP requests to your backend system are from API Gateway.\n\nTo extract the PEM-encoded public key of the SSL certificate, run the following AWS CLI commands, replacing the `client-certificate-id` with the values for `ProdClientCertificate` and `PreProdClientCertificate` from the stack output. \n\n```\naws apigateway get-client-certificate \\\n--client-certificate-id MIIDVDCCAr0CAQAweTEeMBwG...frw3tnx \\\n--output text\naws apigateway get-client-certificate \\\n--client-certificate-id MIIDoTCCAwqgAwIBAgIMCRkox...tt3rdw \\\n--output text\n```\n\nThe output can be used to configure your backend system to verify the client SSL certificate. For more information, see [Generate and Configure an SSL Certificate for Backend Authentication](https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-client-side-ssl-authentication.html) in the *Amazon API Gateway Developer Guide*.\n\n## Step 5: Test the Endpoint Integration\n\nThe next step is to verify that the API Gateway API is integrated with your backend system.\n\n1. Create the string that identifies the path to the custom resource through the API Gateway (the Resource ID). The Resource ID has the following syntax: `[OutputValue][identifier]`. \n   - The `OutputValue` is the \"Prod\" HTTPS prefix from the stack output.   \n   - The `identifier` is a string that identifies a scalable resource in your backend system (the value for *scalableTargetDimensionId* from step 1). This example shows a sample string with *1-23456789* as the identifier in your backend system:\n`https://example.execute-api.us-west-2.amazonaws.com/prod/scalableTargetDimensions/1-23456789`\n1. Follow the instructions in [Use Postman to Call an API](https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-use-postman-to-call-api.html) to send a test request in Postman. If you prefer to view the headers and body, you can convert the response to CURL by using the Postman code snippet generator. \n\nThe following example shows the Postman CURL response to a GET request.\n\n``` \ncurl -X GET \\ https://example.execute-api.us-west-2.amazonaws.com/prod/scalableTargetDimensions/1-23456789 \\\n  -H 'Authorization: AWS4-HMAC-SHA256 Credential=example/20180704/us-west-2/execute-api/aws4_request, SignedHeaders=content-type;host;x-amz-date;x-amz-security-token, Signature=SIGNATURE' \\\n  -H 'Cache-Control: no-cache' \\\n  -H 'Content-Type: application/x-www-form-urlencoded' \\\n  -H 'Host: example.execute-api.us-west-2.amazonaws.com' \\\n  -H 'Postman-Token: POSTMANTOKEN' \\\n  -H 'X-Amz-Date: 20180704T023500Z' \\\n  -H 'X-Amz-Security-Token: SESSIONTOKEN'\n{\n  \"actualCapacity\": 2.0,\n  \"desiredCapacity\": 2.0,\n  \"dimensionName\": \"MyDimension\",\n  \"resourceName\": \"MyService\",\n  \"scalableTargetDimensionId\": \"1-23456789\",\n  \"scalingStatus\": \"Successful\",\n  \"version\": \"MyVersion\"\n}\n```\n\n# Configure Automatic Scaling\n\nThis section describes how to define the scaling policy that Application Auto Scaling uses to scale your application resources.\n\n## Prerequisites\n\nBefore you begin, verify that you have completed all of the steps in the procedure above. Also verify that you have the permissions that allow you to configure automatic scaling and create the required service-linked role. For details on the required permissions, see the [Authentication and Access Control](https://docs.aws.amazon.com/autoscaling/application/userguide/auth-and-access-control.html) topic in the *Application Auto Scaling User Guide*. Be sure to use the correct permissions when registering a scalable target, so that the service-linked role is automatically created. Otherwise, the scaling function will not work. \n\nYou must also have permissions to [publish metrics (PutMetricData)](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/permissions-reference-cw.html) to CloudWatch.\n\n## Step 1: Register Your Scalable Target\n\nRegister your resource as a scalable target with Application Auto Scaling. A scalable target is a resource that Application Auto Scaling can scale out or scale in.\n\nTo get started, run the following command to save your Resource ID in a txt file (with no newline character at the end of the file).\n\nThe command will look like the following example, but with your Resource ID.\n\n```\n$ echo -n \"https://example.execute-api.us-west-2.amazonaws.com/prod/scalableTargetDimensions/1-23456789\" > ~/custom-resource-id.txt\n```\n\nRunning the command saves the file as `custom-resource-id.txt` in your home directory. You can now use the [register-scalable-target](https://docs.aws.amazon.com/cli/latest/reference/application-autoscaling/register-scalable-target.html) command to register your scalable target, as shown in the following example.\n\n```\n$ aws application-autoscaling register-scalable-target \\\n--service-namespace custom-resource \\\n--scalable-dimension custom-resource:ResourceType:Property \\\n--resource-id file://~/custom-resource-id.txt \\\n--min-capacity 0 --max-capacity 10 \\\n--region us-west-2\n```\n\nThis registers your scalable target with Application Auto Scaling, and allows it to manage capacity within the range of 0 to 10 capacity units. \n\n## Step 2: Create a Target Tracking Scaling Policy\n\nCreate a scaling policy for your custom resource that specifies how the scalable target should be scaled when CloudWatch alarms are triggered. \n\nFor target tracking, you can define a scaling policy that meets your resource's specific requirements by creating a custom metric. You can define your custom metric based on any metric that changes in proportion to scaling. \n\nHowever, not all metrics work for target tracking. The metric must be a valid utilization metric, and it must describe how busy your custom resource is. The value of the metric must increase or decrease in inverse proportion to the number of capacity units. That is, the value of the metric should decrease when capacity increases and increase when capacity decreases. \n\nThe following `cat` command creates a sample metric for your scalable target in a `config.json` file in your home directory:\n\n```\n$ cat ~/config.json\n{\n   \"TargetValue\":50,\n   \"CustomizedMetricSpecification\":{\n      \"MetricName\":\"MyAverageUtilizationMetric\",\n      \"Namespace\":\"MyNamespace\",\n      \"Dimensions\":[\n         {\n            \"Name\":\"MyMetricDimensionName\",\n            \"Value\":\"MyMetricDimensionValue\"\n         }\n      ],\n      \"Statistic\":\"Average\",\n      \"Unit\":\"Percent\"\n   }\n}\n```\n\nUse the following [put-scaling-policy](https://docs.aws.amazon.com/cli/latest/reference/application-autoscaling/put-scaling-policy.html) command, along with the `config.json` file that you created previously, to create a scaling policy named `custom-tt-scaling-policy` that keeps the average utilization of your custom resource at 50 percent:\n\n```\n$ aws application-autoscaling put-scaling-policy \\\n--policy-name custom-tt-scaling-policy \\\n--policy-type TargetTrackingScaling \\\n--service-namespace custom-resource \\\n--scalable-dimension custom-resource:ResourceType:Property \\\n--resource-id file://~/custom-resource-id.txt \\\n--target-tracking-scaling-policy-configuration file://~/config.json \\\n--region us-west-2\n```\nThis creates two alarms: one for scaling out and one for scaling in. It also returns the Amazon Resource Name (ARN) of the policy that is registered with CloudWatch, which CloudWatch uses to invoke scaling whenever the metric is in breach. \n\nYou should see output similar to the following example.\n\n```json\n{\n   \"Alarms\": [\n        {\n            \"AlarmName\": \"TargetTracking-https://example.execute-api.us-west-2.amazonaws.com/prod/scalableTargetDimensions/1-23456789-AlarmHigh-b9d32d65-78bb-4d01-8931-d67d10f87052\",\n            \"AlarmARN\": \"arn:aws:cloudwatch:us-west-2:544955126770:alarm:TargetTracking-https://example.execute-api.us-west-2.amazonaws.com/prod/scalableTargetDimensions/1-23456789-AlarmHigh-b9d32d65-78bb-4d01-8931-d67d10f87052\"\n        },\n        {\n            \"AlarmName\": \"TargetTracking-https://example.execute-api.us-west-2.amazonaws.com/prod/scalableTargetDimensions/1-23456789-AlarmLow-a9f90ec7-dccd-4a66-83ea-26bf3f0134dc\",\n            \"AlarmARN\": \"arn:aws:cloudwatch:us-west-2:544955126770:alarm:TargetTracking-https://example.execute-api.us-west-2.amazonaws.com/prod/scalableTargetDimensions/1-23456789-AlarmLow-a9f90ec7-dccd-4a66-83ea-26bf3f0134dc\"\n        }\n    ],\n    \"PolicyARN\": \"arn:aws:autoscaling:us-west-2:544955126770:scalingPolicy:ac852aff-b04f-427d-a80a-3e7ef31d492d:resource/custom-resource/https://example.execute-api.us-west-2.amazonaws.com/prod/scalableTargetDimensions/1-23456789:policyName/custom-tt-scaling-policy\"\n}\n```\n\nFor more information about creating target tracking scaling policies, see [Target Tracking Scaling Policies](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html) in the Application Auto Scaling documentation.\n\nFor more information about creating custom metrics, see [Publish Custom Metrics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html) in the CloudWatch documentation. \n\n## Step 3: Trigger the Scaling Policy Using a Bash Script\n\nTest your scaling policy by publishing sample metric data to CloudWatch. CloudWatch alarms trigger the scaling policy and calculate the scaling adjustment based on the metric and the target value. \n\nRun the following bash script at the command line:\n\n```bash\n// Command to put metric data that breaches AlarmHigh\n$ while sleep 3\ndo\n  aws cloudwatch put-metric-data \\\n  --metric-name MyAverageUtilizationMetric \\\n  --namespace MyNamespace --value 70 \\\n  --unit Percent \\\n  --dimensions MyMetricDimensionName=MyMetricDimensionValue\n  echo -n \".\"\ndone\n```\n\nThis script publishes data points to CloudWatch to trigger the scaling policy based on live metric data. The `while` loop is used to perform the CLI command an unknown number of times. The `sleep 3` condition pauses the execution for 3 seconds on each iteration. After you verify that scaling works (which is the next step in this procedure), press Ctrl+C to stop the script.\n\nIt may take a few minutes before your scaling policy is invoked. When the target ratio exceeds 50 percent for a sustained period of time, Application Auto Scaling notifies your custom resource to adjust capacity upward, so that the 50 percent target utilization can be maintained.\n\n## Step 4: View the Scaling Activities \n\nView the scaling activities that were created in response to the bash script in the previous step.\n\nRun the [describe-scaling-activities](https://docs.aws.amazon.com/cli/latest/reference/application-autoscaling/describe-scaling-activities.html) command, as shown in the following example.\n\n```\n$ aws application-autoscaling describe-scaling-activities \\\n--service-namespace custom-resource \\\n--resource-id file://~/custom-resource-id.txt \\\n--max-results 20 --region us-west-2\n```\n\nYou should eventually see output similar to the following example.\n```JSON\n{\n    \"ScalingActivities\": [\n        {\n            \"ScalableDimension\": \"custom-resource:ResourceType:Property\",\n            \"Description\": \"Setting desired capacity to 6.\",\n            \"ResourceId\": \"https://example.execute-api.us-west-2.amazonaws.com/prod/scalableTargetDimensions/1-23456789\",\n            \"ActivityId\": \"2fca0873-3e4d-4c05-a83d-40c6394e6b9b\",\n            \"StartTime\": 1530744698.087,\n            \"ServiceNamespace\": \"custom-resource\",\n            \"EndTime\": 1530744730.766,\n            \"Cause\": \"monitor alarm TargetTracking-https://example.execute-api.us-west-2.amazonaws.com/prod/scalableTargetDimensions/1-23456789-AlarmHigh-b9d32d65-78bb-4d01-8931-d67d10f87052 in state ALARM triggered policy custom-tt-scaling-policy\",\n            \"StatusMessage\": \"Successfully set desired capacity to 6. Change successfully fulfilled by custom-resource.\",\n            \"StatusCode\": \"Successful\"\n        }\n    ]\n}\n```\n\nNote: If you are using the [sample-api-server](./sample-api-server/) that is provided in this project, you can also see the scaling activities in the API log.\n## Step 5: Deregister the Scalable Target\n\nTo deregister a scalable target that you no longer need, use the same txt file that you specified to register the scalable target and run the [deregister-scalable-target](https://docs.aws.amazon.com/cli/latest/reference/application-autoscaling/deregister-scalable-target.html) command:\n\n```\n$ aws application-autoscaling deregister-scalable-target \\\n--service-namespace custom-resource \\\n--scalable-dimension custom-resource:ResourceType:Property \\\n--resource-id file://~/custom-resource-id.txt \\\n--region us-west-2\n```\n\nDeregistering a scalable target deletes the scaling policy and the CloudWatch alarms that Application Auto Scaling created on your behalf. \n\n# Documentation\n\nThe [Application Auto Scaling User Guide](https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html) provides in-depth guidance about the Application Auto Scaling service. \n\n# License Summary\n\nThis sample code is made available under a modified MIT-0 license. See the [LICENSE](https://github.com/aws/aws-auto-scaling-custom-resource/blob/master/LICENSE) file.\n", "release_dates": []}, {"name": "aws-cdi-sdk", "description": "AWS Cloud Digital Interface (CDI) SDK. Documentation at: https://aws.github.io/aws-cdi-sdk/mainline/index.html", "language": "C", "license": {"key": "bsd-2-clause", "name": "BSD 2-Clause \"Simplified\" License", "spdx_id": "BSD-2-Clause", "url": "https://api.github.com/licenses/bsd-2-clause", "node_id": "MDc6TGljZW5zZTQ="}, "readme": "# AWS CDI SDK\n\nThe AWS Cloud Digital Interface (CDI) Software Development Kit (SDK) is a set of libraries and documentation for you to build live video solutions on AWS. The AWS CDI SDK provides access to the network performance, reliability, and uncompressed video capabilities required to build applications including TV channel playout, live video production switching, motion graphic insertion, multi-viewers, video frame rate and color space conversion, forensic watermarking, and video decoding and encoding. If you are developing a distributed live video application that runs across multiple compute instances, integrating the AWS CDI SDK into your software application can give you the tools and performance you need. The AWS CDI SDK contains libraries that can be easily integrated into your application.\n\n---\n\n- [AWS CDI SDK](#aws-cdi-sdk)\n- [Glossary](#glossary)\n- [Overview](#overview)\n  - [Features](#features)\n  - [Technical details](#technical-details)\n  - [Related technology](#related-technology)\n- [Installation](#installation)\n  - [Create an AWS account](#create-an-aws-account)\n  - [Create an EFA-enabled instance](#create-an-efa-enabled-instance)\n  - [Additional OS-dependent steps](#additional-os-dependent-steps)\n  - [Creating additional instances](#creating-additional-instances)\n  - [Browse the HTML documentation](#browse-the-html-documentation)\n- [Using the AWS CDI SDK test applications](#using-the-aws-cdi-sdk-test-applications)\n- [Using AWS CDI SDK in an application](#using-aws-cdi-sdk-in-an-application)\n  - [High level overview](#high-level-overview)\n  - [Application programming interface](#application-programming-interface)\n    - [Documentation](#documentation)\n    - [Includes](#includes)\n    - [Libraries](#libraries)\n  - [Example test applications](#example-test-applications)\n  - [Performance metrics in the AWS CDI SDK](#performance-metrics-in-the-aws-cdi-sdk)\n    - [IAM permissions required by the compute instance](#iam-permissions-required-by-the-compute-instance)\n    - [Customer option to disable the collection of performance metrics by the AWS CDI SDK](#customer-option-to-disable-the-collection-of-performance-metrics-by-the-aws-cdi-sdk)\n    - [CDI Between Shared Instances in Shared Subnets](#cdi-between-shared-instances-in-shared-subnets)\n    - [Learn more](#learn-more)\n- [Known issues, limitations and performance](#known-issues-limitations-and-performance)\n  - [Known issues](#known-issues)\n  - [Known limitations](#known-limitations)\n  - [Performance](#performance)\n    - [AWS CDI SDK CPU usage](#aws-cdi-sdk-cpu-usage)\n    - [Other performance considerations](#other-performance-considerations)\n- [Troubleshooting](#troubleshooting)\n  - [Transmitter and receiver failing to communicate](#transmitter-and-receiver-failing-to-communicate)\n  - [CDI enabled application only works when run as root](#cdi-enabled-application-only-works-when-run-as-root)\n  - [Communication failure and firewall settings](#communication-failure-and-firewall-settings)\n- [Security](#security)\n- [Licensing](#licensing)\n- [Issues](#issues)\n\n---\n\n# Glossary\n\n**Amazon EC2** - *Amazon Elastic Compute Cloud.*\n\n**EFA** - *Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at scale on AWS.*\n\n**IAM** - *AWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources.*\n\n**Placement Group** - *Placement groups influence the placement of a group of interdependent instances to meet the needs of your workload.*\n\n**Security Group** - *A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance.*\n\n**SGL** - *Scatter-Gather List, a list of addresses/data sizes stored in non-contiguous memory.*\n\n**SRD** - *Scalable Reliable Datagram (SRD) enhances the performance of inter-instance communications by providing reliable packet delivery.*\n\n**VPC** - *Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways.*\n\n---\n\n# Overview\n\nFor time-sensitive data transmission of uncompressed live video, latency and quality must be optimized.  Historically this type of video workload was built on-premises using Serial Digital Interface (SDI) connections or with dedicated IP networks.\n\nThe AWS CDI SDK is designed to enable you to deploy live uncompressed video solutions in the AWS Cloud through the reliable transmission of blocks of data between Elastic Fabric Adapter (EFA) enabled Amazon EC2 instances.\n\nThis SDK provides an Audio/Video/Metadata (AVM) API to send video data between instances, and a RAW (data is not interpreted in any way) API to send generic data. The AVM API allows you to easily map SMPTE ST-2110 video over IP streams to the AWS CDI SDK for transport.\n\nThe [Installation](#installation) section of this document will help you get started using the AWS CDI SDK.\n\n## Features\n\n* Transfers data between EC2 instances. This includes data of any format: raw packetized video, audio, ancillary data, frame buffers, or compressed data.\n* Schedules and sends packetized streams to remote hosts designed for high reliability for transfer of data using Scalable Reliable Datagram (SRD).\n* A single instance of the AWS CDI SDK supports multiple transmitters and receivers.\n\n## Technical details\n\nThe implementation for this transit occurs over the Scalable Reliable Datagram (SRD) protocol. To achieve the highest performance and lowest latency, the AWS CDI SDK relies on EC2 instances that support the [Elastic Fabric Adapter (EFA)](http://aws.amazon.com/hpc/efa/) and are placed within a single Placement Group.\n\nThe AWS CDI SDK opens one specified User Datagram Protocol (UDP) port per connection to control communication between Amazon EC2 instances running AWS CDI SDK. The receiving side listens on the specified port number. The transmitting side uses a random port number from the ephemeral port range, as determined by the operating system.\n\nFor network security best practices concerning how to block UDP packets from the public Internet, see [Security best practices for your VPC](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.html).\n\nThe AWS CDI SDK also relies on EC2 instances using a Security Group that allows all inbound and outbound traffic to and from the Security Group itself. For more information, see [Prepare an EFA-Enabled Security Group](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start.html#efa-start-security).\n\n## Related technology\n\nIn addition to Amazon's EFA and SRD technology described above, your implementation of the AWS CDI SDK may also involve:\n\n* [libfabric](https://github.com/ofiwg/libfabric) by [OpenFabrics](https://ofiwg.github.io/libfabric/).\n* [EFA driver for Windows](./INSTALL_GUIDE_WINDOWS.md#install-the-windows-efa-driver) (available via AWS S3 Bucket).\n* [AWS CLI version 2](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)\n* [PDCurses](https://pdcurses.org/) in Windows.\n\nInstructions are provided in the documentation below for installing these packages, some of which is published by third parties. If you elect to download and/or use this content, you may be subject to additional terms and conditions. Amazon is not the distributor of content you elect to download from third party sources, and expressly disclaims all liability with respect to such content.\n\n---\n\n# Installation\n\n## Create an AWS account\n\n1. [Create an AWS account](http://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/). Skip this step if you already have an AWS account.\n1. [Get started with EC2](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html), which includes setting up an Identity and Access Management role (IAM), a key pair, a Virtual Private Cloud (VPC), and a Security Group, among other things.\n\n## Create an EFA-enabled instance\n\nFollow these instructions to create an EFA-enabled EC2 instance.\n\n**Note**: When launching a Linux instance, choosing **Amazon Linux 2 (AL2)** is recommended because it is optimized for running on EC2 and is the primary Linux distribution used for the majority of CDI development and testing. When launching a Windows instance, choose **Windows Server 2019**.\nTo launch an EFA-enabled instance, follow the prepare and launch steps in the [launch an EFA-capable instance](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start.html) guide, with the additions described below. Refer to the [Install Guide Linux](INSTALL_GUIDE_LINUX.md) and [Install Guide Windows](INSTALL_GUIDE_WINDOWS.md) for additional information on installing EFA software and the AWS CDI SDK.\n\n1. During step **Prepare an EFA-enabled security group**, when configuring **Inbound rules**, edit the settings for your newly created \u201cEFA\u201d security group.\n    1. Select **Actions->Edit inbound security rules**\n        * Select **Add Rule**\n        * Under **type** select **All traffic**.\n        * Under **source** select **Custom**.\n        * In the field next to **Custom**, start typing **sg** and a dropdown list of your security groups will appear. Select the security group you made for EFA.\n        * Select **Add rule**.\n        * Under **type** select **SSH**.\n        * Under **source**, select **Anywhere**.\n        * Select **Save Rules**.\n        * Below is an example of the inbound rules for a correctly configured security group.\n\n        ![Inbound Rules](doc/inbound_rules.png)\n\n          **Note**: For Windows instances, an additional inbound rule is needed to enable Remote Desktop Protocol (RDP) connections:\n        * Select **Add rule**.\n        * Under **type** select **RDP**.\n        * Under **source**, select **Anywhere**.\n        * Click **Save Rules**.\n    2. Select **Actions->Edit outbound security rules**\n        * Select **Add rule**.\n        * Under **type** select **All traffic**.\n        * Under **source** select **Custom**.\n        * In the field next to **Custom**, start typing **sg** and a dropdown list of your security groups will appear. Select the security group you made for EFA.\n        * Select **Save Rules**.\n        * This is an example of the outbound rules for a correctly configured security group:\n\n        ![Outbound Rules](doc/outbound_rules.png)\n2. During step **Launch a temporary instance** under **Configure Instance Details**:\n    1. Choose a VPC. AWS provides default VPCs for all accounts for all regions, but you may create a new VPC for this exercise. For more information on how to create a vpc, see [Virtual Private Clouds](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-vpc.html). If you don\u2019t already have a subnet for the chosen VPC, you can select **Create new subnet** on this page.\n    1. If access to this instance from outside the Amazon network is needed, enable **Auto-assign public IP.**\n    1. Make sure to enable EFA by checking the **Elastic Fabric Adapter** checkbox here. **Note**: To enable the checkbox, you must select the subnet even if using the default subnet value.\n    1. Amazon recommends putting EFA-enabled instances using AWS CDI SDK in a placement group, so select or create one under **Placement Group \u2013 Add instance to placement group.**  The **Placement Group Strategy** should be set to **cluster**.\n\n## Additional OS-dependent steps\n\nDepending on your operating system, follow the AWS CDI SDK installation instructions for [Linux](./INSTALL_GUIDE_LINUX.md) or [Windows](./INSTALL_GUIDE_WINDOWS.md).\n\n## Creating additional instances\n\nIn order to use AWS CDI SDK in an application, two instances are needed.\nTo create an additional instance:\n\n  1. Select the previous instance in the EC2 console, and then select **Launch More Like This** in the **Action** menu.\n  1. Perform the [OS-Dependent steps above](#additional-os-dependent-steps).\n  1. This instance will now be considered the receiver.\n\n## Browse the HTML documentation\n\nThe HTML documentation provides a complete overview for using the AWS CDI SDK.\nDocumentation builds during installation and can be rebuilt any time using these [Linux](./INSTALL_GUIDE_LINUX.md#build-the-html-documentation) or [Windows](./INSTALL_GUIDE_WINDOWS.md#build-the-html-documentation) instructions.\nOnce built, the HTML documentation will be located at aws-cdi-sdk/build/documentation/index.html.\n\n---\n\n# Using the AWS CDI SDK test applications\n\nThe AWS CDI SDK comes with test applications for confirming correct installation as well as providing example usage to assist with application development. Please see the [Test Application User Guide](./USER_GUIDE_TEST_APP.md) for details.\n\n---\n\n# Using AWS CDI SDK in an application\n\n## High level overview\n\nAn application uses AWS CDI SDK API\u2019s to create transmitter instances to send payloads or receiver instances to receive payloads. Here is a diagram of the program flow:\n\n![CDI Raw API Workflow](./doc/raw_api_workflow.jpg)\n\n## Application programming interface\n\n### Documentation\n\nThe API documentation can be found in the [HTML Documentation](#browse-the-html-documentation).\n\n### Includes\n\nIncludes can be found at the following paths:\n\n* aws-cdi-sdk/include\n* aws-cdi-sdk/src/common/include\n\n### Libraries\n\nAfter the libraries have been built, library locations that are referenced by aws-cdi-sdk can be found at the following paths:\n\n1. Linux:\n    1. libfabric.so.x: build/debug|release/lib\n    1. libcdisdk.so.x.x: build/debug|release/lib\n1. Windows:\n    1. proj/x64/Debug/cdi_sdk.lib\n\n## Example test applications\n\n* The source code for the CDI test application, ```cdi_test```, is located at ```aws-cdi-sdk/src/test```.\n* The source code used to create the minimal test applications, ```cdi_test_min_tx``` and ```cdi_test_min_rx```, can be used as simplified usage examples. The source code is located at ```aws-cdi-sdk/src/test_minimal```.\n* Example usage of the SDK can be found in the [test applications](USER_GUIDE_TEST_APP.md).\n* The source code for the NDI/CDI test application, ```ndi_test```, is located at ```aws-cdi-sdk/src/ndi_test```. Installation steps and example usage of the application can be found in the [NDI/CDI Test Application User Guide](USER_GUIDE_NDI_TEST_APP.md).\n\n## Performance metrics in the AWS CDI SDK\n\nAt AWS, we develop and launch products and services based on interactions with customers. We use customer feedback to iterate on our offerings. We also collect and use performance metrics to help us to better understand our customers\u2019 needs, diagnose and fix issues, and deliver features that improve the customer experience.\n\nWhen you integrate the AWS CDI SDK into your video applications running on Amazon EC2, the SDK collects related performance metrics about the network traffic such as signal latency, round-trip times, late or dropped frames, errors, and AWS Account IDs (when available). The AWS CDI SDK does not collect your network traffic content, such as the video or audio content you transfer.\n\n### IAM permissions required by the compute instance\n\nIn order for the AWS CDI SDK to be able to connect to the performance metrics service, the instance on which it is running must be assigned an IAM role with at least the ```mediaconnect:PutMetricGroups``` permission.\n\n**Note**: This may result in an IAM warning such as: ```IAM does not recognize one or more actions. The action name might include a typo or might be part of a previewed or custom service```, which can be safely ignored.\n\n### Customer option to disable the collection of performance metrics by the AWS CDI SDK\n\nCustomers control whether to allow the collection of performance metrics by the AWS CDI SDK, and can change their settings at any time. AWS uses the metrics collected by the SDK to help improve the quality of its products and services. The metrics can also assist AWS Support in diagnosing specific issues that AWS customers may encounter when integrating the SDK into EC2 workloads.\n\nShould you choose to, you can disable performance metric collection by the AWS CDI SDK by making the changes below to sections of the source code.\n\n- In the file ```src/cdi/configuration.h```, comment out ```#define METRICS_GATHERING_SERVICE_ENABLED```. For Windows, you may optionally add the preprocessor definition ```CDI_NO_MONITORING``` to the ``cdi_sdk```` Visual Studio project.\n\n**Note**: For the change to take effect, the CDI SDK library and related applications must be rebuilt.\n\nIf you do not make these changes, collection of performance metrics remains enabled and occurs in the background without requiring any additional interaction.\n\nSeparately, customers can also configure the AWS CDI SDK to collect performance metrics to be displayed within their own Amazon CloudWatch account. Instructions to enable and disable the Amazon CloudWatch display are available in the [Linux](./INSTALL_GUIDE_LINUX.md#disabling-the-display-of-performance-metrics-to-your-amazon-cloudwatch-account) and [Windows](./INSTALL_GUIDE_WINDOWS.md#disabling-the-display-of-performance-metrics-to-your-amazon-cloudwatch-account) installation guides.\n\n### CDI Between Shared Instances in Shared Subnets\nCDI can be sent between EC2 instances belonging to different VPCs, but located in the same subnet [shared between the accounts using Resource Access Manager](https://docs.aws.amazon.com/vpc/latest/userguide/example-vpc-share.html).  In this case, in addition to the usual EFA-enabled security group requirements, each EC2 instance security group must also have inbound and outbound rules allowing \"all traffic\" with source or destination of the EFA-enabled security group of the other VPC, referenced with \"account/SG\" notation.\n\n### Learn more\n\nFor more information about how AWS protects data privacy, please see [AWS\u2019s Data Privacy FAQs](https://aws.amazon.com/compliance/data-privacy-faq/).\n\n---\n\n# Known issues, limitations and performance\n\nCommunity reported issues may be found [here](https://github.com/aws/aws-cdi-sdk/issues).\n\n## Known issues\n\n* The maximum data rate supported is approximately 12 Gbps per stream and has been tested with an aggregate of up to 50 Gbps across all streams, in either direction. This assumes the largest size instance that supports EFA, for example, c5n.18xlarge.\n* Windows compilation of the ```cdi_test``` application under the *Debug* configuration does not support 4K bandwidth.\n\n## Known limitations\n\n* Payload frame rates have been tested up to 60 FPS.\n* The maximum payload size tested is 20736000 bytes allowing a 4k video (3840x2160) at 60 frames per second and 3 bytes per pixel.\n\n## Performance\n\nThe best performance for transferring data between instances is *within a **cluster** placement group*. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload.\n\n### AWS CDI SDK CPU usage\n\nWhen using the EFA, a user space poll mode network driver is employed. Each receive connection uses a thread that is dedicated to constantly polling the network interface for received packets and processing them when they arrive. A similar thread is used on the transmit side. This thread is put to sleep when no transmitted packets are awaiting acknowledgment of receipt by the receiving end. The thread used by the connection can be either dedicated or shared with other connections as specified at connection creation time. On Linux-based platforms, the CPU core dedicated to running the threads can also be specified. In this case, the operating system should be set up to prevent other processes from being scheduled on the cores.\n\nNote that when using a shared thread that contains both transmit and receive connections the thread will be constantly running in order to process the receive connections.\n\nWhen using a thread dedicated to a single connection, general guidance is that receiving a 2160p60 stream (20736000 bytes per frame) consumes approximately 80% of a CPU core while 1080p60 requires about 20%. The load scales roughly linearly with the number of packets processed. Therefore, 2160p60 should be about four times that of 1080p60.\n\nThe AWS CDI SDK has other threads running besides the poll threads to perform processing, such as packetization and execution of application callback functions.\n\n### Other performance considerations\n\nThe network driver imposes a limit on the number of scatter gather list (SGL) entries per packet. As a result, the best performance is achieved if the size of SGL entries passed into the AWS CDI API is at least one third the size of the largest network packet size. This works out to be approximately 3 kB per SGL entry. Smaller SGL entries are supported, but will require additional network packets.\n\nDebug builds add additional run-time overhead as determined by the compiler. The best performance is achieved using release builds.\n\n---\n\n# Troubleshooting\n\nThese are some common issues you might encounter when testing AWS CDI SDK.\n\n## Transmitter and receiver failing to communicate\n\nIf there is a problem with the transmitter and receiver communication, it can manifest in some of the following ways:\n\n* For EFA tests, the console output repeatedly logging the following lines and then eventually timing out:\n\n```bash\n[ProbeTxControlProcessProbeMode:234] Probe Tx mode[SendReset]\n[ProbeControlSendCommand:220] Probe Tx sending command[Reset] to Rx. packet_num[20] ack[1]\n```\n\n* For socket (non-EFA) tests, the console output gets stuck at the following line without showing a running count of the packets received:\n\n```bash\n[RxCreateInternal:343] Successfully created Rx connection. Name[Rx_0]\n```\n\n* For ```fi_pingpong``` via ```efa_test.sh``` test, if returned exit code is non-zero, such as ```test returned 5``` or ```prov_error 13```:\n\n```bash\n$ ./efa_test.sh\nStarting server...\nStarting client...\n[error] .../libfabric/util/pingpong.c:1063: cq_readerr: unknown error\nError: fi_pingpong test returned 5.\n\n# increase FI log verbosity by setting ENV VAR and executing efa_test.sh again:\nexport FI_LOG_LEVEL=warn\n\n$ ./efa_test.sh\nStarting server...\nStarting client...\nlibfabric:1428:core:core:fi_getinfo_():962<warn> fi_getinfo: provider ofi_rxm returned -61 (No data available)\nlibfabric:1428:ofi_mrail:fabric:mrail_get_core_info():288<warn> OFI_MRAIL_ADDR_STRC env variable not set!\nlibfabric:1428:core:core:fi_getinfo_():962<warn> fi_getinfo: provider ofi_mrail returned -61 (No data available)\nlibfabric:1428:core:mr:ofi_uffd_init():346<warn> syscall/userfaultfd Operation not permitted\nlibfabric:1428:efa:cq:rxr_cq_handle_cq_error():341<warn> fi_cq_readerr: err: Input/output error (5), prov_err: unknown error (13)\nlibfabric:1428:efa:cq:rxr_cq_handle_tx_error():215<warn> rxr_cq_handle_tx_error: err: 5, prov_err: Unknown error -13 (13)\n[error] .../libfabric/util/pingpong.c:1063: cq_readerr: unknown error\nError: fi_pingpong test returned 5.\n```\n\nIf any of these issues occurs, check your firewall settings. There are three different locations where the packets could be filtered: EC2 Security Groups, VPC Network Access Control Lists (ACLs), or a firewall on the Linux or Windows instance itself.\n\n**Critical**\n> Ensure the EC2 Security Group allows **all traffic** for both inbound and outbound traffic to and from the Security Group itself.\n\nFor socket-based communication and for the control ports for EFA-based communication, make sure that UDP traffic is allowed to access that port from the instance you are sending from. The easiest way to ensure this is to use the same security group on all EC2 instances that you have communicating with each other, and then make sure to use private IPv4 addresses instead of public ones. If you use public IP addresses, you may need to add explicit entries for any IP addresses that you want to allow traffic from. Note that for the control ports for EFA, there is bidirectional communication between the instances, so make sure that the transmitter and receiver instances have ports open for each other. The security group configuration can be found in the EC2 section of the AWS console.\n\nAlso, ensure that the traffic is not being blocked by network ACLs. The network ACL configuration can be found in the VPC section of the AWS console.\n\n## CDI enabled application only works when run as root\n\n**Note**\n> This issue is now handled automatically by `efa-config v1.12` or newer package within the `efa-installer v1.21` or newer installer. A file is now created here: `/etc/systemd/system.conf.d/01-efa.conf` containing these limits. Ensure you reboot to take effect.\n\nThe ```efa-config``` package which is part of the ```efa-installer```, automates the deployment of required ulimits to ```/etc/security/limits.d/01_efa.comf```. However these system-wide ulimits are ignored if the CDI-SDK integrated application is invoked within a GUI (instead of SSH/daemon) on Linux, as the DM (DisplayManager) is owned by ```systemd``` which has its own ulimits at system and user level. The solution is to apply the identical ulimits at the ```systemd``` **system** level only. The ```efa-config``` package may automate this fix in the future. In the example below, we apply the required ulimits at the ```systemd``` **system** level for all users.\n\n```bash\nsudo mkdir /etc/systemd/system.conf.d\nsudo vi /etc/systemd/system.conf.d/limits.conf\n# add the following lines to limits.conf file\n[Manager]\nDefaultLimitNOFILE=8192\nDefaultLimitMEMLOCK=infinity\n# reboot system to take effect\nsudo reboot\n```\n\n## Communication failure and firewall settings\n\nVerify that a firewall on the Linux or Windows instance itself is not blocking the traffic. To check the firewall rules on a Linux instance, run the following command:\n\n```bash\nsudo iptables -L\n```\n\nOn Windows launch the **Windows Defender Firewall** application to control the firewall settings. See [Allow test applications in Windows firewall](./INSTALL_GUIDE_WINDOWS.md#allow-test-applications-in-windows-firewall) for specific instructions.\n\n# Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n# Licensing\n\nThis library is licensed under the BSD-2-Clause license. See [LICENSE](LICENSE) for more details.\n\n# Issues\n\nFor reporting bugs or feature requests please review our [guidelines](CONTRIBUTING.md#reporting-bugs/features-requests). After reviewing our guidelines [report issues here](https://github.com/aws/aws-cdi-sdk/issues).\n\nSee [CONTRIBUTING](CONTRIBUTING.md) for general contribution guidelines.\n\nSupport for AWS customers can be found through the [AWS console](https://console.aws.amazon.com/support/home#/).\n", "release_dates": ["2024-02-02T19:09:42Z", "2023-05-30T18:59:49Z", "2023-01-23T22:43:08Z", "2022-11-14T17:26:25Z", "2022-04-21T17:37:22Z", "2021-11-29T20:07:42Z", "2021-08-31T18:06:26Z", "2021-05-19T20:44:17Z", "2021-01-08T23:59:36Z", "2020-12-17T19:06:14Z", "2020-09-23T22:06:04Z"]}, {"name": "aws-cdk", "description": "The AWS Cloud Development Kit is a framework for defining cloud infrastructure in code", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Cloud Development Kit (AWS CDK)\n\n![Build Status](https://codebuild.us-east-1.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiSy9rWmVENzRDbXBoVlhYaHBsNks4OGJDRXFtV1IySmhCVjJoaytDU2dtVWhhVys3NS9Odk5DbC9lR2JUTkRvSWlHSXZrNVhYQ3ZsaUJFY3o4OERQY1pnPSIsIml2UGFyYW1ldGVyU3BlYyI6IlB3ODEyRW9KdU0yaEp6NDkiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=main)\n[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/aws/aws-cdk)\n[![NPM version](https://badge.fury.io/js/aws-cdk.svg)](https://badge.fury.io/js/aws-cdk)\n[![PyPI version](https://badge.fury.io/py/aws-cdk-lib.svg)](https://badge.fury.io/py/aws-cdk-lib)\n[![NuGet version](https://badge.fury.io/nu/Amazon.CDK.Lib.svg)](https://badge.fury.io/nu/Amazon.CDK.Lib)\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/software.amazon.awscdk/aws-cdk-lib/badge.svg)](https://maven-badges.herokuapp.com/maven-central/software.amazon.awscdk/aws-cdk-lib)\n[![Go Reference](https://pkg.go.dev/badge/github.com/aws/aws-cdk-go/awscdk/v2.svg)](https://pkg.go.dev/github.com/aws/aws-cdk-go/awscdk/v2)\n[![Mergify](https://img.shields.io/endpoint.svg?url=https://gh.mergify.io/badges/aws/aws-cdk&style=flat)](https://mergify.io)\n\n[![View on Construct Hub](https://constructs.dev/badge?package=aws-cdk-lib)](https://constructs.dev/packages/aws-cdk-lib)\n\nThe **AWS Cloud Development Kit (AWS CDK)** is an open-source software development\nframework to define cloud infrastructure in code and provision it through AWS CloudFormation.\n\nIt offers a high-level object-oriented abstraction to define AWS resources imperatively using\nthe power of modern programming languages. Using the CDK\u2019s library of\ninfrastructure constructs, you can easily encapsulate AWS best practices in your\ninfrastructure definition and share it without worrying about boilerplate logic.\n\nThe CDK is available in the following languages:\n\n* JavaScript, TypeScript ([Node.js \u2265 14.15.0](https://nodejs.org/download/release/latest-v14.x/))\n  * We recommend using a version in [Active LTS](https://nodejs.org/en/about/previous-releases)\n* Python ([Python \u2265 3.8](https://www.python.org/downloads/))\n* Java ([Java \u2265 8](https://www.oracle.com/technetwork/java/javase/downloads/index.html) and [Maven \u2265 3.5.4](https://maven.apache.org/download.cgi))\n* .NET ([.NET \u2265 6.0](https://dotnet.microsoft.com/download))\n* Go ([Go \u2265 1.16.4](https://golang.org/))\n\nThird-party Language Deprecation: language version is only supported until its EOL (End Of Life) shared by the vendor or community and is subject to change with prior notice.\n\n\\\nJump To:\n[Developer Guide](https://docs.aws.amazon.com/cdk/latest/guide) |\n[API Reference](https://docs.aws.amazon.com/cdk/api/v2/docs/aws-construct-library.html) |\n[Getting Started](#getting-started) |\n[Getting Help](#getting-help) |\n[Contributing](#contributing) |\n[RFCs](https://github.com/aws/aws-cdk-rfcs) |\n[Roadmap](https://github.com/aws/aws-cdk/blob/main/ROADMAP.md) |\n[More Resources](#more-resources)\n\n-------\n\nDevelopers use the [CDK framework] in one of the\nsupported programming languages to define reusable cloud components called [constructs], which\nare composed together into [stacks], forming a \"CDK app\".\n\nThey then use the [AWS CDK CLI] to interact with their CDK app. The CLI allows developers to\nsynthesize artifacts such as AWS CloudFormation Templates, deploy stacks to development AWS accounts and \"diff\"\nagainst a deployed stack to understand the impact of a code change.\n\nThe [AWS Construct Library] includes a module for each\nAWS service with constructs that offer rich APIs that encapsulate the details of\nhow to use AWS. The AWS Construct Library aims to reduce the complexity and\nglue-logic required when integrating various AWS services to achieve your goals\non AWS.\n\nModules in the AWS Construct Library are designated Experimental while we build\nthem; experimental modules may have breaking API changes in any release.  After\na module is designated Stable, it adheres to [semantic versioning](https://semver.org/),\nand only major releases can have breaking changes. Each module's stability designation\nis available on its Overview page in the [AWS CDK API Reference](https://docs.aws.amazon.com/cdk/api/latest/docs/aws-construct-library.html).\nFor more information, see [Versioning](https://docs.aws.amazon.com/cdk/latest/guide/reference.html#versioning)\nin the CDK Developer Guide.\n\n[CDK framework]: https://docs.aws.amazon.com/cdk/latest/guide/home.html\n[constructs]: https://docs.aws.amazon.com/cdk/latest/guide/constructs.html\n[stacks]: https://docs.aws.amazon.com/cdk/latest/guide/stacks.html\n[apps]: https://docs.aws.amazon.com/cdk/latest/guide/apps.html\n[Developer Guide]: https://docs.aws.amazon.com/cdk/latest/guide\n[AWS CDK CLI]: https://docs.aws.amazon.com/cdk/latest/guide/tools.html\n[AWS Construct Library]: https://docs.aws.amazon.com/cdk/api/latest/docs/aws-construct-library.html\n\n## Getting Started\n\nFor a detailed walkthrough, see the [tutorial](https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html#hello_world_tutorial) in the AWS CDK [Developer Guide](https://docs.aws.amazon.com/cdk/latest/guide/home.html).\n\n### At a glance\n\nInstall or update the [AWS CDK CLI] from npm (requires [Node.js \u2265 14.15.0](https://nodejs.org/download/release/latest-v14.x/)). We recommend using a version in [Active LTS](https://nodejs.org/en/about/previous-releases)\n\n```sh\nnpm i -g aws-cdk\n```\n\n(See [Manual Installation](./MANUAL_INSTALLATION.md) for installing the CDK from a signed .zip file).\n\nInitialize a project:\n\n```sh\nmkdir hello-cdk\ncd hello-cdk\ncdk init sample-app --language=typescript\n```\n\nThis creates a sample project looking like this:\n\n```ts\nexport class HelloCdkStack extends cdk.Stack {\n  constructor(scope: cdk.App, id: string, props?: cdk.StackProps) {\n    super(scope, id, props);\n\n    const queue = new sqs.Queue(this, 'HelloCdkQueue', {\n      visibilityTimeout: cdk.Duration.seconds(300)\n    });\n\n    const topic = new sns.Topic(this, 'HelloCdkTopic');\n\n    topic.addSubscription(new subs.SqsSubscription(queue));\n  }\n}\n```\n\nDeploy this to your account:\n\n```sh\ncdk deploy\n```\n\nUse the `cdk` command-line toolkit to interact with your project:\n\n* `cdk deploy`: deploys your app into an AWS account\n* `cdk synth`: synthesizes an AWS CloudFormation template for your app\n* `cdk diff`: compares your app with the deployed stack\n\n## Getting Help\n\nThe best way to interact with our team is through GitHub. You can open an [issue](https://github.com/aws/aws-cdk/issues/new/choose) and choose from one of our templates for bug reports, feature requests, documentation issues, or guidance.\n\nIf you have a support plan with AWS Support, you can also create a new [support case](https://console.aws.amazon.com/support/home#/).\n\nYou may also find help on these community resources:\n\n* Look through the [API Reference](https://docs.aws.amazon.com/cdk/api/latest/docs/aws-construct-library.html) or [Developer Guide](https://docs.aws.amazon.com/cdk/latest/guide)\n* The #aws-cdk Slack channel in [cdk.dev](https://cdk.dev)\n* Ask a question on [Stack Overflow](https://stackoverflow.com/questions/tagged/aws-cdk)\n  and tag it with `aws-cdk`\n\n## Roadmap\n\nThe [AWS CDK Roadmap project board](https://github.com/orgs/aws/projects/7) lets developers know about our upcoming features and priorities to help them plan how to best leverage the CDK and identify opportunities to contribute to the project. See [ROADMAP.md](https://github.com/aws/aws-cdk/blob/main/ROADMAP.md) for more information and FAQs.\n\n## Contributing\n\nWe welcome community contributions and pull requests. See\n[CONTRIBUTING.md](./CONTRIBUTING.md) for information on how to set up a development\nenvironment and submit code.\n\n## Metrics collection\n\nThis solution collects anonymous operational metrics to help AWS improve the\nquality and features of the CDK. For more information, including how to disable\nthis capability, please see the [developer guide](https://docs.aws.amazon.com/cdk/latest/guide/cli.html#version_reporting).\n\n## More Resources\n\n* [CDK Workshop](https://cdkworkshop.com/)\n* [Construct Hub](https://constructs.dev) - Find and use open-source Cloud Development Kit (CDK) libraries\n* Best Practices\n  * [Best practices for developing cloud applications with AWS CDK](https://aws.amazon.com/blogs/devops/best-practices-for-developing-cloud-applications-with-aws-cdk/)\n  * [Align with best practices while creating infrastructure using cdk aspects](https://aws.amazon.com/blogs/devops/align-with-best-practices-while-creating-infrastructure-using-cdk-aspects/)\n  * [Recommended AWS CDK project structure for Python applications](https://aws.amazon.com/blogs/developer/recommended-aws-cdk-project-structure-for-python-applications/)\n  * [Best practices for discoverability of a construct library on Construct Hub](https://aws.amazon.com/blogs/opensource/best-practices-for-discoverability-of-a-construct-library-on-construct-hub/)\n* [All developer blog posts about AWS CDK](https://aws.amazon.com/blogs/developer/category/developer-tools/aws-cloud-development-kit/)\n* **[CDK Construction Zone](https://www.twitch.tv/collections/9kCOGphNZBYVdA)** - A Twitch live coding series hosted by the CDK team, season one episodes:\n  * Triggers: Join us as we implement [Triggers](https://github.com/aws/aws-cdk-rfcs/issues/71), a Construct for configuring deploy time actions. Episodes 1-3:\n    * [S1E1](https://www.twitch.tv/videos/917691798): Triggers (part 1); **Participants:** @NetaNir, @eladb, @richardhboyd\n    * [S1E2](https://www.twitch.tv/videos/925801382): Triggers (part 2); **Participants:** @NetaNir, @eladb, @iliapolo\n    * [S1E3](https://www.twitch.tv/videos/944565768): Triggers (part 3); **Participants:** @NetaNir, @eladb, @iliapolo, @RomainMuller\n  * [S1E4](https://www.twitch.tv/aws/video/960287598): [Tokens](https://docs.aws.amazon.com/cdk/latest/guide/tokens.html) Deep Dive; **Participants:** @NetaNir,@rix0rrr, @iliapolo, @RomainMuller\n  * [S1E5](https://www.twitch.tv/videos/981481112): [Assets](https://docs.aws.amazon.com/cdk/latest/guide/assets.html) Deep Dive; **Participants:** @NetaNir, @eladb, @jogold\n  * [S1E6](https://www.twitch.tv/aws/video/1005334364): [Best Practices](https://aws.amazon.com/blogs/devops/best-practices-for-developing-cloud-applications-with-aws-cdk/); **Participants:** @skinny85, @eladb, @rix0rrr, @alexpulver\n  * [S1E7](https://www.twitch.tv/videos/1019059654): Tips and Tricks From The CDK Team; **Participants:** All the CDK team!\n* [Examples](https://github.com/aws-samples/aws-cdk-examples)\n* [Changelog](./CHANGELOG.md)\n* [NOTICE](./NOTICE)\n* [License](./LICENSE)\n", "release_dates": ["2024-03-01T23:01:51Z", "2024-02-23T04:07:50Z", "2024-02-21T16:41:44Z", "2024-02-14T23:23:25Z", "2024-02-10T02:33:30Z", "2024-02-02T11:11:29Z", "2024-02-01T02:44:52Z", "2024-01-26T23:02:59Z", "2024-01-24T21:27:54Z", "2024-01-18T14:11:22Z", "2024-01-13T04:40:27Z", "2024-01-12T22:01:39Z", "2024-01-12T11:26:47Z", "2024-01-11T22:37:13Z", "2024-01-03T20:45:25Z", "2023-12-27T20:06:02Z", "2023-12-22T17:48:53Z", "2023-12-22T04:20:04Z", "2023-12-14T13:56:02Z", "2023-12-06T13:58:14Z", "2023-12-05T22:24:19Z", "2023-12-01T20:56:50Z", "2023-12-01T14:05:31Z", "2023-11-27T20:24:47Z", "2023-11-22T00:17:11Z", "2023-11-17T00:17:13Z", "2023-11-16T00:21:41Z", "2023-11-14T21:39:23Z", "2023-11-14T00:46:11Z", "2023-11-13T21:04:23Z"]}, {"name": "aws-cdk-go", "description": "AWS CDK bindings for Go.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Go Bindings for AWS CDK\n\nThis repository included Go bindings for the AWS CDK.\n\n## Security\n\nSee [Security Issue Notifications](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "aws-cdk-rfcs", "description": "RFCs for the AWS CDK", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS CDK RFCs\n\nThis repo is a place to propose and track major upcoming changes to [AWS CDK], [jsii], and\nother related projects. It also is a great place to learn about the current and\nfuture state of the libraries and to discover projects for contribution.\n\n[AWS CDK]: https://github.com/aws/aws-cdk\n[jsii]: https://github.com/aws/jsii\n\n**Jump to**: [What is an RFC?](#what-is-an-rfc) |\n[RFC Process](#rfc-process) |\n[RFC State Diagram](#the-rfc-life-cycle)\n\n## Current RFCs\n\n**Jump to**:\n[Full list](./FULL_INDEX.md) |\n[Accepted](./ACCEPTED.md) |\n[Proposed](./PROPOSED.md) |\n[Closed](./CLOSED.md)\n\n<!--BEGIN_TABLE-->\n\\#|Title|Owner|Status\n---|-----|-----|------\n[473](https://github.com/aws/aws-cdk-rfcs/issues/473)|[EventBridge Pipes L2 Construct](https://github.com/aws/aws-cdk-rfcs/blob/main/text/0473-eventbridge-pipes.md)|[@mrgrain](https://github.com/mrgrain)|\ud83d\udc77 implementing\n[64](https://github.com/aws/aws-cdk-rfcs/issues/64)|[Garbage Collection for Assets](https://github.com/aws/aws-cdk-rfcs/issues/64)|[@kaizencc](https://github.com/kaizencc)|\ud83d\udcc6 planning\n[162](https://github.com/aws/aws-cdk-rfcs/issues/162)|[CDK Refactoring Tools](https://github.com/aws/aws-cdk-rfcs/issues/162)|[@evgenyka](https://github.com/evgenyka)|\ud83d\udcc6 planning\n[228](https://github.com/aws/aws-cdk-rfcs/issues/228)|[CDK CLI Triggers](https://github.com/aws/aws-cdk-rfcs/issues/228)||\ud83d\udcc6 planning\n[300](https://github.com/aws/aws-cdk-rfcs/issues/300)|[Programmatic access of AWS CDK CLI](https://github.com/aws/aws-cdk-rfcs/issues/300)||\ud83d\udcc6 planning\n[491](https://github.com/aws/aws-cdk-rfcs/issues/491)|[CloudFront Origin Access Control L2](https://github.com/aws/aws-cdk-rfcs/issues/491)||\ud83d\udcc6 planning\n[583](https://github.com/aws/aws-cdk-rfcs/issues/583)|[Deployment Debugging](https://github.com/aws/aws-cdk-rfcs/issues/583)||\ud83d\udcc6 planning\n[585](https://github.com/aws/aws-cdk-rfcs/issues/585)|[Local Application Testing](https://github.com/aws/aws-cdk-rfcs/issues/585)||\ud83d\udcc6 planning\n[586](https://github.com/aws/aws-cdk-rfcs/issues/586)|[Understand Deployment Progress](https://github.com/aws/aws-cdk-rfcs/issues/586)||\ud83d\udcc6 planning\n[605](https://github.com/aws/aws-cdk-rfcs/issues/605)|[Rewrite EKS L2 construct (EKSv2)](https://github.com/aws/aws-cdk-rfcs/issues/605)||\ud83d\udcc6 planning\n[502](https://github.com/aws/aws-cdk-rfcs/issues/502)|[Amazon VPC Lattice L2 Construct](https://github.com/aws/aws-cdk-rfcs/blob/main/text/0502_aws-vpclattice.md)|[@TheRealAmazonKendra](https://github.com/TheRealAmazonKendra)|\ud83d\udc4d approved\n[507](https://github.com/aws/aws-cdk-rfcs/issues/507)|[Full control over VPC and subnet configuration](https://github.com/aws/aws-cdk-rfcs/blob/main/text/0507-subnets)|[@otaviomacedo](https://github.com/otaviomacedo)|\ud83d\udc4d approved\n<!--END_TABLE-->\n\n## What is an RFC?\n\nAn RFC is a document that proposes a change to one of the projects led by the\nCDK team at AWS. *Request for Comments* means a request for discussion and\noversight about the future of the project from maintainers, contributors and\nusers.\n\n**When should I write an RFC?** The CDK team proactively decides to write RFCs\non major features or complex changes that we feel require that extra vetting.\nHowever, the process is designed to be as lightweight as needed and can be used\nto request feedback on any change. Quite often, even changes that seem obvious\nand simple at first sight can be significantly improved once a wider group of\ninterested and experienced people have a chance to weigh in.\n\n**Who should submit an RFC?** An RFC can be submitted by anyone. In most cases,\nRFCs are authored by CDK maintainers, but contributors are more than welcome to\nsubmit RFCs.\n\nIf you are a **contributor** and you wish to write an RFC, please contact the\ncore team at the [#aws-cdk-rfcs] to make sure someone from the core team can\nsponsor your work. Otherwise, there is a good chance we won't have bandwidth to\nhelp.\n\n## RFC Process\n\nTo start an RFC process, create a [new tracking issue] and follow the\ninstructions in the issue template. It includes a checklist of the various\nstages an RFC goes through.\n\n[new tracking issue]: https://github.com/aws/aws-cdk-rfcs/issues/new?assignees=&labels=management%2Ftracking%2C+status%2Fproposed&template=tracking-issue.md&title=proposal+title\n\nThis section describes each stage in detail, so you can refer to it for\nguidance.\n\n### 1. Tracking Issue\n\nEach RFC has a GitHub issue which tracks it from start to finish. The issue is\nthe hub for conversations, community signal (+1s) and the issue number is used\nas the unique identifier of this RFC.\n\n> Before creating a tracking issue, please search for similar or related ideas in\nthe RFC table above or in the issue list of this repo. If there is a relevant\nRFC, collaborate on that existing RFC, based on its current stage.\n\nOur [tracking issue template] includes a checklist of all the steps an RFC goes\nthrough and it's the driver's responsibility to update the checklist and assign\nthe correct label to on the RFC throughout the process.\n\n[tracking issue template]: https://github.com/aws/aws-cdk-rfcs/blob/master/.github/ISSUE_TEMPLATE/tracking-issue.md\n\nWhen the issue is created, it is required to fill in the following information:\n\n1. **Title**: the name of the feature or change - think changelog entry.\n2. **Description**: a _short_ description of feature, as if it was already implemented.\n3. **Proposed by**: fill in the GitHub alias of the person who proposed the idea\n   under \"Proposed by\".\n\n### 2. API Bar Raiser\n\nReach us via [#aws-cdk-rfcs] to get an \"API Bar Raiser\" assigned to your RFC.\n\nFor each RFC, CDK leadership will assign an **API Bar Raiser** who reviews and\napproves the public API of the feature. API Bar Raisers have veto rights on\nAPI-related design decisions, such as naming, structure, options, CLI commands\nand others.\n\nThe public API of a feature represents the surface through which users interact\nwith it, and we want to make sure these APIs are consistent, ergonomic and\ndesigned based on the intent and the mental model of our users. Additionally,\nonce we announce that a feature is \"stable\" (1.0, GA, etc) any breaking change\nto its public API will require releasing a new major version, so we like think\nof API decisions as \"one way doors\".\n\nAPI Bar Raisers will be assigned using a tiering model which is generally based\non the size of the user base that will likely get exposed to the feature. As a\ngeneral rule, the more \"significant\" the feature is, we will assign a bar raiser\nwith a wider and longer-term context of the project.\n\nTo merge an RFC, a [sign-off](#6-api-sign-off) from the bar raiser is required\non the public API of the feature, so we encourage to engage with them early in\nthe process to make sure you are aligned on how the API should be designed.\n\n> NOTE: The technical solution proposed in an RFC *does not* require approval\n> beyond the normal pull request approval model (e.g. a core team member needs\n> to approve the RFC PR and any subsequent changes to it).\n\n### 3. Kick-off\n\nBefore diving into writing the RFC, it is highly recommended to organize a\nkick-off meeting that includes the API Bar Raiser and any stakeholders that\nmight be interested in this RFC or can contribute ideas and direction. The goal\nof the meeting is to discuss the feature, its scope and general direction for\nimplementation.\n\nIf you are not part of the CDK team at Amazon, reach out to us via [#aws-cdk-rfcs]\nand we will help to organize the kick-off meeting.\n\nOur experience shows that such a meeting can save a lot of time and energy.\n\nYou can use the tracking issue to record some initial API and design ideas and\ncollect early feedback and use cases as a preparation for the kick-off meeting\nand RFC document itself. You can start the meeting by letting participants\nobtaining context from the tracking issue.\n\nAt the end of the meeting, record any ideas and decisions in the tracking issue\nand update the checklist to indicate that the kick-off meeting has happened.\n\n### 4. RFC Document\n\nThe next step is to write the first revision of the RFC document itself.\n\nCreate a file under `text/NNNN-name.md` based off of the template under\n[`0000-template.md`](./0000-template.md) (where `NNNN` is your tracking issue\nnumber). Follow the template. It includes useful guidance and tips on how to\nwrite a good RFC.\n\n**What should be included in an RFC?** The purpose of an RFC is to reduce\nambiguity and risk and get approval for public-facing interfaces (APIs), which\nare \"one-way doors\" after the feature is released. Another way to think about it\nis that the goal and contents of the document should allow us to create a\n*high-confidence* implementation plan for a feature or a change.\n\nIn many cases, it is useful to develop a **prototype** or even start coding the\nactual implementation while you are writing the RFC document. Take into account\nthat you may need to throw your code away or refactor it substantially, but our\nexperience shows that good RFCs are the ones who dive into the details. A\nprototype is great way to make sure your design \"holds water\".\n\n> [!NOTE]\n> To ensure consistency, the Markdown you write will be checked for common >\nmistakes using a linter. To get early feedback while you are writing, use the\n[VSCode > markdownlint\nextensions](https://marketplace.visualstudio.com/items?itemName=DavidAnson.vscode-markdownlint),\n> or run the `./lint.sh` script in the root of the repository.\n> Run `./lint.sh --fix` auto fix all fixable violations.\n\n### 5. Feedback\n\nOnce you have an initial version of your RFC document (it is completely fine to\nsubmit an unfinished RFC to get initial feedback), submit it as a pull request\nagainst this repo and start collecting feedback.\n\nContact the CDK core team at [#aws-cdk-rfcs] (or via email/Slack if you are part\nof the core team) and reach out to the public and Amazon internal communities\nvia various Slack channels in [cdk.dev](https://cdk.dev), Twitter and any other\nrelevant forum.\n\nThis is the likely going to be the longest part of your RFC process, and where\nmost of the feedback is collected. Some RFCs resolve quickly and some can take\nmonths (!!). *Take into account at least 1-2 weeks to allow community and\nstakeholders to provide their feedback.*\n\nA few tips:\n\n- If you decide to resolve a comment without addressing it, take the time to\n  explain.\n- Try to understand where people are coming from. If a comment seems off, ask\n  folks to elaborate and describe their use case or provide concrete examples.\n- Work with your API bar raiser: if there are disagreements, @mention them in a\n  comment and ask them to provide their opinion.\n- Be patient: it sometimes takes time for an RFC to converge. Our experience\n  shows that some ideas need to \"bake\" and solutions oftentimes emerge via a\n  healthy debate. We've had RFCs that took months to resolve.\n- Not everything must be resolved in the first revision. It is okay to leave\n  some things to resolve later. Make sure to capture them clearly and have an\n  agreement about that. We oftentimes update an RFC doc a few times during the\n  implementation.\n\n### 6. API Sign-off\n\nBefore you can merge your RFC, you will need the API Bar Raiser to sign-off on\nthe public API of your feature. This is will normally be described under the\n**Working Backwards** section of your RFC.\n\nTo sign-off, the API bar raiser will add the **status/api-approved** label to the RFC\npull request.\n\nOnce the API was signed-off, update your RFC document and add a `[x]` the\nrelevant location in the RFC document. For example:\n\n```\n[x] Signed-off by API Bar Raiser @foobar\n```\n\n### 7. Final Comments Period\n\nAt some point, you've reached consensus about most issues that were brought up\nduring the review period, and you are ready to merge. To allow \"last call\" on\nfeedback, the author can announce that the RFC enters \"final comments period\",\nwhich means that within a ~week, if no major concerns are raised, the RFC will\nbe approved and merged.\n\nAdd a comment on the RFC pull request, tracking issue (and possibly slack/email\nif relevant) that the RFC entered this stage so that all relevant stakeholders\nwill be notified.\n\nOnce the final comments period is over, seek an approval of one of the core team\nmembers, and you can merge your PR to the main branch. This will move your RFC\nto the \"approved\" state.\n\n### 8. Implementation\n\nFor large changes, we highly recommend creating an implementation plan which\nlists all the tasks required. In many cases, large implementation  should be\nbroken down and released via multiple iterations. Devising a concrete plan to\nbreak down the break can be very helpful.\n\nThe implementation plan should be submitted through a PR that adds an addendum\nto the RFC document and seeks the approval of any relevant stakeholders.\n\nThroughout this process, update the tracking issue:\n\n- Add the alias of the \"implementation lead\"\n- Execution plan submitted (label: `status/planning`)\n- Plan approved and merged (label: `status/implementing`)\n- Implementation complete (label: `status/done`)\n\n## The RFC Life Cycle\n\nThe following state diagram describes the RFC process:\n\n![rfc-states](./images/lifecycle.png)\n\n<!--\ndigraph states {\n    node [shape=ellipse];\n    edge [color=gray, fontsize=12]\n\n    idea [label = \"Idea\", shape = plaintext]\n    proposed [label = \"Proposed\"];\n    review [label = \"In Review\"];\n    fcp [label = \"Final Comment Period\"];\n    approved [label = \"Approved\"];\n    planning [label = \"Planning\"];\n    implementing [label = \"Implementing\"];\n    done [label = \"Done\"];\n    rejected [label = \"Rejected\"];\n\n    idea -> proposed [label = \"github issue created\"]\n    proposed -> review [label = \"pull request with rfc doc created\"];\n    review -> review [label = \"doc revisions\"];\n    review -> fcp [label = \"shepherd approved\"];\n    review -> rejected [label = \"rejected\"];\n    fcp -> review [label = \"revision requested\"];\n    fcp -> approved [label = \"pull request approved and merged\"];\n    fcp -> rejected [label = \"rfc rejected\"];\n    approved -> planning [label = \"pull request with implementation plan created\"];\n    planning -> implementing [label = \"rfc with implementation plan approved and merged\"];\n    implementing -> done [label = \"implementation completed\"];\n}\n-->\n\n1. **Proposed** - A tracking issue has been created with a basic outline of the\n   proposal.\n2. **Review** - An RFC document has been written with a detailed design and a PR is\n   under review. At this point the PR will be assigned a **shepherd** from the core\n   team.\n3. **Final Comment Period** - The shepherd has approved the RFC PR, and announces\n   that the RFC enters a period for final comments before it will be approved (~1wk).\n   At this stage, if major issues are raised, the RFC may return to **Review**.\n4. **Approved** - The RFC PR is approved and merged to `master`, and the RFC is now\n   ready to be implemented.\n5. **Planning** - A PR is created with the **Implementation Plan** section of the RFC.\n6. **Implementing** - Implementation plan is approved and merged and the RFC is actively\n   being implemented.\n7. **Done** - Implementation is complete and merged across appropriate\n   repositories.\n8. **Rejected** - During the review period, the RFC may be rejected and then it will\n   be marked as such.\n9. **Stale** - The RFC did not get any significant enough progress or tracking and has become stale.\n   We welcome a re-submission with substantial enough changes to overcome the original issues.\n\n---\n\nAWS CDK's RFC process owes its inspiration to the [Yarn RFC process], [Rust\nRFC process], [React RFC process], and [Ember RFC process]\n\n[yarn rfc process]: https://github.com/yarnpkg/rfcs\n[rust rfc process]: https://github.com/rust-lang/rfcs\n[react rfc process]: https://github.com/reactjs/rfcs\n[ember rfc process]: https://github.com/emberjs/rfcs\n\n[#aws-cdk-rfcs]: https://cdk-dev.slack.com/archives/C025ZFGMUCD\n", "release_dates": []}, {"name": "aws-cli", "description": "Universal Command Line Interface for Amazon Web Services", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": null, "release_dates": ["2018-11-26T06:25:44Z"]}, {"name": "aws-cloud-map-mcs-controller-for-k8s", "description": "K8s controller implementing Multi-Cluster Services API based on AWS Cloud Map.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Cloud Map MCS Controller for K8s\n\n[![Documentation](https://img.shields.io/badge/godoc-reference-blue.svg)](https://godoc.org/github.com/aws/aws-cloud-map-mcs-controller-for-k8s)\n[![CodeQL](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/actions/workflows/codeql-analysis.yml/badge.svg?branch=main)](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/actions/workflows/codeql-analysis.yml)\n[![Build status](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/actions/workflows/build.yml/badge.svg?branch=main)](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/actions/workflows/build.yml)\n[![Deploy status](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/actions/workflows/deploy.yml/badge.svg?branch=main)](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/actions/workflows/deploy.yml)\n[![Integration status](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/actions/workflows/integration-test.yml/badge.svg?branch=main)](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/actions/workflows/integration-test.yml)\n[![codecov](https://codecov.io/gh/aws/aws-cloud-map-mcs-controller-for-k8s/branch/main/graph/badge.svg)](https://codecov.io/gh/aws/aws-cloud-map-mcs-controller-for-k8s)\n\n[![License](https://img.shields.io/badge/license-Apache--2.0-blue.svg?color=success)](http://www.apache.org/licenses/LICENSE-2.0)\n[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/issues)\n[![GitHub issues](https://img.shields.io/github/issues-raw/aws/aws-cloud-map-mcs-controller-for-k8s?style=flat)](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/issues)\n[![Go Report Card](https://goreportcard.com/badge/github.com/aws/aws-cloud-map-mcs-controller-for-k8s)](https://goreportcard.com/report/github.com/aws/aws-cloud-map-mcs-controller-for-k8s)\n\n## Introduction\nThe AWS Cloud Map Multi-cluster Service Discovery Controller for Kubernetes (K8s) implements the Kubernetes [KEP-1645: Multi-Cluster Services API](https://github.com/kubernetes/enhancements/tree/master/keps/sig-multicluster/1645-multi-cluster-services-api) and [KEP-2149: ClusterId for ClusterSet identification](https://github.com/kubernetes/enhancements/tree/master/keps/sig-multicluster/2149-clusterid), which allows services to communicate across multiple clusters. The implementation relies on [AWS Cloud Map](https://aws.amazon.com/cloud-map/) for enabling cross-cluster service discovery. We have detailed [step-by-step setup guide](https://aws.amazon.com/blogs/opensource/kubernetes-multi-cluster-service-discovery-using-open-source-aws-cloud-map-mcs-controller/)!\n\n**\u26a0 NOTE: The current version [![GitHub Release](https://img.shields.io/github/release/aws/aws-cloud-map-mcs-controller-for-k8s.svg?style=flat&label=)]() is in *Alpha* phase, and NOT intended for production use. The support will be limited to critical bug fixes.** \n\n*Checkout the [Graduation Criteria](#graduation-criteria) for moving the project to the next phase.*\n\n## Installation\n\nPerform the following installation steps on each participating cluster.\n\n- For multi-cluster service discovery and consumption, the controller should be installed on a minimum of 2 EKS clusters.\n- Participating clusters should be provisioned into a single AWS account, within a single AWS region.\n\n### Dependencies\n\n#### Network\n\n> **The AWS Cloud Map MCS Controller for K8s provides service discovery and communication across multiple clusters, therefore implementations depend on end-end network connectivity between workloads provisioned within each participating cluster.** \n\n- In deployment scenarios where participating clusters are provisioned into separate VPCs, connectivity will depend on correctly configured  [VPC Peering](https://docs.aws.amazon.com/vpc/latest/peering/create-vpc-peering-connection.html), [inter-VPC routing](https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-routing.html), and Security Group configuration. The [VPC Reachability Analyzer](https://docs.aws.amazon.com/vpc/latest/reachability/getting-started.html) can be used to test and validate end-end connectivity between worker nodes within each cluster.\n- Undefined behavior may occur if controllers are deployed without the required network connectivity between clusters.\n\n#### Configure CoreDNS\n\nInstall the CoreDNS multicluster plugin into each participating cluster. The multicluster plugin enables CoreDNS to lifecycle manage DNS records for `ServiceImport` objects.\n\nTo install the plugin, run the following commands.\n\n```bash\nkubectl apply -k \"github.com/aws/aws-cloud-map-mcs-controller-for-k8s/samples/coredns-clusterrole.yaml\"\nkubectl apply -k \"github.com/aws/aws-cloud-map-mcs-controller-for-k8s/samples/coredns-configmap.yaml\"\nkubectl apply -k \"github.com/aws/aws-cloud-map-mcs-controller-for-k8s/samples/coredns-deployment.yaml\"\n```\n\n### Install Controller\n\nTo install the latest release of the controller, run the following commands.\n\n> **_NOTE:_** AWS region environment variable can be _optionaly_ set like `export AWS_REGION=us-west-2` Otherwise the controller will infer region in the order `AWS_REGION` environment variable, ~/.aws/config file, then EC2 metadata (for EKS environment)\n\n```sh\nkubectl apply -k \"github.com/aws/aws-cloud-map-mcs-controller-for-k8s/config/controller_install_release\"\n```\n\n> \ud83d\udccc See [Releases](#Releases) section for details on how to install other versions.\n\nThe controller must have sufficient IAM permissions to perform required Cloud Map operations. Grant IAM access rights `AWSCloudMapFullAccess` to the controller Service Account to enable the controller to manage Cloud Map resources.\n\n## Usage\n\n### Configure `cluster.clusterset.k8s.io` and `clusterset.k8s.io`\n\n`cluster.clusterset.k8s.io` is a unique identifier for the cluster.\n\n`clusterset.k8s.io` is an identifier that relates to the `ClusterSet` in which the cluster belongs. \n\n```yaml\napiVersion: about.k8s.io/v1alpha1\nkind: ClusterProperty\nmetadata:\n  name: cluster.clusterset.k8s.io\nspec:\n  value: [Your Cluster identifier]\n---\napiVersion: about.k8s.io/v1alpha1\nkind: ClusterProperty\nmetadata:\n  name: clusterset.k8s.io\nspec:\n  value: [Your ClusterSet identifier]\n```\n\n**Example:**\n```yaml\napiVersion: about.k8s.io/v1alpha1\nkind: ClusterProperty\nmetadata:\n  name: cluster.clusterset.k8s.io\nspec:\n  value: my-first-cluster\n---\napiVersion: about.k8s.io/v1alpha1\nkind: ClusterProperty\nmetadata:\n  name: clusterset.k8s.io\nspec:\n  value: my-clusterset\n```\n\n### Export services\n\nThen assuming you already have a Service installed, apply a `ServiceExport` yaml to the cluster in which you want to export a service. This can be done for each service you want to export.\n\n```yaml\nkind: ServiceExport\napiVersion: multicluster.x-k8s.io/v1alpha1\nmetadata:\n  namespace: [Your service namespace here]\n  name: [Your service name]\n```\n\n**Example:** This will export a service with name *my-amazing-service* in namespace *hello*\n```yaml\nkind: ServiceExport\napiVersion: multicluster.x-k8s.io/v1alpha1\nmetadata:\n  namespace: hello\n  name: my-amazing-service\n```\n\n*See the `samples` directory for a set of example yaml files to set up a service and export it. To apply the sample files run the following commands.*\n\n```sh\nkubectl create namespace example\nkubectl apply -k \"github.com/aws/aws-cloud-map-mcs-controller-for-k8s/samples/example-deployment.yaml\nkubectl apply -k \"github.com/aws/aws-cloud-map-mcs-controller-for-k8s/samples/example-service.yaml\nkubectl apply -k \"github.com/aws/aws-cloud-map-mcs-controller-for-k8s/samples/example-serviceexport.yaml\n```\n\n### Import services\n\nIn your other cluster, the controller will automatically sync services registered in AWS Cloud Map by applying the appropriate `ServiceImport`. To list them all, run the following command.\n```sh\nkubectl get ServiceImport -A\n```\n\n## Releases\n\nAWS Cloud Map MCS Controller for K8s adheres to the [SemVer](https://semver.org/) specification. Each release updates the major version tag (eg. `vX`), a major/minor version tag (eg. `vX.Y`) and a major/minor/patch version tag (eg. `vX.Y.Z`). To see a full list of all releases, refer to our [Github releases page](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/releases).\n\n> **_NOTE:_** AWS region environment variable can be _optionally_ set like `export AWS_REGION=us-west-2` Otherwise controller will infer region in the order `AWS_REGION` environment variable, ~/.aws/config file, then EC2 metadata (for EKS environment)\n\nThe following command format is used to install from a particular release.\n```sh\nkubectl apply -k \"github.com/aws/aws-cloud-map-mcs-controller-for-k8s/config/controller_install_release[?ref=*git version tag*]\"\n```\n\nRun the following command to install the latest release.\n```sh\nkubectl apply -k \"github.com/aws/aws-cloud-map-mcs-controller-for-k8s/config/controller_install_release\"\n```\n\nThe following example will install release v0.1.0.\n```sh\nkubectl apply -k \"github.com/aws/aws-cloud-map-mcs-controller-for-k8s/config/controller_install_release?ref=v0.1.0\"\n```\n\nWe also maintain a `latest` tag, which is updated to stay in line with the `main` branch. We **do not** recommend installing this on any production cluster, as any new major versions updated on the `main` branch will introduce breaking changes.\n\nTo install from `latest` tag run the following command.\n```sh\nkubectl apply -k \"github.com/aws/aws-cloud-map-mcs-controller-for-k8s/config/controller_install_latest\"\n```\n\n## Graduation Criteria\n\n### Alpha -> Beta Graduation\n* Implement the [resiliency milestone](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/milestone/3).\n\n### Beta -> GA Graduation\n* Scalability/performance testing.\n* Implement the [observability and deployment milestone](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/milestone/6).\n* [KEP-1645: Multi-Cluster Services API](https://github.com/kubernetes/enhancements/tree/master/keps/sig-multicluster/1645-multi-cluster-services-api) and [KEP-2149: ClusterId for ClusterSet identification](https://github.com/kubernetes/enhancements/tree/master/keps/sig-multicluster/2149-clusterid) are Beta or GA. \n\n## Slack community\nWe have an open Slack community where users may get support with integration, discuss controller functionality and provide input on our feature roadmap. https://awsappmesh.slack.com/#k8s-mcs-controller\nJoin the channel with this [invite](https://join.slack.com/t/awsappmesh/shared_invite/zt-dwgbt85c-Sj_md92__quV8YADKfsQSA).\n\n## Contributing\n`aws-cloud-map-mcs-controller-for-k8s` is an open source project. See [CONTRIBUTING](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/blob/main/CONTRIBUTING.md) for details.\n\n## License\n\nThis project is distributed under the\n[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0),\nsee [LICENSE](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/blob/main/LICENSE) and [NOTICE](https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s/blob/main/NOTICE) for more information.\n", "release_dates": ["2022-12-09T20:04:27Z", "2022-09-15T20:11:20Z", "2022-04-20T02:45:21Z", "2021-12-03T22:59:46Z", "2021-11-19T23:18:18Z", "2021-11-19T05:40:27Z", "2021-11-06T00:52:23Z", "2021-10-08T22:04:15Z"]}, {"name": "aws-cloudtrail-processing-library", "description": "The AWS CloudTrail Processing Library helps Java developers to easily consume and process log files from AWS CloudTrail.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2023-12-01T23:33:05Z", "2023-05-22T19:30:40Z", "2022-08-01T17:58:04Z", "2021-01-13T00:06:53Z", "2020-08-26T04:34:04Z", "2019-11-21T18:55:54Z", "2018-10-19T16:08:30Z", "2018-05-16T16:36:29Z", "2017-11-30T18:28:36Z", "2017-06-02T15:44:41Z", "2017-01-18T04:12:51Z", "2016-10-06T00:19:21Z", "2015-10-28T20:49:51Z", "2014-11-05T23:20:28Z"]}, {"name": "aws-codebuild-docker-images", "description": "Official AWS CodeBuild repository for managed Docker images http://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref.html", "language": "Dockerfile", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# AWS CodeBuild curated Docker images\n\nThis repository holds Dockerfiles of official AWS CodeBuild curated Docker images. Please refer to [the AWS CodeBuild User Guide](http://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref.html) for list of environments supported by AWS CodeBuild.\n\n![Build Status](https://codebuild.us-west-2.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiSkJibVVQVEpvUms1cmw3YVlnU1hSdkpBQ0c5SFgyTkJXMFBFdEU2SWtySHREcUlUVlRhbW4zMEd3NlhsOWIzUWgvRkxhUWVSSTFPZGNNakNHRVNLalY0PSIsIml2UGFyYW1ldGVyU3BlYyI6IlV0QjBRZXRvS0F5dE5vbTciLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=master)\n\nThe master branch will sometimes have changes that are still in the process of being released in AWS CodeBuild.  See the latest released versions of the Dockerfiles [here](https://github.com/aws/aws-codebuild-docker-images/releases)\n\n### How to build Docker images\n\nSteps to build Standard 7.0 image\n\n* Run `git clone https://github.com/aws/aws-codebuild-docker-images.git` to download this repository to your local machine\n* Run `cd aws-codebuild-docker-images/ubuntu/standard/7.0` to change the directory in your local workspace. This is the location of the Standard 7.0 Dockerfile with Ubuntu base.\n* Run `docker build -t aws/codebuild/standard:7.0 .` to build Docker image locally\n\nTo poke around in the image interactively, build it and run:\n`docker run -it --entrypoint sh aws/codebuild/standard:7.0 -c bash`\n\nTo let the Docker daemon start up in the container, build it and run:\n`docker run -it --privileged aws/codebuild/standard:7.0 bash`\n\n```\n$ git clone https://github.com/aws/aws-codebuild-docker-images.git\n$ cd aws-codebuild-docker-images\n$ cd ubuntu/standard/7.0\n$ docker build -t aws/codebuild/standard:7.0 .\n$ docker run -it --entrypoint sh aws/codebuild/standard:7.0 -c bash\n```\n\n### Image maintenance\n\nSome of the images in this repository are no longer actively maintained by AWS CodeBuild and may no longer build successfully.  These images will not receive any further updates.  They remain in this repository as a reference for the contents of these images that were previously released by CodeBuild.\n\nThe following images are actively maintained by AWS CodeBuild, and are listed in the CodeBuild console.\n\n+ [standard 5.0](ubuntu/standard/5.0)\n+ [standard 6.0](ubuntu/standard/6.0)\n+ [standard 7.0](ubuntu/standard/7.0)\n+ [amazonlinux2-x86_64-standard:4.0](al2/x86_64/standard/4.0)\n+ [amazonlinux2-x86_64-standard:5.0](al2/x86_64/standard/5.0)\n+ [amazonlinux2-x86_64-standard:corretto8](al2/x86_64/standard/corretto8)\n+ [amazonlinux2-x86_64-standard:corretto11](al2/x86_64/standard/corretto11)\n+ [amazonlinux2-aarch64-standard:2.0](al2/aarch64/standard/2.0)\n+ [amazonlinux2-aarch64-standard:3.0](al2/aarch64/standard/3.0)\n+ [amazonlinux-x86_64-lambda-standard:corretto11](al-lambda/x86_64/corretto11)\n+ [amazonlinux-x86_64-lambda-standard:corretto17](al-lambda/x86_64/corretto17)\n+ [amazonlinux-x86_64-lambda-standard:corretto21](al-lambda/x86_64/corretto21)\n+ [amazonlinux-x86_64-lambda-standard:dotnet6](al-lambda/x86_64/dotnet6)\n+ [amazonlinux-x86_64-lambda-standard:go1.21](al-lambda/x86_64/go1.21)\n+ [amazonlinux-x86_64-lambda-standard:nodejs18](al-lambda/x86_64/nodejs18)\n+ [amazonlinux-x86_64-lambda-standard:nodejs20](al-lambda/x86_64/nodejs20)\n+ [amazonlinux-x86_64-lambda-standard:python3.11](al-lambda/x86_64/python3.11)\n+ [amazonlinux-x86_64-lambda-standard:python3.12](al-lambda/x86_64/python3.12)\n+ [amazonlinux-x86_64-lambda-standard:ruby3.2](al-lambda/x86_64/ruby3.2)\n+ [amazonlinux-aarch64-lambda-standard:corretto11](al-lambda/aarch64/corretto11)\n+ [amazonlinux-aarch64-lambda-standard:corretto17](al-lambda/aarch64/corretto17)\n+ [amazonlinux-aarch64-lambda-standard:corretto21](al-lambda/aarch64/corretto21)\n+ [amazonlinux-aarch64-lambda-standard:dotnet6](al-lambda/aarch64/dotnet6)\n+ [amazonlinux-aarch64-lambda-standard:go1.21](al-lambda/aarch64/go1.21)\n+ [amazonlinux-aarch64-lambda-standard:nodejs18](al-lambda/aarch64/nodejs18)\n+ [amazonlinux-aarch64-lambda-standard:nodejs20](al-lambda/aarch64/nodejs20)\n+ [amazonlinux-aarch64-lambda-standard:python3.11](al-lambda/aarch64/python3.11)\n+ [amazonlinux-aarch64-lambda-standard:python3.12](al-lambda/aarch64/python3.12)\n+ [amazonlinux-aarch64-lambda-standard:ruby3.2](al-lambda/aarch64/ruby3.2)\n", "release_dates": ["2024-03-01T18:01:47Z", "2024-02-08T23:18:52Z", "2024-01-30T00:14:56Z", "2024-01-13T01:00:41Z", "2023-12-14T19:40:40Z", "2023-11-09T00:55:24Z", "2023-07-28T15:47:53Z", "2023-06-09T21:03:30Z", "2023-05-23T02:35:00Z", "2023-04-25T21:24:29Z", "2023-04-13T22:22:49Z", "2023-02-17T19:29:02Z", "2022-07-01T20:45:08Z", "2022-06-09T23:01:30Z", "2022-03-17T20:42:46Z", "2021-10-25T19:41:43Z", "2021-08-25T16:37:40Z", "2021-04-27T20:00:45Z", "2021-03-10T18:27:08Z", "2021-01-08T19:05:53Z", "2020-11-09T23:42:30Z", "2020-08-31T21:26:43Z", "2020-06-20T04:00:57Z", "2020-05-09T23:30:19Z", "2020-03-12T19:55:51Z", "2019-11-26T20:43:36Z", "2019-10-04T21:46:51Z", "2019-08-14T21:34:07Z", "2019-06-25T23:13:45Z", "2019-05-29T23:33:20Z"]}, {"name": "aws-codedeploy-agent", "description": "Host Agent for AWS CodeDeploy", "language": "Ruby", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS CodeDeploy Agent\n\n[![Code Climate](https://codeclimate.com/github/aws/aws-codedeploy-agent.png)](https://codeclimate.com/github/aws/aws-codedeploy-agent) [![Build Status](https://travis-ci.org/aws/aws-codedeploy-agent.png?branch=master)](https://travis-ci.org/aws/aws-codedeploy-agent) [![Coverage Status](https://coveralls.io/repos/aws/aws-codedeploy-agent/badge.svg?branch=master&service=github)](https://coveralls.io/r/aws/aws-codedeploy-agent?branch=master)\n\n## Latest Release: 1.4.0\n[Release Notes](https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html#codedeploy-agent-version-history)\n\n## Build Steps\n\n``` ruby\ngit clone https://github.com/aws/aws-codedeploy-agent.git\ngem install bundler -v 1.3.5\ncd aws-codedeploy-agent\nbundle install\nrake clean && rake\n```\n\n## Starting up the CodeDeploy Agent Locally for manual testing\n\n`bin/codedeploy-agent start`\n\nTo stop it:\n\n`bin/codedeploy-agent stop`\n\n## Integration Test\n\nPlease do the build steps mentioned above before running the integration test.\n\nThe integration test creates the following\n* An IAM role \"codedeploy-agent-integ-test-deployment-role\" if it doesn't exist\n* An IAM role \"codedeploy-agent-integ-test-instance-role\" if it doesn't exist\n* An IAM user \"codedeploy-agent-integ-test-instance-user\" if it doesn't exist. (Access key will be recreated.)\n* A CodeDeploy application\n* Startup the codedeploy agent on your host\n* A CodeDeploy deployment group with your host in it\n* A CodeDeploy deployment to your host.\n* Local Deployments to your host.\n\nIt terminates the test ec2 instance and deletes the CodeDeploy application at the end of each test run.\nIt also terminates any test ec2 instances before starting up the test.\n\nCreate your default aws credentials file in the default location (~/.aws/credentials on linux/mac and %USERPROFILE%.awscredentials on windows). Add your AWS access key, secret key, and optionally your session token there. The access key should have permission to create the above mentioned resources. You can also change the default region. Note that temporary credentials won't work. \n\nSample format of the credentials file:\n\n```\n[default]\naws_access_key_id=<keyID>\naws_secret_access_key=<key>\n```\n\nTo run the integration test execute:\n\n```\nrake test-integration\n```\n", "release_dates": ["2024-01-17T23:05:50Z", "2023-05-22T23:46:01Z", "2023-04-03T19:12:02Z", "2022-09-16T17:47:43Z", "2021-05-24T17:18:00Z", "2021-03-15T05:24:14Z", "2020-09-25T01:05:09Z", "2020-08-01T00:11:33Z", "2020-06-25T01:07:14Z", "2018-12-06T21:40:52Z", "2018-06-12T21:38:56Z", "2018-05-16T18:45:34Z", "2017-04-07T01:19:23Z", "2017-01-06T16:53:57Z", "2016-11-21T19:12:42Z", "2016-10-21T20:52:06Z", "2016-08-15T21:25:50Z", "2016-07-11T19:04:59Z", "2016-06-16T20:33:44Z", "2016-04-14T00:14:27Z", "2016-03-15T18:44:37Z", "2015-11-25T00:23:18Z", "2015-10-27T21:54:05Z", "2015-10-16T00:44:13Z", "2015-07-24T20:58:58Z"]}, {"name": "aws-codeguru-cli", "description": "Command line wrapper to interact with CodeGuru Reviewer", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# CodeGuru Reviewer CLI Wrapper\nSimple CLI wrapper for CodeGuru reviewer that provides a one-line command to scan a local clone of a repository and\nreceive results. This CLI wraps the [AWS CLI](https://aws.amazon.com/cli/) commands to communicate with \n[AWS CodeGuru Reviewer](https://aws.amazon.com/codeguru/). Using CodeGuru Reviewer may generate metering fees\nin your AWS account. See the [CodeGuru Reviewer pricing](https://aws.amazon.com/codeguru/pricing/) for details.\n\n### Table of Contents\n- [Installation](#installation)\n- [Using the CLI](#using-the-cli)\n- [Suppressing Recommendations](#suppressing-recommendations)\n- [Running from CI/CD](#running-from-cicd)\n- [Security](#security)\n- [License](#license)\n\n## Installation\n\n### Prerequisites\n\nTo run the CLI, we need to have a version of git, Java (e.g., [Amazon Corretto](https://aws.amazon.com/corretto/?filtered-posts.sort-by=item.additionalFields.createdDate&filtered-posts.sort-order=desc)) \nand the [AWS Command Line interface](https://aws.amazon.com/cli/) installed. \nVerify that both applications are installed on our machine by running:\n\n```\njava -version\nmvn --version\naws --version\ngit --version\n```\n\nWe will also need working credentials on our machine to interact with our AWS account. \nLearn more about setting up credentials for AWS here: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html.\n\nYou can always use the CLI with *Admin* credentials but if you want to have a specific role to use the CLI, your\n credentials must have at least the following permissions:\n\n```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"codeguru-reviewer:ListRepositoryAssociations\",\n                \"codeguru-reviewer:AssociateRepository\",\n                \"codeguru-reviewer:DescribeRepositoryAssociation\",\n                \"codeguru-reviewer:CreateCodeReview\",\n                \"codeguru-reviewer:DescribeCodeReview\",\n                \"codeguru-reviewer:ListRecommendations\",\n                \"iam:CreateServiceLinkedRole\"\n            ],\n            \"Resource\": \"*\",\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:GetBucket*\",\n                \"s3:List*\",\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::codeguru-reviewer-cli-*\",\n                \"arn:aws:s3:::codeguru-reviewer-cli-*/*\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n```\n\n\n## Using the CLI\n\nYou can download the [aws-codeguru-cli](https://github.com/aws/aws-codeguru-cli/releases/latest) from the releases section.\nDownload the latest version and add it to your `PATH`:\n```\ncurl -OL https://github.com/aws/aws-codeguru-cli/releases/download/0.1.0/aws-codeguru-cli.zip\nunzip aws-codeguru-cli.zip\nexport PATH=$PATH:./aws-codeguru-cli/bin\n```\n\n\n\n### Scan an Example\n\nNow, let's download an example project (requires Maven):\n```\ngit clone https://github.com/aws-samples/amazon-codeguru-reviewer-sample-app\ncd amazon-codeguru-reviewer-sample-app\nmvn clean compile\n```\nAfter compiling, we can run CodeGuru with:\n```\naws-codeguru-cli --root-dir ./ --build target/classes --src src --output ./output\nopen output/codeguru-report.html \n```\nwhere `--root-dir .` specifies that the root of the project that we want to analyze. The option `--build target/classses` states that the build artifacts are located under `./target/classes` and `--src` says that we only want to analyze source files that are\nlocated under `./src`. The option `--output ./output` specifies where CodeGuru should write its recommendations to. By default,\nCodeGuru produces a Json and Html report.\n\nYou can provide your own bucket name using the `--bucket-name` option. Note that, currently, CodeGuru Reviewer only\nsupports bucket names that start with the prefix `codeguru-reviewer-` out of the box. If you choose a different naming\npattern for your bucket you need to:\n1. Grant `S3:GetObject` permissions on the S3 bucket to `codeguru-reviewer.amazonaws.com`\n2. If you are using SSE in the S3 bucket, grant `KMS::Decrypt` permissions to `codeguru-reviewer.amazonaws.com`\n\n### Using Encryption\n\nCodeGuru Reviewer allows you to use a customer managed key (CMCMK) to encrypt the contents of the S3 bucket that is used \nto store source and build artifacts, and all metadata and recommendations that are produced by CodeGuru Reviewer. \nFirst, create a customer managed key in KMS.\nYou will need to grant CodeGuru Reviewer permission to decrypt artifacts with this key by adding the \nfollowing Statement to your Key policy:\n\n```json\n{\n    \"Sid\": \"Allow CodeGuru to use the key to decrypt artifacts\",\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"AWS\": \"*\"\n    },\n    \"Action\": [\n        \"kms:Decrypt\",\n        \"kms:DescribeKey\"\n    ],\n    \"Resource\": \"*\",\n    \"Condition\": {\n        \"StringEquals\": {\n            \"kms:ViaService\": \"codeguru-reviewer.amazonaws.com\",\n            \"kms:CallerAccount\": [Your AWS ACCOUNT ID]\n        }\n    }\n}\n```\nThen, enable server-side encryption for the bucket that you are using with CodeGuru Reviewer. The bucket name should be\n`codeguru-reviewer-cli-[YOUR ACCOUNT]-[YOUR REGION]`, unless you provided a custom name. For encryption, use the\nKMS key that you created in the previous step.\n\nNow you can analyze a repository by providing the KMS key ID (not the alias). For example:\n```\n aws-codeguru-cli -r ./ -kms 12345678-abcd-abcd-1234-1234567890ab\n```\nThe first time you analyze a repository with the CodeGuru Reviewer CLI, a new association will be created and\nthe provided key will be associated with this repository. Fur subsequent scans, you do not need to provide the \nkey again. Note that you can start using a key after the repository is already associated. If you want to switch\nfrom not using a key to using a key, you need to delete the existing association first in the AWS Console and\nthen trigger a new scan with the CLI where you provide the key.\n\n\n## Suppressing Recommendations\n\nThe CodeGuru Reviewer CLI searches for a file named `.codeguru-ignore.yml` where users can specify criteria\nbased on which recommendations should be suppressed. Suppressed recommendations will not be returned by the CLI,\nbut still show up in the AWS console.\n\nThe `.codeguru-ignore.yml` file can use any of the filter criteria shown below:\n\n```yaml\nversion: 1.0  # The Version field is mandatory. All other fields are optional. \n\n# The CodeGuru Reviewer CLI produces a recommendations.json file which contains deterministic IDs for each\n# recommendation. This ID can be excluded so that this recommendation will not be reported in future runs of the\n# CLI.\nExcludeById:\n- '4d2c43618a2dac129818bef77093730e84a4e139eef3f0166334657503ecd88d'\n\n# We can tell the CLI to exclude all recommendations below a certain severity. This can be useful in CI/CD integration.\nExcludeBelowSeverity: 'HIGH'\n\n# We can exclude all recommendations that have a certain tag. Available Tags can be found here:\n# https://docs.aws.amazon.com/codeguru/detector-library/java/tags/\n# https://docs.aws.amazon.com/codeguru/detector-library/python/tags/\nExcludeTags:\n  - 'maintainability'\n\n# We can also exclude recommendations by Detector ID. Detector IDs can be found here:\n# https://docs.aws.amazon.com/codeguru/detector-library\nExcludeRecommendations:\n# Ignore all recommendations for a given Detector ID \n  - detectorId: 'java/aws-region-enumeration@v1.0'\n# Ignore all recommendations for a given Detector ID in a provided set of locations.\n# Locations can be written as Unix GLOB expressions using wildcard symbols.\n  - detectorId: 'java/aws-region-enumeration@v1.0'\n    Locations:\n      - 'src/main/java/com/folder01/*.java'\n\n# Excludes all recommendations in the provided files. Files can be provided as Unix GLOB expressions.\nExcludeFiles:\n  - tst/**\n\n```\n\nOnly the `version` field is mandatory in the `.codeguru-ignore.yml` file. All other entries are optional, and\nthe CLI will understand any combination of those entries.\n\nAn example of such a configuration file can be found [here](https://github.com/aws/aws-codeguru-cli/blob/main/.codeguru-ignore.yml).\n\n## Running from CI/CD\n\nYou can use this CLI to run CodeGuru from inside your CI/CD pipeline. \nSee [this action](.github/workflows/cicd-demo.yml) as an example. To use the CLI in CI/CD, you need working credentials.\nYou can use this [CDK template](https://github:com/aws-samples/aws-codeguru-reviewer-cicd-cdk-sample) to set up OIDC credentials for Github Actions.\n\nThen you can run the CLI in non-interactive mode using the `--no-prompt` option, and use the option\n`--fail-on-recommendations` to return a non-zero exit code if recommendations are reported.\nYou can specify a region and  AWS profile using the `--region` and `--profile` options as needed:\n```\naws-codeguru-cli --region [BUCKET REGION] --no-prompt  --fail-on-recommendations -r ./ ...\n```\nobtain the commit range works differently for different CI/CD providers. For example, GitHub provides the relevant\ncommits via environment variables such as `${{ github.event.before }}` and `${{ github.event.after }}`.\n\nAn end-to-end example is provided in [this action](.github/workflows/cicd-demo.yml).\n\n### Build from Source\n\nTo build the project, you need Java 8 or later. Checkout this repository and run:\n```\n./gradlew installDist\n```\nand now run your local build with:\n```\n./build/install/aws-codeguru-cli/bin/aws-codeguru-cli\n```\nYou can run a self-test with:\n```\n./build/install/aws-codeguru-cli/bin/aws-codeguru-cli -r . -s src/main/java -b build/libs -c HEAD^:HEAD\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2023-05-22T12:53:45Z", "2022-10-10T13:44:50Z", "2022-07-08T19:40:24Z", "2022-07-01T16:50:39Z", "2022-04-05T21:37:16Z", "2022-01-27T18:03:51Z"]}, {"name": "aws-connected-device-framework", "description": null, "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Connected Device Framework\n## Introduction\n\nManaging connected devices involves multiple phases of a device's lifecycle.  The phases a typical connected device goes through are : manufacturing, onboarding, operations, support and analytics.  In each of these phases, a unique set of capabilities are required.  The AWS Connected Device Framework (CDF) encompasses a set of modular micro-services (referred to simply as modules) to cater to connected devices in each of their lifecycle phases.\n\nThe framework is particularly well suited for enterprise use cases which require product definition, onboarding and managing a diverse ecosystem of connected devices. The included modules facilitate:\n\n* Product template definition\n* Provisioning\n* Configuration and software updates\n* Organizing devices into hierarchies\n* Maintaining and updating device configuration\n* Device command and control\n* Device simulation\n* Fleet simulation\n\n## FAQ\n\nTLDR: Read the [FAQ](source/docs/faq.md).\n\n## Changelog\n\nChangelog and release artifacts can be found [here](https://github.com/aws/aws-connected-device-framework/releases).\n\nDetails on any major changes along with migration instructions can be found in the [Migration Guide](source/docs/migration.md).\n\n## Tags / Branches\n\nIf deploying to a production environment, always ensure you are checking out one of the release tags rather than checking out the `main` branch. This will ensure you do not inadvertently deploy a new version of the framework without testing it first.\n\nAny other branches besides `main` and the release tags are considered experimental / in-progress and should be used with caution. Once the branch is merged into the `main` branch, it will be considered stable and will be released as a release.\n\n## Challenges\n\nImplementing, deploying and maintaining IoT services can be significantly more complex than traditional software services due to a number of challenges faced:\n\n**Significant undifferentiated heavy lifting:**  It can take months, if not years, to build out an Facade API.\n\n**Skills gap:**  Finding product and IoT specialists is one problem, but then needing to find those same people who possess AWS knowledge is significantly harder.\n\n**Bridging historically air-gapped systems:**  Traditionally different areas within a business, such as manufacturing, operations, and support, have been isolated from one another.  Implementing a new IoT service is a once in a generation opportunity to look at the efficiencies of bridging these systems and future proof for growth.\n\n**Limitations with turn-key solutions:**  Off-the-shelf solutions may be opinionated in their implementation such that incompatible limitations are introduced, as well as potential scalability issues.\n\n**Legacy devices:**  There may be an existing population of devices deployed in the wild that need to be transitioned into a new IoT service.\n\n**Complex security requirements:**  Constrained, intermittently connected devices, as well as regional governance, introduce complexity.\n\n**Long term maintainability of software:**  If a software's architecture and implementation does not take into consideration its longevity that comes with unforeseen emerging requirements, its maintainability, scalability and reliability can be significantly impacted as well as a business losing its agility to bring new products and services to market.\n\nThe ***AWS Connected Device Framework (CDF)*** is a platform comprising of a number of production ready micro-services, all architected and implemented using software and AWS best practices, which builds upon the AWS IoT building blocks to address these challenges.\n\n## Device Lifecycle View\n\nThe CDF modules span the following life cycle phases:\n\n![Life Cycle Phases](source/docs/images/cdf-core-hla-lifecycle.png)\n\n## Architecture\n\nThe CDF modules can be mostly deployed independently. The following shows dependencies between the modules (dotted line is an optional dependency).\n\n![Dependencies](source/docs/images/cdf-core-hla-HLA.png)\n\nThe CDF modules form a layer above the AWS building blocks as shown in the following architecture diagram. A typical customer deployment will involve the development of facade layer (the consuming application(s) of CDF modules) that contains the customer's unique business logic and orchestrates the underlying CDF modules.\n\n![Dependencies](source/docs/images/cdf-core-hla-hla-aws.png)\n\n## Development\n\n- [Getting Started](source/docs/development/quickstart.md)\n- [Prerequisites for Development](source/docs/development/prerequisites.md)\n- [Consuming application (aka facades)](source/docs/consuming-application.md)\n- [Tech Stack](source/docs/development/tech-stack.md)\n- [Dependency Injection](source/docs/development/dependency-injection.md)\n- [Unit testing](source/docs/development/unit-testing.md)\n- [Integration tests](source/packages/integration-tests/README.md)\n\n## Deployment\n\n- [Release notes](https://github.com/aws/aws-connected-device-framework/releases)\n- [Migrating major changes](source/docs/migration.md)\n- [Deploying](source/docs/deployment.md)\n- [Private API Gateway support](source/docs/private_api_gateways.md)\n- [CI/CD](source/docs/cicd.md)\n\n## AWS Connected Device Framework Modules\n\nAWS CDF is comprised of the following modules. You only need to deploy and enable the modules that you are interested in using:\n\n### Bulk Certificate Creation\n\nWith this module a user can request large batches (think 1000's) of device certificates and public/private keys which can later be loaded onto a device. This is useful where customers have a hardware vendor who may not have the ability to create their own device certificates, and the customer does not want to share their CA, so instead can provide access to this module to create the device certificates as required.\n\nSee [overview](source/packages/services/bulkcerts/README.md).\n\n### Provisioning\n\nThe provisioning module utilizes [AWS IoT Device Provisioning](https://docs.aws.amazon.com/iot/latest/developerguide/iot-provision.html) to provide both programmatic and bulk device provisioning capabilities.  The provisioning module simplifies the use of AWS IoT Device Provisioning by managing a set of provisioning templates to use with both provisioning approaches.\n\nIn addition, it allows for extending the capabilities of the AWS IoT Device Provisioning template functionality.  To provide an example, an AWS IoT Device Provisioning template allows for creating certificate resources by providing a certificate signing request (CSR), a certificate ID of an existing device certificate, or a device certificate created with a CA certificate registered with AWS IoT.  This module extends these capabilities by also providing the ability to automatically create (and return) new keys and certificates for a device, or to create a device certificate without the CA being registered in the account.\n\nSee [overview](source/packages/services/provisioning/README.md).\n\n### Greengrass V2 Provisioning\n\nTakes care of everything cloud side when it come to Greengrass v2. Allows you to define a template of components to deploy as a Greengrass core, as well as the cloud provisioning of Greengrass core and connected devices. In addition allows you to manage and roll out updates at scale..\n\nSee [overview](source/packages/services/greengrass2-provisioning/README.md).\n\n### Device Patcher\n\nAllows for the remote installation and configuration of physical devices, such as remotely installing device certificates and the Greengrass SDK on devices intended to be Greengrass core devices.\n\nSee [overview](source/packages/services/device-patcher/README.md).\n\n### Certificate Renewer\n\n*NOTE: Will be released once fully tested.*\n\nIdentifies soon to expire certificates, and if the device is still active/authorized, will create and register new certificates, then inform the device of the new certificate being available.\n\n### Certificate Vendor\n\nManages the secure delivery of certificates, whether delivered over mqtt or to be downloaded from S3, to a device that can be used for elevating and/or rotating certificates.\n\nSee [overview](source/packages/services/certificatevendor/README.md).\n\n### Certificate Activator\n\nProvides a reference implementation of how to combine JITR (Just In Time Registration) functionality with the rest of CDF:  verifies certificates against a whitelist / certificate revocation list, provisions devices, and uses Asset Library profiles to initialize a device\u2019s data.\n\nSee [overview](source/packages/services/certificateactivator/README.md).\n\n### Asset Library\n\nAn enhanced device registry that augments (not replaces) the AWS IoT Device Registry, allowing one to manage their fleet of devices placed within multiple hierarchical groups.  Each group within a hierarchy can represent something meaningful to the business such as location, manufacturer, device types, firmware versions, etc.\n\nWith the Asset Library one can define complex models, such as modeling the components of a vehicle.\n\nSee [overview](source/packages/services/assetlibrary/README.md).\n\n### Notifications\n\nAllows one to configure types of events (such as a low battery alert) from multiple different event sources (AWS IoT Core, DynamoDB Stream, Kinesis Data Stream, API Gateway), which interested parties (user, service) can subscribe to receive alerts on events via SNS, MQTT republish, mobile push, or to store in a DynamoDB table.\n\nSee [overview](source/packages/services/events-processor/README.md).\n\n### Device Monitoring\n\nDetects the connected status of a device (replaced by Fleet Indexing capabilities, but still useful if a customer is not using Fleet Indexing).\n\nSee [overview](source/packages/services/device-monitoring/README.md).\n\n### Commands\n\n> Note: this module is deprecated and has been replaced with the _Command & Control_ module.\n\nUtilizes AWS IoT Jobs to issue commands to a device or set of devices, and optionally inspect their execution status.  It augments AWS IoT jobs by providing the ability to create Job templates (job document, parameters, and files), and enforcing that each requested command adheres to a template before executing.\n\nAlso allows for sending jobs to thousands of devices, by automatically managing temporary groups to overcome any limitations with the no. of allowed targets.  Can optionally use Asset Library devices, groups, and search queries as Job targets.\n\nSee [overview](source/packages/services/commands/README.md).\n\n### Command & Control\n\nThis module provides a simple zero-code approach to implement command and control functionality using AWS IoT Shadows, AWS IoT Jobs, and/or MQTT topics as desired. In addition, it supports sending to a variety of different targets regardless of the delivery method configured: a thing or list of things, a thing group or list of thing groups, a dynamic group or list of dynamic groups, an Asset Library device or list of devices, an Asset Library group or list of groups, an Asset Library search query, or any combination of.\n\nFinally, any messages sent back from the device related to the command message are correlated to the original message where the entire conversation can be easily retrieved.\n\nSee [overview](source/packages/services/command-and-control/README.md).\n\n### Asset Library History\n\nTracks and stores all changes made to the Asset Library (devices, groups, policies and/or templates) for auditing purposes.\n\nSee [overview](source/packages/services/assetlibraryhistory/README.md).\n\n### Reference implementations\n\n*NOTE: Not released yet.*\n\nA simple demo implementation, as well as a full featured implementation (Connected Mobility Solution) are available for reference.\n\n### CI/CD\n\nFully automated AWS CodePipeline based continuous delivery pipeline, managing the building, testing, and deployment of modules.\n\n### Logging\n\nCloudWatch based logging.  X-Ray support.\n\n### Authn / Authz\n\nSupports multiple authentication options: Lambda (request and token) authorizers, IAM, Cognito, API keys, and private API Gateway.\n\nThe Asset Library supports a fine-grained access control mode, suitable for multi-tenancy scenarios.\n\n### Device simulator\n\n*NOTE: Reference implementation on how to use is not released yet.*\n\nA framework that can be followed to implement a device simulator. Includes a reference implementation of a smart Kettle device, as well as a more advanced reference implementation of a vehicle simulator.\n\n### Fleet simulator\n\nSee [overview](source/packages/services/simulation-manager/README.md).\n\nScales out device simulators for load testing your platform, as well as executing test plans to test other areas of your platform.\n\n## Copyright\n\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\nLicensed under the Apache License Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at\n\n    http://www.apache.org/licenses/\n\nor in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, express or implied. See the License for the specific language governing permissions and limitations under the License.\n", "release_dates": ["2022-08-08T18:58:06Z", "2022-08-03T05:29:54Z", "2022-08-01T01:02:22Z", "2022-06-27T06:49:18Z", "2022-05-17T22:26:05Z", "2022-03-28T18:28:51Z", "2022-03-15T19:20:48Z", "2022-02-18T19:50:40Z", "2022-02-18T03:39:30Z", "2022-02-17T17:37:00Z", "2022-01-28T04:40:14Z", "2022-01-05T20:09:28Z", "2022-01-05T03:13:33Z", "2021-12-21T23:21:12Z", "2021-12-08T23:08:09Z"]}, {"name": "aws-corewcf-extensions", "description": "AWS CoreWCF Extensions is a collection of extension libraries for CoreWCF and WCF that provide cloud-native binding for Amazon SQS.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS CoreWCF Server Extensions\n[![Build and Deploy](https://github.com/aws/aws-corewcf-extensions/actions/workflows/build-and-deploy.yml/badge.svg)](https://github.com/aws/aws-corewcf-extensions/actions/workflows/build-and-deploy.yml)\n\nAWS CoreWCF Extensions is a collection of extension libraries for CoreWCF and WCF that provide cloud-native binding for Amazon SQS.  \n\nThe AWS.CoreWCF.Extensions package contains async binding and transports for CoreWCF Services.\n\nThe AWS.WCF.Extensions package contains extensions for WCF Clients to send messages via a SQS transport.\n\n## Package Status\n\n| Package                                                                                      | NuGet Stable                                                                                     | Downloads                                                                                     |\n|:---------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------:|\n| [AWS.CoreWCF.Extensions](https://www.nuget.org/packages/AWS.CoreWCF.Extensions/) | [![AWS.CoreWCF.Extensions](https://img.shields.io/nuget/v/AWS.CoreWCF.Extensions.svg)](https://www.nuget.org/packages/AWS.CoreWCF.Extensions/) | [![AWS.CoreWCF.Extensions](https://img.shields.io/nuget/dt/AWS.CoreWCF.Extensions)](https://www.nuget.org/packages/AWS.CoreWCF.Extensions/) |\n| [AWS.WCF.Extensions](https://www.nuget.org/packages/AWS.WCF.Extensions/)                                 | [![AWS.WCF.Extensions](https://img.shields.io/nuget/v/AWS.WCF.Extensions.svg)](https://www.nuget.org/packages/AWS.WCF.Extensions/)                                 | [![AWS.WCF.Extensions](https://img.shields.io/nuget/dt/AWS.WCF.Extensions)](https://www.nuget.org/packages/AWS.WCF.Extensions/)\n\n## Getting Started\n\nFor a Full Example, see the [sample](./sample/README.md) app.\n### WCF Client\n\n* Add [**AWS.WCF.Extensions**](https://www.nuget.org/packages/AWS.WCF.Extensions) to your project as a Nuget Package.\n* Follow the example below to see how the library can be integrated into your application for to use SQS as the transport layer for a service.\n\n```csharp\n// this example assumes an existing wCF Service called ILoggingService\n\nvar sqsClient = new AmazonSQSClient();\n\nvar sqsBinding = new AWS.WCF.Extensions.SQS.AwsSqsBinding(sqsClient, queueName);\nvar endpointAddress = new EndpointAddress(new Uri(sqsBinding.QueueUrl));\nvar factory = new ChannelFactory<ILoggingService>(sqsBinding, endpointAddress);\nvar channel = factory.CreateChannel();\n((System.ServiceModel.Channels.IChannel)channel).Open();\n\n// send a message via the wcf client\nchannel.LogMessage(\"Hello World\");\n```\n\n### CoreWCF Server\n\n* Add [**AWS.CoreWCF.Extensions**](https://www.nuget.org/packages/AWS.CoreWCF.Extensions) to your project as a Nuget Package.\n* Follow the example below to see how the library can be integrated into your application for to use SQS as the transport layer for a service.\n\n```csharp\nvar builder = WebApplication.CreateBuilder(args);\n\nbuilder.Services\n    .AddDefaultAWSOptions(new AWSOptions())\n    .AddServiceModelServices()\n    .AddQueueTransport()\n    .AddSQSClient(_queueName);\n\nvar app = builder.Build();\n\nvar queueUrl = app.EnsureSqsQueue(_queueName);\n\napp.UseServiceModel(services =>\n{\n    services.AddService<LoggingService>();\n    services.AddServiceEndpoint<LoggingService, ILoggingService>(\n        new AwsSqsBinding(),\n        queueUrl\n    );\n});\n\napp.Run();\n```\n\n### CoreWCF.SQS Server Performance Tuning\n\nUsing Amazon SQS as a backplane for CoreWCF allows Servers to scale both horizontally and vertically.  Deploying a CoreWCF Server application to additional virtual machines or docker instances will increase overall message throughput, as well as improve resiliency, and does not require modification of the application code.  This is possible because Amazon SQS automatically handles multiple readers.\n\nIt's also possible to control the performance of an individual Server application.  Doing so may offer significant throughput or efficiency increases depending on the characteristics of your application and the host you're running on.\n\nA Server can be configured to use additional threads to process incoming messages and increase total throughput by setting the `ConcurrencyLevel` in the `AwsSqsBinding` constructor.  Setting this to a higher value allows multiple threads to deserilaize and ingest incoming messages as well as go through various extensibility points before CoreWCF dispatches the message for execution by the Service.\n\n_NOTE:_ This will only impact message ingestion and extensability.  It does not change the Service Dispatch concurrency strategy and will not change how many messages are processed by a Service concurrently.\n\n_NOTE:_ Setting this value above 10 will have no meaningful impact.  CoreWCF.SQS will read up to 10 messages in a batch from an Amazon SQS queue, and all messages must be processed before another batch is requested.  Increasing the Concurrency Level above 10 will leave any excess threads without any work to do.\n\nBelow is an example of increasing ConcurrencyLevel to increase message throughput:\n\n```\n/// <summary>\n/// Example of increasing ConcurrencyLevel\n/// </summary>\npublic static void Main(string[] args)\n{\n    var inventoryServiceQueue = \"inventory\";\n\n    var builder = WebApplication.CreateBuilder(args);\n\n    // if needed, customize your aws credentials here,\n    // otherwise it will default to searching ~\\.aws\n    var awsCredentials = new AWSOptions();\n\n    builder.Services\n        .AddDefaultAWSOptions(awsCredentials)\n        .AddServiceModelServices()\n        .AddQueueTransport()\n        .AddSQSClient(inventoryServiceQueue);\n\n    var app = builder.Build();\n\n    var inventoryUrl = app.EnsureSqsQueue(inventoryServiceQueue);\n\n    app.UseServiceModel(services =>\n    {\n        services.AddService<InventoryService>();\n        services.AddServiceEndpoint<InventoryService, IInventoryService>(\n            new AwsSqsBinding(concurrencyLevel: 6), // <----- increase concurrencyLevel\n            inventoryUrl);\n    });\n\n    app.Run();\n}\n```\n\nA Server can also be configured to listen to multiple Amazon SQS Queues.  If expected traffic on several queues is expected to be light, enable one process to multiple queues can increase compute density and reduce operating costs.\n\nBelow is an example of a single Server listening to multiple Queues\n\n```\n/// <summary>\n/// Example of listening to multiple Queues\n/// </summary>\npublic static void Main(string[] args)\n{\n    var inventoryServiceQueue = \"inventory\";\n    var orderProcessingServiceQueue = \"orderProcessing\";\n\n    var builder = WebApplication.CreateBuilder(args);\n\n    // if needed, customize your aws credentials here,\n    // otherwise it will default to searching ~\\.aws\n    var awsCredentials = new AWSOptions();\n\n    builder.Services\n        .AddDefaultAWSOptions(awsCredentials)\n        .AddServiceModelServices()\n        .AddQueueTransport()\n        .AddSQSClient(inventoryServiceQueue) // <----- Add multiple SQS Clients\n        .AddSQSClient(orderProcessingServiceQueue);\n\n    var app = builder.Build();\n\n    var inventoryUrl = app.EnsureSqsQueue(inventoryServiceQueue);\n    var orderProcessingUrl = app.EnsureSqsQueue(orderProcessingServiceQueue);\n\n    app.UseServiceModel(services =>\n    {\n        services.AddService<InventoryService>();\n        services.AddServiceEndpoint<InventoryService, IInventoryService>(\n            new AwsSqsBinding(), \n            inventoryUrl);\n\n        // Add multiple Service / ServiceEndpoints\n        services.AddService<OrderProcessing>();\n        services.AddServiceEndpoint<OrderProcessing, IOrderProcessing>(\n            new AwsSqsBinding(), \n            orderProcessingUrl);\n    });\n\n    app.Run();\n}\n```\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues\nfor tracking bugs and feature requests.\n\n* If it turns out that you may have found a bug,\n  please open an [issue](https://github.com/aws/aws-corewcf-extensions/issues/new)\n  \n  \n## How to use this code?\n* Clone the Git repository.\n* Compile by running `dotnet build .`\n* Edit the solution by opening `AWS.CoreWCF.Extensions.sln` using Visual Studio or Rider.\n\n## Contributing\n\n* We welcome community contributions and pull requests. See\n[CONTRIBUTING](./CONTRIBUTING.md) for information on how to set up a development\nenvironment and submit code.\n\n# Additional Resources\n \n- [CoreWCF](https://github.com/CoreWCF/CoreWCF)\n- [WCF](https://github.com/dotnet/wcf)\n- [Amazon SQS](https://aws.amazon.com/sqs/)\n\n# License\n\nLibraries in this repository are licensed under the Apache 2.0 License.\n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.  \n\n", "release_dates": []}, {"name": "aws-cryptographic-material-providers-library", "description": "AWS Cryptographic Material Providers Library", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Cryptographic Material Providers Library\n\n\ud83d\udce3 Note: This repository contains the source code and related files for all\nlanguage implementations of the AWS Cryptographic Material Providers Library.\nSee our [supported languages](#supported-languages) section for more information.\n\nThe AWS Cryptographic Material Providers Library abstracts lower level cryptographic materials management of encryption and decryption materials.\nIt uses cryptographic best practices to protect the data keys that protect your data.\nThe data key is protected with a key encryption key called a _wrapping key_.\nThe encryption method returns the data key and one or more encrypted data keys.\nSupported libraries use this information to perform envelope encryption.\nThe data key is used to protect your data,\nand the encrypted data keys are stored alongside your data\nso you don't need to keep track of the data keys separately.\nYou can use AWS KMS keys in [AWS Key Management Service](https://aws.amazon.com/kms/)(AWS KMS) as wrapping keys.\nThe AWS Cryptographic Material Providers Library\nalso provides APIs to define and use wrapping keys from other key providers.\n\nThe AWS Cryptographic Material Providers Library provides methods for encrypting and decrypting cryptographic materials used in higher level client side encryption libraries.\n\n[Security issue notifications](./CONTRIBUTING.md#security-issue-notifications)\n\n## Security\n\nIf you discover a potential security issue in this project\nwe ask that you notify AWS/Amazon Security via our\n[vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/).\nPlease **do not** create a public GitHub issue.\n\n## Getting Started\n\n### Repository structure\n\nThis repository is a top level repository which houses all source code in order to compile this library into\ndifferent runtimes.\n\nThis library is written in Dafny, a formally verifiable programming language that can be compiled into\ndifferent runtimes. This library is currently **ONLY** supported in Java and .NET\n\n### Optional Prerequisites\n\n#### AWS Integration\n\nYou don't need an Amazon Web Services (AWS) account to use the AWS Cryptographic Material Providers Library,\nbut some APIs require an AWS account, an AWS KMS key, or an Amazon DynamoDB Table.\nIf you are using the AWS Cryptographic Material Providers Library for Java you will need the AWS SDK for Java V2.\nIf you are using the AWS Cryptographic Material Providers Library for .NET you will need the AWS SDK for .NET V3.\n\n**NOTE**: The `KmsAsyncClient` and `DynamoDBAsyncClient` are not supported, only the synchronous clients.\n\n- **To create an AWS account**, go to [Sign In or Create an AWS Account](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html) and then choose **I am a new user.** Follow the instructions to create an AWS account.\n\n- **To create a symmetric encryption KMS key in AWS KMS**, see [Creating Keys](https://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html).\n\n- **To download and install the AWS SDK for Java 2.x**, see [Installing the AWS SDK for Java 2.x](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/getting-started.html).\n- **To download and install the AWS SDK for .Net 3.x** see [Installing the AWS SDK for .Net v3](https://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/welcome.html)\n\n## Supported Languages\n\n- Java\n- .NET\n- Dafny\n\n## FAQ\n\nSee the [Frequently Asked Questions](https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/faq.html) page in the official documentation.\n", "release_dates": ["2024-01-10T20:24:46Z"]}, {"name": "aws-cryptographic-material-providers-library-java", "description": "AWS Cryptographic Material Providers Library for Java", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Cryptographic Material Providers Library\n\nThe AWS Cryptographic Material Providers Library abstracts lower level cryptographic materials management of encryption and decryption materials.\nIt uses cryptographic best practices to protect the data keys that protect your data.\nThe data key is protected with a key encryption key called a *wrapping key*.\nThe encryption method returns the data key and one or more encrypted data keys.\nSupported libraries use this information to perform envelope encryption.\nThe data key is used to protect your data,\nand the encrypted data keys are stored alongside your data\nso you don't need to keep track of the data keys separately.\nYou can use AWS KMS keys in [AWS Key Management Service](https://aws.amazon.com/kms/)(AWS KMS) as wrapping keys.\nThe AWS Cryptographic Material Providers Library\nalso provides APIs to define and use wrapping keys from other key providers. \n\nThe AWS Cryptographic Material Providers Library provides methods for encrypting and decrypting cryptographic materials used in higher level client side encryption libraries. \n\n[Security issue notifications](./CONTRIBUTING.md#security-issue-notifications)\n\n## Security\nIf you discover a potential security issue in this project\nwe ask that you notify AWS/Amazon Security via our\n[vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/).\nPlease **do not** create a public GitHub issue.\n\n## Getting Started\n\n### Repository structure\nThis repository is a top level repository which houses all source code in order to compile this library into\ndifferent runtimes.\n\nThis library is written in Dafny, a formally verifiable programming language that can be compiled into\ndifferent runtimes. This library is currently **ONLY** supported in Java and .NET \n\n### Optional Prerequisites\n\n#### AWS Integration\nYou don't need an Amazon Web Services (AWS) account to use the AWS Cryptographic Material Providers Library,\nbut some APIs require an AWS account, an AWS KMS key, or an Amazon DynamoDB Table. \nIf you are using the AWS Cryptographic Material Providers Library for Java you will need the AWS SDK for Java V2.\nIf you are using the AWS Cryptographic Material Providers Library for .NET you will need the AWS SDK for .NET V3.\n\n**NOTE**: The `KmsAsyncClient` and `DynamoDBAsyncClient` are not supported, only the synchronous clients.\n\n* **To create an AWS account**, go to [Sign In or Create an AWS Account](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html) and then choose **I am a new user.** Follow the instructions to create an AWS account.\n\n* **To create a symmetric encryption KMS key in AWS KMS**, see [Creating Keys](https://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html).\n\n* **To download and install the AWS SDK for Java 2.x**, see [Installing the AWS SDK for Java 2.x](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/getting-started.html).\n* **To download and install the AWS SDK for .Net 3.x** see [Installing the AWS SDK for .Net v3](https://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/welcome.html)\n\n## FAQ\n\nSee the [Frequently Asked Questions](https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/faq.html) page in the official documentation.\n", "release_dates": ["2023-10-19T21:14:39Z", "2023-07-28T17:51:05Z", "2023-07-21T18:23:31Z", "2023-06-22T18:50:02Z", "2023-06-19T17:36:45Z", "2023-06-07T21:06:47Z"]}, {"name": "aws-cryptographic-material-providers-library-net", "description": null, "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Cryptographic Material Providers Library for .NET \n\nThe AWS Cryptographic Material Providers Library abstracts lower level cryptographic materials management of encryption and decryption materials.\nIt uses cryptographic best practices to protect the data keys that protect your data.\nThe data key is protected with a key encryption key called a *wrapping key*.\nThe encryption method returns the data key and one or more encrypted data keys.\nSupported libraries use this information to perform envelope encryption.\nThe data key is used to protect your data,\nand the encrypted data keys are stored alongside your data\nso you don't need to keep track of the data keys separately.\nYou can use AWS KMS keys in [AWS Key Management Service](https://aws.amazon.com/kms/)(AWS KMS) as wrapping keys.\nThe AWS Cryptographic Material Providers Library\nalso provides APIs to define and use wrapping keys from other key providers. \n\nThe AWS Cryptographic Material Providers Library for .NET provides methods for encrypting and decrypting cryptographic materials used in higher level client side encryption libraries. \n\n\n## Security\nIf you discover a potential security issue in this project\nwe ask that you notify AWS/Amazon Security via our\n[vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/).\nPlease **do not** create a public GitHub issue.\n\n## Getting Started\n\n### Required Prerequisites\nTo use the AWS Cryptographic Material Providers Library for .NET you must have:\n\n* **A .NET Framework 6.0 development environment**\n\n  If you do not have it installed, you can find installation instructions [here](https://dotnet.microsoft.com/en-us/download/dotnet/6.0).\n\n* **Bouncy Castle**\n\n  The AWS Cryptographic Material Providers Library for .NET uses Bouncy Castle for the underlying cryptography and to serialize and deserialize cryptographic objects.\n\n  If you do not have Bouncy Castle, go to https://www.bouncycastle.org/csharp/ to learn more. \n  You can also download it from NuGet\n  ```\n    <PackageReference Include=\"BouncyCastle.Cryptography\" Version=\"2.2.1\" />\n  ```\n\n### Optional Prerequisites\n\n#### AWS Integration\nYou don't need an Amazon Web Services (AWS) account to use the AWS Cryptographic Material Providers Library, \nbut some APIs require an AWS account, an AWS KMS key, or an AWS DynamoDB Table.\nHowever, all APIs require the AWS SDK for .NET V3.\n\nNote that `Async AmazonKeyManagementServiceClient` and `Async DynamoDBAsyncClient` methods are not supported, only the synchronous methods.\n\n* **To create an AWS account**, go to [Sign In or Create an AWS Account](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html) and then choose **I am a new user.** Follow the instructions to create an AWS account.\n\n* **To create a KMS key in AWS KMS**, see [Creating Keys](https://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html).\n\n* **To download and install the AWS SDK for .NET 3.x**, see [Installing the AWS SDK for .NET 3.x](https://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/net-dg-install-assemblies.html).\n\n### Download the AWS Cryptographic Material Providers Library for .NET\nThe AWS Cryptographic Material Providers Library for .NET is available on NuGet and can be referenced\nfrom an existing .csproj.\n\nUsing the dotnet CLI:\n```shell\ndotnet add <your-project-name>.csproj package AWS.Cryptography.MaterialProviders\n```\n\nAlternatively, you may directly modify the `.csproj` and add the\nAWS Cryptographic Material Providers Library to `PackageReference` `ItemGroup`:\n```xml\n<PackageReference Include=\"AWS.Cryptography.MaterialProviders\" />\n```\n\nThe AWS Cryptographic Material Providers Library targets [.NET Framework](https://docs.microsoft.com/en-us/dotnet/framework/) 6.0.\n\n### Additional setup for macOS only\n\nIf you are using macOS then you must install OpenSSL 1.1,\nand the OpenSSL 1.1 `lib` directory must be on the dynamic linker path at runtime.\nAlso, if using an M1-based Mac, you must install OpenSSL and the .NET SDK for x86-64.\nPlease refer to [this wiki](https://github.com/aws/aws-encryption-sdk-dafny/wiki/Using-the-AWS-Encryption-SDK-for-.NET-on-macOS) for detailed instructions.\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": ["2023-09-30T01:05:05Z"]}, {"name": "aws-database-encryption-sdk-dynamodb", "description": "AWS Database Encryption SDK for DynamoDB in Java", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Database Encryption SDK for DynamoDB\n\n\ud83d\udce3 Note: This repository contains the source code and related files for all\nlanguage implementations of the AWS Database Encryption SDK for DynamoDB.\nSee our [supported languages](#supported-languages) section for more information.\n\nThe AWS Database Encryption SDK (DB-ESDK) for DynamoDB is a client-side encryption \nlibrary that allows you to perform attribute-level encryption, enabling you to encrypt specific \nattribute values within items before storing them in your DynamoDB table. All encryption and \ndecryption are performed within your application. This lets you protect sensitive data in-transit \nand at-rest, as data cannot be exposed unless decrypted by your application.\n\nFor more details about the design and architecture of the DB-ESDK for DynamoDB, \nsee the [AWS Database Encryption SDK Developer Guide](https://docs.aws.amazon.com/database-encryption-sdk/latest/devguide/).\n\n# Security\nIf you discover a potential security issue in this project\nwe ask that you notify AWS/Amazon Security via our\n[vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/).\nPlease **do not** create a public GitHub issue.\n\n# Support Policy\nSee [Support Policy](./SUPPORT_POLICY.rst) for details \non the current support status of all major versions of this library.\n\n## Giving Feedback\nWe need your help in making this SDK great.\nPlease participate in the community and contribute to this effort by\nsubmitting issues,\nparticipating in discussion forums and\nsubmitting pull requests through the following channels:\n\n* Submit [issues](https://github.com/aws/aws-database-encryption-sdk-dynamodb-java/issues)\n  \\- this is the **preferred** channel to interact with our team\n* Articulate your\n  [feature request](https://github.com/aws/aws-database-encryption-sdk-dynamodb-java/issues?q=is%3Aopen+is%3Aissue+label%3A%22feature-request%22)\n  or upvote existing ones\n* Ask [questions](https://repost.aws/tags/TAc3VKZnkNQyimpHnCHetNOQ/aws-crypto-tools) on AWS re:Post under AWS Crypto Tools tag\n\n# Getting Started\n\n### Repository structure\nThis repository is a top level repository which houses all source code in order to compile this library into\ndifferent runtimes.\n\nThis library is written in Dafny, a formally verifiable programming language that can be compiled into\ndifferent runtimes. This library is currently **ONLY** supported in Java and .NET \n\n### AWS Integration\nYou need an Amazon Web Services (AWS) account to use the DB-ESDK for DynamoDB as it's specifically designed to work with Amazon DynamoDB. Optionally, you can use AWS Key Management Service (AWS KMS) as your main keyring provider.\n\n* **To create an AWS account**, go to \n  [Sign In or Create an AWS Account](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html)\n  and then choose **I am a new user.**  \n  Follow the instructions to create an AWS account.\n\n* **(Optional) To create a key in AWS KMS**, see\n  [Creating Keys](https://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html).\n\n## Supported Languages\n\n- Java\n- .NET\n- Dafny\n\n# Contributing\n\nSee [CONTRIBUTING](CONTRIBUTING.md) for more information.\n\n# License\n\nThis project is licensed under the Apache-2.0 License.\n\n[ddbenhanced]: https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/dynamodb-enhanced-client.html\n", "release_dates": ["2024-01-17T19:18:14Z", "2023-11-13T20:24:59Z", "2023-11-08T00:52:20Z", "2023-09-11T19:24:23Z", "2023-07-24T20:18:39Z", "2023-06-09T21:25:53Z", "2023-06-09T21:28:09Z"]}, {"name": "aws-dax-go", "description": "AWS DAX SDK for the Go programming language. https://aws.amazon.com/dynamodb/dax", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS DAX SDK for Go\n\naws-dax-go is the official AWS DAX SDK for the Go programming language. https://aws.amazon.com/dynamodb/dax\n\nCheckout our [release notes](https://github.com/aws/aws-dax-go/releases) for\ninformation about the latest bug fixes, updates, and features added to the SDK.\n\n## Getting started\nThe best way to get started working with the SDK is to use go get to add the SDK\nto your Go Workspace manually.\n\n    go get github.com/aws/aws-dax-go\n\nYou could also use [Dep](https://github.com/golang/dep) to add the SDK to your\napplication's dependencies. Using Dep will simplify your update story and help\nyour application keep pinned to a specific version of the SDK.\n\n    dep ensure -add github.com/aws/aws-dax-go\n\n## Making API requests\nThis example shows how you can use the AWS DAX SDK to make an API request.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"github.com/aws/aws-dax-go/dax\"\n\t\"github.com/aws/aws-sdk-go/aws\"\n\t\"github.com/aws/aws-sdk-go/service/dynamodb\"\n)\n\nfunc main() {\n\t\n\tcfg := dax.DefaultConfig()\n\tcfg.HostPorts = []string{\"dax://mycluster.frfx8h.clustercfg.dax.usw2.amazonaws.com:8111\"}\n\tcfg.Region = \"us-west-2\"\n\tclient, err := dax.New(cfg)\n\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"unable to initialize client %v\", err))\n\t}\n\t\n\t\n\t//Connecion to a secure cluster\n\tsecureEndpoint := \"daxs://mycluster.frfx8h.clustercfg.dax.usw2.amazonaws.com\"\n\tsecureCfg := dax.DefaultConfig()\n\tsecureCfg.HostPorts = []string{secureEndpoint}\n\tsecureCfg.Region = \"us-west-2\"\n\t\n\t//WARN: Skip hostname verification of TLS connections. \n\t//The default is to perform hostname verification, setting this to True will skip verification. \n\t//Be sure you understand the implication of doing so, which is the inability to authenticate\n\t//the cluster that you are connecting to.\n\tsecureCfg.SkipHostnameVerification = false\n\t\n\t// DialContext is an optional field in Config.\n\t// If DialContext is being set in Config for a secure/ encrypted cluster, then use dax.SecureDialContext to \n\t// return DialContext. An example of how DailContext can be set using dax.SecureDialContext is shown below.\n\tsecureCfg.DialContext = func(ctx context.Context, network string, address string) (net.Conn, error) {\n\t\t//    fmt.Println(\"Write your custom logic here\")\n\t\tdialCon, err := dax.SecureDialContext(secureEndpoint, secureCfg.SkipHostnameVerification)\n\t\tif err != nil {\n\t\t\tpanic(fmt.Errorf(\"secure dialcontext creation failed %v\", err))\n\t\t}\n\t\treturn dialCon(ctx, network, address)\n\t}\n\tsecureClient, err := dax.New(secureCfg)\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"unable to initialize client %v\", err))\n\t}\n\tfmt.Println(\"secure client created\", secureClient)\n\t\n\t\n\n\tinput := &dynamodb.PutItemInput{\n\t\tTableName: aws.String(\"TryDaxGoTable\"),\n\t\tItem: map[string]*dynamodb.AttributeValue{\n\t\t\t\"pk\":    {S: aws.String(\"mykey\")},\n\t\t\t\"sk\":    {N: aws.String(\"0\")},\n\t\t\t\"value\": {S: aws.String(\"myvalue\")},\n\t\t},\n\t}\n\n\toutput, err := client.PutItem(input)\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"unable to make request %v\", err))\n\t}\n\n\tfmt.Println(\"Output: \", output)\n}\n```\n\n## Feedback and contributing\n**GitHub issues:** To provide feedback or report bugs, file GitHub\n[Issues](https://github.com/aws/aws-dax-go/issues) on the SDK.\nThis is the preferred mechanism to give feedback so that other users can engage in\nthe conversation, +1 issues, etc. Issues you open will be evaluated, and included\nin our roadmap.\n\n**Contributing:** You can open pull requests for fixes or additions to the\nAWS DAX SDK for Go. All pull requests must be submitted under the Apache 2.0\nlicense and will be reviewed by an SDK team member before being merged in.\nAccompanying unit tests, where possible, are appreciated.\n\n## License\n\nThis library is licensed under the Apache 2.0 License. \n", "release_dates": []}, {"name": "aws-deep-learning-containers-utils", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS-Deep-Learning-Containers-Utils\n\nUtilities used by Deep Learning Containers (DLC)\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "aws-dotnet-deploy", "description": "Opinionated tooling that simplifies deployment of .NET applications to AWS.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS .NET deployment tool \n[![nuget](https://img.shields.io/nuget/v/AWS.Deploy.Tools.svg) ![downloads](https://img.shields.io/nuget/dt/AWS.Deploy.Tools.svg)](https://www.nuget.org/packages/AWS.Deploy.Tools/)\n[![build status](https://img.shields.io/github/actions/workflow/status/aws/aws-dotnet-deploy/codebuild-ci.yml?branch=dev)](https://github.com/aws/aws-dotnet-deploy/actions/workflows/codebuild-ci.yml)\n[![code coverage](https://img.shields.io/codecov/c/github/aws/aws-dotnet-deploy/dev.svg)](https://codecov.io/gh/aws/aws-dotnet-deploy)\n\n## Overview\nThis repository contains the AWS Deploy Tool for .NET CLI - the opinionated tooling that simplifies deployment of .NET applications. The tool suggests the right AWS compute service to deploy your application to.  It then builds and packages your application as required by the chosen compute service, generates the deployment infrastructure, deploys your application by using the appropriate deployment engine (Cloud Development Kit (CDK) or native service APIs), and displays the endpoint.\n\nThe tool assumes minimal knowledge of AWS. It is designed to guide you through the deployment process and provides suggested defaults. The tool will show you all compute service options available to deploy your application, and will recommend a default with information about why it was chosen. The other compute service options will be shown with an explanation of their differences. If the selected compute option does not match your needs, you can select a different compute service.\n\nThe goal of the deployment tool is to deploy cloud-native .NET applications that are built with .NET Core 3.1 and above. A cloud-native .NET application is written in .NET with the intent to deploy to Linux. It is not tied to any Windows specific technology such as Windows registry, IIS or MSMQ, and can be deployed on virtualized compute. The tool **cannot** be used to deploy .NET Framework, Desktop, Xamarin, or other applications that do not fit the \"cloud-native\" criteria.\n\nWe welcome your feedback! Please let us know what you think by opening an [issue](https://github.com/aws/aws-dotnet-deploy/issues).\n\n\n## Useful Links\n* [Complete Documentation Guide on GitHub.io](https://aws.github.io/aws-dotnet-deploy/)\n* [Contributing to the Project](https://aws.github.io/aws-dotnet-deploy/contributing/)\n* [AWS Deploy Tool for .NET on NuGet](https://www.nuget.org/packages/AWS.Deploy.Tools)\n* Blog posts:\n  * [AWS Streamlines Deployment experience for .NET applications](https://aws.amazon.com/blogs/developer/aws-announces-a-streamlined-deployment-experience-for-net-applications/)\n  * [Reimagining the AWS .NET deployment experience](http://aws.amazon.com/blogs/developer/reimagining-the-aws-net-deployment-experience/)\n  * [Update on our new AWS .NET Deployment Experience](https://aws.amazon.com/blogs/developer/update-new-net-deployment-experience/)\n  * [Deployment Projects with the new AWS .NET Deployment Experience](https://aws.amazon.com/blogs/developer/dotnet-deployment-projects/)\n* Youtube videos:\n  * [AWS On Air ft. New .NET Deployment Experience - Command Line](https://www.youtube.com/watch?v=5uyL8MXxljc)\n  * [Re:Invent 2021: \u201cWhat\u2019s new with .NET development and deployment on AWS\u201d](https://www.youtube.com/watch?v=UvTJ_Inb634)\n\n## Pre-requisites\n\nTo take advantage of this library you\u2019ll need:\n\n* An AWS account with a local credential profile configured in the shared AWS config and credentials files.\n  * The local credential profile can be configured by a variety of tools. For example, the credential profile can be configured with the [AWS Toolkit for Visual Studio](https://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/credentials.html) or the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html), among others.\n  * Note: You need to make sure to add the appropriate CloudFormation permissions to your credentials's profile / assumed role.\n  * For SSO, please visit the [.NET SDK Reference Guide](https://docs.aws.amazon.com/sdkref/latest/guide/access-sso.html).\n* [.NET 6](https://dotnet.microsoft.com/download) or later\n* [Node.js 14](https://nodejs.org/en/download/) or later\n  * The [AWS Cloud Development Kit (CDK)](https://aws.amazon.com/cdk/) is used by this tool to create the AWS infrastructure to run applications. The CDK requires Node.js to function. This dependency is needed for deployments that are CDK based. If you will be using deployments that are not CDK based, you are not required to have this dependency.\n* (optional) [Docker](https://docs.docker.com/get-docker/)\n  * Used when deploying to a container based service like Amazon Elastic Container Service (Amazon ECS)\n* (optional) The zip cli tool\n   *   Mac / Linux only. Used when creating zip packages for deployment bundles. The zip cli is used to maintain Linux file permissions.\n\n## Getting started\n\nThe deployment tool is distributed as a .NET Tool from NuGet.org. The installation of the tool is managed with the `dotnet` CLI.\n\n### Installing the tool\n\nTo install the deployment tool, use the dotnet tool install command:\n\n```\ndotnet tool install -g aws.deploy.tools\n```\n\nTo update to the latest version of the deployment tool, use the dotnet tool update command.\n\n```\ndotnet tool update -g aws.deploy.tools\n```\n\nTo uninstall it, simply type:\n```\ndotnet tool uninstall -g aws.deploy.tools\n```\n\nOnce you install the tool, you can view the list of available commands by typing:\n```\ndotnet aws --help\n```\n\nTo get help about individual commands like deploy or delete-deployment you can use the `--help` switch with the commands. For example to get help for the deploy command type:\n```\ndotnet aws deploy --help\n```\n\n## Deploying your application\n\nTo deploy your application, `cd` to the directory that contains the .csproj or .fsproj file and type:\n```\ndotnet aws deploy\n```\n\n*(Alternatively the `--project-path` switch can be used to point to specific directory or project file.)*\n\nYou will be prompted to enter the name of the stack that your application will be deployed to. (A **stack** is a collection of **AWS** resources that you can manage as a single unit. In other words, you can create, update, or delete a collection of resources by creating, updating, or deleting stacks.)\n\nOnce you enter the name of the stack, the deployment tool's recommendation engine will inspect your project codebase and provide its recommendation for how you should deploy the application. If possible the tool will also show other compatible ways to deploy the application that you can choose to use over the recommendation.\n\n```\nName the AWS stack to deploy your application to\n(a stack is a collection of AWS resources that you can manage as a single unit.)\n--------------------------------------------------------------------------------\nEnter value (default MyApplication):\n\n\nRecommended Deployment Option\n-----------------------------\n1: ASP.NET Core App to Amazon ECS using Fargate\nASP.NET Core applications built as a container and deployed to Amazon Elastic Container Service (ECS) with compute power managed by AWS Fargate compute engine. Recommended for applications that can be deployed as a container image. If your project does not contain a Dockerfile, one will be generated for the project.\n\nAdditional Deployment Options\n------------------------------\n2: ASP.NET Core App to AWS Elastic Beanstalk on Linux\nDeploy an ASP.NET Core application to AWS Elastic Beanstalk. Recommended for applications that are not set up to be deployed as containers.\n\nChoose deployment option (recommended default: 1)\n```\n\n## Supported application types\n\n### ASP.NET Core web applications\nASP.NET Core applications can be deployed either to virtual servers with AWS Elastic Beanstalk or containers with Amazon Elastic Container Service (Amazon ECS) and AWS App Runner. If you wish to deploy your application as a container and your project does not yet have a `Dockerfile` one will be generated for you into your project during deployment.\nUsing the deployment tool, you will be able to deploy ASP.NET Core web applications to AWS using the AWS Cloud Development Kit (CDK). \nFor users coming from the previous AWS Visual Studio Toolkit experience who have previous Elastic Beanstalk deployments and environments, you will be able to use the deployment tool to deploy to those environments.\n\n#### Elastic Beanstalk backwards compatibility support\nCurrently, the AWS Visual Studio toolkit allows users to deploy their ASP.NET Core web applications to Elastic Beanstalk using a wizard that uses the AWS .NET SDK on the backend to perform the deployment. This process creates the necessary Elastic Beanstalk resources such as a Beanstalk Application and Beanstalk Environment using the AWS .NET SDK. \n\nThe deployment tool uses AWS CDK to perform the deployments to AWS which create a CloudFormation stack that manages all the Elastic Beanstalk resources. In addition to CloudFormation stacks, the tool supports deployments to the existing Elastic Beanstalk environments that were created using the the older version of the AWS Toolkit for Visual Studio.\n\nThe deployment tool will detect existing Elastic Beanstalk environments in your AWS account and list them alongside the CloudFormation stacks. You can deploy your application to an existing Beanstalk environment and update the necessary environment settings. The deployment will be performed using the AWS .NET SDK.\n\nNote: Deploying to existing Beanstalk Environments will not migrate your existing resources to CloudFormation. To create a new CloudFormation stack, you need to explicitly deploy your application to the new deployment target.\n\n### Blazor WebAssembly applications\nBlazor WebAssembly applications can be deployed to an Amazon S3 bucket for web hosting. The Amazon S3 bucket will be created and configured automatically by the tool, which will then upload your Blazor application to the S3 bucket.\n\n### Long running service applications\nPrograms that are meant to run indefinitely can be deployed as an Amazon ECS service. This is common for backend services that process messages. The application will be deployed as a container image. If your project does not yet have a Dockerfile, one will be generated for you into your project during deployment.\n\n### Schedule tasks\nPrograms that need to run periodically, for example, once every hour, can be deployed as a schedule task using Amazon ECS and Amazon CloudWatch Events. The application will be deployed as a container image. If your project does not yet have a `Dockerfile`, one will be generated for you into your project during deployment.\n\n## Supported AWS Services\n\n### AWS Elastic Beanstalk\nAWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time\n\n### Amazon Elastic Container Service\nAmazon ECS is a fully managed container orchestration service that helps you easily deploy, manage, and scale containerized applications. It deeply integrates with the rest of the AWS platform to provide a secure and easy-to-use solution for running container workloads in the cloud and now on your infrastructure with Amazon ECS Anywhere.\n\n### AWS App Runner\nAWS App Runner is a fully managed service that makes it easy for developers to quickly deploy containerized web applications and APIs, at scale and with no prior infrastructure experience required. Start with your source code or a container image. App Runner builds and deploys the web application automatically, load balances traffic with encryption, scales to meet your traffic needs, and makes it easy for your services to communicate with other AWS services and applications that run in a private Amazon VPC. With App Runner, rather than thinking about servers or scaling, you have more time to focus on your applications.\n\n## Getting Help\n\nFor feature requests or issues using this tool please open an [issue in this repository](https://github.com/aws/aws-dotnet-deploy/issues).\n\n## Contributing\nWe welcome community contributions and pull requests. See [CONTRIBUTING](https://github.com/aws/aws-dotnet-deploy/blob/main/CONTRIBUTING.md) for information on how to set up a development environment and submit code.\n\n## Additional Resources\n\n* [AWS Developer Center - Explore .NET on AWS](https://aws.amazon.com/developer/language/net/) Find all the .NET code samples, step-by-step guides, videos, blog content, tools, and information about live events that you need in one place.\n* [AWS Developer Blog - .NET](https://aws.amazon.com/blogs/developer/category/programing-language/dot-net/) Come see what .NET developers at AWS are up to! Learn about new .NET software announcements, guides, and how-to's.\n* [@dotnetonaws](https://twitter.com/dotnetonaws) Follow us on twitter!\n\n\n## License\n\nLibraries in this repository are licensed under the Apache 2.0 License.\nSee [LICENSE](https://github.com/aws/aws-dotnet-deploy/blob/main/LICENSE) and [NOTICE](https://github.com/aws/aws-dotnet-deploy/blob/main/NOTICE) for more information.\n", "release_dates": ["2024-01-04T22:48:36Z", "2023-11-18T00:22:35Z", "2023-10-20T22:33:37Z", "2023-09-22T00:27:29Z", "2023-07-03T15:36:03Z", "2023-06-08T19:55:55Z", "2023-03-31T22:41:11Z", "2023-03-08T19:31:50Z", "2023-01-13T22:26:32Z", "2023-01-06T20:41:41Z", "2022-12-16T15:55:58Z", "2022-11-09T20:57:36Z", "2022-11-03T23:31:30Z", "2022-10-06T14:54:47Z", "2022-09-29T16:55:59Z", "2022-09-08T22:45:40Z", "2022-08-24T18:15:23Z", "2022-08-05T17:16:25Z", "2022-07-05T19:40:14Z", "2022-06-29T23:29:33Z", "2022-06-28T17:46:55Z", "2022-06-23T16:59:24Z", "2022-06-17T02:00:12Z", "2022-06-02T16:14:04Z", "2022-05-23T18:42:02Z", "2022-05-12T23:48:43Z", "2022-05-04T13:14:49Z", "2022-04-27T21:52:42Z", "2022-04-25T19:14:16Z", "2022-04-25T17:57:37Z"]}, {"name": "aws-dotnet-extensions-configuration", "description": "This repository hosts various libraries that help developers configure .NET applications using AWS services.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![.NET on AWS Banner](./logo.png \".NET on AWS\")\n\n# AWS .NET Configuration Extension for Systems Manager\n\n[![nuget](https://img.shields.io/nuget/v/Amazon.Extensions.Configuration.SystemsManager.svg)](https://www.nuget.org/packages/Amazon.Extensions.Configuration.SystemsManager/)\n\n[Amazon.Extensions.Configuration.SystemsManager](https://www.nuget.org/packages/Amazon.Extensions.Configuration.SystemsManager/) simplifies using [AWS SSM's](https://aws.amazon.com/systems-manager) [Parameter Store](https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html) and [AppConfig](https://docs.aws.amazon.com/appconfig/latest/userguide/what-is-appconfig.html) as a source for configuration information for .NET Core applications. This project was contributed by [@KenHundley](https://github.com/KenHundley) and [@MichalGorski](https://github.com/mgorski-mg).\n\nThe library introduces the following dependencies:\n\n* [AWSSDK.Extensions.NETCore.Setup](https://www.nuget.org/packages/AWSSDK.Extensions.NETCore.Setup/)\n* [AWSSDK.SimpleSystemsManagement](https://www.nuget.org/packages/AWSSDK.SimpleSystemsManagement/)\n* [AWSSDK.AppConfig](https://www.nuget.org/packages/AWSSDK.AppConfig/)\n* [Microsoft.Extensions.Configuration](https://www.nuget.org/packages/Microsoft.Extensions.Configuration)\n\n# Getting Started\n\nFollow the examples below to see how the library can be integrated into your application. This extension adheres to the same practices and conventions of [Configuration in ASP.NET Core](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration/?view=aspnetcore-2.1).\n\n## ASP.NET Core Example\n\nOne of the common use cases for this library is to pull configuration from Parameter Store. You can easily add this functionality by adding 1 line of code.\n\n```csharp\npublic class Program\n{\n    public static void Main(string[] args)\n    {\n        CreateWebHostBuilder(args).Build().Run();\n    }\n\n    public static IWebHostBuilder CreateWebHostBuilder(string[] args) =>\n        WebHost.CreateDefaultBuilder(args)\n            .ConfigureAppConfiguration(builder =>\n            {\n                builder.AddSystemsManager(\"/my-application/\");\n            })\n            .UseStartup<Startup>();\n}\n```\nIt is also possible to load AWS Secrets Manager secrets from Parameter Store parameters. When retrieving a Secrets Manager secret from Parameter Store, the name must begin with the following reserved path: /aws/reference/secretsmanager/`{Secret-Id}`. Below example demonstrates this use case:\n```csharp\npublic class Program\n{\n    public static void Main(string[] args)\n    {\n        CreateWebHostBuilder(args).Build().Run();\n    }\n\n    public static IWebHostBuilder CreateWebHostBuilder(string[] args) =>\n        WebHost.CreateDefaultBuilder(args)\n            .ConfigureAppConfiguration(builder =>\n            {\n                builder.AddSystemsManager(\"/aws/reference/secretsmanager/SomeSecret\");\n            })\n            .UseStartup<Startup>();\n}\n```\nFor loading secrets, the library will use `JsonParameterProcessor` to load Key/Value pairs stored in the secret. These Key/Value pairs could be retrieved from the `ConfigurationManager` object. For more details, kindly refer [Referencing AWS Secrets Manager secrets from Parameter Store parameters](https://docs.aws.amazon.com/systems-manager/latest/userguide/integration-ps-secretsmanager.html).\n\nAnother possibility is to pull configuration from AppConfig. You can easily add this functionality by adding 1 line of code.\n\n```csharp\npublic class Program\n{\n    public static void Main(string[] args)\n    {\n        CreateWebHostBuilder(args).Build().Run();\n    }\n\n    public static IWebHostBuilder CreateWebHostBuilder(string[] args) =>\n        WebHost.CreateDefaultBuilder(args)\n            .ConfigureAppConfiguration(builder =>\n            {\n                builder.AddAppConfig(\"AppConfigApplicationId\", \"AppConfigEnvironmentId\", \"AppConfigConfigurationProfileId\", TimeSpan.FromSeconds(20));\n            })\n            .UseStartup<Startup>();\n}\n```\n\n## HostBuilder Example\n\nMicrosoft introduced [.NET Generic Host](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/generic-host?view=aspnetcore-2.1) to de-couple HTTP pipeline from the Web Host API. The Generic Host library allows you to write non-HTTP services using configuration, dependency injection, and logging features. The sample code below shows you how to use the AWS .NET Configuration Extension library:\n\n```csharp\nnamespace HostBuilderExample\n{\n    public static async Task Main(string[] args)\n    {\n        var host = new HostBuilder()\n            .ConfigureAppConfiguration((hostingContext, config) =>\n            {\n                config.AddSystemsManager(\"/my-application/\");\n                config.AddAppConfig(\"AppConfigApplicationId\", \"AppConfigEnvironmentId\", \"AppConfigConfigurationProfileId\", TimeSpan.FromSeconds(20));\n            })\n            .ConfigureServices((sc) => { ... })\n            .Build();\n\n        await host.RunAsync();\n    }\n}\n```\n\n## AWS Lambda Example\n\nFor improved performance with AppConfig and Lambda it is recommended to use the `AddAppConfigUsingLambdaExtension` method and deploy the Lambda function with the AWS AppConfig Lambda extension. More information including the AppConfig Lambda extension layer arn can be found in the [AWS AppConfig user guide](https://docs.aws.amazon.com/appconfig/latest/userguide/appconfig-integration-lambda-extensions.html).\n\n\n```csharp\nvar configurations = new ConfigurationBuilder()\n                        .AddSystemsManager(\"/my-application/\")\n                        .AddAppConfigUsingLambdaExtension(\"AppConfigApplicationId\", \"AppConfigEnvironmentId\", \"AppConfigConfigurationProfileId\")\n                        .Build();\n```\n\n# Config reloading\n\nThe `reloadAfter` parameter on `AddSystemsManager()` and `AddAppConfig()` enables automatic reloading of configuration data from Parameter Store or AppConfig as a background task. When using `AddAppConfigUsingLambdaExtension` reload is automatically configured.\n\n## Config reloading in AWS Lambda\n\nIn AWS Lambda, background tasks are paused after processing the AWS Lambda event. This could disrupt the provider from retrieving the latest configuration data from Parameter Store or AWS AppConfig. To ensure the reload is performed within the AWS Lambda event, we recommend calling the extension\nmethod `WaitForSystemsManagerReloadToComplete` from the `IConfiguration` object in the beginning of your AWS Lambda function handler. This method will immediately return unless a reload is currently being performed.\n\nRemember to build `IConfiguration` in the AWS Lambda constructor! It is the only way to cache the configuration and reload it using `reloadAfter` parameter.\n\n```csharp\npublic class SampleLambda\n{\n    private readonly IConfiguration _configurations;\n    \n    public SampleLambda()\n    {\n        _configurations = new ConfigurationBuilder()\n                            .AddSystemsManager(\"/my-application/\")\n                            .AddAppConfigForLambda(\"AppConfigApplicationId\", \"AppConfigEnvironmentId\", \"AppConfigConfigurationProfileId\", TimeSpan.FromSeconds(20))\n                            .Build();\n    }\n\n    protected void Invoke()\n    {\n        _configurations.WaitForSystemsManagerReloadToComplete(TimeSpan.FromSeconds(2));\n    }\n}\n```\n\n## Samples\n\n### Custom ParameterProcessor Sample\n\nExample of using a custom `IParameterProcessor` which provides a way to store and retrieve `null` values. Since AWS Parameter Store params are string literals, there is no way to store a `null` value by default.\n\n```csharp\nnamespace CustomParameterProcessorExample\n{\n    public class CustomParameterProcessor : DefaultParameterProcessor\n    {\n        const string NULL_STRING_LITERAL = \"NULL\";\n        \n        public override string GetValue(Parameter parameter, string path)\n        {\n            string value = base.GetValue(parameter, path);\n            return value == NULL_STRING_LITERAL ? null : value;\n        }\n    }\n    \n    public class Program\n    {\n        public static void Main(string[] args)\n        {\n            CreateWebHostBuilder(args).Build().Run();\n        }\n\n        public static IWebHostBuilder CreateWebHostBuilder(string[] args) =>\n            WebHost.CreateDefaultBuilder(args)\n                .ConfigureAppConfiguration(builder =>\n                {\n                    builder.AddSystemsManager(config => {\n                        config.Path = \"/my-application/\";\n                        config.ParameterProcessor = new CustomParameterProcessor();\n                    });\n                })\n                .UseStartup<Startup>();\n    }\n}\n```\n\nFor more complete examples, take a look at sample projects available in [samples directory](https://github.com/aws/aws-dotnet-extensions-configuration/tree/master/samples).\n\n# Configuring Systems Manager Client\n\nThis extension is using [AWSSDK.Extensions.NETCore.Setup](https://www.nuget.org/packages/AWSSDK.Extensions.NETCore.Setup/) to get AWSOptions from `Configuration` object and create AWS Systems Manager Client. You can edit and override the configuration by adding AWSOptions to your configuration providers such as appsettings.Development.json. Below is an example of a configuration provider:\n\n```JSON\n{\n  ...\n\n  \"AWS\": {\n    \"Profile\": \"default\",\n    \"Region\": \"us-east-1\",\n    \"ResignRetries\": true\n  }\n\n  ...\n}\n```\n\nFor more information and other configurable options please refer to [Configuring the AWS SDK for .NET with .NET Core](https://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/net-dg-config-netcore.html).\n\n# Permissions\n## Parameter Store\nThe AWS credentials used must have access to the `ssm:GetParameters` service operation from AWS System Manager. Below is an example IAM policy for this action.\n```JSON\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"SSMPermissionStatement\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"ssm:GetParameters\",\n            \"Resource\": \"arn:aws:ssm:${Region}:${Account}:parameter/${ParameterNamePrefix}*\"\n        }\n    ]\n}\n```\nThe above policy gives user access to get and use parameters which begin with the specified prefix.\n\nFor more details, refer [Restricting access to Systems Manager parameters using IAM policies](https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-access.html).\n\nAdditionally, for referencing secrets from AWS Secrets Manager from Paramater Store parameters, AWS credentials used must have permissions to access the secret.\n\n## AppConfig\nIf the application reads configuration values from AWS Systems Manager AppConfig, the AWS credentials used must have access to `appconfig:StartConfigurationSession` and `appconfig:GetLatestConfiguration` service operations. Below is an example IAM policy for this action.\n```JSON\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"appconfig:StartConfigurationSession\",\n        \"appconfig:GetLatestConfiguration\",\n      ],\n      \"Resource\": \"arn:${Partition}:appconfig:${Region}:${Account}:application/${ApplicationId}/environment/${EnvironmentId}/configuration/${ConfigurationProfileId}\"\n    }\n  ]\n}\n```\nFor more details, refer [Configuring permissions for AWS AppConfig](https://docs.aws.amazon.com/appconfig/latest/userguide/getting-started-with-appconfig-permissions.html) and [Actions, resources, and condition keys for AWS AppConfig](https://docs.aws.amazon.com/service-authorization/latest/reference/list_awsappconfig.html#awsappconfig-GetConfiguration).\n\n# Getting Help\n\nWe use the [GitHub issues](https://github.com/aws/aws-dotnet-extensions-configuration/issues) for tracking bugs and feature requests and have limited bandwidth to address them.\n\nIf you think you may have found a bug, please open an [issue](https://github.com/aws/aws-dotnet-extensions-configuration/issues/new).\n\n# Contributing\n\nWe welcome community contributions and pull requests. See\n[CONTRIBUTING.md](./CONTRIBUTING.md) for information on how to set up a development environment and submit code.\n\n# Additional Resources\n\n[AWS .NET GitHub Home Page](https://github.com/aws/dotnet)  \nGitHub home for .NET development on AWS. You'll find libraries, tools, and resources to help you build .NET applications and services on AWS.\n\n[AWS Developer Center - Explore .NET on AWS](https://aws.amazon.com/developer/language/net/)  \nFind all the .NET code samples, step-by-step guides, videos, blog content, tools, and information about live events that you need in one place.\n\n[AWS Developer Blog - .NET](https://aws.amazon.com/blogs/developer/category/programing-language/dot-net/)  \nCome see what .NET developers at AWS are up to!  Learn about new .NET software announcements, guides, and how-to's.\n\n[@dotnetonaws](https://twitter.com/dotnetonaws)\nFollow us on twitter!\n\n# License\n\nLibraries in this repository are licensed under the Apache 2.0 License.\n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.\n", "release_dates": []}, {"name": "aws-dotnet-session-provider", "description": "A session state provider for ASP.NET applications that stores the sessions in Amazon DynamoDB", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS DynamoDB Session State Provider\n\nThe **Amazon DynamoDB Session State Provider** allows ASP.NET applications store their sessions inside DynamoDB. This helps applications scale across multiple application servers while maintaining session state across the system.\n\nIf you are looking to cache session state in DynamoDB from an _ASP.NET Core_ application, try the [AWS .NET Distributed Cache Provider](https://github.com/awslabs/aws-dotnet-distributed-cache-provider) instead.\n\n## Change Log\n\nThe change log for the can be found in the [CHANGELOG.md](https://github.com/aws/aws-dotnet-session-provider/blob/master/CHANGELOG.md) file.\n\n\n## Usage Information\n\nThis project builds a ASP.NET Session State provider that stores session in a DynamoDB table. The session state provider can retrieved from [NuGet][nuget-package].\n\nFor more information on using the session manager, see the session manager section in the [AWS SDK for .NET Developer Guide][developer-guide].\n\n\n## Links\n\n* [AWS Session State Provider NuGet package][nuget-package]\n* [AWS Session Provider Developer Guide][developer-guide]\n* [AWS .NET Developer Blog][dotnet-blog]\n* [AWS SDK for .NET GitHub Repository][github-awssdk]\n* [AWS SDK for .NET SDK][sdk-website]\n\n\n[developer-guide]: https://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/dynamodb-session-net-sdk.html\n[nuget-package]: http://www.nuget.org/packages/AWS.SessionProvider/\n[github-awssdk]: https://github.com/aws/aws-sdk-net\n[sdk-website]: http://aws.amazon.com/sdkfornet\n[dotnet-blog]: http://blogs.aws.amazon.com/net/\n", "release_dates": []}, {"name": "aws-dotnet-trace-listener", "description": "A trace listener for System.Diagnostics that can be used to log events straight to Amazon DynamoDB.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS DynamoDB Trace Listener\n\nThe **AWS DynamoDB Trace Listener** allows System.Diagnostics.Trace calls to be written to Amazon DynamoDB.\n\n## Usage Information\n\nThe trace listener can be included in your project from the [NuGet][nuget-package] package. Once included the trace listener can be configured in your application's app.config or web.config.\nHere is an example configuration that writes all Trace.Write calls to DynamoDB.\n\n<pre>\n&lt;system.diagnostics&gt;\n  &lt;trace autoflush=\"true\"&gt;\n    &lt;listeners&gt;\n      &lt;add name=\"dynamo\" type=\"Amazon.TraceListener.DynamoDBTraceListener, AWS.TraceListener\"\n                      Region=\"us-west-2\"\n                      ExcludeAttributes=\"Callstack\"\n                      HashKeyFormat=\"%ComputerName%-{EventType}-{ProcessId}\"\n                      RangeKeyFormat=\"{Time}\"\n        /&gt;\n    &lt;/listeners&gt;\n  &lt;/trace&gt;    \n&lt;/system.diagnostics&gt;\n</pre>\n\nGo [here](http://blogs.aws.amazon.com/net/post/Tx16NZPGUZK6LDU/DynamoDBTraceListener) for more information on using the trace listener.\n\n\n## Links\n\n* [AWS Trace Listener NuGet package][nuget-package]\n* [AWS .NET Developer Blog][dotnet-blog]\n* [AWS SDK for .NET GitHub Repository][github-awssdk]\n* [AWS SDK for .NET SDK][sdk-website]\n\n\n[nuget-package]: https://www.nuget.org/packages/AWS.TraceListener/\n[github-awssdk]: https://github.com/aws/aws-sdk-net\n[sdk-website]: http://aws.amazon.com/sdkfornet\n[dotnet-blog]: http://blogs.aws.amazon.com/net/\n", "release_dates": []}, {"name": "aws-dynamodb-encryption-java", "description": "Amazon DynamoDB Encryption Client for Java", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Client-side Encryption for Amazon DynamoDB\n\n\ud83d\udce3 Note: Version 3.x of this library is available,\nand has been renamed to the AWS Database Encryption SDK.\nSee the [AWS Database Encryption SDK 3.x](#aws-database-encryption-sdk-3.x) section for more information.\n\nThe **[Amazon DynamoDB][ddb] Client-side Encryption in Java** supports encryption and signing of your data when stored in Amazon DynamoDB.\n\nA typical use of this library is when you are using [DynamoDBMapper][ddbmapper], where transparent protection of all objects serialized through the mapper can be enabled via configuring an [AttributeEncryptor][attrencryptor].\n\n**Important: Use `SaveBehavior.PUT` or `SaveBehavior.CLOBBER` with `AttributeEncryptor`. If you do not do so you risk corrupting your signatures and encrypted data.**\nWhen PUT or CLOBBER is not specified, fields that are present in the record may not be passed down to the encryptor, which results in fields being left out of the record signature. This in turn can result in records failing to decrypt.\n\nFor more advanced use cases where tighter control over the encryption and signing process is necessary, the low-level [DynamoDBEncryptor][ddbencryptor] can be used directly.\n\n[Security issue notifications](./CONTRIBUTING.md#security-issue-notifications)\n\nSee [Support Policy](./SUPPORT_POLICY.rst) for for details on the current support status of all major versions of this library.\n\n## AWS Database Encryption SDK 3.x\n\nThe 3.x version of this library is generally available, and has been [renamed to the AWS Database Encryption SDK](https://docs.aws.amazon.com/database-encryption-sdk/latest/devguide/DDBEC-rename.html).\nIt is a major rewrite of the DynamoDB Encryption Client for Java\nand includes many updates, such as a new structured data format, improved multitenancy support, seamless schema changes, and searchable encryption support.\n\nFor more information see the [AWS Database Encryption SDK Developer Guide](https://docs.aws.amazon.com/database-encryption-sdk/latest/devguide/what-is-database-encryption-sdk.html).\nor check the project repository at [https://github.com/aws/aws-database-encryption-sdk-dynamodb-java/](https://github.com/aws/aws-database-encryption-sdk-dynamodb-java/).\n\n## Getting Started\n\n### Required Prerequisites\nTo use this SDK you must have:\n\n* **A Java 8 development environment**\n\n  If you do not have one, go to [Java SE Downloads](https://www.oracle.com/technetwork/java/javase/downloads/index.html) on the Oracle website, then download and install the Java SE Development Kit (JDK). Java 8 or higher is required.\n\n  **Note:** If you use the Oracle JDK, you must also download and install the [Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files](http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html).\n\n### Get Started\n\nSuppose you have created ([sample code][createtable]) a DynamoDB table \"MyStore\", and want to store some Book objects.  The security requirement involves classifying the attributes Title and Authors as sensitive information.  This is how the Book class may look like:\n\n```java\n@DynamoDBTable(tableName=\"MyStore\")\npublic class Book {\n    private Integer id;\n    private String title;\n    private String ISBN;\n    private Set<String> bookAuthors;\n    private String someProp;\n \n    // Not encrypted because it is a hash key    \n    @DynamoDBHashKey(attributeName=\"Id\")  \n    public Integer getId() { return id;}\n    public void setId(Integer id) {this.id = id;}\n \n    // Encrypted by default\n    @DynamoDBAttribute(attributeName=\"Title\")  \n    public String getTitle() {return title; }\n    public void setTitle(String title) { this.title = title; }\n \n    // Specifically not encrypted\n    @DoNotEncrypt\n    @DynamoDBAttribute(attributeName=\"ISBN\")  \n    public String getISBN() { return ISBN; }\n    public void setISBN(String ISBN) { this.ISBN = ISBN; }\n \n    // Encrypted by default\n    @DynamoDBAttribute(attributeName = \"Authors\")\n    public Set<String> getBookAuthors() { return bookAuthors; }\n    public void setBookAuthors(Set<String> bookAuthors) { this.bookAuthors = bookAuthors; }\n \n    // Not encrypted nor signed\n    @DoNotTouch\n    public String getSomeProp() { return someProp;}\n    public void setSomeProp(String someProp) {this.someProp = someProp;}\n}\n```\n\nAs a typical use case of [DynamoDBMapper][ddbmapper], you can easily save and retrieve a Book object to and from Amazon DynamoDB _without encryption (nor signing)_.  For example,\n\n```java\n    AmazonDynamoDBClient client = new AmazonDynamoDBClient(...);\n    DynamoDBMapper mapper = new DynamoDBMapper(client);\n    Book book = new Book();\n    book.setId(123);\n    book.setTitle(\"Secret Book Title \");\n    // ... etc. setting other properties\n\n    // Saves the book unencrypted to DynamoDB\n    mapper.save(book);\n\n    // Loads the book back from DynamoDB\n    Book bookTo = new Book();\n    bookTo.setId(123);\n    Book bookTo = mapper.load(bookTo);\n\n```\n\nTo enable transparent encryption and signing, simply specify the necessary encryption material via an [EncryptionMaterialsProvider][materialprovider].  For example:\n\n```java\n    AmazonDynamoDBClient client = new AmazonDynamoDBClient(...);\n    SecretKey cek = ...;        // Content encrypting key\n    SecretKey macKey =  ...;    // Signing key\n    EncryptionMaterialsProvider provider = new SymmetricStaticProvider(cek, macKey);\n    mapper = new DynamoDBMapper(client, DynamoDBMapperConfig.builder().withSaveBehavior(SaveBehavior.PUT).build(),\n                new AttributeEncryptor(provider));\n    Book book = new Book();\n    book.setId(123);\n    book.setTitle(\"Secret Book Title \");\n    // ... etc. setting other properties\n\n    // Saves the book both encrypted and signed to DynamoDB\n    mapper.save(bookFrom);\n\n    // Loads the book both with signature verified and decrypted from DynamoDB\n    Book bookTo = new Book();\n    bookTo.setId(123);\n    Book bookTo = mapper.load(bookTo);\n\n```\n\nNote that by default all attributes except the primary keys are both encrypted and signed for maximum security.  To selectively disable encryption, the annotation [@DoNotEncrypt][donotencrypt] can be used as shown in the [Book](#getting-started) class above.  To disable both encryption and signing, the annotation [@DoNotTouch][donottouch] can be used.\n\nThere is a variety of existing [EncryptionMaterialsProvider][materialprovider] implementations that you can use to provide the encryption material, including [KeyStoreMaterialsProvider][keystoreprovider] which makes use of a Java keystore.  Alternatively, you can also plug in your own custom implementation.\n\n### Changing Your Data Model\n\nEvery time you encrypt or decrypt an item, you need to provide attribute actions that tell the DynamoDB Encryption\nClient which attributes to encrypt and sign, which attributes to sign (but not encrypt), and which to ignore. Attribute\nactions are not saved in the encrypted item and the DynamoDB Encryption Client does not update your attribute actions\nautomatically.\n\nWhenever you change your data model, that is, when you add or remove attributes from your table items, you need to take\nadditional steps to safely migrate the client-side encryption configuration.\n\nFor guidance on this process, please see the developer guide on [Changing Your Data Model](https://docs.aws.amazon.com/database-encryption-sdk/latest/devguide/data-model.html).\n\n### Downloads\n\nYou can download the [latest snapshot release][download] or pick it up from Maven:\n\n```xml\n  <dependency>\n    <groupId>com.amazonaws</groupId>\n    <artifactId>aws-dynamodb-encryption-java</artifactId>\n    <version>2.0.3</version>\n  </dependency>\n```\n\nDon't forget to enable the download of snapshot jars from Maven:\n\n```xml\n  <profiles>\n    <profile>\n       <id>allow-snapshots</id>\n          <activation><activeByDefault>true</activeByDefault></activation>\n       <repositories>\n         <repository>\n           <id>snapshots-repo</id>\n           <url>https://oss.sonatype.org/content/repositories/snapshots</url>\n           <releases><enabled>false</enabled></releases>\n           <snapshots><enabled>true</enabled></snapshots>\n         </repository>\n       </repositories>\n     </profile>\n  </profiles>\n```\n\n## Supported Algorithms\n\nFor content encryption, the encryption algorithm is determined by the user specified [SecretKey][secretkey], as long as it is a block cipher that can be used with the encryption mode \"CBC\" and \"PKCS5Padding\".  Typically, this means \"AES\".\n\nFor signing, the user specified signing key can be either symmetric or asymmetric.  For asymmetric signing (where the user would provide a signing key in the form of a [PrivateKey][privatekey]), the default algorithm is \"SHA256withRSA\".  For symmetric signing (where the user would provide the signing key in the form of a [SecretKey][secretkey]), the algorithm would be determined by the provided key.  A typical algorithm for a symmetric signing key is \"HmacSHA256\".\n\n## FAQ\n\n1. Do the content-encrypting key and signing key get encrypted and stored along side with the data in Amazon DynamoDB ?\n  * No, neither the content-encrypting key nor the signing key get persisted by this library.  However, in order to locate the material for decryption purposes, the identifying information (i.e. material descriptions) for the encryption material is indeed stored along side with the data in Amazon DynamoDB.  In particular, the user specified [EncryptionMaterialsProvider][materialprovider] is responsible for not only providing the keys, but also the corresponding material descriptions.\n\n2. How is the IV generated and where is it stored ?\n  * For each attribute that needs to be encrypted, a unique IV is randomly generated, and get stored along side with the binary representation of the attribute value.\n\n3. How many bits are used for the random IV ?\n  * The bit size used for each random IV is the same as the block size of the block cipher used for content encryption.  The IV bit-size therefore depends on the specific algorithm of the content encrypting key provided by the user.  Typically this means AES, or 128 bits.\n\n4. What is the key length for the content encrypting key ?\n  * This depends on the specific content encrypting key provided by the user.  A typical length of an AES key is 128 bits or 256 bits.\n\n## Known Limitations\n\n1. During retrieval of an item, all the attributes of the item that have been involved for encryption or signing must also be included for signature verification.  Otherwise, the signature would fail to verify.\n\n[attrencryptor]: sdk1/src/main/java/com/amazonaws/services/dynamodbv2/datamodeling/AttributeEncryptor.java\n[createtable]: https://github.com/aws/aws-sdk-java/blob/master/src/samples/AmazonDynamoDBDocumentAPI/quick-start/com/amazonaws/services/dynamodbv2/document/quickstart/A_CreateTableTest.java\n[ddb]: http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\n[ddbencryptor]: sdk1/src/main/java/com/amazonaws/services/dynamodbv2/datamodeling/encryption/DynamoDBEncryptor.java\n[ddbmapper]: http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/datamodeling/DynamoDBMapper.html\n[donotencrypt]: sdk1/src/main/java/com/amazonaws/services/dynamodbv2/datamodeling/encryption/DoNotEncrypt.java\n[donottouch]: sdk1/src/main/java/com/amazonaws/services/dynamodbv2/datamodeling/encryption/DoNotTouch.java\n[keystoreprovider]: sdk1/src/main/java/com/amazonaws/services/dynamodbv2/datamodeling/encryption/providers/KeyStoreMaterialsProvider.java\n[materialprovider]: sdk1/src/main/java/com/amazonaws/services/dynamodbv2/datamodeling/encryption/providers/EncryptionMaterialsProvider.java\n[privatekey]: http://docs.oracle.com/javase/7/docs/api/java/security/PrivateKey.html\n[secretkey]: http://docs.oracle.com/javase/7/docs/api/javax/crypto/SecretKey.html\n[download]: https://github.com/aws/aws-dynamodb-encryption-java/releases\n", "release_dates": ["2022-01-06T23:53:11Z", "2021-10-14T20:52:06Z", "2021-09-02T17:40:47Z", "2021-02-12T18:26:18Z", "2021-02-12T19:11:09Z", "2021-02-04T22:18:25Z", "2021-02-04T19:56:18Z", "2018-12-10T23:01:30Z", "2018-08-30T18:00:12Z", "2018-08-03T21:22:31Z", "2018-08-02T22:15:33Z", "2016-07-13T16:38:07Z", "2016-06-02T17:34:19Z", "2016-06-01T22:02:54Z", "2016-05-19T18:31:38Z", "2016-04-15T16:06:35Z", "2015-12-16T20:46:18Z", "2015-01-08T03:33:24Z"]}, {"name": "aws-dynamodb-encryption-python", "description": "Amazon DynamoDB Encryption Client for Python", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2023-01-19T22:03:26Z", "2023-01-17T22:50:57Z", "2023-01-10T20:16:40Z", "2021-11-10T21:29:16Z", "2021-07-15T22:52:35Z", "2021-07-15T18:09:56Z", "2021-02-04T22:18:31Z", "2021-02-04T19:56:01Z", "2019-10-10T21:55:26Z", "2019-08-29T23:40:03Z", "2019-01-15T23:14:00Z"]}, {"name": "aws-eb-glassfish-dockerfiles", "description": "Official Elastic Beanstalk repository for GlassFish docker files. ", "language": "Dockerfile", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "#aws-eb-glassfish-dockerfiles\n\n#####This is the GlassFish Dockerfile repository for AWS Elastic Beanstalk.\n\nSee also <https://aws.amazon.com/elasticbeanstalk/>\n\n##LICENSE\n\nCopyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"). \nYou may not use this file except in compliance with the License. \nA copy of the License is located at\n\n    http://aws.amazon.com/apache2.0/\n\nor in the \"license\" file accompanying this file. This file is \ndistributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS \nOF ANY KIND, either express or implied. See the License for the \nspecific language governing permissions and limitations under the \nLicense.\n", "release_dates": []}, {"name": "aws-eb-python-dockerfiles", "description": "Official Elastic Beanstalk repository for Python docker files.", "language": "Shell", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "#aws-eb-python-dockerfiles\n\n#####This is the Python Dockerfile repository for AWS Elastic Beanstalk.\n\nSee also <https://aws.amazon.com/elasticbeanstalk/>\n\n##LICENSE\n\nCopyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"). \nYou may not use this file except in compliance with the License. \nA copy of the License is located at\n\n    http://aws.amazon.com/apache2.0/\n\nor in the \"license\" file accompanying this file. This file is \ndistributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS \nOF ANY KIND, either express or implied. See the License for the \nspecific language governing permissions and limitations under the \nLicense.\n", "release_dates": []}, {"name": "aws-ebpf-sdk-go", "description": "Golang based SDK for kernel eBPF operations i.e, load/attach/detach eBPF programs and create/delete/update maps. SDK relies on Unix bpf() system calls.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# aws-ebpf-sdk-go\n\nGolang based SDK for kernel eBPF operations i.e, load/attach/detach eBPF programs and create/delete/update maps. SDK relies on Unix bpf() system calls.\n\nSDK currently supports -\n\n1. eBPF program types -\n   a. Traffic Classifiers\n   b. XDP\n   c. Kprobes/Kretprobes\n   d. Tracepoint probes\n2. Ring buffer (would need kernel 5.10+)\n\nSDK currently do not support -\n\n1. Map in Map\n2. Perf buffer\n\nContributions welcome!\n\nNote: This is the first version of SDK and interface is subject to change so kindly review the release notes before upgrading.\n\n# Getting started\n\n## How to build SDK?\n\nRun `make build-linux` - this will build the sdk binary.\n\n## How to build elf file?\n\n```\nclang -I../../.. -O2 -target bpf -c <C file> -o <ELF file>\n```\n\n## How to use the SDK?\n\n**Note:** SDK expects the BPF File System (/sys/fs/bpf) to be mounted.\n \nIn your application, \n\n1. Get the latest SDK -\n\n```\nGOPROXY=direct go get github.com/aws/aws-ebpf-sdk-go\n```\n\n2. Import the elfparser - \n\n```\ngoebpfelfparser \"github.com/aws/aws-ebpf-sdk-go/pkg/elfparser\"\n```\n\n3. Load the elf -\n\n```\ngoebpfelfparser.LoadBpfFile(<ELF file>, <custom pin path>)\n```\n\nOn a successful load, SDK returns -\n\n1. loaded programs (includes associated maps) \n\n```\nThis is indexed by the pinpath - \n\ntype BpfData struct {\n\tProgram ebpf_progs.BpfProgram       // Return the program\n\tMaps    map[string]ebpf_maps.BpfMap // List of associated maps\n}\n```\n\n2. All maps in the elf file\n```\nThis is indexed by the map name -\n\ntype BpfMap struct {\n\tMapFD       uint32\n\tMapID       uint32\n\tMapMetaData CreateEBPFMapInput\n}\n```\n\nApplication can specify custom pinpath while loading the elf file.\n\nMaps and Programs pinpath location is not customizable with the current version of SDK and will be installed under the below locations by default -\n\nProgram PinPath - \"/sys/fs/bpf/globals/aws/programs/\"\n\nMap PinPath - \"/sys/fs/bpf/globals/aws/maps/\"\n\nMap defintion should follow the below definition else the SDK will fail to create the map.\n\n```\nstruct bpf_map_def_pvt {\n\t__u32 type;\n\t__u32 key_size;\n\t__u32 value_size;\n\t__u32 max_entries;\n\t__u32 map_flags;\n\t__u32 pinning;\n\t__u32 inner_map_fd;\n};\n```\n\n## How to debug SDK issues?\n\nSDK logs are located here `/var/log/aws-routed-eni/ebpf-sdk.log`.\n\n## How to run unit-test\n\nRun `sudo make unit-test`\n\nNote: you would need to run this on you linux system\n\n## How to run functional tests\n\nGo to -\n\n```\ncd test/\nsudo make run-test\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues. Instead, please follow the\ninstructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2024-02-02T22:06:31Z", "2023-12-06T01:02:36Z", "2023-12-05T22:48:35Z", "2023-12-06T00:45:05Z", "2023-11-09T18:37:01Z", "2023-11-08T20:46:30Z", "2023-11-08T01:20:10Z", "2023-10-18T23:07:26Z", "2023-09-26T21:04:59Z", "2023-09-06T22:10:01Z", "2023-09-06T21:12:10Z", "2023-08-29T16:50:28Z", "2023-08-09T21:03:03Z", "2023-08-08T22:12:45Z"]}, {"name": "aws-ec2-imdsv2-get", "description": "EC2 imds get tool to help with interfacing with IMDS on EC2 instances", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS EC2 IMDSv2 Get Tool\n\nThis simple tool is useful for fetching data from imds on ec2 instances.\nIt will automatically check if imdsv2 is enabled and if so will utilize it instead\nof imdsv1.\n\nTo use the tool simply call the cli and as its argument the path you want to extrac tinformation for.\n\nUsage examples:\n```\n# Get the user data script\naws-ec2-imdsv2-get latest/user-data\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2024-01-29T19:02:55Z", "2023-03-09T19:19:43Z", "2023-03-08T23:23:48Z", "2023-03-08T20:50:36Z", "2023-01-19T19:10:23Z"]}, {"name": "aws-ec2-instance-connect-cli", "description": "This is an all-in-one client for EC2 Instance Connect that handles key brokerage and establishing connection to EC2 Instances through an interface near-identical to standard system ssh, sftp, and other utilities. ", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS EC2 Instance Connect CLI\n\n**_[IMPORTANT]_  Since June 2023, the AWS CLI includes the [ssh](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ec2-instance-connect/ssh.html) command. The AWS CLI ssh command allows you to connect to your instances directly over the internet or to instances in a private subnet using [EC2 Instance Connect](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html). We strongly recommend using the AWS CLI ssh command rather than this package.\n\nThis is a Python client for accessing EC2 instances via AWS EC2 Instance Connect.\nThis module supports Python 3.6.x+.  [This package is available on PyPI for pip installation](https://pypi.org/project/ec2instanceconnectcli/), ie, `pip install ec2instanceconnectcli`\n\n## Setup\n\nIt is strongly encouraged you set up a virtual environment for building and testing.\n\n### Prerequisites\n\nTo set up this package you need to have pip installed.\n\n### Package Setup\n\nInstall the package dependencies\n\n`pip install -r requirements.txt`\n\n## Running\n\nEnsure your PYTHONPATH includes the package top-level directory.\n\nRun the desired script with standard UNIX pathing.  For example,\n\n`./bin/mssh ec2-user@ec2-54-245-189-134.us-west-2.compute.amazonaws.com -pr dev -t i-0b01816d5c99826d8 -z us-west-2a`\n\n## Testing\n\nUnit tests can be run with standard pytest.  They may be run, for example, by\n\n`python -m pytest`\n\nAlso, for correcting import when using virtualenv, you have to export PYTHONPATH by running: `export PYTHONPATH=$(pwd)`\n\n## Generating Documentation\n\nSphinx configuration has been included in this package.  To generate Sphinx documentation, run\n\n`pip install -r requirements-docs.txt`\n\nto pull dependencies.  Then, run\n\n`sphinx-apidoc -o doc/source ec2instanceconnectcli`\n\nto generate the module documentation reStructuredText files.  Finally, run\n\n`sphinx-build ./doc/source [desired output directory]`\n\nto generate the actual documentation html.\n", "release_dates": ["2023-03-10T21:55:31Z", "2020-09-23T20:41:52Z", "2020-07-23T20:56:03Z", "2019-07-31T20:45:03Z"]}, {"name": "aws-ec2-instance-connect-config", "description": "This is the ssh daemon configuration and necessary EC2 instance scripting to enable EC2 Instance Connect. Also included is various package manager configurations for packaging for various Linux distributions. ", "language": "Shell", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS EC2 Instance Connect Configuration\n\nThis package contains the EC2 instance configuration and scripts necessary to enable AWS EC2 Instance Connect.\n\n## AuthorizedKeysCommand\n\nThe AuthorizedKeysCommand is split into three parts:\n\n* eic_run_authorized_keys is the main entry point and wraps the rest in a 5 second timeout\n* eic_curl_authorized_keys, which is the entry point for sshd on an ssh call\n* eic_parse_authorized_keys, which is the main authorized keys command logic\n\nThis split is intentional - as parse takes all necessary pieces as command inputs is can be unit tested independently.  curl, however, obviously needs to curl EC2 Instance Metadata Service and so cannot be tested without mocking the actual service.\n\n### eic_curl_authorized_keys\n\nThe curl script verifies we are actually running on an EC2 instance and cURLs relevant information from EC2 Instance Metadata Service and send it to parse.\n\nNote that it must make several curl commands to proceed.  If it cannot do so it fast-fails to prevent blocking the ssh daemon.\n\nThe command also queries several OCSP staples from EC2 Instance Metadata Service.\nThese OCSP staples are provided from the AWS EC2 Instance Connect Service to avoid needing to query each CRL or OCSP authority at ssh calltime as that would have major performance implications.\nThe staples are passed to and used by parse_authorized_keys to check certificate validity without the need for extra external calls.\n\n### eic_parse_authorized_keys\n\nIn addition to the fields required to complete all the below process, a key fingerprint may be provided.  If a key fingerprint is specified then only ssh keys found matching that fingerprint will be returned.\n\n#### Certificate Validation\n\nThe core idea behind AWS EC2 Instance Connect is that all ssh keys vended have been trusted by AWS.  This can be verified by checking each key's signature (more on that later) and vetting the signing certificate used.\n\nThe signing certificate goes through a deep verification flow that checks:\n\n* The CN matches the service\n* The certificate trust chain can be verified up through the Amazon Trust Services Certificate Authority\n* The entire certificate chain is valid - ie, none of them have been revoked\n\nThe first and second are done via standard openssl checks.  The third, however, does not query the relevant CRLs or OCSP authorities at runtime to avoid adding a network call to sshd.\nInstead, OCSP staples are expected to be provided by the invoker (i.e., eic_curl_authorized_keys).\nAs OCSP staples are cryptographically signed and can be verified against a trusted authority these are considered a sufficient validity check.\n\n#### Key Processing\n\nThe keys are expected to be presented to the script in the format\n\n<pre>\n# Key Metadata\n# Key Metadata\n# Key Metadata\n[ssh key]\nsignature\n</pre>\n\nCurrently, the expected metadata is, in order,\n\n1. Expiration timestamp\n2. Instance ID\n3. IAM Caller\n4. Request ID\n\nOnce this data has been loaded, the following checks are run:\n\n* Has the expiration timestamp passed?\n* Is the instance ID correct?\n* Does the signature match the provided data?\n* If a specific key fingerprint was provided to search for, does this key match that fingerprint?\n\nThe signature is specifically expected to be for the *entire* key blob - all metadata entries plus the ssh key.  It should have been generated by the AWS EC2 Instance Connect service's private key, which is verified using the vetted signing certificate.\n\nIf all of these checks pass then the key will be presented to the ssh daemon.  Otherwise, it will be ignored and the next key will be processed.\n\nAny time a key is provided to the ssh daemon it will be logged to the system authpriv log for auditing purposes.\n\n## Unit Testing\n\nAs parse_authorized_keys requires a valid certificate, CA, and OCSP staples, unit testing is a somewhat involved process.\n\nThis has been automated to a convenient entry point: `bin/unit_test_suite.sh`.  This will\n\n1. Invoke `bin/unit-test/setup_certificates.sh` to generate a test CA and trust chain\n2. Invoke `bin/unit-test/generate_ocsp.sh` to generate OCSP staples for the certificates\n3. Iterate over unit-test/input/direct and unit-test/input/unsigned and test all entries via `bin/unit-test/test_authorized_keys.sh`, expecting the matching contents of unit-test/expected-output\n\nunit-test/input/direct's contents are passed directly as-is as they are not expected to contain valid signatures.\n\nunit-test/input/unsigned's contents, however, are expected to get far enough in the process to (potentially) need a valid signature.  As such, signatures are generated on-the-fly using the pre-generated certificate's private key.\n\nThe structure of unit-test/input/unsigned, rather than files, is directories to test.  The directory's contents should be in a numeric order that the test script will iterate.\nEach file should have *one* test key blob to sign.\nThe actual test input will be the result of generating signatures for each file and constructing an imitation of the service's key bundle.\n\n## End-to-End Testing\n\nIntegration testing requires running these scriptlets on an actual EC2 instance.  This has been scripted for your convenience.\n\nTo run integration tests against a given platform (eg, RHEL or Ubuntu):\n1. Build a test package (for example, `make deb`)\n2. Select an AMI of the platform you desire (for example, the latest Ubuntu AMI)\n3. Configure your VPC for testing\n   * Configure an EC2 keypair.  Make sure to save the private key!\n   * Configure your security group to allow SSH from your test-run machine\n   * Set your subnet to auto-assign public IPs.  Testing currently requires SSHing via public IP.\n4. Determine the appropriate EC2 username for your platform (see below)\n5. Determine the \"config name\" for your platform (see below)\n6. Determine instance types you would like to test (or all supported in your subnet's zone)\n7. The actual tests are invoked by the following command:\n`./bin/integration_test_suite.sh -r [region] -a [ami id] -u [ec2 username] -k [ec2 keypair name] -s [subnet id] -g [security group id] -l [config name] -p [/path/to/private/key] -i [/path/to/test/package] [-t [instance,type,list]] [-d [/output/directory]]`\n\nThis will run all tests under the integration-test/test directory against all requested instance types (or all types in the zone if not specified).  Any error output will be written in files pathed /path/instance-type/test-name in the output directory.  An output directory will be autogenerated if none is specified.\n\nEC2 usernames and \"config names\" for each supported platform is as follows:\n\n| Platform | EC2 Username | \"Config\" Name |\n| --- | --- | --- |\n| Amazon Linux | ec2-user | amazon-linux |\n| Ubuntu | ubuntu | ubuntu | \n| RHEL | ec2-user | rhel |\n\nAgain, please note that if you do not specify instance types then all supported by the subnet's zone will be run.  This can take a very long time.  It is therefore recommended you specify at least one Nitro and at least one non-Nitro instance type, such as t2.micro and m5.large.\n\n## Building RPM/Debian Packaging\n\nIf desired, this scripting can be added to an EC2 instance for further testing.  A convenience pair of scripts - bin/make_rpm.sh and bin/make_deb.sh - have been provided to quickly build test packages.\nEach may be invoked via `make rpm` and `make deb` respectively.\n\nDebian packaging in particular requires you have a GPG key configured on your system.\n\nBe sure to update VERSION!  If you use the same VERSION as the latest public release then test instances will not see it as an update!\n\n### [EXPERIMENTAL] Docker Container Build\n\nYou can build .rpm packages using Docker via `make docker-build-rpm` and .deb packages via `make docker-build-deb`. You can run both via `make docker-build`. The built packages will be in the `out` directory. This currently does not support the Amazon-proprietary build process.\n\n**Debian packaging tools are intended for testing purposes only.  Please follow standard Debian process to submit patches.**\n\n### Why is it generic.rpm?\n\n\"generic.rpm\" is used internally to differentiate the specfile for Fedora/RHEL/CentOS from Amazon Linux.\nThere is a separate Amazon-proprietary rpmspec and build process optimized for Amazon Linux.\n\n## Why UNIX shell?\n\nThis package is intended as a simple reference implementation of the instance-side pieces of the EC2 Instance Connect feature, though as time has gone on it has become much more complex than originally intended.\nAmazon is considering reimplementation in another language for the future but for the time being we will continue to iterate on the shell implementation.\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": ["2022-05-09T21:48:12Z", "2022-01-11T15:12:53Z", "2021-03-16T15:13:59Z", "2020-11-18T03:33:51Z", "2020-01-17T17:12:36Z", "2019-10-02T22:12:49Z", "2019-08-21T21:23:20Z", "2019-07-31T20:48:35Z"]}, {"name": "aws-eks-best-practices", "description": "A best practices guide for day 2 operations, including operational excellence, security, reliability, performance efficiency, and cost optimization.", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## Amazon Elastic Kubernetes Service (Amazon EKS) Best Practices\n\nA best practices guide for day 2 operations, including operational excellence, security, reliability, performance efficiency, and cost optimization.\n\nReturn to [Live Docs](https://aws.github.io/aws-eks-best-practices/).\n\n## Contributing\n\nWhile the best practices were originally authored by AWS employees, we encourage and welcome contributions from the Kubernetes user community. If you have a best practice that you would like to share, please review the [Contributing Guidelines](https://github.com/aws/aws-eks-best-practices/blob/master/CONTRIBUTING.md) before submitting a PR. \n\n## License Summary\n\nThe documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.\n\nThe sample code within this documentation is made available under the MIT-0 license. See the LICENSE-SAMPLECODE file.\n", "release_dates": []}, {"name": "aws-elastic-beanstalk-cli", "description": "The EB CLI is a command line interface for Elastic Beanstalk that provides interactive commands that simplify creating, updating and monitoring environments from a local repository.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": []}, {"name": "aws-elastic-beanstalk-cli-setup", "description": "Simplified EB CLI installation mechanism.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Elastic Beanstalk CLI Installer\n- [1. Overview](#1-overview)\n  - [1.1. Prerequisites](#11-prerequisites)\n- [2. Quick start](#2-quick-start)\n  - [2.1. Clone this repository](#21-clone-this-repository)\n  - [2.2. Install/Upgrade the EB CLI](#22-installupgrade-the-eb-cli)\n    - [MacOS/Linux](#macoslinux)\n    - [Windows](#windows)\n  - [2.3. After installation](#23-after-installation)\n- [3. Usage](#3-usage)\n  - [3.1 Advanced usage](#31-advanced-usage)\n  - [3.2 Options](#32-options)\n- [4. Troubleshooting](#4-troubleshooting)\n- [5. Frequently asked questions](#5-frequently-asked-questions)\n  - [5.1. For the **experienced Python developer**, what's the advantage of this mode of installation instead of regular `pip` inside a `virtualenv`?](#51-for-the-experienced-python-developer-whats-the-advantage-of-this-mode-of-installation-instead-of-regular-pip-inside-a-virtualenv)\n  - [5.2. On macOS (or Linux systems with `brew`), is this better than `brew install awsebcli`?](#52-on-macos-or-linux-systems-with-brew-is-this-better-than-brew-install-awsebcli)\n  - [5.3. I already have the EB CLI installed. Can I still execute `ebcli_installer.py`?](#53-i-already-have-the-eb-cli-installed-can-i-still-execute-ebcli_installerpy)\n  - [5.4. How does `ebcli_installer.py` work?](#54-how-does-ebcli_installerpy-work)\n  - [5.5. Are there dependency problems that this mode of installation doesn't solve?](#55-are-there-dependency-problems-that-this-mode-of-installation-doesnt-solve)\n- [6. License](#6-license)\n\n## 1. Overview\n\nThis repository hosts scripts to generate self-contained installations of the [EB CLI](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3.html).\n\n### 1.1. Prerequisites\n\nYou will need to have the following prerequisites installed before running the install script.\n\n* **Git**\n  * If not already installed you can download git from the [Git downloads page](https://git-scm.com/downloads).\n* **Python**\n  * We recommend that you install Python using the [pyenv](https://github.com/pyenv/pyenv) Python version manager. Alternately, you can download Python from the [Python downloads page](https://www.python.org/downloads/).\n* **virtualenv**\n  * Follow the [virtualenv documentation](https://virtualenv.pypa.io/en/latest/installation.html) to install virtualenv.\n\n## 2. Quick start\n\n### 2.1. Clone this repository\n\nUse the following:\n\n```\ngit clone https://github.com/aws/aws-elastic-beanstalk-cli-setup.git\n```\n\n### 2.2. Install/Upgrade the EB CLI\n\n#### MacOS/Linux\nOn **Bash** or **Zsh**:\n\n```\npython ./aws-elastic-beanstalk-cli-setup/scripts/ebcli_installer.py\n```\n\n#### Windows\nIn **PowerShell** or in a **Command Prompt** window:\n\n```\npython .\\aws-elastic-beanstalk-cli-setup\\scripts\\ebcli_installer.py\n```\n\n### 2.3. After installation\n\nOn Linux and macOS, the output contains instructions to add the EB CLI (and Python) executable file to the shell's `$PATH` variable, if it isn't already in it.\n\n## 3. Usage\n\nThe `ebcli_installer.py` Python script will install the [awsebcli](https://pypi.org/project/awsebcli/) package in a virtual environment to prevent potential conflicts with other Python packages.\n\nFor most use cases you can execute the `ebcli_installer.py` script with no arguments.\n\n```\npython ./aws-elastic-beanstalk-cli-setup/scripts/ebcli_installer.py\n```\n\n### 3.1 Advanced usage\n\n  - To install a **specific version** of the EB CLI:\n\n    ```shell\n    python scripts/ebcli_installer.py --version 3.14.13\n    ```\n\n  - To install the EB CLI with a specific **version of Python** (the Python version doesn't need to be in `$PATH`):\n\n    ```shell\n    python scripts/ebcli_installer.py --python-installation /path/to/some/python/on/your/computer\n    ```\n\n  - To install the EB CLI **from source** (Git repository, .tar file, .zip file):\n    ```shell\n    python scripts/ebcli_installer.py --ebcli-source /path/to/awsebcli.zip\n\n    python scripts/ebcli_installer.py --ebcli-source /path/to/EBCLI/codebase/on/your/computer\n    ```\n  - To install the EB CLI at a **specific location**, instead of in the standard `.ebcli-virtual-env` directory in the user's home directory:\n\n    ```shell\n    python scripts/ebcli_installer.py --location /path/to/ebcli/installation/location\n    ```\n### 3.2 Options\n\n```\noptions:\n  -h, --help            show this help message and exit\n  -e VIRTUALENV_EXECUTABLE, --virtualenv-executable VIRTUALENV_EXECUTABLE\n                        path to the virtualenv installation to use to create the EBCLI's virtualenv\n  -i, --hide-export-recommendation\n                        boolean to hide recommendation to modify PATH\n  -l LOCATION, --location LOCATION\n                        location to store the awsebcli packages and its dependencies in\n  -p PYTHON_INSTALLATION, --python-installation PYTHON_INSTALLATION\n                        path to the python installation under which to install the awsebcli and its\n                        dependencies\n  -q, --quiet           enable quiet mode to display only minimal, necessary output\n  -s EBCLI_SOURCE, --ebcli-source EBCLI_SOURCE\n                        filesystem path to a Git repository of the EBCLI, or a .zip or .tar file of\n                        the EBCLI source code; useful when testing a development version of the EBCLI.\n  -v VERSION, --version VERSION\n                        version of EBCLI to install\n```\n\n## 4. Troubleshooting\n\n- **Linux**\n\n    Most installation problems have been due to missing libraries such as `OpenSSL`.\n\n  - On **Ubuntu and Debian**, run the following command to install dependencies.\n\n    ```shell\n    apt-get install \\\n        build-essential zlib1g-dev libssl-dev libncurses-dev \\\n        libffi-dev libsqlite3-dev libreadline-dev libbz2-dev\n    ```\n\n  - On **Amazon Linux and Fedora**, run the following command to install dependencies.\n\n    ```shell\n    yum group install \"Development Tools\"\n    yum install \\\n        zlib-devel openssl-devel ncurses-devel libffi-devel \\\n        sqlite-devel.x86_64 readline-devel.x86_64 bzip2-devel.x86_64\n    ```\n\n- **macOS**\n\n  Most installation problems on macOS are related to loading and linking OpenSSL and zlib. The following command installs the necessary packages and tells the Python installer where to find them:\n\n    ```\n    brew install zlib openssl readline\n    CFLAGS=\"-I$(brew --prefix openssl)/include -I$(brew --prefix readline)/include -I$(xcrun --show-sdk-path)/usr/include\" LDFLAGS=\"-L$(brew --prefix openssl)/lib -L$(brew --prefix readline)/lib -L$(brew --prefix zlib)/lib\"\n    ```\n    Run `brew info` to get the latest environment variable export suggestions, such as `brew info zlib`\n\n- **Windows**\n\n    - In PowerShell, if you encounter an error with the message \"execution of scripts is disabled on this system\", set the [execution policy](https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.security/set-executionpolicy?view=powershell-6) to `\"RemoteSigned\"` and then rerun `bundled_installer`.\n\n      ```ps1\n      Set-ExecutionPolicy RemoteSigned\n      ```\n    - If you encounter an error with the message \"No module named 'virtualenv'\", use the following commands to install `virtualenv` and the EB CLI:\n      ```ps1\n      pip uninstall -y virtualenv\n      pip install virtualenv\n      python .\\aws-elastic-beanstalk-cli-setup\\scripts\\ebcli_installer.py\n      ```\n## 5. Frequently asked questions\n\n### 5.1. For the **experienced Python developer**, what's the advantage of this mode of installation instead of regular `pip` inside a `virtualenv`?\n\nEven within a `virtualenv`, a developer might need to install multiple packages whose dependencies are in conflict. For example, at times the AWS CLI and the EB CLI have used conflicting versions of `botocore`. [One such instance](https://github.com/aws/aws-cli/issues/3550) was particularly egregious. When there are conflicts, users have to manage separate `virtualenvs` for each of the conflicting packages, or find a combination of the packages without conflicts.\n\nBoth of these workarounds become unmanageable over time, and as the number of packages that are in conflict increases.\n\n### 5.2. On macOS (or Linux systems with `brew`), is this better than `brew install awsebcli`?\n\n**Yes**, for these reasons:\n\n  - The AWS Elastic Beanstalk team has no control over how `brew` operates.\n  - The `brew install ...` mechanism doesn't solve the problem of dependency conflicts, which is a primary goal of this project.\n\n### 5.3. I already have the EB CLI installed. Can I still execute `ebcli_installer.py`?\n\n**Yes**.\n\nConsider the following two cases:\n\n- `ebcli_installer.py` was previously run, creating `.ebcli-virtual-env` in the user's home directory (or the user's choice of a directory indicated through the `--location` argument). In this case, the EB CLI will overwrite `.ebcli-virtual-env` and attempt to install the latest version of the EB CLI in the `virtualenv` within it.\n\n- `eb` is in `$PATH`, however, it wasn't installed by `ebcli_installer.py`. In this case, the installer will install `eb` within `.ebcli-virtual-env` in the\nuser's home directory (or the user's choice of a directory indicated through the `--location` argument), and prompt the user to prefix\n`/path-to/.ebcli-virtual-env/executables` to `$PATH`. Until you perform this action, the older `eb` executable file will continue to be referenced when you type `eb`.\n\n### 5.4. How does `ebcli_installer.py` work?\n\nWhen executing the Python script, `ebcli_installer.py` does the following:\n\n- Creates a `virtualenv` exclusive to the `eb` installation.\n- Installs `eb` inside that `virtualenv`.\n- In the `<installation-location>/executables` directory, it generates:\n  - A `.py` wrapper for `eb` on Linux or macOS.\n  - `.bat` and `.ps1` wrappers for `eb` on Windows.\n- When complete, you will be prompted to add `<installation-location>/executables` to `$PATH`, only if the directory is not already in it.\n\n### 5.5. Are there dependency problems that this mode of installation doesn't solve?\n\nUnfortunately, **yes**.\n\nSuppose the dependencies of `eb`, say `Dep A` and `Dep B`, are in conflict. Because `pip` lacks dependency management capabilities, the resulting `eb` installation might not work.\n\n## 6. License\n\nThis library is licensed under the Apache-2.0 License.\n", "release_dates": ["2022-01-21T23:00:57Z", "2019-05-23T18:28:00Z"]}, {"name": "aws-elb-best-practices", "description": "ELB Best Practices Guides", "language": null, "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "## Amazon Elastic Load Balancing (ELB) Best Practices\n\nThis is the code repository for the [ELB Best Practices Guides](https://aws.github.io/aws-elb-best-practices/)\n\nThe aim of this project is to offer prescriptive guidance for utilizing ELB in the AWS Cloud. The content in the guides is a collection of best practices aligned with the AWS Well-Architected Framework and other sources such as the AWS documentation, AWS Whitepapers, Amazon Builders' Library. As the ELB Team, we have incorporated insights gained from our direct experiences with customers. We also welcome suggestions from the community.\n\nFor official ELB Documentation, [click here](https://docs.aws.amazon.com/elasticloadbalancing/)\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.\n\n", "release_dates": []}, {"name": "aws-emr-best-practices", "description": "A best practices guide for using AWS EMR. The guide will cover best practices on the topics of cost, performance, security, operational excellence, reliability and application specific best practices across Spark, Hive, Hudi, Hbase and more. ", "language": "Shell", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## Amazon EMR on Amazon Best Practices\n\nA best practices guide for submitting spark applications, integration with hive metastore, security, storage options, debugging options and performance considerations.\n\nReturn to [Live Docs](https://aws.github.io/aws-emr-best-practices/).\n\n## License Summary\n\nThe documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.\n\nThe sample code within this documentation is made available under the MIT-0 license. See the LICENSE-SAMPLECODE file.", "release_dates": []}, {"name": "aws-emr-containers-best-practices", "description": "Best practices and recommendations for getting started with Amazon EMR on EKS.", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## Amazon EMR on Amazon EKS Best Practices\n\nA best practices guide for submitting spark applications, integration with hive metastore, security, storage options, debugging options and performance considerations.\n\nReturn to [Live Docs](https://aws.github.io/aws-emr-containers-best-practices/).\n\n## License Summary\n\nThe documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.\n\nThe sample code within this documentation is made available under the MIT-0 license. See the LICENSE-SAMPLECODE file.\n\n## How to make a change\n1. [Fork the repository](https://docs.github.com/en/get-started/quickstart/fork-a-repo#forking-a-repository)\n2. Make your change and double-check the [mkdocs.yml](./mkdocs.yml) is updated accordingly.\n3. Install the MkDocs command tool and material theme if needed:\n```bash\npip install mkdocs\npip install mkdocs-material # material theme\n```\n4. MkDocs comes with a built-in dev-server that lets you preview your documentation as you work on it. Make sure you're in the same directory as the `mkdocs.yml` configuration file, then run the command:\n```bash\nmkdocs serve\n```\n5. Open up http://127.0.0.1:8000/ in your browser, and you'll see the best practice website being displayed locally.\n6. Adjust your document changes in real time.\n7. When everything looks good and you're ready to deploy the change, run the command to build/compile the website content:\n```bash\nmkdocs build\n```\n8. This will refresh the directory `site`. Take a look inside the directory and make sure your changes are included.\n```bash\nls site\n```\n9. Commit change to github and send us a [pull request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork).\n", "release_dates": []}, {"name": "aws-encryption-sdk-c", "description": "AWS Encryption SDK for C", "language": "C", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Encryption SDK for C\n\nThe AWS Encryption SDK for C is a client-side encryption library designed to make it easy for\neveryone to encrypt and decrypt data using industry standards and best practices. It uses a\ndata format compatible with the AWS Encryption SDKs in other languages. For more information on\nthe AWS Encryption SDKs in all languages, see the [Developer Guide](https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html).\n\nAlso, see the [API documentation](https://aws.github.io/aws-encryption-sdk-c/html/) for the AWS Encryption SDK for C.\n\n[Security issue notifications](./CONTRIBUTING.md#security-issue-notifications)\n\nSee [Support Policy](./SUPPORT_POLICY.rst) for for details on the current support status of all major versions of this library.\n\n## Building the AWS Encryption SDK for C with support for AWS KMS\n\nWe will demonstrate some simple build recipes for Linux, Mac, and Windows operating systems. These \nrecipes assume a newly installed system with default installs of dependency packages.\n\nThe Windows instructions install everything in your current directory (where you run the build process). To change the installation directory, see the Tips and Tricks section at the end of this README.\n\nThe AWS Encryption SDK for C can be used with AWS KMS, but it is not required. If you want to build\na minimal version of the ESDK without AWS KMS support, see \"Minimal C build without AWS KMS support\", below.\n\nFor best results when doing a build with AWS KMS integration, do not install aws-c-common directly.\nBuild and install the AWS SDK for C++, which will build and install aws-c-common for you (see the C++ SDK dependencies\n [here](https://github.com/aws/aws-sdk-cpp/blob/master/third-party/CMakeLists.txt#L18)). If\nyou install aws-c-common before building the AWS SDK for C++, this will fool the AWS SDK for\nC++ install logic, and you will be forced to install several other dependencies manually. Version 1.8.32 of the\nAWS SDK for C++ is supported by version v1.0.1 of the AWS Encryption SDK for C.\n\n### If you are working on an EC2 instance, regardless of operating system\n\nFor best results, create an EC2 instance with an instance profile that at a\nminimum has AWS KMS permissions for Encrypt, Decrypt, and GenerateDataKey for\nat least one KMS key in your account. You will not need any other AWS\npermissions to use the AWS Encryption SDK for C.\n\n### Dependencies\n\n1. OpenSSL 1.0.2 or newer, or 1.1.0 or newer\n1. CMake 3.9 or newer\n1. C/C++ compiler\n1. aws-c-common, typically bundled with the AWS SDK for C++\n1. The AWS SDK for C++ version 1.9.35 or newer\n\nThe AWS SDK for C++ and the AWS Encryption SDK for C share dependencies on OpenSSL, aws-c-common, and CMake. The AWS SDK for C++ has additional dependencies and prerequisites. See [AWS SDK for\nC++: Getting Started](https://github.com/aws/aws-sdk-cpp#getting-started).\n\nYou need to compile the AWS Encryption SDK for C and its dependencies as either all\nshared or all static libraries. \n\nTo build shared libraries, specify the `-DBUILD_SHARED_LIBS=ON` flag to build\naws-c-common, the AWS SDK for C++, and the AWS Encryption SDK.\n\nTo build static libraries, specify the `-DBUILD_SHARED_LIBS=OFF` flag to build\naws-c-common, the AWS SDK for C++, and the AWS Encryption SDK.\n\n### Linux Build Recipe\n\nFirst, build the AWS SDK for C++. That will install the shared dependencies.\n\nIf you only need AWS SDK for C++ to use the AWS Encryption SDK, you have the option to build only the AWS KMS SDK.\nAdd the `-DBUILD_ONLY=\"kms\"` flag and `-DBUILD_SHARED_LIBS=ON|OFF` to `cmake` in the instructions provided.\n\n[Follow the AWS SDK for C++ build instructions](https://docs.aws.amazon.com/sdk-for-cpp/v1/developer-guide/setup-linux.html).\n\nNow, build and install the AWS Encryption SDK for C:\n\n    git clone --recurse-submodules https://github.com/aws/aws-encryption-sdk-c.git\n    mkdir build-aws-encryption-sdk-c && cd build-aws-encryption-sdk-c\n    cmake ../aws-encryption-sdk-c -DBUILD_SHARED_LIBS=ON\n    make && sudo make install ; cd ..\n\n### MacOS Build Recipe\n\n[Homebrew](https://brew.sh) is a convenient way to get build tools for MacOS systems.\n\nWith Homebrew installed, run the following:\n\n    brew install openssl@1.1 cmake\n\nThe AWS SDK for C++ can be installed with Homebrew, which will install the full AWS SDK. If you \nneed the AWS SDK for C++ only to use the AWS Encryption SDK, you can build only the AWS KMS SDK.\nSee [these directions](https://docs.aws.amazon.com/sdk-for-cpp/v1/developer-guide/setup-linux.html#setup-linux-from-source)\nand specify the `-DBUILD_ONLY=\"kms\"` flag to `cmake` in the instructions provided.\n\nNow, build and install the AWS Encryption SDK for C:\n\n    git clone --recurse-submodules https://github.com/aws/aws-encryption-sdk-c.git\n    mkdir build-aws-encryption-sdk-c && cd build-aws-encryption-sdk-c\n    cmake -G Xcode -DBUILD_SHARED_LIBS=ON -DOPENSSL_ROOT_DIR=\"/usr/local/opt/openssl@1.1\" ../aws-encryption-sdk-c \n    xcodebuild -target install; cd ..\n\n### Windows Build Recipe\n\n**Note**: See the [docker-images folder](https://github.com/aws/aws-encryption-sdk-c/tree/master/docker-images) for some Windows build recipes that automate many of these steps.\n\n**Note**: These instructions do not yet correctly build a statically-linked AWS Encryption SDK for C on Windows systems, i.e. with `-DBUILD_SHARED_LIBS=OFF`. We will update this README with instructions for that configuration soon.\n\nInstall Visual Studio version 15 or later with the Windows Universal C Runtime and [Git for Windows](https://git-scm.com/download/win).\n\nUse the \"x64 Native Tools Command Prompt\" for all commands listed here. Run the following commands in the directory where you want to do the build and installation.\n\nInstall Microsoft vcpkg by [following these directions](https://github.com/microsoft/vcpkg#quick-start-windows). Note the path to your `vcpkg.cmake` tool. This path will be something like `vcpkg\\scripts\\buildsystems\\vcpkg.cmake` and you will need to pass it as `-DCMAKE_TOOLCHAIN_FILE` in your builds.\n\nUse vcpkg to install prerequisites:\n\n    vcpkg install curl:x64-windows openssl:x64-windows\n\nYou may also want to [configure vcpkg for Visual Studio CMake projects](https://github.com/microsoft/vcpkg#vcpkg-with-visual-studio-cmake-projects).\n\nBuild the AWS SDK for C++. This installs the aws-c-common dependency, too. If you need the AWS SDK for C++ only to use the \nAWS Encryption SDK, you have the option to build only the AWS KMS SDK. Add the `-DBUILD_ONLY=kms` to build only the AWS KMS client.\n\nFollow [these instructions](https://docs.aws.amazon.com/sdk-for-cpp/v1/developer-guide/setup-windows.html#setup-windows-from-source), using `-DCMAKE_TOOLCHAIN_FILE` as described in the vcpkg setup instructions. Add `-DBUILD_SHARED_LIBS=ON` for shared libraries or `-DBUILD_SHARED_LIBS=OFF` for static libraries.\n\nNow, build and install the AWS Encryption SDK for C. Be sure to update `-DCMAKE_TOOLCHAIN_FILE` in the commands below.\n\nUpdate `-DCMAKE_PREFIX_PATH` to the location of your AWS SDK for C++ and aws-c-common installations.\n\n    git clone --recurse-submodules https://github.com/aws/aws-encryption-sdk-c.git\n    mkdir build-aws-encryption-sdk-c && cd build-aws-encryption-sdk-c\n    cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON -DCMAKE_TOOLCHAIN_FILE\"=%cd%\\..\\vcpkg\\scripts\\buildsystems\\vcpkg.cmake\" -DCMAKE_PREFIX_PATH=\"C:/Program Files (x86)/aws-cpp-sdk-all;C:/Program Files (x86)/aws-cpp-sdk-all/lib/aws-c-common\" ..\\aws-encryption-sdk-c\n    cmake --build . && cmake --build . --target install && cd ..\n\n## Building C only, without AWS KMS support\n\nIf you don't need AWS KMS support, you can build the AWS Encryption SDK for C without the AWS SDK. This build is C-only with no C++ support\nrequired.\n\nTo build without AWS KMS support, follow the build instructions above for your platform, substituting aws-c-common for aws-sdk-cpp.\n\n## Doxygen Documentation\n\nTo build the documentation, you'll need [doxygen](http://www.doxygen.nl/) installed.  Check the [downloads](http://www.doxygen.nl/download.html) page \nor use your OS package manager.\n\nNext, rerun the above cmake with a `-DBUILD_DOC=\"ON\"` flag before building aws-encryption-sdk-c.\n\nFinally, run either `make doc_doxygen` (Unix), `MSBuild.exe .\\doc_doxygen.vcxproj` (Windows) or `xcodebuild -scheme doc_doxygen` (Mac) to generate the documentation.\n\n## Compiling your program using the AWS Encryption SDK for C\n\nOnce you have installed the AWS Encryption SDK for C, you are ready to start writing\nyour own programs with it.\n\nWhen doing a C compilation (not using the AWS KMS keyring) be sure to include the flags\n``-lcrypto -laws-encryption-sdk -laws-c-common``.\n\nWhen doing a C++ compilation (using the AWS KMS keyring) be sure to include the flags\n``-std=c++11 -lcrypto -laws-encryption-sdk -laws-encryption-sdk-cpp -laws-c-common -laws-cpp-sdk-kms -laws-cpp-sdk-core``.\n\nIn the examples directory of this repo are several self-standing C and C++ files.\nOn a successful build, these files will already be compiled in the examples subdirectory\nof the build directory. Additionally, if you want a quick test that you have\nbuilt and installed the AWS Encryption SDK successfully, you can copy any of\nthe example files out of that directory and build them yourself.\n\nHere are sample command lines using gcc/g++ to build a couple of the example files,\nassuming that the libraries and headers have been installed where your system\nknows how to find them.\n\n```\ng++ -o string string.cpp -std=c++11 -lcrypto -laws-encryption-sdk -laws-encryption-sdk-cpp -laws-c-common -laws-cpp-sdk-kms -laws-cpp-sdk-core -laws-crt-cpp\ngcc -o raw_aes_keyring raw_aes_keyring.c -lcrypto -laws-encryption-sdk -laws-c-common\n```\n\nNote that the C++ files using the AWS KMS keyring will require\nyou to make sure that AWS credentials are set up on your machine to run properly.\n\n## Tips and tricks\n\nWhen building aws-sdk-cpp, you can save time by only building the subcomponents needed\nby the AWS Encryption SDK for C with `-DBUILD_ONLY=\"kms\"`\n\nThe `-DENABLE_UNITY_BUILD=ON` option will further speed up the aws-sdk-cpp build.\n\nTo enable debug symbols, set `-DCMAKE_BUILD_TYPE=Debug` at initial cmake time,\nor use `ccmake .` to update the configuration after the fact.\n\nIf desired, you can set the installations to happen in an arbitrary\ndirectory with `-DCMAKE_INSTALL_PREFIX=[path to install to]` as an argument to cmake.\n\nIf you set `CMAKE_INSTALL_PREFIX` for the dependencies, when building the AWS\nEncryption SDK for C you must either (1) set `CMAKE_INSTALL_PREFIX` to the same path,\nwhich will cause it to pick up the dependencies and install to the same directory\nor (2) set `CMAKE_PREFIX_PATH` to include the same path, which will cause it to pick\nup the dependencies but NOT install to the same directory.\n\nYou can also use `-DOPENSSL_ROOT_DIR=[path to where openssl was installed]` to make\nthe build use a particular installation of OpenSSL.\n\nBy default the cmake for this project detects whether you have aws-sdk-cpp-core\nand aws-sdk-cpp-kms installed in a place it can find, and only if so will it build\nthe C++ components, (enabling use of the AWS KMS keyring.) You can override the detection\nlogic by setting `-DBUILD_AWS_ENC_SDK_CPP=OFF` to never build the C++ components or\nby setting `-DBUILD_AWS_ENC_SDK_CPP=ON` to require building the C++ components (and\nfail if the C++ dependencies are not found.)\n\n## CBMC\n\nTo learn more about CBMC and proofs specifically, review the training material [here](https://model-checking.github.io/cbmc-training).\n\nThe `verification/cbmc/proofs` directory contains CBMC proofs.\n\nIn order to run these proofs you will need to install CBMC and other tools by following the instructions [here](https://model-checking.github.io/cbmc-training/installation.html).\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": ["2023-04-05T20:08:58Z", "2022-08-30T18:30:25Z", "2021-10-20T21:26:50Z", "2021-10-19T18:02:48Z", "2021-07-13T21:19:21Z", "2021-06-16T18:06:17Z", "2021-05-27T20:22:35Z", "2021-05-27T18:41:46Z", "2020-09-25T01:28:21Z", "2020-09-24T21:41:11Z", "2019-11-04T22:21:21Z", "2019-05-20T22:55:31Z", "2019-05-13T20:49:26Z", "2019-02-28T22:41:03Z", "2019-02-22T00:06:28Z", "2019-02-05T01:29:41Z"]}, {"name": "aws-encryption-sdk-cli", "description": "CLI wrapper around aws-encryption-sdk-python", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2021-11-11T21:10:51Z", "2021-07-13T19:26:13Z", "2021-07-13T16:09:39Z", "2021-06-16T20:21:33Z", "2021-05-27T21:10:27Z", "2021-05-27T18:32:13Z", "2020-10-27T23:35:13Z", "2020-10-27T22:56:52Z", "2020-09-25T02:13:52Z", "2020-09-25T01:25:01Z", "2019-10-15T21:06:04Z", "2019-09-30T20:59:53Z"]}, {"name": "aws-encryption-sdk-dafny", "description": "AWS Encryption SDK for Dafny", "language": "Dafny", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Encryption SDK for Dafny\n\n![Build Status - master branch](https://codebuild.us-west-2.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiVmIzeGwwQmY5bXdMQXg2aVBneWtDc3FHSWRHTjYrNnVUem9nNXJFUmY2Rk1yRnJvSjJvK3JCL2RScFRjSVF1UjA1elR3L0xpTVpiNmRZS0RyWjJpTnBFPSIsIml2UGFyYW1ldGVyU3BlYyI6InBBQm1tT1BPNjB3RU9XUS8iLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=master)\n\nAWS Encryption SDK for Dafny\n\n[Security issue notifications](./CONTRIBUTING.md#security-issue-notifications)\n\n## Building the AWS Encryption SDK for Dafny\n\nTo build, the AWS Encryption SDK requires the most up to date version of [dafny](https://github.com/dafny-lang/dafny) on your PATH.\nIn addition, this project uses the parallel verification tasks provided by the [dafny.msbuild](https://github.com/dafny-lang/dafny.msbuild) MSBuild plugin,\nand thus requires [dotnet 3.0](https://dotnet.microsoft.com/download/dotnet-core/3.0).\n\nTo run the dafny verifier across all files:\n\n```\n# Currently, test depends on src, so verifying test will also verify src\ndotnet build -t:VerifyDafny test\n```\n\nThe tests currently require native implementations of cryptographic primitives and other methods,\nso they can only be run when embedding this library into one of the compilation target languages supported by Dafny:\n\n- [.NET](aws-encryption-sdk-net)\n\n## Generating Code from Smithy Model\n\nTo generate code from the Smithy models for either the AWS Encryption SDK or for any of its dependencies, you will need the [Polymorph](https://github.com/awslabs/polymorph) project set up locally.\n\nTo run the code generator, open any of the modules (e.g. AwsCryptographyPrimitives), then run:\n\n```\n make polymorph_code_gen CODEGEN_CLI_ROOT=/[path]/[to]/smithy-dafny/codegen/smithy-dafny-codegen-cli\n```\n\n### Transpiling Generated Code to a Runtime\n\nThe AWS Encryption SDK for Dafny must be transpiled to a runtime to be used.\nThere is no Dafny runtime, so there is no concept of \"running the AWS Encryption SDK for Dafny\".\n\nTo transpile the generated code to a runtime (e.g. Dotnet), open the module, then run:\n\n```\nmake transpile_net\n```\n\n## Generate Duvet Reports\n\nThis repo uses Duvet to directly document the [specification](https://github.com/awslabs/aws-encryption-sdk-specification) alongside this implementation.\nRefer to the [specification](https://github.com/awslabs/aws-encryption-sdk-specification/blob/master/README.md) for how to install duvet in order to generate reports.\n\nTo generate a report for this AWS Encryption SDK for Dafny, run the following command:\n\n```\nmake duvet\n```\n\nIt will output if there is any missing coverage.\n\nBy default this will extract the spec to the `compliance` directory.\nIf you only want to generate the report you can do so with the following:\n\n```\nmake duvet_report\n```\n\n```\nopen specification_compliance_report.html\n```\n\nTo view the report, look at the generated `specification_compliance_report.html`:\n\n### To install Duvet\n\n```\ncargo +stable install duvet\n```\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n\n", "release_dates": ["2023-11-22T00:12:25Z", "2023-10-11T19:10:24Z", "2022-09-08T00:47:18Z", "2022-05-17T19:54:57Z"]}, {"name": "aws-encryption-sdk-java", "description": "AWS Encryption SDK", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Encryption SDK for Java\n\nThe AWS Encryption SDK enables secure client-side encryption. It uses cryptography best practices to protect your data and protect the encryption keys that protect your data. Each data object is protected with a unique data encryption key, and the data encryption key is protected with a key encryption key called a *wrapping key* or *master key*. The encryption method returns a single, portable [encrypted message](https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/message-format.html) that contains the encrypted data and the encrypted data key, so you don't need to keep track of the data encryption keys for your data. You can use KMS keys in [AWS Key Management Service](https://aws.amazon.com/kms/) (AWS KMS) as wrapping keys. The AWS Encryption SDK also provides APIs to define and use encryption keys from other key providers. \n\nThe AWS Encryption SDK for Java provides methods for encrypting and decrypting strings, byte arrays, and byte streams. For details, see the [example code][examples] and the [Javadoc](https://aws.github.io/aws-encryption-sdk-java).\n\nFor more details about the design and architecture of the AWS Encryption SDK, see the [AWS Encryption SDK Developer Guide](https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/).\n\n[Security issue notifications](./CONTRIBUTING.md#security-issue-notifications)\n\nSee [Support Policy](./SUPPORT_POLICY.rst) for details on the current support status of all major versions of this library.\n\n## Getting Started\n\n### Required Prerequisites\nTo use the AWS Encryption SDK for Java you must have:\n\n* **A Java 8 or newer development environment**\n\n  If you do not have one, we recommend [Amazon Corretto](https://aws.amazon.com/corretto/).\n\n  **Note:** If you use the Oracle JDK, you must also download and install the [Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files](http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html).\n\n* **Declare a Dependency on the AWS Encryption SDK in Java and its dependencies**\n\n  This library requires the AWS Cryptographic Material Providers Library in Java, and the KMS and DynamoDB clients from the AWS Java SDK V2.\n\n  The KMS client from the AWS SDK for Java V1 is an **optional** dependency.\n\n  **Note:** The AWS Cryptographic Material Providers Library in Java only supports the AWS SDK for Java V2 and requires a HARD dependency on the AWS SDK for Java V2's KMS and DynamoDB clients, regardless of whether a KMS Keyring or Hierarchical Keyring is used.\n\n  * **Via Apache Maven**  \n    Add the following to your project's `pom.xml`.\n    ```xml\n    <project>\n    ...\n    <dependencyManagement>\n     <dependencies>\n        <dependency>\n          <groupId>software.amazon.awssdk</groupId>\n          <artifactId>bom</artifactId>\n          <version>2.20.91</version>\n          <type>pom</type>\n          <scope>import</scope>\n        </dependency>\n     </dependencies>\n    </dependencyManagement>\n    <dependencies>\n      <dependency>\n        <groupId>com.amazonaws</groupId>\n        <artifactId>aws-encryption-sdk-java</artifactId>\n        <version>3.0.0</version>\n      </dependency>\n      <dependency>\n        <groupId>software.amazon.cryptography</groupId>\n        <artifactId>aws-cryptographic-material-providers</artifactId>\n        <version>1.0.2</version>\n      </dependency>\n      <dependency>\n        <groupId>software.amazon.awssdk</groupId>\n        <artifactId>dynamodb</artifactId>\n      </dependency>\n      <dependency>\n        <groupId>software.amazon.awssdk</groupId>\n        <artifactId>kms</artifactId>\n      </dependency>\n      <!-- The following are optional -->\n      <dependency>\n          <groupId>com.amazonaws</groupId>\n          <artifactId>aws-java-sdk</artifactId>\n          <version>1.12.394</version>\n          <optional>true</optional>\n      </dependency>\n    </dependencies>\n    ...\n    </project>\n    ```\n\n  * **Via Gradle Kotlin**  \n    In a Gradle Java Project, add the following to the _dependencies_ section:\n    ```kotlin\n    implementation(\"com.amazonaws:aws-encryption-sdk-java:3.0.0\")\n    implementation(\"software.amazon.cryptography:aws-cryptographic-material-providers:1.0.2\")\n    implementation(platform(\"software.amazon.awssdk:bom:2.20.91\"))\n    implementation(\"software.amazon.awssdk:kms\")\n    implementation(\"software.amazon.awssdk:dynamodb\")\n    // The following are optional:\n    implementation(\"com.amazonaws:aws-java-sdk:1.12.394\")\n    ```\n\n* **Bouncy Castle** or **Bouncy Castle FIPS**\n\n  The AWS Encryption SDK for Java uses Bouncy Castle to serialize and deserialize cryptographic objects.\n  It does not explicitly use Bouncy Castle (or any other [JCA Provider](https://docs.oracle.com/javase/8/docs/api/java/security/Provider.html)) for the underlying cryptography.\n  Instead, it uses the platform default, which you can configure or override as documented in the\n  [Java Cryptography Architecture (JCA) Reference Guide](https://docs.oracle.com/javase/9/security/java-cryptography-architecture-jca-reference-guide.htm#JSSEC-GUID-2BCFDD85-D533-4E6C-8CE9-29990DEB0190).\n\n  If you do not have Bouncy Castle, go to https://bouncycastle.org/latest_releases.html, then download the provider file that corresponds to your JDK.\n  Or, you can pick it up from Maven (groupId: `org.bouncycastle`, artifactId: `bcprov-jdk18on`).\n\n  Beginning in version 1.6.1, the AWS Encryption SDK for Java also works with Bouncy Castle FIPS (groupId: `org.bouncycastle`, artifactId: `bc-fips`)\n  as an alternative to non-FIPS Bouncy Castle. For help installing and configuring Bouncy Castle FIPS, see [BC FIPS documentation](https://www.bouncycastle.org/documentation.html), in particular, **User Guides** and **Security Policy**.\n\n### Optional Prerequisites\n\n#### AWS Integration\nYou don't need an Amazon Web Services (AWS) account to use the AWS Encryption SDK, but some [example code][examples] require an AWS account, an AWS KMS key, and the AWS SDK for Java (either 1.x or 2.x). Note that the `KmsAsyncClient` is not supported, only the synchronous client.\n\n* **To create an AWS account**, go to [Sign In or Create an AWS Account](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html) and then choose **I am a new user.** Follow the instructions to create an AWS account.\n\n* **To create a key in AWS KMS**, see [Creating Keys](https://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html).\n\n* **To download and install the AWS SDK for Java 2.x**, see [Installing the AWS SDK for Java 2.x](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/getting-started.html).\n\n* **To download and install the AWS SDK for Java 1.x**, see [Installing the AWS SDK for Java 1.x](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/getting-started.html).\n\n#### Amazon Corretto Crypto Provider\nMany users find that the Amazon Corretto Crypto Provider (ACCP) significantly improves the performance of the AWS Encryption SDK.\nFor help installing and using ACCP, see the [amazon-corretto-crypto-provider repository](https://github.com/corretto/amazon-corretto-crypto-provider).\n\n### Get Started\nTo get started with the AWS Encryption SDK for Java\n\n1. Instantiate the AWS Encryption SDK.\n2. Create a Keyring from the AWS Cryptographic Material Providers Library.\n3. Encrypt and decrypt data.\n\n```java\n// This sample code encrypts and then decrypts a string using an AWS KMS key.\n// You provide the KMS key ARN and plaintext string as arguments.\npackage com.amazonaws.crypto.examples;\n\nimport com.amazonaws.encryptionsdk.AwsCrypto;\nimport com.amazonaws.encryptionsdk.CommitmentPolicy;\nimport com.amazonaws.encryptionsdk.CryptoResult;\nimport software.amazon.cryptography.materialproviders.IKeyring;\nimport software.amazon.cryptography.materialproviders.MaterialProviders;\nimport software.amazon.cryptography.materialproviders.model.CreateAwsKmsMultiKeyringInput;\nimport software.amazon.cryptography.materialproviders.model.MaterialProvidersConfig;\n\nimport java.nio.charset.StandardCharsets;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.Map;\n\npublic class StringExample {\n    private static String keyArn;\n    private static String plaintext;\n\n    public static void main(final String[] args) {\n        keyArn = args[0];\n        plaintext = args[1];\n\n        // Instantiate the SDK\n        final AwsCrypto crypto = AwsCrypto.standard();\n        \n        // Create the AWS KMS keyring.\n        // We create a multi keyring, as this interface creates the KMS client for us automatically.\n        final MaterialProviders materialProviders = MaterialProviders.builder()\n                .MaterialProvidersConfig(MaterialProvidersConfig.builder().build())\n                .build();\n        final CreateAwsKmsMultiKeyringInput keyringInput = \n                CreateAwsKmsMultiKeyringInput.builder().generator(keyArn).build();\n        final IKeyring kmsKeyring = materialProviders.CreateAwsKmsMultiKeyring(keyringInput);\n        \n        // Set up the encryption context\n        // NOTE: Encrypted data should have associated encryption context\n        // to protect its integrity. This example uses placeholder values.\n        // For more information about the encryption context, see\n        // https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/concepts.html#encryption-context\n        final Map<String, String> encryptionContext = Collections.singletonMap(\"ExampleContextKey\", \"ExampleContextValue\");\n\n        // Encrypt the data\n        final CryptoResult<byte[], ?> encryptResult = crypto.encryptData(kmsKeyring, plaintext.getBytes(StandardCharsets.UTF_8), encryptionContext);\n        final byte[] ciphertext = encryptResult.getResult();\n        System.out.println(\"Ciphertext: \" + Arrays.toString(ciphertext));\n\n        // Decrypt the data\n        final CryptoResult<byte[], ?> decryptResult = \n                crypto.decryptData(\n                        kmsKeyring, \n                        ciphertext,\n                        // Verify that the encryption context in the result contains the\n                        // encryption context supplied to the encryptData method\n                        encryptionContext);\n\n        assert Arrays.equals(decryptResult.getResult(), plaintext.getBytes(StandardCharsets.UTF_8));\n\n        // The data is correct, so return it. \n        System.out.println(\"Decrypted: \" + new String(decryptResult.getResult(), StandardCharsets.UTF_8));\n    }\n}\n```\n\nYou can find more examples in the [example directory][examples].\n\n## Public API\n\nOur [versioning policy](./VERSIONING.rst) applies to all public and protected classes/methods/fields\nin the  `com.amazonaws.encryptionsdk` package unless otherwise documented.\n\nThe `com.amazonaws.encryptionsdk.internal` package is not included in this public API.\n\n## FAQ\n\nSee the [Frequently Asked Questions](https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/faq.html) page in the official documentation.\n\n[examples]: https://github.com/aws/aws-encryption-sdk-java/tree/master/src/examples/java/com/amazonaws/crypto/examples\n", "release_dates": ["2023-12-06T23:27:25Z", "2023-08-09T19:02:56Z", "2022-08-30T18:59:24Z", "2022-03-10T00:01:05Z", "2021-10-29T17:00:10Z", "2021-09-01T18:42:23Z", "2021-07-20T18:35:08Z", "2021-06-29T21:26:56Z", "2021-06-16T19:08:31Z", "2021-05-27T21:04:55Z", "2021-05-27T18:27:46Z", "2020-09-25T01:32:17Z", "2020-09-24T21:41:11Z", "2020-05-26T23:49:44Z", "2019-10-29T22:42:02Z", "2019-05-31T00:44:59Z", "2019-05-30T20:08:26Z", "2019-05-11T01:30:19Z", "2019-05-10T22:43:40Z", "2018-12-11T23:04:47Z", "2018-08-03T21:50:03Z", "2018-08-03T18:53:05Z", "2018-08-03T00:26:12Z", "2018-03-23T20:48:42Z", "2017-08-04T21:25:03Z", "2017-08-04T18:16:03Z", "2016-10-04T20:47:37Z", "2016-03-22T22:19:11Z"]}, {"name": "aws-encryption-sdk-javascript", "description": "AWS Encryption SDK for Javascript and Node.js", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS Encryption SDK for Javascript\n\nThe AWS Encryption SDK for Javascript provides a fully compliant,\nnative Javascript implementation of the [AWS Encryption SDK](https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html)\n\n[Security issue notifications](./CONTRIBUTING.md#security-issue-notifications)\n\nSee [Support Policy](./SUPPORT_POLICY.rst) for for details on the current support status of all major versions of this library.\n\n## Client Packages\n\n| Package | Description |\n|:--------|:------------|\n| [@aws-crypto/client-browser](https://npmjs.com/package/@aws-crypto/client-browser) | Client SDK for **Web applications** |\n| [@aws-crypto/client-node](https://npmjs.com/package/@aws-crypto/client-node) | Client SDK for Node.js client applications |\n\nThese client packages have everything you need to encrypt/decrypt.\nThey are the primary starting point.\nThe AWS Encryption SDK for Javascript is built from a group of modularized packages.\nYou can also compose the functional packages you need.\n\n### Functional Packages\n\n| Package | Description |\n|:--------|:------------|\n| [@aws-crypto/encrypt-browser](https://npmjs.com/package/@aws-crypto/encrypt-browser) | Encrypt function for **Web applications** |\n| [@aws-crypto/encrypt-node](https://npmjs.com/package/@aws-crypto/encrypt-node) | Encrypt function for Node.js client applications |\n| [@aws-crypto/decrypt-browser](https://npmjs.com/package/@aws-crypto/decrypt-browser) | Decrypt function for **Web applications** |\n| [@aws-crypto/decrypt-node](https://npmjs.com/package/@aws-crypto/decrypt-node) | Decrypt function for Node.js client applications |\n| [@aws-crypto/kms-keyring-browser](https://npmjs.com/package/@aws-crypto/kms-keyring-browser) | Kms keyring for **Web applications** |\n| [@aws-crypto/kms-keyring-node](https://npmjs.com/package/@aws-crypto/kms-keyring-node) | Kms keyring for Node.js client applications |\n| [@aws-crypto/raw-rsa-keyring-browser](https://npmjs.com/package/@aws-crypto/raw-rsa-keyring-browser) | Raw RSA keyring for **Web applications** |\n| [@aws-crypto/raw-rsa-keyring-node](https://npmjs.com/package/@aws-crypto/raw-rsa-keyring-node) | Raw RSA keyring for Node.js client applications |\n| [@aws-crypto/raw-aes-keyring-browser](https://npmjs.com/package/@aws-crypto/raw-aes-keyring-browser) | Raw AES keyring for **Web applications** |\n| [@aws-crypto/raw-aes-keyring-node](https://npmjs.com/package/@aws-crypto/raw-aes-keyring-node) | Raw AES keyring for Node.js client applications |\n| [@aws-crypto/caching-materials-manager-browser](https://npmjs.com/package/@aws-crypto/caching-materials-manager-browser) | Caching Materials Manager for **Web applications** |\n| [@aws-crypto/caching-materials-manager-node](https://npmjs.com/package/@aws-crypto/caching-materials-manager-node) | Caching Materials Manager for Node.js client applications |\n\n## Concepts\n\nThere are four main concepts that you need to understand to use this library:\n\n### Cryptographic Materials Managers\n\nCryptographic materials managers (CMMs) are resources that collect cryptographic materials\nand prepare them for use by the Encryption SDK core logic.\n\nAn example of a CMM is the default CMM,\nwhich is automatically generated anywhere a caller provides a keyring.\nThe default CMM collects encrypted data keys from it's keyrings.\n\nAn example of a more advanced CMM is the caching CMM,\nwhich caches cryptographic materials provided by another CMM.\n\n### Keyrings\n\nKeyrings use wrapping keys to generate, encrypt, and decrypt data keys.\nThe keyring that you use determines the source of the unique data keys that protect each message,\nand the wrapping keys that encrypt that data key.\nAn example of a keyring is the `KmsKeyringNode`.\n\nAn example of a more advanced keyring is the multi keyring.\nA multi keyring can be used to compose keyrings together.\n\n### Wrapping Keys\n\nWrapping keys are used to protect data keys.\nAn example of a wrapping key is a `KMS customer master key (CMK)`.\n\n### Data Keys\n\nData keys are the encryption keys that are used to encrypt your data.\nIf your algorithm suite uses a key derivation function,\nthe data key is used to generate the key that directly encrypts the data.\n\n## test\n\n```sh\nnpm test\n```\n\n## License\n\nThis SDK is distributed under the\n[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0),\nsee LICENSE.txt and NOTICE.txt for more information.\n\n# Breaking changes from `preview` to `1.0.0`\n\nThe AWS Encryption SDK for JavaScript is generally available\nas of October 1, 2019.\nThere were breaking changes during the `preview`.\n\n* Passing encryption context to `encrypt` is now `{ encryptionContext?: EncryptionContext }` [#148][encryptionContext]\n* The return value of `encrypt` is now `{result: Uint8Array, messageHeader: MessageHeader}` [#211][encryptResult]\n* `encrypt` strictly enforces `plaintextLength` [#213][plaintextLength]\n\n[encryptionContext]: https://github.com/aws/aws-encryption-sdk-javascript/pull/148\n[encryptResult]: https://github.com/aws/aws-encryption-sdk-javascript/pull/211\n[plaintextLength]: https://github.com/aws/aws-encryption-sdk-javascript/pull/213\n", "release_dates": []}, {"name": "aws-encryption-sdk-python", "description": "AWS Encryption SDK", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2022-08-30T19:59:32Z", "2022-08-30T19:42:50Z", "2022-06-21T20:44:15Z", "2022-06-21T20:50:44Z", "2022-06-21T19:18:31Z", "2021-11-11T00:17:20Z", "2021-07-01T22:42:01Z", "2021-07-01T18:49:51Z", "2021-06-16T16:37:28Z", "2021-05-27T20:25:03Z", "2021-05-27T17:56:37Z", "2021-04-20T15:40:05Z", "2020-09-25T01:30:24Z", "2020-09-25T00:19:11Z", "2020-09-24T21:41:11Z", "2019-09-20T22:17:34Z", "2019-05-28T19:13:05Z", "2018-11-15T19:15:59Z", "2018-09-20T19:08:35Z", "2017-08-09T19:38:02Z"]}, {"name": "aws-extensions-for-dotnet-cli", "description": "Extensions to the dotnet CLI to simplify the process of building and publishing .NET Core applications to AWS services", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Extensions for .NET CLI\n\nThis repository contains AWS tool extensions to the .NET CLI. These tool extensions are focused on building \n.NET Core and ASP.NET Core applications and deploying them to AWS services. Many of these deployment \ncommands are the same commands the [AWS Toolkit for Visual Studio](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.AWSToolkitforVisualStudio2017)\nuses to perform its deployment features. This allows you to do initial deployment in Visual Studio\nand then easily transition from Visual Studio to the command line and automate the deployment.\n\nFor example with the AWS Lambda .NET CLI tool extension configured you can deploy a Lambda function from the \ncommand line in the Lambda function's project root directory.\n\n```\ndotnet lambda deploy-function MyFunction\n```\n\nThe extension will prompt you for missing required parameters. To disable the extension from prompting, set the \ncommand line switch **--disable-interactive** to **true**.\n\n\nFor a history of releases view the [release change log](RELEASE.CHANGELOG.md)\n\n\n\n## Installing Extensions\n\nAs of September 10th, 2018 these extensions have migrated to be .NET Core [Global Tools](https://docs.microsoft.com/en-us/dotnet/core/tools/global-tools).\nAs part of the migration each of these tools version number was set to 3.0.0.0\n\nTo install these tools use the **dotnet tool install** command.\n```\ndotnet tool install -g Amazon.Lambda.Tools\n```\n\nTo update to the latest version of one of these tools use the **dotnet tool update** command.\n```\ndotnet tool update -g Amazon.Lambda.Tools\n```\n\n### Migrating from DotNetCliToolReference\n\nTo migrate an existing project away from the older project tool, you need to edit your project file and remove the **DotNetCliToolReference** for the tool package. For example, let's look at an existing Lambda project file.\n```xml\n<Project Sdk=\"Microsoft.NET.Sdk\">\n  <PropertyGroup>\n    <TargetFramework>netcoreapp2.1</TargetFramework>\n    <GenerateRuntimeConfigurationFiles>true</GenerateRuntimeConfigurationFiles>\n\n    <-- The new property indicating to AWS Toolkit for Visual Studio this is a Lambda project -->\n    <AWSProjectType>Lambda</AWSProjectType>\n  </PropertyGroup>\n  \n  <ItemGroup>\n    <-- This line needs to be removed -->\n    <DotNetCliToolReference Include=\"Amazon.Lambda.Tools\" Version=\"2.2.0\" />\n  </ItemGroup>\n\n  <ItemGroup>\n    <PackageReference Include=\"Amazon.Lambda.Core\" Version=\"1.0.0\" />\n    <PackageReference Include=\"Amazon.Lambda.Serialization.Json\" Version=\"1.3.0\" />\n  </ItemGroup>\n</Project>\n```\nTo migrate this project, you need to delete the **DotNetCliToolReference** element, including **Amazon.Lambda.Tools**. If you don't remove this line, the older project tool version of **Amazon.Lambda.Tools** will be used instead of an installed Global Tool.\n\nThe AWS Toolkit for Visual Studio before .NET Core 2.1 would look for the presence of **Amazon.Lambda.Tools** in the project file to determine whether to show the Lambda deployment menu item. Because we knew we were going to switch to Global Tools, and the reference to **Amazon.Lambda.Tools** in the project was going away, we added the **AWSProjectType** property to the project file. The current version of the AWS Toolkit for Visual Studio now looks for either the presence of **Amazon.Lambda.Tools** or the **AWSProjectType** set to **Lambda**. Make sure when removing the **DotNetCliToolReference** that your project file has the **AWSProjectType** property to continue deploying with the AWS Toolkit for Visual Studio.\n\n## Supported AWS Services\n\nThe following AWS services each have their own .NET CLI tool extension to make it easy to deploy a .NET Core Application\nto them.\n\n* [Amazon Elastic Container Service](#amazon-elastic-container-service-amazonecstools)\n* [AWS Elastic Beanstalk](#aws-elastic-beanstalk-amazonelasticbeanstalktools)\n* [AWS Lambda](#aws-lambda-amazonlambdatools)\n\n\n## Defaults File\n\nEach tool extension supports a defaults JSON file that is used to preset values for all of the command line switches.\nWhen a command is executed it will look for values for the command line switches in this file if they \nare not specified on the command line. The file is a JSON document where each property name matches the full \ncommand line switch excluding the -- prefix. \n\nTo avoid confusing missing properties from different tool extensions, each tool extension looks for a different named\nfile in the root of the project.\n\n\n| Tool Extension | Defaults File Name |\n| -------------- | ---------------|\n| Amazon.ECS.Tools | aws-ecs-tools-defaults.json |\n| Amazon.ElasticBeanstalk.Tools | aws-beanstalk-tools-defaults.json |\n| Amazon.Lambda.Tools | aws-lambda-tools-defaults.json |\n\nWhen deploying with the AWS Toolkit for Visual Studio, you can choose to have the deployment wizard save\nchosen values into the defaults file. This makes it easy to switch to the command line.\n\nFor example, the following **aws-ecs-tools-defaults.json** has values for the AWS region, \nAWS credential profile and build configuration. If you use it with an ECS command, you will\nnot need to enter those values.\n\n```json\n{\n    \"region\": \"us-west-2\",\n    \"profile\": \"default\",\n    \"configuration\": \"Release\"\n}\n```\n\nUse the **--config-file** switch to use an alternative file. Set the **--persist-config-file** switch \nis set to true to persist all of its settings in the defaults file.\n\n\n\n### Amazon Elastic Container Service ([Amazon.ECS.Tools](https://www.nuget.org/packages/Amazon.ECS.Tools/))\n---\n\nThis tool extension takes care of building a Docker image from a .NET application and then deploying \nthe Docker image to Amazon Elastic Container Service (**ECS**). The application must contain a **dockerfile** \ninstructing this tool and the Docker CLI which this tool uses to build the Docker image.\n\nYou must install Docker before using this extension to deploy your application.\n\n#### Install\n\nTo install the extension run the following command.\n\n```\ndotnet tool install -g Amazon.ECS.Tools\n```\n\n\n#### Available Commands\n\n\n##### Deploy Service\n\n```\ndotnet ecs deploy-service ...\n```\n\nDeploys the .NET Core application as service on an ECS cluster. Services are for long lived process like\nweb applications. Services have a desired number of tasks that will run the application. If a task instance \ndies for whatever reason the service will spawn a new task instance. Services can also be associated with\nan Elastic Load Balancer so that each of the tasks in the services will be registered as targets for the load balancer.\n\n##### Deploy Task\n\n```\ndotnet ecs deploy-task\n```\n\nDeploys the .NET Core application as task on an ECS Cluster. This is good for batch processing and similar jobs\nwhere once the process identified in the dockerfile exits the ECS task should end.\n\n##### Deploy Scheduled Task\n\n```\ndotnet ecs deploy-scheduled-task\n```\n\nCreates a new ECS task definition and then configures a Amazon CloudWatch Event rule to run a task using\nthe new task definition and a scheduled interval.\n\n\n##### Push Image\n\n```\ndotnet ecs push-image\n```\n\nBuilds the Docker image from the .NET Core application and pushes it to Amazon Elastic Container Registery (ECR).\nThe other ECS deployment tasks first run this command before continuing on with deployment.\n\n### AWS Elastic Beanstalk ([Amazon.ElasticBeanstalk.Tools](https://www.nuget.org/packages/Amazon.ElasticBeanstalk.Tools/))\n---\n\nThis tool extension deploys ASP.NET Core applications to AWS Elastic Beanstalk environment.\n\n#### Install\n\nTo install the extension run the following command.\n\n```\ndotnet tool install -g Amazon.ElasticBeanstalk.Tools\n```\n\n\n\n#### Available Commands\n\n\n##### Deploy Environment\n```\ndotnet eb deploy-environment\n```\n\nDeploys the ASP.NET Core application to a Elastic Beanstalk environment after building and packaging up the application.\nIf the Elastic Beanstalk environment does not exist then the command will create the environment.\n\n##### Delete Environment\n```\ndotnet eb delete-environment\n```\n\nDeletes an environment.\n\n##### List Environments\n```\ndotnet eb list-environments\n```\n\nLists all of the current running environments along with the URL to access the environment.\n\n### AWS Lambda ([Amazon.Lambda.Tools](https://www.nuget.org/packages/Amazon.Lambda.Tools/))\n---\n\nThis tool extension deploys AWS Lambda .NET Core functions. \n\n#### Install\n\nTo install the extension run the following command.\n\n```\ndotnet tool install -g Amazon.Lambda.Tools\n```\n\n\n#### Available Commands\n\n##### Deploy Function\n```\ndotnet lambda deploy-function\n```\n\nDeploys the .NET Core Lambda project directly to the AWS Lambda service. The function is created if\nthis is the first deployment. If the Lambda function already exists then the function code is updated.\nIf any of the function configuration properties specified on the command line are different, the existing \nfunction configuration is updated. To avoid accidental function configuration changes during a redeployment, \nonly default values explicitly set on the command line are used. The defaults file is not used.\n\n##### Invoke Function\n```\ndotnet lambda invoke-function MyFunction --payload \"The Function Payload\"\n```\n\nInvokes the Lambda function in AWS Lambda passing in the value of **--payload** as the input parameter to the Lambda function.\n\n##### List Functions\n```\ndotnet lambda list-functions\n```\n\nList all of the currently deployed Lambda functions.\n\n##### Delete Function\n```\ndotnet lambda delete-function\n```\n\nDelete a Lambda function\n\n##### Get Function Configuration\n```\ndotnet lambda get-function-config\n```\n\nGet the Lambda function's configuration like memory limit and timeout.\n\n##### Update Function Configuration\n```\ndotnet lambda update-function-config\n```\n\nUpdate the Lambda function's configuration without uploading new code.\n##### Deploy Serverless\n```\ndotnet lambda deploy-serverless\n```\n\nDeploys one or more Lambda functions from the Lambda project through CloudFormation. The project uses the \n**serverless.template** CloudFormation template to deploy the serverless app along with any additional \nAWS resources defined in the **serverless.template**. \n\nCloudFormation stacks created with this command are tagged with the **AWSServerlessAppNETCore** tag.\n\n##### List Serverless\n```\ndotnet lambda list-serverless\n```\n\nLists the .NET Core Serverless applications which are identified by looking for the **AWSServerlessAppNETCore** tag \non existing CloudFormation Stacks.\n\n##### Delete Serverless\n```\ndotnet lambda delete-serverless\n```\n\nDeletes the serverless application by deleting the CloudFormation stack.\n\n##### Package CI\n```\ndotnet lambda package-ci\n```\n\nUsed for serverless applications. It creates the Lambda application bundle and uploads it to Amazon S3. It then writes \na new version of the serverless.template with the location of the Lambda function code updated to \nwhere the application bundle was uploaded. In an AWS CodePipeline this command can be executed as part of a **CodeBuild** \nstage returning the transformed template as the build artifact. Later in the pipeline that transformed serverless.template can\nbe used with a CloudFormation stage to deploy the application.\n\n##### Notes\n\n`dotnet lambda package-ci` inspects and uses the [`Architectures` property of `AWS::Serverless::Function`](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-resource-function.html#sam-function-architectures) to determine the runtime of the package.\n\nFor example:\n\n```\n      \"Architectures\": [\n        \"arm64\"\n      ],\n```\n\nwill execute `dotnet publish` with a `--runtime` argument of value `linux-arm64`:\n\nThe default is `linux-x64` (which will execute `dotnet publish` with a `--runtime` argument of value `linux-x64`: )\n\nThe `Architectures` array can be specified either by:\n1. Directly with the path `AWS::Serverless::Function` \n2. Within the [AWS SAM Template syntax `Globals`](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy-globals.html)\n\nExample of directly with the path `AWS::Serverless::Function` in a `serverless.template`:\n\n```\n  ...\n  \"Resources\": {\n    \"ApiFnMFS3hGenerated\": {\n      \"Type\": \"AWS::Serverless::Function\",\n      \"Properties\": {\n        \"Architectures\": [\n          \"arm64\"\n        ],\n        \"CodeUri\": \".\",\n        \"MemorySize\": 10240,\n        \"Timeout\": 900,\n        \"Role\": {\n          \"Ref\": \"LambdaExecutionRole\"\n        },\n        \"PackageType\": \"Zip\",\n        \"Handler\": \"MyAssembly.MyNamespace::MyAssembly.MyNamespace.MyClass::MyFunction\"\n      }\n    }\n  },\n  \"Parameters\" ...\n```\n\nExample of within the AWS SAM Template syntax `Globals` in a `serverless.template`:\n\n```\n...\n  \"Globals\": {\n    \"Function\": {\n      \"Runtime\": \"dotnet6\",\n      \"Architectures\": [\n        \"arm64\"\n      ]\n    }\n  },\n  \"Resources\": {\n    \"ApiFnMFS3hGenerated\": {\n      \"Type\": \"AWS::Serverless::Function\",\n      \"Properties\": {\n        \"Architectures\": [\n          \"arm64\"\n        ],\n        \"CodeUri\": \".\",\n        \"MemorySize\": 10240,\n        \"Timeout\": 900,\n        \"Role\": {\n          \"Ref\": \"LambdaExecutionRole\"\n        },\n        \"PackageType\": \"Zip\",\n        \"Handler\": \"MyAssembly.MyNamespace::MyAssembly.MyNamespace.MyClass::MyFunction\"\n      }\n    }\n  },\n  \"Parameters\" ...\n```\n\n##### Package\n```\ndotnet lambda package\n```\n\nCreates the Lambda application bundle that can later be deployed to Lambda.\n", "release_dates": []}, {"name": "aws-for-fluent-bit", "description": "The source of the amazon/aws-for-fluent-bit container image", "language": "Shell", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS for Fluent Bit Docker Image\n\n### Contents\n\n- [AWS Distro versioning scheme FAQ](#aws-distro-versioning-scheme-faq)\n- [Compliance and Patching](#compliance-and-patching)\n- [Consuming AWS for Fluent Bit versions](#consuming-aws-for-fluent-bit-versions)\n    - [AWS Distro for Fluent Bit Release Tags](#aws-distro-for-fluent-bit-release-tags)\n    - [AWS Distro for Fluent Bit release testing](#aws-distro-for-fluent-bit-release-testing)\n    - [Latest stable version](#latest-stable-version)\n    - [CVE scans and latest stable](#cve-scans-and-latest-stable)\n    - [Guidance on consuming versions](#guidance-on-consuming-versions)\n- [Debugging Guide](troubleshooting/debugging.md)\n- [Use Case Guide](use_cases/)\n- [Public Images](#public-images)\n    - [arm64 and x86 images](#arm64-and-amd64-images)\n    - [Using the init tag](#using-the-init-tag)\n    - [Using SSM to find available versions and aws regions](#using-ssm-to-find-available-versions-and-aws-regions)\n    - [Using SSM Parameters in CloudFormation Templates](#using-ssm-parameters-in-cloudFormation-templates)\n    - [Using image tags](#using-image-tags)\n        - [Amazon ECR Public Gallery](#amazon-ecr-public-gallery)\n        - [Docker Hub](#docker-hub)\n        - [Amazon ECR](#amazon-ecr)\n    - [Using the debug images](#Using-the-debug-images)\n- [Plugins](#plugins)\n- [Using the AWS Plugins outside of a container](#using-the-aws-plugins-outside-of-a-container)\n- [Running aws-for-fluent-bit Windows containers](#running-aws-for-fluent-bit-windows-containers)\n- [Development](#development)\n    - [Local testing](#local-testing)\n    - [Developing Features in the AWS Plugins](#developing-features-in-the-aws-plugins)\n- [Fluent Bit Examples](#fluent-bit-examples)\n- [License](#license)\n\n\n### AWS Distro versioning scheme FAQ\n\nThe version of the AWS for Fluent Bit image is not linked to the version of Fluent Bit which it contains.\n\n**What does the version number signify?**\n\nWe use the standard `major.minor.patch` versioning scheme for our image, AKA Semantic Versioning. The initial release with this versioning scheme is `2.0.0`. Bug fixes are released in patch version bumps. New features are released in new minor versions. We strive to only release backwards incompatible changes in new major versions.\n\nPlease read the below on CVE patches in base images and dependencies. The semantic version number applies to the Fluent Bit code and [AWS Go plugin](https://github.com/aws/aws-for-fluent-bit/blob/mainline/troubleshooting/debugging.md#aws-go-plugins-vs-aws-core-c-plugins) code compiled and installed in the image.\n\n**Image Versions and CVE Patches**\n\nThe AWS for Fluent Bit image includes the following contents:\n* A base image (currently Amazon Linux or Windows Server Core 2019 or Windows Server Core 2022)\n* Runtime dependencies installed on top of the base image\n* Fluent Bit binary\n* Several Fluent Bit [Go Plugin binaries](https://github.com/aws/aws-for-fluent-bit/blob/mainline/troubleshooting/debugging.md#aws-go-plugins-vs-aws-core-c-plugins)\n\nThe process for pushing out new builds with CVE patches in the base image or installed dependencies is different for Windows vs Linux. \n\nFor Windows, every month after the [B release date/\"patch tuesday\"](https://learn.microsoft.com/en-us/windows/deployment/update/release-cycle#monthly-security-update-release), we re-build and update all Windows images currently found in the [windows.versions](windows.versions) file in this repo with the newest base images from Microsoft. The Fluent Bit and go plugin binaries are copied into the newly released base windows image. Thus, the windows image tags are not immutable images; only the Fluent Bit and Go plugin binaries are immutable over time.\n\nAt any point in time, [windows.versions](windows.versions) file will contain at least 5 versions, including latest and latest stable. AWS for Fluent Bit Windows are guaranteed to be patched for 4 months after their release date. Therefore, the [windows.versions](windows.versions) file always contains all versions released in the last 4 months, and may contain more if the latest stable release is older than 4 months. \n\nFor Linux, each image tag is immutable. When there is a report of high or critical CVEs reported in the base amazon linux image or installed linux packages, we will work to push out a new image [per our patching policy](#compliance-and-patching). However, we will not increment the semantic version number to simply re-build to pull in new linux dependencies. Instead, we will add a 4th version number signifying the date the image was built.\n\nFor example, a series of releases in time might look like:\n\n1. `2.31.12`: New Patch release with changes in Fluent Bit code compared to `2.31.11`. This release will have standard release notes and will have images for both linux and windows. \n2. `2.31.12-20230629`: Re-build of `2.31.12` just for Linux CVEs found in the base image or installed dependencies. The Fluent Bit code contents are the same as `2.31.12`. There only be linux images with this version tag, and no windows images. The `latest` tag for linux will be updated to point to this new image. There will be short release notes that call out it is simply a re-build for linux. \n3. `2.31.12-20230711`: Another re-build of `2.31.12` for Linux CVEs on a subsequent date. This release is special as explained above in the way same as `2.31.12-20230629`.\n4. `2.31.13`: New Patch release with changes in Fluent Bit code compared to `2.31.12`. This might be for bugs found in the Fluent Bit code. It could also be for a CVE found in the Fluent Bit code. This release has standard release notes and linux and windows images. \n\n\n**Why do some image tags contain 4 version numbers?**\n\nPlease see the above explanation on our Linux image re-build process for CVEs found in dependencies. \n\n**Are there edge cases to the rules on breaking backwards compatibility?**\n\nOne edge case for the above semantic versioning rules is changes to configuration validation. Between Fluent Bit upstream versions 1.8 and 1.9, validation of config options was fixed/improved. Previous to this distro's upgrade to Fluent Bit upstream 1.9, configurations that included certain invalid options would run without error (the invalid options were ignored). After we released Fluent Bit usptream 1.9 support, these invalid options were validated and Fluent Bit would exit with an error. See the [issue discussion here](https://github.com/aws/aws-for-fluent-bit/issues/371#issuecomment-1160663682). \n\nAnother edge case to the above rules are bug fixes that require removing a change. We have and will continue to occasionally remove new changes in a patch version if they were found to be buggy. We do this to unblock customers who do not depend on the recent change. Please always check our release notes for the changes in a specific version. A past example of a patch release that removed something is [2.31.4](https://github.com/aws/aws-for-fluent-bit/releases/tag/v2.31.4). A prior release had fixed how S3 handles the timestamps in S3 keys and the `Retry_Limit` configuration option. Those changes were considered to be bug fixes. However, they introduced instability so we subsequently removed them in a patch. \n\n\n**What about the 1.x image tags in your repositories?**\n\nThe AWS for Fluent Bit image was launched in July 2019. Between July and October of 2019 we simply versioned the image based on the version of Fluent Bit that it contained. During this time we released `1.2.0`, `1.2.2` and `1.3.2`.\n\nThe old versioning scheme was simple and it made it clear which version of Fluent Bit our image contained. However, it had a serious problem- how could we signify that we had changed the other parts of the image? If we did not update Fluent Bit, but updated one of the plugins, how would we signify this in a new release? There was no answer- we could only release an update when Fluent Bit released a new version. We ultimately realized this was unacceptable- bug fixes or new features in our plugins should not be tied to the Fluent Bit release cadence.\n\nThus, we moved to the a new versioning scheme. Because customers already are relying on the `1.x` tags, we have left them in our repositories. The first version with the new scheme is `2.0.0`. From now on we will follow semantic versioning- but the move from `1.3.2` did not follow semantic versioning. There are no backwards incompatible changes between `aws-for-fluent-bit:1.3.2` and `aws-for-fluent-bit:2.0.0`. Our release notes for `2.0.0` clearly explain the change.\n\n**Does this mean you are diverging from fluent/fluent-bit?**\n\nNo. We continue to consume Fluent Bit from its main repository. We are not forking Fluent Bit.\n\n### Compliance and Patching\n\n**Q: Is AWS for Fluent Bit HIPAA Compliant?**\n\nFluent Bit can be used in a HIPAA compliant matter to send logs to AWS, even if the logs contain PHI. Please see the call outs in the [AWS HIPAA white paper for ECS](https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/amazon-ecs.html).  \n\n**Q: What is the policy for patching AWS for Fluent Bit for vulnerabilities, CVEs and image scan findings?**\n\nAWS for Fluent Bit uses ECR image scanning in its release pipeline and any scan that finds high or critical vulnerabilities will block a release: [scripts/publish.sh](https://github.com/aws/aws-for-fluent-bit/blob/mainline/scripts/publish.sh#L487)\n\nIf you find an issue from a scan on our latest images please follow the reporting guidelines below and we will work quickly to introduce a new release. To be clear, we do not patch existing images, we just will release a new image without the issue. The team uses [Amazon ECR Basic image scanning](https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-basic.html) and [Amazon ECR Enhanced scanning powered by AWS Inspector](https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html) as the primary source of truth for whether or not the image contains a vulnerability in a dependency. \n\nIf your concern is about a vulnerability in the Fluent Bit upstream ([github.com/fluent/fluent-bit](https://github.com/fluent/fluent-bit/) open source code), please let us know as well. However, fixing upstream issues requires additional work and time because we must work closely with upstream maintainers to commit a fix and cut an upstream release, and then we can cut an AWS for Fluent Bit release. \n\n**Q: How do I report security disclosures?**\n\nIf you think you\u2019ve found a potentially sensitive security issue, please do not post it in the Issues on GitHub.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or email AWS security directly at [aws-security@amazon.com](mailto:aws-security@amazon.com).\n\n### Consuming AWS for Fluent Bit versions\n\n#### AWS Distro for Fluent Bit Release Tags\n\nOur image repos contain the following types of tags, which are explained in the sections below:\n\n* `latest`: The most recently released image version. We do not recommend deploying this to production environments ever, see [Guidance on consuming versions](#guidance-on-consuming-versions).\n* `Version number tag`: Each release has a version number, for example `2.28.4`. These are the only tags we recommend consuming in production environments: [Guidance on consuming versions](#guidance-on-consuming-versions).\n* `stable`: Some time after a version is released, it may be designated as the latest stable. See [Latest stable version](#latest-stable-version) and  [Guidance on consuming versions](#guidance-on-consuming-versions).\n\n#### AWS Distro for Fluent Bit release testing\n\n**Types of tests we run**\n\n* [Simple integration tests](https://github.com/aws/aws-for-fluent-bit/tree/mainline/integ): Short running tests of the AWS output plugins that send log records and verify that all of them were received correctly formatted at the destination.\n* [Load Tests:](https://github.com/aws/aws-for-fluent-bit/tree/mainline/load_tests) Test Fluent Bit AWS output plugins at various throughputs and check for log loss, the results are posted in our release notes: https://github.com/aws/aws-for-fluent-bit/releases\n* Long running stability tests: Highly parallel tests run in Amazon ECS for the AWS output plugins using the [aws/firelens-datajet](https://github.com/aws/firelens-datajet) project. These tests simulate real Fluent Bit deployments and use cases to test for bugs that crashes. \n\n\n**Latest release testing bar**\n\n* [Simple integration tests](https://github.com/aws/aws-for-fluent-bit/tree/mainline/integ): Must fully pass with all log events received properly formatted at the destination. \n* [Load Tests:](https://github.com/aws/aws-for-fluent-bit/tree/mainline/load_tests) Must pass the [thresholds here](https://github.com/aws/aws-for-fluent-bit/blob/mainline/load_tests/validation_bar.py). Results are posted in our release notes: https://github.com/aws/aws-for-fluent-bit/releases\n* Long running stability tests: No crashes observed for at least 1 day. \n\n\n**CVE Patch release testing bar**\n\n* [Simple integration tests](https://github.com/aws/aws-for-fluent-bit/tree/mainline/integ): Must fully pass with all log events received properly formatted at the destination. \n* [Load Tests:](https://github.com/aws/aws-for-fluent-bit/tree/mainline/load_tests) Must pass the [thresholds here](https://github.com/aws/aws-for-fluent-bit/blob/mainline/load_tests/validation_bar.py). Results are posted in our release notes: https://github.com/aws/aws-for-fluent-bit/releases\n\nWe do not run our long running stability tests for CVE patches. This is because the goal is to get the CVE patch out as quickly as possible, and because CVE patch releases never include Fluent Bit code changes. CVE patch releases only include base image dependency upgrades. *If there is ever a CVE in the Fluent Bit code base itself, the patch for it would be considered a bug fix that might introduce instability and it would undergo the normal latest release testing.* \n\n**Latest stable release testing bar**\n\nFor a version to be made the latest `stable`, it must already have been previously released as the latest release. Thus it will have already passed the testing bar noted above for `latest`. \n\nIn addition, our stable release undergoes additional testing:\n\n* Long running stability tests: The version undergoes and passes these tests for at least 2 weeks. After the version is promoted to stable we continue to run the long running stability tests, and may roll back the stable designation if issues later surface.  \n\n#### Latest stable version\n\nOur latest stable version is the most recent version that we have high confidence is stable for AWS use cases. *We recommend using the stable version number in your prod deployments; see* [Guidance on consuming versions](#guidance-on-consuming-versions)\n\nThe latest stable version is marked with the tag `stable`/`windowsservercore-stable`. The version number that is currently designated as the latest stable can always be found in the [AWS_FOR_FLUENT_BIT_STABLE_VERSION](https://github.com/aws/aws-for-fluent-bit/blob/mainline/AWS_FOR_FLUENT_BIT_STABLE_VERSION) file in the root of this repo. \n\n*There is no guarantee that `stable` has no issues- stable simply has a higher testing bar than our latest releases. The `stable` tag can be downgraded and rolled back to the previous stable if new test results or customer bug reports surface issues. This has occurred* [*in the past*](https://github.com/aws/aws-for-fluent-bit/issues/542)*.  *Consequently, we recommend locking to a specific version tag and informing your choice of version using our current stable designation.*\n\n\nPrior to being designated as the latest stable, a version must pass the following criteria:\n\n* It has been out for at least 2 weeks or is a CVE patch with no Fluent Bit changes. Stable designation is based on the Fluent Bit code in the image. A version released for CVE patches can be made stable if the underlying if the underlying Fluent Bit code is already designated as stable.\n* No bugs have been reported in Fluent Bit which we expect will have high impact for AWS customers. This means bugs in the components that are most frequently used by AWS customers, such as the AWS outputs or the tail input.\n* The version has passed our long running stability tests for at least 2 weeks or is a CVE patch with no Fluent Bit changes that has passed our long running stability tests for at least 1 day. The version would have already passed our simple integration and load tests when it was first released as the latest image. \n\n#### CVE scans and latest stable\n\n[Please read our CVE patching policy.](https://github.com/aws/aws-for-fluent-bit#compliance-and-patching) \n\nThe stable designation is for the Fluent Bit code contents of the image, not CVE scan results for dependencies installed in the image. We will upgrade a CVE patch to be the latest stable if it contains no Fluent Bit code changes compared to the previous latest stable. \n\n\n#### Guidance on consuming versions\n\nOur [release notes](https://github.com/aws/aws-for-fluent-bit/releases) call out the key AWS changes in each new version. \n\n*We recommend that you only consume non-stable releases in your test/pre-prod stages. Consuming the `latest` tag directly is widely considered to be an anti-pattern in the software industry.* \n\n*We strongly recommend that you always lock deployments to a specific immutable version tag, rather than using our `stable` or `latest` tags.* Using the `stable` or `latest` tag directly in prod has the following downsides:\n\n1. *Difficulty in determining which version was deployed*: If you experience an issue, you will need to [check the Fluent Bit log output to determine which specific version tag](https://github.com/aws/aws-for-fluent-bit/blob/mainline/troubleshooting/debugging.md#what-version-did-i-deploy) was deployed. This is because the `stable` and `latest` tags are mutable and change over time. \n2. *Mixed deployments*: If you are in the middle of a deployment when we release an update to the `stable` or `latest` immutable tags, some of your deployment may have deployed the previous version, and the rest will deploy the new version. \n\n\n*The best practice for consuming AWS for Fluent Bit is to check the [AWS_FOR_FLUENT_BIT_STABLE_VERSION](https://github.com/aws/aws-for-fluent-bit/blob/mainline/AWS_FOR_FLUENT_BIT_STABLE_VERSION) file and lock your prod deployments to that specific version tag.* For example, if the current stable is `2.28.4`, your deployment should use `public.ecr.aws/aws-observability/aws-for-fluent-bit:2.28.4` not `public.ecr.aws/aws-observability/aws-for-fluent-bit:stable`.\n\n\n### Debugging Guide\n\n[Please read the debugging.md](troubleshooting/debugging.md)\n\n### Use Case Guide\n\n[A set of tutorials on use cases that Fluent Bit can solve](use_cases/).\n\n### Public Images\n\n#### Linux Images\nEach release updates the `latest` tag and adds a tag for the version of the image. The `stable` tag is also available which marks a release as the latest stable version.\n\n#### Windows Images\nFor Windows images, we update the `windowsservercore-latest` tag and add a tag as `<VERSION>-windowsservercore`. The stable tag is available as `windowsservercore-stable`. We update all the supported versions each month when [Microsoft releases the latest security\npatches for Windows](https://support.microsoft.com/en-gb/topic/windows-server-container-update-history-23c939c5-3ca5-3a16-27b8-d18e00d2408a).  \n\n\n**Note:** Deploying `latest`/`windowsservercore-latest` to prod without going through a test stage first is not recommended.\n#### arm64 and amd64 images\n\nAWS for Fluent Bit currently distributes container images for arm64 and amd64 CPU architectures. Our images all use [mutli-archictecture tags](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-multi-architecture-image.html). For example, this means that if you pull the `latest` tag on a Graviton instance, you would get the arm64 image build. \n\nFor Windows, [we release images](#windows-images) only for amd64 CPU architecture of the following Windows releases-\n- Windows Server 2019\n- Windows Server 2022\n\n#### Using the init tag\n\nThe `init` tags indicate that an image contains init process and supports multi-config. Init tag is used in addition to our other tags, e.g. `aws-for-fluent-bit:init-latest` means this is a latest released image supports multi-config. For more information about the usage of multi-config please see our [use case guide](https://github.com/aws/aws-for-fluent-bit/blob/mainline/use_cases/init-process-for-fluent-bit/README.md) and [FireLens example](https://github.com/aws-samples/amazon-ecs-firelens-examples/tree/mainline/examples/fluent-bit/multi-config-support).\n\nNote: Windows images with init tag are not available at the moment.\n\n#### Using SSM to find available versions and aws regions\n\nAs of 2.0.0, there are SSM Public Parameters which allow you to see available versions. These parameters are available in every region that the image is available in. Any AWS account can query these parameters.\n\nTo see a list of available version tags, run the following command:\n\n```\naws ssm get-parameters-by-path --path /aws/service/aws-for-fluent-bit/ --query 'Parameters[*].Name'\n```\n\nExample output:\n\n```\n[\n    \"/aws/service/aws-for-fluent-bit/latest\"\n    \"/aws/service/aws-for-fluent-bit/windowsservercore-latest\"\n    \"/aws/service/aws-for-fluent-bit/2.0.0\"\n    \"/aws/service/aws-for-fluent-bit/2.0.0-windowsservercore\"\n]\n```\n\nIf there is no output, it means the aws for fluent bit image is not available in current region.\n\nTo see the ECR repository ID for a given image tag, run the following:\n\n```\n$ aws ssm get-parameter --name /aws/service/aws-for-fluent-bit/2.0.0\n{\n    \"Parameter\": {\n        \"Name\": \"/aws/service/aws-for-fluent-bit/2.0.0\",\n        \"Type\": \"String\",\n        \"Value\": \"906394416424.dkr.ecr.us-east-1.amazonaws.com/aws-for-fluent-bit:2.0.0\",\n        \"Version\": 1,\n        \"LastModifiedDate\": 1539908129.759,\n        \"ARN\": \"arn:aws:ssm:us-west-2::parameter/aws/service/aws-for-fluent-bit/2.0.0\"\n    }\n}\n```\n\n#### Using SSM Parameters in CloudFormation Templates\n\nYou can use these SSM Parameters as parameters in your CloudFormation templates.\n\n```\nParameters:\n  FireLensImage:\n    Description: Fluent Bit image for the FireLens Container\n    Type: AWS::SSM::Parameter::Value<String>\n    Default: /aws/service/aws-for-fluent-bit/latest\n```\n\n#### Using image tags\n\nYou should lock your deployments to a specific version tag. We guarantee that these tags will be immutable- once they are released the will not change. \nWindows images will be updated each month to include the latest security patches in the base layers but the contents of the image will not change in a tag. \n\n\n##### Amazon ECR Public Gallery\n\n[aws-for-fluent-bit](https://gallery.ecr.aws/aws-observability/aws-for-fluent-bit)\n\nOur images are available in Amazon ECR Public Gallery. We recommend our customers to download images from this public repo. You can get images with different tags by following command:\n\n```\ndocker pull public.ecr.aws/aws-observability/aws-for-fluent-bit:<tag>\n```\n\nFor example, you can pull the image with latest version by:\n\n```\ndocker pull public.ecr.aws/aws-observability/aws-for-fluent-bit:latest\n```\n\nIf you see errors for image pull limits, or get the following error:\n\n```\nError response from daemon: pull access denied for public.ecr.aws/amazonlinux/amazonlinux, repository does not exist or may require 'docker login': denied: Your authorization token has expired. Reauthenticate and try again.\n```\n\nThen try log into public ECR with your AWS credentials:\n\n```\naws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws\n```\n\nYou can check the [Amazon ECR Public official doc](https://docs.aws.amazon.com/AmazonECR/latest/public/get-set-up-for-amazon-ecr.html) for more details.\n\n##### Docker Hub\n\n[amazon/aws-for-fluent-bit](https://hub.docker.com/r/amazon/aws-for-fluent-bit/tags)\n\n##### Amazon ECR\n\nWe also provide images in Amazon ECR for high availability. These images are available in almost every AWS region, included AWS Gov Cloud.\n\nThe official way to find the ECR image URIs for your region is to use the SSM Parameters. In your region, run the following command:\n\n```\naws ssm get-parameters-by-path --path /aws/service/aws-for-fluent-bit/\n```\n\n#### Using the debug images\n\nDeploying AWS for Fluent Bit debug images can help the AWS team troubleshoot an issue. If you experience a bug, especially a [crash/SIGSEGV issue](https://github.com/aws/aws-for-fluent-bit/blob/mainline/troubleshooting/debugging.md#caught-signal-sigsegv), then please consider deploying the debug version of the image. After a crash, the debug image can print out a stacktrace and upload a core dump to S3. See our [debugging guide](https://github.com/aws/aws-for-fluent-bit/blob/mainline/troubleshooting/debugging.md#1-build-and-distribute-a-core-dump-s3-uploader-image) for more info on using debug images.\n\nFor debug images, we update the `debug-latest` tag and add a tag as `debug-<Version>`.\n\n### Plugins\n\nWe currently bundle the following projects in this image:\n* [amazon-kinesis-firehose-for-fluent-bit](https://github.com/aws/amazon-kinesis-firehose-for-fluent-bit)\n* [amazon-cloudwatch-logs-for-fluent-bit](https://github.com/aws/amazon-cloudwatch-logs-for-fluent-bit)\n* [amazon-kinesis-streams-for-fluent-bit](https://github.com/aws/amazon-kinesis-streams-for-fluent-bit)\n\n### Using the AWS Plugins outside of a container\n\nYou can use the AWS Fluent Bit plugins with [td-agent-bit](https://docs.fluentbit.io/manual/installation/supported-platforms).\n\nWe provide a [tutorial](examples/fluent-bit/systems-manager-ec2/) on using SSM to configure instances with td-agent-bit and the plugins.\n\n### Running `aws-for-fluent-bit` Windows containers\nYou can run `aws-for-fluent-bit` Windows containers using the image tags as specified under [Windows Images section](#windows-images). These are distributed as multi-arch images with the manifests for the supported Windows releases as specified above.\n\nFor more details about running Fluent Bit Windows containers in Amazon EKS, please visit our [blog post](https://aws.amazon.com/blogs/containers/centralized-logging-for-windows-containers-on-amazon-eks-using-fluent-bit/).\n\nFor more details about running Fluent Bit Windows containers in Amazon ECS, please visit our [blog post](https://aws.amazon.com/blogs/containers/centralized-logging-for-windows-containers-on-amazon-ecs-using-fluent-bit/). For running Fluent Bit as a Amazon ECS Service using `daemon` scheduling strategy, please visit our Amazon ECS [tutorial](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/tutorial-deploy-fluentbit-on-windows.html). For more details about using the AWS provided default configurations for Amazon ECS, please visit our [documentation](ecs_windows_forward_daemon/README.md).\n\n**Note**: There is a known issue with networking failure when running Fluent Bit in Windows containers on `default` container network. Check out the guidance in our debugging guide for a [workaround to this issue](troubleshooting/debugging.md#networking-issue-with-windows-containers-when-using-async-dns-resolution-by-plugins).\n\n### Development\n\n#### Local testing\n\nUse `make release` to build the image.\n\nTo run the integration tests, run `make integ-dev`. The `make integ-dev` command will run the integration tests for all of our plugins-\nkinesis streams, kinesis firehose, and cloudwatch.\n\nThe integ tests require the following env vars to be set:\n* `CW_INTEG_VALIDATOR_IMAGE`: Build the [integ/validate_cloudwatch/](integ/validate_cloudwatch/) folder with `docker build` and set the resulting image as the value of this env var.\n* `S3_INTEG_VALIDATOR_IMAGE`: Build the [integ/s3/](integ/s3/) folder with `docker build` and set the resulting image as the value of this env var.\n\nTo run integration tests separately, execute `make integ-cloudwatch` or `make integ-kinesis` or `make integ-firehose`.\n\n[Documentation on GitHub steps for releases](Release_Process.md).\n\n#### Developing Features in the AWS Plugins\n\nYou can build a version of the image with code in your GitHub fork. To do so, you must need to set the following environment variables.\nOtherwise, you will see an error message like the following one:\n`fatal: repository '/kinesis-streams' or '/kinesis-firehose' or '/cloudwatch' does not exist.`\n\nSet the following environment variables for CloudWatch:\n\n```\nexport CLOUDWATCH_PLUGIN_CLONE_URL=\"Your GitHub fork clone URL\"\nexport CLOUDWATCH_PLUGIN_BRANCH=\"Your branch on your fork\"\n```\n\nOr for Kinesis Streams:\n```\nexport KINESIS_PLUGIN_CLONE_URL=\"Your GitHub fork clone URL\"\nexport KINESIS_PLUGIN_BRANCH=\"Your branch on your fork\"\n```\n\nOr for Kinesis Firehose:\n```\nexport FIREHOSE_PLUGIN_CLONE_URL=\"Your GitHub fork clone URL\"\nexport FIREHOSE_PLUGIN_BRANCH=\"Your branch on your fork\"\n```\n\nThen run `make cloudwatch-dev` or `make kinesis-dev` or `make firehose-dev` to build the image with your changes.\n\nTo run the integration tests on your code, execute `make integ-cloudwatch-dev` or `make integ-kinesis-dev` or `make integ-firehose-dev`.\n\n## Fluent Bit Examples\nCheck out Fluent Bit examples from our [amazon-ecs-firelens-examples](https://github.com/aws-samples/amazon-ecs-firelens-examples#fluent-bit-examples) repo.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2024-01-25T23:47:47Z", "2023-12-06T18:41:39Z", "2023-11-13T19:48:49Z", "2023-10-20T18:21:11Z", "2023-10-13T02:11:52Z", "2023-10-04T20:00:23Z", "2023-09-12T18:53:44Z", "2023-08-22T11:04:59Z", "2023-07-28T05:50:00Z", "2023-06-30T02:21:06Z", "2023-06-19T19:54:40Z", "2023-05-16T17:38:35Z", "2023-04-26T18:35:44Z", "2023-04-18T19:50:42Z", "2023-04-12T23:33:01Z", "2023-03-31T02:41:07Z", "2023-03-15T05:30:48Z", "2023-03-08T03:17:08Z", "2023-03-03T18:41:12Z", "2023-03-01T19:38:48Z", "2023-02-24T18:14:20Z", "2023-02-16T19:51:06Z", "2023-02-11T00:55:15Z", "2023-02-03T01:24:52Z", "2023-01-19T02:05:40Z", "2023-01-13T00:31:21Z", "2022-12-07T01:52:57Z", "2022-11-12T20:23:33Z", "2022-10-12T19:51:02Z", "2022-10-06T21:17:32Z"]}, {"name": "aws-fpga", "description": "Official repository of the AWS EC2 FPGA Hardware and Software Development Kit", "language": "VHDL", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Table of Contents\n\n1. [Overview of AWS EC2 FPGA Development Kit](#overview-of-aws-ec2-fpga-development-kit)\n    - [Developer Support](#developer-support)\n    - [Development Flow](#development-flow)\n    - [Development environments](#development-environments)\n    - [FPGA Developer AMI](#fpga-developer-ami)\n    - [FPGA Hardware Development Kit (HDK)](#hardware-development-kit-hdk)\n    - [FPGA Software Development Kit (SDK)](#runtime-tools-sdk)\n    - [Software Defined Development Environment](#software-defined-development-environment)\n1. [Amazon EC2 F1 platform features](#amazon-ec2-f1-platform-features)\n1. [Getting Started](#getting-started)\n    - [Getting Familiar with AWS](#getting-familiar-with-aws)\n    - [First time setup](#setting-up-development-environment-for-the-first-time)\n    - [Quickstarts](#quickstarts)\n    - [How To's](#how-tos)\n1. [Documentation Overview](#documentation-overview)\n\n# Overview of AWS EC2 FPGA Development Kit\n\nAWS EC2 FPGA Development Kit is a set of development and runtime tools to develop, simulate, debug, compile and run hardware accelerated applications on [Amazon EC2 F1 instances](https://aws.amazon.com/ec2/instance-types/f1/).\nIt is distributed between this github repository and FPGA Developer AMI - [Centos](https://aws.amazon.com/marketplace/pp/B06VVYBLZZ)/[AL2](https://aws.amazon.com/marketplace/pp/B08NTMMZ7X) provided by AWS with no cost of development tools.\n\n\u26a0\ufe0f <b>NOTE:</b> The developer kit is supported for Linux operating systems only.\n\n## Developer Support\n\nOpening a [GitHub Issue](https://github.com/aws/aws-fpga/issues) is the preferred method to get support with the AWS FPGA Development Kit. In addition, the [FPGA Development re:Post Tag](https://repost.aws/tags/TAc7ofO5tbQRO57aX1lBYbjA/fpga-development) is available to find FPGA-related discussion topics from the AWS community of customers, AWS customer support, and the AWS FPGA development team.\n\n## Development Flow\nAfter creating an FPGA design (also called CL - Custom logic), developers can create an Amazon FPGA Image (AFI) and easily deploy it to an F1 instance. AFIs are reusable, shareable and can be deployed in a scalable and secure way.\n\n![Alt text](hdk/docs/images/f1-Instance-How-it-Works-flowchart.jpg)\n\n## Development Environments\n\n| Development Environment | Description | Accelerator Language | Hardware Interface | Debug Options| Typical Developer                                                     |\n| --------|---------|-------|---------|-------|-----------------------------------------------------------------------|\n| Software Defined Accelerator Development using [Vitis](Vitis/README.md)/[SDAccel](SDAccel/README.md)| Development experience leverages an optimized compiler to allow easy new accelerator development or migration of existing C/C++/openCL, Verilog/VHDL to AWS FPGA instances | C/C++/OpenCL, Verilog/VHDL (RTL) | OpenCL APIs and XRT | SW/HW Emulation, Simulation, GDB, Virtual JTAG (Chipscope) | SW or HW Developer with zero FPGA experience                          |\n| [Hardware Accelerator Development using Vivado](hdk/README.md) | Fully custom hardware development experience provides hardware developers with the tools required for developing AFIs for AWS FPGA instances  | Verilog/VHDL | [XDMA Driver](sdk/linux_kernel_drivers/xdma/README.md), [peek/poke](sdk/userspace/README.md) | Simulation, Virtual JTAG | HW Developer with advanced FPGA experience                            |\n| [IP Integrator/High Level Design(HLx) using Vivado](hdk/docs/IPI_GUI_Vivado_Setup.md) | Graphical interface development experience for integrating IP and high level synthesis development | Verilog/VHDL/C | [XDMA Driver](sdk/linux_kernel_drivers/xdma/README.md), [peek/poke](sdk/userspace/README.md) | Simulation, Virtual JTAG | HW Developer with intermediate FPGA experience                        |\n | [On-premise development for Alveo U200 using Vitis targetted for migration to F1](Vitis/docs/Alveo_to_AWS_F1_Migration.md) | Vitis flow development using on-premise U200 platform targeted for migration to F1 |  C/C++/OpenCL, Verilog/VHDL (RTL) | OpenCL APIs and XRT | SW/HW Emulation, Simulation, GDB, JTAG (Chipscope) | SW or HW Developer with zero FPGA experience and on-premise U200 card |\n | [On-premise development for Alveo U200 using F1.A.1.4 shell](hdk/docs/U200_to_F1_migration_HDK.md) | HDK flow for on-premise U200 card using F1.A.1.4 shell targetted for migration to F1 | Verilog/VHDL | XDMA driver, peek/poke | Simulation, JTAG | HW Developer with advanced FPGA experience and on-premise U200 card   |\n> For on-premise development, SDAccel/Vitis/Vivado must have the [correct license and use one of the supported tool versions](./docs/on_premise_licensing_help.md). \n\n## FPGA Developer AMI\n\nThe [FPGA Developer AMI](https://aws.amazon.com/marketplace/pp/B06VVYBLZZ) is available on the AWS marketplace without a software charge and includes tools needed for developing FPGA Designs to run on AWS F1. \n\nGiven the large size of the FPGA used inside AWS F1 Instances, Xilinx tools work best with 32GiB Memory. \nz1d.xlarge/c5.4xlarge and z1d.2xlarge/c5.8xlarge instance types would provide the fastest execution time with 30GiB+ and 60GiB+ of memory respectively. \nDevelopers who want to save on cost, could start coding and run simulations on low-cost instances, like t2.2xlarge, and move to the aforementioned larger instances to run the synthesis of their acceleration code.\n\nAWS marketplace offers multiple versions of the FPGA Developer AMI. The following section table describes the mapping of currently supported developer kit versions to AMI versions.\n\n## Xilinx tool support\n\n| Developer Kit Version | Tool Version Supported | Compatible FPGA Developer AMI Version       |\n|-----------------------|------------------------|---------------------------------------------|\n| 1.4.23+               | 2021.2                 | v1.12.X (Xilinx Vivado/Vitis 2021.2)        |\n| 1.4.21+               | 2021.1                 | v1.11.X (Xilinx Vivado/Vitis 2021.1)        |\n| 1.4.18+               | 2020.2                 | v1.10.X (Xilinx Vivado/Vitis 2020.2)        |\n| 1.4.16+               | 2020.1                 | v1.9.0-v1.9.X (Xilinx Vivado/Vitis 2020.1)  |\n| 1.4.13+               | 2019.2                 | v1.8.0-v1.8.X (Xilinx Vivado/Vitis 2019.2)  |\n| 1.4.11+               | 2019.1                 | v1.7.0-v1.7.X (Xilinx Vivado/SDx 2019.1)    |\n| 1.4.8 - 1.4.15b       | 2018.3                 | v1.6.0-v1.6.X (Xilinx Vivado/SDx 2018.3)    |\n| 1.4.3 - 1.4.15b       | 2018.2                 | v1.5.0-v1.5.X (Xilinx Vivado/SDx 2018.2)    |\n| \u26a0\ufe0f 1.3.7 - 1.4.15b    | 2017.4                 | v1.4.0-v1.4.X (Xilinx Vivado/SDx 2017.4) \u26a0\ufe0f |\n\n\u26a0\ufe0f Developer kit release v1.4.16 will remove support for Xilinx 2017.4, 2018.2, 2018.3 toolsets. While developer kit release v1.4.16 onwards will not support older Xilinx tools, you can still use them using HDK releases v1.4.15b or earlier. \nPlease check out [the latest v1.4.15b release tag from Github](https://github.com/aws/aws-fpga/releases/tag/v1.4.15b) to use Xilinx 2017.4, 2018.2, 2018.3 toolsets.\n\nFor deprecation notices, please check the [End of life announces](./README.md#end-of-life-announcements)\n\nFor software-defined development please look at the runtime compatibility table based on the Xilinx toolset in use:\n[SDAccel](SDAccel/docs/Create_Runtime_AMI.md#runtime-ami-compatibility-table) or [Vitis](Vitis/docs/Create_Runtime_AMI.md#runtime-ami-compatibility-table)\n\n### End of life Announcements\n\n| Xilinx Tool version | State | Statement                                                                                                                                                                 | \n|-----------|-----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2017.1 | \ud83d\udeab Deprecated on 09/01/2018 | Developer kit versions prior to v1.3.7 and Developer AMI prior to v1.4 (2017.1) [reached end-of-life](https://forums.aws.amazon.com/ann.jspa?annID=6068).                 |\n| 2017.4 | \ud83d\udeab Deprecated on 12/31/2021 | [Support for Xilinx 2017.4 toolsets was deprecated on 12/31/2021](https://forums.aws.amazon.com/ann.jspa?annID=8949). |\n| 2020.1 and below | Discontinued on 02/2022 | Removed the ability for customers to newly subscribe to 2020.1 and below AMI versions to remove exposure to [CVE-2021-44228](https://www.cve.org/CVERecord?id=CVE-2021-44228) as these versions of tools do not have patches from xilinx |\n\n## Hardware Development Kit (HDK)\n\nThe [HDK directory](./hdk/README.md) contains documentation, examples, simulation, build and AFI creation scripts to start building Amazon FPGA Images (AFI).  \nThe HDK can be installed on any on-premises server or an EC2 instance. \nThe developer kit is not required if you plan to use a pre-built AFI shared from another developer.\n\n### AWS Shells\n\nWith Amazon EC2 FPGA instances, each FPGA is divided into two partitions:\n\n* Shell (SH) \u2013 AWS platform logic implementing the FPGA external peripherals, PCIe, DRAM, and Interrupts.\n\n* Custom Logic (CL) \u2013 Custom acceleration logic created by an FPGA Developer.\n\nAt the end of the development process, combining the Shell and CL creates an Amazon FPGA Image (AFI) that can be loaded onto the Amazon EC2 FPGA Instances.\n\nThe following table provides the shells currently available to develop your CL with. Each shell provides specific interfaces and features and currently needs to be used with the Dev Kit branch listed in the table.\n\n| Shell Name| Shell Version | Dev Kit Branch | Description|\n|--------|--------|---------|-------|\n| F1 XDMA Shell | F1.X.1.4 | [master](https://github.com/aws/aws-fpga/) | Provides all the [interfaces listed here](https://github.com/aws/aws-fpga/blob/master/hdk/docs/AWS_Shell_Interface_Specification.md), includes DMA | \n| F1 Small Shell | F1.S.1.0 | [small_shell](https://github.com/aws/aws-fpga/tree/small_shell) | Provides all the [interfaces listed here](https://github.com/aws/aws-fpga/blob/small_shell/hdk/docs/AWS_Shell_Interface_Specification.md). This shell does not include DMA engine and provides significant reduction in Shell resource usage. |\n\nFor more details, check the [FAQ](./FAQs.md#general-aws-fpga-shell-faqs)\n\n## Software-defined Development Environment\n\nThe software-defined development environment allows customers to compile their C/C++/OpenCL code into the FPGA as kernels, and use OpenCL APIs to pass data to the FPGA. \nSoftware developers with no FPGA experience will find a familiar development experience that supercharges cloud applications.\n\nIn addition, this development environment allows for a mix of C/C++ and RTL accelerator designs into a C/C++ software based development environment. This method enables faster prototyping using C/C++ while supporting manual optimization of critical blocks within RTL. This approach is similar to optimizing time critical functions using software compiler optimization methods.\n\nTo get started with Xilinx SDAccel, review the [Software-defined development environment readme](SDAccel/README.md).\nTo get started with Xilinx Vitis, review the [Vitis unified development environment readme](Vitis/README.md).\n\n## Runtime Tools (SDK)\n\nThe [SDK directory](./sdk/README.md) includes the runtime environment required to run on EC2 FPGA instances. It includes the drivers and tools to manage the AFIs that are loaded on the FPGA instance. The SDK isn't required during the AFI development process; it is only required once an AFI is loaded onto an EC2 FPGA instance. The following sdk resources are provided:\n  * Linux Kernel Drivers - The developer kit includes three drivers:\n    * [XDMA Driver](sdk/linux_kernel_drivers/xdma/README.md) - DMA interface to/from HDK accelerators.\n  * [FPGA Libraries](sdk/userspace/fpga_libs) - APIs used by C/C++ host applications.\n  * [FPGA Management Tools](sdk/userspace/fpga_mgmt_tools/README.md) - AFI management APIs for runtime loading/clearing FPGA image, gathering metrics and debug interface on the F1 instance.\n\n# Amazon EC2 F1 Platform Features\n* 1-8 Xilinx UltraScale+ VU9P based FPGA slots\n* Per FPGA Slot, Interfaces available for Custom Logic(CL):\n    * One x16 PCIe Gen 3 Interface\n    * Four DDR4 RDIMM interfaces (72-bit with ECC, 16 GiB each; 64 GiB total)\n    * AXI4 protocol support on all interfaces\n* User-defined clock frequency driving all CL to Shell interfaces\n* Multiple free running auxiliary clocks\n* PCI-E endpoint presentation to Custom Logic(CL)\n    * Management PF (physical function)\n    * Application PF\n* Virtual JTAG, Virtual LED, Virtual DIP Switches\n* PCI-E interface between Shell(SH) and Custom Logic(CL).\n    * SH to CL inbound 512-bit AXI4 interface\n    * CL to SH outbound 512-bit AXI4 interface\n    * Multiple 32-bit AXI-Lite buses for register access, mapped to different PCIe BARs\n    * Maximum payload size set by the Shell\n    * Maximum read request size set by the Shell\n    * AXI4 error handling \n* DDR interface between SH and CL\n    * CL to SH 512-bit AXI4 interface\n    * 1 DDR controller implemented in the SH (always available)\n    * 3 DDR controllers implemented in the CL (configurable number of implemented controllers allowed)\n\n# Getting Started\n\n### Getting familiar with AWS\nIf you have never used AWS before, we recommend you start with [AWS getting started training](https://aws.amazon.com/getting-started/), and focus on the basics of the [AWS EC2](https://aws.amazon.com/ec2/) and [AWS S3](https://aws.amazon.com/s3/) services. \nUnderstanding the fundamentals of these services will make it easier to work with AWS F1 and the FPGA Developer Kit.\n\nFPGA Image generation and EC2 F1 instances are supported in the us-east-1 (N. Virginia), us-west-2 (Oregon), eu-west-1 (Ireland) and us-gov-west-1 ([GovCloud US](https://aws.amazon.com/govcloud-us/)) [regions](https://aws.amazon.com/about-aws/global-infrastructure/).\n\n> \u26a0\ufe0f <b>NOTE:</b> By default, your AWS Account will have an EC2 F1 Instance launch limit of 0. \n> Before using F1 instances, you will have to open a [Support Case](https://console.aws.amazon.com/support/home#/case/create) to increase the EC2 Instance limits to allow launching F1 instances.\n\n### Setting up development environment for the first time \n\nYou have the choice to develop on AWS EC2 using the [FPGA Developer AMI](https://aws.amazon.com/marketplace/pp/B06VVYBLZZ) or on-premise. \n\n> \u2139\ufe0f <b>INFO:</b> We suggest starting with the FPGA Developer AMI with [build instances](#fpga-developer-ami) on EC2 as it has Xilinx tools and licenses setup for you to be able to quickly get into development.\n\n> \u2139\ufe0f <b>INFO:</b> For on-premise development, you will need to have [Xilinx tools and licenses available for you to use](./docs/on_premise_licensing_help.md)\n\n1. Start a Build Instance first to start your development. \n    > \ud83d\udca1 <b>TIP:</b> This instance does not have to be an F1 instance. You only require an F1 instance to run your AFI's(Amazon FPGA Image) once you have gone through your design build and AFI creation steps.\n    \n    > \u2139\ufe0f <b>INFO:</b> If you need to follow GUI Development flows, please checkout our [Developer Resources](./developer_resources/README.md) where we provide Step-By-Step guides to setting up a GUI Desktop.\n1. Clone the [FPGA Developer Kit](https://github.com/aws/aws-fpga) on your instance.\n    ```git clone https://github.com/aws/aws-fpga.git```\n1. Follow the quickstarts from the next section.\n\n### Quickstarts\nBefore you create your own AWS FPGA design, we recommend that you go through one of the step-by-step Quickstart guides:\n\n| Description | Quickstart | Next Steps |\n|----|----|----|\n| Software Defined Accelerator Development using Xilinx Vitis | [Vitis hello_world Quickstart](Vitis/README.md) | [60+ Vitis examples](./Vitis/examples/), [Vitis Library Examples](./docs/examples/example_list.md) |\n| Software Defined Accelerator Development using Xilinx SDAccel | [SDAccel hello_world Quickstart](SDAccel/README.md) | [60+ SDAccel examples](./SDAccel/examples/) |\n| Custom Hardware Development(HDK) | [HDK hello_world Quickstart](hdk/README.md) | [CL to Shell and DRAM connectivity example](./hdk/cl/examples/cl_dram_dma), [Virtual Ethernet Application](./sdk/apps/virtual-ethernet) using the [Streaming Data Engine](./hdk/cl/examples/cl_sde) |\n| IP Integrator/High Level Design(HLx) | [IPI hello_world Quickstart](hdk/cl/examples/cl_hello_world_hlx/README.md) | [IPI GUI Examples](hdk/docs/IPI_GUI_Examples.md) |\n\n\u2139\ufe0f <b>INFO:</b> For more in-depth applications and examples of using High level synthesis, Vitis Libraries, App Notes and Workshops, please refer to our [Example List](./docs/examples/example_list.md)\n\n### How Tos\n| How To                                                                                | Description                                                                            | \n|---------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|\n| [Migrate Alveo U200 designs to F1 - Vitis](./Vitis/docs/Alveo_to_AWS_F1_Migration.md) | This application note shows the ease of migrating an Alveo U200 design to F1.          | \n | [Migrate Alveo U200 designs to F1 - HDK](./hdk/docs/U200_to_F1_migration_HDK.md)      | Path to migrate from U200 vivado design flow to F1 HDK flow using AWS provided shells. |                                                                 \n# Documentation Overview\n\nDocumentation is located throughout this developer kit and the table below consolidates a list of key documents to help developers find information:\n\n| Topic | Document Name |  Description |\n|-----------|-----------|------|\n| AWS setup | [Setup AWS CLI and S3 Bucket](./SDAccel/docs/Setup_AWS_CLI_and_S3_Bucket.md) | Setup instructions for preparing for AFI creation |\n| Developer Kit | [RELEASE NOTES](./RELEASE_NOTES.md), [Errata](./ERRATA.md) | Release notes and Errata for all developer kit features, excluding the shell  |\n| Developer Kit | [Errata](./ERRATA.md) | Errata for all developer kit features, excluding the shell  |\n| F1 Shell | [AWS Shell RELEASE NOTES](./hdk/docs/AWS_Shell_RELEASE_NOTES.md) | Release notes for F1 shell |\n| F1 Shell | [AWS Shell ERRATA](./hdk/docs/AWS_Shell_ERRATA.md) | Errata for F1 shell |\n| F1 Shell | [AWS Shell Interface Specification](./hdk/docs/AWS_Shell_Interface_Specification.md) | Shell-CL interface specification for HDK developers building AFI |\n| F1 Shell - Timeout and AXI Protocol Protection | [How to detect a shell timeout](hdk/docs/HOWTO_detect_shell_timeout.md) | The shell will terminate transactions after a time period or on an illegal transaction.  This describes how to detect and gather data to help debug CL issues caused by timeouts. |\n| Vitis | [Debug Vitis Kernel](./Vitis/docs/Debug_Vitis_Kernel.md) | Instructions on debugging Vitis Kernel |\n| Vitis | [Create Runtime AMI](./Vitis/docs/Create_Runtime_AMI.md) | Instructions on creating a runtime AMI when using Xilinx Vitis|\n| Vitis | [XRT Instructions](./Vitis/docs/XRT_installation_instructions.md) | Instructions on building, installing XRT with MPD daemon considerations for F1 |\n| SDAccel | [Debug RTL Kernel](./SDAccel/docs/Debug_RTL_Kernel.md) | Instructions on debugging RTL Kernel with SDAccel |\n| SDAccel | [Create Runtime AMI](./SDAccel/docs/Create_Runtime_AMI.md) | Instructions on creating a runtime AMI when using Xilinx SDAccel|\n| HDK - Host Application | [Programmer View](./hdk/docs/Programmer_View.md) | Host application to CL interface specification |\n| HDK - CL Debug | [Debug using Virtual JTAG](./hdk/docs/Virtual_JTAG_XVC.md) | Debugging CL using Virtual JTAG (Chipscope)  |\n| HDK - Simulation | [Simulating CL Designs](./hdk/docs/RTL_Simulating_CL_Designs.md) | Shell-CL simulation specification |\n| HDK - Driver | [README](./sdk/linux_kernel_drivers/xdma/README.md) | Describes the DMA driver (XDMA) used by HDK examples and includes a link to an installation guide |\n| AFI | [AFI Management SDK](./sdk/userspace/fpga_mgmt_tools/README.md) | CLI documentation for managing AFI on the F1 instance |\n| AFI - EC2 CLI | [copy\\_fpga\\_image](./hdk/docs/copy_fpga_image.md), [delete\\_fpga\\_image](./hdk/docs/delete_fpga_image.md), [describe\\_fpga\\_images](./hdk/docs/describe_fpga_images.md), [fpga\\_image\\_attributes](./hdk/docs/fpga_image_attributes.md) | CLI documentation for administering AFIs |\n| AFI - Creation Error Codes | [create\\_fpga\\_image\\_error\\_codes](hdk/docs/create_fpga_image_error_codes.md) | CLI documentation for managing AFIs |\n| AFI - Power | [FPGA Power, recovering from clock gating](./hdk/docs/afi_power.md) | Helps developers with understanding FPGA power usage, preventing power violations on the F1 instance and recovering from a clock gated slot. |\n| On-premise Development | [Tools, Licenses required for on-premise development](./docs/on_premise_licensing_help.md) | Guidance for developer wanting to develop AFIs from on-premises instead of using the [FPGA Developer AMI](https://aws.amazon.com/marketplace/pp/B06VVYBLZZ) |\n| PCIe Peer-2-Peer     | [P2P](https://github.com/awslabs/aws-fpga-app-notes/blob/master/Using-PCIe-Peer2Peer/README.md) | Guidance on using PCIe P2P |\n| PCIe write combining | [PCIe write combine](https://github.com/awslabs/aws-fpga-app-notes/blob/master/Using-PCIe-Write-Combining/README.md) | Documentation on PCIe write combining for performance improvement |\n| Frequently asked questions | [FAQ](./FAQs.md)| Q/A are added based on developer feedback and common AWS forum questions  |\n", "release_dates": ["2022-06-01T18:14:55Z", "2022-02-15T20:51:49Z", "2021-10-08T21:58:21Z", "2021-07-21T15:39:17Z", "2021-07-08T23:56:57Z", "2021-06-23T13:59:37Z", "2021-03-31T22:55:02Z", "2021-03-18T22:00:26Z", "2020-10-20T14:59:09Z", "2020-09-17T21:48:58Z", "2020-09-10T16:03:29Z", "2020-07-12T18:20:46Z", "2020-05-15T22:17:39Z", "2020-02-12T05:01:11Z", "2019-12-13T23:56:31Z", "2019-09-20T01:12:22Z", "2019-08-01T15:06:02Z", "2019-05-14T17:42:44Z", "2019-04-22T16:58:41Z", "2019-04-12T17:56:04Z", "2019-02-11T23:16:19Z", "2019-01-25T21:15:27Z", "2018-12-04T04:47:00Z", "2018-11-06T13:27:04Z", "2018-11-06T13:30:57Z", "2018-08-29T22:08:13Z", "2018-08-15T02:32:58Z", "2018-07-09T03:06:01Z", "2018-06-11T15:46:51Z", "2018-05-14T02:03:39Z"]}, {"name": "aws-fpga-f1-u200", "description": null, "language": "VHDL", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Table of Contents\n\n1. [Overview](#Overview)\n1. [F1.A.1.4 shell](#F1.A.1.4-shell)\n1. [Development Flow](#Development-Flow)\n1. [Development migration between Alveo Shell and F1 shell](#Development-migration)\n     - [Development for Alveo U200 using F1.A.1.4 shell](#Development-for-U200)\n     - [Migration to F1](#Migration-to-F1)\n\n# Overview <a name=Overview></a>\n\nF1 customers want to seamlessly migrate between Alveo and F1 platforms to enable scaling in AWS cloud. \nThe F1 development flow currently supports the requirement with [Vitis](https://github.com/aws/aws-fpga/blob/master/Vitis/docs/Alveo_to_AWS_F1_Migration.md), but there is no such support available for full custom RTL development flow (Vivado design flow). \nAWS F1.A.1.4 shell is targetted at enabling this.\n\n# F1.A.1.4 shell <a name=F1.A.1.4-shell></a>\n\nAWS F1.A.1.4 shell enables customers to migrate designs seamlessly between on-premise U200 Alveo card and F1 platform in a full custom RTL development flow. This gives choice to the customers to use either a Vitis flow or a full custom RTL/Vivado design flow and seamlessly migrate designs between Alveo U200 and F1. The F1.A.1.4 shell for Alveo U200 is fully compatible with F1.X.1.4 shell thus making the migration as simple as swapping the shell with one another, with no design changes to user logic. Xilinx device,  xcu200-fsgd2104-2-e (Part used in U200) is based on VU9P and has the same base SLR die with the same number of SLRs as VU9P (F1 device) and is therefore highly compatible with F1 implementation.\n\nThe F1.A.1.4 shell is fully interface compatible and footprint compatible with F1.X.1.4 and no changes to custom logic design are required. Similar to the existing F1 shells, the F1.A.1.4 shell provides all the required communication interfaces to the custom logic. This helps our customers with the effort required to implement their own interfaces for Alveo U200 platform and also provides them with a seamless migration path to F1 in the custom RTL development flow. The F1.A.1.4 shell does not support partial reconfiguration/DFX. Dynamic clock configuration, VDIP/VLED are controlled via registers in PF1-B0 in F1.A.1.4 shell, using tools provided in the developer kit. SW APIs supported by F1.X.1.4 shell are not available/compatible with F1.A.1.4 shell.\n\n*Note:* F1.A.1.4 shell requires the host to have an appropriate BIOS to enumerate 128G BAR. Most x86 hosts can support this with BIOS support. The F1.A.1.4 shell is verified on asrock ROMED8-2T motherboard model. Also make sure to enable large BAR configuration in the BIOS settings\n\n# Development Flow <a name=Development-Flow></a>\nThe F1 provided F1.A.1.4 shell based Vivado design flow will have the shell provided as dcp and the associated developer kit (including examples, build scripts, constraints) provided through xilinx website as a tar file and also through github, along with Readme/Tutorial documents for use in Alveo development environment. The developer kit will also include examples to help customers get started with development in Alveo Vivado design flow using F1 provided Alveo shell.\n\nDevelopers can download/clone the F1.A.1.4 development kit on their development environment and get started with the development. Xilinx tool version 2021.2 or above is required. The developer kit is structured very similar to the F1 FPGA developer kit. cl_hello_world and cl_dram_dma examples from F1 environment are ported to F1.A.1.4 shell and are provided with associated simulation and test environment. Customers are encouraged to run the examples to get familiarized with the development flow.\n\nThe developer flow using F1.A.1.4 shell is maintained to resemble the current F1 development flow and thus a seamless portability is established with F1 development as shown in Figure1.\n\n\n\n![Alt text](./hdk/docs/images/AlveoF1FlowChart.png)\n\nfigure1: Flow chart for design portability between Alveo and F1 development environment\n\nCustomer can start with either Alveo development environment or the F1 environment and work on their CL design/development. Once they are ready to migrate to the other environment, they need to clone the other developer environment, copy their custom rtl files and port any xilinx ip used to the new environment and get started with the build flow. Since the interface compatibility is maintained between the shells, no changes to the CL is required.\n\n*Note*: Customers are required to port the Xilinx IP used in their designs from one environment to the other during migration. For ddr4 controller IP used in the design, customers are not required to port the IP. Instead, the ddr4_core IP and the sh_ddr provided in the corresponding development environment should be used and the build scripts (synth_example.tcl) should point to the corresponding IP used.\n\n\n# Development migration between Alveo Shell and F1 shell <a name=Development-migration></a>\n\n## Development for Alveo U200 using F1.A.1.4 shell <a name=Development-for-U200></a>\n*STEP1*:\nThe customers will clone the dev kit using\n\n    git clone https://github.com/aws/aws-fpga-f1-u200.git\n\nThey can also simply download the dev kit provided as tar ball from xilinx Alveo Vivado flow [webpage](https://www.xilinx.com/products/boards-and-kits/alveo/u200.html#gettingStarted).\n\n*STEP2*:\n\nsetup the software development environment\n\n\tsource sdk_setup.sh\n\n*STEP3(optinal, only if you'd like to run designs with DMA capabilities)*:\n\nInstall the patched xdma driver according to instructions in [xdma installation guide](https://github.com/aws/aws-fpga-f1-u200/blob/main/sdk/xdma_patch/xdma_install.md)\n\n*STEP4*:\nsource the development setup through hdk_setup.sh\n\n    source hdk_setup.sh\n\nThe hdk_setup.sh script sets up all the required environment for Alveo U200 development using F1 based Alveo F1.A.1.4 shell. It points to the vivado installation, checks for supported version of vivado tool for compatibility, sets up key environment variables and downloads F1.A.1.4 shell dcp. In addition, it also prepares any required patches for tool and IP blocks.\n\n*STEP5*:\nAfter setting up the development environment, in order to build cl_hello_world example provided, run the following steps\n\n    $ cd $HDK_DIR/cl/examples/cl_hello_world\n    $ export CL_DIR=$(pwd)\n    $ cd $CL_DIR/build/scripts\n    $ ./aws_build_bit_from_cl.sh\n\nThe above steps generate a bit file in the $CL_DIR/build/bitstreams directory along with associated reports in $CL_DIR/build/reports directory. This bit file can be programmed on an on-prem Alveo U200 card using physical JTAG cables.\n\nIn order to build a custom design, copy the example directory to $HDK_DIR/cl/developer_designs and update CL_DIR environment variable to point to the new design\n\n    $ cd $HDK_DIR/cl/developer_designs\n    $ cp -r $HDK_DIR/cl/examples/<example> .\n    $ export CL_DIR=$(pwd)\n\nThe developer_designs directory also comes with a prepare_new_cl.sh script to set up the directory structure from scratch, as required by HDK simulation and build scripts. Also modify the build scripts for timing, clock and placement constraints and point to corresponding source files and IP and run the aws_build_bit_from_cl.sh script to generate corresponding bit file for U200.\n\nEach CL example comes with a runtime software under $CL_DIR/software/runtime/ subdirectory. You will need to build the runtime application that matches your build.\n\n    $ cd $CL_DIR/software/runtime/\n    $ make all\n    \nBefore running the compiled c code for any build, setup the pcie regs\n    \n    $ setpci -v -s 0000:c1:00.0 0x4.w=0x7\n    $ setpci -v -s 0000:c1:00.1 0x4.w=0x7\n    \nThen run\n\n    $ sudo ./test_hello_world\n\n## Migration to F1 <a name=\"Migration-to-F1\"></a>\nCustomers are encouraged to get familiar with F1 development environment before building their custom logic for F1 using various examples provided as part of F1 developer kit. For further instructions, on getting started with F1 developer kit, please refer to getting started [documentation](https://github.com/aws/aws-fpga/blob/master/hdk/README.md) provided with aws-fpga developer kit\n\nWhen ready for migration to F1, customers create an aws account and launch an instance type of interest, as described in https://github.com/aws/aws-fpga/blob/master/hdk/README.md\n\nAfter creating the aws instance, clone the F1 developer kit using\n\n    $ git clone https://github.com/aws/aws-fpga.git $AWS_FPGA_REPO_DIR\n\nCustomers can set up their custom logic in place of the examples provided using the instructions provided [here](https://github.com/aws/aws-fpga/blob/master/hdk/cl/developer_designs/Starting_Your_Own_CL.md). \nTo port the customer logic design from F1 based Alveo U200 development environment to F1 development environment, customers need to port three sets of design and build files:\n\n1.) *custom RTL files* (e.g: .sv or .v files): No changes are required in Custom Logic RTL files. Customers can simply copy all the custom RTL files from U200 to F1 dev environment\n\n    e.g: cp -r <U200>/design/* <F1>/design/\n\n2.) *Xilinx IP*: Customers are required to port the Xilinx IP used in their designs from one environment to the other during migration. \n*Note:* For ddr4 controller IP used in the design, customers are not required to port the IP. Instead, the ddr4_core IP and the sh_ddr provided in the corresponding development environment should be used.\n\n3.) *Build scripts*: Update the associated build scripts (synth_<design>.tcl) to point to the correct IP files. Also modify the build scripts for F1 for appropriate timing constraints and source file list.\n\nOnce the RTL files are copied, IPs ported and scripts updated; standard F1 development flow can be used to build the ported design in F1 Development environment can be used to build the design seamlessly for F1 platform as described [here](https://github.com/aws/aws-fpga/blob/master/hdk/cl/developer_designs/Starting_Your_Own_CL.md)\n\nsource the hdk_setup script using\n\n    $ cd $AWS_FPGA_REPO_DIR\n    $ source hdk_setup.sh\n\n After the set up, a dcp is built from cl using\n\n    $ cd $CL_DIR/build/scripts\n    $ ./aws_build_dcp_from_cl.sh\n\nOnce the dcp is generated, it is submitted for ingestion to generate AFI for running on F1\n\n    $ aws ec2 create-fpga-image \\\n        --region <region> \\\n        --name <afi-name> \\\n        --description <afi-description> \\\n        --input-storage-location Bucket=<dcp-bucket-name>,Key=<path-to-tarball> \\\n        --logs-storage-location Bucket=<logs-bucket-name>,Key=<path-to-logs> \\\n    [ --client-token <value> ] \\\n    [ --dry-run | --no-dry-run ]\n\nInstructions for the F1 flow can be found on git documentation, provided as part of F1 developer kit [here](https://github.com/aws/aws-fpga/blob/master/hdk/README.md).\n", "release_dates": ["2022-06-15T16:39:18Z"]}, {"name": "aws-gamekit", "description": "A C++ Library for AWS GameKit", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS GameKit C++ SDK\n\n## Prerequisites\n\nDownload and Install [CMake v3.21](https://github.com/Kitware/CMake/releases/tag/v3.21.6)\nSome versions of aws-sdk-cpp have a [known issue](https://github.com/aws/aws-sdk-cpp/issues/1820) compiling with CMake 3.22 or later.\n\n### Enable Windows long path support\nFor the build steps of the dependencies and the SDK to work properly on Windows,\nyou will have to enable long path support.\n\nRun the following command in an Administrator level Powershell terminal:\n```powershell\nSet-ItemProperty `\n  -Path HKLM:\\SYSTEM\\CurrentControlSet\\Control\\FileSystem `\n  -Name LongPathsEnabled -Value 1\n```\n\n### Dependencies\n\nInstall the dependencies in the following order:\n\n1. AWS SDK C++\n2. yaml-cpp\n3. GTest\n4. Boost\n5. Pybind\n\n### Third party Tools for iOS Builds\n\n**iOS Toolchain file:** Get the toolchain file from https://github.com/leetal/ios-cmake.\n\n**cURL, OpenSSL, nghttp:** Get the script that will build cURL, OpenSSL, and nghttp for iOS from: https://github.com/jasonacox/Build-OpenSSL-cURL\n\n- Needs Xcode 13.1 or above\n  - If multiple Xcode versions are installed,\n    run: `sudo xcode-select -s <YOUR_XCODE13_APP>/Contents/Developer`\n    example: `sudo xcode-select -s /Applications/Xcode13.app/Contents/Developer`\n- Run: `./build.sh`\n  - If needed the '-e' flag can be used to compile with OpenSSL engine support although this may cause some 'undefined symbols for architecture' depending on your OS version.\n\n**Boost:** Get the shell script that will download, bootstrap, and build Boost for iOS from: https://github.com/faithfracture/Apple-Boost-BuildScript/blob/master/boost.sh.\n\n- The variables in boost.sh should be replaced with the following to avoid building unecessary libraries \n  - BOOST_LIBS=(\"regex\" \"filesystem\" \"iostreams\")\n  - ALL_BOOST_LIBS_1_68=(\"regex\" \"filesystem\" \"iostreams\")\n  - ALL_BOOST_LIBS_1_69=(\"regex\" \"filesystem\" \"iostreams\")\n\n### Third party Tools for Android Builds (On Windows)\n\n**Ninja**: Available as part of Visual Studio 2019\n\n**Developer Command Prompt for VS 2019**: Available as part of Visual Studio 2019. Note: all Android build commands should be executed from this console. Usually located at `\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\Tools\\VsDevCmd.bat\"`\n\n- For Ninja build tool, you may need to use the dev command prompt from VS Build Tools instead of VS Community edition, if you do not have VS Professional installed. This is especially important when using an EC2 instance instead of your local machine.\n- You can launch the dev cmd prompt using the `LaunchDevCmd.bat` in the above path.\n\n**Android NDK r21d for Unreal**: Use Unreal's NDK, follow these steps to install: https://docs.unrealengine.com/4.27/en-US/SharingAndReleasing/Mobile/Android/Setup/AndroidStudio/\n\n**Android NDK with Unity**: (API Level 24, NDK version 21), you can use the NDK supplied by Unity. To do this, set the following (replace <Unity Version> with the Unity version you are using): \n```bat\nset ndkroot=C:\\Program Files\\Unity\\Hub\\Editor\\<UNITY VERSION>\\Editor\\Data\\PlaybackEngines\\AndroidPlayer\\NDK\n```\n\n**Android Studio Version 4**: Follow these steps to install: https://docs.unrealengine.com/4.27/en-US/SharingAndReleasing/Mobile/Android/Setup/AndroidStudio/\n\nAfter following the steps to install Android Studio 4.0 and downloaded the NDK using Unreal\u00b4s script, open Android Studio again and make the following modifications:\n\n- Install Android SDK Build Tools 30.0.3 in Configure > Appeareance and Behavior > System Settings > Android SDK > SDK Tools. You might need to uncheck \"Hide obsolete packages\"\n  - This may also be accessed from the startup screen: Configure \u2192 SDK Manager \u2192 SDK Tools\n  - You may need to check `Show Package Details` to see all the versions of the `Android SDK Build Tools`\n- Uninstall newer versions of the SDK Build Tools\n\n### AWS SDK C++\n\n#### Build and Install AWS SDK C++\n\nNote: `BUILD_TYPE` is either `Debug` or `Release`\nNote: Make sure your development path is absolute, not relative, the install might silently fail otherwise.\n\n##### Windows\n\n1. Clone the AWS SDK C++: `git clone --recurse-submodules https://github.com/aws/aws-sdk-cpp` (make sure you're on tag 1.9.162 or later)\n2. Install CMake 3.21: https://github.com/Kitware/CMake/releases/tag/v3.21.6\n3. Use the Windows Command Prompt (or the Legacy command prompt in Windows Terminal) for the following:\n4. Create a build directory called AWSSDK in a separate directory than where you cloned the SDK.\n\n    ```bat\n    mkdir AWSSDK\n    ```\n\n5. Change directory into AWSSDK:\n\n    ```bat\n    cd AWSSDK\n    ```\n\n6. Generate the SDK Project files (~5 min):\n\n    ```bat\n    cmake <YOUR_DEVELOPMENT_PATH>\\aws-sdk-cpp -G \"Visual Studio 16 2019\" -DCMAKE_BUILD_TYPE=<BUILD_TYPE> -DFORCE_SHARED_CRT=ON -DBUILD_SHARED_LIBS=ON -DMINIMIZE_SIZE=ON -DCMAKE_INSTALL_PREFIX=<YOUR_DEVELOPMENT_PATH>\\AWSSDK\\install\\x86_64\\windows\\<BUILD_TYPE>\n    ```\n\n7. Build the SDK (~10 min depends on your workstation specs):\n\n    ```bat\n    \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\MSBuild\\Current\\Bin\\msbuild.exe\" ALL_BUILD.vcxproj /p:Configuration=<BUILD_TYPE> -maxcpucount\n    ```\n\n8. Install the SDK to the <YOUR_DEVELOPMENT_PATH>\\AWSSDK\\install location\n\n    ```bat\n    \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\MSBuild\\Current\\Bin\\msbuild.exe\" INSTALL.vcxproj /p:Configuration=<BUILD_TYPE> -maxcpucount\n    ```\n\n##### macOS\n\n1. Clone the AWS SDK C++: `git clone --recurse-submodules https://github.com/aws/aws-sdk-cpp` inside `~/<YOUR_DEVELOPMENT_PATH>` (make sure you're on tag 1.9.188 for macOS)\n   1. Make sure to use the same tag for `aws-sdk-cpp` for all builds (macOS and iOS)\n2. Install CMake: https://cmake.org/download/\n3. For both macOS and iOS, you will need to pull in the tag `v0.17.13` for `crt/aws-crt-cpp` submodule of `aws-sdk-cpp`. This fixes a bug for iOS builds related to `SetKeychainPath`. Make sure to `init` and recursive `update` further submodules of `aws-crt-cpp`.\n\n##### For macOS SHARED LIBRARIES\n\n3. Create a build directory called AWSSDK_mac e.g., `~/development/AWSSDK_mac`\n\n    ```bat\n    mkdir AWSSDK_mac\n    ```\n\n4. Change directory into AWSSDK_mac:\n\n    ```bat\n    cd AWSSDK_mac\n    ```\n\n5. Run CMake\n\n    ```bat\n    rm -rf CMakeFiles/; rm -rf CMakeScripts/; rm CMakeCache.txt; rm -rf crt/aws-crt-cpp/CMakeFiles/; rm -rf crt/aws-crt-cpp/CMakeScripts/;\n\n    cmake ~/development/aws-sdk-cpp -G Xcode -DCMAKE_BUILD_TYPE=<BUILD_TYPE> -DBUILD_ONLY=\"core;apigateway;cloudformation;cognito-idp;lambda;s3;ssm;secretsmanager;sts\" -DBUILD_SHARED_LIBS=ON -DCMAKE_INSTALL_PREFIX=<YOUR_DEVELOPMENT_PATH>/AWSSDK_mac/install/x86_64/macos/<BUILD_TYPE> -DTARGET_ARCH=\"APPLE\" -DCMAKE_OSX_DEPLOYMENT_TARGET=10.15\n    ```\n\n6. Build and install\n\n    ```bat\n    xcodebuild -parallelizeTargets -configuration <BUILD_TYPE> -target ALL_BUILD\n    xcodebuild -parallelizeTargets -target install\n    ```\n\n##### For iOS Static Libraries\n\n3. Create a build directory called AWSSDK_ios e.g., `~/development/AWSSDK_ios`\n\n    ```bat\n    mkdir AWSSDK_ios\n    ```\n\n4. Change directory into AWSSDK_ios:\n\n    ```bat\n    cd AWSSDK_ios\n    ```\n\n5.  Run CMake\n    Use XCode 12 to compile for iOS 14.0 which is the highest version supported by Unreal 4.27.\n\n    - If multiple Xcode versions are installed,\n\n    Run: `sudo xcode-select -s <YOUR_XCODE12_APP>/Contents/Developer`\n    Example: `sudo xcode-select -s /Applications/Xcode12.app/Contents/Developer`\n\n    ```bat\n    cmake ~/development/aws-sdk-cpp -G Xcode -DCMAKE_TOOLCHAIN_FILE=<YOUR_DEVELOPMENT_PATH>/ios-cmake/ios.toolchain.cmake -DPLATFORM=OS64 -DCMAKE_BUILD_TYPE=<BUILD_TYPE> -DBUILD_ONLY=\"core;apigateway;cloudformation;cognito-idp;lambda;s3;ssm;secretsmanager;sts\" -DBUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=<YOUR_DEVELOPMENT_PATH>/AWSSDK_ios/install/arm64/ios/<BUILD_TYPE> -DENABLE_TESTING=NO -DCURL_LIBRARY=<YOUR_DEVELOPMENT_PATH>/Build-OpenSSL-cURL/curl/lib/libcurl.a -DCURL_INCLUDE_DIR=<YOUR_DEVELOPMENT_PATH>/Build-OpenSSL-cURL/curl/include -DUSE_OPENSSL=ON -DENABLE_PROXY_INTEGRATION_TESTS=OFF -DENABLE_COMMONCRYPTO_ENCRYPTION=OFF -DENABLE_OPENSSL_ENCRYPTION=ON -DOPENSSL_ROOT_DIR=<YOUR_DEVELOPMENT_PATH>/Build-OpenSSL-cURL/openssl/iOS -DCMAKE_CXX_FLAGS=\"-Wno-shorten-64-to-32\" -DENABLE_CURL_CLIENT=ON -DENABLE_CURL_LOGGING=OFF\n    ```\n\n6. Build and install\n\n```\nxcodebuild -parallelizeTargets -configuration <BUILD_TYPE> -target ALL_BUILD\nxcodebuild -parallelizeTargets -target install\n```\n\n##### For Android Libraries\n\n1. Set these environment variables inside a Developer Command Prompt for VS 2019.\n\n    ```bat\n    set ANDROID_API_LEVEL=24\n    set BUILD_TYPE=Debug\n    set ARCH=arm\n    set PLATFORM=android\n    set ANDROID_ABI=armeabi-v7a\n    ```\n\n  * For Unreal, set:\n    ```bat\n    set BUILD_SHARED=OFF\n    set STL_TYPE=c++_static\n    ```\n  * For Unity, set:\n    ```bat\n    set BUILD_SHARED=ON\n    set STL_TYPE=c++_shared \n    ```\n\n2. Create a build directory called AWSSDK_android\n\n    ```bat\n    mkdir AWSSDK_android\n    ```\n\n3. Change directory into AWSSDK_android:\n\n    ```bat\n    cd AWSSDK_android\n    ```\n\n4. Run CMake\n\n    ```bat\n    cmake <YOUR_DEVELOPMENT_PATH>\\aws-sdk-cpp -DNDK_DIR=%NDKROOT% -DBUILD_ONLY=\"core;apigateway;cloudformation;cognito-idp;lambda;s3;ssm;secretsmanager;sts\" -DBUILD_SHARED_LIBS=%BUILD_SHARED% -DCMAKE_BUILD_TYPE=%BUILD_TYPE% -DCUSTOM_MEMORY_MANAGEMENT=ON -DTARGET_ARCH=ANDROID -DANDROID_ABI=%ANDROID_ABI% -DANDROID_NATIVE_API_LEVEL=%ANDROID_API_LEVEL% -DANDROID_BUILD_CURL=1 -DANDROID_BUILD_OPENSSL=1 -DANDROID_BUILD_ZLIB=1 -DBUILD_ZLIB=1 -DCMAKE_INSTALL_PREFIX=<YOUR_DEVELOPMENT_PATH>\\AWSSDK_android\\install\\%ARCH%\\%PLATFORM%\\%BUILD_TYPE% -G \"Ninja\" -DANDROID_STL=%STL_TYPE% -DANDROID_PLATFORM=android-%ANDROID_API_LEVEL% -DENABLE_TESTING=NO\n    ```\n\n5. Build and install\n\n    ```bat\n    cmake --build .\n    cmake --build . --target install\n    ```\n\n6. Before building `Release` after you have built `Debug`, make sure to delete all `CMakeCache` files and also the folder `external-build` in `AWSSDK_android` root path.\n\n### yaml-cpp\n\nAWS GameKit uses a fixed version of yaml-cpp: commit `2f899756`\n\n#### Windows Build and Install\n\n1. `git clone https://github.com/jbeder/yaml-cpp/`\n1. `cd yaml-cpp`\n1. `git checkout -b gamekit_fixed_version 2f899756`\n1. `mkdir build`\n1. `cd build`\n1. `cmake -DYAML_BUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=<DIRECTORY_IN_STEP_TWO>\\install\\<BUILD_TYPE> ..`\n1. `\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\MSBuild\\Current\\Bin\\msbuild.exe\" INSTALL.vcxproj /p:Configuration=<BUILD_TYPE> -maxcpucount`\n\n#### macOS Build and Install\n\n1. `git clone https://github.com/jbeder/yaml-cpp/`\n1. `cd yaml-cpp`\n1. `git checkout -b gamekit_fixed_version 2f899756`\n1. `mkdir build_mac`\n1. `cd build_mac`\n1. `cmake -G Xcode -DYAML_BUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=install/x86_64/<BUILD_TYPE> ..`\n1. `xcodebuild -parallelizeTargets -configuration <BUILD_TYPE> -target ALL_BUILD`\n1. `xcodebuild -parallelizeTargets -target install`\n\n#### iOS Build and Install\n\n1. `git clone https://github.com/jbeder/yaml-cpp/` (if not already cloned for `macos`)\n1. `cd yaml-cpp`\n1. `git checkout -b gamekit_fixed_version 2f899756`\n1. `mkdir build_ios` (same level as `macos`)\n1. `cd build_ios`\n1. `cmake -G Xcode -DCMAKE_TOOLCHAIN_FILE=<YOUR_DEVELOPMENT_PATH>/ios-cmake/ios.toolchain.cmake -DPLATFORM=OS64 -DCMAKE_BUILD_TYPE=<BUILD_TYPE> -DCMAKE_INSTALL_PREFIX=install/arm64/<BUILD_TYPE> ..`\n1. `xcodebuild -parallelizeTargets -configuration <BUILD_TYPE> -target ALL_BUILD`\n1. `xcodebuild -parallelizeTargets -target install`\n\n#### Android Build and Install\n\n1. Set these environment variables inside a Developer Command Prompt for VS 2019.\n\n    ```bat\n    set ANDROID_API_LEVEL=24\n    set BUILD_TYPE=Debug\n    set ARCH=arm\n    set PLATFORM=android\n    set ANDROID_ABI=armeabi-v7a\n    ```\n\n  * For Unreal, set:\n    ```bat\n    set BUILD_SHARED=OFF\n    set STL_TYPE=c++_static\n    ```\n  * For Unity, set:\n    ```bat\n    set BUILD_SHARED=ON\n    set STL_TYPE=c++_shared \n    ```\n\n1. `git clone https://github.com/jbeder/yaml-cpp/`\n1. `cd yaml-cpp`\n1. `git checkout -b gamekit_fixed_version 2f899756`\n1. `mkdir build_android`\n1. `cd build_android`\n\n    ```bat\n    cmake .. -DCMAKE_TOOLCHAIN_FILE=%NDKROOT%\\build\\cmake\\android.toolchain.cmake -DYAML_BUILD_SHARED_LIBS=%BUILD_SHARED% -DANDROID_ABI=%ANDROID_ABI% -DANDROID_NATIVE_API_LEVEL=%ANDROID_API_LEVEL% -DCMAKE_INSTALL_PREFIX=install\\%BUILD_TYPE% -G \"Ninja\" -DANDROID_STL=%STL_TYPE% -DBUILD_TESTING=OFF\n    cmake --build .\n    cmake --build . --target install\n    ```\n\n### GTest\n\n#### Windows Build and Install\n\n1. Create a directory to contain the Google test repo (example: `D:\\development`), navigate into it and clone: `git clone https://github.com/google/googletest`. You should end up having a directory tree like `D:\\development\\googletest`.\n2. Navigate into the googletest directory inside newly cloned googletest repo, your cwd should look like `D:\\development\\googletest\\googletest`.\n3. Run CMake to generate the configuration files: `cmake -DBUILD_SHARED_LIBS=ON -Dgtest_force_shared_crt=ON -DCMAKE_INSTALL_PREFIX=<path to gtest repo>\\build\\install -DGTEST_CREATE_SHARED_LIBRARY=1 ..`\n4. Build with `\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\MSBuild\\Current\\Bin\\msbuild.exe\" INSTALL.vcxproj /p:Configuration=Debug -maxcpucount`\n\n#### macOS Build and Install\n\n1. Clone https://github.com/google/googletest\n1. `cd` into the directory where you cloned googletest\n1. `mkdir build_mac`\n1. `cd build_mac`\n1. `cmake -G Xcode -DBUILD_SHARED_LIBS=ON -Dgtest_force_shared_crt=ON -DCMAKE_INSTALL_PREFIX=install/x86_64 -DGTEST_CREATE_SHARED_LIBRARY=1 ..`\n1. `xcodebuild -parallelizeTargets -target ALL_BUILD`\n1. `xcodebuild -parallelizeTargets -target install`\n\n#### iOS Build and Install\n\n1. Not needed.\n\n#### Android Build and Install\n\n1. Set these environment variables inside a Developer Command Prompt for VS 2019.\n\n    ```bat\n    set ANDROID_API_LEVEL=24\n    set BUILD_TYPE=Debug\n    set ARCH=arm\n    set PLATFORM=android\n    set ANDROID_ABI=armeabi-v7a\n    ```\n\n  * For Unreal, set:\n    ```bat\n    set STL_TYPE=c++_static\n    ```\n  * For Unity, set:\n    ```bat\n    set STL_TYPE=c++_shared \n    ```\n\n1. Clone https://github.com/google/googletest\n2. `cd` into the directory where you cloned googletest\n3. `mkdir build_android`\n4. `cd build_android`\n\n    ```bat\n    cmake .. -DBUILD_SHARED_LIBS=ON -Dgtest_force_shared_crt=ON -DCMAKE_INSTALL_PREFIX=install\\%BUILD_TYPE% -DGTEST_CREATE_SHARED_LIBRARY=1 -DCMAKE_TOOLCHAIN_FILE=%NDKROOT%\\build\\cmake\\android.toolchain.cmake -DANDROID_ABI=%ANDROID_ABI% -DANDROID_NATIVE_API_LEVEL=%ANDROID_API_LEVEL% -G \"Ninja\" -DANDROID_STL=%STL_TYPE%\n    cmake --build .\n    cmake --build . --target install\n    ```\n\n### Boost\n\n#### Windows Build and Install\n\n1. Download and extract https://www.boost.org/users/history/version_1_76_0.html\n2. `cd` into the directory you extracted Boost\n3. Run `bootstrap.bat`\n4. Run `.\\b2 link=static`\n\n#### macOS Build and Install\n\n1. Download and extract https://www.boost.org/users/history/version_1_76_0.html\n2. `cd` into the directory you extracted Boost\n3. Run `./bootstrap.sh`\n4. Run `./b2 link=static`\n\n#### iOS Build and Install\n\n1. Make the following updates to the script from https://gist.github.com/faithfracture/c629ae4c7168216a9856#file-boost-sh :\n   1. Make sure it's in a separate folder from the boost folder for win64 and macos, for example: `~/development/ios-boost`.\n   2. May need to give `r+w` permissions to all files in this folder with `chmod`.\n   3. Update boost version in to `1.76.0` (or whichever one we are currently using for win64 and macos).\n   4. Change BOOST_LIBS to `BOOST_LIBS=\"regex filesystem iostreams\"`\n2. Run the script using `./boost.sh -ios`.\n3. Note: You can continue to use the other boost folder (ex: `<BOOST_PARENT_DIR>\\boost_1_76_0`) when asked for the boost path in the `scripts/aws_gamekit_cpp_build.py` build script since the build needs boost source/includes. Use the `ios-boost` directory when prompted for the boost path in the `scripts/refresh_plugin.py` script that copies ios static libs.\n\n#### Android Build and Install\n\n1. Set these environment variables inside a Developer Command Prompt for VS 2019.\n\n    ```bat\n    set ANDROID_API_LEVEL=24\n    set BUILD_TYPE=Debug\n    set ARCH=arm\n    set PLATFORM=android\n    set CLANGPATH=%NDKROOT%\\toolchains\\llvm\\prebuilt\\windows-x86_64\\bin\n    set PATH=%PATH%;%CLANGPATH%\n    ```\n\n    * For Unreal, set:\n    ```bat\n    set STL_TYPE=c++_static\n    ```\n    * For Unity, set:\n    ```bat\n    set STL_TYPE=c++_shared \n    ```\n\n2. Copy the `scripts\\Android\\sample-android-user-config.jam` to your User directory with the name `user-config.jam` e.g. `C:\\users\\<your username>\\user-config.jam`. \n    \n    **Note:** Remember to remove or rename this file after building Boost for Android, otherwise all future Boost builds will use this configuration.  \n\n3. Download and extract https://www.boost.org/users/history/version_1_76_0.html. Make sure it's in a separate folder from the boost folder for win64 and macos, for example: `~/development/android-boost`.\n4. `cd` into the directory you extracted Boost\n5. Run `bootstrap.bat --with-libraries=regex,filesystem,iostreams`\n\n\n\n6. Build boost:\n * For **Unreal**, run \n    ```bat\n    .\\b2 toolset=clang-armeabiv7a target-os=android architecture=arm --without-python threading=multi link=static --layout=tagged variant=debug,release\n    ```\n * For **Unity**:  \n    Open `boostcpp.jam` in your boost dir and add `android` to the platforms on line 210, so it ends up as\n    ```jam\n    ! [ $(property-set).get <target-os> ] in windows cygwin darwin aix android &&\n    ```\n    Then, run in the Command Prompt\n    ```bat\n    .\\b2 toolset=clang-armeabiv7a target-os=android architecture=arm --without-python link=shared --layout=system variant=debug,release stage\n    ```\n\n### pybind11\n\n#### Windows Build and Install\n\n- Make sure `python3` is on your `PATH`\n\n1. Clone https://github.com/pybind/pybind11/\n2. `cd` into the directory where you cloned pybind11\n3. `cmake -DBoost_INCLUDE_DIR=<BOOST_PARENT_DIR>\\boost_1_76_0 -DCMAKE_INSTALL_PREFIX=<PYBIND_PARENT_DIR>\\pybind11\\install\\<BUILD_TYPE> .`\n4. `\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\MSBuild\\Current\\Bin\\msbuild.exe\" INSTALL.vcxproj /p:Configuration=<BUILD_TYPE> -maxcpucount`\n\n#### macOS Build and Install\n\n1. Clone https://github.com/pybind/pybind11/\n2. `cd` into the directory where you cloned pybind11\n3. `cmake -DBOOST_ROOT=<BOOST_PARENT_DIR>/boost_1_76_0 .`\n4. `make`\n5. `sudo make install`\n\n#### iOS Build and Install\n\nNot supported.\n\n#### Android Build and Install\n\nNot supported.\n\n## Build GameKit C++\nUse the `scripts/aws_gamekit_cpp_build.py` utility to generate project files and compile for the specified platform.\n- Usage: `python scripts/aws_gamekit_cpp_build.py <Platform> <Platform specific arguments> <BUILD_TYPE>`\n- Example: `python scripts/aws_gamekit_cpp_build.py Windows --test Debug`\n- Use `--help` to see all arguments and directions. If used after supplying a platform it will list and explain platform sepecific arguments.\n\n * For Android support in Unity, call the command as follows (replacing <BUILD_TYPE> with Release or Debug):\n    ```bat\n    python scripts/aws_gamekit_cpp_build.py  Android --shared <BUILD_TYPE>\n    ```\n\nThe script will first search your environment variables for dependency path variables. If they are not present there, it will search in\nthe `.env` file at the root of this repository (created by running this script), else it will prompt you for input on where the dependencies\nare located and save those locations in the `.env` file.\n\n\n## Run Unit Tests\n\n### Windows\n\n1. `cd` into `tests\\<BUILD_TYPE>` directory\n2. Run `aws-gamekit-cpp-tests.exe`\n\n### macOS\n\n1. cd into the `tests/Debug` directory\n2. Run `./aws-gamekit-cpp-tests`\n\n## Update Plugin with new binaries and headers\nUse the `scripts/refresh_plugin.py` script to update your game engine plugin with the new libraries and header files.\n- Usage: `python scripts/refresh_plugin.py --platform <target_platform> <BuildType> <GameEngine> <game engine specific arguments>`\n- Example: `python scripts/refresh_plugin.py --platform Windows Debug Unreal --unreal_plugin_path D:\\development\\MyUnrealGame\\Plugins\\AwsGameKit`\n- Example: `python scripts/refresh_plugin.py --platform Windows Debug Unity --unity_plugin_path D:\\development\\MyUnityGame\\Packages\\com.amazonaws.gamekit`\n- Use `--help` with the script to see argument options, and using `--help` after supplying a `GameEngine` argument, it will list and explain game engine specific arguments.\n- **If refreshing MacOS or iOS binaries you will have to codesign them to distribute them to other developers, supply the `--certificate_name` argument and give a Developer ID Application certificate common name.**", "release_dates": []}, {"name": "aws-gamekit-unity", "description": "AWS GameKit for Unity", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS GameKit Unity Package\nThis repository contains modifiable source code for the AWS GameKit package for Unity in the form of an [Embedded Package](https://docs.unity3d.com/Manual/CustomPackages.html#EmbedMe). If you don\u2019t need to modify the AWS GameKit package, you can download the latest tarball `.tgz` file from the [releases page](https://github.com/aws/aws-gamekit-unity/releases) and add it to a Unity project using the Unity Package Manager.\nTo modify this package we recommend cloning this repo and opening it as a blank Unity project. After making your changes, follow the Packaging steps below to generate a custom plugin package, which you can then add to your Unity projects.\n\n## Installing the AWS GameKit Unity Package\nSee [Packages/com.amazonaws.gamekit/README.md](Packages/com.amazonaws.gamekit/README.md) for instructions on installing the\nPackage Manager package.\n\n## Setup for making changes to the package\n\n### Clone this repository\n```\ngit clone https://github.com/aws/aws-gamekit-unity\n```\n\n## Add GameKit C++ SDK binaries\nThis package needs the GameKit C++ SDK binaries in order to work. There are two ways to get the binaries:\n\nUse pre-built binaries:\n1. Download the latest AWS GameKit Unity Package from the [releases page](https://github.com/aws/aws-gamekit-unity/releases).\n1. Expand the `com.amazonaws.gamekit-<version>.tgz` file.\n1. Copy the folder `<expanded contents>/package/Plugins/]` into `<root of this repo>/Package/com.amazonaws.gamekit/Plugins` and overwrite any existing files with the same name.\n\nBuild the binaries:\n1. Clone and setup the [AWS GameKit C++ SDK](https://github.com/aws/aws-gamekit) using the tag specified in the [.gkcpp_version](.gkcpp_version) file in this repository.\n1. Follow these steps each time you modify the C++ SDK:\n    1. Close Unity. Otherwise the `.meta` files for the binaries will be deleted by Unity in the next step.\n    1. Compile the [AWS GameKit C++ SDK](https://github.com/aws/aws-gamekit#aws-gamekit-c-sdk).\n    1. Run `refresh_plugin.py` as described in the `Update Plugin with new binaries and headers` section of the\n    [AWS GameKit C++ SDK README](https://github.com/aws/aws-gamekit#update-plugin-with-new-binaries-and-headers).\n\n### Add this Unity project to Unity Hub\n1. Open Unity Hub.\n1. Open the \"Projects\" tab along the left side.\n1. Click \"Add\", navigate to the root of the repository, then click \"Select Folder\".\n\n### Generate the C# Project\n1. Open Unity Hub.\n1. Open the `aws-gamekit-unity` project.\n1. In the Project window, navigate to `Packages/AWS GameKit/Runtime/Scripts`.\n1. Double click on any C# file.  \n*This will open the C# project in your IDE. Unity will automatically create the project if none exists. If you haven't set up an IDE or code editor, follow the guide in the [Unity documentation](https://docs.unity3d.com/Manual/ScriptingToolsIDEs.html).*\n\nIf you ever need to regenerate the solution:\n1. Open Unity.\n1. Go to `Edit > Preferences` and click `Regenerate project files`\n\n## Unit Tests\nSee [Assets/AWS_GameKit_Tests/README.md](Assets/AWS_GameKit_Tests/README.md) for instructions on running the unit tests.\n\n## Debugging\n### Unity\nFollow these instructions to debug the C# code: https://docs.unity3d.com/Manual/ManagedCodeDebugging.html\n\n### Debugging AWS GameKit C++ SDK on Windows\nFollow these instructions to debug the [AWS GameKit C++ SDK](https://github.com/aws/aws-gamekit) on Windows and step through from\nC# into C++ and back. You'll have two instances of Visual Studio open: one for C# and one for C++.\n\nOne-time setup:\n1. Follow the steps in section `Optional - Building the GameKit C++ SDK` above, ensure Debug binaries are being created instead of release.\n1. Open the AWS GameKit C++ SDK solution file in Visual Studio.\n1. Add the C++ debugging symbols to Visual Studio:\n    * Debug > Options > Debugging > Symbols > + (top right) >\n        * Add the full path to `/Packages/com.amazonaws.gamekit/Plugins/Windows/x64`\n\nEach time:\n1. Open the AWS GameKit C++ SDK solution file in Visual Studio.\n1. Attach the Visual Studio debugger to the Unity project. In Visual Studio:\n    * Debug > Attach to Process >\n        * Process: `Unity.exe`\n        * Title (Example): `aws-gamekit-unity - Untitled Scene - PC, Mac & Linux`\n1. Open AWS GameKit Unity in a separate Visual Studio window.\n1. Attach the debugger to Unity (see previous section titled `Debugging > Unity`).\n1. Set breakpoints.\n1. Trigger breakpoints via `Play Mode` or by interacting with AWS GameKit through the Unity editor.\n\n## Packaging\nRun the following commands in order to create the `com.amazonaws.gamekit-<version>.tgz` file:\n```\ncd path/to/aws-gamekit-unity\npython export_unitypackage.py UNITY_APPLICATION_FULL_PATH\n```\n\nWhere `UNITY_APPLICATION_FULL_PATH` is like:\n* Windows: `\"C:\\Program Files\\Unity\\Hub\\Editor\\2021.3.4f1\\Editor\\Unity.exe\"`\n* Mac: `/Applications/Unity/Hub/Editor/2021.3.4f1/Unity.app/Contents/MacOS/Unity`\n", "release_dates": ["2022-09-28T17:09:30Z", "2022-08-31T03:10:01Z"]}, {"name": "aws-gamekit-unreal", "description": "The AWS GameKit Plugin for Unreal", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS GameKit Plugin for Unreal\n\n## Dev Setup\n\n### Unreal/C++\n**The plugin is developed using Unreal version 5.0. Install before proceeding.**\n\nGame projects created in Unreal versions later than 5.0 are not backward compatible. AWS GameKit supports Unreal Engine version 5.0.\n\n#### Build\n\n##### Windows\n1. Create the Visual Studio Solution for your game. In File Explorer, right click on `[your_game].uproject` and select \"Generate Visual Studio project files\"\n\n2. Build [GameKit C++](https://github.com/aws/aws-gamekit) and copy over the DLL/PDB files. Make sure to checkout the matching version from the .gkcpp_version file in this repository. Note: this step is only needed if the plugin is being rebuilt. Prebuilt plugin for Windows, macOS, Android and iOS can be downloaded from this repository's [Releases](https://github.com/aws/aws-gamekit-unreal/releases).\n\n##### macOS\n1. Generate the Xcode workspace\n\n```\n/Users/Shared/Epic\\ Games/UE_5.0/Engine/Build/BatchFiles/Mac/GenerateProjectFiles.sh -project <path to your .uproject file>\n```\n\n2. Build [GameKit C++](https://github.com/aws/aws-gamekit) and copy over the libraries. Make sure to checkout the matching version from the .gkcpp_version file in this repository. Note: this step is only needed if the plugin is being rebuilt. Prebuilt plugin for Windows, macOS, Android and iOS can be downloaded from this repository's [Releases](https://github.com/aws/aws-gamekit-unreal/releases).\n\n##### Android and iOS\nDetailed steps for building and packaging a game for Android and iOS are available in the [Game Packaging section of our Production Readiness Guide](https://docs.aws.amazon.com/gamekit/latest/DevGuide/launch-package.html).\n\n### Using the Plugin\nCheck the updated [GUIDE](https://docs.aws.amazon.com/gamekit/latest/DevGuide/setting-up.html) for in-depth details about how to use AWS GameKit.\n", "release_dates": ["2022-09-29T22:38:24Z", "2022-05-05T16:29:25Z", "2022-03-16T23:37:59Z"]}, {"name": "aws-glue-databrew-jupyter-extension", "description": null, "language": "TypeScript", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# aws_glue_databrew_jupyter\n\nThis is an extension for Jupyter Lab that allows you to manage your AWS Glue Databrew resources in-context of your existing Jupyter workflows. \n\n### Prerequisites\n\n1. boto3 version 1.16.17 or greater\n1. botocore version 1.19.17 or greater\n1. configure the aws cli. https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n1. jupyter lab version 3.x\n\n### Installation instructions for Jupyter Lab\n\n1. run `pip install aws-jupyter-proxy`\n1. run `jupyter serverextension enable --py aws_jupyter_proxy`\n1. Search for aws_glue_databrew_jupyter in the Jupyter Lab plugin store and click install\n\n### Command-line installation instructions\n1. run `jupyter labextension install aws_glue_databrew_jupyter`\n1. Start jupyter lab: `jupyter lab`\n\n\n### Build and install instructions\n1. Install npm dependencies: `npm install`\n1. Build the extension: `npm run build`\n1. Install python dependencies: `pip install ./`\n1. Install the extension: `jupyter labextension install ./`\n1. Build jupyter lab assets: `jupyter lab build`\n1. Start jupyter lab in debug mode: `jupyter lab --debug`\n\n### Publishing new versions\n1. Update the version and push via https://docs.npmjs.com/updating-your-published-package-version-number\n1. tag the commit with a new version tag\n\n", "release_dates": ["2021-03-02T02:25:00Z", "2021-02-05T01:13:44Z", "2021-01-27T18:57:30Z", "2020-12-03T01:33:04Z"]}, {"name": "aws-glue-studio-notebook-jupyter-extension", "description": null, "language": "CSS", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWSGlueNotebookJupyterExtension\n\n![Github Actions Status](https://code.amazon.com/packages/AWSGlueNotebookJupyterExtension/trees/mainline/workflows/Build/badge.svg)\n\nA JupyterLab extension.\n\n\n\n## Requirements\n\n* JupyterLab >= 3.0\n\n## Install\n\nTo install the extension, execute:\n\n```bash\npip install AWSGlueNotebookJupyterExtension\n```\n\n## Uninstall\n\nTo remove the extension, execute:\n\n```bash\npip uninstall AWSGlueNotebookJupyterExtension\n```\n\n\n## Contributing\n\n### Development install\n\nNote: You will need NodeJS to build the extension package.\n\nThe `jlpm` command is JupyterLab's pinned version of\n[yarn](https://yarnpkg.com/) that is installed with JupyterLab. You may use\n`yarn` or `npm` in lieu of `jlpm` below.\n\n```bash\n# Clone the repo to your local environment\n# Change directory to the AWSGlueNotebookJupyterExtension directory\n# Install package in development mode\npip install -e .\n# Link your development version of the extension with JupyterLab\njupyter labextension develop . --overwrite\n# Rebuild extension Typescript source after making changes\njlpm run build\n```\n\nYou can watch the source directory and run JupyterLab at the same time in different terminals to watch for changes in the extension's source and automatically rebuild the extension.\n\n```bash\n# Watch the source directory in one terminal, automatically rebuilding when needed\njlpm run watch\n# Run JupyterLab in another terminal\njupyter lab\n```\n\nWith the watch command running, every saved change will immediately be built locally and available in your running JupyterLab. Refresh JupyterLab to load the change in your browser (you may need to wait several seconds for the extension to be rebuilt).\n\nBy default, the `jlpm run build` command generates the source maps for this extension to make it easier to debug using the browser dev tools. To also generate source maps for the JupyterLab core extensions, you can run the following command:\n\n```bash\njupyter lab build --minimize=False\n```\n\n### Development uninstall\n\n```bash\npip uninstall AWSGlueNotebookJupyterExtension\n```\n\nIn development mode, you will also need to remove the symlink created by `jupyter labextension develop`\ncommand. To find its location, you can run `jupyter labextension list` to figure out where the `labextensions`\nfolder is located. Then you can remove the symlink named `AWSGlueNotebookJupyterExtension` within that folder.\n", "release_dates": ["2023-09-20T16:25:00Z"]}, {"name": "aws-graviton-getting-started", "description": "Helping developers to use AWS Graviton2 and Graviton3 processors which power the 6th and 7th generation of Amazon EC2 instances (C6g[d], M6g[d], R6g[d], T4g, X2gd, C6gn, I4g, Im4gn, Is4gen, G5g, C7g[d][n], M7g[d], R7g[d]).", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# AWS Graviton Technical Guide\n\nThis repository provides technical guidance for users and developers using [Amazon EC2 instances powered by AWS Graviton processors](https://aws.amazon.com/ec2/graviton/) (including the latest generation Graviton3 processors). While it calls out specific features of the Graviton processors themselves, this repository is also generally useful for anyone running code on Arm-based systems.\n\n# Contents\n* [Transitioning to Graviton](#transitioning-to-graviton)\n* [Building for Graviton](#building-for-graviton2-graviton3-and-graviton3e)\n* [Optimizing for Graviton](optimizing.md)\n* [Taking advantage of Arm Advanced SIMD instructions](SIMD_and_vectorization.md)\n* [Recent software updates relevant to Graviton](#recent-software-updates-relevant-to-graviton)\n* Language-specific considerations\n\t* [C/C++](c-c++.md)\n\t* [Go](golang.md)\n\t* [Java](java.md)\n\t* [.NET](dotnet.md)\n\t* [PHP](php.md)\n\t* [Python](python.md)\n\t* [Rust](rust.md)\n* [Containers on Graviton](containers.md)\n* [Headless website testing with Chrome and Puppeteer on Graviton](software/ChromeAndPuppeteer.md)\n* [Lambda on Graviton](#lambda-on-graviton)\n* [Operating Systems support](os.md)\n* [Third-party Software Vendors](isv.md)\n* [Finding and managing AMIs for Graviton, with AWS SystemManager or CloudFormation](amis_cf_sm.md)\n* [DPDK, SPDK, and other datapath software](dpdk_spdk.md)\n* [PyTorch](machinelearning/pytorch.md)\n* [R](R.md)\n* [TensorFlow](machinelearning/tensorflow.md)\n* [Spark on Graviton](DataAnalytics.md)\n* [Known issues and workarounds](#known-issues-and-workarounds)\n* [AWS Managed Services available on Graviton](managed_services.md)\n* [Graviton Performance Runbook](perfrunbook/README.md)\n* [Assembly Optimization Guide for Graviton Arm64 Processors](arm64-assembly-optimization.md)\n* [Additional resources](#additional-resources)\n* [How To Resources](howtoresources.md)\n\n# Transitioning to Graviton\nIf you are new to Graviton and want to understand how to identify target workloads, how to plan a transition project, how to test your workloads on AWS Graviton and finally how deploy in production, please read [the key considerations to take into account when transitioning workloads to AWS Graviton based Amazon EC2 instances](transition-guide.md).\n\n# Building for Graviton2, Graviton3 and Graviton3E\n\n|Processor\t|Graviton2\t|Graviton3(E)\t|\n|---\t|---\t|---\t|\n|Instances\t|[M6g/M6gd](https://aws.amazon.com/ec2/instance-types/m6g/), [C6g/C6gd/C6gn](https://aws.amazon.com/ec2/instance-types/c6g/), [R6g/R6gd](https://aws.amazon.com/ec2/instance-types/r6g/), [T4g](https://aws.amazon.com/ec2/instance-types/t4g), [X2gd](https://aws.amazon.com/ec2/instance-types/x2/), [G5g](https://aws.amazon.com/ec2/instance-types/g5g/), and [I4g/Im4gn/Is4gen](https://aws.amazon.com/ec2/instance-types/i4g/)\t|[C7g/C7gd/C7gn](https://aws.amazon.com/ec2/instance-types/c7g/), [M7g/M7gd](https://aws.amazon.com/ec2/instance-types/m7g/), [R7g/R7gd](https://aws.amazon.com/ec2/instance-types/r7g/), and [Hpc7g](https://aws.amazon.com/ec2/instance-types/)\t|\n|Core\t|[Neoverse-N1](https://developer.arm.com/documentation/100616/0301)\t|[Neoverse-V1](https://developer.arm.com/documentation/101427/latest/)\t|\n|Frequency\t|2500MHz\t|2600MHz\t|\n|Turbo supported\t|No\t|No\t|\n|Software Optimization Guide (Instruction Throughput and Latency)|[SWOG](https://developer.arm.com/documentation/pjdoc466751330-9707/latest/)|[SWOG](https://developer.arm.com/documentation/pjdoc466751330-9685/latest/) |\n|Interconnect\t|CMN-600\t|CMN-650\t|\n|Architecture revision\t|ARMv8.2-a\t|ARMv8.4-a\t|\n|Additional  features\t|fp16, rcpc, dotprod, crypto\t|sve, rng, bf16, int8, crypto\t|\n|Recommended `-mcpu` flag\t([more information](c-c++.md#enabling-arm-architecture-specific-features))|`neoverse-n1` | `neoverse-512tvb` |\n|RNG Instructions\t|No\t|Yes\t|\n|SIMD instructions\t|2x Neon 128bit vectors\t|4x Neon 128bit vectors / 2x SVE 256bit\t|\n|LSE (atomic mem operations)\t|yes\t|yes\t|\n|Pointer Authentication\t|no\t|yes\t|\n|Cores\t|64\t|64\t|\n|L1 cache (per core)\t|64KB inst / 64KB data\t|64KB inst / 64KB data\t|\n|L2 cache (per core)\t|1MB\t|1MB\t|\n|LLC (shared)\t|32MB\t|32MB\t|\n|DRAM\t|8x DDR4\t|8x DDR5\t|\n|DDR Encryption\t|yes\t|yes\t|\n\n# Optimizing for Graviton\nPlease refer to [optimizing](optimizing.md) for general debugging and profiling information.  For detailed checklists on optimizing and debugging performance on Graviton, see our [performance runbook](perfrunbook/README.md).\n\nDifferent architectures and systems have differing capabilities, which means some tools you might be familiar with on one architecture don't have equivalent on AWS Graviton. Documented [Monitoring Tools](Monitoring_Tools_on_Graviton.md) with some of these utilities.\n\n# Recent software updates relevant to Graviton\nThere is a huge amount of activity in the Arm software ecosystem and improvements are being\nmade on a daily basis. As a general rule later versions of compilers, language runtimes, and applications\nshould be used whenever possible. The table below includes known recent changes to popular\npackages that improve performance (if you know of others please let us know).\n\nPackage | Version | Improvements\n--------|:-:|-------------\nbazel\t| [3.4.1+](https://github.com/bazelbuild/bazel/releases) | Pre-built bazel binary for Graviton/Arm64. [See below](#bazel-on-linux) for installation.\nCassandra | 4.0+ | Supports running on Java/Corretto 11, improving overall performance\nFFmpeg  | 6.0+ | Improved performance of libswscale by 50% with better NEON vectorization which improves the performance and scalability of FFmpeg multi-threaded encoders. The changes are available in FFmpeg version 4.3, with further improvements to scaling and motion estimation available in 5.1. Additional improvements to both are available in 6. For encoding h.265, build with the master branch of x265 because the released version of 3.5 does not include important optimizations for Graviton. For more information about FFmpeg on Graviton, read the blog post on AWS Open Source Blog, [Optimized Video Encoding with FFmpeg on AWS Graviton Processors](https://aws.amazon.com/blogs/opensource/optimized-video-encoding-with-ffmpeg-on-aws-graviton-processors/).\nHAProxy  | 2.4+  | A [serious bug](https://github.com/haproxy/haproxy/issues/958) was fixed. Additionally, building with `CPU=armv81` improves HAProxy performance by 4x so please rebuild your code with this flag.\nMariaDB | 10.4.14+ | Default build now uses -moutline-atomics, general correctness bugs for Graviton fixed.\nmongodb | 4.2.15+ / 4.4.7+ / 5.0.0+ | Improved performance on graviton, especially for internal JS engine. LSE support added in [SERVER-56347](https://jira.mongodb.org/browse/SERVER-56347).\nMySQL   | 8.0.23+ | Improved spinlock behavior, compiled with -moutline-atomics if compiler supports it.\nPostgreSQL | 15+ | General scalability improvements plus additional [improvements to spin-locks specifically for Arm64](https://commitfest.postgresql.org/37/3527/)\n.NET | [5+](https://dotnet.microsoft.com/download/dotnet/5.0) | [.NET 5 significantly improved performance for ARM64](https://devblogs.microsoft.com/dotnet/Arm64-performance-in-net-5/). Here's an associated [AWS Blog](https://aws.amazon.com/blogs/compute/powering-net-5-with-aws-graviton2-benchmark-results/) with some performance results. \nOpenH264 | [2.1.1+](https://github.com/cisco/openh264/releases/tag/v2.1.1) | Pre-built Cisco OpenH264 binary for Graviton/Arm64. \nPCRE2   | 10.34+  | Added NEON vectorization to PCRE's JIT to match first and pairs of characters. This may improve performance of matching by up to 8x. This fixed version of the library now is shipping with Ubuntu 20.04 and PHP 8.\nPHP     | 7.4+    | PHP 7.4 includes a number of performance improvements that increase perf by up to 30%\npip     | 19.3+   | Enable installation of python wheel binaries on Graviton\nPyTorch | 2.0+    | Optimize Inference latency and throughput on Graviton. [AWS DLCs and python wheels are available](machinelearning/pytorch.md).\nruby    | 3.0+ | Enable arm64 optimizations that improve performance by as much as 40%. These changes have also been back-ported to the Ruby shipping with AmazonLinux2, Fedora, and Ubuntu 20.04.\nSpark | 3.0+ | Supports running on Java/Corretto 11, improving overall performance.\nzlib    | 1.2.8+  | For the best performance on Graviton please use [zlib-cloudflare](https://github.com/cloudflare/zlib).\n\n# Containers on Graviton\nYou can run Docker, Kubernetes, Amazon ECS, and Amazon EKS on Graviton. Amazon ECR supports multi-arch containers.\nPlease refer to [containers](containers.md) for information about running container-based workloads on Graviton.\n\n# [Lambda on Graviton](/aws-lambda/README.md)\n[AWS Lambda](https://aws.amazon.com/lambda/) now allows you to configure new and existing functions to run on Arm-based AWS Graviton2 processors in addition to x86-based functions. Using this processor architecture option allows you to get up to 34% better price performance. Duration charges are 20 percent lower than the current pricing for x86 with [millisecond granularity](https://aws.amazon.com/blogs/aws/new-for-aws-lambda-1ms-billing-granularity-adds-cost-savings/). This also applies to duration charges when using [Provisioned Concurrency](https://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/). Compute [Savings Plans](https://aws.amazon.com/blogs/aws/savings-plan-update-save-up-to-17-on-your-lambda-workloads/) supports Lambda functions powered by Graviton2.\n\nThe [Lambda](/aws-lambda/README.md) page highlights some of the migration considerations and also provides some simple to deploy demos you can use to explore how to build and migrate to Lambda functions using Arm/Graviton2.\n\n# Operating Systems\n\nPlease check [os.md](os.md) for more information about which operating system to run on Graviton based instances.\n\n# Known issues and workarounds\n\n## Postgres\nPostgres performance can be heavily impacted by not using [LSE](https://github.com/aws/aws-graviton-getting-started/blob/main/c-c%2B%2B.md#large-system-extensions-lse).\nToday, postgres binaries from distributions (e.g. Ubuntu) are not built with `-moutline-atomics` or `-march=armv8.2-a` which would enable LSE.  Note: Amazon RDS for PostgreSQL isn't impacted by this. \n\nIn November 2021 PostgreSQL started to distribute Ubuntu 20.04 packages optimized with `-moutline-atomics`.\nFor Ubuntu 20.04, we recommend using the PostgreSQL PPA instead of the packages distributed by Ubuntu Focal.\nPlease follow [the instructions to set up the PostgreSQL PPA.](https://www.postgresql.org/download/linux/ubuntu/)\n\n## Python installation on some Linux distros\nThe default installation of pip on some Linux distributions is old \\(<19.3\\) to install binary wheel packages released for Graviton.  To work around this, it is recommended to upgrade your pip installation using:\n```\nsudo python3 -m pip install --upgrade pip\n```\n\n## Bazel on Linux\nThe [Bazel build tool](https://www.bazel.build/) now releases a pre-built binary for arm64. As of October 2020, this is not available in their custom Debian repo, and Bazel does not officially provide an RPM. Instead, we recommend using the [Bazelisk installer](https://docs.bazel.build/versions/master/install-bazelisk.html), which will replace your `bazel` command and [keep bazel up to date](https://github.com/bazelbuild/bazelisk/blob/master/README.md).\n\nBelow is an example using the [latest Arm binary release of Bazelisk](https://github.com/bazelbuild/bazelisk/releases/latest) as of October 2020:\n```\nwget https://github.com/bazelbuild/bazelisk/releases/download/v1.7.1/bazelisk-linux-arm64\nchmod +x bazelisk-linux-arm64\nsudo mv bazelisk-linux-arm64 /usr/local/bin/bazel\nbazel\n```\n\nBazelisk itself should not require further updates, as its only purpose is to keep Bazel updated.\n\n## zlib on Linux\nLinux distributions, in general, use the original zlib without any optimizations. zlib-cloudflare has been updated to provide better and faster compression on Arm and x86. To use zlib-cloudflare:\n```\ngit clone https://github.com/cloudflare/zlib.git\ncd zlib\n./configure --prefix=$HOME\nmake\nmake install\n```\nMake sure to have the full path to your lib at $HOME/lib in /etc/ld.so.conf and run ldconfig.\n\nFor users of OpenJDK, which is dynamically linked to the system zlib, you can set LD_LIBRARY_PATH to point to the directory where your newly built version of zlib-cloudflare is located or load that library with LD_PRELOAD.\n\nYou can check the libz that JDK is dynamically linked against with:\n```\n$ ldd /Java/jdk-11.0.8/lib/libzip.so | grep libz\nlibz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007ffff7783000)\n```\n\nCurrently, users of Amazon Corretto cannot link against zlib-cloudflare.\n\n# Additional resources\n\n * [AWS Graviton](https://aws.amazon.com/ec2/graviton/)\n * [Neoverse N1 Software Optimization Guide](https://developer.arm.com/documentation/pjdoc466751330-9707/latest)\n * [Armv8 reference manual](https://documentation-service.arm.com/static/60119835773bb020e3de6fee)\n * [Package repository search tool](https://pkgs.org/)\n\n**Feedback?** ec2-arm-dev-feedback@amazon.com\n", "release_dates": []}, {"name": "aws-greengrass-core-sdk-c", "description": "SDK to use with functions running on Greengrass Core using C", "language": "C", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Greengrass Core C SDK - README.md\n\n## Overview\n\nThe AWS Greengrass Core SDK for C provides an interface to interact with the Greengrass Core system on the edge. It is c89 compliant and is meant to be performant while minimizing dependencies.\n\nAWS Greengrass Core SDK for C is now in General Availability.\n\n## Requirements\n* libc 2.14+\n* cmake 2.8+\n* Greengrass Core 1.6+\n\n## Building the SDK\n\nClone the SDK into local workspace:\n\n```\ngit clone https://github.com/aws/aws-greengrass-core-sdk-c.git\n```\n\nBuild the SDK:\n\n```\ncd aws-greengrass-core-sdk-c\nmkdir build && cd build\ncmake ..\ncmake --build .\n```\n\nThe build will produce a shared object named **libaws-greengrass-core-sdk-c.so** under the build/aws-greengras-core-sdk-c directory. This is the shared object that the Lambda executable links to.\n\n**Note:**\n  - The shared object is a stub implementation that helps Lambda executables to link against. It will be overridden by the actual shared object that comes with the Greengrass Core release bundle.\n  - The `-Wl,--enable-new-dtags` flag is needed for adding the Greengrass C SDK shared object path into the Lambda executable's RUNPATH, so that the stub shared object can be overridden by the **libaws-greengrass-core-sdk-c.so** which comes along with the Greengrass Core Release bundle. It is automatically included when linking to the aws-greengrass-core-sdk-c.\n\n## Building Greengrass Native Lambda Executables with CMake\nYou can use CMake to build Greengrass Lambda executables. Other build tools could work as well.\nHere cmake is shown as an example:\n\n### Setting up a CMake Project\nCreate a directory to hold your project:\n\n```\nmkdir my_gg_native_function\n```\n\nOpen the directory and add a `CMakeLists.txt` file that specifies your project's name, executables, source files, and linked libraries. The following is a minimal example:\n\n```\n# minimal CMakeLists.txt for the AWS Greengrass SDK for C\ncmake_minimum_required(VERSION 2.8)\n# \"my_gg_native_function\" is just an example value.\nproject(my_gg_native_function)\n\n# Locate the AWS Greengras SDK for C package.\n# Requires that you build with:\n#   -Daws-greengrass-core-sdk-c_DIR=/path/to/sdk_build\n# or export/set:\n#   CMAKE_PREFIX_PATH=/path/to/sdk_build\nfind_package(aws-greengrass-core-sdk-c REQUIRED)\n\nadd_executable(my_gg_native_funtion main.cpp)\ntarget_link_libraries(my_gg_native_funtion aws-greengrass-core-sdk-c)\n```\n\nTo get started quickly, there are several pre-made examples in the aws-greengras-core-sdk-c-example directory for your reference. Those examples are built along with the SDK.\n\n## Creating Deployment Package\nAfter the Lambda executable is built, it should be packaged into a deployment package, which is a zip file consisting of your executable and any dependencies. Then you can upload the package to AWS Lambda and deploy it to a device by following the normal AWS Greengrass process.\n\n## Using the SDK\n### Initialization\nAll code that uses the AWS Greengrass SDK for C must do **gg_global_init()** before calling any other APIs and **gg_runtime_start()** to start the runtime, as follows:\n```\n#include \"greengrasssdk.h\"\n\nvoid handler(const gg_lambda_context *cxt) {\n    /* Your lambda logic goes here. */\n}\n\nint main() {\n    gg_global_init(0);\n    /* start the runtime in blocking mode. This blocks forever. */\n    gg_runtime_start(handler, 0);\n}\n```\n\n### Reading Event Payload\nTo read the event payload for the Lambda handler to process, use **gg_lambda_handler_read()**. The amount of data written into the buffer is not guaranteed to be complete until **amount_read** is zero.\n\nThe following is a sample usage of the method:\n\n```\n/* loop read handler event into buffer. */\ngg_error loop_lambda_handler_read(void *buffer,\n        size_t buffer_size, size_t *total_read) {\n    gg_error err = GGE_SUCCESS;\n    uint8_t *read_index = (uint8_t*)buffer;\n    size_t remaining_buf_size = buffer_size;\n    size_t amount_read = 0;\n\n    do {\n        err = gg_lambda_handler_read(read_index, remaining_buf_size,\n                &amount_read);\n        if(err) {\n            gg_log(GG_LOG_ERROR, \"gg_lambda_handler_read had an error\");\n            goto cleanup;\n        }\n        *total_read += amount_read;\n        read_index += amount_read;\n        remaining_buf_size -= amount_read;\n    } while(amount_read);\n\ncleanup:\n    return err;\n}\n```\n### Writing Handler Response\nAfter the Lambda handler finishes processing, a response can be returned using **gg_lambda_handler_write_response()**. If this method is not called, an empty response will be returned.\n\n### Writing Handler Error Response\nWhen the Lambda handler encounters any error, the error response can be returned to the caller using **gg_lambda_handler_write_error()**.\n\n### Initialize API Request\nEvery API request must be initialized using **gg_request_init()** before making the actual request.\n\n### Reading API Response\nThere are several cases which require reading a response after an API call, such as **gg_invoke()**, **gg_get_thing_shadow()**, etc. **gg_request_read()** should be used after the method call to retrieve the output response.\n\nThe following is a sample usage of the method:\n\n```\n/* loop read request bytes into buffer. */\ngg_error loop_request_read(gg_request ggreq, void *buffer,\n        size_t buffer_size, size_t *total_read) {\n    gg_error err = GGE_SUCCESS;\n    uint8_t *read_index = (uint8_t*)buffer;\n    size_t remaining_buf_size = buffer_size;\n    size_t amount_read = 0;\n\n    do {\n        err = gg_request_read(ggreq, read_index, remaining_buf_size,\n                &amount_read);\n        if(err) {\n            gg_log(GG_LOG_ERROR, \"gg_lambda_handler_read had an error\");\n            goto cleanup;\n        }\n        *total_read += amount_read;\n        read_index += amount_read;\n        remaining_buf_size -= amount_read;\n    } while(amount_read);\n\ncleanup:\n    return err;\n}\n```\n\n### Error Handling\nWhen there is an error on method call, all the APIs have **gg_error** returned as the return code. And for **gg_publish_with_options()**, **gg_publish()**, **gg_invoke()** and **gg_xxx_thing_shadow()** APIs, you can check server side error from request status from the **gg_request_result()** struct.\n\n## Opening Issues\nIf you encounter a bug with the AWS Greengrass SDK for C, we would like to hear about it. Search the [existing issues](https://github.com/aws/aws-greengrass-core-sdk-c/issues) and see if others are also experiencing the issue before opening a new issue. When creating issue, please fill in the following template:\n```\n[Problem Description]:  Describe the problem in detail\n[Reproduction Steps]: Describe the steps in detail\n[Target OS/Architecture]: eg. x86_64 Ubuntu 16.04\n[Greengrass Core Version]: eg. 1.6.0\n[Greengrass Core SDK for C Version]: eg. 1.0.0\n[Compiler and Version] : eg. GCC 5.0 / LLVM 7\n[Cmake Version] eg. Cmake 3.2\n[Logs] runtime.log, lambda log\n```\n", "release_dates": ["2019-07-15T20:53:10Z"]}, {"name": "aws-greengrass-core-sdk-java", "description": "Greengrass Java SDK", "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Greengrass Core SDK for Java\n\nThe **AWS Greengrass Core SDK for Java** enables Java developers to develop Lambda functions which will run within Greengrass.\n\n## Overview\n\nThis document provides instructions for preparing your Greengrass Core environment to run Lambda functions written in Java. It also includes examples on how to develop a Lambda function in Java as well as packaging and running an example Hello World file in Java for your Greengrass core.\n\n## Changes to 1.5.0\n*  Stream manager supports automatic data export to AWS S3 and AWS IoT SiteWise, provides new API method to update existing streams, and pause or resume exporting.\n\n## Changes to 1.4.1\n\n*   StreamManager client sets socket option `TCP_NODELAY=true` to prevent multi-millisecond delays when writing small messages.\n\n## Changes to 1.4.0\n\n*   SDK supports StreamManager client.\n\n## Changes to 1.3.1\n\n*   Improved log level granularity.\n\n## Changes to 1.3.0\n\n*   SDK supports SecretsManager client.\n\n## Changes to 1.2.0\n\n*   SDK and GGC compatibility check takes place in the background.\n\n## Changes to 1.1.0\n\n*   You can now invoke lambda with binary data type. Please refer to the examples folder.\n\n## Preparing your Greengrass to run Java Lambda functions\n\nThe environment where Greengrass is running on needs to be able to run Java 8 packages.\n\n*   Install Java 8 for your platform. The method will be different based on your platform.\n*   Installation of Java 8 will create _**java**_ or _**java8**_ executable in _**/usr/bin**_ or _**/usr/local/bin**_.\n*   If the file name is _**java**_, rename it or copy it as _**java8**_ in _**/usr/bin**_ or _**/usr/local/bin**_ folder.\n*   Make sure the file is not a symlink.\n\n## Getting Started - Hello World\n\n*   Copy `samples/HelloWorld` folder to your workspace.\n*   Create `libs` folder within `HelloWorld` folder and copy `GreengrassJavaSDK.jar`/`GreengrassJavaSDK-Slim.jar` file from `sdk`/`slim-sdk` folder into \n    the `libs` folder.\n*   Run `gradle build`\n*   You should see a `HelloWorld.zip` in `build/distributions` folder.\n*   Go to AWS Lambda Console.\n*   Create a new function.\n*   Choose the Runtime as `Java 8`.\n*   Upload `HelloWorld.zip` file in _Lambda function code_ section.\n*   Handler is `com.amazonaws.greengrass.examples.HelloWorld::handleRequest`. The format of handler for java functions is `package.class::method-reference`.\n*   Choose any role as the role is not used within Greengrass.\n*   After creating the function, publish the Lambda.\n*   Create an Alias and point to the Published version (not $LATEST).\n*   Go to your Greengrass Group and add the Lambda under Lambdas section.\n*   Click on the Lambda that was just added and modify the configuration.\n    *   Change the _Lambda lifecycle_ to _Make this function long-lived and keep it running indefinitely._.\n    *   Change the _Memory limit_ to at least _64 MB_.\n*   Add a Subscription with the following configuration:\n    *   Source: Lambda which you just created and added to the group.\n    *   Target: IoT Cloud\n    *   Topic: hello/world\n*   Deploy. A message from your Lambda should be published to the topic _hello/world_ in the cloud every 5 seconds. You can check this by going to AWS IoT's _Test_ page and subscribing to topic _hello/world_.\n\n## Creating a .zip Deployment Package\n\nYou can use any building and packaging tool you like to create this zip. Regardless of the tools you use, the resulting .zip file must have the following structure:\n\n*   All compiled class files and resource files at the root level.\n*   All required jars to run the code in the `/lib` directory.\n\nAll the examples and instructions in this manual use Gradle build and deployment tool to create the .zip.\n\n### Downloading Gradle\n\nYou will need to download Gradle. For instructions, go to the gradle website, [https://gradle.org/](https://gradle.org)\n\n### Including Greengrass Core SDK for Java with your function with Gradle\n\nTwo types of jar files are provided. The `GreengrassJavaSDK-Slim.jar` in `sdk-slim` directory is a jar file with \nno dependencies built in. This is the recommended way to consume Greengrass Core SDK. `GreengrassJavaSDK.jar` in \n`sdk` directory is provided for backward compatibility for developers already using it.\n\nFor `GreengrassJavaSDK-Slim.jar`, follow the example below:\n\n*   Create `libs` folder.\n*   Copy `GreengrassJavaSDK-Slim.jar` to `libs` folder.\n*   Example `build.gradle` file for Greengrass function looks like the following. You may add additional dependencies as necessary for your function.  \n\n    ```java  \n\n    repositories {  \n    \u00a0\u00a0\u00a0\u00a0mavenCentral()  \n    }  \n\n    dependencies {\n    \u00a0\u00a0\u00a0\u00a0compile 'com.fasterxml.jackson.core:jackson-annotations:2.12.3'\n    \u00a0\u00a0\u00a0\u00a0compile 'com.fasterxml.jackson.core:jackson-core:2.12.3'\n    \u00a0\u00a0\u00a0\u00a0compile 'com.fasterxml.jackson.core:jackson-databind:2.12.3'\n    \u00a0\u00a0\u00a0\u00a0compile 'com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.12.3'\n    \u00a0\u00a0\u00a0\u00a0compile 'org.apache.httpcomponents:httpclient:4.5.13'\n    \u00a0\u00a0\u00a0\u00a0compile 'org.apache.httpcomponents:httpcore:4.4.14'\n    \u00a0\u00a0\u00a0\u00a0compile 'com.amazonaws:aws-lambda-java-core:1.1.0'\n    \u00a0\u00a0\u00a0\u00a0compile 'javax.validation:validation-api:1.0.0.GA'\n    \u00a0\u00a0\u00a0\u00a0compile 'org.slf4j:slf4j-api:1.7.0'\n    \u00a0\u00a0\u00a0\u00a0compile fileTree(dir: 'libs', include: ['*.jar'])  \n    }  \n\n    task buildZip(type: Zip) {  \n    \u00a0\u00a0\u00a0\u00a0from compileJava  \n    \u00a0\u00a0\u00a0\u00a0from processResources  \n    \u00a0\u00a0\u00a0\u00a0into('lib') {  \n    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0from configurations.runtime  \n    \u00a0\u00a0\u00a0\u00a0}  \n    }  \n\n    build.dependsOn buildZip  \n\n    ```\n\n*   Place your lambda function under `src` folder.\n*   Run `gradle build`\n*   This should place a zipped file of your function under `build/distributions` folder which you can now upload to AWS Lambda to be used by Greengrass.\n\n## Logging in Java Lambdas\n\nYour _System.out.println_ operation will be logged as INFO. A _System.err.println_ operation will be logged as ERROR. Alternatively, you can also log using `context.getLogger().log` operation which will log at INFO level. Currently, our Java SDK only allows you to log at INFO or ERROR level only.\n\n## Supported Datatypes\n\nFrom GGC version 1.5.0, you can send binary data with the SDK. However, in order to make a lambda function be able to handle binary payload. You need to do the following:\n\n*   Make sure to choose \"binary\" input payload type in lamba configuration page in Greengrass condole and then do a deployment. Your lambda function will be marked as a \"binary\" lambda by GGC.\n*   Make sure your lambda handler signature is one of the following:\n    ```java  \n    void (InputStream, OutputStream, Context)  \n    void (InputStream, OutputStream)  \n    void (OutputStream, Context)  \n    void (InputStream, Context)  \n    void (InputStream)  \n    void (OutputStream)\n    ```\n\n## Supported Function Signatures\n\nIn addition to the function signatures mentioned above, there are also more supported function signatures being introduced:\n\nSupported \"json\" function handler signatures:\n```java\n    Anything (Context)  \n    Anything (AlmostAnything, Context)  \n    Anything (AlmostAnything)  \n    Anything ()\n```\n\n## Handler Overload Resolution\n\nIf your Java code contains multiple methods with same name as the handler name, then GGC uses the following rules to pick a method to invoke:\n\n*   Select the method with the largest number of parameters.\n*   If two or more methods have the same number of parameters, GGC selects the method that has the Context as the last parameter.\n\n## Supported Context\n\nIn Greengrass, you can send a context object in a JSON format to be passed to another Lambda that is being invoked. The context format looks like this: { custom: { customData: 'customData', }, }\n\n<div class=\"section\" id=\"compatibility\">\n\n## Compatibility\n\nAs new features are added to AWS IoT Greengrass, newer versions of the AWS IoT Greengrass SDK may be incompatible with older versions of the AWS IoT Greengrass core. The following table lists the compatible SDKs for all GGC releases.\n\n| GGC Version   | Compatible SDK Versions |\n| ------------- | ------------- |\n| 1.0.x-1.6.x   | 1.0.x         |\n| 1.7.x-1.9.x   | 1.0.x-1.3.x   |\n| 1.10.x        | 1.0.x-1.4.x   |\n| 1.11.x        | 1.0.x-1.5.x   |\n", "release_dates": ["2021-05-19T22:09:49Z", "2020-02-19T21:07:12Z", "2020-01-14T19:59:07Z", "2019-07-15T20:45:35Z"]}, {"name": "aws-greengrass-core-sdk-js", "description": "Greengrass Nodejs SDK", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS Greengrass Core SDK for JavaScript\n\nThe **AWS Greengrass Core SDK for JavaScript** allows developers to write JavaScript Lambda functions which will run within Greengrass.\n\n## Overview\n\nThis document provides instructions for preparing your Greengrass Core environment to run Lambda functions written in JavaScript. It also includes examples on how to develop a Lambda function in JavaScript as well as packaging and running an example Hello World file in JavaScript for your Greengrass core.\n\n## Preparing your Greengrass to run NodeJS Lambda functions\n\nThe environment where Greengrass is running on needs to be able to run NodeJS 12.x applications.\n\n*   Install NodeJS 12 for your platform. You can download the newest NodeJS from [https://nodejs.org/en/download/](https://nodejs.org/en/download/).\n*   When you untar the package downloaded from NodeJS website, you will find `node` file under `bin` directory.\n*   Copy the file to _**/usr/bin**_ or _**/usr/local/bin**_ folder.\n*   Rename the file to _**nodejs12.x**_\n\n## Getting Started - Hello World\n\n*   Copy `greengrassExamples/HelloWorld` folder to your workspace.\n*   Create a folder `node_modules` under `HelloWorld` folder.\n*   Unzip aws-greengrass-core-sdk-js.zip into the folder. It should create a folder HelloWorld/node_modules/aws-greengrass-core-sdk\n*   Use NPM to install the required dependency, cbor. `npm i cbor@5.0.1`.\n*   Zip up the content of HelloWorld folder so that the index.js is on the top of the zip file structure.\n*   Go to AWS Lambda Console.\n*   Create a new function.\n*   Choose the Runtime as `Node.js 12.x`\n*   Upload the zip file in _Lambda function code_ section.\n*   Handler is _index.handler_\n*   Choose any role as the role is not used within Greengrass.\n*   After creating the function, publish the Lambda.\n*   Create an Alias and point to the Published version (not $LATEST).\n*   Go to your Greengrass Group and add the Lambda under Lambdas section.\n*   Click on the Lambda and change the _Lambda lifecycle_ to _Make this function long-lived and keep it running indefinitely._\n*   Add a Subscription with the following configuration:\n    *   Source: Lambda which you just created and added to the group\n    *   Target: IoT Cloud\n    *   Topic: hello/world\n*   Deploy. A message from your Lambda should be published to the topic _hello/world_ in the cloud every 5 seconds. You can check this by going to AWS IoT's _Test_ page and subscribing to topic _hello/world_.\n\n## Including aws-greengrass-core-sdk with your function\n\nUnzip the SDK into your node_modules folder of your function. This should create a aws-greengrass-core-sdk folder which includes the SDK.\n\n## Logging in NodeJS Lambdas\n\nYour _console.log_ operation will be logged as INFO. A _console.error_ operation will be logged as ERROR. Currently, our NodeJS SDK only allows you to log at INFO or ERROR level only.\n\n## Supported Datatypes\n\nFrom GGC version 1.5, you can send both JSON and binary data as a payload when you invoking other Lambdas or publishing a message using IotData service. In order to make your lambda be able to handle binary payload, you need to configure the lambda in Greengrass console to mark it using binary input payload so that GGC can know how to deal with the data.\n\n## Supported Context\n\nIn Greengrass, you can send a context object in a JSON format to be passed to another Lambda that is being invoked. The context format looks like this: `{ custom: { customData: 'customData', }, }`\n\n<div class=\"section\" id=\"compatibility\">\n\n## Compatibility[\u00b6](#compatibility \"Permalink to this headline\")\n\nAs new features are added to AWS IoT Greengrass, newer versions of the AWS IoT Greengrass SDK may be incompatible with older versions of the AWS IoT Greengrass core. The following table lists the compatible SDKs for all GGC releases.\n\n\n<table style=\"width:50%\">\n\n<tbody>\n\n<tr>\n\n<th>GGC Version</th>\n\n<th>Compatible SDK Versions</th>\n\n</tr>\n\n<tr>\n\n<td>1.0.x-1.6.x</td>\n\n<td>1.0.x-1.2.x</td>\n\n</tr>\n\n<tr>\n\n<td>1.7.x-1.8.x</td>\n\n<td>1.0.x-1.3.x</td>\n\n</tr>\n\n<tr>\n\n<td>1.9.x</td>\n\n<td>1.0.x-1.4.x</td>\n\n</tr>\n\n<tr>\n\n<td>1.10.x</td>\n\n<td>1.0.x-1.6.x</td>\n\n</tr>\n\n<tr>\n\n<td>1.11.x</td>\n\n<td>1.0.x-1.7.x</td>\n\n</tr>\n\n</tbody>\n\n</table>\n\n</div>\n\n<div class=\"Section\" id=\"1.7.0updates\">\n\n## 1.7.0 Updates[\u00b6](#1.7.0updates \"Permalink to this headline\")\n\nStream manager supports automatic data export to AWS S3 and AWS IoT SiteWise, provides new API method to update existing streams, and pause or resume exporting.\n\n</div>\n\n<div class=\"Section\" id=\"1.6.0updates\">\n\n## 1.6.0 Updates[\u00b6](#1.6.0updates \"Permalink to this headline\")\n\nAdded support for StreamManager, see [AWS Docs](https://docs.aws.amazon.com/greengrass/latest/developerguide/stream-manager.html)\nfor more information.\n\n### Compatibility\n\nStreamManager has adds a new dependency to this package, `cbor==5.0.1`. \nPlease make sure to include it in your lambda or else the SDK will not function.\nIn addition to the new dependency, the 1.6.0 version of this SDK now requires NodeJS version 12\nor greater since NodeJS 6 and 8 are end-of-life. See [https://docs.aws.amazon.com/lambda/latest/dg/runtime-support-policy.html](https://docs.aws.amazon.com/lambda/latest/dg/runtime-support-policy.html).\n\n### StreamManager Usage\n\n```javascript\nconst {\n    StreamManagerClient\n} = require('aws-greengrass-core-sdk').StreamManager;\n\nconst c = new StreamManagerClient({\n    onConnectCb: async () => {\n        try {\n            // Let's start with something simple.\n            // Just print out all the available stream names on the server \n            console.log(await c.listStreams());\n        } finally {\n            c.close(); // Always close the client when you're done\n        }\n    }\n});\n```\n\n</div>\n\n<div class=\"Section\" id=\"1.5.0updates\">\n\n## 1.5.0 Updates[\u00b6](#1.5.0updates \"Permalink to this headline\")\n\nAdded support for publish() parameter queueFullPolicy which can be set to 'AllOrError' to enforce that the published message is either delivered to all subscription destinations or delivered to no destinations and returns an error when Greengrass Core's internal work queue is full.\n\n</div>\n\n<div class=\"Section\" id=\"1.4.0updates\">\n\n## 1.4.0 Updates[\u00b6](#1.4.0updates \"Permalink to this headline\")\n\nAdded support for Node.js 8.10 Lambda runtime. Lambda functions that use Node.js 8.10 runtime can now run on an AWS IoT Greengrass core. (Existing Lambda functions that use Node.js 6.10 runtime can still run on Greengrass core, but they can't be updated after 5/30/2019. Please refer to [AWS Lambda Runtimes Support Policy](https://docs.aws.amazon.com/lambda/latest/dg/runtime-support-policy.html).)\n\n</div>\n\n<div class=\"Section\" id=\"1.3.1updates\">\n\n## 1.3.1 Updates[\u00b6](#1.3.1updates \"Permalink to this headline\")\n\nImproved log level granularity.\n\n</div>\n\n<div class=\"Section\" id=\"1.3.0updates\">\n\n## 1.3.0 Updates[\u00b6](#1.3.0updates \"Permalink to this headline\")\n\nSDK supports SecretsManager client.\n\n</div>\n\n<div class=\"Section\" id=\"1.2.0updates\">\n\n## 1.2.0 Updates[\u00b6](#1.2.0updates \"Permalink to this headline\")\n\nSDK and GGC compatibility check takes place in the background.\n\n</div>\n\n<div class=\"Section\" id=\"1.1.0updates\">\n\n## 1.1.0 Updates[\u00b6](#1.1.0updates \"Permalink to this headline\")\n\nLambda only accepted payload in JSON format. With this update, Invoking or publishing binary payload to a lambda is supported.\n\n</div>\n\n<div class=\"Section\" id=\"1.0.1updates\">\n\n## 1.0.1 Updates[\u00b6](#1.0.1updates \"Permalink to this headline\")\n\nShadow operations were not receiving responses from the local shadow properly. This has been fixed.\n\nLambda Invoke function's InvocationType's default value was Event. This has been changed to RequestResponse.\n\n</div>\n\n## Getting Help\n\n*   [Ask on a Greengrass forum](https://forums.aws.amazon.com/forum.jspa?forumID=254)\n\n## License\n\nApache 2.0\n", "release_dates": ["2020-09-17T02:07:30Z", "2020-05-01T01:09:28Z", "2020-01-20T17:13:13Z", "2019-11-25T20:22:36Z", "2019-07-15T20:52:29Z", "2020-04-07T20:52:23Z"]}, {"name": "aws-greengrass-core-sdk-python", "description": "SDK to use with functions running on Greengrass Core using Python", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2020-09-17T00:39:52Z", "2019-11-25T19:46:29Z", "2019-07-15T20:52:08Z"]}, {"name": "aws-health-tools", "description": "The samples provided in AWS Health Tools can help users to build automation and customized alerting in response to AWS Health events.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS Health Tools\n\n### Description\nThe samples provided in AWS Health Tools can help you build automation and customized alerts in response to AWS Health events.\n\nAWS Health provides ongoing visibility into the state of your AWS resources, services, and accounts. The service gives you awareness and remediation guidance for resource performance or availability issues that may affect your applications that run on AWS. AWS Health provides relevant and timely information to help you manage events in progress, as well as be aware of and prepare for planned activities. The service delivers alerts and notifications triggered by changes in the health of AWS resources, so you get near-instant event visibility and guidance to help accelerate troubleshooting.\n\nMore information about AWS Health and Personal Health Dashboard (PHD) is available here: http://docs.aws.amazon.com/health/latest/ug/what-is-aws-health.html\n\n> NOTE: To get notifications about public events (global events that are not tied to your account), you must have a Business or Enterprise support plan from AWS Support. If you call the AWS Health API from an AWS account that doesn't have a Business or Enterprise support plan, you receive a SubscriptionRequiredException error.\n\nSetup and usage instructions are present for each tool in its respective directory: <br />\n\n\n\n#### Solutions:\n[AWS Health Aware (AHA) - automated notification solution for sending well-formatted AWS Health Alerts across accounts and regions](https://github.com/aws-samples/aws-health-aware/) <br />\n\n#### Custom Notifications:\n[AWS Health event SMS notifier](sms-notifier/) <br />\n[AWS Health event Chime notifier](chime-notifier/) <br />\n[AWS Health event Amazon Simple Notification Service (SNS) Topic Publisher](sns-topic-publisher/) <br />\n[AWS Health event Slack notifier](slack-notifier/) <br />\n[AWS Health event Direct Connect maintenance notifier](dx-maintenance-notifier/) <br />\n[AWS Health event Coralogix notifier](coralogix-notifier/) <br />\n[AWS Health Abuse event DOS report notifier](dos-report-notifier/) <br />\n[AWS Health SHD event Chime/Slack/SNS notifier](shd-notifier/) <br />\n[AWS Health Organizational View Alerts](https://github.com/aws-samples/aws-health-organizational-view-alerts) <br />\n#### Automated Actions:\n[AWS Codepipeline disable stage transition triggered when AWS Health issue event generated](automated-actions/AWS_Codepipeline_Disable_Stage_Transition/) <br />\n[AWS Health AWS_EC2_INSTANCE_STORE_DRIVE_PERFORMANCE_DEGRADED triggers automated EC2 Instance stop or terminate](automated-actions/AWS_EC2_INSTANCE_STORE_DRIVE_PERFORMANCE_DEGRADED/) <br />\n[AWS Health AWS_ELASTICLOADBALANCING_ENI_LIMIT_REACHED triggers freeing up of unused ENIs](automated-actions/AWS_ELASTICLOADBALANCING_ENI_LIMIT_REACHED/) <br />\n[AWS Health AWS_RISK_CREDENTIALS_EXPOSED remediation](automated-actions/AWS_RISK_CREDENTIALS_EXPOSED/) <br />\n[AWS Health AWS_EBS_VOLUME_LOST Remediation](automated-actions/AWS_EBS_VOLUME_LOST/) <br />\n#### Demos:\n[AWS Health API high availability endpoint](high-availability-endpoint/) <br />\n\n![Architecture](images/AWSHealthToolsArchitecture.jpg)\n\n### License\nAWS Health Tools are licensed under the Apache 2.0 License.\n\nDisclaimer: The \u201cAWS_<serviceName>_OPERATIONAL_ISSUE\u201d Amazon CloudWatch event type codes are for events where AWS is posting details to specific AWS accountIds. General service health events are not posted to this event type code at this time. Instead they are currently posted to the Service Health Dashboard (SHD) and are visible via the Personal Health Dashboard (PHD) in the AWS management console as well as returned via the AWS Health API.\n", "release_dates": []}, {"name": "aws-imds-packet-analyzer", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Introduction\n\nThe AWS ImdsPacketAnalyzer is a tool that traces TCP interactions with the EC2 Instance Metadata Service (IMDS). This can assist in identifying the processes making IMDSv1 calls on a host. Traces contain the `pid`, the `argv` used to launch the process, and the parent `pids` up to four levels deep. This information allows you to identify a Process making IMDSv1 calls for further investigation.\n\nThe ImdsPacketAnalyzer leverages the [BCC (BPF Compiler Collection)](https://github.com/iovisor/bcc/blob/master/INSTALL.md#Amazon-Linux-2---Binary). In order to successfully run the analyzer the BCC pre-requisites need to be installed.\n\n\n# AWS ImdsPacketAnalyzer\n\n- [Packages - Installing BCC](#packages---installing-bcc)\n    - [Amazon Linux 2023](#amazon-linux-2023)\n\t- [Amazon Linux 2](#amazon-linux-2)\n\t- [Amazon Linux 1, 2018.03](#amazon-linux-1-201803)\n\t- [Debian 11](#debian-11)\n\t- [Debian 10](#debian-10)\n\t- [Ubuntu 20 / 22](#ubuntu-20--22)\n\t- [RHEL 8 / 9](#rhel-8--9)\n\t- [SLES 15](#sles-15)\n\t- [Windows](#windows)\n- [Usage](#usage-)\n    - [Amazon Linux 2023](#amazon-linux-2023-1)\n\t- [Amazon Linux 2](#amazon-linux-2-1)\n\t- [Amazon Linux 1](#amazon-linux-1)\n\t- [Debian 11](#debian-11-1)\n\t- [Debian 10](#debian-10-1)\n\t- [Ubuntu 20 / 22](#ubuntu-20--22-1)\n\t- [RHEL 8 / 9](#rhel-8--9-1)\n\t- [SLES 15](#sles-15-1)\n\t- [Windows](#windows-1)\n- [Logging](#logging)\n- [Running the tool as a service](#running-the-tool-as-a-service)\n\t- [Activating the tool as a service](#activating-the-tool-as-a-service)\n\t- [Deactivating the tool as a service](#deactivating-the-tool-as-a-service)\n- [Limitations](#limitations)\n\n\n# Packages - Installing BCC\nFor hosts with internet access, the install script can be used. It is advised that this script is run only on non-production instances. Installation will update dependancies and may affect other functionality.\nFor instances without internet access you will need to share the files on an S3 folder.\n```\nsudo bash install-deps.sh\n```\n---\n\n**OR** run  the following commands per OS\n\n\n## Amazon Linux 2023\n\nInstall BCC\n\n```\nsudo dnf install bcc-tools\n```\n---\n\n## Amazon Linux 2\n\nInstall [BCC (BPF Compiler Collection)](https://github.com/iovisor/bcc/blob/master/INSTALL.md#Amazon-Linux-2---Binary):\n\n```\nsudo amazon-linux-extras enable BCC\nsudo yum install kernel-devel-$(uname -r)\nsudo yum install bcc\n```\n---\n\n## Amazon Linux 1, 2018.03\n \nInstall [BCC (BPF Compiler Collection)](https://github.com/iovisor/bcc/blob/master/INSTALL.md#Amazon-Linux-1---Binary):\n \n```\nsudo yum install kernel-headers-$(uname -r | cut -d'.' -f1-5)\nsudo yum install kernel-devel-$(uname -r | cut -d'.' -f1-5)\nsudo yum install bcc\n```\n---\n\n## Debian 11\n\n```\necho deb http://cloudfront.debian.net/debian sid main | sudo tee -a /etc/apt/sources.list\nsudo apt-get update\nsudo apt-get install -y bpfcc-tools libbpfcc libbpfcc-dev linux-headers-$(uname -r)\nsudo apt-get install linux-headers-$(uname -r) bcc\n```\n---\n\n## Debian 10\n\nNote : During the Dependency installation, the [\"libcrypt1\"](https://www.mail-archive.com/debian-bugs-dist@lists.debian.org/msg1818037.html) related error occurs so the execution has step to fix and continue with the installation process further, also the OS libraries can cause the restart of the system releated services like sshd and crond.\n\n```\necho deb http://cloudfront.debian.net/debian sid main | sudo tee -a /etc/apt/sources.list\nsudo -i         # Need to switch to root user in the CLI before running below command\napt-get update\n#Set the environment variable DEBIAN_FRONTEND to 'noninteractive' to avoid the prompts and accept the default answers\nexport DEBIAN_FRONTEND=noninteractive\napt-get install -y bpfcc-tools libbpfcc libbpfcc-dev linux-headers-$(uname -r) bcc --no-install-recommends\n#Steps to fix the libcrypt1 error\ncd /tmp/\napt -y download libcrypt1\ndpkg-deb -x libcrypt1* .\ncp -av lib/x86_64-linux-gnu/* /lib/x86_64-linux-gnu/\n#Re-run the install command\napt-get install -y bpfcc-tools libbpfcc libbpfcc-dev linux-headers-$(uname -r) bcc --no-install-recommends\napt install -y --fix-broken\n# Run the install command\napt-get install -y bpfcc-tools libbpfcc libbpfcc-dev linux-headers-$(uname -r) bcc --no-install-recommends\n```\n---\n\n## Ubuntu 20 / 22\n```\nsudo apt install -y bison build-essential cmake flex git libedit-dev libllvm14 llvm-14-dev libclang-14-dev python3 zlib1g-dev libelf-dev libfl-dev python3-distutils\ngit clone https://github.com/iovisor/bcc.git\nmkdir bcc/build; cd bcc/build\ncmake ..\nmake\nsudo make install\ncmake -DPYTHON_CMD=python3 .. # build python3 binding\npushd src/python/\nmake\nsudo make install\npopd\nsudo apt-get install linux-headers-$(uname -r)\n```\n---\n\n## RHEL 8 / 9\n\n```\nsudo yum -y install bcc-tools libbpf\n```\n---\n\n## SLES 15\n```\nsudo zypper ref\nsudo zypper in bcc-tools bcc-examples\nsudo zypper in --oldpackage kernel-default-devel-$(zypper se -s kernel-default-devel | awk '{split($0,a,\"|\"); print a[4]}' | grep $(uname -r | awk '{gsub(\"-default\", \"\");print}') | sed -e 's/^[ \\t]*//' | tail -n 1)\n```\n\n---\n\n## WINDOWS\n\nINSTALL PYTHON\n- Check if Python is installed with ```python -V```\n- Download python3 msi https://www.python.org/downloads/\n- Select to add python.exe to PATH\n\n\nINSTALL PIP\n- Check if PIP is installed with ```pip help```\n- Download PIP ```curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py```\n- Install with ```python get-pip.py```\n\n\nINSTALL GIT\n- Check if GIT is installed with ```git version```\n- Download ad install the MSI for Windows via ```https://git-scm.com/download/win```\n\n\nINSTALL AWS CLI\n- Run ```msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi```\n- Run and configure the CLI with ```aws configure```\n- Ensure EC2 Instance IAM Profile is assigned with access to Cloudwatch.\n\n\nINSTALL METABADGER (https://github.com/salesforce/metabadger)\n- Run ```pip3 install --user metabadger```\n- Go to working directory ```cd C:\\<Users>\\<Administrator>\\AppData\\Roaming\\Python\\Python311\\scripts```\n\n---\n\n**Note:** Troubleshooting + Installation on other distros please see: [BCC (BPF Compiler Collection)](https://github.com/iovisor/bcc/blob/master/INSTALL.md)\n\n---\n\n## Usage \nBCC requires that the analyzer is run with root permissions. Typically, you can execute the following script and IMDS calls will be logged to the console and to a log file by default (see [logging.conf](#log-configuration)).\n```\nsudo python3 src/imds_snoop.py\n```\n\n\n#### Example v1 call:\nThe following IMDSv1 curl command\n```\ncurl http://169.254.169.254/latest/meta-data/\n```\nwill result the following IMDS packet analyzer output\n```\nIMDSv1(!) (pid:6028:curl argv:curl http://169.254.169.254/latest/meta-data/) called by -> (pid:6027:makeCalls.sh argv:/bin/bash ./makeCalls.sh) -> (pid:4081:zsh argv:-zsh) -> (pid:4081:sshd argv:sshd: kianred@pts/0)\n```\n---\n\n## Amazon Linux 2023\n```\nsudo python3 src/imds_snoop.py\n```\n---\n\n## Amazon Linux 2\n```\nsudo python3 src/imds_snoop.py\n```\n---\n\n## Amazon Linux 1\n```\nsudo python3 src/imds_snoop.py\n```\n---\n\n## Debian 11\n```\nsudo python3 src/imds_snoop.py\n```\n---\n\n## Debian 10\n```\nsudo python3 src/imds_snoop.py\n```\n---\n\n## Ubuntu 20 / 22\n```\nsudo LD_PRELOAD=/home/ubuntu/bcc/build/src/cc/libbcc.so.0 PYTHONPATH=/home/ubuntu/bcc/build/src/python src/imds_snoop.py\n```\n---\n\n## RHEL 8 / 9\n```\nsudo python3 src/imds_snoop.py\n```\n---\n\n## SLES 15\n```\nsudo python3 src/imds_snoop.py\n```\n---\n\n## WINDOWS\n- From the Working directory E.g ```cd C:\\<Users>\\<Administrator>\\AppData\\Roaming\\Python\\Python311\\scripts```\n- Run to view IMDSv1 calls: ```metabadger cloudwatch-metrics --region us-east-1```\n\nThe output table will highlight if the instance has made IMDSv1 calls\n\nTo find the specific app making the IMDSv1 calls, use the inbuilt Windows Resource Monitor Network monitor to find the Image and PID of the application making calls.\n\nTo do this, open Resource Monitor (Start->Search ->Resource Monitor) and click on the Network tab.\nThen look for calls in the Network Activity section made to either the IP or DNS entries listed:\n- IP: ```169.254.169.254```\n- DNS: instance-data.<region>.compute.internal E.g ```instance-data.us-east-1.compute.internal```\n\nNetwork Analyzer will show the calls and you should proceed to update the software/application.\n\nMore details and thanks to https://github.com/salesforce/metabadger and https://www.greystone.co.uk/2022/03/24/how-greystone-upgraded-its-aws-ec2-instances-to-use-instance-meta-data-service-version-2-imdsv2/\n\n\n---\n\n# Logging\nThe ImdsPacketAnalyzer will also capture IMDS calls to log files. Log entries follow the format: `[Time] [Level] [message]` where:\n- **Time:** the time at which the IMDS call was made in the format: `%Y-%m-%dT%H:%M:%S` eg.) [2022-12-20T12:57:51]\n- **Level:** the level of the log entry, where IMDSv2 calls are logged at `INFO` level and IMDSv1 calls are logged at `WARNING` level\n    - If there are any instances where an ImdsPacketAnalyser fail to interpret the packets, `ERROR` level messages will be traced.\n    - **Note** The only reason a call cannot be identified is if the analyzer is unable to find a request payload for the IMDS call, this missing payload means the analyzer will not be able to discern V1 from V2 IMDS calls. (see what to do in case of missing payload below).\n    - Errors (due to a *missing payload*) in the log indicate that the analyzer was not able to capture the payload that was sent to the IMDS ip.  This is expected for AL2 kernel 4.14 on Graviton (ARM) instances (see \"Limitations\" heading below).   If this is a new error case, please log a defect with detailed information and consider alternative ways to identify the source of the IMDS call.\n- **message:** the details of the IMDS call as it would be outputted to the terminal\n\nExample of a IMDSv1 log entry:\n```\n[2022-12-20T11:03:58] [WARNING] IMDSv1(!) (pid:1016:curl argv:curl http://169.254.169.254/latest/meta-data/) called by -> (pid:1015:makeCalls.sh argv:/bin/bash ./makeCalls.sh) -> (pid:32678:zsh argv:-zsh) -> (pid:32678:sshd argv:sshd: )\n```\nExample of a IMDSv2 log entry:\n```\n[2022-12-20T11:03:58] [INFO] IMDSv2 (pid:1018:curl argv:curl -H X-aws-ec2-metadata-token: AQAEAFEOMInKb-S7me-hLqzu83lYdeDV7r-sPh2D4SJF6v5IwD4S8g== -v http://169.254.169.254/latest/meta-data/) called by -> (pid:1015:makeCalls.sh argv:/bin/bash ./makeCalls.sh) -> (pid:32678:zsh argv:-zsh) -> (pid:32678:sshd argv:sshd: )\n```\n\n### Log configuration\n\nThe logging configuration can be adjusted by editing the **logging.conf** file.\n\nBy default:\n- Logs will be saved to the `/var/log/imds/` folder in a file called `imds-trace.log`\n- Log files will be appended (if the analyzer is stopped and then run again on multiple occasions)\n- Each log file will reach a maximum size of 1 megabyte before rollover occurs\n- When a log file reaches 1mb in size it will rollover to a new log file **i.e) imds-trace.log.1 or imds-trace.log.2** \n- Rollover occurs a maximum of 5 times meaning that at most log files will at most take up 6 x 1mb => 6mb storage space (the prominent `imds-trace.log` file + 5 rollover log files `imds-trace.log.x` where x ranges from 1 to **backupCount**)\n\n### Analyzing log files\n**Assuming default logging setup:** \n- Running the command `cat /var/log/imds/imds-trace.* | grep WARNING` will output all IMDSv1 calls to the terminal. \n- Note that this grep will only identify the call, sometimes the calls leading up to the V1 call can provide additional context.   \n\n# Running the tool as a service\n\n## Activating the tool as a service\nConfiguring the analyzer to run as a service will ensure that the tool will run as soon as possible upon the boot up of an instance. This will increase the chances of identifying services making IMDSv1 calls as early as the instance is inited onto a network. \n\nA shell script has been provided in the package that will automate the process of setting up the analyzer tool as a service. **Note:** the script/service will only work if the structure of the package is left unchanged. \n\nRun the script from the command line as follows:\n\n```\nsudo ./activate-tracer-service.sh\n```\n\nor\n\n```\nsudo bash activate-tracer-service.sh\n```\n\nThe permissions for the shell script may need to be changed using:\n```\nchmod +x activate-tracer-service.sh\n```\n\nYou can check if the service is running after activating the service or a host reboot:\n```\nsystemctl status -l imds_tracer_tool.service\n```\n\n## Deactivating the tool as a service\nWhen the tool is configured as a service using the previous script, a service file is added into the OS. In order to restore the system, run the script from the command line:\n\n```\nsudo ./deactivate-tracer-service.sh\n```\n\nor\n\n```\nsudo bash deactivate-tracer-service.sh\n```\n\nPermissions for the script may need to be changed:\n```\nchmod +x deactivate-tracer-service.sh\n```\n\n---\n\n## Limitations\nWe are aware of some limitations with the current version of the ImdsPacketAnalyzer.  Contributions are welcomed.\n- The `install-deps.sh` script assumes AL2 and internet connectivity\n- Althought the ImdsPacketAnalyser have been run on multiple distributions, it is only tested on AL2 before new commits are pushed.\n- ImdsPacketAnalyzer only supports IPv4\n- ImdsPacketAnalyzer is intended to be used to identify processes making IMDSv1 calls. There is no guarantee that IMDS calls will be detected.  Also be aware that a network can be configured to route other (non-IMDS) traffic to the IMDS ip address. The analyzer is not reliable enough to be used to alarm or audit IMDSv1 calls.\n- ImdsPacketAnalyser has not been tested with production traffic in mind, it is intended to be run as a analysis tool to be removed once the source of IMDSv1 calls have been identified.\n- AL2 kernel 4.14 on Graviton (ARM) lack the eBPF features required to determine if a call is IMDSv1 or V2.  This is reported as `{MISSING PAYLOAD}` error.  We do not have a workaround for this and do not have a planned fix.\n", "release_dates": []}, {"name": "aws-iot-core-mqtt-file-streams-embedded-c", "description": null, "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "## MQTT File Streams library\n\nIn AWS IoT, a **stream** is a publicly addressable resource that is an abstraction for a list of files that can be transferred to an IoT device. Using MQTT file streams library, files from a stream can be transfer to an IoT device. The file is transferred using MQTT protocol. It supports both JSON and CBOR format to send requests and receive data.\n\nMore information about streams and MQTT based file delivery can be found [here](https://docs.aws.amazon.com/iot/latest/developerguide/mqtt-based-file-delivery.html)\n\n### MQTT File Streams library workflow\n\n![alt text](./docs/doxygen/images/MqttStreams_flowChart.jpg)\n\n## Building MQTT File Streams library\n\n### Dependencies\n\nThe MQTT streams library depend on the following two libraries for encoding/decoding GetStream Request and Data blocks.\n\n1. tinyCBOR\n2. coreJSON\n\n### Build instructions\n\nThe [mqttFileDownloaderFilePaths.cmake](mqttFileDownloaderFilePaths.cmake) file contains the information of all source files and the header include path required to build this library.\n\n## Building unit tests\n\n### Platform Prerequisites\n\n- For running unit tests\n  - C90 compiler like gcc\n  - CMake 3.13.0 or later\n  - Ruby 2.0.0 or later is additionally required for the Unity test framework\n    (that we use).\n- For running the coverage target, gcov is additionally required.\n\n### Steps to build Unit Tests\n\n1. Go to the root directory of this repository.\n\n1. Create build directory: `mkdir build && cd build`\n\n1. Run _cmake_ while inside build directory: `cmake -S ../test`\n\n1. Run this command to build the library and unit tests: `make all`\n\n1. The generated test executables will be present in `build/bin/tests` folder.\n\n1. Run `ctest` to execute all tests and view the test run summary.\n\n## CBMC\n\nTo learn more about CBMC and proofs specifically, review the training material\n[here](https://model-checking.github.io/cbmc-training).\n\nThe `test/cbmc` directory contains CBMC proofs.\n\nIn order to run these proofs you will need to install CBMC and other tools by\nfollowing the instructions\n[here](https://model-checking.github.io/cbmc-training/installation.html).\n\n## Contributing\n\nSee [CONTRIBUTING.md](./.github/CONTRIBUTING.md) for information on\ncontributing.\n\n## Security\n\nSee [SECURITY](SECURITY.md) for more information.\n\n## License\n\nThis library is licensed under the [MIT Open Source License](LICENSE).\n\n", "release_dates": ["2023-11-21T20:23:53Z"]}, {"name": "aws-iot-device-embedded-c-sdk-for-zephyr", "description": null, "language": "C", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# AWS IoT Device Embedded C SDK for Zephyr\n\n## Table of Contents\n\n* [Overview](#overview)\n    * [License](#license)\n    * [Features](#features)\n        * [coreMQTT](#coremqtt)\n        * [coreHTTP](#corehttp)\n        * [coreJSON](#corejson)\n        * [AWS IoT Device Shadow](#aws-iot-device-shadow)\n        * [backoffAlgorithm](#backoffalgorithm)\n    * [Sending metrics to AWS IoT](#sending-metrics-to-aws-iot)\n* [Porting Guide for 202009.00 and newer releases](#porting-guide-for-20200900-and-newer-releases)\n    * [Porting coreMQTT](#porting-coremqtt)\n    * [Porting coreHTTP](#porting-corehttp)\n    * [Porting AWS IoT Device Shadow](#porting-aws-iot-device-shadow)\n* [Getting Started](#getting-started)\n    * [1. Setting Up Zepyhr](#1-setup-zephyr)\n    * [2. Setting ESP32](#2-setup-esp32)\n    * [3. Clone This Repository](#3-clone-this-repository)\n    * [4. Configuring Demos](#4-configuring-demos)\n         * [Wi-Fi Connection Setup](#wi-fi-connection-setup)\n         * [AWS IoT Account Setup](#aws-iot-account-setup)\n         * [Configuring mutual authentication demos of MQTT and HTTP](#configuring-mutual-authentication-demos-of-mqtt-and-http)\n         * [Configuring AWS IoT Device Shadow demo](#configuring-aws-iot-device-shadow-demo)\n    * [5. Building and Flashing Demos](#5-building-and-flashing-demos)\n* [Adding C-SDK to Zephyr Application](#adding-c-sdk-to-zephyr-application)\n\n\n\n## Overview\n\nThis repository is a port of AWS IoT Device SDK for Embedded C (C-SDK) for Zephyr RTOS. The AWS IoT Device SDK for Embedded C (C-SDK) is a collection of C source files under the [MIT open source license](LICENSE) that can be used in embedded applications to securely connect IoT devices to [AWS IoT Core](https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html). It contains MQTT client, HTTP client, JSON Parser, and AWS IoT Device Shadow libraries. See https://github.com/aws/aws-iot-device-sdk-embedded-C for main documentation.\n\nThe libraries in the SDK are not dependent on any operating system. However, the demos for the libraries in this SDK are built and tested on the Zephyr platform for the ESP32 board. The demos build with [west](https://docs.zephyrproject.org/latest/guides/west/index.html), Zephyr's meta-tool.\n\n**C-SDK includes libraries that are part of the [FreeRTOS 202012.01 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202012.01-LTS) release. Learn more about the FreeRTOS 202012.01 LTS libraries by [clicking here](https://freertos.org/lts-libraries.html).**\n\n### License\n\nThe C-SDK libraries are licensed under the [MIT open source license](LICENSE).\n\n### Features\n\nC-SDK simplifies access to various AWS IoT services. C-SDK has been tested to work with [AWS IoT Core](https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html) and an open source MQTT broker to ensure interoperability. The AWS IoT Device Shadow library is flexible to work with any MQTT client and JSON parser. The MQTT client and JSON parser libraries are offered as choices without being tightly coupled with the rest of the SDK. This C-SDK for Zephyr contains the following libraries:\n\n#### coreMQTT\n\nThe [coreMQTT](https://github.com/FreeRTOS/coreMQTT) library provides the ability to establish an MQTT connection with a broker over a customer-implemented transport layer, which can either be a secure channel like a TLS session (mutually authenticated or server-only authentication) or a non-secure channel like a plaintext TCP connection. This MQTT connection can be used for publishing and subscribing to MQTT topics. The library provides a mechanism to register customer-defined callbacks for receiving incoming PUBLISH, acknowledgement and keep-alive response events from the broker. The library has been refactored for memory optimization and is compliant with the [MQTT 3.1.1](https://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html) standard. It has no dependencies on any additional libraries other than the standard C library, a customer-implemented network transport interface, and optionally a customer-implemented platform time function. The refactored design embraces different use-cases, ranging from resource-constrained platforms using only QoS 0 MQTT PUBLISH messages to resource-rich platforms using QoS 2 MQTT PUBLISH over TLS connections.\n\nSee memory requirements for the latest release [here](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/standard/coreMQTT/docs/doxygen/output/html/index.html#mqtt_memory_requirements).\n\n#### coreHTTP\n\nThe [coreHTTP](https://github.com/FreeRTOS/coreHTTP) library provides the ability to establish an HTTP connection with a server over a customer-implemented transport layer, which can either be a secure channel like a TLS session (mutually authenticated or server-only authentication) or a non-secure channel like a plaintext TCP connection. The HTTP connection can be used to make \"GET\" (include range requests), \"PUT\", \"POST\" and \"HEAD\" requests. The library provides a mechanism to register a customer-defined callback for receiving parsed header fields in an HTTP response. The library has been refactored for memory optimization, and is a client implementation of a subset of the [HTTP/1.1](https://tools.ietf.org/html/rfc2616) standard.\n\nSee memory requirements for the latest release [here](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/standard/coreHTTP/docs/doxygen/output/html/index.html#http_memory_requirements).\n\n#### coreJSON\n\nThe [coreJSON](https://github.com/FreeRTOS/coreJSON) library is a JSON parser that strictly enforces the [ECMA-404 JSON standard](https://www.json.org/json-en.html). It provides a function to validate a JSON document, and a function to search for a key and return its value. A search can descend into nested structures using a compound query key. A JSON document validation also checks for illegal UTF8 encodings and illegal Unicode escape sequences.\n\nSee memory requirements for the latest release [here](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/standard/coreJSON/docs/doxygen/output/html/index.html#json_memory_requirements).\n\n#### AWS IoT Device Shadow\n\nThe AWS IoT Device Shadow library enables you to retrieve and update the current state of one or more shadows of every registered device. A device\u2019s shadow is a persistent, virtual representation of your device that you can interact with from AWS IoT Core even if the device is offline. The device state is captured in its \"shadow\" is represented as a [JSON](https://www.json.org/) document. The device can send commands over MQTT to get, update and delete its latest state as well as receive notifications over MQTT about changes in its state. The device\u2019s shadow(s) are uniquely identified by the name of the corresponding \"thing\", a representation of a specific device or logical entity on the AWS Cloud. See [Managing Devices with AWS IoT](https://docs.aws.amazon.com/iot/latest/developerguide/iot-thing-management.html) for more information on IoT \"thing\". This library supports named shadows, a feature of the AWS IoT Device Shadow service that allows you to create multiple shadows for a single IoT device. More details about AWS IoT Device Shadow can be found in [AWS IoT documentation](https://docs.aws.amazon.com/iot/latest/developerguide/iot-device-shadows.html).\n\nThe AWS IoT Device Shadow library has no dependencies on additional libraries other than the standard C library. It also doesn\u2019t have any platform dependencies, such as threading or synchronization. It can be used with any MQTT library and any JSON library (see [demos](demos/shadow) with coreMQTT and coreJSON).\n\nSee memory requirements for the latest release [here](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/aws/device-shadow-for-aws-iot-embedded-sdk/docs/doxygen/output/html/index.html#shadow_memory_requirements).\n\n#### backoffAlgorithm\n\nThe [backoffAlgorithm](https://github.com/FreeRTOS/backoffAlgorithm) library is a utility library to calculate backoff period using an exponential backoff with jitter algorithm for retrying network operations (like failed network connection with server). This library uses the \"Full Jitter\" strategy for the exponential backoff with jitter algorithm. More information about the algorithm can be seen in the [Exponential Backoff and Jitter AWS blog](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/).\n\nExponential backoff with jitter is typically used when retrying a failed connection or network request to the server. An exponential backoff with jitter helps to mitigate the failed network operations with servers, that are caused due to network congestion or high load on the server, by spreading out retry requests across multiple devices attempting network operations. Besides, in an environment with poor connectivity, a client can get disconnected at any time. A backoff strategy helps the client to conserve battery by not repeatedly attempting reconnections when they are unlikely to succeed.\n\nThe backoffAlgorithm library has no dependencies on libraries other than the standard C library.\n\nSee memory requirements for the latest release [here](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/standard/backoffAlgorithm/docs/doxygen/output/html/index.html#backoff_algorithm_memory_requirements).\n\n### Sending metrics to AWS IoT\n\nWhen establishing a connection with AWS IoT, users can optionally report the Operating System, Hardware Platform and MQTT client version information of their device to AWS. This information can help AWS IoT provide faster issue resolution and technical support. If users want to report this information, they can send a specially formatted string (see below) in the username field of the MQTT CONNECT packet.\n\nFormat\n\nThe format of the username string with metrics is:\n\n```\n<Actual_Username>?SDK=<OS_Name>&Version=<OS_Version>&Platform=<Hardware_Platform>&MQTTLib=<MQTT_Library_name>@<MQTT_Library_version>\n```\n\nWhere\n\n* <Actual_Username> is the actual username used for authentication, if username and password are used for authentication. When username and password based authentication is not used, this\nis an empty value.\n* <OS_Name> is the Operating System the application is running on (e.g. Zephyr)\n* <OS_Version> is the version number of the Operating System (e.g. 2.6.0)\n* <Hardware_Platform> is the Hardware Platform the application is running on (e.g. ESP32)\n* <MQTT_Library_name> is the MQTT Client library being used (e.g. coreMQTT)\n* <MQTT_Library_version> is the version of the MQTT Client library being used (e.g. 1.1.0)\n\nExample\n\n*  Actual_Username = \u201ciotuser\u201d, OS_Name = Zephyr, OS_Version = 2.6.0, Hardware_Platform_Name = ESP32, MQTT_Library_Name = coremqtt, MQTT_Library_version = 1.1.0. If username is not used, then \u201ciotuser\u201d can be removed.\n\n```\n/* Username string:\n * iotuser?SDK=Zephyr&Version=2.6.0&Platform=ESP32&MQTTLib=coremqtt@1.1.0\n */\n\n#define OS_NAME                   \"Zephyr\"\n#define OS_VERSION                \"2.6.0\"\n#define HARDWARE_PLATFORM_NAME    \"ESP32\"\n#define MQTT_LIB                  \"coremqtt@1.1.0\"\n\n#define USERNAME_STRING           \"iotuser?SDK=\" OS_NAME \"&Version=\" OS_VERSION \"&Platform=\" HARDWARE_PLATFORM_NAME \"&MQTTLib=\" MQTT_LIB\n#define USERNAME_STRING_LENGTH    ( ( uint16_t ) ( sizeof( USERNAME_STRING ) - 1 ) )\n\nMQTTConnectInfo_t connectInfo;\nconnectInfo.pUserName = USERNAME_STRING;\nconnectInfo.userNameLength = USERNAME_STRING_LENGTH;\nmqttStatus = MQTT_Connect( pMqttContext, &connectInfo, NULL, CONNACK_RECV_TIMEOUT_MS, pSessionPresent );\n```\n\n## Porting Guide for 202009.00 and newer releases\n\nAll libraries depend on the ISO C90 standard library and additionally on the `stdint.h` library for fixed-width integers, including `uint8_t`, `int8_t`, `uint16_t`, `uint32_t` and `int32_t`, and constant macros like `UINT16_MAX`. If your platform does not support the `stdint.h` library, definitions of the mentioned fixed-width integer types will be required for porting any C-SDK library to your platform.\n\n### Porting coreMQTT\n\nGuide for porting coreMQTT library to your platform is available [here](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/standard/coreMQTT/docs/doxygen/output/html/mqtt_porting.html).\n\n### Porting coreHTTP\n\nGuide for porting coreHTTP library is available [here](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/standard/coreHTTP/docs/doxygen/output/html/http_porting.html).\n\n### Porting AWS IoT Device Shadow\n\nGuide for porting AWS IoT Device Shadow library is available [here](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/aws/device-shadow-for-aws-iot-embedded-sdk/docs/doxygen/output/html/shadow_porting.html).\n\n## Getting Started\n\n### 1. Setup Zephyr\n\nTo setup Zephyr, please follow Zephyr's documentation: https://docs.zephyrproject.org/latest/getting_started/index.html\n\nThese steps should setup a Zephyr workspace with a root directory named \"zephyrproject\"\n\n### 2. Setup ESP32\n\nTo setup development for the ESP32 board, please follow Zephyr's documentation for the board: https://docs.zephyrproject.org/latest/boards/xtensa/esp32/doc/index.html\n\n### 3. Clone This Repository\n\nThis repository may be cloned anywhere, not necessarily inside a Zephyr workspace.\n\nThis repository uses [Git Submodules](https://git-scm.com/book/en/v2/Git-Tools-Submodules) to bring in the C-SDK libraries (eg, MQTT ) and third-party dependencies (eg, mbedtls for Zephyr platform transport layer).\nNote: If you download the ZIP file provided by GitHub UI, you will not get the contents of the submodules (The ZIP file is also not a valid git repository).\nTo clone the latest commit to main branch using HTTPS:\n\n```sh\ngit clone --recurse-submodules https://github.com/gilbefan/zephyr-c-sdk.git\n```\n\nUsing SSH:\n\n```sh\ngit clone --recurse-submodules git@github.com:gilbefan/zephyr-c-sdk.git\n```\n\nIf you have downloaded the repo without using the `--recurse-submodules` argument, you need to run:\n\n```sh\ngit submodule update --init --recursive\n```\n\n### 4. Configuring Demos\n\n#### Wi-Fi Connection Setup\n\nAll demos in this repository will to be configured to connect to Wi-Fi. Edit `demo_config.h` in the desired demo's folder to `#define` the following:\n\n* Set `WIFI_NETWORK_SSID` to the name of the Wi-Fi network to connect to.\n* Set `WIFI_NETWORK_PASSWORD` to the password of the Wi-Fi network to connect to.\n\n#### AWS IoT Account Setup\n\nYou need to setup an AWS account and access the AWS IoT console for running the AWS IoT Device Shadow library demos.\nAlso, the AWS account can be used for running the MQTT mutual auth demo against AWS IoT broker.\nNote that running the AWS IoT Device Shadow library demo requires the setup of a Thing resource for the device running the demo.\nFollow the links to:\n- [Setup an AWS account](https://portal.aws.amazon.com/billing/signup#/start).\n- [Sign-in to the AWS IoT Console](https://console.aws.amazon.com/iot/home) after setting up the AWS account.\n- [Create a Thing resource](https://docs.aws.amazon.com/iot/latest/developerguide/iot-moisture-create-thing.html).\n\nThe MQTT Mutual Authentication and AWS IoT Shadow demos include example AWS IoT policy documents to run each respective demo with AWS IoT. You may use the [MQTT Mutual auth](./demos/mqtt/mqtt_demo_mutual_auth/aws_iot_policy_example_mqtt.json) and [Shadow](./demos/shadow/shadow_demo_main/aws_iot_policy_example_shadow.json) example policies by replacing `[AWS_REGION]` and `[AWS_ACCOUNT_ID]` with the strings of your region and account identifier. While the IoT Thing name and MQTT client identifier do not need to match for the demos to run, the example policies have the Thing name and client identifier identical as per [AWS IoT best practices](https://docs.aws.amazon.com/iot/latest/developerguide/security-best-practices.html).\n\nIt can be very helpful to also have the [AWS Command Line Interface tooling](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html) installed.\n\n#### Configuring mutual authentication demos of MQTT and HTTP\n\nEdit `demo_config.h` in `demos/mqtt/mqtt_mutual_auth/` to `#define` the following:\n\n* Set `AWS_IOT_ENDPOINT` to your custom endpoint. This is found on the *Settings* page of the AWS IoT Console and has a format of `ABCDEFG1234567.iot.<aws-region>.amazonaws.com` where `<aws-region>` can be an AWS region like `us-east-2`.  \n   * Optionally, it can also be found with the AWS CLI command `aws iot describe-endpoint --endpoint-type iot:Data-ATS`.\n* Set `ROOT_CA_CERT_PEM` to the root CA certificate downloaded when setting up the device certificate in [AWS IoT Account Setup](#aws-iot-account-setup), if it is different from the default [AmazonRootCA1.pem](https://www.amazontrust.com/repository/AmazonRootCA1.pem). It is also possible to configure `ROOT_CA_CERT_PEM` to any PEM-encoded Root CA Certificate. A different Amazon Root CA certificate can be downloaded from [here](https://www.amazontrust.com/repository/). \n* Set `CLIENT_CERT_PEM` to the client certificate downloaded when setting up the device certificate in [AWS IoT Account Setup](#aws-iot-account-setup).\n* Set `CLIENT_PRIVATE_KEY_PEM` to the private key downloaded when setting up the device certificate in [AWS IoT Account Setup](#aws-iot-account-setup).\n\n#### Configuring AWS IoT Device Shadow demo\n\nEdit `demo_config.h` in `demos/mqtt/mqtt_mutual_auth/` and `demos/http/http_mutual_auth/` to `#define` the following:\n\n* Set `AWS_IOT_ENDPOINT` to your custom endpoint. This is found on the *Settings* page of the AWS IoT Console and has a format of `ABCDEFG1234567.iot.us-east-2.amazonaws.com`.\n* Set `ROOT_CA_CERT_PEM` to the root CA certificate downloaded when setting up the device certificate in [AWS IoT Account Setup](#aws-iot-account-setup), if it is different from the default [AmazonRootCA1.pem](https://www.amazontrust.com/repository/AmazonRootCA1.pem). A different Amazon Root CA certificate can be downloaded from [here](https://www.amazontrust.com/repository/). \n* Set `CLIENT_CERT_PEM` to the client certificate downloaded when setting up the device certificate in [AWS IoT Account Setup](#aws-iot-account-setup).\n* Set `CLIENT_PRIVATE_KEY_PEM` to the private key downloaded when setting up the device certificate in [AWS IoT Account Setup](#aws-iot-account-setup).\n* Set `THING_NAME` to the name of the Thing created in [AWS IoT Account Setup](#aws-iot-account-setup).\n\n### 5. Building and Flashing Demos\n\n1. Make sure the build environemnt variables are properly set up for the ESP32 by running these commands:\n\n  On Linux and MacOS:\n\n```\nexport ZEPHYR_TOOLCHAIN_VARIANT=\"espressif\"\nexport ESPRESSIF_TOOLCHAIN_PATH=\"${HOME}/.espressif/tools/xtensa-esp32-elf/esp-2020r3-8.4.0/xtensa-esp32-elf\"\nexport PATH=$PATH:$ESPRESSIF_TOOLCHAIN_PATH/bin\n```\n\n  On Windows:\n\n```\non CMD:\nset ESPRESSIF_TOOLCHAIN_PATH=%USERPROFILE%\\.espressif\\tools\\xtensa-esp32-elf\\esp-2020r3-8.4.0\\xtensa-esp32-elf\nset ZEPHYR_TOOLCHAIN_VARIANT=espressif\nset PATH=%PATH%;%ESPRESSIF_TOOLCHAIN_PATH%\\bin\n\non PowerShell:\nenv:ESPRESSIF_TOOLCHAIN_PATH=\"$env:USERPROFILE\\.espressif\\tools\\xtensa-esp32-elf\\esp-2020r3-8.4.0\\xtensa-esp32-elf\"\nenv:ZEPHYR_TOOLCHAIN_VARIANT=\"espressif\"\nenv:Path += \"$env:ESPRESSIF_TOOLCHAIN_PATH\\bin\"\n```\n\n2. Make sure this and following steps are done inside the Zephyr workspace. Run `west update` followed by `west espressif update` to retrieve required Zephyr submodules for ESP32.\n  \n3. Run `west build -b esp32 file_path_to_demo` to build a demo, replacing `file_path_to_demo` with the path to the desired demo. (Note: if another demo had been previously built in the west workspace, the additional option `--pristine` may need to be added to the end of the command.\n\n4. Run `west flash` to flash the demo. The option `--esp-device *ESP_DEVICE*`, where `*ESP_DEVICE*` is the serial port to flash, may also be useful to flash for ESP boards not connected to the default port. For documentation on additional options when flashing, please refer to https://docs.zephyrproject.org/latest/boards/xtensa/esp32/doc/index.html#flashing.\n\n## Adding C-SDK to a Zephyr Application\n\nTo use C-SDK libraries in an existing Zephyr application, edit the CMakeLists.txt file of the application to target the library's sources and directories as defined in the `*.cmake` file in the library's root directory. For example, to add the coreMQTT library, add the following lines to CMakeLists.txt:\n\n```\ninclude( include( ${CSDK_BASE}/libraries/standard/coreMQTT/mqttFilePaths.cmake )\n\n...\n\ntarget_sources(\n  ...\n  ${MQTT_SOURCES}\n  ${MQTT_SERIALIZER_SOURCES}\n  ...\n)\n\n...\n\ntarget_include_directories(\n  ...\n  ${MQTT_INCLUDE_PUBLIC_DIRS}\n  ...\n)\n\n...\n```\nWhere `${CSDK_BASE}` is the file path to the root of this repository.\n", "release_dates": []}, {"name": "aws-iot-device-sdk-arduino-yun", "description": "SDK for connecting to AWS IoT from an Arduino Y\u00fan. ", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<h1 align = \"center\">AWS IoT Arduino Y\u00fan SDK</h1>\n\n## What is AWS IoT Arduino Y\u00fan SDK\n\nThe AWS-IoT-Arduino-Y\u00fan-SDK allows developers to connect their Arduino Y\u00fan compatible Board to AWS IoT. By connecting the device to the AWS IoT, users can securely work with the message broker, rules and the Thing Shadow provided by AWS IoT and with other AWS services like AWS Lambda, Amazon Kinesis, Amazon S3, etc.\n\n* [Overview](#overview)\n* [Credentials](#credentials)\n* [Installation](#installation)\n* [API documentation](#api)\n* [Key features](#keyfeatures)\n* [Using the SDK](#usingthesdk)\n* [Example](#example)\n* [Error code](#errorcode)\n* [Support](#support)\n\n****\n\n<a name=\"overview\"></a>\n## Overview\nThis document provides step by step instructions to install the Arduino Y\u00fan SDK and connect your device to the AWS IoT.  \nThe AWS-IoT-Arduino-Y\u00fan-SDK consists of two parts, which take use of the resources of the two chips on Arduino Y\u00fan, one for native Arduino IDE API access and the other for functionality and connections to the AWS IoT built on top of [AWS IoT Device SDK for Python](https://github.com/aws/aws-iot-device-sdk-python).\n### MQTT connection\nThe AWS-IoT-Arduino-Y\u00fan-SDK provides APIs to let users publish messages to AWS IoT and subscribe to MQTT topics to receive messages transmitted by other devices or coming from the broker. This allows to interact with the standard MQTT PubSub functionality of AWS IoT. More information on MQTT protocol is available [here](http://docs.aws.amazon.com/iot/latest/developerguide/protocols.html).\n### Thing shadow\nThe AWS-IoT-Arduino-Y\u00fan-SDK also provides APIs to provide access to thing shadows in AWS IoT. Using this SDK, users will be able to sync the data/status of their devices as JSON files to the cloud and respond to change of status requested by other applications. More information on Thing Shadow is available [here](http://docs.aws.amazon.com/iot/latest/developerguide/iot-thing-shadows.html).\n\n****\n\n<a name=\"credentials\"></a>\n## Credentials  \nThe SDK supports two types of credentials that correspond to the two connection types:\n\n### X.509 certificate\n\nFor the certificate-based mutual authentication connection type. Download the [AWS IoT root CA](https://www.symantec.com/content/en/us/enterprise/verisign/roots/VeriSign-Class%203-Public-Primary-Certification-Authority-G5.pem). Use the AWS IoT console to create and download the certificate and private key. You must upload these credentials along with the Python runtime code base to AR9331 on Y\u00fan board and specify the location of these files in a configuration file `aws_iot_config.h`.\n\n### IAM credentials\n\nFor the Websocket with Signature Version 4 authentication type. You will need IAM credentials: an access key ID, a secret access key. You must also download and upload the [AWS IoT root CA](https://www.symantec.com/content/en/us/enterprise/verisign/roots/VeriSign-Class%203-Public-Primary-Certification-Authority-G5.pem). A tooling script `AWSIoTArduinoYunWebsocketCredentialConfig.sh` is provided for Mac OS/Linux users to update the IAM credentials as environment variables on AR9331, Y\u00fan board.  \n\n****\n\n<a name=\"installation\"></a>\n## Installation\n### Download AWS-IoT-Arduino-Y\u00fan-SDK  \nClick [here](https://s3.amazonaws.com/aws-iot-device-sdk-arduino-yun/AWS-IoT-Arduino-Yun-SDK-latest.zip) to download AWS-IoT-Arduino-Y\u00fan-SDK zip package and extract it to `AWS-IoT-Arduino-Yun-SDK` on your computer.\n### Set up your Arduino Y\u00fan Board\nPlease follow the instructions from official website: [Arduino Y\u00fan Guide](https://www.arduino.cc/en/Guide/ArduinoYun).\n\n### Installation on Mac OS/Linux\nBefore proceeding to the following steps, please make sure that you have `expect` installed on your computer and correctly installed the Arduino IDE.  \nTo install `expect`:  \nFor Ubuntu, simply run `sudo apt-get install expect`.  \nFor Mac, `expect` is installed as default.  \nFor Arduino IDE installation on Linux, please visit [here](http://playground.arduino.cc/Linux/All).\n\n1. Setup the Arduino Y\u00fan board and connect it to WiFi. Obtain its IP address and password.  \n2. Make sure your computer is connected to the same network (local IP address range).  \n3. Download the AWS IoT CA file from [here](https://www.symantec.com/content/en/us/enterprise/verisign/roots/VeriSign-Class%203-Public-Primary-Certification-Authority-G5.pem).\n4. Put your AWS IoT CA file, private key and certificate into `AWS-IoT-Arduino-Yun-SDK/AWS-IoT-Python-Runtime/certs`. If you are using MQTT over Websocket, you can put only your AWS IoT CA file into the directory. You should have a correctly configured AWS Identity and Access Management (IAM) role with a proper policy, and a pair of AWS Access Key and AWS Secret Access Key ID, which will be used in step 6. For more information about IAM, please visit [AWS IAM home page](https://aws.amazon.com/iam/).  \n5. Open a terminal, cd to `AWS-IoT-Arduino-Yun-SDK`. Do `chmod 755 AWSIoTArduinoYunInstallAll.sh` and execute it as `./AWSIoTArduinoYunInstallAll.sh <Board IP> <UserName> <Board Password>`. By default for Arduino Y\u00fan Board, your user name will be `root` and your password will be `arduino`.  \n\tThis script will upload the python runtime code base and credentials to openWRT running on the more powerful micro-controller on you Arduino Y\u00fan board.  \n\tThis script will also download and install libraries for openWRT to implement the necessary scripting environment as well as communication protocols.\n\n  Step 5 can take 10-15 minutes for the device to download and install the required packages (distribute, python-openssl, pip, AWSIoTPythonSDKv1.0.0).  \n\n  NOTE: Do NOT close the terminal before the script finishes, otherwise you have to start over with step 5. Make sure you are in your local terminal before repeating step 5.  \n\n6. Optional, only for Websocket. Open a terminal, cd to `AWS-IoT-Arduino-Yun-SDK`. Do `chmod 755 AWSIoTArduinoYunWebsocketCredentialConfig.sh` and execute it as `./AWSIoTArduinoYunWebsocketCredentialConfig.sh <Board iP> <UserName> <Board Password> <AWS_ACCESS_KEY_ID_STRING> <AWS_SECRET_ACCESS_KEY_STRING>`.  \n\tThis script will add the given key ID and secret key onto the OpenWRT as environment variables `$AWS_ACCESS_KEY_ID` and `$AWS_SECRET_ACCESS_KEY`.  \n\t\n\tNOTE1: You can always use this script to update your IAM credentials used on the board. The script will handle the update of the environment variables for you.  \n\t\n\tNOTE2: Please follow the instructions on the screen after the script completes to power-cycle your board to enable all the environment changes on OpenWRT.  \n\t\n7. Copy and paste `AWS-IoT-Arduino-Yun-SDK/AWS-IoT-Arduino-Yun-Library` folder into Arduino libraries that was installed with your Arduino SDK installation. For Mac OS default, it should be under `Documents/Arduino/libraries`.\n8. Restart the Arduino IDE if it was running during the installation. You should be able to see the AWS IoT examples in the Examples folder in your IDE.  \n\nThere are the other two scripts: `AWSIoTArduinoYunScp.sh` and `AWSIoTArduinoYunSetupEnvironment.sh`, which are utilized in `AWSIoTArduinoYunInstallAll.sh`. You can always use `AWSIoTArduinoYunScp.sh` to upload your new credentials to your board. When you are in the directory `AWS-IoT-Arduino-Yun-SDK/`, the command should be something like this:  \n\n    ./AWSIoTArduinoYunScp.sh <Board IP> <UserName> <Board Password> <File> <Destination>\u2028\n\n### Installation on Windows\n\nBefore proceeding to the following steps, please make sure that you have `Putty` and `WinSCP` installed on your PC. If you prefer to use other tools for SSH-ing into your Arduino Y\u00fan board and transferring files, you will have to adjust the steps below according to your tools.  \n`Putty` can be downloaded from [here](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).  \n`WinSCP` can be downloaded from [here](http://winscp.net/eng/download.php).\n\n1. Setup the Arduino Y\u00fan Cloud board and connect it to WiFi. Obtain its IP address and password.  \n2. Make sure your PC is connected to the same network (local IP address range).  \n3. Download the AWS IoT CA file from [here](https://www.symantec.com/content/en/us/enterprise/verisign/roots/VeriSign-Class%203-Public-Primary-Certification-Authority-G5.pem).  \n4. Put your AWS IoT CA file, private key and certificate into `AWS-IoT-Arduino-Yun-SDK/AWS-IoT-Python-Runtime/certs`. If you are using MQTT over Websocket, you can put only your AWS IoT CA file into the directory. You should have a correctly configured AWS Identity and Access Management (IAM) role with a proper policy, and a pair of AWS Access Key and AWS Secret Access Key ID, which will be used in step 7. For more information about IAM, please visit [AWS IAM home page](https://aws.amazon.com/iam/).  \n5. Start WinSCP and upload `AWS-IoT-Python-Runtime/` folder to `/root` on the board.  \n6. Use Putty to ssh into OpenWRT on your board and execute the following commands to install the necessary libraries:\n\n\t\topkg update\n\t\topkg install distribute\n\t\topkg install python-openssl\n\t\teasy_install pip\n\t\tpip install AWSIoTPythonSDK==1.0.0\n\t\n  It can take 10-15 minutes for the device to download and install the required packages.\n  \n7. Optional, only for Websocket. Use Putty to ssh into OpenWRT on your board and modify `/etc/profile` to include your IAM credentials as environment variables:  \n\n\t\t...\n\t\texport AWS_ACCESS_KEY_ID=<AWS_ACCES_KEY_ID_STRING>\n\t\texport AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY_STRING>\n\t\t...\n  \n  After that, run `source /etc/profile` and power-cycle the board to enable the changes.  \n  \n8. Copy and paste `AWS-IoT-Arduino-Yun-SDK/AWS-IoT-Arduino-Yun-Library` folder into Arduino libraries that was installed with your Arduino SDK installation. For Windows default, it should be under `Documents/Arduino/libraries`.  \n9. Restart the Arduino IDE if it was running during the installation. You should be able to see the AWS IoT examples in the Examples folder in your IDE.\n\n****\n\n<a name=\"api\"></a>\n## API documentation\nClass Name:\n\n\taws_iot_mqtt_client\n\nAPI:\n\n* MQTT connection  \n[IoT\\_Error\\_t setup(const char\\* client\\_id, bool clean\\_session, MQTTv\\_t MQTT\\_version, bool useWebsocket)](#setup)  \n[IoT\\_Error\\_t config(const char\\* host, unsigned int port, const char\\* cafile_path, const char\\* keyfile\\_path, const char\\* certfile\\_path)](#config)  \n[IoT\\_Error\\_t configWss(const char\\* host, unsigned int port, const char\\* cafile_path)](#configWss)  \n[IoT\\_Error\\_t configBackoffTiming(unsigned int baseReconnectQuietTimeSecond, unsigned int maxReconnectQuietTimeSecond, unsigned int stableConnectionTimeSecond)](#configBackoffTiming)  \n[IoT\\_Error\\_t configOfflinePublishQueue(unsigned int queueSize, DropBehavior\\_t behavior)](#configOfflinePublishQueue)  \n[IoT\\_Error\\_t configDrainingInterval(float numberOfSeconds)](#configDrainingInterval)  \n[IoT\\_Error\\_t connect(unsigned int keepalive\\_interval)](#connect)  \n[IoT\\_Error\\_t publish(const char\\* topic, const char\\* payload, unsigned int payload\\_len, unsigned int qos, bool retain)](#publish)  \n[IoT\\_Error\\_t subscribe(const char\\* topic, unsigned int qos, message\\_callback cb)](#subscribe)  \n[IoT\\_Error\\_t unsubscribe(const char\\* topic)](#unsubscribe)  \n[IoT\\_Error\\_t yield()](#yield)  \n[IoT\\_Error\\_t disconnect()](#disconnect)  \n* Thing shadow  \n[IoT\\_Error\\_t shadow\\_init(const char\\* thingName)](#shadow_init)  \n[IoT\\_Error\\_t shadow\\_update(const char\\* thingName, const char\\* payload, unsigned int payload_len, message\\_callback cb, unsigned int timeout)](#shadow_update)  \n[IoT\\_Error\\_t shadow\\_get(const char\\* thingName, message\\_callback cb, unsigned int timeout)](#shadow_get)  \n[IoT\\_Error\\_t shadow\\_delete(const char\\* thingName, message\\_callback cb, unsigned int timeout)](#shadow_delete)  \n[IoT\\_Error\\_t shadow\\_register\\_delta\\_func(const char\\* thingName, message\\_callback cb)](#shadow_register_delta_func)  \n[IoT\\_Error\\_t shadow\\_unregister\\_delta\\_func(const char\\* thingName)](#shadow_unregister_delta_func)  \n[IoT\\_Error\\_t getDesiredValueByKey(const char\\* JSONIdentifier, const char\\* key, char\\* externalJSONBuf, unsigned int bufSize)](#getDesiredValueByKey)  \n[IoT\\_Error\\_t getReportedValueByKey(const char\\* JSONIdentifier, const char\\* key, char\\* externalJSONBuf, unsigned int bufSize)](#getReportedValueByKey)  \n[IoT\\_Error\\_t getDeltaValueByKey(const char\\* JSONIdentifier, const char\\* key, char\\* externalJSONBuf, unsigned int bufSize)](#getDeltaValueByKey)  \n[IoT\\_Error\\_t getValueByKey(const char\\* JSONIdentifier, const char\\* key, char\\* externalJSONBuf, unsigned int bufSize)](#getValueByKey)  \n\nMessage Callback:  \n[void(\\*message\\_callback)(char\\*, unsigned int, Message\\_status\\_t)](#message_callback)\n\n<a name=\"setup\"></a>\n### IoT\\_Error\\_t setup(const char\\* client\\_id, bool clean\\_session, MQTTv\\_t MQTT\\_version, bool useWebsocket)\n**Description**  \nStart the Python runtime and set up connection for aws\\_iot\\_mqtt\\_client object. Must be called before any of aws\\_iot\\_mqtt\\_client API is called.\n\n**Syntax**  \n\n\tobject.setup(\"myClientID\"); // setup a client with client_id set to \"myClientID\"\n\tobject.setup(\"myClientID\", true, MQTTv31, true); // setup a client with client_id set to \"myClientID\", with clean_session set to true, using MQTT 3.1, over Websocket\n\n**Parameters**  \n*client\\_id* - The client id for this connection.  \n*clean\\_session* - Clear the previous connection with this id or not. Default value is true.  \n*MQTT\\_version* - Version of MQTT protocol for this connection, either MQTTv31 (MQTT version 3.1) or MQTTv311 (MQTT version 3.1.1). Default value is MQTTv311.  \n*useWebsocket* - Enable the use of Websocket or not. Default value is false. IAM credentials must be included in the environment variables on OpenWRT to make a successful MQTT connection over Websocket.  \n\n**Returns**  \nNONE\\_ERROR if the setup on openWRT side and connection settings are correct.  \nNULL\\_VALUE\\_ERROR if input parameters have NULL value.  \nOVERFLOW\\_ERROR if input string exceeds the internal buffer size.  \nSET\\_UP\\_ERROR if the setup failed.  \nSERIAL1\\_COMMUNICATION\\_ERROR if there is an error in Serial1 communication between the two chips.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"config\"></a>\n### IoT\\_Error\\_t config(const char\\* host, unsigned int port, const char\\* cafile\\_path, const char\\* keyfile\\_path, const char\\* certfile_path)\n**Description**  \nConfigure host, port and certs location used to connect to AWS IoT. If the input strings for host, cafile\\_path, keyfile\\_path and certfile\\_path are set to NULL, the default value will be used to connect. Must be called to load user settings right after `aws_iot_mqtt_client::setup` and before connect.\n\n**Syntax**\n\n\tobject.config(\"example.awsamazon.com\", 1234, \"./cafile\", \"./keyfile\", \"./certfile\");\n\t\n**Parameters**  \n*host* - The endpoint to connect to. Must be a NULL-terminated string.  \n*port* - The port number to connect to.  \n*cafile_path* - The path of CA file on OpenWRT. Must be a NULL-terminated string.  \n*keyfile_path* - The path of private key file on OpenWRT. Must be a NULL-terminated string.  \n*certfile_path* - The path of certificate file on OpenWRT. Must be a NULL-terminated string.\n\n**Returns**  \nNONE\\_ERROR if the configuration is successful.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nCONFIG\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"configWss\"></a>\n### IoT\\_Error\\_t configWss(const char\\* host, unsigned int port, const char\\* cafile\\_path)\n**Description**  \nConfigure host, port and rootCA location used to connect to AWS IoT over Websocket. If the input strings for host and cafile\\_path are set to NULL, the default value will be used to connect. Must be called to load user settings right after `aws_iot_mqtt_client::setup` and before connect. The client must be configured in setup to use Websocket.\n\n**Syntax**\n\n\tobject.configWss(\"example.awsamazon.com\", 1234, \"./cafile\");\n\t\n**Parameters**  \n*host* - The endpoint to connect to. Must be a NULL-terminated string.  \n*port* - The port number to connect to.  \n*cafile_path* - The path of CA file on OpenWRT. Must be a NULL-terminated string.  \n\n**Returns**  \nNONE\\_ERROR if the configuration is successful.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nCONFIG\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"configBackoffTiming\"></a>\n### IoT\\_Error\\_t configBackoffTiming(unsigned int baseReconnectQuietTimeSecond, unsigned int maxReconnectQuietTimeSecond, unsigned int stableConnectionTimeSecond)  \n**Description**  \nConfigure the progressive back-off timing on reconnect. Time interval for reconnect attempt will increase/double from baseReconnectQuietTimeSecond to maxReconnectQuietTimeSecond. Once a connection is live for longer than stableConnectionTimeSecond, the time interval will be reset to baseReconnectQuietTimeSecond. Note that stableConnectionTimeSecond must be greater than baseReconnectQuietTimeSecond. More details about progressive back-off can be found [here](#progressiveBackoff).  \n\nNote that if this API is not called, the following default values will be used to configure the back-off timing:  \n\n\tbaseReconnectQuietTimeSecond = 1;\n\tmaxReconnectQuietTimeSecond = 128;\n\tstableConnectionTimeSecond = 20;\n\n**Syntax**  \n\n\tobject.configBackoffTiming(1, 32, 20); // Configure baseReconnectQuietTimeSecond to be 1 second. Configure maxReconnectQuietTimeSecond to be 32 seconds. Configure stableConnectionTimeSecond to be 20 seconds.\n\n**Parameters**  \n*baseReconnectQuietTimeSecond* - Number of seconds to start with for progressive back-off on reconnect.  \n*maxReconnectQuietTimeSecond* - Maximum number of seconds for progressive back-off on reconnect.   \n*stableConnectionTimeSecond* - Number of seconds for a valid connection to be considered stable.  \n\n<a name=\"configOfflinePublishQueue\"></a>\n### IoT\\_Error\\_t configOfflinePublishQueue(unsigned int queueSize, DropBehavior\\_t behavior)  \n**Description**  \nConfigure the internal queue size and its drop behavior once the queue is full in the Python runtime on the OpenWRT side. When the client is offline, publish requests will be queued up and get resent once the network connection is back. If the number of publish requests exceeds the maximum size of the queue configured, dropping will happen according the drop behavior configured by this API. More details about offline publish requests queuing can be found [here](#offlinePublishQueueDraining).  \n\n**Syntax**  \n\n\tobject.configOfflinePublishQueue(20, DROP_OLDEST); // Configure size of the offline publish requests queue to be 20. Configure the queue to drop the oldest requests once the queue is full.\n\tobject.configOfflinePublishQueue(20, DROP_NEWEST); // Configure size of the offline publish requests queue to be 20. Configure the queue to drop the newest requests once the queue is full.\n\tobject.configOfflinePublishQueue(0, DROP_OLDEST); // Configure size of the offline publish requests queue to be infinite. The queue-full drop behavior is ignored when the size if infinite.\n\tobject.configOfflinePublishQueue(-1, DROP_OLDEST); // Disable the offline publish requests queue. The queue-full drop behavior is ignored when the queue is disabled.\n\t\n**Parameters**  \n*queueSize* - The size of the offline publish requests queue, determining how many publish requests can be queued up while the client it offline.  \n*behavior* - The drop behavior when the offline publish requests queue is full. Can be configured to drop the oldest requests or the newest requests in the queue.  \n\n**Returns**  \nNONE\\_ERROR if the configuration is successful.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nCONFIG\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"configDrainingInterval\"></a>\n### IoT\\_Error\\_t configDrainingInterval(float numberOfSeconds)  \n**Description**  \nConfigure the draining interval for requests to be sent out when client is back online and gets connected. This will affect the outbound rate for republish and resubscribe requests. More details about draining can be found [here](#offlinePublishQueueDraining).  \n\n**Syntax**  \n\n\tobject.configDrainingInterval(0.5); // Configure the draining interval to be 0.5 seconds. In this way, resubscribe requests, if any, will be sent per 0.5 seconds. Offline publish requests, if any in the queue, will be sent per 0.5 seconds.\n\n**Parameters**  \n*numberOfSeconds* - Number of seconds to wait between every resubscribe/republish request when the client is back online and connected.  \n\n**Returns**  \nNONE\\_ERROR if the configuration is successful.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nCONFIG\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.  \n\n<a name=\"connect\"></a>\n### IoT\\_Error\\_t connect(unsigned int keepalive\\_interval)\n**Description**  \nConnect to AWS IoT, using user-specific keepalive setting.\n\n**Syntax**\n\t\n\tobject.connect(); // connect to AWS IoT with default keepalive set to 60 seconds\n\tobject.connect(55); // connect to AWS IoT with keepalive set to 55 seconds\n\t\n**Parameters**  \n*keepalive\\_interval* - amount of time for MQTT ping request interval, in seconds. Default is set to 60 seconds.\n\n**Returns**  \nNONE\\_ERROR if the connect is successful.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nCONNECT\\_SSL\\_ERROR if the TLS handshake failed.  \nCONNECT\\_ERROR if the connection failed.  \nCONNECT\\_TIMEOUT if the connection gets timeout.  \nCONNECT\\_CREDENTIAL\\_NOT\\_FOUND if the specified credentials are not found on OpenWRT.  \nWEBSOCKET\\_CREDENTIAL\\_NOT\\_FOUND if the IAM credentials do not exist as environment variables on OpenWRT.  \nCONNECT\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"publish\"></a>\n### IoT\\_Error\\_t publish(const char\\* topic, const char\\* payload, unsigned int payload\\_len, unsigned int qos, bool retain)\n**Description**  \nPublish a new message to the desired topic with qos and retain flag settings using MQTT protocol\n\n**Syntax**\n\n    object.publish(\"myTopic\", \"myMessage\", strlen(\"myMessage\"), 0, false); // publish \"myMessage\" to topic \"myTopic\" in QoS 0 with retain flag set to false\n\n**Parameters**  \n*topic* - Topic name to publish to. Must be a NULL-terminated string.  \n*payload* - Payload to publish.  \n*payload_len* - Length of payload.  \n*qos* - Qualiy of service, could be 0 or 1.  \n*retain* - retain flag.\n\n**Returns**  \nNONE\\_ERROR if the publish is successful.  \nNULL\\_VALUE\\_ERROR if input parameters have NULL value.  \nOVERFLOW\\_ERROR if topic/payload exceeds the internal buffer size.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nPUBLISH\\_ERROR if the publish failed.  \nPUBLISH\\_TIMEOUT if the publish gets timeout.  \nPUBLISH\\_QUEUE\\_FULL if the internal offline publish requests queue is full.  \nPUBLISH\\_QUEUE\\_DISABLED if the internal offline publish requests queue is disabled.  \nPUBLISH\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"subscribe\"></a>\n### IoT\\_Error\\_t subscribe(const char\\* topic, unsigned int qos, message\\_callback cb)\n**Description**  \nSubscribe to the desired topic and register a callback for new messages from this topic. \n\n**Syntax**\n\n    object.subscribe(\"myTopic\", 0, myCallbackFunc); // subscribe to topic \"myTopic\" in QoS 0 and register its callback function as myCallbackFunc\n\n**Parameters**  \n*topic* - The topic to subscribe to. Must be a NULL-terminated string.  \n*qos* - Quality of service, could be 0 or 1.  \n*cb* - Function pointer to user-specific callback function to call when a new message comes in for the subscribed topic. The callback function should have a parameter list of (char*, unsigned int, Message_status_t) to store the incoming message content and the length of the message.\n\n**Returns**  \nNONE\\_ERROR if the subscribe is successful.  \nNULL\\_VALUE\\_ERROR if input parameters have NULL value.  \nOVERFLOW\\_ERROR if topic/payload exceeds the internal buffer size.  \nOUT\\_OF\\_SKETCH\\_SUBSCRIBE\\_MEMORY if the number of current subscribe exceeds the configured number in aws\\_iot\\_config\\_SDK.h.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nSUBSCRIBE\\_ERROR if the subscribe failed.  \nSUBSCRIBE\\_TIMEOUT if the subscribe gets timeout.  \nSUBSCRIBE\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"unsubscribe\"></a>\n### IoT\\_Error\\_t unsubscribe(const char\\* topic)\n**Description**  \nUnsubscribe to the desired topic.\n\n**Syntax**\n\n    object.unsubscribe(\"myTopic\");\n\n**Parameters**  \n*topic* - The topic to unsubscribe to. Must be a NULL-terminated string.\n\n**Returns**  \nNONE\\_ERROR if the unsubscribe is successful.  \nNULL\\_VALUE\\_ERROR if input parameters have NULL value.  \nOVERFLOW\\_ERROR if topic exceeds the internal buffer size.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nUNSUBSCRIBE\\_ERROR if the unsubscribe failed.  \nUNSUBSCRIBE\\_TIMEOUT if the unsubscribe gets timeout.  \nUNSUBSCRIBE\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"yield\"></a>\n### IoT\\_Error\\_t yield()\n**Description**  \nCalled in the loop to check if there is a new message from all subscribed topics, as well as thing shadow topics. Registered callback functions will be called according to the sequence of messages if there is any. Specifically, unnecessary shadow thing topics (accepted/rejected) will be unsubscribed according to the incoming new messages to free subscribe slots. Users should call this function frequently to receive new messages and free subscribe slots for new subscribes, especially for shadow thing requests.\n \n**Syntax**\n\n    object.yield();\n\n**Parameters**  \nNone\n\n**Returns**  \nNONE\\_ERROR if the yield is successful, whether there is a new message or not.  \nOVERFLOW\\_ERROR if the new message exceeds the internal buffer size.  \nYIELD\\_ERROR if the yield failed.\n\n<a name=\"disconnect\"></a>\n### IoT\\_Error\\_t disconnect()\n**Description**  \nDisconnect from AWS IoT.\n\n**Syntax**\n\n\tobject.disconnect();\n\t\n**Parameters**  \nNone\n\n**Returns**  \nNONE\\_ERROR if disconnect is successful.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nDISCONNECT\\_ERROR if the disconnect failed.  \nDISCONNECT\\_TIMEOUT if the disconnect gets timeout.  \nDISCONNECT\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"shadow_init\"></a>\n### IoT\\_Error\\_t shadow\\_init(const char\\* thingName)\n**Description**  \nInitialize thing shadow configuration and a shadow instance with thingName. Should be called before any of the thing shadow API call for thingName shadow operations.\n\n**Syntax**\n\n\tobject.shadow_init(\"NewThingName\"); // Init thing shadow configuration and set thing name to \"NewThingName\"\n\n**Parameters**  \n*thingName* - Thing name for the shadow instance to be created. Must be a NULL-terminated string.\n\n**Returns**  \nNONE\\_ERROR if thing shadow is successfully initialized.  \nNULL\\_VALUE\\_ERROR if input parameters have NULL value.  \nOVERFLOW\\_ERROR if thing name exceeds the internal buffer size.  \nSHADOW\\_INIT\\_ERROR if thing shadow initialization failed.  \nGENERIC\\_ERROR if an unknown error happens. \n\n<a name=\"shadow_update\"></a>\n### IoT\\_Error\\_t shadow\\_update(const char\\* thingName, const char* payload, unsigned int payload_len, message\\_callback cb, unsigned int timeout)\n**Description**  \nUpdate the thing shadow data in the cloud by publishing a new JSON file onto the corresponding thing shadow topic and subscribing accepted/rejected thing shadow topics to get feedback of whether it is a successful/failed request. Timeout can be set in seconds as the maximum waiting time for feedback. Once the request gets timeout, a timeout message will be received. The registered callback function will be called whenever there is an accepted/rejected/timeout feedback. Subscription to accepted/rejected topics will be processed in a persistent manner and will not be unsubscribed once this API is called for this shadow.\n\n**Syntax**\n\n\tobject.shadow_update(\"UserThingName\", JSON_FILE, strlen(JSON_FILE), UserCallbackFunction, 5); // update the data of \"UserThingName\" thing shadow in the cloud to JSON_FILE, with a timeout of 5 seconds and UserCallbackFunction as the callback function\n\n**Parameters**  \n*thingName* - The name of the thing shadow in the cloud. Must be a NULL-terminated string.  \n*payload* - The data that needs to be updated into the cloud, in JSON file format.  \n*payload_len* - Length of payload  \n*cb* - Function pointer to user-specific callback function to call when a new message comes in for the subscribed topic. The callback function should have a parameter list of (char\\*, unsigned int, Message_status_t) to store the incoming message content and the length of the message.  \n*timeout* - The maximum time to wait for feedback.  \n\n\n**Returns**  \nNONE\\_ERROR if the shadow update request succeeds.  \nNULL\\_VALUE\\_ERROR if input parameters have NULL value.  \nOVERFLOW\\_ERROR if thing name/payload exceeds the internal buffer size.  \nOUT\\_OF\\_SKETCH\\_SUBSCRIBE\\_MEMORY if the number of current subscribe exceeds the configured number in aws\\_iot\\_config\\_SDK.h.  \nNO\\_SHADOW\\_INIT\\_ERROR if the shadow with thingName is initialized before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nSUBSCRIBE\\_ERROR if the subscribe (accepted/rejected) failed.  \nSUBSCRIBE\\_TIMEOUT if the subscribe gets timeout.  \nPUBLISH\\_ERROR if the publish (payload) failed.  \nPUBLISH\\_TIMEOUT if the publish (payload) gets timeout.  \nPUBLISH\\_QUEUE\\_FULL if the publish action failed because of a full internal offline publish requests queue.  \nPUBLISH\\_QUEUE\\_DISABLED if the publish action failed because of a disabled internal offline publish requests queue.  \nSHADOW\\_UPDATE\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens. \n\n<a name=\"shadow_get\"></a>\n### IoT\\_Error\\_t shadow\\_get(const char\\* thingName, message\\_callback cb, unsigned int timeout)\n**Description**  \nObtain the thing shadow data in the cloud by publishing an empty JSON file onto the corresponding thing shadow topic and subscribing accepted/rejected thing shadow topics to get feedback of whether it is a successful/failed request. Timeout can be set in seconds as the maximum waiting time for feedback. Once the request gets timeout, a timeout message will be received. The registered callback function will be called whenever there is an accepted/rejected/timeout feedback. Subscription to accepted/rejected topics will be processed in a persistent manner and will not be unsubscribed once this API is called for this shadow. Thing shadow data will be available as a JSON file in the callback.\n\n**Syntax**  \n\n\tobject.shadow_get(\"UserThingName\", UserCallbackFunction, 5); // get the data of the thing shadow \"UserThingName\", with a timeout of 5 seconds and UserCallbackFunction as the callback function\n\n**Parameters**  \n*thingName* - The name of the thing shadow in the cloud. Must be a NULL-terminated string.  \n*cb* - Function pointer to user-specific callback function to call when a new message comes in for the subscribed topic. The callback function should have a parameter list of (char\\*, unsigned int, Message_status_t) to store the incoming message content and the length of the message.  \n*timeout* - The maximum time to wait for feedback.  \n\n**Returns**  \nNONE\\_ERROR if the shadow get request succeeds.  \nNULL\\_VALUE\\_ERROR if input parameters have NULL value.  \nOVERFLOW\\_ERROR if thing name exceeds the internal buffer size.  \nOUT\\_OF\\_SKETCH\\_SUBSCRIBE\\_MEMORY if the number of current subscribe exceeds the configured number in aws\\_iot\\_config\\_SDK.h.  \nNO\\_SHADOW\\_INIT\\_ERROR if the shadow with thingName is initialized before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nSUBSCRIBE\\_ERROR if the subscribe (accepted/rejected) failed.  \nSUBSCRIBE\\_TIMEOUT if the subscribe gets timeout.  \nPUBLISH\\_ERROR if the publish (payload) failed.  \nPUBLISH\\_TIMEOUT if the publish (payload) gets timeout.  \nPUBLISH\\_QUEUE\\_FULL if the publish action failed because of a full internal offline publish requests queue.  \nPUBLISH\\_QUEUE\\_DISABLED if the publish action failed because of a disabled internal offline publish requests queue.  \nSHADOW\\_GET\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"shadow_delete\"></a>\n### IoT\\_Error\\_t shadow\\_delete(const char\\* thingName, message\\_callback cb, unsigned int timeout)\n**Description**  \nDelete the thing shadow data in the cloud by publishing an empty JSON file onto the corresponding thing shadow topic and subscribing accepted/rejected thing shadow topics to get feedback of whether it is a successful/failed request. Timeout can be set in seconds as the maximum waiting time for feedback. Once the request gets timeout, a timeout message will be received. The registered callback function will be called whenever there is an accepted/rejected/timeout feedback. After the feedback comes in, it will automatically unsubscribe accepted/rejected shadow topics. \n\n**Syntax**  \n\n\tobject.shadow_delete(\"UserThingName\", UserCallbackFunction, 5); // delete the data of the thing shadow \"UserThingName\", with a timeout of 5 seconds and UserCallbackFunction as the callback function\n\n**Parameters**  \n*thingName* - The name of the thing shadow in the cloud. Must be a NULL-terminated string.  \n*cb* - Function pointer to user-specific callback function to call when a new message comes in for the subscribed topic. The callback function should have a parameter list of (char\\*, unsigned int, Message_status_t) to store the incoming message content and the length of the message.  \n*timeout* - The maximum time to wait for feedback.  \n\n**Returns**  \nNONE\\_ERROR if the shadow delete request succeeds.  \nNULL\\_VALUE\\_ERROR if input parameters have NULL value.  \nOVERFLOW\\_ERROR if thing name exceeds the internal buffer size.  \nOUT\\_OF\\_SKETCH\\_SUBSCRIBE\\_MEMORY if the number of current subscribe exceeds the configured number in aws\\_iot\\_config\\_SDK.h.  \nNO\\_SHADOW\\_INIT\\_ERROR if the shadow with thingName is initialized before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nSUBSCRIBE\\_ERROR if the subscribe (accepted/rejected) failed.  \nSUBSCRIBE\\_TIMEOUT if the subscribe gets timeout.  \nPUBLISH\\_ERROR if the publish (payload) failed.  \nPUBLISH\\_TIMEOUT if the publish (payload) gets timeout.  \nPUBLISH\\_QUEUE\\_FULL if the publish action failed because of a full internal offline publish requests queue.  \nPUBLISH\\_QUEUE\\_DISABLED if the publish action failed because of a disabled internal offline publish requests queue.  \nSHADOW\\_DELETE\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"shadow_register_delta_func\"></a>\n### IoT\\_Error\\_t shadow\\_register\\_delta\\_func(const char\\* thingName, message\\_callback cb)\n**Description**  \nSubscribe to the delta topic of the corresponding thing shadow with the given name and register a callback. Whenever there is a difference between the desired and reported state data, the registered callback will be called and the feedback/message will be available in the callback.\n\n**Syntax**\n\n\tobject.shadow_register_delta_func(\"UserThingName\", UserCallBackFunction); // register UserCallbackFunction as the  delta callback function for the thing shadow \"UserThingName\"\n\n**Parameters**  \n*thingName* - The name of the thing shadow in the cloud. Must be a NULL-terminated string.  \n*cb* - Function pointer to user-specific callback function to call when a new message comes in for the subscribed topic. The callback function should have a parameter list of (char\\*, unsigned int, Message_status_t) to store the incoming message content and the length of the message.\n\n**Return**  \nNONE\\_ERROR if the shadow delta topic is successfully subscribed and the callback function is successfully registered.  \nNULL\\_VALUE\\_ERROR if input parameters have NULL value.  \nOVERFLOW\\_ERROR if thing name exceeds the internal buffer size.  \nOUT\\_OF\\_SKETCH\\_SUBSCRIBE\\_MEMORY if the number of current subscribe exceeds the configured number in aws\\_iot\\_config\\_SDK.h.  \nNO\\_SHADOW\\_INIT\\_ERROR if the shadow with thingName is initialized before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nSUBSCRIBE\\_ERROR if the subscribe (accepted/rejected) failed.  \nSUBSCRIBE\\_TIMEOUT if the subscribe gets timeout.  \nSHADOW\\_REGISTER\\_DELTA\\_CALLBACK\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"shadow_unregister_delta_func\"></a>\n### IoT\\_Error\\_t shadow\\_unregister\\_delta\\_func(const char\\* thingName)\n**Description**  \nUnsubscribe to the delta topic of the corresponding thing shadow with the given name and unregister the callback. There will be no message coming after this API call if another difference occurs between the desired and reported state data for this thing shadow.\n\n**Syntax**\n\n\tobject.shadow_unregister_delta_func(\"UserThingName\"); // unregister the delta topic of the thing shadow \"UserThingName\"\n\n**Parameters**  \n*thingName* - The name of the thing shadow in the cloud. Must be a NULL-terminated string.\n\n**Returns**  \nNONE\\_ERROR if the shadow delta topic is successfully unsubscribed and the callback function is successfully unregistered.  \nNULL\\_VALUE\\_ERROR if input parameters have NULL value.  \nOVERFLOW\\_ERROR if thing name exceeds the internal buffer size.  \nNO\\_SHADOW\\_INIT\\_ERROR if the shadow with thingName is initialized before this call.  \nWRONG\\_PARAMETER\\_ERROR if there is an error for the Python Runtime to get enough input parameters for this command.  \nUNSUBSCRIBE\\_ERROR if the subscribe (accepted/rejected) failed.  \nUNSUBSCRIBE\\_TIMEOUT if the subscribe gets timeout.  \nSHADOW\\_UNREGISTER\\_DELTA\\_CALLBACK\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.  \nGENERIC\\_ERROR if an unknown error happens.\n\n<a name=\"getDesiredValueByKey\"></a>\n### IoT\\_Error\\_t getDesiredValueByKey(const char\\* JSONIdentifier, const char\\* key, char\\* externalJSONBuf, unsigned int bufSize)  \n**Description**  \nGet the value by key in the desired section in the shadow JSON document denoted by the provided identifier. The corresponding value will be stored as a string into a user-specified externalBuffer. Nested key-value access is available. More information can be found [here](#individualKVAccess).  \n\n**Syntax**  \n\n\tobject.getDesiredValueByKey(\"JSON-0\", \"property1\", someBuffer, someBufferSize); // Access JSONDocument[\"state\"][\"desired\"][\"property1\"] denoted by JSONIdentifier \"JSON-0\" and store the value in someBuffer\n\tobject.getDesiredValueByKey(\"JSON-0\", \"property2\\\"subproperty\", someBuffer, someBufferSize); // Access JSONDocument[\"state\"][\"desired\"][\"property2\"][\"subproperty\"] denoted by JSONIdentifier \"JSON-0\" and store the value in someBuffer \n\n**Parameters**  \n*JSONIdentifier* - The JSON Identifier string to access a certain JSON document stored in Python runtime on the OpenWRT side. This is obtained from the registered shadow callback as shadow responses.  \n*key* - The key for dereferencing out the value in the desired section of the JSON document. Nested key can be specified using `\"` as the delimiter.  \n*externalJSONBuf* - Buffer specified by the user to store the incoming value, as string.  \n*bufSize* - Size of the buffer to store the incoming value, as string.  \n\n**Returns**  \nNONE\\_ERROR if the value is retrieved successfully.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nOVERFLOW\\_ERROR if the length of the incoming value exceeds the size of the provided externalJSONBuf.  \nJSON\\_FILE\\_NOT\\_FOUND if the JSON document with the provided JSON identifier does not exist.  \nJSON\\_KEY\\_NOT\\_FOUND if the specified key does not exist in the JSON document denoted by the provided JSON identifier.  \nJSON\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.   \nGENERIC\\_ERROR if an unknown error happens.   \n\n<a name=\"getReportedValueByKey\"></a>\n### IoT\\_Error\\_t getReportedValueByKey(const char\\* JSONIdentifier, const char\\* key, char\\* externalJSONBuf, unsigned int bufSize)]  \n**Description**  \nGet the value by key in the reported section in the shadow JSON document denoted by the provided identifier. The corresponding value will be stored as a string into a user-specified externalBuffer. Nested key-value access is available. More information can be found [here](#individualKVAccess).  \n\n**Syntax**  \n\n\tobject.getReportedValueByKey(\"JSON-0\", \"property1\", someBuffer, someBufferSize); // Access JSONDocument[\"state\"][\"reported\"][\"property1\"] denoted by JSONIdentifier \"JSON-0\" and store the value in someBuffer\n\tobject.getReportedValueByKey(\"JSON-0\", \"property2\\\"subproperty\", someBuffer, someBufferSize); // Access JSONDocument[\"state\"][\"reported\"][\"property2\"][\"subproperty\"] denoted by JSONIdentifier \"JSON-0\" and store the value in someBuffer \n\n**Parameters**  \n*JSONIdentifier* - The JSON Identifier string to access a certain JSON document stored in Python runtime on the OpenWRT side. This is obtained from the registered shadow callback as shadow responses.  \n*key* - The key for dereferencing out the value in the reported section of the JSON document. Nested key can be specified using `\"` as the delimiter.  \n*externalJSONBuf* - Buffer specified by the user to store the incoming value, as string.  \n*bufSize* - Size of the buffer to store the incoming value, as string.  \n\n**Returns**  \nNONE\\_ERROR if the value is retrieved successfully.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nOVERFLOW\\_ERROR if the length of the incoming value exceeds the size of the provided externalJSONBuf.  \nJSON\\_FILE\\_NOT\\_FOUND if the JSON document with the provided JSON identifier does not exist.  \nJSON\\_KEY\\_NOT\\_FOUND if the spedified key does not exist in the JSON document denoted by the provided JSON identifier.  \nJSON\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.   \nGENERIC\\_ERROR if an unknown error happens.   \n\n<a name=\"getDeltaValueByKey\"></a>\n### IoT\\_Error\\_t getDeltaValueByKey(const char\\* JSONIdentifier, const char\\* key, char\\* externalJSONBuf, unsigned int bufSize)]  \n**Description**  \nGet the value by key in the state section in the shadow JSON document denoted by the provided identifier. The corresponding value will be stored as a string into a user-specified externalBuffer. Nested key-value access is available. More information can be found [here](#individualKVAccess).  \n\n**Syntax**  \n\n\tobject.getDeltaValueByKey(\"JSON-0\", \"property1\", someBuffer, someBufferSize); // Access JSONDocument[\"state\"][\"property1\"] denoted by JSONIdentifier \"JSON-0\" and store the value in someBuffer\n\tobject.getDeltaValueByKey(\"JSON-0\", \"property2\\\"subproperty\", someBuffer, someBufferSize); // Access JSONDocument[\"state\"][\"property2\"][\"subproperty\"] denoted by JSONIdentifier \"JSON-0\" and store the value in someBuffer \n\n**Parameters**  \n*JSONIdentifier* - The JSON Identifier string to access a certain JSON document stored in Python runtime on the OpenWRT side. This is obtained from the registered shadow callback as shadow responses.  \n*key* - The key for dereferencing out the value in the state section of the JSON document. Nested key can be specified using `\"` as the delimiter.  \n*externalJSONBuf* - Buffer specified by the user to store the incoming value, as string.  \n*bufSize* - Size of the buffer to store the incoming value, as string.  \n\n**Returns**  \nNONE\\_ERROR if the value is retrieved successfully.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nOVERFLOW\\_ERROR if the length of the incoming value exceeds the size of the provided externalJSONBuf.  \nJSON\\_FILE\\_NOT\\_FOUND if the JSON document with the provided JSON identifier does not exist.  \nJSON\\_KEY\\_NOT\\_FOUND if the specified key does not exist in the JSON document denoted by the provided JSON identifier.  \nJSON\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.   \nGENERIC\\_ERROR if an unknown error happens.   \n\n<a name=\"getValueByKey\"></a>\n### IoT\\_Error\\_t getValueByKey(const char\\* JSONIdentifier, const char\\* key, char\\* externalJSONBuf, unsigned int bufSize)]  \n**Description**  \nGet the value by key in the shadow JSON document denoted by the provided identifier. The corresponding value will be stored as a string into a user-specified externalBuffer. Nested key-value access is available. More information can be found [here](#individualKVAccess).  \n\n**Syntax**  \n\n\tobject.getValueByKey(\"JSON-0\", \"property1\", someBuffer, someBufferSize); // Access JSONDocument[\"property1\"] denoted by JSONIdentifier \"JSON-0\" and store the value in someBuffer\n\tobject.getValueByKey(\"JSON-0\", \"property2\\\"subproperty\", someBuffer, someBufferSize); // Access JSONDocument[\"property2\"][\"subproperty\"] denoted by JSONIdentifier \"JSON-0\" and store the value in someBuffer \n\n**Parameters**  \n*JSONIdentifier* - The JSON Identifier string to access a certain JSON document stored in Python runtime on the OpenWRT side. This is obtained from the registered shadow callback as shadow responses.  \n*key* - The key for dereferencing out the value in the JSON document. Nested key can be specified using `\"` as the delimiter.  \n*externalJSONBuf* - Buffer specified by the user to store the incoming value, as string.  \n*bufSize* - Size of the buffer to store the incoming value, as string.  \n\n**Returns**  \nNONE\\_ERROR if the value is retrieved successfully.  \nNO\\_SET\\_UP\\_ERROR if no setup is called before this call.  \nOVERFLOW\\_ERROR if the length of the incoming value exceeds the size of the provided externalJSONBuf.  \nJSON\\_FILE\\_NOT\\_FOUND if the JSON document with the provided JSON identifier does not exist.  \nJSON\\_KEY\\_NOT\\_FOUND if the specified key does not exist in the JSON document denoted by the provided JSON identifier.  \nJSON\\_GENERIC\\_ERROR if there is an error in executing the command in Python Runtime.   \nGENERIC\\_ERROR if an unknown error happens.   \n\n<a name=\"message_callback\"></a>\n### void(\\*message\\_callback)(char\\*, unsigned int, Message\\_status\\_t)]  \n**Description**  \nCallback function for received MQTT messages, used for plain MQTT communications as well as shadow communications.  \nFor plain MQTT messages, message payload and size will be passed into the callback. Message\\_status\\_t will be STATUS\\_NORMAL. See Parameters below for more details.  \nFor shadow messages, JSON identifier and its size will be passed into the callback. Message\\_status\\_t varies for shadow messages from different topics. For accept shadow responses, Message\\_status\\_t will be STATUS\\_SHADOW\\_ACCEPTED. For reject shadow responses, Message\\_status\\_t will be STATUS\\_SHADOW\\_REJECTED. For delta shadow responses, Message\\_status\\_t will be STATUS\\_NORMAL. See Parameters below for more details.  \n\n**Syntax**  \n\n\tvoid custom_message_callback(char* msg, unsigned int length, Message_status_t status) {\n\t\t// * Access the incoming message from msg, length\n\t\t// Message payload for plain MQTT messages\n\t\t// JSON identifer for shadow messages\n\t\t// * Access the message status from status\n\t\t// STATUS_NORMAL for plain MQTT/shadow delta messages\n\t\t// STATUS_SHADOW_ACCEPTED for shadow accept responses\n\t\t// STATUS_SHADOW_REJECTED for shadow reject responses\n\t}\n\n**Parameters**  \nchar\\* - Incoming message. Message payload for plain MQTT messages and JSON identifier for shadow message/responses.  \nunsigned int - Length of bytes of the incoming message.  \nMessage\\_status\\_t - Status of the incoming responses/messages. It has the following values:   \n\n\ttypedef enum {\n\t\tSTATUS_DEBUG = -1,\n\t\tSTATUS_NORMAL = 0,\n\t\tSTATUS_SHADOW_TIMEOUT = 1,\n\t\tSTATUS_SHADOW_ACCEPTED = 2,\n\t\tSTATUS_SHADOW_REJECTED = 3,\n\t\tSTATUS_MESSAGE_OVERFLOW = 4\n\t} Message_status_t;\n\n`STATUS_NORMAL` indicates that a new plain MQTT message/shadow delta message has arrived.  \n`STATUS_SHADOW_TIMEOUT` indicates that the incoming message is a shadow response for an operation timeout. There was no response received for the corresponding shadow operation within the preconfigured timeout.  \n`STATUS_SHADOW_ACCEPTED` indicates that the incoming message is a shadow response for accept. The corresponding shadow operation was accepted by the AWS IoT service and has succeeded.  \n`STATUS_SHADOW_REJECTED` indicates that the incoming message is a shadow response for reject. The corresponding shadow operation was rejected by the AWS IoT service and has failed.  \n`STATUS_MESSAGE_OVERFLOW` indicates that the size of the incoming message has exceeded the size of the internal message buffer. Internal message buffer size, configured in `aws_iot_config_SDK.h`, needs to be increased to receive the complete incoming message.  \n`STATUS_DEBUG` is for SDK internal use.  \n\n**Returns**  \nNo returns.  \n\n****\n\n<a name=\"keyfeatures\"></a>\n## Key features  \n<a name=\"individualKVAccess\"></a>\n#### Individual Key Value Access for Shadow  \nAs for shadow operations (Get/Update/Delete) and shadow delta messages, instead of detailed the message content for shadow JSON document, a JSON identifier will be passed through into the registered callback so that it can be used for key/value pair access on demand through the individual key/value pair access APIs. A JSON identifier of a shadow JSON response received looks like this:  \n\n\tJSON-i\n\nwhere i is an integer number.  \n\nNote that there is a limitation on the total number of history JSON documents that can be kept on the OpenWRT side for key/value pair retrieval. The limits are:  \n\n512 entries for shadow JSON accepted responses  \n512 entries for shadow JSON rejected responses  \n512 entries for shadow JSON delta messages  \n\nOnce the limits are exceeded, new incoming shadow JSON documents will overwrite history entries starting from the beginning (`JSON-0`, `JSON-1` and `JSON-2`).  \n\nThe following APIs are provided for uses to access shadow JSON key value pair from Arduino sketch in an easier manner:  \n[IoT\\_Error\\_t getDesiredValueByKey(const char\\* JSONIdentifier, const char\\* key, char\\* externalJSONBuf, unsigned int bufSize)](#getDesiredValueByKey)  \n[IoT\\_Error\\_t getReportedValueByKey(const char\\* JSONIdentifier, const char\\* key, char\\* externalJSONBuf, unsigned int bufSize)](#getReportedValueByKey)  \n[IoT\\_Error\\_t getDeltaValueByKey(const char\\* JSONIdentifier, const char\\* key, char\\* externalJSONBuf, unsigned int bufSize)](#getDeltaValueByKey)  \n[IoT\\_Error\\_t getValueByKey(const char\\* JSONIdentifier, const char\\* key, char\\* externalJSONBuf, unsigned int bufSize)](#getValueByKey)  \n\nFor a typical shadow JSON document (responses for get/update/delete), it should look like this:  \n\n\t{\n\t\t\"state\": {\n\t\t\t\"desired\": {\n\t\t\t\t...\n\t\t\t},\n\t\t\t\"reported\": {\n\t\t\t\t...\n\t\t\t}\n\t\t},\n\t\t...\n\t}\n\n`getDesiredValueByKey` and `getReportedValueByKey` can be used to access the key value pair in the desired/reported section respectively.  \n\nFor a typical shadow JSON document (messages for delta), it should look like this:  \n\n\t{\n\t\t\"state\": {\n\t\t\t...\n\t\t},\n\t\t...\n\t}\n\n`getDeltaValueByKey` can be used to access the key value pair in the delta shadow JSON messages.  \n\nMore generally, `getValueByKey` can be used to access key value pair in a more generic way where users can specify their own key patterns. Nested JSON key is denoted using `\"` as the delimiter.  \n\nFor example, we have the following shadow JSON document with a JSON identifier `JSON-0`:  \n\n\t{\n\t\t\"state\": {\n\t\t\t\"desired\": {\n\t\t\t\t\"property1\": \"value1\",\n\t\t\t\t\"property2\": {\n\t\t\t\t\t\"subproperty\": \"value2\"\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"reported\": {\n\t\t\t\t\"property3\": \"value3\"\n\t\t\t}\n\t\t},\n\t\t...\n\t}\n\nTo access a series of key value pair, we can use the following function calls:  \n\n\tobject.getDesiredValueByKey(\"JSON-0\", \"property1\", buffer, bufferSize); // Access JSONDocument[\"state\"][\"desired\"][\"property1\"]\n\tobject.getReportedValueByKey(\"JSON-0\", \"property3\", buffer, bufferSize); // Access JSONDocument[\"state\"][\"reported\"][\"property3\"]\n\tobject.getDesiredValueByKey(\"JSON-0\", \"property2\\\"subproperty\", buffer, bufferSize); // Access JSONDocument[\"state\"][\"desired\"][\"property2\"][\"subproperty\"]\n\t\nNote that the following two function calls are equivalent. They both access the nested JSON key value pair `JSONDocument[\"state\"][\"desired\"][\"property2\"][\"subproperty\"]`:  \n\n\tobject.getDesiredValueByKey(\"JSON-0\", \"property2\\\"subproperty\", buffer, bufferSize);\n\tobject.getValueByKey(\"JSON-0\", \"state\\\"desired\\\"property2\\\"subproperty\", buffer, bufferSize);\n\nSee that `getValueByKey` is a more generic way for shadow JSON key value access.  \n\nFor detailed use cases, please check out [Examples](#example).\n\n<a name=\"progressiveBackoff\"></a>\n#### Progressive Reconnect Back-off  \nAPI is provided to configure the progressive back-off timing parameters:  \n[IoT\\_Error\\_t configBackoffTiming(unsigned int baseReconnectQuietTimeSecond, unsigned int maxReconnectQuietTimeSecond, unsigned int stableConnectionTimeSecond)](#configBackoffTiming)\n\nThe auto-reconnect happens with a progressive back-off, which follows the following mechanism for reconnect quiet time:   \n\n<h5 align=\"center\">t<sup>current</sup> = min(2<sup>n</sup>t<sup>base</sup>, t<sup>max</sup>),</h5>  \n\nwhere t<sup>current</sup> is the current reconnect quiet time, t<sup>base</sup> is the base reconnect quiet time, t<sup>max</sup> is the maximum reconnect quiet time.  \n\nThe reconnect quiet time will be doubled on disconnect and reconnect attempt until it reaches the preconfigured maximum reconnect quiet time. Once the connection is stable for over the stableConnectionTime, the reconnect quiet time will be reset to the baseReconnectQuietTime.  \n\nIf no `configBackoffTiming` gets called, the following default configuration for back-off timing will be done on `setup` call:  \n\n\tbaseReconnectQuietTimeSecond = 1;\n\tmaxReconnectQuietTimeSecond = 128;\n\tstableConnectionTimeSecond = 20;\n\n<a name=\"offlinePublishQueueDraining\"></a>\n#### Offline publish requests queuing with draining  \nAPIs are provided to configure the offline publish requests queuing (size and drop behavior) as well as draining intervals:  \n[IoT\\_Error\\_t configOfflinePublishQueue(unsigned int queueSize, DropBehavior\\_t behavior)](#configOfflinePublishQueue)  \n[IoT\\_Error\\_t configDrainingInterval(float numberOfSeconds)](#configDrainingInterval)  \n\nWhen the client is temporarily offline and gets disconnected due to some network failure, publish requests will be queued up into an internal queue in the Python runtime on the OpenWRT side until the number of queued-up requests reaches to the size limit of the queue. Once the queue is full, offline publish requests will be discarded or replaced according to different configuration of the drop behavior:  \n\n\ttypedef enum {\n\t\tDROP_OLDEST = 0,\n\t\tDROP_NEWEST = 1\n\t} DropBehavior_t;\n\nLets say we configure the size of offlinePublishQueue to be 5 and we have 7 offline publish requests coming in...  \n\nIn a `DROP_OLDEST` configuration:  \n\n\tmyClient.configOfflinePublishQueue(5, DROP_OLDEST);\n\nThe internal queue should be like this when the queue is just full:  \n\n\tHEAD ['pub_req0', 'pub_req1', 'pub_req2', 'pub_req3', 'pub_req4']\n\t\nWhen the 6th and the 7th publish requests are made offline, the internal queue will be like this:  \n\n\tHEAD ['pub_req2', 'pub_req3', 'pub_req4', 'pub_req5', 'pub_req6']\n\nSince the queue is already full, the oldest requests `pub_req0` and `pub_req1` are discarded.  \n\nIn a `DROP_NEWEST` configuration:  \n\n\tmyClient.configOfflinePublishQueue(5, DROP_NEWEST);\n\nThe internal queue should be like this when the queue is just full:  \n\n\tHEAD ['pub_req0', 'pub_req1', 'pub_req2', 'pub_req3', 'pub_req4']\n\t\nWhen the 6th and the 7th publish requests are made offline, the internal queue will be like this:  \n\n\tHEAD ['pub_req0', 'pub_req1', 'pub_req2', 'pub_req3', 'pub_req4']\n\nSince the queue is already full, the newest requests `pub_req5` and `pub_req6` are discarded.  \n\nWhen the client is back online, connected and resubscribed to all topics that it has previously subscribed to, the draining starts. All requests in the offline publish queue will be resent at the configured draining rate.  \n\nif no `configOfflinePublishQueue` or `configDrainingInterval` is called, the following default configuration for offline publish queuing and draining will be done on setup call:  \n\n\tofflinePublishQueueSize = 20\n\tdropBehavior = DROP_NEWEST\n\tdrainingInterval = 0.5 sec\n\nNote that before the draining process finishes, any new publish request within this time will be added to the queue. Therefore, draining rate should be higher than the normal publish rate to avoid an endless draining process after reconnect.  \n\nAlso note that disconnect event is detected based on PINGRESP MQTT packet loss. Offline publish queuing will NOT be triggered until the disconnect event gets detected. Configuring a shorter keep-alive interval allows the client to detect disconnects more quickly. Any QoS0 publish requests issued after the network failure and before the detection of the PINGRESP loss will be lost.  \n\n<a name=\"usingthesdk\"></a>\n## Using the SDK\n**Make sure you have properly installed the AWS-IoT-Arduino-Y\u00fan-SDK and setup the board.**\n\n<a name=\"slowstartup\"></a>\n**Make sure to start the sketch after openWRT is ready and gets connected to WiFi. It takes about 80-90 seconds for Arduino Y\u00fan board to start the openWRT and get connected to WiFi for each power cycle.**\n\n**Make sure you have properly configured SDK settings in `aws_iot_config.h` inside each sketch directory:**\n\n\t//===============================================================\n\t#define AWS_IOT_MQTT_HOST \"<RANDOM_STRING>.iot.<REGION>.amazonaws.com\" \t// your endpoint\n\t#define AWS_IOT_MQTT_PORT 8883\t\t\t\t\t\t\t\t\t// your port, use 443 for MQTT over Websocket\n\t#define AWS_IOT_CLIENT_ID\t\"My_ClientID\"\t\t\t\t\t\t// your client ID\n\t#define AWS_IOT_MY_THING_NAME \"My_Board\"\t\t\t\t\t\t// your thing name\n\t#define AWS_IOT_ROOT_CA_FILENAME \"aws-iot-rootCA.crt\"           // your root-CA filename\n\t#define AWS_IOT_CERTIFICATE_FILENAME \"cert.pem\"                 // your certificate filename\n\t#define AWS_IOT_PRIVATE_KEY_FILENAME \"privkey.pem\"              // your private key filename\n\t//===============================================================\n\n**These settings can be downloaded from the AWS IoT console after you created a device and clicked on \"Connect a device\".**  \t\n\n**Make sure you have included the AWS-IoT-Arduino-Y\u00fan-SDK library:**\n\n    #include <aws_iot_mqtt.h>\n    \n\n**Make sure you have included your configuration header file:**\n\n\t#include \"aws_iot_config.h\"\n\n**Make sure you have enough memory for subscribe, messages and sketch runtime. Internal buffer size is defined in SDK library source directory `libraries/AWS-IoT-Arduino-Yun-Library/aws_iot_config_SDK.h`. The following are default settings:**\n\n\t#define MAX_BUF_SIZE 256\t\t\t\t\t\t\t\t\t\t// maximum number of bytes to publish/receive\n\t#define MAX_SUB 15 \t\t\t\t\t\t\t\t\t\t\t\t// maximum number of subscribe\n\t#define CMD_TIME_OUT 200\t\t\t\t\t\t\t\t\t\t// maximum time to wait for feedback from AR9331, 200 = 10 sec\n\n**Make sure you setup the client, configure it using your configuration and connect it to AWS IoT first. Remember to use certs path macros for configuration:**\n\n    aws_iot_mqtt_client myClient;\n    myClient.setup(AWS_IOT_CLIENT_ID);\n    // myClient.setup(AWS_IOT_CLIENT_ID, true, MQTTv31, true); // Use Websocket\n    myClient.config(AWS_IOT_MQTT_HOST, AWS_IOT_MQTT_PORT, AWS_IOT_ROOT_CA_PATH, AWS_IOT_PRIVATE_KEY_PATH, AWS_IOT_CERTIFICATE_PATH);\n    // myClient.configWss(AWS_IOT_MQTT_HOST, AWS_IOT_MQTT_PORT, AWS_IOT_ROOT_CA_PATH); // Use Websocket\n    myClient.connnect();\n\n**Remember to check incoming messages in a loop**:\n\n    void loop() { \n      ...  \n      myClient.yield();\n      ...\n    }\n\n**When you are using thing shadow API for a specific device shadow name, make sure you initialize the shadow with your device shadow name first:**\n\n\tmyClient.shadow_init(AWS_IOT_MY_THING_NAME); // For shadow of this device\n\tmyClient.shadow_init(\"AnotherDevice\"); // For another shadow\n\n**When you are using thing shadow API, always make sure MAX\\_SUB is big enough for a thing shadow request in the loop:**\n\n\t...\n\tmyClient.shadow_get(\"myThingName\", myCallback, 5); // need 2 in MAX_SUB\n\t...\n\tvoid loop() {\n\t\t...\n\t\tmyClient.shadow_get(\"myThingName\", myCallback, 5); // need 4 in MAX_SUB\n\t\tmyClient.yield(); // unsubscribe thing shadow topics when necessary\n\t\t...\n\t}\n\n**When you are using thing shadow API, make sure you set the timeout to a proper value and frequently call yield to free subscribe resources. Long timeout with low rate of yielding and high rate of shadow request will result in exhaustion of subscribe resources:**\n\n\tvoid loop() {\n\t\t...\n\t\tmyClient.shadow_get(\"myThingName\", myCallback, 5); // 5 sec timeout is fine for a request per 5 sec\n\t\t\n\t\t// myClient.shadow_get(\"myThingName\", myCallback, 50);\n\t\t// 50 sec timeout is too long. When missing feedback happens frequently, with a rate of 1 request per 5 sec, subscribed topics will soon accumulate and exceed MAX_SUB before any of the previously-subscribed topic gets timeout and unsubscribed\n\t\t\n\t\tmyClient.yield();\n\t\t\n\t\tdelay(5000); // 5 sec delay\n\t\t...\n\t}\n\n**Enjoy the Internet of Things!**\n\n****\n\n<a name=\"example\"></a>\n## Example\n### BasicPubSub\nThis [example](https://github.com/aws/aws-iot-device-sdk-arduino-yun/tree/master/AWS-IoT-Arduino-Yun-Library/examples/BasicPubSub) demonstrates a simple MQTT publish/subscribe using AWS IoT from Arduino Y\u00fan board. It first subscribes to a topic once and registers a callback to print out new messages to Serial monitor and then publishes to the topic in a loop. Whenever it receives a new message, it will be printed out to Serial monitor indicating the callback function has been called.\n\n* **Hardware Required**  \nArduino Y\u00fan  \nComputer connected with Arduino Y\u00fan using USB serial\n\n* **Software Required**  \nNone\n\n* **Circuit Required**  \nNone\n\n* **Attention**  \nPlease make sure to start the example sketch after the board is fully set up and openWRT is up and connected to WiFi. See [here](#slowstartup).\n\n* **Code**  \n\tCreate an instance of aws\\_iot\\_mqtt\\_client. \n\n\t\taws_iot_mqtt_client myClient;\n\t\t\n\tIn `setup()`, open the Serial. Set the instance up and connect it to the AWS IoT.\n\n\t\tSerial.begin(115200);\n\t\t...\n \t\tif((rc = myClient.setup(AWS_IOT_CLIENT_ID)) == 0) {\n    \t\t// Load user configuration\n    \t\tif((rc = myClient.config(AWS_IOT_MQTT_HOST, AWS_IOT_MQTT_PORT, AWS_IOT_ROOT_CA_PATH, AWS_IOT_PRIVATE_KEY_PATH, AWS_IOT_CERTIFICATE_PATH)) == 0) {\n      \t\t\t// Use default connect: 60 sec for keepalive\n      \t\t\tif((rc = myClient.connect()) == 0) {\n        \t\t\tsuccess_connect = true;\n        \t\t\t...\n        \t\t}\n        \t\telse {...}\n        \t}\n        \telse {...}\n        }\n        else {...}\n        ...\n          \t\t\n  \tIn `setup()`, subscribe to the desired topic and wait for some delay time.\n  \t\n  \t  \tif((rc = myClient.subscribe(\"topic1\", 1, msg_callback)) != 0) {\n    \t\tSerial.println(F(\"Subscribe failed!\"));\n    \t\tSerial.println(rc);\n    \t}\n    \tdelay(2000);\n    \t\n  \tIn `loop()`, publish to this topic and call yield function to receive the message every 5 seconds.\n  \t\n  \t\tsprintf(msg, \"new message %d\", cnt);\n  \t\tif((rc = myClient.publish(\"topic1\", msg, strlen(msg), 1, false)) != 0) {\n  \t\t\tSerial.println(F(\"Publish failed!\"));\n  \t\t\tSerial.println(rc);\n  \t\t}\n\t\tif((rc = myClient.yield()) != 0) {\n\t\t\tSerial.println(F(\"Yield failed!\"));\n\t\t\tSerial.println(rc);\n\t\t}\n\t\t...\n\t\tdelay(5000);\n\t\t\n\tThe full sketch can be found in `AWS-IoT-Arduino-Yun-Library/examples/BasicPubSub`.\n\n### ThingShadowEcho sample app\nThis [example](https://github.com/aws/aws-iot-device-sdk-arduino-yun/tree/master/AWS-IoT-Arduino-Yun-Library/examples/ThingShadowEcho) demonstrates Arduino Y\u00fan board as a device communicating with AWS IoT, syncing data into the thing shadow in the cloud and receiving commands from an app. Whenever there is a new command from the app side to change the desired state of the device, the board will receive this request and apply the change by publishing it as the reported state. By registering a delta callback function, users will be able to see this incoming message and notice the syncing of the state.  \n\n* **Hardware Required**  \nArduino Y\u00fan  \nComputer connected with Arduino Y\u00fan using USB serial\n\n* **Software Required**  \nApp-side code that updates the state of the corresponding thing shadow in the cloud  \n*Note:* You can also use [AWS IoT console](https://aws.amazon.com/iot/) to update the shadow data.\n\n* **Circuit Required**  \nNone\n\n* **Attention**  \nPlease make sure to start the example sketch after the board is fully set up and openWRT is up and connected to WiFi. See [here](#slowstartup).\n\n* **Code**  \n\tCreate an instance of aws\\_iot\\_mqtt\\_client. \n\n\t\taws_iot_mqtt_client myClient;\n\t\t\n\tCreate logging function for execution tracking.\n\t\n\t\tbool print_log(const char* src, int code) {\n\t\t\t...\n\t\t}\n\t\n\tIn `setup()`, open the Serial. Set the instance up and connect it to the AWS IoT. Init the shadow and register a delta callback function. All steps are tracked using logging function.\n\t\n\t\tif(print_log(\"setup\", myClient.setup(AWS_IOT_CLIENT_ID))) {\n\t\t\tif(print_log(\"config\", myClient.config(AWS_IOT_MQTT_HOST, AWS_IOT_MQTT_PORT, AWS_IOT_ROOT_CA_PATH, AWS_IOT_PRIVATE_KEY_PATH, AWS_IOT_CERTIFICATE_PATH))) {\n\t\t\t\tif(print_log(\"connect\", myClient.connect())) {\n        \t\t\tsuccess_connect = true;\n        \t\t\tprint_log(\"shadow init\", myClient.shadow_init(AWS_IOT_MY_THING_NAME));\n        \t\t\tprint_log(\"register thing shadow delta function\", myClient.shadow_register_delta_func(AWS_IOT_MY_THING_NAME, msg_callback_delta));\n      \t\t\t}\n    \t\t}\n  \t\t}\n  \t\t  \t\t\n  \tIn `loop()`, yield to check and receive new incoming messages every 1 second.\n  \t\n  \t\tif(myClient.yield()) {\n  \t\t\tSerial.println(\"Yield failed.\");\n  \t\t}\n  \t\tdelay(1000);\n  \t\t\n  \tFor delta callback function, obtain the desired/delta state and put it as the reported state in the JSON file that needs to be updated.\n  \t\n  \t\tvoid msg_callback_delta(const char* src, unsigned int len, Message_status_t flag) {\n  \t\t\tif(flag == STATUS_NORMAL) {\n  \t\t\t\t// Get the whole delta section\n  \t\t\t\tprint_log(\"getDeltaKeyValue\", myClient.getDeltaValueByKey(src, \"\", JSON_buf, 100));\n  \t\t\t\tString payload = \"{\\\"state\\\":{\\\"reported\\\":\";\n  \t\t\t\tpayload += delta;\n  \t\t\t\tpayload += \"}}\";\n  \t\t\t\tpayload.toCharArray(JSON_buf, 100);\n  \t\t\t\tprint_log(\"update thing shadow\", myClient.shadow_update(AWS_IOT_MY_THING_NAME, JSON_buf, strlen(JSON_buf), NULL, 5));\n  \t\t\t}\n  \t\t}\n  \t\n  \tOnce an update of the desired state for this device is received, a delta message will be received and displayed in the Serial monitor. The device will update this data into the cloud.  \n  \t\n\tThe full sketch can be found in `AWS-IoT-Arduino-Yun-Library/examples/ThingShadowEcho`.\n\n### Simple Thermostat Simulator\nThis [example](https://github.com/aws/aws-iot-device-sdk-arduino-yun/tree/master/AWS-IoT-Arduino-Yun-Library/examples/ThermostatSimulatorDevice) demonstrates Arduino Y\u00fan as a device accepting instructions and syncing reported state in shadow in AWS IoT, which simulates a thermostat to control the temperature of a room. With the provided example App script, users will be able to get real-time temperature data coming from the board and be able to remotely set the desired temperature. This example also demonstrates how to retrieve shadow JSON data received on Arduino Y\u00fan Board.  \n[AWS IoT Device SDK for Python](https://github.com/aws/aws-iot-device-sdk-python) is used in App scripts for MQTT connections. Users can modify it to use other MQTT library according to their needs.\n\n* **Hardware Required**  \nArduino Y\u00fan  \nComputer connected with Arduino Y\u00fan using USB serial and running example App scripts\n\n* **Software Required**  \nTkinter for App GUI  \n[AWS IoT Device SDK for Python](https://github.com/aws/aws-iot-device-sdk-python) for MQTT connectivity  \nExample App script, which is included in the `ExampleAppScripts/ThermostatSimulatorApp/`  \n\n* **Circuit Required**  \nNone\n\n* **Attention**  \nPlease make sure to start the example sketch after the board is fully set up and openWRT is up and connected to WiFi. See [here](#slowstartup).\n\n* **Getting started**  \nBefore proceeding to the following steps, please make sure you have your board set up, with all code base and credentials properly installed. Please make sure you attach the correct policy to your certificate.  \n  1. Modify your configuration file to match your own credentials and host address.  \n  2. Start the sketch when the board boots up. It should pass the initialization steps and then start to update shadow data. It should have a similar display in the serial monitor as follows:<p/>\n  <img align=\"center\" src = \"https://s3.amazonaws.com/aws-iot-device-sdk-arduino-yun-suppliemental/images/ThermostatSimulatorDevice_sketch.png\"/> \n  3. On the App side, please make sure you have [Tkinter](http://tkinter.unpythonic.net/wiki/How_to_install_Tkinter) and [AWS IoT Device SDK for Python](https://github.com/aws/aws-iot-device-sdk-python) pre-installed on your computer.\n  4. Copy and paste your credentials (certificate, private key and rootCA) into `ThermostatSimulatorApp/certs/`. Please make sure to keep the file names as they are downloaded and make sure the CA file name ends with `CA.crt`.  \n  5. In the directory `ThermostatSimulatorApp/`, start the App script by executing:\n\n  \t\t\tpython ThermostatSimulatorApp.py -e <Your AWS IoT Endpoint>  # For X.509 certificate based mutual authentication\n  \t\t\tpython ThermostatSimulatorApp.py -e <Your AWS IoT Endpoint> -w  # For MQTT over WebSocket using IAM credentials\n  \t\t\t\n  \t\t\t\n  \t\tFor more details about command line options, you can use the following command:  \n  \t\t\n  \t\t\tpython ThermostatSimulatorApp.py -h\n  \t\t\n  \t\tYou should be able to see a GUI prompt up with default reported temperature:<p/>\n  <img align=\"center\" src= \"https://s3.amazonaws.com/aws-iot-device-sdk-arduino-yun-suppliemental/images/ThermostatSimulatorApp_start.png\"/>\n  6. Try to input a desired temperature and click <kbd>SET</kbd>. If it succeeds, you should be able to see the desired temperature on the panel and a log printed out in the console space. The board will start continuously syncing temperature settings:<p/>\n  <img align=\"center\" src= \"https://s3.amazonaws.com/aws-iot-device-sdk-arduino-yun-suppliemental/images/ThermostatSimulatorApp_setTemp.png\"/>  \n  *Note:* The temperature is configured to be lower than 100 F and higher than -100 F. Error message will be printed out if there is a malformed setting:<p/>\n  <img align=\"center\" src= \"https://s3.amazonaws.com/aws-iot-device-sdk-arduino-yun-suppliemental/images/ThermostatSimulatorApp_setAboveLimits.png\"/><p/>\n  <img align=\"center\" src= \"https://s3.amazonaws.com/aws-iot-device-sdk-arduino-yun-suppliemental/images/ThermostatSimulatorApp_setBelowLimits.png\"/>\n\n* **Code**  \n\tCreate an instance of aws\\_iot\\_mqtt\\_client. \n\n\t\taws_iot_mqtt_client myClient;\n\t\t\n\tCreate logging function for execution tracking.\n\t\n\t\tbool print_log(const char* src, int code) {\n\t\t\t...\n\t\t}\n\t\n  In `setup()`, open the Serial. Set the instance up and connect it to the AWS IoT. Init the shadow and register a delta callback function. All steps are tracked using logging function.\n\t\n        if(print_log(\"setup\", myClient.setup(AWS_IOT_CLIENT_ID))) {\n          if(print_log(\"config\", myClient.config(AWS_IOT_MQTT_HOST, AWS_IOT_MQTT_PORT, AWS_IOT_ROOT_CA_PATH, AWS_IOT_PRIVATE_KEY_PATH, AWS_IOT_CERTIFICATE_PATH))) {\n            if(print_log(\"connect\", myClient.connect())) {\n              success_connect = true;\n              print_log(\"shadow init\", myClient.shadow_init(AWS_IOT_MY_THING_NAME));\n              print_log(\"register thing shadow delta function\", myClient.shadow_register_delta_func(AWS_IOT_MY_THING_NAME, msg_callback_delta));\n            }\n          }\n        }\n\n  In `loop()`, simulate the behavior of a thermostat. Check to see the difference between the desired and the reported temperature. If the desired temperature is higher, increase the reported temperature by 0.1 degree per 1 second (per loop). If the desired one is lower, decrease the reported temperature by 0.1 degree per 1 second (per loop). Increase/Decrease action will happen until the reported reaches the desired. Update the reported temperature and then yield to check if there is any new delta message.\n\n        void loop() {\n          if(success_connect) {\n            if(desiredTemp - reportedTemp > 0.001) {reportedTemp += 0.1;}\n            else if(reportedTemp - desiredTemp > 0.001) {reportedTemp -= 0.1;}\n            dtostrf(reportedTemp, 4, 1, float_buf);\n            float_buf[4] = '\\0';\n            sprintf_P(JSON_buf, PSTR(\"{\\\"state\\\":{\\\"reported\\\":{\\\"Temp\\\":%s}}}\"), float_buf);\n            print_log(\"shadow update\", myClient.shadow_update(AWS_IOT_MY_THING_NAME, JSON_buf, strlen(JSON_buf), NULL, 5));\n            if(myClient.yield()) {\n              Serial.println(\"Yield failed.\");\n            }\n            delay(1000); // check for incoming delta per 1000 ms\n          }\n        }\n\n  For delta callback function, get the desired state and the desired temperature data in it. Update the desired temperature record on board so that the board knows what to do, heating or cooling.\n\n        void msg_callback_delta(const char* src, unsigned int len, Message_status_t flag) {\n        \tif(flag == STATUS_NORMAL) {\n        \t\t// Get Temp section in delta messages\n        \t\tprint_log(\"getDeltaKeyValue\", myClient.getDeltaValueByKey(src, \"Temp\", JSON_buf, 50));\t\t\t\tString delta = data.substring(st, ed);\n        \t\tdesiredTemp = String(JSON_buf).toFloat();\n        \t}\n        }\n        \n  Each time the board receives a new desired temperature different from its reported temperature. Changes will happen, synced into shadow and captured by the example App. Users will be able to see the whole process of temperature updating from the App side.\n\n\tThe full sketch can be found in `AWS-IoT-Arduino-Yun-Library/examples/ThermostatSimulatorDevice`.\n\n\t\n<a name=\"errorcode\"></a>\n## Error code\nThe following error codes are defined in `AWS-IoT-Arduino-Yun-Library/aws_iot_error.h`:  \n\n\ttypedef enum {\n\t\tNONE_ERROR = 0,\n\t\tGENERIC_ERROR = -1,\n\t\tNULL_VALUE_ERROR = -2,\n\t\tOVERFLOW_ERROR = -3,\n\t\tOUT_OF_SKETCH_SUBSCRIBE_MEMORY = -4,\n\t\tSERIAL1_COMMUNICATION_ERROR = -5,\n\t\tSET_UP_ERROR = -6,\n\t\tNO_SET_UP_ERROR = -7,\n\t\tWRONG_PARAMETER_ERROR = -8,\n\t\tCONFIG_GENERIC_ERROR = -9,\n\t\tCONNECT_SSL_ERROR = -10,\n\t\tCONNECT_ERROR = -11,\n\t\tCONNECT_TIMEOUT = -12,\n\t\tCONNECT_CREDENTIAL_NOT_FOUND = -13,\n\t\tCONNECT_GENERIC_ERROR = -14,\n\t\tPUBLISH_ERROR = -15,\n\t\tPUBLISH_TIMEOUT = -16,\n\t\tPUBLISH_GENERIC_ERROR = -17,\n\t\tSUBSCRIBE_ERROR = -18,\n\t\tSUBSCRIBE_TIMEOUT = -19,\n\t\tSUBSCRIBE_GENERIC_ERROR = -20,\n\t\tUNSUBSCRIBE_ERROR = -21,\n\t\tUNSUBSCRIBE_TIMEOUT = -22,\n\t\tUNSUBSCRIBE_GENERIC_ERROR = -23,\n\t\tDISCONNECT_ERROR = -24,\n\t\tDISCONNECT_TIMEOUT = -25,\n\t\tDISCONNECT_GENERIC_ERROR = -26,\n\t\tSHADOW_INIT_ERROR = -27,\n\t\tNO_SHADOW_INIT_ERROR = -28,\n\t\tSHADOW_GET_GENERIC_ERROR = -29,\n\t\tSHADOW_UPDATE_GENERIC_ERROR = -30,\n\t\tSHADOW_UPDATE_INVALID_JSON_ERROR = -31,\n\t\tSHADOW_DELETE_GENERIC_ERROR = -32,\n\t\tSHADOW_REGISTER_DELTA_CALLBACK_GENERIC_ERROR = -33,\n\t\tSHADOW_UNREGISTER_DELTA_CALLBACK_GENERIC_ERROR = -34,\n\t\tYIELD_ERROR = -35,\n\t\tWEBSOCKET_CREDENTIAL_NOT_FOUND = -36,\n\t\tJSON_FILE_NOT_FOUND = -37,\n\t\tJSON_KEY_NOT_FOUND = -38,\n\t\tJSON_GENERIC_ERROR = -39,\n\t\tPUBLISH_QUEUE_FULL = -40,\n\t\tPUBLISH_QUEUE_DISABLED = -41\n\t} IoT_Error_t;\n\t\n<a name=\"support\"></a>\n## Support\nIf you have technical questions about AWS IoT Device SDK, please use [AWS IoT forum](https://forums.aws.amazon.com/forum.jspa?forumID=210).  \nFor any other questions on AWS IoT, please contact [AWS Support](https://aws.amazon.com/contact-us/).\n", "release_dates": ["2016-07-08T23:30:06Z", "2016-05-24T22:31:51Z", "2016-05-12T22:51:47Z", "2016-04-28T21:10:08Z", "2016-03-24T21:53:42Z", "2016-03-17T23:10:16Z", "2016-01-26T23:35:19Z", "2015-11-12T21:35:14Z", "2015-11-04T23:37:17Z"]}, {"name": "aws-iot-device-sdk-cpp", "description": "SDK for connecting to AWS IoT from a device using C++", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## New Version Available\n\nA new AWS IoT Device SDK is [now available](https://github.com/awslabs/aws-iot-device-sdk-cpp-v2). It is a complete rework, built to improve reliability, performance, and security. We invite your feedback!\n\nThis SDK will no longer receive feature updates, but will receive security updates.\n\n## AWS IoT C++ Device SDK\n\n* [Overview](#overview)\n* [Features](#features)\n* [Design Goals](#design)\n* [Collection of Metrics](#metrics)\n* [Getting Started](#getstarted)\n* [Installation](#installation)\n* [Porting to different platforms](#porting)\n* [Quick Links](#quicklinks)\n* [Sample APIs](#sampleapis)\n* [License](#license)\n* [Support](#support)\n\n<a name=\"overview\"></a>\n## Overview\nThis document provides information about the AWS IoT device SDK for C++.\n\n<a name=\"features\"></a>\n## Features\nThe Device SDK simplifies access to the Pub/Sub functionality of the AWS IoT broker via MQTT and provides APIs to interact with Thing Shadows. The SDK has been tested to work with the AWS IoT platform to ensure best interoperability of a device with the AWS IoT platform.\n\n### MQTT Connection\nThe Device SDK provides functionality to create and maintain a MQTT Connection. It expects to be provided with a Network Connection class that connects and authenticates to AWS IoT using either direct TLS or WebSocket over TLS. This connection is used for any further publish operations. It also allows for subscribing to MQTT topics which will call a configurable callback function when these messages are received on these topics.\n\n### Thing Shadow\nThis SDK implements the specific protocol for Thing Shadows to retrieve, update and delete Thing Shadows adhering to the protocol that is implemented to ensure correct versioning and support for client tokens. It abstracts the necessary MQTT topic subscriptions by automatically subscribing to and unsubscribing from the reserved topics as needed for each API call. Inbound state change requests are automatically signalled via a configurable callback.\n\n### Jobs\nThis SDK also implements the Jobs protocol to interact with the AWS IoT Jobs service. The IoT Job service manages deployment of IoT fleet wide tasks such as device software/firmware deployments and updates, rotation of security certificates, device reboots, and custom device specific management tasks. For additional information please see the [Jobs developer guide](https://docs.aws.amazon.com/iot/latest/developerguide/iot-jobs.html).\n\n<a name=\"design\"></a>\n## Design Goals of this SDK\nThe C++ SDK was specifically designed for devices that are not resource constrained and required advanced features such as Message queueing, multi-threading support and the latest language features\n\nPrimary aspects are:\n * Designed around the C++11 standard\n * Platform neutral, as long as the included CMake can find a C++11 compatible compiler and threading library\n * Network layer abstracted from the SDK. Can use any TLS library and initialization method\n * Support for multiple platforms and compilers. Tested on Linux, Windows (with VS2015) and Mac OS\n * Flexibility in picking and choosing functionality, can create Clients which only perform a subset of MQTT operations\n * Support for Rapidjson allowing use of complex shadow document structures\n\n <a name=\"metrics\"></a>\n ## Collection of Metrics\n Beginning with Release v1.2.0 of the SDK, AWS collects usage metrics indicating which language and version of the SDK is being used. This allows us to prioritize our resources towards addressing issues faster in SDKs that see the most and is an important data point. However, we do understand that not all customers would want to report this data by default. In that case, the sending of usage metrics can be easily disabled by the user by using the overloaded Connect action which takes in a boolean for enabling or disabling the SDK metrics:\n```\np_iot_client_->Connect(ConfigCommon::mqtt_command_timeout_, ConfigCommon::is_clean_session_,\n                                        mqtt::Version::MQTT_3_1_1, ConfigCommon::keep_alive_timeout_secs_,\n                                        std::move(client_id), nullptr, nullptr, nullptr, false); // false for disabling metrics\n```\n\n<a name=\"getstarted\"></a>\n## How to get started ?\nEnsure you understand the AWS IoT platform and create the necessary certificates and policies. For more information on the AWS IoT platform please visit the [AWS IoT developer guide](http://docs.aws.amazon.com/iot/latest/developerguide/iot-security-identity.html).\n\n<a name=\"installation\"></a>\n## Installation\nThis section explains the individual steps to retrieve the necessary files and be able to build your first application using the AWS IoT C++ SDK.\nThe SDK uses CMake to generate the necessary Makefile. CMake version 3.2 and above is required.\n\nPrerequisites:\n\n * Make sure to have latest CMake installed. Minimum required version is 3.2\n * Compiler should support C++11 features. We have tested this SDK with gcc 5+, clang 3.8 and on Visual Studio 2015.\n * OpenSSL has version 1.1.0 and libssl-dev has version 1.1.0.\n * You can find basic information on how to set up the above on some popular platforms in [Platform.md](https://github.com/aws/aws-iot-device-sdk-cpp/blob/master/Platform.md)\n\nBuild Targets:\n\n * The SDK itself builds as a library by default. All the samples/tests link to the library. The library target is `aws-iot-sdk-cpp`\n * Unit tests - `aws-iot-unit-tests`\n * Integration tests - `aws-iot-integration-tests`\n * Sample - `pub-sub-sample`\n * Sample - `shadow-delta-sample`\n\n This following sample targets are generated only if OpenSSL is being used:\n * Sample - `discovery-sample`.\n * Sample - `robot-arm-sample`.\n * Sample - `switch-sample`\n\nSteps:\n\n * Clone the SDK from the github repository\n * Change to the repository folder. Create a folder called `build` to hold the build files and change to this folder. In-source builds are NOT allowed\n * Run `cmake ../.` to build the SDK with the CLI.\n * The command will download required third party libraries automatically and generate a Makefile\n * Type `make <target name>` to build the desired target. It will create a folder called `bin` that will have the build output\n\n<a name=\"porting\"></a>\n## Porting to different platforms\nThe SDK has been written to adhere to C++11 standard without any additional compiler specific features enabled. It should compile on any platform that has a modern C++11 enabled compiler without issue.\nThe platform should be able to provide a C++11 compatible threading implementation (eg. pthread on linux).\nTLS libraries can be added by simply implementing a derived class of NetworkConnection and providing an instance to the Client.\nWe provide the following reference implementations for the Network layer:\n\n * OpenSSL - MQTT over TLS using OpenSSL v1.1.0. Tested on Windows (VS 2015) and Linux\n \t* The provided implementation requires OpenSSL to be pre-installed on the device\n \t* Use the mqtt port setting from the config file while setting up the network instance\n * MbedTLS - MQTT over TLS using MbedTLS. Tested on Linux\n \t* The provided implementation will download MbedTLS v2.3.0 from the github repo and build and link to the libraries. Please be warned that the default configuration of MbedTLS limits packet sizes to 16K\n \t* Use the mqtt port setting from the config file while setting up the network instance\n * WebSocket - MQTT over WebSocket. Tested on both Windows (VS 2015) and Linux. Uses OpenSSL 1.1.0 as the underlying TLS layer\n \t* The provided implementation requires OpenSSL to be pre-installed on the device\n \t* Please be aware that while the provided reference implementation allows initialization of credentials from any source, the recommended way to do so is to use the aws cli to generate credential files and read the generated files\n \t* Use the https port setting from the config file while setting up the network instance\n\n### Cross-compiling the SDK for other platforms\nThe included ToolchainFile.cmake file can be used to cross-compile the SDK for other platforms.\nProcedure for testing cross compiling (if using OpenSSL):\n\n1. build/download toolchain for specific platform\n2. modify the ToolchainFile.cmake with location and target of toolchain.\n ```\n # specify toolchain directory\n SET(TOOLCHAIN_DIR /home/toolchain/dir/here/bin)\n\n # specify cross compilation target\n SET(TARGET_CROSS target-here)`\n ```\n3. Cross-compile OpenSSL using the same toolchain\n4. modify `network/CMakeLists.txt.in` and change OpenSSL library location to cross-compiled OpenSSL\n\n5.\n```\ncd build\ncmake ../. -DCMAKE_TOOLCHAIN_FILE=../ToolchainFile.cmake\nmake\n```\n6. Scp the application binary, certs and config for the application into the platform you're testing\n7. Run `./<application>`\n\nFor MbedTLS, you don't need to cross-compile MbedTLS as it gets compiled when you run `make` with the same compiler as pointed to by the toolchain file.\n\nAlso included is a simple example 'toolchain' which is used for setting the default compiler as clang++ instead of g++ as an example to show how the toolchain file can be modified.\n\n\n<a name=\"quicklinks\"></a>\n## Quick Links\n\n * [SDK Documentation](https://aws.github.io/aws-iot-device-sdk-cpp/) - API documentation for the SDK\n * [Platform Guide](./Platform.md) - This file lists the steps needed to set up the pre-requisites on some popular platforms\n * [Developers Guide](./DevGuide.md) - Provides a guide on how the SDK can be included in custom code\n * [Greengrass Discovery Support Guide](./GreengrassDiscovery.md) - Provides information on support for AWS Greengrass Discovery Service\n * [Network Layer Implementation Guide](./network/README.md) - Detailed description about the Network Layer and how to implement a custom wrapper class\n * [Sample Guide](./samples/README.md) - Details about the included samples\n * [Test Information](./tests/README.md) - Details about the included unit and integration tests\n * [MQTT 3.1.1 Spec](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/csprd02/mqtt-v3.1.1-csprd02.html) - Link to the MQTT v3.1.1 spec that this SDK implements\n\n<a name=\"sampleapis\"></a>\n## Sample APIs\n### Sync\n\nCreating a basic MQTT Client requires a NetworkConnection instance and MQTT Command timeout in milliseconds for any internal blocking operations.\n\n```\nstd::shared_ptr<NetworkConnection> p_network_connection = <Create Instance>;\nstd::shared_ptr<MqttClient> p_client = MqttClient::Create(p_network_connection, std::chrono::milliseconds(30000));\n```\n\nConnecting to the AWS IoT MQTT platform\n\n```\nrc = p_client->Connect(std::chrono::milliseconds(30000), false, mqtt::Version::MQTT_3_1_1, std::chrono::seconds(60), Utf8String::Create(\"<client_id>\"), nullptr, nullptr, nullptr);\n```\n\nSubscribe to a topic\n\n```\nutil::String p_topic_name_str = <topic>;\nstd::unique_ptr<Utf8String> p_topic_name = Utf8String::Create(p_topic_name_str);\nmqtt::Subscription::ApplicationCallbackHandlerPtr p_sub_handler = std::bind(&<handler>, this, std::placeholders::_1, std::placeholders::_2, std::placeholders::_3);\nstd::shared_ptr<mqtt::Subscription> p_subscription = mqtt::Subscription::Create(std::move(p_topic_name), mqtt::QoS::QOS0, p_sub_handler, nullptr);\nutil::Vector<std::shared_ptr<mqtt::Subscription>> topic_vector;\ntopic_vector.push_back(p_subscription);\nrc = p_client->Subscribe(topic_vector, std::chrono::milliseconds(30000));\n```\n\nPublish to a topic\n```\nutil::String p_topic_name_str = <topic>;\nstd::unique_ptr<Utf8String> p_topic_name = Utf8String::Create(p_topic_name_str);\nrc = p_client->Publish(std::move(p_topic_name), false, false, mqtt::QoS::QOS1, payload, std::chrono::milliseconds(30000));\n```\n\nUnsubscribe from a topic\n\n```\nutil::String p_topic_name_str = <topic>;\nstd::unique_ptr<Utf8String> p_topic_name = Utf8String::Create(p_topic_name_str);\nutil::Vector<std::unique_ptr<Utf8String>> topic_vector;\ntopic_vector.push_back(std::move(p_topic_name));\nrc = p_client->Subscribe(topic_vector, std::chrono::milliseconds(30000));\n```\n\n### Async\nConnect is a sync only API in this version of the SDK.\nSubscribe to a topic\n\n```\nuint16_t packet_id_out;\nutil::String p_topic_name_str = <topic>;\nstd::unique_ptr<Utf8String> p_topic_name = Utf8String::Create(p_topic_name_str);\nmqtt::Subscription::ApplicationCallbackHandlerPtr p_sub_handler = std::bind(&<handler>, this, std::placeholders::_1, std::placeholders::_2, std::placeholders::_3);\nstd::shared_ptr<mqtt::Subscription> p_subscription = mqtt::Subscription::Create(std::move(p_topic_name), mqtt::QoS::QOS0, p_sub_handler, nullptr);\nutil::Vector<std::shared_ptr<mqtt::Subscription>> topic_vector;\ntopic_vector.push_back(p_subscription);\nrc = p_client->SubscribeAsync(topic_vector, nullptr, packet_id_out);\n```\n\nPublish to a topic\n```\nuint16_t packet_id_out;\nutil::String p_topic_name_str = <topic>;\nstd::unique_ptr<Utf8String> p_topic_name = Utf8String::Create(p_topic_name_str);\nrc = p_client->PublishAsync(std::move(p_topic_name), false, false, mqtt::QoS::QOS1, payload, packet_id_out);\n```\n\nUnsubscribe from a topic\n\n```\nuint16_t packet_id_out;\nutil::String p_topic_name_str = <topic>;\nstd::unique_ptr<Utf8String> p_topic_name = Utf8String::Create(p_topic_name_str);\nutil::Vector<std::unique_ptr<Utf8String>> topic_vector;\ntopic_vector.push_back(std::move(p_topic_name));\nrc = p_client->Subscribe(topic_vector, packet_id_out);\n```\n\n### Logging\nTo enable logging, create an instance of the ConsoleLogSystem in the main() of your application as shown below:\n\n```\nstd::shared_ptr<awsiotsdk::util::Logging::ConsoleLogSystem> p_log_system =\n    std::make_shared<awsiotsdk::util::Logging::ConsoleLogSystem>(awsiotsdk::util::Logging::LogLevel::Info);\nawsiotsdk::util::Logging::InitializeAWSLogging(p_log_system);\n```\n\nCreate a log tag for your application to distinguish it from the SDK logs:\n```\n#define LOG_TAG_APPLICATION \"[Application]\"\n```\n\nYou can now add logging to any part of your application using AWS_LOG_ERROR or AWS_LOG_INFO as shown below:\n\n```\nAWS_LOG_ERROR(LOG_TAG_APPLICATION, \"Failed to perform action. %s\",\n              ResponseHelper::ToString(rc).c_str());\n```\n\n<a name=\"license\"></a>\n## License\n\nThis SDK is distributed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0), see LICENSE and NOTICE.txt for more information.\n\n<a name=\"support\"></a>\n## Support\n\nIf you have any technical questions about AWS IoT C++ SDK, use the [AWS IoT forum](https://forums.aws.amazon.com/forum.jspa?forumID=210).\nFor any other questions on AWS IoT, contact [AWS Support](https://aws.amazon.com/contact-us/).\n\nA list of known issues is maintained in [KnownIssues.md](KnownIssues.md).\n\nNote: customers have [reported deadlocks](https://github.com/aws/aws-iot-device-sdk-cpp/issues/14) while using the AWS IoT Device SDK for C++. If you are affected, a fix is available in the [locking-fixes](https://github.com/aws/aws-iot-device-sdk-cpp/tree/locking-fixes) branch. This issue is also resolved in the new AWS IoT Device SDK for C++, which is [currently in Developer Preview](https://github.com/awslabs/aws-iot-device-sdk-cpp-v2).\n", "release_dates": ["2018-05-10T20:14:15Z", "2017-12-07T01:16:07Z", "2017-07-14T00:46:54Z", "2017-06-07T17:34:37Z", "2016-12-08T22:39:22Z"]}, {"name": "aws-iot-device-sdk-cpp-v2", "description": "Next generation AWS IoT Client SDK for C++ using the AWS Common Runtime", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS IoT Device SDK for C++ v2\n\nThis document provides information about the AWS IoT device SDK for C++ V2. This SDK is built on the [AWS Common Runtime](https://docs.aws.amazon.com/sdkref/latest/guide/common-runtime.html)\n\n__Jump To:__\n\n* [Installation](#installation)\n* [Samples](./samples)\n* [Getting Help](#getting-help)\n* [FAQ](./documents/FAQ.md)\n* [API Docs](https://aws.github.io/aws-iot-device-sdk-cpp-v2/)\n* [MQTT5 User Guide](./documents/MQTT5_Userguide.md)\n* [Migration Guide from the AWS IoT SDK for C++ v1](./documents/MIGRATION_GUIDE.md)\n\n\n## Installation\n\n### Minimum Requirements\n* C++ 11 or higher\n    * Clang 3.9+ or GCC 4.8+ or MSVC 2015+\n* CMake 3.1+\n\n[Step-by-step instructions](./documents/PREREQUISITES.md)\n\n\n### Build from source\n\n``` sh\n# Create a workspace directory to hold all the SDK files\nmkdir sdk-workspace\ncd sdk-workspace\n# Clone the repository\ngit clone --recursive https://github.com/aws/aws-iot-device-sdk-cpp-v2.git\n# Make a build directory for the SDK. Can use any name.\nmkdir aws-iot-device-sdk-cpp-v2-build\ncd aws-iot-device-sdk-cpp-v2-build\n# continue with the build steps below based on OS\n```\n\n#### MacOS and Linux\n```sh\n# Generate the SDK build files.\n# -DCMAKE_INSTALL_PREFIX needs to be the absolute/full path to the directory.\n#     (Example: \"/Users/example/sdk-workspace/).\n# -DCMAKE_BUILD_TYPE can be \"Release\", \"RelWithDebInfo\", or \"Debug\"\ncmake -DCMAKE_INSTALL_PREFIX=\"<absolute path to sdk-workspace>\" -DCMAKE_BUILD_TYPE=\"Debug\" ../aws-iot-device-sdk-cpp-v2\n# Build and install the library. Once installed, you can develop with the SDK and run the samples\ncmake --build . --target install\n```\n\n#### Windows\n``` sh\n# Generate the SDK build files.\n# -DCMAKE_INSTALL_PREFIX needs to be the absolute/full path to the directory.\n#     (Example: \"C:/users/example/sdk-workspace/).\ncmake -DCMAKE_INSTALL_PREFIX=\"<absolute path sdk-workspace dir>\" ../aws-iot-device-sdk-cpp-v2\n# Build and install the library. Once installed, you can develop with the SDK and run the samples\n# -config can be \"Release\", \"RelWithDebInfo\", or \"Debug\"\ncmake --build . --target install --config \"Debug\"\n```\n\n**Windows specific notes**:\n* Due to maximum path length limitations in the Windows API, we recommend cloning to a short path like: `C:\\dev\\iotsdk`\n* `--config` is only REQUIRED for multi-configuration build tools (VisualStudio/MsBuild being the most common).\n\n**Linux specific notes**:\n\nIf your application uses OpenSSL, configure with `-DUSE_OPENSSL=ON`.\n\nThe IoT SDK does not use OpenSSL for TLS.\nOn Apple and Windows, the OS's default TLS library is used.\nOn Linux, [s2n-tls](https://github.com/aws/s2n-tls) is used.\nBut s2n-tls uses libcrypto, the cryptography math library bundled with OpenSSL.\nTo simplify the build process, the source code for s2n-tls and libcrypto are\nincluded as git submodules and built along with the IoT SDK.\nBut if your application is also loading the system installation of OpenSSL\n(i.e. your application uses libcurl which uses libssl which uses libcrypto)\nthere may be crashes as the application tries to use two different versions of libcrypto at once.\n\nSetting `-DUSE_OPENSSL=ON` will cause the IoT SDK to link against your system's\nexisting `libcrypto`, instead of building its own copy.\n\n## Samples\n\n[Samples README](./samples)\n\n## Getting Help\n\nThe best way to interact with our team is through GitHub. You can open a [discussion](https://github.com/aws/aws-iot-device-sdk-cpp-v2/discussions) for guidance questions or an [issue](https://github.com/aws/aws-iot-device-sdk-cpp-v2/issues/new/choose) for bug reports, or feature requests. You may also find help on community resources such as [StackOverFlow](https://stackoverflow.com/questions/tagged/aws-iot) with the tag [#aws-iot](https://stackoverflow.com/questions/tagged/aws-iot) or if you have a support plan with [AWS Support](https://aws.amazon.com/premiumsupport/), you can also create a new support case.\n\nPlease make sure to check out our resources too before opening an issue:\n\n* [FAQ](./documents/FAQ.md)\n* [API Docs](https://aws.github.io/aws-iot-device-sdk-cpp-v2/)\n* [IoT Guide](https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html) ([source](https://github.com/awsdocs/aws-iot-docs))\n* Check for similar [Issues](https://github.com/aws/aws-iot-device-sdk-cpp-v2/issues)\n* [AWS IoT Core Documentation](https://docs.aws.amazon.com/iot/)\n* [Dev Blog](https://aws.amazon.com/blogs/?awsf.blog-master-iot=category-internet-of-things%23amazon-freertos%7Ccategory-internet-of-things%23aws-greengrass%7Ccategory-internet-of-things%23aws-iot-analytics%7Ccategory-internet-of-things%23aws-iot-button%7Ccategory-internet-of-things%23aws-iot-device-defender%7Ccategory-internet-of-things%23aws-iot-device-management%7Ccategory-internet-of-things%23aws-iot-platform)\n* Integration with AWS IoT Services such as\n[Device Shadow](https://docs.aws.amazon.com/iot/latest/developerguide/iot-device-shadows.html)\nand [Jobs](https://docs.aws.amazon.com/iot/latest/developerguide/iot-jobs.html)\nis provided by code that been generated from a model of the service.\n* [Secure Tunnel User Guide](./documents/Secure_Tunnel_Userguide.md)\n* [Contributions Guidelines](./documents/CONTRIBUTING.md)\n\n## License\n\nThis library is licensed under the [Apache 2.0 License](./documents/LICENSE).\n\nLatest released version: v1.32.2\n", "release_dates": ["2024-03-01T21:37:54Z", "2024-01-11T21:18:57Z", "2024-01-02T17:21:04Z", "2023-12-28T17:55:16Z", "2023-12-14T00:28:23Z", "2023-11-14T19:25:48Z", "2023-11-10T20:09:54Z", "2023-10-26T16:00:53Z", "2023-10-06T00:36:57Z", "2023-09-29T16:38:28Z", "2023-08-23T15:47:38Z", "2023-08-14T18:13:22Z", "2023-08-04T23:40:04Z", "2023-07-28T02:16:26Z", "2023-07-07T18:14:24Z", "2023-06-08T16:06:36Z", "2023-05-26T18:54:41Z", "2023-05-14T14:59:49Z", "2023-05-09T23:11:39Z", "2023-05-02T22:31:32Z", "2023-04-19T23:21:38Z", "2023-04-13T20:12:28Z", "2023-02-23T23:54:04Z", "2023-02-01T01:09:07Z", "2023-01-27T16:50:06Z", "2023-01-18T15:06:14Z", "2022-12-05T20:06:36Z", "2022-11-11T21:16:28Z", "2022-10-04T16:55:35Z", "2022-09-20T23:49:30Z"]}, {"name": "aws-iot-device-sdk-embedded-C", "description": "SDK for connecting to AWS IoT from a device using embedded C.", "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "\n# AWS IoT Device SDK for Embedded C\n\n## Table of Contents\n\n* [Overview](#overview)\n    * [License](#license)\n    * [Features](#features)\n        * [coreMQTT](#coremqtt)\n        * [coreHTTP](#corehttp)\n        * [coreJSON](#corejson)\n        * [corePKCS11](#corepkcs11)\n        * [AWS IoT Device Shadow](#aws-iot-device-shadow)\n        * [AWS IoT Jobs](#aws-iot-jobs)\n        * [AWS IoT Device Defender](#aws-iot-device-defender)\n        * [AWS IoT Over-the-air Update Library](#aws-iot-over-the-air-update)\n        * [AWS IoT Fleet Provisoning](#aws-iot-fleet-provisioning)\n        * [AWS SigV4](#aws-sigv4)\n        * [backoffAlgorithm](#backoffalgorithm)\n    * [Sending metrics to AWS IoT](#sending-metrics-to-aws-iot)\n* [Versioning](#versioning)\n* [Releases and Documentation](#releases-and-documentation)\n    * [202211.00](#20221100)\n    * [202108.00](#20210800)\n    * [202103.00](#20210300)\n    * [202012.01](#20201201)\n    * [202011.00](#20201100)\n    * [202009.00](#20200900)\n    * [v3.1.5](#v315)\n* [Porting Guide for 202009.00 and newer releases](#porting-guide-for-20200900-and-newer-releases)\n    * [Porting coreMQTT](#porting-coremqtt)\n    * [Porting coreHTTP](#porting-corehttp)\n    * [Porting AWS IoT Device Shadow](#porting-aws-iot-device-shadow)\n    * [Porting AWS IoT Device Defender](#porting-aws-iot-device-defender)\n    * [Porting AWS IoT Over-the-air Update](#porting-aws-iot-over-the-air-update)\n* [Migration guide from v3.1.5 to 202009.00 and newer releases](#migration-guide-from-v315-to-20200900-and-newer-releases)\n    * [MQTT Migration](#mqtt-migration)\n    * [Shadow Migration](#shadow-migration)\n    * [Jobs Migration](#jobs-migration)\n* [Branches](#branches)\n    * [main](#main-branch)\n    * [v4_beta_deprecated](#v4_beta_deprecated-branch-formerly-named-v4_beta)\n* [Getting Started](#getting-started)\n    * [Cloning](#cloning)\n    * [Configuring Demos](#configuring-demos)\n        * [Prerequisites](#prerequisites)\n            * [Build Dependencies](#build-dependencies)\n        * [AWS IoT Account Setup](#aws-iot-account-setup)\n        * [Configuring mutual authentication demos of MQTT and HTTP](#configuring-mutual-authentication-demos-of-mqtt-and-http)\n        * [Configuring AWS IoT Device Defender and AWS IoT Device Shadow demos](#configuring-aws-iot-device-defender-and-aws-iot-device-shadow-demos)\n        * [Configuring the AWS IoT Fleet Provisioning demo](#configuring-the-aws-iot-fleet-provisioning-demo)\n        * [Configuring the S3 demos](#configuring-the-s3-demos)\n        * [Setup for AWS IoT Jobs demo](#setup-for-aws-iot-jobs-demo)\n        * [Setup for the Greengrass local auth demo](#setup-for-the-greengrass-local-auth-demo)\n        * [Prerequisites for the AWS Over-The-Air Update (OTA) demos](#prerequisites-for-the-aws-over-the-air-update-ota-demos)\n        * [Scheduling an OTA Update Job](#scheduling-an-ota-update-job)\n    * [Building and Running Demos](#building-and-running-demos)\n        * [Build a single demo](#build-a-single-demo)\n        * [Build all configured demos](#build-all-configured-demos)\n        * [Running corePKCS11 demos](#running-corepkcs11-demos)\n        * [Alternative option of Docker containers for running demos locally](#alternative-option-of-docker-containers-for-running-demos-locally)\n            * [Installing Mosquitto to run MQTT demos locally](#installing-mosquitto-to-run-mqtt-demos-locally)\n            * [Installing httpbin to run HTTP demos locally](#installing-httpbin-to-run-http-demos-locally)\n* [Generating Documentation](#generating-documentation)\n\n## Overview\n\nThe AWS IoT Device SDK for Embedded C (C-SDK) is a collection of C source files under the [MIT open source license](LICENSE) that can be used in embedded applications to securely connect IoT devices to [AWS IoT Core](https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html). It contains MQTT client, HTTP client, JSON Parser, AWS IoT Device Shadow, AWS IoT Jobs, and AWS IoT Device Defender libraries. This SDK is distributed in source form, and can be built into customer firmware along with application code, other libraries and an operating system (OS) of your choice. These libraries are only dependent on standard C libraries, so they can be ported to various OS's - from embedded Real Time Operating Systems (RTOS) to Linux/Mac/Windows. You can find sample usage of C-SDK libraries on POSIX systems using OpenSSL (e.g. [Linux demos](demos) in this repository), and on [FreeRTOS](https://github.com/FreeRTOS/FreeRTOS/) using mbedTLS (e.g. [FreeRTOS demos](https://github.com/FreeRTOS/FreeRTOS/tree/main/FreeRTOS-Plus/Demo) in [FreeRTOS](https://github.com/FreeRTOS/FreeRTOS/) repository).\n\nFor the latest release of C-SDK, please see the section for [Releases and Documentation](#releases-and-documentation).\n\n**C-SDK includes libraries that are part of the [FreeRTOS 202210.01 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202210.01-LTS) release. Learn more about the FreeRTOS 202210.01 LTS libraries by [clicking here](https://freertos.org/lts-libraries.html).**\n\n### License\n\nThe C-SDK libraries are licensed under the [MIT open source license](LICENSE).\n\n### Features\n\nC-SDK simplifies access to various AWS IoT services. C-SDK has been tested to work with [AWS IoT Core](https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html) and an open source MQTT broker to ensure interoperability. The AWS IoT Device Shadow, AWS IoT Jobs, and AWS IoT Device Defender libraries are flexible to work with any MQTT client and JSON parser. The MQTT client and JSON parser libraries are offered as choices without being tightly coupled with the rest of the SDK. C-SDK contains the following libraries:\n\n#### coreMQTT\n\nThe [coreMQTT](https://github.com/FreeRTOS/coreMQTT) library provides the ability to establish an MQTT connection with a broker over a customer-implemented transport layer, which can either be a secure channel like a TLS session (mutually authenticated or server-only authentication) or a non-secure channel like a plaintext TCP connection. This MQTT connection can be used for performing publish operations to MQTT topics and subscribing to MQTT topics. The library provides a mechanism to register customer-defined callbacks for receiving incoming PUBLISH, acknowledgement and keep-alive response events from the broker. The library has been refactored for memory optimization and is compliant with the [MQTT 3.1.1](https://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html) standard. It has no dependencies on any additional libraries other than the standard C library, a customer-implemented network transport interface, and optionally a customer-implemented platform time function. The refactored design embraces different use-cases, ranging from resource-constrained platforms using only QoS 0 MQTT PUBLISH messages to resource-rich platforms using QoS 2 MQTT PUBLISH over TLS connections.\n\nSee memory requirements for the latest release [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/standard/coreMQTT/docs/doxygen/output/html/index.html#mqtt_memory_requirements).\n\n#### coreHTTP\n\nThe [coreHTTP](https://github.com/FreeRTOS/coreHTTP) library provides the ability to establish an HTTP connection with a server over a customer-implemented transport layer, which can either be a secure channel like a TLS session (mutually authenticated or server-only authentication) or a non-secure channel like a plaintext TCP connection. The HTTP connection can be used to make \"GET\" (include range requests), \"PUT\", \"POST\" and \"HEAD\" requests. The library provides a mechanism to register a customer-defined callback for receiving parsed header fields in an HTTP response. The library has been refactored for memory optimization, and is a client implementation of a subset of the [HTTP/1.1](https://tools.ietf.org/html/rfc2616) standard.\n\nSee memory requirements for the latest release [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/standard/coreHTTP/docs/doxygen/output/html/index.html#http_memory_requirements).\n\n#### coreJSON\n\nThe [coreJSON](https://github.com/FreeRTOS/coreJSON) library is a JSON parser that strictly enforces the [ECMA-404 JSON standard](https://www.json.org/json-en.html). It provides a function to validate a JSON document, and a function to search for a key and return its value. A search can descend into nested structures using a compound query key. A JSON document validation also checks for illegal UTF8 encodings and illegal Unicode escape sequences.\n\nSee memory requirements for the latest release [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/standard/coreJSON/docs/doxygen/output/html/index.html#json_memory_requirements).\n\n#### corePKCS11\n\nThe [corePKCS11](https://github.com/FreeRTOS/corePKCS11) library is an implementation of the PKCS #11 interface (API) that makes it easier to develop applications that rely on cryptographic operations. Only a subset of the [PKCS #11 v2.4](https://docs.oasis-open.org/pkcs11/pkcs11-base/v2.40/os/pkcs11-base-v2.40-os.html) standard has been implemented, with a focus on operations involving asymmetric keys, random number generation, and hashing.\n\nThe Cryptoki or PKCS #11 standard defines a platform-independent API to manage and use cryptographic tokens. The name, \"PKCS #11\", is used interchangeably to refer to the API itself and the standard which defines it.\n\nThe PKCS #11 API is useful for writing software without taking a dependency on any particular implementation or hardware. By writing against the PKCS #11 standard interface, code can be used interchangeably with multiple algorithms, implementations and hardware.\n\nGenerally vendors for secure cryptoprocessors such as Trusted Platform Module ([TPM](https://en.wikipedia.org/wiki/Trusted_Platform_Module)), Hardware Security Module ([HSM](https://en.wikipedia.org/wiki/Hardware_security_module)), Secure Element, or any other type of secure hardware enclave, distribute a PKCS #11 implementation with the hardware.\nThe purpose of corePKCS11 mock is therefore to provide a PKCS #11 implementation that allows for rapid prototyping and development before switching to a cryptoprocessor specific PKCS #11 implementation in production devices.\n\nSince the PKCS #11 interface is defined as part of the PKCS #11 [specification](https://docs.oasis-open.org/pkcs11/pkcs11-base/v2.40/os/pkcs11-base-v2.40-os.html) replacing corePKCS11 with another implementation\nshould require little porting effort, as the interface will not change. The system tests distributed in corePKCS11 repository can be leveraged to verify the behavior of a different implementation is similar to corePKCS11.\n\nSee memory requirements for the latest release [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/standard/corePKCS11/docs/doxygen/output/html/pkcs11_design.html#pkcs11_memory_requirements).\n\n#### AWS IoT Device Shadow\n\nThe [AWS IoT Device Shadow](https://github.com/aws/device-shadow-for-aws-iot-embedded-sdk) library enables you to store and retrieve the current state one or more shadows of every registered device. A device\u2019s shadow is a persistent, virtual representation of your device that you can interact with from AWS IoT Core even if the device is offline. The device state is captured in its \"shadow\" is represented as a [JSON](https://www.json.org/) document. The device can send commands over MQTT to get, update and delete its latest state as well as receive notifications over MQTT about changes in its state. The device\u2019s shadow(s) are uniquely identified by the name of the corresponding \"thing\", a representation of a specific device or logical entity on the AWS Cloud. See [Managing Devices with AWS IoT](https://docs.aws.amazon.com/iot/latest/developerguide/iot-thing-management.html) for more information on IoT \"thing\". This library supports named shadows, a feature of the AWS IoT Device Shadow service that allows you to create multiple shadows for a single IoT device. More details about AWS IoT Device Shadow can be found in [AWS IoT documentation](https://docs.aws.amazon.com/iot/latest/developerguide/iot-device-shadows.html).\n\nThe AWS IoT Device Shadow library has no dependencies on additional libraries other than the standard C library. It also doesn\u2019t have any platform dependencies, such as threading or synchronization. It can be used with any MQTT library and any JSON library (see [demos](demos/shadow) with coreMQTT and coreJSON).\n\nSee memory requirements for the latest release [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/aws/device-shadow-for-aws-iot-embedded-sdk/docs/doxygen/output/html/index.html#shadow_memory_requirements).\n\n\n#### AWS IoT Jobs\n\nThe [AWS IoT Jobs](https://github.com/aws/jobs-for-aws-iot-embedded-sdk) library enables you to interact with the AWS IoT Jobs service which notifies one or more connected devices of a pending \u201cJob\u201d. A Job can be used to manage your fleet of devices, update firmware and security certificates on your devices, or perform administrative tasks such as restarting devices and performing diagnostics. For documentation of the service, please see the [AWS IoT Developer Guide](https://docs.aws.amazon.com/iot/latest/developerguide/iot-jobs.html). Interactions with the Jobs service use the MQTT protocol. This library provides an API to compose and recognize the MQTT topic strings used by the Jobs service.\n\nThe AWS IoT Jobs library has no dependencies on additional libraries other than the standard C library. It also doesn\u2019t have any platform dependencies, such as threading or synchronization. It can be used with any MQTT library and any JSON library (see [demos](demos/jobs) with [libmosquitto](https://mosquitto.org/) and coreJSON).\n\nSee memory requirements for the latest release [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/aws/jobs-for-aws-iot-embedded-sdk/docs/doxygen/output/html/index.html#jobs_memory_requirements).\n\n\n#### AWS IoT Device Defender\n\nThe [AWS IoT Device Defender](https://github.com/aws/device-defender-for-aws-iot-embedded-sdk) library enables you to interact with the AWS IoT Device Defender service to continuously monitor security metrics from devices for deviations from what you have defined as appropriate behavior for each device. If something doesn\u2019t look right, AWS IoT Device Defender sends out an alert so you can take action to remediate the issue. More details about Device Defender can be found in [AWS IoT Device Defender documentation](https://docs.aws.amazon.com/iot/latest/developerguide/device-defender.html). This library supports custom metrics, a feature that helps you monitor operational health metrics that are unique to your fleet or use case. For example, you can define a new metric to monitor the memory usage or CPU usage on your devices.\n\nThe AWS IoT Device Defender library has no dependencies on additional libraries other than the standard C library. It also doesn\u2019t have any platform dependencies, such as threading or synchronization. It can be used with any MQTT library and any JSON library (see [demos](demos/defender) with coreMQTT and coreJSON).\n\nSee memory requirements for the latest release [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/aws/device-defender-for-aws-iot-embedded-sdk/docs/doxygen/output/html/index.html#defender_memory_requirements).\n\n#### AWS IoT Over-the-air Update\n\nThe [AWS IoT Over-the-air Update](https://github.com/aws/ota-for-aws-iot-embedded-sdk) (OTA) library enables you to manage the notification of a newly available update, download the update, and perform cryptographic verification of the firmware update. Using the OTA library, you can logically separate firmware updates from the application running on your devices. You can also use the library to send other files (e.g. images, certificates) to one or more devices registered with AWS IoT. More details about OTA library can be found in [AWS IoT Over-the-air Update documentation](https://docs.aws.amazon.com/freertos/latest/userguide/freertos-ota-dev.html).\n\nThe AWS IoT Over-the-air Update library has a dependency on [coreJSON](https://github.com/FreeRTOS/coreJSON) for parsing of JSON job document and [tinyCBOR](https://github.com/intel/tinycbor.git) for decoding encoded data streams, other than the standard C library. It can be used with any MQTT library, HTTP library, and operating system (e.g. Linux, FreeRTOS) (see [demos](demos/ota) with coreMQTT and coreHTTP over Linux).\n\nSee memory requirements for the latest release [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/aws/ota-for-aws-iot-embedded-sdk/docs/doxygen/output/html/index.html#ota_memory_requirements).\n\n#### AWS IoT Fleet Provisioning\n\nThe [AWS IoT Fleet Provisioning](https://github.com/aws/fleet-provisioning-for-aws-iot-embedded-sdk) library enables you to interact with the [AWS IoT Fleet Provisioning MQTT APIs](https://docs.aws.amazon.com/iot/latest/developerguide/fleet-provision-api.html) in order to provison IoT devices without preexisting device certificates. With AWS IoT Fleet Provisioning, devices can securely receive unique device certificates from AWS IoT when they connect for the first time. For an overview of all provisioning options offered by AWS IoT, see [device provisioning documentation](https://docs.aws.amazon.com/iot/latest/developerguide/iot-provision.html). For details about Fleet Provisioning, refer to the [AWS IoT Fleet Provisioning documentation](https://docs.aws.amazon.com/iot/latest/developerguide/provision-wo-cert.html).\n\nSee memory requirements for the latest release [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/aws/fleet-provisioning-for-aws-iot-embedded-sdk/docs/doxygen/output/html/index.html#fleet_provisioning_memory_requirements).\n\n#### AWS SigV4\n\nThe [AWS SigV4](https://github.com/aws/SigV4-for-AWS-IoT-embedded-sdk) library enables you to sign HTTP requests with [Signature Version 4 Signing Process](https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html). Signature Version 4 (SigV4) is the process to add authentication information to HTTP requests to AWS services. For security, most requests to AWS must be signed with an access key. The access key consists of an access key ID and secret access key.\n\nSee memory requirements for the latest release [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/aws/sigv4-for-aws-iot-embedded-sdk/docs/doxygen/output/html/index.html#sigv4_memory_requirements).\n\n#### backoffAlgorithm\n\nThe [backoffAlgorithm](https://github.com/FreeRTOS/backoffAlgorithm) library is a utility library to calculate backoff period using an exponential backoff with jitter algorithm for retrying network operations (like failed network connection with server). This library uses the \"Full Jitter\" strategy for the exponential backoff with jitter algorithm. More information about the algorithm can be seen in the [Exponential Backoff and Jitter AWS blog](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/).\n\nExponential backoff with jitter is typically used when retrying a failed connection or network request to the server. An exponential backoff with jitter helps to mitigate the failed network operations with servers, that are caused due to network congestion or high load on the server, by spreading out retry requests across multiple devices attempting network operations. Besides, in an environment with poor connectivity, a client can get disconnected at any time. A backoff strategy helps the client to conserve battery by not repeatedly attempting reconnections when they are unlikely to succeed.\n\nThe backoffAlgorithm library has no dependencies on libraries other than the standard C library.\n\nSee memory requirements for the latest release [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/standard/backoffAlgorithm/docs/doxygen/output/html/index.html#backoff_algorithm_memory_requirements).\n\n### Sending metrics to AWS IoT\n\nWhen establishing a connection with AWS IoT, users can optionally report the Operating System, Hardware Platform and MQTT client version information of their device to AWS. This information can help AWS IoT provide faster issue resolution and technical support. If users want to report this information, they can send a specially formatted string (see below) in the username field of the MQTT CONNECT packet.\n\nFormat\n\nThe format of the username string with metrics is:\n\n```\n<Actual_Username>?SDK=<OS_Name>&Version=<OS_Version>&Platform=<Hardware_Platform>&MQTTLib=<MQTT_Library_name>@<MQTT_Library_version>\n```\n\nWhere\n\n* <Actual_Username> is the actual username used for authentication, if username and password are used for authentication. When username and password based authentication is not used, this\nis an empty value.\n* <OS_Name> is the Operating System the application is running on (e.g. Ubuntu)\n* <OS_Version> is the version number of the Operating System (e.g. 20.10)\n* <Hardware_Platform> is the Hardware Platform the application is running on (e.g. RaspberryPi)\n* <MQTT_Library_name> is the MQTT Client library being used (e.g. coreMQTT)\n* <MQTT_Library_version> is the version of the MQTT Client library being used (e.g. 1.1.0)\n\nExample\n\n*  Actual_Username = \u201ciotuser\u201d, OS_Name = Ubuntu, OS_Version = 20.10, Hardware_Platform_Name = RaspberryPi, MQTT_Library_Name = coremqtt, MQTT_Library_version = 1.1.0. If username is not used, then \u201ciotuser\u201d can be removed.\n\n```\n/* Username string:\n * iotuser?SDK=Ubuntu&Version=20.10&Platform=RaspberryPi&MQTTLib=coremqtt@1.1.0\n */\n\n#define OS_NAME                   \"Ubuntu\"\n#define OS_VERSION                \"20.10\"\n#define HARDWARE_PLATFORM_NAME    \"RaspberryPi\"\n#define MQTT_LIB                  \"coremqtt@1.1.0\"\n\n#define USERNAME_STRING           \"iotuser?SDK=\" OS_NAME \"&Version=\" OS_VERSION \"&Platform=\" HARDWARE_PLATFORM_NAME \"&MQTTLib=\" MQTT_LIB\n#define USERNAME_STRING_LENGTH    ( ( uint16_t ) ( sizeof( USERNAME_STRING ) - 1 ) )\n\nMQTTConnectInfo_t connectInfo;\nconnectInfo.pUserName = USERNAME_STRING;\nconnectInfo.userNameLength = USERNAME_STRING_LENGTH;\nmqttStatus = MQTT_Connect( pMqttContext, &connectInfo, NULL, CONNACK_RECV_TIMEOUT_MS, pSessionPresent );\n```\n\n## Versioning\n\nC-SDK releases will now follow a date based versioning scheme with the format YYYYMM.NN, where:\n\n* Y represents the year.\n* M represents the month.\n* N represents the release order within the designated month (00 being the first release).\n\nFor example, a second release in June 2021 would be 202106.01. Although the SDK releases have moved to date-based versioning, each library within the SDK will still retain semantic versioning. In semantic versioning, the version number itself (X.Y.Z) indicates whether the release is a major, minor, or point release. You can use the semantic version of a library to assess the scope and impact of a new release on your application.\n\n## Releases and Documentation\n\nAll of the released versions of the C-SDK libraries are available as git tags. For example, the last release of the v3 SDK version is available at [tag 3.1.5](https://github.com/aws/aws-iot-device-sdk-embedded-C/tree/v3.1.5).\n\n### 202211.00\n[API documentation of 202211.00 release](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/index.html)\n\nThis release includes an update to how the [Coverity static analysis](https://scan.coverity.com/) scans are performed. There is now a **tools/coverity/README.md** file in each library with instructions on how to perform a scan with 0 [MISRA coding standard](https://www.misra.org.uk) warnings or errors.\n\nAdditionally, this release brings in major version upgrades to [coreMQTT](https://github.com/FreeRTOS/coreMQTT) and [coreHTTP](https://github.com/FreeRTOS/coreHTTP). It also brings in minor version upgrades to all other libraries.\n\n### 202108.00\n\n[API documentation of 202108.00 release](https://aws.github.io/aws-iot-device-sdk-embedded-C/202108.00/index.html)\n\nThis release introduces the refactored [AWS IoT Fleet Provisioning](https://github.com/aws/fleet-provisioning-for-aws-iot-embedded-sdk) library and the new [AWS SigV4](https://github.com/aws/SigV4-for-AWS-IoT-embedded-sdk) library.\n\nAdditionally, this release brings minor version updates in the [AWS IoT Over-the-Air Update](https://github.com/aws/ota-for-aws-iot-embedded-sdk) and [corePKCS11](https://github.com/FreeRTOS/corePKCS11) libraries.\n\n### 202103.00\n\n[API documentation of 202103.00 release](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/index.html)\n\nThis release includes a [major update](https://github.com/aws/ota-for-aws-iot-embedded-sdk/blob/v3.0.0/CHANGELOG.md#v300-march-2021) to the APIs of the AWS IoT Over-the-air Update library.\n\nAdditionally, AWS IoT Device Shadow library introduces a [minor update](https://github.com/aws/Device-Shadow-for-AWS-IoT-embedded-sdk/blob/v1.1.0/CHANGELOG.md#v110-march-2021) by adding support for named shadow, a feature of the AWS IoT Device Shadow service that allows you to create multiple shadows for a single IoT device. AWS IoT Jobs library introduces a [minor update](https://github.com/aws/Jobs-for-AWS-IoT-embedded-sdk/blob/v1.1.0/CHANGELOG.md#v110-march-2021) by introducing macros for `$next` job ID and compile-time generation of topic strings. AWS IoT Device Defender library introduces a [minor update](https://github.com/aws/Device-Defender-for-AWS-IoT-embedded-sdk/blob/v1.1.0/CHANGELOG.md#v110-march-2021) that adds macros to API for custom metrics feature of AWS IoT Device Defender service.\n\ncorePKCS11 also introduces a [patch update](https://github.com/FreeRTOS/corePKCS11/blob/v3.0.1/CHANGELOG.md#v301-february-2021) by removing the `pkcs11configPAL_DESTROY_SUPPORTED` config and mbedTLS platform abstraction layer of `DestroyObject`. Lastly, no code changes are introduced for backoffAlgorithm, coreHTTP, coreMQTT, and coreJSON; however, patch updates are made to improve documentation and CI.\n\n### 202012.01\n\n[API documentation of 202012.01 release](https://docs.aws.amazon.com/embedded-csdk/202012.00/lib-ref/index.html)\n\nThis release includes [AWS IoT Over-the-air Update(Release Candidate)](https://github.com/aws/ota-for-aws-iot-embedded-sdk), [backoffAlgorithm](https://github.com/FreeRTOS/backoffAlgorithm), and [PKCS #11](https://github.com/FreeRTOS/corePKCS11) libraries. Additionally, there is a major update to the coreJSON and coreHTTP APIs. All libraries continue to undergo code quality checks (e.g. MISRA-C compliance), and Coverity static analysis. In addition, all libraries except AWS IoT Over-the-air Update and backoffAlgorithm undergo validation of memory safety with the C Bounded Model Checker (CBMC) automated reasoning tool.\n\n### 202011.00\n\n[API documentation of 202011.00 release](https://docs.aws.amazon.com/embedded-csdk/202011.00/lib-ref/index.html)\n\nThis release includes refactored HTTP client, AWS IoT Device Defender, and AWS IoT Jobs libraries. Additionally, there is a major update to the coreJSON API. All libraries continue to undergo code quality checks (e.g. MISRA-C compliance), Coverity static analysis, and validation of memory safety with the C Bounded Model Checker (CBMC) automated reasoning tool.\n\n### 202009.00\n\n[API documentation of 202009.00 release](https://docs.aws.amazon.com/freertos/latest/lib-ref/embedded-csdk/202009.00/lib-ref/index.html)\n\nThis release includes refactored MQTT, JSON Parser, and AWS IoT Device Shadow libraries for optimized memory usage and modularity. These libraries are included in the SDK via [Git submoduling](https://git-scm.com/book/en/v2/Git-Tools-Submodules). These libraries have gone through code quality checks including verification that no function has a [GNU Complexity](https://www.gnu.org/software/complexity/manual/complexity.html) score over 8, and checks against deviations from mandatory rules in the [MISRA coding standard](https://www.misra.org.uk). Deviations from the MISRA C:2012 guidelines are documented under [MISRA Deviations](MISRA.md). These libraries have also undergone both static code analysis from [Coverity static analysis](https://scan.coverity.com/), and validation of memory safety and data structure invariance through the [CBMC automated reasoning tool](https://www.cprover.org/cbmc/).\n\nIf you are upgrading from v3.x API of the C-SDK to the 202009.00 release, please refer to [Migration guide from v3.1.5 to 202009.00 and newer releases](#migration-guide-from-v315-to-20200900-and-newer-releases). If you are using the C-SDK v4_beta_deprecated branch, note that we will continue to maintain this branch for critical bug fixes and security patches but will not add new features to it. See the C-SDK v4_beta_deprecated branch [README](https://github.com/aws/aws-iot-device-sdk-embedded-C/blob/v4_beta_deprecated/README.md) for additional details.\n\n### v3.1.5\n\nDetails available [here](https://github.com/aws/aws-iot-device-sdk-embedded-C/tree/v3.1.5).\n\n## Porting Guide for 202009.00 and newer releases\n\nAll libraries depend on the ISO C90 standard library and additionally on the `stdint.h` library for fixed-width integers, including `uint8_t`, `int8_t`, `uint16_t`, `uint32_t` and `int32_t`, and constant macros like `UINT16_MAX`. If your platform does not support the `stdint.h` library, definitions of the mentioned fixed-width integer types will be required for porting any C-SDK library to your platform.\n\n### Porting coreMQTT\n\nGuide for porting coreMQTT library to your platform is available [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/standard/coreMQTT/docs/doxygen/output/html/mqtt_porting.html).\n\n### Porting coreHTTP\n\nGuide for porting coreHTTP library is available [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/standard/coreHTTP/docs/doxygen/output/html/http_porting.html).\n\n### Porting AWS IoT Device Shadow\n\nGuide for porting AWS IoT Device Shadow library is available [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/aws/device-shadow-for-aws-iot-embedded-sdk/docs/doxygen/output/html/shadow_porting.html).\n\n### Porting AWS IoT Device Defender\n\nGuide for porting AWS IoT Device Defender library is available [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/aws/device-defender-for-aws-iot-embedded-sdk/docs/doxygen/output/html/defender_porting.html).\n\n### Porting AWS IoT Over-the-air Update\n\nGuide for porting OTA library to your platform is available [here](https://aws.github.io/aws-iot-device-sdk-embedded-C/202211.00/libraries/aws/ota-for-aws-iot-embedded-sdk/docs/doxygen/output/html/ota_porting.html).\n\n## Migration guide from v3.1.5 to 202009.00 and newer releases\n\n### MQTT Migration\n\nMigration guide for MQTT library is available [here](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/docs/doxygen/output/html/mqtt_migration.html).\n\n### Shadow Migration\n\nMigration guide for Shadow library is available [here](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/docs/doxygen/output/html/shadow_migration.html).\n\n### Jobs Migration\n\nMigration guide for Jobs library is available [here](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/docs/doxygen/output/html/jobs_migration.html).\n\n## Branches\n\n### main branch\n\nThe [main](https://github.com/aws/aws-iot-device-sdk-embedded-C/tree/main) branch hosts the continuous development of the AWS IoT Embedded C SDK (C-SDK) libraries. Please be aware that the development at the tip of the main branch is continuously in progress, and may have bugs. Consider using the [tagged releases](https://github.com/aws/aws-iot-device-sdk-embedded-C/releases) of the C-SDK for production ready software.\n\n### v4_beta_deprecated branch (formerly named v4_beta)\n\nThe [v4_beta_deprecated](https://github.com/aws/aws-iot-device-sdk-embedded-C/tree/v4_beta_deprecated) branch contains a beta version of the C-SDK libraries, which is now deprecated. This branch was earlier named as v4_beta, and was renamed to v4_beta_deprecated. The libraries in this branch will not be released. However, critical bugs will be fixed and tested. No new features will be added to this branch.\n\n## Getting Started\n\n### Cloning\n\nThis repository uses [Git Submodules](https://git-scm.com/book/en/v2/Git-Tools-Submodules) to bring in the C-SDK libraries (eg, MQTT ) and third-party dependencies (eg, mbedtls for POSIX platform transport layer).\nNote: If you download the ZIP file provided by GitHub UI, you will not get the contents of the submodules (The ZIP file is also not a valid git repository). If you download from the [202012.00 Release Page](https://github.com/aws/aws-iot-device-sdk-embedded-C/releases/tag/202012.00) page, you will get the entire repository (including the submodules) in the ZIP file, aws-iot-device-sdk-embedded-c-202012.00.zip.\nTo clone the latest commit to main branch using HTTPS:\n\n```sh\ngit clone --recurse-submodules https://github.com/aws/aws-iot-device-sdk-embedded-C.git\n```\n\nUsing SSH:\n\n```sh\ngit clone --recurse-submodules git@github.com:aws/aws-iot-device-sdk-embedded-C.git\n```\n\nIf you have downloaded the repo without using the `--recurse-submodules` argument, you need to run:\n\n```sh\ngit submodule update --init --recursive\n```\n\nWhen building with CMake, submodules are also recursively cloned automatically. However, `-DBUILD_CLONE_SUBMODULES=0`\ncan be passed as a CMake flag to disable this functionality. This is useful when you'd like to build CMake while using a\ndifferent commit from a submodule.\n\n### Configuring Demos\n\nThe libraries in this SDK are not dependent on any operating system. However, the demos for the libraries in this SDK are built and tested on a Linux platform. The demos build with [CMake](https://cmake.org/), a cross-platform build tool.\n\n#### Prerequisites\n\n* CMake 3.2.0 or any newer version for utilizing the build system of the repository.\n* C90 compiler such as gcc\n    * Due to the use of mbedtls in corePKCS11, a C99 compiler is required if building the PKCS11 demos.\n* Although not a part of the ISO C90 standard, `stdint.h` is required for fixed-width integer types that include `uint8_t`, `int8_t`, `uint16_t`, `uint32_t` and `int32_t`, and constant macros like `UINT16_MAX`, while `stdbool.h` is required for boolean parameters in coreMQTT. For compilers that do not provide these header files, [coreMQTT](https://github.com/FreeRTOS/coreMQTT) provides the files [stdint.readme](https://github.com/FreeRTOS/coreMQTT/blob/main/source/include/stdint.readme) and [stdbool.readme](https://github.com/FreeRTOS/coreMQTT/blob/main/source/include/stdbool.readme), which can be renamed to `stdint.h` and `stdbool.h`, respectively, to provide the required type definitions.\n* A supported operating system. The ports provided with this repo are expected to work with all recent versions of the following operating systems, although we cannot guarantee the behavior on all systems.\n    * Linux system with POSIX sockets, threads, RT, and timer APIs. (We have tested on Ubuntu 18.04).\n\n##### Build Dependencies\n\nThe follow table shows libraries that need to be installed in your system to run certain demos. If a dependency is\nnot installed and cannot be built from source, demos that require that dependency will be excluded\nfrom the default `all` target.\n\nDependency | Version | Usage\n:---: | :---: | :---:\n[OpenSSL](https://github.com/openssl/openssl) | 1.1.0 or later | All TLS demos and tests with the exception of PKCS11\n[Mosquitto Client](https://github.com/eclipse/mosquitto) | 1.4.10 or later | AWS IoT Jobs Mosquitto demo\n\n#### AWS IoT Account Setup\n\nYou need to setup an AWS account and access the AWS IoT console for running the AWS IoT Device Shadow library, AWS IoT Device Defender library, AWS IoT Jobs library,\nAWS IoT OTA library and coreHTTP S3 download demos.\nAlso, the AWS account can be used for running the MQTT mutual auth demo against AWS IoT broker.\nNote that running the AWS IoT Device Defender, AWS IoT Jobs and AWS IoT Device Shadow library demos require the setup of a Thing resource for the device running the demo.\nFollow the links to:\n- [Setup an AWS account](https://portal.aws.amazon.com/billing/signup#/start).\n- [Sign-in to the AWS IoT Console](https://console.aws.amazon.com/iot/home) after setting up the AWS account.\n- [Create a Thing resource](https://docs.aws.amazon.com/iot/latest/developerguide/iot-moisture-create-thing.html).\n\nThe MQTT Mutual Authentication and AWS IoT Shadow demos include example AWS IoT policy documents to run each respective demo with AWS IoT. You may use the [MQTT Mutual auth](./demos/mqtt/mqtt_demo_mutual_auth/aws_iot_policy_example_mqtt.json) and [Shadow](./demos/shadow/shadow_demo_main/aws_iot_policy_example_shadow.json) example policies by replacing `[AWS_REGION]` and `[AWS_ACCOUNT_ID]` with the strings of your region and account identifier. While the IoT Thing name and MQTT client identifier do not need to match for the demos to run, the example policies have the Thing name and client identifier identical as per [AWS IoT best practices](https://docs.aws.amazon.com/iot/latest/developerguide/security-best-practices.html).\n\nIt can be very helpful to also have the [AWS Command Line Interface tooling](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html) installed.\n\n#### Configuring mutual authentication demos of MQTT and HTTP\n\nYou can pass the following configuration settings as command line options in order to run the mutual auth demos. Make sure to run the following command in the root directory of the C-SDK:\n\n```sh\n## optionally find your-aws-iot-endpoint from the command line\naws iot describe-endpoint --endpoint-type iot:Data-ATS\ncmake -S . -Bbuild\n-DAWS_IOT_ENDPOINT=\"<your-aws-iot-endpoint>\" -DCLIENT_CERT_PATH=\"<your-client-certificate-path>\" -DCLIENT_PRIVATE_KEY_PATH=\"<your-client-private-key-path>\"\n```\n\nIn order to set these configurations manually, edit `demo_config.h` in `demos/mqtt/mqtt_demo_mutual_auth/` and `demos/http/http_demo_mutual_auth/` to `#define` the following:\n\n* Set `AWS_IOT_ENDPOINT` to your custom endpoint. This is found on the *Settings* page of the AWS IoT Console and has a format of `ABCDEFG1234567.iot.<aws-region>.amazonaws.com` where `<aws-region>` can be an AWS region like `us-east-2`.\n   * Optionally, it can also be found with the AWS CLI command `aws iot describe-endpoint --endpoint-type iot:Data-ATS`.\n* Set `CLIENT_CERT_PATH` to the path of the client certificate downloaded when setting up the device certificate in [AWS IoT Account Setup](#aws-iot-account-setup).\n* Set `CLIENT_PRIVATE_KEY_PATH` to the path of the private key downloaded when setting up the device certificate in [AWS IoT Account Setup](#aws-iot-account-setup).\n\nIt is possible to configure `ROOT_CA_CERT_PATH` to any PEM-encoded Root CA Certificate.\nHowever, this is optional because CMake will download and set it to [AmazonRootCA1.pem](https://www.amazontrust.com/repository/AmazonRootCA1.pem) when unspecified.\nIf unspecified, the default Root CA path will be interpreted relative to where the demo binary is executed.\nFor many demos, you can change this path by modifying it in the corresponding `demo_config.h`.\n\n#### Configuring AWS IoT Device Defender and AWS IoT Device Shadow demos\n\nTo build the AWS IoT Device Defender and AWS IoT Device Shadow demos, you can pass the following configuration settings as command line options. Make sure to run the following command in the root directory of the C-SDK:\n\n```sh\ncmake -S . -Bbuild -DAWS_IOT_ENDPOINT=\"<your-aws-iot-endpoint>\" -DROOT_CA_CERT_PATH=\"<your-path-to-amazon-root-ca>\" -DCLIENT_CERT_PATH=\"<your-client-certificate-path>\" -DCLIENT_PRIVATE_KEY_PATH=\"<your-client-private-key-path>\" -DTHING_NAME=\"<your-registered-thing-name>\"\n```\n\nAn Amazon Root CA certificate can be downloaded from [here](https://www.amazontrust.com/repository/).\n\nIn order to set these configurations manually, edit `demo_config.h` in the demo folder to `#define` the following:\n\n* Set `AWS_IOT_ENDPOINT` to your custom endpoint. This is found on the *Settings* page of the AWS IoT Console and has a format of `ABCDEFG1234567.iot.us-east-2.amazonaws.com`.\n* Set `ROOT_CA_CERT_PATH` to the path of the root CA certificate downloaded when setting up the device certificate in [AWS IoT Account Setup](#aws-iot-account-setup).\n* Set `CLIENT_CERT_PATH` to the path of the client certificate downloaded when setting up the device certificate in [AWS IoT Account Setup](#aws-iot-account-setup).\n* Set `CLIENT_PRIVATE_KEY_PATH` to the path of the private key downloaded when setting up the device certificate in [AWS IoT Account Setup](#aws-iot-account-setup).\n* Set `THING_NAME` to the name of the Thing created in [AWS IoT Account Setup](#aws-iot-account-setup).\n\n#### Configuring the AWS IoT Fleet Provisioning demo\n\nTo build the AWS IoT Fleet Provisioning Demo, you can pass the following configuration settings as command line options. Make sure to run the following command in the root directory of the C-SDK:\n\n```sh\ncmake -S . -Bbuild -DAWS_IOT_ENDPOINT=\"<your-aws-iot-endpoint>\" -DROOT_CA_CERT_PATH=\"<your-path-to-amazon-root-ca>\" -DCLAIM_CERT_PATH=\"<your-claim-certificate-path>\" -DCLAIM_PRIVATE_KEY_PATH=\"<your-claim-private-key-path>\" -DPROVISIONING_TEMPLATE_NAME=\"<your-template-name>\" -DDEVICE_SERIAL_NUMBER=\"<your-serial-number>\"\n```\n\nAn Amazon Root CA certificate can be downloaded from [here](https://www.amazontrust.com/repository/).\n\nTo create a provisioning template and claim credentials, sign into your AWS account and visit [here][create_provtemplate]. Make sure to enable the \"Use the AWS IoT registry to manage your device fleet\" option. Once\nyou have created the template and credentials, modify the claim certificate's policy to match the [sample policy][sample_claim_policy].\n\nIn order to set these configurations manually, edit `demo_config.h` in the demo folder to `#define` the following:\n\n* Set `AWS_IOT_ENDPOINT` to your custom endpoint. This is found on the *Settings* page of the AWS IoT Console and has a format of `ABCDEFG1234567.iot.us-east-2.amazonaws.com`.\n* Set `ROOT_CA_CERT_PATH` to the path of the root CA certificate downloaded when setting up the device certificate in [AWS IoT Account Setup](#aws-iot-account-setup).\n* Set `CLAIM_CERT_PATH` to the path of the claim certificate downloaded when setting up the template and claim credentials.\n* Set `CLAIM_PRIVATE_KEY_PATH` to the path of the private key downloaded when setting up the template and claim credentials.\n* Set `PROVISIONING_TEMPLATE_NAME` to the name of the provisioning template created.\n* Set `DEVICE_SERIAL_NUMBER` to an arbitrary string representing a device identifier.\n\n[create_provtemplate]: https://console.aws.amazon.com/iot/home#/provisioningtemplate/create/instruction\n[sample_claim_policy]: demos/fleet_provisioning/fleet_provisioning_with_csr/example_claim_policy.json\n\n#### Configuring the S3 demos\n\nYou can pass the following configuration settings as command line options in order to run the S3 demos. Make sure to run the following command in the root directory of the C-SDK:\n\n```sh\ncmake -S . -Bbuild -DS3_PRESIGNED_GET_URL=\"s3-get-url\" -DS3_PRESIGNED_PUT_URL=\"s3-put-url\"\n```\n\n`S3_PRESIGNED_PUT_URL` is only needed for the S3 upload demo.\n\nIn order to set these configurations manually, edit `demo_config.h` in `demos/http/http_demo_s3_download_multithreaded`, and `demos/http/http_demo_s3_upload` to `#define` the following:\n\n* Set `S3_PRESIGNED_GET_URL` to a S3 presigned URL with GET access.\n* Set `S3_PRESIGNED_PUT_URL` to a S3 presigned URL with PUT access.\n\nYou can generate the presigned urls using [demos/http/common/src/presigned_urls_gen.py](demos/http/common/src/presigned_urls_gen.py). More info can be found [here](demos/http/common/src/README.md).\n\n####  Configure S3 Download HTTP Demo using SigV4 Library:\n\nRefer this [demos/http/http_demo_s3_download/README.md](demos/http/http_demo_s3_download/README.md) to follow the steps needed to configure and run the S3 Download HTTP Demo using SigV4 Library that generates the authorization HTTP header needed to authenticate the HTTP requests send to S3.\n\n#### Setup for AWS IoT Jobs demo\n\n1. The demo requires the Linux platform to contain curl and libmosquitto. On a Debian platform, these dependencies can be installed with:\n\n```\n    apt install curl libmosquitto-dev\n```\nIf the platform does not contain the `libmosquitto` library, the demo will build the library from source.\n\n`libmosquitto` 1.4.10 or any later version of the first major release is required to run this demo.\n\n2. A job that specifies the URL to download for the demo needs to be created on the AWS account for the Thing resource that will be used by the demo.\nThe job can be created directly from the [AWS IoT console](https://console.aws.amazon.com/iot/home) or using the aws cli tool.\n\nThe following creates a job that specifies a Linux Kernel link for downloading.\n\n```\n aws iot create-job \\\n        --job-id 'job_1' \\\n        --targets arn:aws:iot:us-west-2:<account-id>:thing/<thing-name> \\\n        --document '{\"url\":\"https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.8.5.tar.xz\"}'\n```\n\n#### Setup for the Greengrass local auth demo\n\nFor setting up the Greengrass local auth demo, see [the README in the demo folder](./demos/greengrass/greengrass_demo_local_auth/README.md).\n\n#### Prerequisites for the AWS Over-The-Air Update (OTA) demos\n\n1. To perform a successful OTA update, you need to complete the prerequisites mentioned [here](https://docs.aws.amazon.com/freertos/latest/userguide/ota-prereqs.html).\n1. A code signing certificate is required to authenticate the update. A code signing certificate based on the SHA-256 ECDSA algorithm will work with the current demos. An example of how to generate this kind of certificate can be found [here](https://docs.aws.amazon.com/freertos/latest/userguide/ota-code-sign-cert-esp.html).\n1. The code signing certificate can be either baked into firmware as a string, or stored as a file.\n    1. For baked in certificate method, copy the certificate to signingcredentialSIGNING_CERTIFICATE_PEM in [ota_pal_posix.c](https://github.com/aws/aws-iot-device-sdk-embedded-C/blob/main/platform/posix/ota_pal/source/ota_pal_posix.c).\n    2. For file storage method, store the certificate as a file and supply the file path in \"Path name of code signing certificate on device\" field when creating the OTA job in AWS IoT Console.\n\n#### Scheduling an OTA Update Job\n\nAfter you build and run the initial executable you will have to create another executable and schedule an OTA update job with this image.\n1. Increase the version of the application by setting macro `APP_VERSION_BUILD` in `demos/ota/ota_demo_core_[mqtt/http]/demo_config.h` to a different version than what is running.\n1. Rebuild the application using the [build steps](#building-and-running-demos) below into a different directory, say `build-dir-2`.\n1. Rename the demo executable to reflect the change, e.g. `mv ota_demo_core_mqtt ota_demo_core_mqtt2`\n1. Create an OTA job:\n    1. Go to the [AWS IoT Core console](https://console.aws.amazon.com/iot/).\n    1. Manage \u2192 Jobs \u2192 Create \u2192 Create a FreeRTOS OTA update job \u2192 Select the corresponding name for your device from the thing list.\n    1. Sign a new firmware \u2192 Create a new profile \u2192 Select any SHA-ECDSA signing platform \u2192 Upload the code signing certificate(from prerequisites) and provide its path on the device.\n    1. Select the image \u2192 Select the bucket you created during the [prerequisite steps](#prerequisites-for-the-aws-over-the-air-update-ota-demos) \u2192 Upload the binary `build-dir-2/bin/ota_demo2`.\n    1. The path on device should be the absolute path to place the executable and the binary name: e.g. `/home/ubuntu/aws-iot-device-sdk-embedded-C-staging/build-dir/bin/ota_demo_core_mqtt2`.\n    1. Select the IAM role created during the [prerequisite steps](#prerequisites-for-the-aws-over-the-air-update-ota-demos).\n    1. Create the Job.\n1. Run the initial executable again with the following command: `sudo ./ota_demo_core_mqtt` or `sudo ./ota_demo_core_http`.\n1. After the initial executable has finished running, go to the directory where the downloaded firmware image resides which is the path name used when creating an OTA job.\n1. Change the permissions of the downloaded firmware to make it executable, as it may be downloaded with read (user default) permissions only: `chmod 775 ota_demo_core_mqtt2`\n1. Run the downloaded firmware image with the following command: `sudo ./ota_demo_core_mqtt2`\n\n### Building and Running Demos\n\nBefore building the demos, ensure you have installed the [prerequisite software](#prerequisites). On Ubuntu 18.04 and 20.04, `gcc`, `cmake`, and OpenSSL can be installed with:\n```sh\nsudo apt install build-essential cmake libssl-dev\n```\n\n#### Build a single demo\n* Go to the root directory of the C-SDK.\n* Run *cmake* to generate the Makefiles: `cmake -S . -Bbuild && cd build`\n* Choose a demo from the list below or alternatively, run `make help | grep demo`:\n```\ndefender_demo\nhttp_demo_basic_tls\nhttp_demo_mutual_auth\nhttp_demo_plaintext\nhttp_demo_s3_download\nhttp_demo_s3_download_multithreaded\nhttp_demo_s3_upload\njobs_demo_mosquitto\nmqtt_demo_basic_tls\nmqtt_demo_mutual_auth\nmqtt_demo_plaintext\nmqtt_demo_serializer\nmqtt_demo_subscription_manager\nota_demo_core_http\nota_demo_core_mqtt\npkcs11_demo_management_and_rng\npkcs11_demo_mechanisms_and_digests\npkcs11_demo_objects\npkcs11_demo_sign_and_verify\nshadow_demo_main\n```\n* Replace `demo_name` with your desired demo then build it: `make demo_name`\n* Go to the `build/bin` directory and run any demo executables from there.\n\n#### Build all configured demos\n* Go to the root directory of the C-SDK.\n* Run *cmake* to generate the Makefiles: `cmake -S . -Bbuild && cd build`\n* Run this command to build all configured demos: `make`\n* Go to the `build/bin` directory and run any demo executables from there.\n\n#### Running corePKCS11 demos\n\nThe corePKCS11 demos do not require any AWS IoT resources setup, and are standalone. The demos build upon each other to introduce concepts in PKCS #11 sequentially. Below is the recommended order.\n1. `pkcs11_demo_management_and_rng`\n1. `pkcs11_demo_mechanisms_and_digests`\n1. `pkcs11_demo_objects`\n1. `pkcs11_demo_sign_and_verify`\n    1. Please note that this demo requires the private and public key generated from `pkcs11_demo_objects` to be in the directory the demo is executed from.\n\n#### Alternative option of Docker containers for running demos locally\n\nInstall Docker:\n\n```sh\ncurl -fsSL https://get.docker.com -o get-docker.sh\n\nsh get-docker.sh\n```\n\n##### Installing Mosquitto to run MQTT demos locally\n\nThe following instructions have been tested on an Ubuntu 18.04 environment with Docker and OpenSSL installed.\n\n1. Download the official Docker image for Mosquitto 1.6.14. This version is deliberately chosen so that the Docker container can load certificates from the host system.\nAny version after 1.6.14 will drop privileges as soon as the configuration file has been read (before TLS certificates are loaded).\n\n    ```sh\n    docker pull eclipse-mosquitto:1.6.14\n    ```\n\n1. If a Mosquitto broker with TLS communication needs to be run, ignore this step and proceed to the next step. A Mosquitto broker with plain text communication can be run by executing the command below.\n\n    ```\n    docker run -it -p 1883:1883 --name mosquitto-plain-text eclipse-mosquitto:1.6.14\n    ```\n\n1. Set `BROKER_ENDPOINT` defined in `demos/mqtt/mqtt_demo_plaintext/demo_config.h` to `localhost`.\n\n    Ignore the remaining steps unless a Mosquitto broker with TLS communication also needs to be run.\n\n1. For TLS communication with Mosquitto broker, server and CA credentials need to be created. Use OpenSSL commands to generate the credentials for the Mosquitto server.\n\n    Note: Make sure to use different Common Name (CN) detail between the CA and server certificates; otherwise, SSL handshake fails with exactly same Common Name (CN) detail in both the certificates.\n\n    ```sh\n    # Generate CA key and certificate. Provide the Subject field information as appropriate for CA certificate.\n    openssl req -x509 -nodes -sha256 -days 365 -newkey rsa:2048 -keyout ca.key -out ca.crt\n    ```\n\n    ```sh\n    # Generate server key and certificate.# Provide the Subject field information as appropriate for Server certificate. Make sure the Common Name (CN) field is different from the root CA certificate.\n    openssl req -nodes -sha256 -new -keyout server.key -out server.csr # Sign with the CA cert.\n    openssl x509 -req -sha256 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365\n    ```\n\n1. Create a mosquitto.conf file to use port 8883 (for TLS communication) and providing path to the generated credentials.\n\n    ```\n    port 8883\n\n    cafile /mosquitto/config/ca.crt\n    certfile /mosquitto/config/server.crt\n    keyfile /mosquitto/config/server.key\n\n    # Use this option for TLS mutual authentication (where client will provide CA signed certificate)\n    #require_certificate true\n    tls_version tlsv1.2\n    #use_identity_as_username true\n    ```\n\n1. Run the docker container from the local directory containing the generated credential and mosquitto.conf files.\n\n    ```sh\n    docker run -it -p 8883:8883 -v $(pwd):/mosquitto/config/ --name mosquitto-basic-tls eclipse-mosquitto:1.6.14\n    ```\n\n1. Update `demos/mqtt/mqtt_demo_basic_tls/demo_config.h` to the following:\n   Set `BROKER_ENDPOINT` to `localhost`.\n   Set `ROOT_CA_CERT_PATH` to the absolute path of the CA certificate created in step 4. for the local Mosquitto server.\n\n##### Installing httpbin to run HTTP demos locally\n\nRun httpbin through port 80:\n\n```sh\ndocker pull kennethreitz/httpbin\ndocker run -p 80:80 kennethreitz/httpbin\n```\n\n`SERVER_HOST` defined in `demos/http/http_demo_plaintext/demo_config.h` can now be set to `localhost`.\n\nTo run `http_demo_basic_tls`, you could use either [Tunnelmole](https://github.com/robbie-cahill/tunnelmole-client), an open source tunneling tool, or [ngrok](https://ngrok.com/download), a popular closed source tunneling tool, to create an HTTPS tunnel to the httpbin server currently hosted on port 80:\n\n**Using Tunnelmole**\nFirst, install Tunnelmole. On Linux, Mac and Windows Subsystem for Linux, use\n\n```sh\ncurl -O https://tunnelmole.com/sh/install.sh && sudo bash install.sh\n```\n\nFor Windows without WSL, [download tmole.exe](https://tunnelmole.com/downloads/tmole.exe) and add it to your [PATH](https://www.wikihow.com/Change-the-PATH-Environment-Variable-on-Windows).\n\nThen run `tmole 80`\n\n```sh\ntmole 80\n```\n\nTunnelmole will provide a https link that can be substituted in `demos/http/http_demo_basic_tls/demo_config.h` and has a format of `https://bvdo5f-ip-49-183-170-144.tunnelmole.net`.\n\nSet `SERVER_HOST` in `demos/http/http_demo_basic_tls/demo_config.h` to the https link provided by Tunnelmole, without `https://` preceding it.\n\n**Using ngrok**\n\n```sh\n./ngrok http 80 # May have to use ./ngrok.exe depending on OS or filename of the executable\n```\n\n`ngrok` will provide an https link that can be substituted in `demos/http/http_demo_basic_tls/demo_config.h` and has a format of `https://ABCDEFG12345.ngrok.io`.\n\nSet `SERVER_HOST` in `demos/http/http_demo_basic_tls/demo_config.h` to the https link provided by ngrok, without `https://` preceding it.\n\nYou must also download the Root CA certificate provided by the ngrok https link and set `ROOT_CA_CERT_PATH` in `demos/http/http_demo_basic_tls/demo_config.h` to the file path of the downloaded certificate.\n\n## Generating Documentation\nNote: For pre-generated documentation, please visit [Releases and Documentation](#releases-and-documentation) section.\n\nThe Doxygen references were created using Doxygen version 1.9.2. To generate the Doxygen pages, use the provided Python script at [tools/doxygen/generate_docs.py](tools/doxygen/generate_docs.py). Please ensure that each of the library submodules under `libraries/standard/` and `libraries/aws/` are cloned before using this script.\n\n```sh\ncd <CSDK_ROOT>\ngit submodule update --init --recursive --checkout\npython3 tools/doxygen/generate_docs.py\n```\n\nThe generated documentation landing page is located at `docs/doxygen/output/html/index.html`.\n", "release_dates": ["2022-12-01T02:32:11Z", "2021-08-17T00:11:03Z", "2021-03-02T19:12:31Z", "2020-12-15T15:47:23Z", "2020-12-14T18:38:11Z", "2020-11-04T22:36:08Z", "2020-09-17T01:08:21Z", "2018-05-10T23:24:11Z", "2018-04-17T21:17:59Z", "2018-03-26T18:06:31Z", "2017-12-26T19:07:42Z", "2017-12-06T23:10:45Z", "2016-12-08T22:27:02Z"]}, {"name": "aws-iot-device-sdk-java", "description": "Java SDK for connecting to AWS IoT from a device. ", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## New Version Available\n\nA new AWS IoT Device SDK is [now available](https://github.com/awslabs/aws-iot-device-sdk-java-v2). It is a complete rework, built to improve reliability, performance, and security. We invite your feedback!\n\nThis SDK will no longer receive feature updates, but will receive security updates.\n\n# AWS IoT Device SDK for Java\nThe **AWS IoT Device SDK for Java** enables Java developers to access the AWS\nIoT Platform through [MQTT or MQTT over the WebSocket protocol][aws-iot-protocol].\nThe SDK is built with [AWS IoT device shadow support][aws-iot-thing], providing\naccess to thing shadows (sometimes referred to as device shadows) using shadow methods, including GET, UPDATE, and DELETE.\nIt also supports a simplified shadow access model, which allows developers to\nexchange data with their shadows by just using getter and setter methods without\nhaving to serialize or deserialize any JSON documents.\n\nTo get started, use the Maven repository or download the [latest JAR file][latest-jar].\n\n* [Overview](#overview)\n* [Install the SDK](#install-the-sdk)\n* [Use the SDK](#use-the-sdk)\n* [Sample Applications](#sample-applications)\n* [API Documentation](#api-documentation)\n* [License](#license)\n* [Support](#support)\n\n## Overview\nThis document provides instructions for installing and configuring the AWS\nIoT device SDK for Java. It also includes some examples that demonstrate the use of different\nAPIs.\n\n### MQTT Connection Types\nThe SDK is built on top of the [Paho MQTT Java client library][paho-mqtt-java-download].\nDevelopers can choose from two types of connections to connect to\nthe AWS IoT service:\n\n * MQTT (over TLS 1.2) with X.509 certificate-based mutual authentication\n * MQTT over WebSocket with AWS Signature Version 4 authentication\n\nFor MQTT over TLS (port 8883), a valid certificate and private key are required\nfor authentication. For MQTT over WebSocket (port 443), a valid AWS Identity and Access Management (IAM)\naccess key ID and secret access key pair is required for authentication.\n\n### Thing Shadows\nA thing shadow represents the cloud counterpart of a physical device or thing.\nAlthough a device is not always online, its thing shadow is. A thing shadow\nstores data in and out of the device in a JSON based document. When the device is offline, its shadow document is still\naccessible to the application. When the device comes back online,\nthe thing shadow publishes the delta to the device (which the device didn't\nsee while it was offline).\n\nThe SDK implements the protocol for applications to retrieve, update, and\ndelete shadow documents mentioned [here][aws-iot-thing].\nWhen you use the simplified access model, you have the option to enable strict document versioning. To reduce the overhead of subscribing to shadow topics\nfor each method requested, the SDK automatically subscribes to all of the method\ntopics when a connection is established.\n\n#### Simplified Shadow Access Model\nUnlike the shadow methods, which operate on JSON documents, the simplified\nshadow access model allows developers to access their shadows with getter and\nsetter methods.\n\nTo use this feature, you must extend the device class ```AWSIotDevice```,\nuse the annotation ```AWSIotDeviceProperty``` to mark class member variables to be\nmanaged by the SDK, and provide getter and setter methods for accessing these\nvariables. The getter methods will be used by the SDK to report to the shadow\nperiodically. The setter methods will be invoked whenever there is a change\nto the desired state of the shadow document. For more information, see [Use the SDK](#use-the-sdk)\nlater in this document.\n\n## Install the SDK\n\n### Minimum Requirements\nTo use the SDK, you will need Java 1.7+.\n\n### Install the SDK Using Maven\nThe recommended way to use the AWS IoT Device SDK for Java in your project is\nto consume it from Maven. Simply add the following dependency to the POM file\nof your Maven project.\n\n```xml\n<dependencies>\n  <dependency>\n    <groupId>com.amazonaws</groupId>\n    <artifactId>aws-iot-device-sdk-java</artifactId>\n    <version>1.3.9</version>\n  </dependency>\n</dependencies>\n```\n\nThe sample applications included with the SDK can also be installed using the following dependency definition.\n\n```xml\n<dependencies>\n  <dependency>\n    <groupId>com.amazonaws</groupId>\n    <artifactId>aws-iot-device-sdk-java-samples</artifactId>\n    <version>1.3.9</version>\n  </dependency>\n</dependencies>\n```\n\n### Install the SDK Using the Latest JAR\nThe latest JAR files can be downloaded [here][latest-jar]. You can simply extract\nand copy the JAR files to your project's library directory, and then update your IDE to\ninclude them to your library build path.\n\nYou will also need to add two libraries the SDK depends on:\n * Jackson 2.x, including [Jackson-core] [jackson-core] and [Jackson-databind] [jackson-databind]\n * Paho MQTT client for Java 1.1.x. [download instructions][paho-mqtt-java-download]\n\n### Build the SDK from the GitHub Source\nYou can build both the SDK and its sample applications from the source\nhosted at GitHub.\n\n```sh\n$ git clone https://github.com/aws/aws-iot-device-sdk-java.git\n$ cd aws-iot-device-sdk-java\n$ mvn clean install -Dgpg.skip=true\n```\n\n## Use the SDK\nThe following sections provide some basic examples of using the SDK to access the\nAWS IoT service over MQTT. For more information about each API, see the [API documentation][api-docs].\n\n### Initialize the Client\nTo access the AWS IoT service, you must initialize ```AWSIotMqttClient```. The\nway in which you initialize the client depends on the connection\ntype (MQTT or MQTT over WebSocket) you choose. In both cases,\na valid client endpoint and client ID are required for setting up the connection.\n\n* Initialize the Client with MQTT (over TLS 1.2):\nFor this MQTT connection type (port 8883), the AWS IoT service requires TLS\nmutual authentication, so a valid client certificate (X.509)\nand RSA keys are required. You can use the\n[AWS IoT console][aws-iot-console] or the AWS command line tools to generate certificates and keys. For the SDK,\nonly a certificate file and private key file are required.\n\n```java\nString clientEndpoint = \"<prefix>-ats.iot.<region>.amazonaws.com\";   // use value returned by describe-endpoint --endpoint-type \"iot:Data-ATS\"\nString clientId = \"<unique client id>\";                              // replace with your own client ID. Use unique client IDs for concurrent connections.\nString certificateFile = \"<certificate file>\";                       // X.509 based certificate file\nString privateKeyFile = \"<private key file>\";                        // PKCS#1 or PKCS#8 PEM encoded private key file\n\n// SampleUtil.java and its dependency PrivateKeyReader.java can be copied from the sample source code.\n// Alternatively, you could load key store directly from a file - see the example included in this README.\nKeyStorePasswordPair pair = SampleUtil.getKeyStorePasswordPair(certificateFile, privateKeyFile);\nAWSIotMqttClient client = new AWSIotMqttClient(clientEndpoint, clientId, pair.keyStore, pair.keyPassword);\n\n// optional parameters can be set before connect()\nclient.connect();\n```\n\n* Initialize the Client with MQTT Over WebSocket:\nFor this MQTT connection type (port 443), you will need valid IAM credentials\nto initialize the client. This includes an AWS access key ID and secret\naccess key. There are a number of ways to get IAM credentials (for example, by creating\npermanent IAM users or by requesting temporary credentials through the Amazon Cognito\nservice). For more information, see the developer guides for these services.\n\nAs a best practice for application security, do not embed\ncredentials directly in the source code.\n\n```java\nString clientEndpoint = \"<prefix>-ats.iot.<region>.amazonaws.com\";   // use value returned by describe-endpoint --endpoint-type \"iot:Data-ATS\"\nString clientId = \"<unique client id>\";                              // replace with your own client ID. Use unique client IDs for concurrent connections.\n\n// AWS IAM credentials could be retrieved from AWS Cognito, STS, or other secure sources\nAWSIotMqttClient client = new AWSIotMqttClient(clientEndpoint, clientId, awsAccessKeyId, awsSecretAccessKey, sessionToken);\n\n// optional parameters can be set before connect()\nclient.connect();\n```\n\n### Publish and Subscribe\nAfter the client is initialized and connected, you can publish messages and subscribe\nto topics.\n\nTo publish a message using a blocking API:\n\n```java\nString topic = \"my/own/topic\";\nString payload = \"any payload\";\n\nclient.publish(topic, AWSIotQos.QOS0, payload);\n```\n\nTo publish a message using a non-blocking API:\n\n```java\npublic class MyMessage extends AWSIotMessage {\n    public MyMessage(String topic, AWSIotQos qos, String payload) {\n        super(topic, qos, payload);\n    }\n\n    @Override\n    public void onSuccess() {\n        // called when message publishing succeeded\n    }\n\n    @Override\n    public void onFailure() {\n        // called when message publishing failed\n    }\n\n    @Override\n    public void onTimeout() {\n        // called when message publishing timed out\n    }\n}\n\nString topic = \"my/own/topic\";\nAWSIotQos qos = AWSIotQos.QOS0;\nString payload = \"any payload\";\nlong timeout = 3000;                    // milliseconds\n\nMyMessage message = new MyMessage(topic, qos, payload);\nclient.publish(message, timeout);\n```\n\nTo subscribe to a topic:\n\n```java\npublic class MyTopic extends AWSIotTopic {\n    public MyTopic(String topic, AWSIotQos qos) {\n        super(topic, qos);\n    }\n\n    @Override\n    public void onMessage(AWSIotMessage message) {\n        // called when a message is received\n    }\n}\n\nString topicName = \"my/own/topic\";\nAWSIotQos qos = AWSIotQos.QOS0;\n\nMyTopic topic = new MyTopic(topicName, qos);\nclient.subscribe(topic);\n```\n\n**Note**: all operations (publish, subscribe, unsubscribe) will not timeout unless\nyou define a timeout when performing the operation. If no timeout is defined, then\na value of `0` is used, meaning the operation will never timeout and, in rare cases,\nwait forever for the server to respond and block the calling thread indefinitely.\nIt is recommended to set a timeout for QoS1 operations if your application expects\nresponses within a fixed duration or if there is the possibility the server you are\ncommunicating with may not respond.\n\n### Shadow Methods\nTo access a shadow using a blocking API:\n\n```java\nString thingName = \"<thing name>\";                    // replace with your AWS IoT Thing name\n\nAWSIotDevice device = new AWSIotDevice(thingName);\n\nclient.attach(device);\nclient.connect();\n\n// Delete existing shadow document\ndevice.delete();\n\n// Update shadow document\nState state = \"{\\\"state\\\":{\\\"reported\\\":{\\\"sensor\\\":3.0}}}\";\ndevice.update(state);\n\n// Get the entire shadow document\nString state = device.get();\n```\n\nTo access a shadow using a non-blocking API:\n\n```java\npublic class MyShadowMessage extends AWSIotMessage {\n    public MyShadowMessage() {\n        super(null, null);\n    }\n\n    @Override\n    public void onSuccess() {\n        // called when the shadow method succeeded\n        // state (JSON document) received is available in the payload field\n    }\n\n    @Override\n    public void onFailure() {\n        // called when the shadow method failed\n    }\n\n    @Override\n    public void onTimeout() {\n        // called when the shadow method timed out\n    }\n}\n\nString thingName = \"<thing name>\";      // replace with your AWS IoT Thing name\n\nAWSIotDevice device = new AWSIotDevice(thingName);\n\nclient.attach(device);\nclient.connect();\n\nMyShadowMessage message = new MyShadowMessage();\nlong timeout = 3000;                    // milliseconds\ndevice.get(message, timeout);\n```\n\n### Simplified Shadow Access Model\nTo use the simplified shadow access model, you need to extend the device class\n```AWSIotDevice```, and then use the annotation class ```AWSIotDeviceProperty```\nto mark the device attributes and provide getter and setter methods for them.\nThe following very simple example has one attribute, ```someValue```, defined.\nThe code will report the attribute to the shadow, identified by ***thingName***\nevery 5 seconds, in the ***reported*** section of the shadow document. The SDK\nwill call the setter method ```setSomeValue()``` whenever there's\na change to the ***desired*** section of the shadow document.\n\n```java\npublic class MyDevice extends AWSIotDevice {\n    public MyDevice(String thingName) {\n        super(thingName);\n    }\n\n    @AWSIotDeviceProperty\n    private String someValue;\n\n    public String getSomeValue() {\n        // read from the physical device\n    }\n\n    public void setSomeValue(String newValue) {\n        // write to the physical device\n    }\n}\n\nMyDevice device = new MyDevice(thingName);\n\nlong reportInterval = 5000;            // milliseconds. Default interval is 3000.\ndevice.setReportInterval(reportInterval);\n\nclient.attach(device);\nclient.connect();\n```\n\n### Other Topics\n#### Enable Logging\nThe SDK uses ```java.util.logging``` for logging. To change\nthe logging behavior (for example, to change the logging level or logging destination), you can\nspecify a property file using the JVM property\n```java.util.logging.config.file```. It can be provided through JVM arguments like so:\n\n```sh\n-Djava.util.logging.config.file=\"logging.properties\"\n```\n\nTo change the console logging level, the property file ***logging.properties***\nshould contain the following lines:\n\n```\n# Override of console logging level\njava.util.logging.ConsoleHandler.level=INFO\n```\n\n#### Load KeyStore from File to Initialize the Client\nYou can load a KeyStore object directly from JKS-based keystore files.\nYou will first need to import X.509 certificate and the private key into the keystore\nfile like so:\n\n```sh\n$ openssl pkcs12 -export -in <certificate-file> -inkey <private-key-file> -out p12.keystore -name alias\n(type in the export password)\n\n$ keytool -importkeystore -srckeystore p12.keystore -srcstoretype PKCS12 -srcstorepass <export-password> -alias alias -deststorepass <keystore-password> -destkeypass <key-password> -destkeystore my.keystore\n```\n\nAfter the keystore file ***my.keystore*** is created, you can use it to\ninitialize the client like so:\n\n```java\nString keyStoreFile = \"<my.keystore>\";                               // replace with your own key store file\nString keyStorePassword = \"<keystore-password>\";                     // replace with your own key store password\nString keyPassword = \"<key-password>\"                                // replace with your own key password\n\nKeyStore keyStore = KeyStore.getInstance(KeyStore.getDefaultType());\nkeyStore.load(new FileInputStream(keyStoreFile), keyStorePassword.toCharArray());\n\nString clientEndpoint = \"<prefix>.iot.<region>.amazonaws.com\";       // replace <prefix> and <region> with your own\nString clientId = \"<unique client id>\";                              // replace with your own client ID. Use unique client IDs for concurrent connections.\n\nAWSIotMqttClient client = new AWSIotMqttClient(clientEndpoint, clientId, keyStore, keyPassword);\n```\n\n#### Use ECC-Based Certificates\nYou can use Elliptic Curve Cryptography (ECC)-based certificates to initialize the client. To create an ECC key and certificate, see [this blog post][aws-iot-ecc-blog]. After you have created and registered the key and certificate, use the following command to convert\nthe ECC key file to PKCK#8 format.\n\n```sh\n$ openssl pkcs8 -topk8 -nocrypt -in ecckey.key -out ecckey-pk8.key\n(type in the key password)\n```\n\nYou can then use the instruction described in [this section](#initialize-the-client) to initialize the client\nwith just this one change.\n\n```java\n// SampleUtil.java and its dependency PrivateKeyReader.java can be copied from the sample source code.\n// Alternatively, you could load key store directly from a file - see the example included in this README.\nKeyStorePasswordPair pair = SampleUtil.getKeyStorePasswordPair(certificateFile, privateKeyFile, \"EC\");\n```\n\n#### Increase in-flight publish limit (`too many publishes in Progress` error)\n\nIf you are getting a `too many publishes in Progress` error this means that your application\nhas more operations in-flight (meaning they have not succeeded or failed, but they are waiting\nfor a response from the server) than Paho supports by default.\nBy default, the Paho client supports a maximum of `10` in-flight operations.\n\nThe recommended way to resolve this issue is to track how many QoS1 operations you\nhave sent that are in-flight and when you reach\nthe limit of `10`, you add any further operations into a queue. Then as the QoS1 operations\nare no longer in-flight you grab QoS1 operations from the queue until it is empty or until you\nhave hit the maximum of `10` in-flight operations. You then repeat this process until all the operations\nare sent. This will prevent your application from ever trying to send too many operations at once and\nexceeding the maximum in-flight limit of the Paho client.\n\nAnother way to help reduce this issue is to increase the maximum number of in-flight operations\nthat the Paho client can process. To do this, you will need to modify the source code to increase\nthis limit. Download the source code from GitHub, navigate to the `AwsIotMqttConnection.java`\nfile, and add the following line of code in the `buildMqttConnectOptions` function just under\nthe line `options.setKeepAliveInterval(client.getKeepAliveInterval() / 1000);` (around line `151`):\n\n~~~\noptions.setMaxInflight(100);\n~~~\n\nThen compile the source code and use the compiled Jar in your application.\n\nThis will increase Paho's in-flight limit to 100 and allow you to have more in-flight\nat the same time, giving additional room for sending larger volumes of QoS1 operations.\nNote that these in-flight operations still need to be acknowledged by the server or timeout\nbefore they are no longer in-flight, you can just have up to `100` in-flight rather than\nthe default of `10`.\n\nFor AWS IoT Core, you can only send a maximum of **`100` QoS1 operations per second**.\nAny operations sent after the first 100 per second will be\nignored by AWS IoT Core. For this reason, it is **highly** recommended you perform\nall operations with a timeout if you increase the maximum in-flight limit, to prevent a situation\nwhere you send more than 100 QoS1 operations per second and are waiting on an operation to get\nan acknowledgement from the sever that will never come.\n\n## Sample Applications\nThere are three samples applications included with the SDK. The easiest way to\nrun these samples is through Maven, which will take care of getting the\ndependencies.\n\n* Publish/Subscribe sample:\nThis sample consists of two publishers publishing one message per second to a\ntopic. One subscriber subscribing to the same topic receives and prints the\nmessages.\n\n* Shadow sample:\nThis sample consists of a simple demo of the simplified shadow access\nmodel. The device contains two attributes: window state and room temperature.\nWindow state can be modified (therefore, controlled) remotely through\n***desired*** state. To demonstrate this control function, you can use the AWS\nIoT console to modify the desired window state, and then see its change from the\nsample output.\n\n* Shadow echo sample:\nThis sample consists of a simple demo that uses Shadow methods to send a shadow\nupdate and then retrieve it back every second.\n\n### Arguments for the Sample Applications\nTo run the samples, you will also need to provide the following arguments\nthrough the command line:\n\n* clientEndpoint: client endpoint, obtained via calling describe-endpoint\n* clientId: client ID\n* thingName: AWS IoT thing name (not required for the Publish/Subscribe sample)\n\nYou will also need to provide either set of the following arguments for authentication.\nFor an MQTT connection, provide these arguments:\n\n* certificateFile: X.509 based certificate file (For Just-in-time registration, this\nis the concatenated file from both the device certificate and CA certificate. For more information\nabout Just-in-Time Registration, please see [this blog][Just-in-Time-Registration].\n* privateKeyFile: private key file\n* keyAlgorithm: (optional) RSA or EC. If not specified, RSA is used.\n\nFor an MQTT over WebSocket connection, provide these arguments:\n\n* awsAccessKeyId: IAM access key ID\n* awsSecretAccessKey: IAM secret access key\n* sessionToken: (optional) if temporary credentials are used\n\n### Run the Sample Applications\nYou can use the following commands to execute the sample applications (assuming\nTLS mutual authentication is used).\n\n* To run the Publish/Subscribe sample, use the following command:\n```sh\n$ mvn exec:java -pl aws-iot-device-sdk-java-samples -Dexec.mainClass=\"com.amazonaws.services.iot.client.sample.pubSub.PublishSubscribeSample\" -Dexec.args=\"-clientEndpoint <prefix>-ats.iot.<region>.amazonaws.com -clientId <unique client id> -certificateFile <certificate file> -privateKeyFile <private key file>\"\n```\n\n* To run the Shadow sample, use the following command:\n```sh\n$ mvn exec:java -pl aws-iot-device-sdk-java-samples -Dexec.mainClass=\"com.amazonaws.services.iot.client.sample.shadow.ShadowSample\" -Dexec.args=\"-clientEndpoint <prefix>-ats.iot.<region>.amazonaws.com -clientId <unique client id> -thingName <thing name> -certificateFile <certificate file> -privateKeyFile <private key file>\"\n```\n\n* To run the Shadow echo sample, use the following command:\n```sh\n$ mvn exec:java -pl aws-iot-device-sdk-java-samples -Dexec.mainClass=\"com.amazonaws.services.iot.client.sample.shadowEcho.ShadowEchoSample\" -Dexec.args=\"-clientEndpoint <prefix>-ats.iot.<region>.amazonaws.com -clientId <unique client id> -thingName <thing name> -certificateFile <certificate file> -privateKeyFile <private key file>\"\n```\n\n### Sample Source Code\nYou can get the sample source code either from the GitHub repository as described\n[here](#build-the-sdk-from-the-github-source) or from [the latest SDK binary][latest-jar].\nThey both provide you with Maven project files that you can use to build and run the samples\nfrom the command line or import them into an IDE, such as Eclipse.\n\nThe sample source code included with the latest SDK binary is shipped with a modified Maven\nproject file (pom.xml) that allows you to build the sample source indepedently, without the\nneed to reference the parent POM file as with the GitHub source tree.\n\n## API Documentation\nYou'll find the API documentation for the SDK [here][api-docs].\n\n## License\nThis SDK is distributed under the [Apache License, Version 2.0][apache-license-2]. For more information, see\nLICENSE.txt and NOTICE.txt.\n\n## Support\nIf you have technical questions about the AWS IoT Device SDK, use the [AWS IoT Forum][aws-iot-forum].\nFor any other questions about AWS IoT, contact [AWS Support][aws-support].\n\n[aws-iot-protocol]: http://docs.aws.amazon.com/iot/latest/developerguide/protocols.html\n[aws-iot-thing]: http://docs.aws.amazon.com/iot/latest/developerguide/iot-thing-shadows.html\n[aws-iot-forum]: https://forums.aws.amazon.com/forum.jspa?forumID=210\n[aws-iot-console]: http://aws.amazon.com/iot/\n[latest-jar]: https://s3.amazonaws.com/aws-iot-device-sdk-java/aws-iot-device-sdk-java-LATEST.zip\n[jackson-core]: https://github.com/FasterXML/jackson-core\n[jackson-databind]: https://github.com/FasterXML/jackson-databind\n[paho-mqtt-java-download]: https://eclipse.org/paho/clients/java/\n[api-docs]: http://aws-iot-device-sdk-java-docs.s3-website-us-east-1.amazonaws.com/\n[aws-iot-ecc-blog]: https://aws.amazon.com/blogs/iot/elliptic-curve-cryptography-and-forward-secrecy-support-in-aws-iot-3/\n[aws-support]: https://aws.amazon.com/contact-us\n[apache-license-2]: http://www.apache.org/licenses/LICENSE-2.0\n[Just-in-Time-Registration]: https://aws.amazon.com/blogs/iot/just-in-time-registration-of-device-certificates-on-aws-iot/\n", "release_dates": ["2022-08-18T18:58:42Z", "2022-04-27T22:16:41Z", "2021-03-31T16:31:44Z", "2021-03-30T17:52:39Z"]}, {"name": "aws-iot-device-sdk-java-v2", "description": "Next generation AWS IoT Client SDK for Java using the AWS Common Runtime", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS IoT Device SDK for Java v2\n\nThis document provides information about the AWS IoT device SDK for Java V2. This SDK is built on the [AWS Common Runtime](https://docs.aws.amazon.com/sdkref/latest/guide/common-runtime.html)\n\n*__Jump To:__*\n\n* [Installation](#installation)\n* [Android](./documents/ANDROID.md)\n* [Samples](samples)\n* [Getting Help](#getting-help)\n* [FAQ](./documents/FAQ.md)\n* [API Docs](https://aws.github.io/aws-iot-device-sdk-java-v2/)\n* [MQTT5 User Guide](./documents/MQTT5_Userguide.md)\n* [Migration Guide from the AWS IoT SDK for Java v1](./documents/MIGRATION_GUIDE.md)\n\n## Installation\n\n### Minimum Requirements\n\n* Java 8+ ([Download and Install Java](https://www.java.com/en/download/help/download_options.html))\n* Java JDK 8+ ([Download and Install JDK](https://docs.oracle.com/en/java/javase/18/install/overview-jdk-installation.html#GUID-8677A77F-231A-40F7-98B9-1FD0B48C346A))\n  * [Set JAVA_HOME](./documents/PREREQUISITES.md#set-java_home)\n\n[Step-by-step instructions](./documents/PREREQUISITES.md)\n\n### Requirements to build the AWS CRT locally\n* C++ 11 or higher\n   * Clang 3.9+ or GCC 4.4+ or MSVC 2015+\n* CMake 3.1+\n\n[Step-by-step instructions](./documents/PREREQUISITES.md)\n\n### Consuming IoT Device SDK from Maven in your application\n\nConsuming this SDK via Maven is the preferred method of consuming it and using it within your application. To consume the Java V2 SDK in your application, add the following to your `pom.xml` dependencies:\n\n``` xml\n<dependency>\n  <groupId>software.amazon.awssdk.iotdevicesdk</groupId>\n  <artifactId>aws-iot-device-sdk</artifactId>\n  <version>1.20.1</version>\n</dependency>\n```\n\nReplace `1.20.1` in `<version>1.20.1</version>` with the latest release version for the SDK.\nLook up the latest SDK version here: https://github.com/aws/aws-iot-device-sdk-java-v2/releases\n\n### Build IoT Device SDK from source\n\n[Install Maven and Set PATH](https://maven.apache.org/install.html)\n\n``` sh\n# Create a workspace directory to hold all the SDK files\nmkdir sdk-workspace\ncd sdk-workspace\n# Clone the repository\ngit clone https://github.com/awslabs/aws-iot-device-sdk-java-v2.git\ncd aws-iot-device-sdk-java-v2\n# Compile and install\nmvn clean install\n```\n\nIf you wish to use the latest CRT release, rather than the latest tested with the IoT SDK, you can run the following before running `mvn clean install`:\n\n~~~ sh\n# Update the version of the CRT being used\nmvn versions:use-latest-versions -Dincludes=\"software.amazon.awssdk.crt*\"\n~~~\n\n## Samples\n\n[Samples README](samples)\n\n## Getting Help\n\nThe best way to interact with our team is through GitHub. You can open a [discussion](https://github.com/aws/aws-iot-device-sdk-java-v2/discussions) for guidance questions or an [issue](https://github.com/aws/aws-iot-device-sdk-java-v2/issues/new/choose) for bug reports, or feature requests. You may also find help on community resources such as [StackOverFlow](https://stackoverflow.com/questions/tagged/aws-iot) with the tag [#aws-iot](https://stackoverflow.com/questions/tagged/aws-iot) or if you have a support plan with [AWS Support](https://aws.amazon.com/premiumsupport/), you can also create a new support case.\n\nPlease make sure to check out our resources too before opening an issue:\n\n* [FAQ](./documents/FAQ.md)\n* [IoT Guide](https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html) ([source](https://github.com/awsdocs/aws-iot-docs))\n* [MQTT5 User Guide](./documents/MQTT5_Userguide.md)\n* Check for similar [Issues](https://github.com/aws/aws-iot-device-sdk-java-v2/issues)\n* [AWS IoT Core Documentation](https://docs.aws.amazon.com/iot/)\n* [Dev Blog](https://aws.amazon.com/blogs/?awsf.blog-master-iot=category-internet-of-things%23amazon-freertos%7Ccategory-internet-of-things%23aws-greengrass%7Ccategory-internet-of-things%23aws-iot-analytics%7Ccategory-internet-of-things%23aws-iot-button%7Ccategory-internet-of-things%23aws-iot-device-defender%7Ccategory-internet-of-things%23aws-iot-device-management%7Ccategory-internet-of-things%23aws-iot-platform)\n* Integration with AWS IoT Services such as\n[Device Shadow](https://docs.aws.amazon.com/iot/latest/developerguide/iot-device-shadows.html)\n[Jobs](https://docs.aws.amazon.com/iot/latest/developerguide/iot-jobs.html)\n[Fleet Provisioning](https://docs.aws.amazon.com/iot/latest/developerguide/provision-wo-cert.html)\nis provided by code that been generated from a model of the service.\n* [Contributions Guidelines](./documents/CONTRIBUTING.md)\n* [DEVELOPING](./documents/DEVELOPING.md)\n\n## License\n\nThis library is licensed under the [Apache 2.0 License](./documents/LICENSE).\n\nLatest released version: v1.20.1\n", "release_dates": ["2024-02-29T22:49:28Z", "2024-02-09T19:08:30Z", "2024-01-26T22:48:05Z", "2024-01-04T19:12:07Z", "2024-01-03T16:38:19Z", "2023-12-14T21:29:25Z", "2023-12-01T21:36:20Z", "2023-11-08T18:47:56Z", "2023-10-26T18:15:31Z", "2023-10-06T17:35:27Z", "2023-10-06T00:47:14Z", "2023-09-22T03:02:11Z", "2023-08-23T14:09:47Z", "2023-07-28T02:15:34Z", "2023-07-07T18:12:55Z", "2023-06-08T16:06:21Z", "2023-05-11T17:52:02Z", "2023-05-09T23:12:12Z", "2023-05-02T20:38:21Z", "2023-04-13T18:34:06Z", "2023-04-04T23:50:39Z", "2023-02-09T19:29:56Z", "2023-02-01T00:37:33Z", "2023-01-27T19:36:51Z", "2023-01-18T15:17:14Z", "2023-01-18T15:16:09Z", "2022-11-30T22:37:03Z", "2022-11-25T18:28:04Z", "2022-11-11T21:17:39Z", "2022-10-04T20:21:36Z"]}, {"name": "aws-iot-device-sdk-js", "description": "SDK for connecting to AWS IoT from a device using JavaScript/Node.js", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## New Version Available\n\nA new AWS IoT Device SDK is [now available](https://github.com/awslabs/aws-iot-device-sdk-js-v2). It is a complete rework, built to improve reliability, performance, and security. We invite your feedback!\n\nThis SDK will no longer receive feature updates, but will receive security updates.\n\n# AWS IoT SDK for JavaScript\nThe aws-iot-device-sdk.js package allows developers to write JavaScript\napplications which access the AWS IoT Platform via [MQTT or MQTT over the Secure WebSocket Protocol](http://docs.aws.amazon.com/iot/latest/developerguide/protocols.html).  It can be used in Node.js environments as well as in browser applications.\n\n* [Overview](#overview)\n* [Installation](#install)\n* [Mac-Only TLS Behavior](#mac-tls-warning)\n* [Examples](#examples)\n* [API Documentation](#api)\n* [Connection Types](#connections)\n* [Example Programs](#programs)\n* [Browser Applications](#browser)\n* [Troubleshooting](#troubleshooting)\n* [Unit Tests](#unittests)\n* [License](#license)\n* [Support](#support)\n\n<a name=\"overview\"></a>\n## Overview\nThis document provides instructions on how to install and configure the AWS\nIoT device SDK for JavaScript, and includes examples demonstrating use of the\nSDK APIs.\n\n### MQTT Connection\nThis package is built on top of [mqtt.js](https://github.com/mqttjs/MQTT.js/blob/master/README.md) and provides three classes: 'device', 'thingShadow' and 'jobs'.  The 'device' class wraps [mqtt.js](https://github.com/mqttjs/MQTT.js/blob/master/README.md) to provide a\nsecure connection to the AWS IoT platform and expose the [mqtt.js](https://github.com/mqttjs/MQTT.js/blob/master/README.md) interfaces upward.  It provides features to simplify handling of intermittent connections, including progressive backoff retries, automatic re-subscription upon connection, and queued offline publishing with configurable drain rate.\n\n### Collection of Metrics\nBeginning with Release v2.2.0 of the SDK, AWS collects usage metrics indicating which language and version of the SDK is being used. This allows us to prioritize our resources towards addressing issues faster in SDKs that see the most and is an important data point. However, we do understand that not all customers would want to report this data by default. In that case, the sending of usage metrics can be easily disabled by set options.enableMetrics to false.\n\n### Thing Shadows\nThe 'thingShadow' class implements additional functionality for accessing Thing Shadows via the AWS IoT\nAPI; the thingShadow class allows devices to update, be notified of changes to,\nget the current state of, or delete Thing Shadows from AWS IoT.  Thing\nShadows allow applications and devices to synchronize their state on the AWS IoT platform.\nFor example, a remote device can update its Thing Shadow in AWS IoT, allowing\na user to view the device's last reported state via a mobile app.  The user\ncan also update the device's Thing Shadow in AWS IoT and the remote device\nwill synchronize with the new state.  The 'thingShadow' class supports multiple\nThing Shadows per mqtt connection and allows pass-through of non-Thing-Shadow\ntopics and mqtt events.\n\n### Jobs\nThe 'jobs' class implements functionality to interact with the AWS IoT Jobs service. The\nIoT Job service manages deployment of IoT fleet wide tasks such as device software/firmware\ndeployments and updates, rotation of security certificates, device reboots, and custom device\nspecific management tasks.\n\nIncluded in this package is an example 'agent'. The agent can be used either as a stand-alone\nprogram to manage installation and maintenance of files and other running processes or it can\nbe incorporated into a customized agent to meet specific application needs.\n\n<a name=\"install\"></a>\n## Installation\n\n**NOTE:** AWS IoT Node.js SDK will only support Node version 8.17 or above.\n\nYou can check your node version by\n\n```sh\nnode -v\n```\n\nInstalling with npm:\n\n```sh\nnpm install aws-iot-device-sdk\n```\n\nInstalling from github:\n\n```sh\ngit clone https://github.com/aws/aws-iot-device-sdk-js.git\ncd aws-iot-device-sdk-js\nnpm install\n```\n\n<a name=\"mac-tls-warning\"></a>\n## Mac-Only TLS Behavior\n\nPlease note that on Mac, once a private key is used with a certificate,\nthat certificate-key pair is imported into the Mac Keychain.\nAll subsequent uses of that certificate will use the stored private key and\nignore anything passed in programmatically.\n\n<a name=\"examples\"></a>\n## Examples\n\n### Device Class\n```js\n// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: MIT-0\nvar awsIot = require('aws-iot-device-sdk');\n\n//\n// Replace the values of '<YourUniqueClientIdentifier>' and '<YourCustomEndpoint>'\n// with a unique client identifier and custom host endpoint provided in AWS IoT.\n// NOTE: client identifiers must be unique within your AWS account; if a client attempts\n// to connect with a client identifier which is already in use, the existing\n// connection will be terminated.\n//\nvar device = awsIot.device({\n   keyPath: <YourPrivateKeyPath>,\n  certPath: <YourCertificatePath>,\n    caPath: <YourRootCACertificatePath>,\n  clientId: <YourUniqueClientIdentifier>,\n      host: <YourCustomEndpoint>\n});\n\n//\n// Device is an instance returned by mqtt.Client(), see mqtt.js for full\n// documentation.\n//\ndevice\n  .on('connect', function() {\n    console.log('connect');\n    device.subscribe('topic_1');\n    device.publish('topic_2', JSON.stringify({ test_data: 1}));\n  });\n\ndevice\n  .on('message', function(topic, payload) {\n    console.log('message', topic, payload.toString());\n  });\n```\n\n### Thing Shadow Class\n```js\n// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: MIT-0\nvar awsIot = require('aws-iot-device-sdk');\n\n//\n// Replace the values of '<YourUniqueClientIdentifier>' and '<YourCustomEndpoint>'\n// with a unique client identifier and custom host endpoint provided in AWS IoT cloud\n// NOTE: client identifiers must be unique within your AWS account; if a client attempts\n// to connect with a client identifier which is already in use, the existing\n// connection will be terminated.\n//\nvar thingShadows = awsIot.thingShadow({\n   keyPath: <YourPrivateKeyPath>,\n  certPath: <YourCertificatePath>,\n    caPath: <YourRootCACertificatePath>,\n  clientId: <YourUniqueClientIdentifier>,\n      host: <YourCustomEndpoint>\n});\n\n//\n// Client token value returned from thingShadows.update() operation\n//\nvar clientTokenUpdate;\n\n//\n// Simulated device values\n//\nvar rval = 187;\nvar gval = 114;\nvar bval = 222;\n\nthingShadows.on('connect', function() {\n//\n// After connecting to the AWS IoT platform, register interest in the\n// Thing Shadow named 'RGBLedLamp'.\n//\n    thingShadows.register( 'RGBLedLamp', {}, function() {\n\n// Once registration is complete, update the Thing Shadow named\n// 'RGBLedLamp' with the latest device state and save the clientToken\n// so that we can correlate it with status or timeout events.\n//\n// Thing shadow state\n//\n       var rgbLedLampState = {\"state\":{\"desired\":{\"red\":rval,\"green\":gval,\"blue\":bval}}};\n\n       clientTokenUpdate = thingShadows.update('RGBLedLamp', rgbLedLampState  );\n//\n// The update method returns a clientToken; if non-null, this value will\n// be sent in a 'status' event when the operation completes, allowing you\n// to know whether or not the update was successful.  If the update method\n// returns null, it's because another operation is currently in progress and\n// you'll need to wait until it completes (or times out) before updating the\n// shadow.\n//\n       if (clientTokenUpdate === null)\n       {\n          console.log('update shadow failed, operation still in progress');\n       }\n    });\n});\nthingShadows.on('status',\n    function(thingName, stat, clientToken, stateObject) {\n       console.log('received '+stat+' on '+thingName+': '+\n                   JSON.stringify(stateObject));\n//\n// These events report the status of update(), get(), and delete()\n// calls.  The clientToken value associated with the event will have\n// the same value which was returned in an earlier call to get(),\n// update(), or delete().  Use status events to keep track of the\n// status of shadow operations.\n//\n    });\n\nthingShadows.on('delta',\n    function(thingName, stateObject) {\n       console.log('received delta on '+thingName+': '+\n                   JSON.stringify(stateObject));\n    });\n\nthingShadows.on('timeout',\n    function(thingName, clientToken) {\n       console.log('received timeout on '+thingName+\n                   ' with token: '+ clientToken);\n//\n// In the event that a shadow operation times out, you'll receive\n// one of these events.  The clientToken value associated with the\n// event will have the same value which was returned in an earlier\n// call to get(), update(), or delete().\n//\n    });\n\n```\n### Jobs Class\n```js\n// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: MIT-0\nvar awsIot = require('aws-iot-device-sdk');\n\n//\n// Replace the values of '<YourUniqueClientIdentifier>' and '<YourCustomEndpoint>'\n// with a unique client identifier and custom host endpoint provided in AWS IoT cloud\n// NOTE: client identifiers must be unique within your AWS account; if a client attempts\n// to connect with a client identifier which is already in use, the existing\n// connection will be terminated.\n//\nvar jobs = awsIot.jobs({\n   keyPath: <YourPrivateKeyPath>,\n  certPath: <YourCertificatePath>,\n    caPath: <YourRootCACertificatePath>,\n  clientId: <YourUniqueClientIdentifier>,\n      host: <YourCustomEndpoint>\n});\n\n//\n// Jobs is built on top of awsIot.device and inherits all of the same functionality.\n//\njobs\n  .on('connect', function() {\n    console.log('connect');\n    device.subscribe('topic_1');\n    device.publish('topic_2', JSON.stringify({ test_data: 1}));\n    });\n\njobs\n  .on('message', function(topic, payload) {\n    console.log('message', topic, payload.toString());\n  });\n\n//\n// To subscribe to job execution events call the subscribeToJobs method which takes\n// a callback that will be invoked when a job execution is available or an error\n// occurs. The job object passed to the callback contains information about the job\n// execution and methods for updating the job execution status. Details covered\n// in the API documentation below.\n//\njobs.subscribeToJobs(thingName, function(err, job) {\n   if (isUndefined(err)) {\n      console.log('default job handler invoked, jobId: ' + job.id.toString());\n      console.log('job document: ' + job.document);\n   }\n   else {\n      console.error(err);\n   }\n});\n\njobs.subscribeToJobs(thingName, 'customJob', function(err, job) {\n   if (isUndefined(err)) {\n      console.log('customJob operation handler invoked, jobId: ' + job.id.toString());\n      console.log('job document: ' + job.document);\n   }\n   else {\n      console.error(err);\n   }\n});\n\n//\n// After calling subscribeToJobs for each operation on a particular thing call\n// startJobNotifications to cause any existing queued job executions for the given\n// thing to be published to the appropriate subscribeToJobs handler. Only needs\n// to be called once per thing.\n//\njobs.startJobNotifications(thingName, function(err) {\n   if (isUndefined(err)) {\n      console.log('job notifications initiated for thing: ' + thingName);\n   }\n   else {\n      console.error(err);\n   }\n});\n\n```\n\n<a name=\"api\"></a>\n## API Documentation\n\n  * <a href=\"#device\"><code>awsIot.<b>device()</b></code></a>\n  * <a href=\"#thingShadow\"><code>awsIot.<b>thingShadow()</b></code></a>\n  * <a href=\"#jobs\"><code>awsIot.<b>jobs()</b></code></a>\n  * <a href=\"#register\"><code>awsIot.thingShadow#<b>register()</b></code></a>\n  * <a href=\"#unregister\"><code>awsIot.thingShadow#<b>unregister()</b></code></a>\n  * <a href=\"#update\"><code>awsIot.thingShadow#<b>update()</b></code></a>\n  * <a href=\"#get\"><code>awsIot.thingShadow#<b>get()</b></code></a>\n  * <a href=\"#delete\"><code>awsIot.thingShadow#<b>delete()</b></code></a>\n  * <a href=\"#publish\"><code>awsIot.thingShadow#<b>publish()</b></code></a>\n  * <a href=\"#subscribe\"><code>awsIot.thingShadow#<b>subscribe()</b></code></a>\n  * <a href=\"#unsubscribe\"><code>awsIot.thingShadow#<b>unsubscribe()</b></code></a>\n  * <a href=\"#end\"><code>awsIot.thingShadow#<b>end()</b></code></a>\n  * <a href=\"#subscribeToJobs\"><code>awsIot.jobs#<b>subscribeToJobs()</b></code></a>\n  * <a href=\"#unsubscribeFromJobs\"><code>awsIot.jobs#<b>unsubscribeFromJobs()</b></code></a>\n  * <a href=\"#startJobNotifications\"><code>awsIot.jobs#<b>startJobNotifications()</b></code></a>\n  * <a href=\"#job\"><code><b>job</b></code></a>\n  * <a href=\"#document\"><code>job#<b>document</b></code></a>\n  * <a href=\"#id\"><code>job#<b>id</b></code></a>\n  * <a href=\"#operation\"><code>job#<b>operation</b></code></a>\n  * <a href=\"#status\"><code>job#<b>status</b></code></a>\n  * <a href=\"#inProgress\"><code>job#<b>inProgress()</b></code></a>\n  * <a href=\"#failed\"><code>job#<b>failed()</b></code></a>\n  * <a href=\"#succeeded\"><code>job#<b>succeeded()</b></code></a>\n\n-------------------------------------------------------\n<a name=\"device\"></a>\n### awsIot.device(options)\n\nReturns a wrapper for the [mqtt.Client()](https://github.com/mqttjs/MQTT.js/blob/master/README.md#client)\nclass, configured for a TLS connection with the AWS IoT platform and with\narguments as specified in `options`.  The AWSIoT-specific arguments are as\nfollows:\n\n  * `host`: the AWS IoT endpoint you will use to connect\n  * `clientId`: the client ID you will use to connect to AWS IoT\n  * `certPath`: path of the client certificate file\n  * `keyPath`: path of the private key file associated with the client certificate\n  * `caPath`: path of your CA certificate file\n  * `clientCert`: same as `certPath`, but can also accept a buffer containing client certificate data\n  * `privateKey`: same as `keyPath`, but can also accept a buffer containing private key data\n  * `caCert`: same as `caPath`, but can also accept a buffer containing CA certificate data\n  * `autoResubscribe`: set to 'true' to automatically re-subscribe to topics after reconnection (default 'true')\n  * `offlineQueueing`: set to 'true' to automatically queue published messages while offline (default 'true')\n  * `offlineQueueMaxSize`: enforce a maximum size for the offline message queue (default 0, e.g. no maximum)\n  * `offlineQueueDropBehavior`: set to 'oldest' or 'newest' to define drop behavior on a full queue when offlineQueueMaxSize > 0\n  * `drainTimeMs`: the minimum time in milliseconds between publishes when draining after reconnection (default 250)\n  * `baseReconnectTimeMs`: the base reconnection time in milliseconds (default 1000)\n  * `maximumReconnectTimeMs`: the maximum reconnection time in milliseconds (default 128000)\n  * `minimumConnectionTimeMs`: the minimum time in milliseconds that a connection must be maintained in order to be considered stable (default 20000)\n  * `protocol`: the connection type, either 'mqtts' (default), 'wss' (WebSocket/TLS), or 'wss-custom-auth' (WebSocket/TLS with custom authentication).  Note that when set to 'wss', values must be provided for the Access Key ID and Secret Key in either the following options or in environment variables as specified in [WebSocket Configuration](#websockets). When set to 'wss-custom-auth', valid headers must be provided as specified in [Custom Auth](#custom-auth)\n  * `websocketOptions`: if `protocol` is set to 'wss', you can use this parameter to pass additional options to the underlying WebSocket object; these options are documented [here](https://github.com/websockets/ws/blob/master/doc/ws.md#class-wswebsocket).\n  * `filename`: used to load credentials from the file different than the default location when `protocol` is set to 'wss'. Default value is '~/.aws/credentials'\n  * `profile`: used to specify which credential profile to be used when `protocol` is set to 'wss'.  Default value is 'default'\n  * `accessKeyId`: used to specify the Access Key ID when `protocol` is set to 'wss'.  Overrides the environment variable `AWS_ACCESS_KEY_ID` and `AWS_ACCESS_KEY_ID` from `filename` if set.\n  * `secretKey`: used to specify the Secret Key when `protocol` is set to 'wss'.  Overrides the environment variable `AWS_SECRET_ACCESS_KEY`and `AWS_SECRET_ACCESS_KEY`  from `filename` if set.\n  * `sessionToken`: (required when authenticating via Cognito, optional otherwise) used to specify the Session Token when `protocol` is set to 'wss'.  Overrides the environment variable `AWS_SESSION_TOKEN` if set.\n  * `region`: used to specify AWS account region (e.g. 'us-east-1') when `protocol` is set to `wss`. If undefined, a value is derived from `host`.\n  * `customAuthHeaders`: used to specify your custom authorization headers when `protocol` is set to 'wss-custom-auth'. The fields 'X-Amz-CustomAuthorizer-Name', 'X-Amz-CustomAuthorizer-Signature', and the field for your token name are required.\n  * `servername`: used for SNI. If undefined, a value is derived from `host`.\n  * `port`: used to specify which port to connect to. If undefined, 443 or 8883 will be chosen depending on `protocol`.\n  * `customAuthQueryString`: used to specify the token credentials in a query string for custom authorization when `protocol` is set to `wss-custom-auth`. More info can be found [here](https://docs.aws.amazon.com/iot/latest/developerguide/custom-auth.html#custom-auth-websockets).\n  * `keepalive`: used to specify the time interval for each ping request. Default is set to 300 seconds to connect to AWS IoT.\n  * `enableMetrics`: used to report SDK version usage metrics. It is set to true by default. To disable metrics collection, set value to false.\n  * `debug`: set to 'true' for verbose logging (default 'false').\n\nAll certificates and keys must be in PEM format.\n\n`options` also contains arguments specific to mqtt.  See [the mqtt client documentation]\n(https://github.com/mqttjs/MQTT.js/blob/master/README.md#client) for details\nof these arguments. Note, AWS IoT doesn't support retained messages; setting `retain` flag to\n'true' for message publishing, including Last Will and Testament messages, will result in\nconnection termination. For AWS IoT protocol specifics, please visit [here](http://docs.aws.amazon.com/iot/latest/developerguide/protocols.html).\n\nSupports all events emitted by the [mqtt.Client()](https://github.com/mqttjs/MQTT.js/blob/master/README.md#client) class.\n\n<a name=\"updateWebSocketCredentials\"></a>\n### awsIot.device#updateWebSocketCredentials(accessKeyId, secretKey, sessionToken, expiration)\n\nUpdate the credentials set used to authenticate via WebSocket/SigV4.  This method is designed to be invoked during the callback of the [getCredentialsForIdentity method](http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentity.html#getCredentialsForIdentity-property) in the [AWS SDK for JavaScript](http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/index.html).\n\n* `accessKeyId`: the latest Access Key to use when connecting via WebSocket/SigV4\n* `secretKey`: the latest Secret Key to use when connecting via WebSocket/SigV4\n* `sessionToken`: the latest Session Token to use when connecting via WebSocket/SigV4\n* `expiration`: the time this credentials set will expire\n\n-------------------------------------------------------\n<a name=\"thingShadow\"></a>\n### awsIot.thingShadow(deviceOptions, thingShadowOptions)\n\nThe `thingShadow` class wraps an instance of the `device` class with additional\nfunctionality to operate on Thing Shadows via the AWS IoT API.  The\narguments in `deviceOptions` include all those in the [device class](#device).\nthingShadowOptions has the addition of the following arguments specific to the `thingShadow` class:\n\n* `operationTimeout`: the timeout for thing operations (default 10 seconds)\n\nSupports all events emitted by the [mqtt.Client()](https://github.com/mqttjs/MQTT.js/blob/master/README.md#client) class; however, the semantics for the\n`message` event are slightly different and additional events are available\nas described below:\n\n### Event `'message'`\n\n`function(topic, message) {}`\n\nEmitted when a message is received on a topic not related to any Thing Shadows:\n* `topic` topic of the received packet\n* `message` payload of the received packet\n\n### Event `'status'`\n\n`function(thingName, stat, clientToken, stateObject) {}`\n\nEmitted when an operation `update|get|delete` completes.\n* `thingName` name of the Thing Shadow for which the operation has completed\n* `stat` status of the operation `accepted|rejected`\n* `clientToken` the operation's clientToken\n* `stateObject` the stateObject returned for the operation\n\nApplications can use clientToken values to correlate status events with the\noperations that they are associated with by saving the clientTokens returned\nfrom each operation.\n\n### Event `'delta'`\n\n`function(thingName, stateObject) {}`\n\nEmitted when a delta has been received for a registered Thing Shadow.\n* `thingName` name of the Thing Shadow that has received a delta\n* `stateObject` the stateObject returned for the operation\n\n### Event `'foreignStateChange'`\n\n`function(thingName, operation, stateObject) {}`\n\nEmitted when a different client's update or delete operation is accepted on\nthe shadow.\n\n* `thingName` name of the Thing Shadow for which the operation has completed\n* `operation` operation performed by the foreign client `update|delete`\n* `stateObject` the stateObject returned for the operation\n\nThis event allows an application to be aware of successful update or\ndelete operations performed by different clients.\n\n### Event `'timeout'`\n\n`function(thingName, clientToken) {}`\n\nEmitted when an operation `update|get|delete` has timed out.\n* `thingName` name of the Thing Shadow that has received a timeout\n* `clientToken` the operation's clientToken\n\nApplications can use clientToken values to correlate timeout events with the\noperations that they are associated with by saving the clientTokens returned\nfrom each operation.\n\n-------------------------------------------------------\n<a name=\"register\"></a>\n### awsIot.thingShadow#register(thingName, [options], [callback] )\n\nRegister interest in the Thing Shadow named `thingName`.  The thingShadow class will\nsubscribe to any applicable topics, and will fire events for the Thing Shadow\nuntil [awsIot.thingShadow#unregister()](#unregister) is called with `thingName`.  `options`\ncan contain the following arguments to modify how this Thing Shadow is processed:\n\n* `ignoreDeltas`: set to `true` to not subscribe to the `delta` sub-topic for this Thing Shadow; used in cases where the application is not interested in changes (e.g. update only.) (default `false`)\n* `persistentSubscribe`: set to `false` to unsubscribe from all operation sub-topics while not performing an operation (default `true`)\n* `discardStale`: set to `false` to allow receiving messages with old version numbers (default `true`)\n* `enableVersioning`: set to `true` to send version numbers with shadow updates (default `true`)\n\nThe `persistentSubscribe` argument allows an application to get faster operation\nresponses at the expense of potentially receiving more irrelevant response\ntraffic (i.e., response traffic for other clients who have registered interest\nin the same Thing Shadow).  When `persistentSubscribe` is set to `false`, operation\nsub-topics are only subscribed to during the scope of that operation;\nnote that in this mode, update, get, and delete operations will be much slower;\nhowever, the application will be less likely to receive irrelevant response traffic.\n\nThe `discardStale` argument allows applications to receive messages which have\nobsolete version numbers.  This can happen when messages are received out-of-order;\napplications which set this argument to `false` should use other methods to\ndetermine how to treat the data (e.g. use a time stamp property to know how old/stale\nit is).\n\nIf `enableVersioning` is set to true, version numbers will be sent with each operation.\nAWS IoT maintains version numbers for each shadow, and will reject operations which\ncontain the incorrect version; in applications where multiple clients update the same\nshadow, clients can use versioning to avoid overwriting each other's changes.\n\nIf the `callback` parameter is provided, it will be invoked after registration is complete (i.e., when subscription ACKs have been received for all shadow topics).  Applications should wait until shadow registration is complete before performing update/get/delete operations.\n\n-------------------------------------------------------\n<a name=\"unregister\"></a>\n### awsIot.thingShadow#unregister(thingName)\n\nUnregister interest in the Thing Shadow named `thingName`.  The thingShadow class\nwill unsubscribe from all applicable topics and no more events will be fired\nfor `thingName`.\n\n-------------------------------------------------------\n<a name=\"update\"></a>\n### awsIot.thingShadow#update(thingName, stateObject)\n\nUpdate the Thing Shadow named `thingName` with the state specified in the\nJavaScript object `stateObject`.  `thingName` must have been previously\nregistered\nusing [awsIot.thingShadow#register()](#register).  The thingShadow class will subscribe\nto all applicable topics and publish `stateObject` on the <b>update</b> sub-topic.\n\nThis function returns a `clientToken`, which is a unique value associated with\nthe update operation.  When a 'status' or 'timeout' event is emitted,\nthe `clientToken` will be supplied as one of the parameters, allowing the\napplication to keep track of the status of each operation.  The caller may\ncreate their own `clientToken` value; if `stateObject` contains a `clientToken`\nproperty, that will be used rather than the internally generated value.  Note\nthat it should be of atomic type (i.e. numeric or string).  This function\nreturns 'null' if an operation is already in progress.\n\n-------------------------------------------------------\n<a name=\"get\"></a>\n### awsIot.thingShadow#get(thingName, [clientToken])\n\nGet the current state of the Thing Shadow named `thingName`, which must have\nbeen previously registered using [awsIot.thingShadow#register()](#register).  The\nthingShadow class will subscribe to all applicable topics and publish on the\n<b>get</b> sub-topic.\n\nThis function returns a `clientToken`, which is a unique value associated with\nthe get operation.  When a 'status or 'timeout' event is emitted,\nthe `clientToken` will be supplied as one of the parameters, allowing the\napplication to keep track of the status of each operation.  The caller may\nsupply their own `clientToken` value (optional); if supplied, the value of\n`clientToken` will be used rather than the internally generated value.  Note\nthat this value should be of atomic type (i.e. numeric or string).  This\nfunction returns 'null' if an operation is already in progress.\n\n-------------------------------------------------------\n<a name=\"delete\"></a>\n### awsIot.thingShadow#delete(thingName, [clientToken])\n\nDelete the Thing Shadow named `thingName`, which must have been previously\nregistered using [awsIot.thingShadow#register()](#register).  The thingShadow class\nwill subscribe to all applicable topics and publish on the <b>delete</b>\nsub-topic.\n\nThis function returns a `clientToken`, which is a unique value associated with\nthe delete operation.  When a 'status' or 'timeout' event is emitted,\nthe `clientToken` will be supplied as one of the parameters, allowing the\napplication to keep track of the status of each operation.  The caller may\nsupply their own `clientToken` value (optional); if supplied, the value of\n`clientToken` will be used rather than the internally generated value.  Note\nthat this value should be of atomic type (i.e. numeric or string).  This\nfunction returns 'null' if an operation is already in progress.\n\n-------------------------------------------------------\n<a name=\"publish\"></a>\n### awsIot.thingShadow#publish(topic, message, [options], [callback])\n\nIdentical to the [mqtt.Client#publish()](https://github.com/mqttjs/MQTT.js/blob/master/README.md#publish)\nmethod, with the restriction that the topic may not represent a Thing Shadow.\nThis method allows the user to publish messages to topics on the same connection\nused to access Thing Shadows.\n\n-------------------------------------------------------\n<a name=\"subscribe\"></a>\n### awsIot.thingShadow#subscribe(topic, [options], [callback])\n\nIdentical to the [mqtt.Client#subscribe()](https://github.com/mqttjs/MQTT.js/blob/master/README.md#subscribe)\nmethod, with the restriction that the topic may not represent a Thing Shadow.\nThis method allows the user to subscribe to messages from topics on the same\nconnection used to access Thing Shadows.\n\n-------------------------------------------------------\n<a name=\"unsubscribe\"></a>\n### awsIot.thingShadow#unsubscribe(topic, [callback])\n\nIdentical to the [mqtt.Client#unsubscribe()](https://github.com/mqttjs/MQTT.js/blob/master/README.md#unsubscribe)\nmethod, with the restriction that the topic may not represent a Thing Shadow.\nThis method allows the user to unsubscribe from topics on the same\nused to access Thing Shadows.\n\n-------------------------------------------------------\n<a name=\"end\"></a>\n### awsIot.thingShadow#end([force], [callback])\n\nInvokes the [mqtt.Client#end()](https://github.com/mqttjs/MQTT.js/blob/master/README.md#end)\nmethod on the MQTT connection owned by the `thingShadow` class.  The `force`\nand `callback` parameters are optional and identical in function to the\nparameters in the [mqtt.Client#end()](https://github.com/mqttjs/MQTT.js/blob/master/README.md#end) method.\n\n-------------------------------------------------------\n<a name=\"jobs\"></a>\n### awsIot.jobs(deviceOptions)\n\nThe `jobs` class wraps an instance of the `device` class with additional functionality to\nhandle job execution management through the AWS IoT Jobs platform. Arguments in `deviceOptions`\nare the same as those in the [device class](#device) and the `jobs` class supports all of the\nsame events and functions as the `device` class.\n\nThe `jobs` class also supports the following methods:\n\n-------------------------------------------------------\n<a name=\"subscribeToJobs\"></a>\n### awsIot.jobs#subscribeToJobs(thingName, [operationName], callback)\n\nSubscribes to job execution notifications for the thing named `thingName`. If\n`operationName` is specified then the callback will only be called when a job\nready for execution contains a property called `operation` in its job document with\na value matching `operationName`. If `operationName` is omitted then the callback\nwill be called for every job ready for execution that does not match another\n`subscribeToJobs` subscription.\n\n* `thingName` name of the Thing to receive job execution notifications\n* `operationName` optionally filter job execution notifications to jobs with a value\n         for the `operation` property that matches `operationName`\n* `callback` - function (err, job) callback for when a job execution is ready for processing or an error occurs\n        - `err` a subscription error or an error that occurs when client is disconnecting\n        - `job` an object that contains  [job](#job) execution information and functions for updating job execution status.\n\n-------------------------------------------------------\n<a name=\"unsubscribeFromJobs\"></a>\n### awsIot.jobs#unsubscribeFromJobs(thingName, [operationName], callback)\n\nUnsubscribes from job execution notifications for the thing named `thingName` having\noperations with a value of the given `operationName`. If `operationName` is omitted then\nthe default handler for the thing with the given name is unsubscribed.\n\n* `thingName` name of the Thing to cancel job execution notifications for\n* `operationName` optional name of previously subscribed operation names\n* `callback` - function (err) callback for when the unsubscribe operation completes\n\n-------------------------------------------------------\n<a name=\"startJobNotifications\"></a>\n### awsIot.jobs#startJobNotifications(thingName, [callback])\n\nCauses any existing queued job executions for the given thing to be published\nto the appropriate subscribeToJobs handler. Only needs to be called once per thing.\n\n* `thingName` name of the Thing to cancel job execution notifications for\n* `callback` - function (err) callback for when the startJobNotifications operation completes\n\n-------------------------------------------------------\n<a name=\"job\"></a>\n## job\nObject that contains job execution information and functions for updating job execution status.\n\n-------------------------------------------------------\n<a name=\"document\"></a>\n### job.document\nThe JSON document describing details of the job to be executed eg.\n```\n{\n    \"operation\": \"install\",\n    \"otherProperty\": \"value\",\n    ...\n}\n```\n-------------------------------------------------------\n<a name=\"id\"></a>\n### job.id\nReturns the job id.\n\n-------------------------------------------------------\n<a name=\"operation\"></a>\n### job.operation\nReturns the job operation from the job document. Eg. 'install', 'reboot', etc.\n\n-------------------------------------------------------\n<a name=\"status\"></a>\n### job.status\nReturns the current job status according to AWS Orchestra.\n```\n{\n    \"status\":\"IN_PROGRESS|QUEUED\",\n    \"statusDetails\": {\n        \"progress\":\"50%\"\n    }\n}\n```\n-------------------------------------------------------\n<a name=\"inProgress\"></a>\n### job.inProgress([statusDetails],[callback])\nUpdate the status of the job execution to be IN_PROGRESS for the thing associated with the job.\n\n* `statusDetails` optional document describing the status details of the in progress job e.g.\n```\n{\n    \"string\": \"string\",\n    \"progress\": \"50%\"\n}\n```\n* `callback` - function(err) optional callback for when the operation completes, err is null if no error occurred\n\n-------------------------------------------------------\n<a name=\"failed\"></a>\n### job.failed([statusDetails],[callback])\nUpdate the status of the job execution to be FAILED for the thing associated with the job.\n\n* `statusDetails` optional document describing the status details of the in progress job e.g.\n```\n{\n    \"string\": \"string\",\n    \"progress\": \"0%\"\n}\n```\n* `callback` - function(err) optional callback for when the operation completes, err is null if no error occurred\n\n-------------------------------------------------------\n<a name=\"succeeded\"></a>\n### job.succeeded([statusDetails],[callback])\nUpdate the status of the job execution to be SUCCESS for the thing associated with the job.\n\n* `statusDetails` optional document describing the status details of the in progress job e.g.\n```\n{\n    \"string\": \"string\",\n    \"progress\": \"100%\"\n}\n```\n* `callback` - function(err) optional callback for when the operation completes, err is null if no error occurred\n\n<a name=\"connections\"></a>\n## Connection Types\n\nThis SDK supports three types of connections to the AWS IoT platform:\n\n* MQTT over TLS with mutual certificate authentication using port 8883\n* MQTT over WebSocket/TLS with SigV4 authentication using port 443\n* MQTT over WebSocket/TLS using a custom authorization function to authenticate\n\nThe default connection type is MQTT over TLS with mutual certificate authentication; to\nconfigure a WebSocket/TLS connection, set the `protocol` option to `wss` when instantiating\nthe [awsIot.device()](#device) or [awsIot.thingShadow()](#thingShadow) classes. To use custom auth,\nset the `protocol` option to `wss-custom-auth`.\n\n<a name=\"custom-auth\"></a>\n### Custom Authorization Configuration\n\nTo use custom authorization, you must first set up an authorizer function in Lambda and register it\nwith IoT. Once you do, you will be able to authenticate using this function. There are two ways to use custom auth:\n* set the `customAuthHeaders` option to your headers object when instantiating the [awsIotDevice()](#device)\nor [awsIot.thingShadow()](#thingShadow) classes. The headers object is an object containing the header name\nand values as key-value pairs:\n\n```js\n    {\n        'X-Amz-CustomAuthorizer-Name': 'TestAuthorizer',\n        'X-Amz-CustomAuthorizer-Signature': 'signature',\n        'TestAuthorizerToken': 'token'\n    }\n```\n* set the `customAuthQueryString` option to your headers object when instantiating the [awsIotDevice()](#device) class. The query string is a string containing the values as key-value pairs:\n\n```js\n    '?X-Amz-CustomAuthorizer-Name=TestAuthorizer&X-Amz-CustomAuthorizer-Signature=signature&TestAuthorizerToken=token'\n```\n\n\n<a name=\"programs\"></a>\n## Example Programs\n\nThe 'examples' directory contains several programs which demonstrate usage\nof the AWS IoT APIs:\n\n* device-example.js: demonstrate simple MQTT publish and subscribe\noperations.\n\n* [echo-example.js](#echoExample): test Thing Shadow operation by echoing all delta\nstate updates to the update topic; used in conjunction with the [AWS\nIoT Console](https://console.aws.amazon.com/iot) to verify connectivity\nwith the AWS IoT platform.\n\n* thing-example.js: use a Thing Shadow to automatically synchronize\nstate between a simulated device and a control application.\n\n* thing-passthrough-example.js: demonstrate use of a Thing Shadow with\npasthrough of standard MQTT publish and subscribe messages.\n\n* temperature-control/temperature-control.js: an interactive device simulation which uses\nThing Shadows.\n\n* [jobs-example.js](#jobsExample): receive example job execution messages and update\njob execution status.\n\n* [jobs-agent.js](#jobsAgent): example agent to handle standard operations such as reboot,\nreport system status, and shutdown. It also handles installation of files including but not\nlimited to configuration files, program updates and security certificates. It also can install\nand launch other programs and manage their executions (start, stop and restart).\n\nThe example programs use command line parameters to set options.  To see\nthe available options, run the program and specify the '-h' option as\nfollows:\n\n```sh\nnode examples/<EXAMPLE-PROGRAM> -h\n```\n**NOTE:**  You have to use the certificate created in the same region as your host end point.\nYou will also need to use unique custom endpoint with '-H' command line option when connect examples to IoT cloud.\n<a name=\"websockets\"></a>\n### WebSocket Configuration\n\nThe example programs can be configured to use a WebSocket/TLS connection to\nthe AWS IoT platform by adding '--protocol=wss' to the command line to\noverride the default setting of 'mqtts'.\n\n```sh\n  -P, --protocol=PROTOCOL          connect using PROTOCOL (mqtts|wss)\n```\n\nWhen using a WebSocket/TLS connection, you have the following options to set credentials.\n#### Export variables to system environment\n\n```sh\nexport AWS_ACCESS_KEY_ID=[a valid AWS access key ID]\nexport AWS_SECRET_ACCESS_KEY=[a valid AWS secret access key]\n```\n#### Load IAM credentials from shared credential file\nThe default shared credential file is located in `~/.aws/credentials` for Linux\nusers and `%UserProfile%\\.aws\\credentials` for Windows users. This could be\nconfigured using AWS CLI [visit the AWS CLI home page.](https://aws.amazon.com/cli/)\nAlternatively, you could provide credential file in different path or another profile by specifying in the `awsIot.device(options)` .\n\nThe values of `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` must contain valid\nAWS Identity and Access Management (IAM) credentials.  For more information about AWS\nIAM, [visit the AWS IAM home page.](https://aws.amazon.com/iam/)\n\n<a name=\"certificates\"></a>\n### Certificate Configuration\n\nWhen not configured to use a WebSocket/TLS connection, the example programs\nrequire a client certificate and private key (created using either the [AWS\nIoT Console](https://console.aws.amazon.com/iot) or the\n[AWS IoT CLI](https://aws.amazon.com/cli/)) in order to authenticate with\nAWS IoT.  Each example program uses command line options to specify the\nnames and/or locations of certificates as follows:\n\n#### Specify a directory containing default-named certificates\n\n```sh\n  -f, --certificate-dir=DIR        look in DIR for certificates\n```\n\nThe --certificate-dir (-f) option will read all certificate and key files from the\ndirectory specified.  Default certificate/key file names are as follows:\n\n* certificate.pem.crt: your AWS IoT certificate\n* private.pem.key: the private key associated with your AWS IoT certificate\n* root-CA.crt: the root CA certificate [(available from the AWS documentation here)](https://docs.aws.amazon.com/iot/latest/developerguide/server-authentication.html#server-authentication-certs)\n\n#### Specify certificate names and locations individually\n\n```sh\n  -k, --private-key=FILE           use FILE as private key\n  -c, --client-certificate=FILE    use FILE as client certificate\n  -a, --ca-certificate=FILE        use FILE as CA certificate\n```\n\nThe '-f' (certificate directory) option can be combined with these so that\nyou don't have to specify absolute pathnames for each file.\n\n<a href=\"configurationFile\"></a>\n#### Use a configuration file\n\nThe [AWS IoT Console](https://console.aws.amazon.com/iot) can generate JSON\nconfiguration data specifying the parameters required to connect a device\nto the AWS IoT Platform.  The JSON configuration data includes pathnames\nto certificates, the hostname and port number, etc...  The command line\noption '--configuration-file (-F)' is used when reading parameters from a\nconfiguration file.\n\n```sh\n  -F, --configuration-file=FILE    use FILE (JSON format) for configuration\n```\n\nThe configuration file is in JSON format, and may contain the following\nproperties:\n\n* host - the host name to connect to\n* port - the port number to use when connecting to the host (8883 for AWS IoT with client certificate)\n* clientId - the client ID to use when connecting\n* privateKey - file containing the private key\n* clientCert - file containing the client certificate\n* caCert - file containing the CA certificate\n* thingName - thing name to use\n\n##### Tips for using JSON configuration files\n* _The '-f' (certificate directory) and '-F' (configuration file) options\ncan be combined so that you don't have to use absolute pathnames in the\nconfiguration file._\n* _When using a configuration file to run any of the example programs other\nthan [echo-example.js](#echoExample), you **must** specify different client\nIDs for each process using the '-i' command line option._\n\n### device-example.js\n\ndevice-example.js is run as two processes which communicate with one\nanother via the AWS IoT platform using MQTT publish and subscribe.\nThe command line option '--test-mode (-t)' is used to set which role\neach process performs.  It's easiest to run each process in its own\nterminal window so that you can see the output generated by each.  Note\nthat in the following examples, all certificates are located in the\n~/certs directory and have the default names as specified in the\n[Certificate Configuration section](#certificates).\n\n#### _Terminal Window 1_\n```sh\nnode examples/device-example.js -f ~/certs --test-mode=1 -H <PREFIX>.iot.<REGION>.amazonaws.com\n```\n\n#### _Terminal Window 2_\n```sh\nnode examples/device-example.js -f ~/certs --test-mode=2 -H <PREFIX>.iot.<REGION>.amazonaws.com\n```\n\n### thing-example.js\nSimilar to device-example.js, thing-example.js is also run as two\nprocesses which communicate with one another via the AWS IoT platform.\nthing-example.js uses a Thing Shadow to synchronize state between the\ntwo processes, and the command line option '--test-mode (-t)' is used\nto set which role each process performs.  As with device-example.js,\nit's best to run each process in its own terminal window or on separate\nhosts.  In this example, the example programs are configured to use\nWebSocket/TLS connections to the AWS IoT platform as specified in the\n[WebSocket Configuration](#websockets).\n\n#### _Terminal Window 1_\n```sh\nnode examples/thing-example.js -P=wss --test-mode=1 -H <PREFIX>.iot.<REGION>.amazonaws.com\n```\n\n#### _Terminal Window 2_\n```sh\nnode examples/thing-example.js -P=wss --test-mode=2 -H <PREFIX>.iot.<REGION>.amazonaws.com\n```\n\n### thing-passthrough-example.js\nSimilar to thing-example.js, thing-passthrough-example.js is also run\nas two processes which communicate with one another via the AWS IoT platform.\nthing-passthrough-example.js uses a Thing Shadow to synchronize state\nfrom one process to another, and uses MQTT publish/subscribe to send\ninformation in the other direction.  The command line option '--test-mode (-t)'\nis used to set which role each process performs.  As with thing-example.js,\nit's best to run each process in its own terminal window.  Note\nthat in the following examples, all certificates are located in the\n~/certs directory and have the default names as specified in the\n[Certificate Configuration section](#certificates).\n\n#### _Terminal Window 1_\n```sh\nnode examples/thing-passthrough-example.js -f ~/certs --test-mode=1 -H <PREFIX>.iot.<REGION>.amazonaws.com\n```\n\n#### _Terminal Window 2_\n```sh\nnode examples/thing-passthrough-example.js -f ~/certs --test-mode=2 -H <PREFIX>.iot.<REGION>.amazonaws.com\n```\n\n<a name=\"echoExample\"></a>\n### echo-example.js\necho-example.js is used in conjunction with the\n[AWS IoT Console](https://console.aws.amazon.com/iot) to verify\nconnectivity with the AWS IoT platform and to perform interactive\nobservation of Thing Shadow operation.  In the following example, the\nprogram is run using the configuration file '../config.json', and\nthe certificates are located in the '~/certs' directory.  Here, the\n'-f' (certificate directory) and '-F' (configuration file) options\nare combined so that the configuration file doesn't need to contain\nabsolute pathnames.\n\n```sh\nnode examples/echo-example.js -F ../config.json -f ~/certs --thing-name testThing1\n```\n\n<a name=\"temp-control\"></a>\n### temperature-control.js\ntemperature-control.js is an interactive simulation which demonstrates\nhow Thing Shadows can be used to easily synchronize applications\nand internet-connected devices.\n\nLike thing-example.js, temperature-control.js runs in two\nseparate terminal windows and is configured via command-line options;\nin the following example, all certificates are located in the ~/certs\ndirectory and have the default names as specified in the\n[Certificate Configuration section](#certificates).  The process running\nwith '--test-mode=2' simulates an internet-connected temperature control\ndevice, and the process running with '--test-mode=1' simulates a mobile\napplication which is monitoring/controlling it.  The processes may be\nrun on different hosts if desired.\n\n#### _Installing Dependencies_\ntemperature-control.js\nuses the [blessed.js](https://github.com/chjj/blessed) and [blessed-contrib.js](https://github.com/yaronn/blessed-contrib) libraries to provide an\ninteractive terminal interface; it looks best on an 80x25 terminal with a\nblack background and white or green text and requires UTF-8 character\nencoding.  You'll need to install these libraries in the examples/temperature-control\ndirectory as follows:\n\n```sh\ncd examples/temperature-control\nnpm install\n```\n\n#### _Running the Simulation - Terminal Window 1_\n```sh\nnode examples/temperature-control/temperature-control.js -f ~/certs --test-mode=1 -H <PREFIX>.iot.<REGION>.amazonaws.com\n```\n![temperature-control.js, 'mobile application' mode](https://s3.amazonaws.com/aws-iot-device-sdk-js-supplemental/images/temperature-control-mobile-app-mode.png)\n\n#### _Running the Simulation - Terminal Window 2_\n```sh\nnode examples/temperature-control/temperature-control.js -f ~/certs --test-mode=2 -H <PREFIX>.iot.<REGION>.amazonaws.com\n```\n![temperature-control.js, 'device' mode](https://s3.amazonaws.com/aws-iot-device-sdk-js-supplemental/images/temperature-control-device-mode.png)\n\n#### _Using the simulation_\nThe simulated temperature control device has two controls; _Setpoint_ and\n_Status_.  _Status_ controls whether or not the device is active, and\n_Setpoint_ controls the interior temperature the device will attempt to\nachieve.  In addition, the device reports the current interior and exterior\ntemperatures as well as its operating state (_heating_, _cooling_, or\n_stopped_).\n\nTwo Thing Shadows are used to connect the simulated device and mobile\napplication; one contains the controls and the other contains the\nmeasured temperatures and operating state.  Both processes can update the\ncontrols, but only the device can update the measured temperatures and\nthe operating state.\n\nControlling the simulation is done using the <kbd>up</kbd>,\n<kbd>down</kbd>, <kbd>left</kbd>, <kbd>right</kbd>, and\n<kbd>Enter</kbd> keys as follows:\n\n* <kbd>up</kbd> increase the Setpoint\n* <kbd>down</kbd> decrease the Setpoint\n* <kbd>left</kbd> move left on the menu bar\n* <kbd>right</kbd> move right on the menu bar\n* <kbd>Enter</kbd> select the current menu option\n\n##### Operating State\n\nThe operating state of the device is indicated by the color of the\nInterior temperature field as follows:\n\n* Red: _heating_\n* Cyan: _cooling_\n* White: _stopped_\n\nThe following example shows the temperature control simulation in 'device' mode\nwhile the operating state is 'heating'.\n\n![temperature-control.js, 'device' mode, 'heating' operating state](https://s3.amazonaws.com/aws-iot-device-sdk-js-supplemental/images/temperature-control-device-mode-heating.png)\n\n##### Log\n\nThe log window displays events of interest, e.g. network connectivity,\n_Status_ toggles, re-synchronization with the Thing Shadow, etc...\n\n##### Menu Options\n\n* Mode: Toggle the device _Status_.  _Status_ can be controlled from both\nthe simulated device and the mobile application.\n* Network: Toggle the network connectivity of the device or mobile\napplication; this can be used to observe how both sides re-synchronize\nwhen connectivity is restored.\n\nIn this example, the mobile application is disconnected from the network.  Although it has\nrequested that the Setpoint be lowered to 58 degrees, the command can't be sent to\nthe device as there is no network connectivity, so the operating state still shows as\n'stopped'.  When the mobile application is reconnected to the network, it will attempt\nto update the Thing Shadow for the device's controls; if no control changes have been\nmade on the device side during the disconnection period, the device will synchronize to\nthe mobile application's requested state; otherwise, the mobile application will re-\nsynchronize to the device's current state.\n\n![temperature-control.js, 'mobile application' mode, network disconnected](https://s3.amazonaws.com/aws-iot-device-sdk-js-supplemental/images/temperature-control-mobile-app-mode-network-disconnected.png)\n\n##### Exiting the Simulation\n\nThe simulation can be exited at any time by pressing <kbd>q</kbd>,\n<kbd>Ctrl</kbd>+<kbd>c</kbd>, or by selecting 'exit' on the menu bar.\n\n<a name=\"jobsExample\"></a>\n### jobs-example.js\njobs-example.js, like the [echo-example.js](#echoExample) can receive messages via the [AWS IoT Console](https://console.aws.amazon.com/iot)\nto verify connectivity with the AWS IoT platform. But it can also receive and process job\nexecutions initiated through the AWS IoT device jobs management platform. See the AWS IoT\nJobs documentation [here](https://aws.amazon.com/documentation/iot/) for more information on creating and deploying jobs.\n\n#### _Running the jobs-example_\n```sh\nnode examples/jobs-example.js -f ~/certs -H <PREFIX>.iot.<REGION>.amazonaws.com -T thingName\n```\n\n\n<a name=\"jobsAgent\"></a>\n### jobs-agent.js\njobs-agent.js can be run on a device as-is or it can be modified to suit specific use cases.\nExample job documents are provided below. For more information see the AWS IoT connected device\nmanagement documentation [here](https://aws.amazon.com/documentation/iot/).\n#### _Running the jobs-agent_\n```sh\nnode examples/jobs-agent.js -f ~/certs -H <PREFIX>.iot.<REGION>.amazonaws.com -T agentThingName\n```\n#### _Using the jobs-agent_\n##### systemStatus operation\nThe jobs-agent will respond to the AWS IoT jobs management platform with system status\ninformation when it receives a job execution notification with a job document that looks like this:\n```\n {\n  \"operation\": \"systemStatus\"\n }\n```\n##### reboot operation\nWhen the jobs-agent receives a reboot job document it will attempt to reboot the device it is\nrunning on while sending updates on its progress to the AWS IoT jobs management platform.\nAfter the reboot the job execution status will be marked as IN_PROGRESS until the jobs-agent\nis also restarted at which point the status will be updated to SUCCESS. To avoid manual steps\nduring reboot it is suggested that device be configured to automatically start the jobs-agent\nat device startup time. Job document format:\n```\n {\n  \"operation\": \"reboot\"\n }\n```\n##### shutdown operation\nWhen the jobs-agent receives a shutdown job document it will attempt to shutdown the device.\n```\n {\n  \"operation\": \"shutdown\"\n }\n```\n##### install operation\nWhen the jobs-agent receives an install job document it will attempt to install the files specified\nin the job document. An install job document should follow this general format.\n```\n {\n  \"operation\": \"install\",\n  \"packageName\": \"uniquePackageName\",\n  \"workingDirectory\": \"../jobs-example-directory\",\n  \"launchCommand\": \"node jobs-example.js -f ~/certs -H <PREFIX>.iot.<REGION>.amazonaws.com -T thingName\",\n  \"autoStart\": \"true\",\n  \"files\": [\n    {\n      \"fileName\": \"jobs-example.js\",\n      \"fileVersion\": \"1.0.2.10\",\n      \"fileSource\": {\n        \"url\": \"https://some-bucket.s3.amazonaws.com/jobs-example.js\"\n      },\n      \"checksum\": {\n        \"inline\": {\n          \"value\": \"9569257356cfc5c7b2b849e5f58b5d287f183e08627743498d9bd52801a2fbe4\"\n        },\n        \"hashAlgorithm\": \"SHA256\"\n      }\n    },\n    {\n      \"fileName\": \"config.json\",\n      \"fileSource\": {\n        \"url\": \"https://some-bucket.s3.amazonaws.com/config.json\"\n      }\n    }\n  ]\n}\n```\n* `packageName`: Each install operation must have a unique package name. If the packageName\nmatches a previous install operation then the new install operation overwrites the previous one.\n* `workingDirectory`: Optional property for working directory\n* `launchCommand`: Optional property for launching an application/package. If omitted copy files only.\n* `autoStart`: If set to true then agent will execute launch command when agent starts up.\n* `files`: Specifies files to be installed\n  * `fileName`: Name of file as written to file system\n  * `fileSource.url`: Location of file to be downloaded from\n  * `checksum`: Optional file checksum\n    * `inline.value`: Checksum value\n    * `hashAlgorithm`: Checksum hash algorithm used\n##### start operation\nWhen the jobs-agent receives a start job document it will attempt to startup the specified package.\n```\n {\n  \"operation\": \"start\",\n  \"packageName\": \"somePackageName\"\n }\n```\n##### stop operation\nWhen the jobs-agent receives a stop job document it will attempt to stop the specified package.\n```\n {\n  \"operation\": \"stop\",\n  \"packageName\": \"somePackageName\"\n }\n```\n##### restart operation\nWhen the jobs-agent receives a restart job document it will attempt to restart the specified package.\n```\n {\n  \"operation\": \"restart\",\n  \"packageName\": \"somePackageName\"\n }\n```\n\n<a name=\"browser\"></a>\n## Browser Applications\nThis SDK can be packaged to run in a browser using [browserify](http://browserify.org/) or [webpack](https://webpack.js.org/), and includes helper scripts and example application code to help you get started writing browser applications that use AWS IoT.\n\n### Background\nBrowser applications connect to AWS IoT using [MQTT over the Secure WebSocket Protocol](http://docs.aws.amazon.com/iot/latest/developerguide/protocols.html).  There are some important differences between Node.js and browser environments, so a few adjustments are necessary when using this SDK in a browser application.\n\nWhen running in a browser environment, the SDK doesn't have access to the filesystem or process environment variables, so these can't be used to store credentials.  While it might be possible for an application to prompt the user for IAM credentials, the [Amazon Cognito Identity Service](https://aws.amazon.com/cognito/) provides a more user-friendly way to retrieve credentials which can be used to access AWS IoT.  The [temperature-monitor](#temperature-monitor-browser-example) browser example application illustrates this use case.\n\n### Using SDK with browserify\n#### Installing browserify\nThis SDK could also work with web applications using `browserify`. First, you'll need to make sure that `browserify` is installed.  The following instructions and the scripts in this package assume that it is installed globally, as with:\n\n```sh\nnpm install -g browserify\n```\n\n#### Browser Application Utility\nThis SDK includes a utility script called `scripts/browserize.sh`.  This script can create a browser bundle containing both the [AWS SDK for JavaScript](https://aws.amazon.com/sdk-for-browser/) and this SDK, or you can use it to create application bundles for browser applications,   like the ones under the `examples/browser` directory.  For Windows user who does not want to use bash shell, the SDK also includes batch file `windows-browserize.bat` which does the same job as `browserize.sh` but able to run in Windows CMD. To create the combined AWS SDK browser    bundle, run this command in the SDK's top-level directory:\n\n```sh\nnpm run-script browserize\n```\nThis command will create a browser bundle in `browser/aws-iot-sdk-browser-bundle.js`.  The browser bundle makes both the `aws-sdk` and `aws-iot-device-sdk` modules available so that you can `require` them from your browserified application bundle.\n\n**NOTE**: For Windows user who running scripts in CMD, since batch script file does not work well with NPM package script, Windows user could just call script directly to replace `npm run-script browserize`. This also applies for example applications demonstrated below.\n\n```sh\n.\\scripts\\windows-browserize.bat\n```\n\n#### Creating Application Bundles\nYou can also use the `scripts/browserize.sh` script to browserify your own applications and use them with the AWS SDK browser bundle.  For example, to prepare the [temperature-monitor](#temperature-monitor-browser-example) browser example application for use, run this command in the SDK's top-level directory:\n\n```sh\nnpm run-script browserize examples/browser/temperature-monitor/index.js\n```\n\nThis command does two things.  First, it creates an application bundle from `examples/browser/temperature-monitor/index.js` and places it in `examples/browser/temperature-monitor/bundle.js`.  Second, it copies the `browser/aws-iot-sdk-browser-bundle.js` into your application's directory where it can be used, e.g.:\n\n```html\n<script src=\"aws-iot-sdk-browser-bundle.js\"></script>\n<script src=\"bundle.js\"></script>\n```\n\n<a name=\"temperature-monitor-browser-example\"></a>\n#### Temperature Monitor Browser Example Application\nThis SDK includes a companion browser application to the [Temperature Control Example Application](#temp-control).  The browser application allows you to monitor the status of the simulated temperature control device.\n\n1. Follow the instructions to install the [Temperature Control Example Application](#temp-control)\n\n1. In order for the browser application to be able to authenticate and connect to AWS IoT, you'll need to configure a Cognito Identity Pool.  In the [Amazon Cognito console](https://console.aws.amazon.com/cognito/), use Amazon Cognito to create a new identity pool, and allow unauthenticated identities to connect.  Obtain the `PoolID` constant. Make sure that the policy attached to the [unauthenticated role](https://console.aws.amazon.com/iam/home?#roles) has permissions to access the required AWS IoT APIs.  More information about AWS IAM roles and policies can be found [here](http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html).\n\n1. Edit `examples/browser/temperature-monitor/aws-configuration.js`, and replace the values of `poolId` and `region` with strings containing the ID of the Cognito Identity Pool and your AWS region (e.g., `'us-east-1'`) from the previous step.\n\n1. Create the application browser bundle by executing the following command in the top-level directory of the SDK:\n\n    ```sh\n    npm run-script browserize examples/browser/temperature-monitor/index.js\n    ```\n1. Start an instance of the device simulation using:\n\n    ```sh\n    node examples/temperature-control/temperature-control.js -f ~/certs --test-mode=2 -H <PREFIX>.iot.<REGION>.amazonaws.com\n    ```\n  _NOTE_: _Although the above example shows connecting using a certificate/private key set, you can use any of the command line options described in the [Example Programs Section](#programs)._\n\n1. Open `examples/browser/temperature-monitor/index.html` in your web browser.  It should connect to AWS IoT and began displaying the status of the simulated temperature control device you started in the previous step.  If you change the device's settings, the browser window should update and display the latest status values.\n\n<a name=\"lifecycle-event-monitor-browser-example\"></a>\n#### Lifecycle Event Monitor Browser Example Application\nThis SDK includes a browser application which demonstrates the functionality of [AWS IoT lifecycle events](http://docs.aws.amazon.com/iot/latest/developerguide/life-cycle-events.html).  AWS IoT generates lifecycle events whenever clients connect or disconnect; applications can monitor these and take action when clients connect or disconnect from AWS IoT.  Follow these instructions to run the application:\n\n1. In order for the browser application to be able to authenticate and connect to AWS IoT, you'll need to configure a Cognito Identity Pool.  In the [Amazon Cognito console](https://console.aws.amazon.com/cognito/), use Amazon Cognito to create a new identity pool, and allow unauthenticated identities to connect.  Obtain the `PoolID` constant. Make sure that the policy attached to the [unauthenticated role](https://console.aws.amazon.com/iam/home?#roles) has permissions to access the required AWS IoT APIs.  More information about AWS IAM roles and policies can be found [here](http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html).\n\n1. Edit `examples/browser/lifecycle/aws-configuration.js`, and replace the values of `poolId` and `region` with strings containing the ID of the Cognito Identity Pool and your AWS region (e.g., `'us-east-1'`) from the previous step.\n1. Create the application browser bundle by executing the following command in the top-level directory of the SDK:\n\n    ```sh\n    npm run-script browserize examples/browser/lifecycle/index.js\n    ```\n\n1. Open `examples/browser/lifecycle/index.html` in your web browser.  After connecting to AWS IoT, it should display 'connected clients'.\n1. Start programs which connect to AWS IoT (e.g., [the example programs in this package](#programs)).  Make sure that these programs are connecting to the same AWS region that your Cognito Identity Pool was created in.  The browser application will display a green box containing the client ID of each client which connects; when the client disconnects, the box will disappear.\n1. If a DynamoDB table named `LifecycleEvents` exists in your account and has a primary key named `clientId`, the lifecycle event browser monitor browser application will display the client ID contained in each row.  By updating this table using an [AWS IoT rule](http://docs.aws.amazon.com/iot/latest/developerguide/iot-rules.html) triggered by [lifecycle events](http://docs.aws.amazon.com/iot/latest/developerguide/life-cycle-events.html), you can maintain a persistent list of all of the currently connected clients within your account.\n\n<a name=\"mqtt-explorer-browser-example\"></a>\n#### MQTT Explorer Browser Example Application\nThis SDK includes a browser application which implements a simple interactive MQTT client.  You can use this application to subscribe to a topic and view the messages that arrive on it, or to publish to a topic.  Follow these instructions to run the application:\n\n1. In order for the browser application to be able to authenticate and connect to AWS IoT, you'll need to configure a Cognito Identity Pool.  In the [Amazon Cognito console](https://console.aws.amazon.com/cognito/), use Amazon Cognito to create a new identity pool, and allow unauthenticated identities to connect.  Obtain the `PoolID` constant. Make sure that the policy attached to the [unauthenticated role](https://console.aws.amazon.com/iam/home?#roles) has permissions to access the required AWS IoT APIs.  More information about AWS IAM roles and policies can be found [here](http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html).\n\n1. Edit `examples/browser/mqtt-explorer/aws-configuration.js`, and replace the values of `poolId` and `region` with strings containing the ID of the Cognito Identity Pool and your AWS region (e.g., `'us-east-1'`) from the previous step.\n1. Create the application browser bundle by executing the following command in the top-level directory of the SDK:\n\n    ```sh\n    npm run-script browserize examples/browser/mqtt-explorer/index.js\n    ```\n\n1. Open `examples/browser/mqtt-explorer/index.html` in your web browser.  After connecting to AWS IoT, it should display input fields allowing you to subscribe or publish to a topic.  By subscribing to '#', for example, you will be able to monitor all traffic within your AWS account as allowed by the policy associated with the unauthenticated role of your Cognito Identity Pool.\n\n#### Reducing Browser Bundle Size\nAfter your application development is complete, you will probably want to reduce the size of the browser bundle.  There are a couple of easy techniques to do this, and by combining both of them you can create much smaller browser bundles.\n\n##### Eliminate unused features from the AWS SDK\n\n1. The [AWS SDK for JavaScript](https://github.com/aws/aws-sdk-js) allows you to install only the features you use in your application.  In order to use this feature when preparing a browser bundle, first you'll need to remove any existing bundle that you've already created:\n\n    ```sh\n    rm browser/aws-iot-sdk-browser-bundle.js\n    ```\n\n2. Define the AWS features your application uses as a comma-separated list in the `AWS_SERVICES` environment variable.  For example, the [MQTT Explorer example](#mqtt-explorer-browser-example) uses only AWS Cognito Identity, so to create a bundle containing only this feature, do:\n\n    ```sh\n    export AWS_SERVICES=cognitoidentity\n    ```\n    For a list of the AWS SDK feature names, refer to the [_features subdirectory_ of the AWS SDK for JavaScript](https://github.com/aws/aws-sdk-js/tree/master/features).  As another example, if your application uses Cognito Identity, DynamoDB, S3, and SQS, you would do:\n\n    ```sh\n    export AWS_SERVICES=cognitoidentity,dynamodb,s3,sqs\n    ```\n\n3. Create the browser app and bundle, e.g. for the [MQTT Explorer example](#mqtt-explorer-browser-example), do:\n\n    ```sh\n    npm run-script browserize examples/browser/mqtt-explorer/index.js\n    ```\n\n#### Uglify the bundle source\n\n[Uglify](https://www.npmjs.com/package/uglify) is an npm utility for minimizing the size of JavaScript source files.  To use it, first install it as a global npm package:\n\n```sh\nnpm install -g uglify\n```\n\nOnce installed, you can use it to reduce the bundle size:\n\n```sh\nuglify -s ./browser/aws-iot-sdk-browser-bundle.js -o ./browser/aws-iot-sdk-browser-bundle-min.js\n```\nAfter you've created the minimized bundle, you'll need to make sure that your application loads this version rather than the non-minimized version, e.g:\n\n```html\n<script src=\"aws-iot-sdk-browser-bundle-min.js\"></script>\n```\n##### Optimization results\nBy using both of the above techniques for the [MQTT Explorer example](#mqtt-explorer-browser-example), the bundle size can be reduced from 2.4MB to 615KB.\n\n<a name=\"troubleshooting\"></a>\n\n### Using SDK with webpack\nIn order to work with webpack, you have to create a webpack package. You can put your file dependencies in `entry.js` and output it as `bundle.js`. An example is provided in the location `./examples/browser/mqtt-webpack`\n\n```sh\ncd ./examples/browser/mqtt-webpack\nnpm install\n./node_modules/.bin/webpack --config webpack.config.js\n```\nThe `index.html` will load the output file `bundle.js` and execute functions defined in `entry.js`. This duplicates the example of mqtt-explore above which loaded SDK into web browser using browserify.\n\n## Troubleshooting\n\nIf you have problems connecting to the AWS IoT Platform when using this SDK or\nrunning the example programs, there are a few things to check:\n\n* _Region Mismatch_:  You have to use the certificate created in the same\nregion as your host end point.\n* _Duplicate Client IDs_:  Within your AWS account, the AWS IoT platform\nwill only allow one connection per client ID.  Many of the example programs\nrun as two processes which communicate with one another.  If you don't\nspecify a client ID, the example programs will generate random client IDs,\nbut if you are using a [JSON configuration file](#configurationFile), you'll\nneed to explictly specify client IDs for both programs using the '-i' command\nline option.\n* _Invalid NPM Version_:  To run the browserize.sh script which prepares the browser example applications, you'll need to use npm version 3.  This is because browserize.sh expects package dependencies to be handled using the npm version 3 strategy, which is [different than the strategy used in npm version 2](https://docs.npmjs.com/how-npm-works/npm3).  If you're having trouble running the browser application examples, make sure that you're using npm version 3.  You can check your npm version with `npm -v`.\n\n<a name=\"unittests\"></a>\n## Unit Tests\n\nThis package includes unit tests which can be run as follows:\n\n```sh\nnpm test\n```\n\nRunning the unit tests will also generate code coverage data in the 'reports'\ndirectory.\n\n<a name=\"license\"></a>\n## License\n\nThis SDK is distributed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0), see LICENSE.txt and NOTICE.txt for more information.\n<a name=\"suport\"></a>\n## Support\nIf you have technical questions about AWS IoT Device SDK, use the [AWS IoT Forum](https://forums.aws.amazon.com/forum.jspa?forumID=210).\nFor any other questions on AWS IoT, contact [AWS Support](https://aws.amazon.com/contact-us).\n\n", "release_dates": ["2023-10-27T16:04:47Z", "2022-03-31T18:18:21Z", "2021-07-30T16:59:32Z", "2021-07-09T21:11:01Z", "2021-07-08T22:54:06Z", "2021-05-20T18:14:10Z", "2018-02-06T18:53:40Z", "2017-10-05T18:19:20Z", "2017-06-22T22:51:28Z", "2016-12-08T22:27:25Z", "2016-11-01T20:08:16Z", "2016-04-19T23:24:15Z", "2016-03-04T23:41:57Z", "2016-01-29T00:33:00Z"]}, {"name": "aws-iot-device-sdk-js-v2", "description": "Next generation AWS IoT Client SDK for Node.js using the AWS Common Runtime", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS IoT Device SDK for JavaScript v2\n\nThis document provides information about the AWS IoT device SDK for Javascript V2. This SDK is built on the [AWS Common Runtime](https://docs.aws.amazon.com/sdkref/latest/guide/common-runtime.html)\n\n*__Jump To:__*\n* [Installation](#installation)\n* [Samples](https://github.com/aws/aws-iot-device-sdk-js-v2/tree/main/samples)\n* [Getting Help](#getting-help)\n* [FAQ](https://github.com/aws/aws-iot-device-sdk-js-v2/blob/main/documents/FAQ.md)\n* [API Docs](https://aws.github.io/aws-iot-device-sdk-js-v2/)\n* [MQTT5 User Guide](https://github.com/awslabs/aws-crt-nodejs/blob/main/MQTT5-UserGuide.md)\n* [Migration Guide from the AWS IoT SDK for JavaScript v1](./documents/MIGRATION_GUIDE.md)\n\n## Installation\n\n### Minimum Requirements\n\nFor use with Node, the following are required:\n* Node v14+\n  * Run `node -v` to check Node version.\n* CMake 3.1+\n\n[Step-by-step instructions](https://github.com/aws/aws-iot-device-sdk-js-v2/blob/main/documents/PREREQUISITES.md)\n\n### Build SDK in existing project with NPM\n``` sh\n# Navigate to the Javascript project you want to add the\n# Javascript V2 SDK to.\ncd <your javascript project here>\n# Install the V2 SDK.\nnpm install aws-iot-device-sdk-v2\n# Now you can use the Javascript V2 SDK in your project.\n```\n\n### Build the V2 SDK from source\n\n``` sh\n# Create a workspace directory to hold all the SDK files.\nmkdir sdk-workspace\ncd sdk-workspace\n\n# Clone the repository to access the samples.\ngit clone https://github.com/aws/aws-iot-device-sdk-js-v2.git\n\n# Install the SDK.\ncd aws-iot-device-sdk-js-v2\nnpm install\n\n# Then you can run the samples following the instructions in the samples README.\n```\n\n## Samples\n\n[Samples README](https://github.com/aws/aws-iot-device-sdk-js-v2/blob/main/samples/README.md)\n\n## Getting Help\n\nThe best way to interact with our team is through GitHub. You can open a [discussion](https://github.com/aws/aws-iot-device-sdk-js-v2/discussions) for guidance questions or an [issue](https://github.com/aws/aws-iot-device-sdk-js-v2/issues/new/choose) for bug reports, or feature requests. You may also find help on community resources such as [StackOverFlow](https://stackoverflow.com/questions/tagged/aws-iot) with the tag [#aws-iot](https://stackoverflow.com/questions/tagged/aws-iot) or if you have a support plan with [AWS Support](https://aws.amazon.com/premiumsupport/), you can also create a new support case.\n\nPlease make sure to check out our resources too before opening an issue:\n\n*  [FAQ](https://github.com/aws/aws-iot-device-sdk-js-v2/blob/main/documents/FAQ.md)\n* [API Docs](https://aws.github.io/aws-iot-device-sdk-js-v2/)\n* [IoT Guide](https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html) ([source](https://github.com/awsdocs/aws-iot-docs))\n* [MQTT5 User Guide](https://github.com/awslabs/aws-crt-nodejs/blob/main/MQTT5-UserGuide.md)\n* Check for similar [Issues](https://github.com/aws/aws-iot-device-sdk-js-v2/issues)\n* [AWS IoT Core Documentation](https://docs.aws.amazon.com/iot/)\n* [Dev Blog](https://aws.amazon.com/blogs/?awsf.blog-master-iot=category-internet-of-things%23amazon-freertos%7Ccategory-internet-of-things%23aws-greengrass%7Ccategory-internet-of-things%23aws-iot-analytics%7Ccategory-internet-of-things%23aws-iot-button%7Ccategory-internet-of-things%23aws-iot-device-defender%7Ccategory-internet-of-things%23aws-iot-device-management%7Ccategory-internet-of-things%23aws-iot-platform)\n* Integration with AWS IoT Services such as\n[Device Shadow](https://docs.aws.amazon.com/iot/latest/developerguide/iot-device-shadows.html)\nand [Jobs](https://docs.aws.amazon.com/iot/latest/developerguide/iot-jobs.html)\nis provided by code that been generated from a model of the service.\n* [Contributions Guidelines](https://github.com/aws/aws-iot-device-sdk-js-v2/blob/main/documents/CONTRIBUTING.md)\n\n## License\n\nThis library is licensed under the [Apache 2.0 License](https://github.com/aws/aws-iot-device-sdk-js-v2/blob/main/documents/LICENSE).\n\nLatest released version: v1.19.1\n", "release_dates": ["2024-02-12T18:53:58Z", "2024-01-04T19:11:44Z", "2023-12-14T19:00:52Z", "2023-11-06T21:32:54Z", "2023-10-26T20:43:57Z", "2023-10-23T16:28:02Z", "2023-08-23T20:10:12Z", "2023-07-28T02:14:48Z", "2023-07-07T18:12:27Z", "2023-07-03T23:45:23Z", "2023-06-30T19:13:02Z", "2023-06-08T15:51:53Z", "2023-06-07T22:04:20Z", "2023-05-26T21:24:26Z", "2023-05-10T00:39:40Z", "2023-05-02T22:01:22Z", "2023-05-02T20:03:42Z", "2023-05-02T17:29:32Z", "2023-04-13T20:05:29Z", "2023-03-23T19:36:04Z", "2023-03-22T17:35:39Z", "2023-02-01T00:38:55Z", "2023-01-27T18:52:43Z", "2023-01-18T15:13:44Z", "2023-01-18T15:12:44Z", "2022-12-09T18:37:51Z", "2022-11-28T19:48:59Z", "2022-11-11T21:15:33Z", "2022-10-04T17:10:00Z", "2022-09-06T15:39:25Z"]}, {"name": "aws-iot-device-sdk-python", "description": "SDK for connecting to AWS IoT from a device using Python. ", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2024-02-14T21:24:34Z", "2024-02-14T19:30:54Z", "2022-04-13T21:27:34Z", "2022-03-04T20:48:59Z", "2022-01-17T16:49:38Z", "2018-06-29T00:07:41Z", "2018-01-02T21:41:06Z", "2017-09-08T17:49:23Z", "2017-06-19T21:36:29Z", "2016-11-14T23:40:00Z", "2016-10-12T22:45:52Z", "2016-08-08T23:43:02Z", "2016-06-14T21:21:22Z"]}, {"name": "aws-iot-device-sdk-python-v2", "description": "Next generation AWS IoT Client SDK for Python using the AWS Common Runtime", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS IoT Device SDK v2 for Python\n\n[![Version](https://img.shields.io/pypi/v/awsiotsdk.svg?style=flat)](https://pypi.org/project/awsiotsdk/)\n\nThis document provides information about the AWS IoT Device SDK v2 for Python. This SDK is built on the [AWS Common Runtime](https://docs.aws.amazon.com/sdkref/latest/guide/common-runtime.html)\n\n*__Jump To:__*\n* [Installation](#installation)\n* [Samples](samples)\n* [Getting Help](#getting-help)\n* [FAQ](./documents/FAQ.md)\n* [API Docs](https://aws.github.io/aws-iot-device-sdk-python-v2/)\n* [MQTT5 User Guide](./documents/MQTT5_Userguide.md)\n* [Migration Guide from the AWS IoT SDK for Python v1](./documents/MIGRATION_GUIDE.md)\n\n\n## Installation\n\n### Minimum Requirements\n* Python 3.7+\n\n[Step-by-step instructions](./documents/PREREQUISITES.md)\n\n### Install from PyPI\n\n#### MacOS and Linux:\n\n```\npython3 -m pip install awsiotsdk\n```\n\n#### Windows:\n\n```\npython -m pip install awsiotsdk\n```\n\n### Install from source\n\n```bash\n# 1. Create a workspace directory to hold all the SDK files\nmkdir sdk-workspace\ncd sdk-workspace\n\n# 2. Clone the repository. You could select the version of the SDK you desire to use.\ngit clone -b <SDK_VERSION> https://github.com/aws/aws-iot-device-sdk-python-v2.git\n\n# 3. (Optional) Setup the version number of your local build. The default version \n#    for awsiotsdk is set to \"1.0.0-dev\", you can set the version number of the\n#    local build in \"aws-iot-device-sdk-python-v2/awsiot/__init__.py\"\nsed -i \"s/__version__ = '1.0.0-dev'/__version__ = '<SDK_VERSION>'/\" \\\n  aws-iot-device-sdk-python-v2/awsiot/__init__.py\n\n# 4. Install using Pip (use 'python' instead of 'python3' on Windows)\npython3 -m pip install ./aws-iot-device-sdk-python-v2\n```\n\n## Samples\n\n[Samples README](samples)\n\n## Getting Help\n\nThe best way to interact with our team is through GitHub. You can open a [discussion](https://github.com/aws/aws-iot-device-sdk-python-v2/discussions) for guidance questions or an [issue](https://github.com/aws/aws-iot-device-sdk-python-v2/issues/new/choose) for bug reports, or feature requests. You may also find help on community resources such as [StackOverFlow](https://stackoverflow.com/questions/tagged/aws-iot) with the tag [#aws-iot](https://stackoverflow.com/questions/tagged/aws-iot) or if you have a support plan with [AWS Support](https://aws.amazon.com/premiumsupport/), you can also create a new support case.\n\nPlease make sure to check out our resources too before opening an issue:\n\n* [FAQ](./documents/FAQ.md)\n* [API Docs](https://aws.github.io/aws-iot-device-sdk-python-v2/)\n* [IoT Guide](https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html) ([source](https://github.com/awsdocs/aws-iot-docs))\n* Check for similar [Issues](https://github.com/aws/aws-iot-device-sdk-python-v2/issues)\n* [AWS IoT Core Documentation](https://docs.aws.amazon.com/iot/)\n* [Dev Blog](https://aws.amazon.com/blogs/?awsf.blog-master-iot=category-internet-of-things%23amazon-freertos%7Ccategory-internet-of-things%23aws-greengrass%7Ccategory-internet-of-things%23aws-iot-analytics%7Ccategory-internet-of-things%23aws-iot-button%7Ccategory-internet-of-things%23aws-iot-device-defender%7Ccategory-internet-of-things%23aws-iot-device-management%7Ccategory-internet-of-things%23aws-iot-platform)\n* Integration with AWS IoT Services such as\n[Device Shadow](https://docs.aws.amazon.com/iot/latest/developerguide/iot-device-shadows.html)\nand [Jobs](https://docs.aws.amazon.com/iot/latest/developerguide/iot-jobs.html)\nis provided by code that been generated from a model of the service.\n* [Contributions Guidelines](./documents/CONTRIBUTING.md)\n\n## License\n\nThis library is licensed under the [Apache 2.0 License](./documents/LICENSE).\n\nLatest released version: v1.21.1\n", "release_dates": ["2024-02-29T18:47:31Z", "2024-01-04T19:12:41Z", "2023-12-14T00:47:06Z", "2023-10-25T20:44:35Z", "2023-08-28T21:09:39Z", "2023-08-14T18:12:14Z", "2023-07-28T02:16:11Z", "2023-07-07T18:13:16Z", "2023-06-15T23:09:46Z", "2023-06-08T16:05:37Z", "2023-05-31T21:18:49Z", "2023-05-25T17:58:02Z", "2023-05-09T23:13:09Z", "2023-05-05T18:03:31Z", "2023-05-02T18:19:30Z", "2023-04-13T20:01:54Z", "2023-03-22T17:28:43Z", "2023-02-08T23:26:35Z", "2023-02-01T00:36:44Z", "2023-01-27T19:22:04Z", "2023-01-18T15:21:46Z", "2023-01-18T15:20:06Z", "2022-12-02T20:15:39Z", "2022-11-11T21:17:04Z", "2022-10-04T16:58:06Z", "2022-09-08T22:34:16Z", "2022-09-02T23:23:22Z", "2022-08-30T20:11:00Z", "2022-08-15T19:45:08Z", "2022-08-15T17:04:15Z"]}, {"name": "aws-iot-fleetwise-edge", "description": "Reference Implementation for AWS IoT FleetWise", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Reference Implementation for AWS IoT FleetWise\n\n:robot: AWS IoT FleetWise now supports [ROS2](https://docs.ros.org) for collecting vision system\ndata.<br> :information_source: To quickly get started, jump to the\n**[Jupyter Notebook demo](./docs/dev-guide/vision-system-data/vision-system-data-demo.ipynb)** and\ncollect camera data from a [CARLA](https://carla.org) vehicle simulation.\n\n:information_source: To quickly get started with telematics data collection, jump to the\n[Edge Agent Developer Guide](./docs/dev-guide/edge-agent-dev-guide.md), the\n[Android Guide](./tools/android-app/README.md), or the\n[Raspberry Pi Tutorial](./docs/rpi-tutorial/raspberry-pi-tutorial.md).\n\nAWS IoT FleetWise is a service that makes it easy for Automotive OEMs, Fleet operators, Independent\nSoftware vendors (ISVs) to collect, store, organize, and monitor data from vehicles at scale. The\nReference Implementation for AWS IoT FleetWise (\"FWE\") provides C++ libraries that can be run with\nsimulated vehicle data on certain supported vehicle hardware or that can help you develop an Edge\nAgent to run an application on your vehicle that integrates with AWS IoT FleetWise. You can then use\nAWS IoT FleetWise's to process the collected data, gain insights about the vehicle's health and use\nthe service's visual interface to help diagnose and troubleshoot potential issues with your\nvehicles. Furthermore, AWS IoT FleetWise's capability to collect ECU data and store them on cloud\ndatabases enables you to utilize different AWS services (Analytics Services, ML, etc.) to develop\nnovel use-cases that augment your existing vehicle functionality. In particular, AWS IoT FleetWise\ncan leverage fleet data (Big Data) and enable you to develop use cases that create business value,\nfor example: improve electric vehicle range estimation, optimized battery life charging, optimized\nvehicle routing, etc. AWS IoT FleetWise can be extended to utilize cloud computing capabilities for\nuse-cases such as helping to improve pet/child detection, Driver Monitoring System applications,\nPredictive Diagnostics, electric vehicle's battery cells outlier detection, etc. You can use the\nincluded sample C++ application to learn more about the FWE, develop an Edge Agent for your use case\nand test interactions before integration.\n\n> _**Important**_ As provided in the AWS IoT FleetWise\n> [Service Terms](https://aws.amazon.com/service-terms/), you are solely responsible for your Edge\n> Agent, including ensuring that your Edge Agent and any updates and modifications to it are\n> deployed and maintained safely and securely in any vehicles.\n\n## AWS IoT FleetWise Architecture\n\nAWS IoT FleetWise is an AWS service that enables automakers and fleet operators to collect, store,\norganize, and monitor data from vehicles. Automakers need the ability to connect remotely to their\nfleet of vehicles and collect vehicle ECU/sensor data. AWS IoT FleetWise can be used by OEM\nengineers and data scientists to build vehicle models that can be used to build custom data\ncollection schemes. These data collection schemes enables the OEM to optimize the data collection\nprocess by defining what signals to collect, how often to collect them, and most importantly the\ntrigger conditions (\"events\") that enable the collection process.\n\nCustomers can define the data collection schemes to trigger based on a schedule or on specific\nconditions such as, but not limited to: 1. Ambient temperature dropping to below 0 degree or 2.\nVehicle crosses state lines or 3. Active diagnostic trouble codes. These conditions are sent to the\nvehicle through a set of documents called data collection schemes. In summary, your Edge Agent\ncollects the data of interest according to the data collection schemes and decoding rules as\nspecified by the OEM on the [AWS IoT FleetWise Console](https://aws.amazon.com/iot-fleetwise/).\n\nThe following diagram illustrates a high-level architecture of the system.\n\n<img src=\"./docs/iot-FleetWise-architecture.png\" />\n\n**FWE receives two documents:**\n\n1. _Decoder Manifest_ - this document describes how signals are collected from the vehicle, and will\n   include details such as, but not limited to: Bus ID, network name, decoding information, etc.\n\n2. _Data Collection Schemes_ - this document describes what signals to collect. It also describes\n   the condition logic that defines the enablement of the trigger logic that allows these signals to\n   be collected, for example, when Vehicle Speed > 100 km/Hr and Driver Seatbelt is Off and Ambient\n   Temperature < 0 degree C.\n\n## FWE Deployment & Supported Platforms\n\nThe functional flexibility of FWE and its use of dynamic memory allocation means that it cannot\nreside in the real-time safety vehicle ECUs. FWE must also be connected to the internet and\npreferably has access to a \"good\" portion of vehicle ECU data. OEMs have the flexibility to decide\nwhere they can deploy their Edge Agent binary. Possible options include (if present):\n\n1. Vehicle Gateway such as the\n   [NXP S32G](https://www.nxp.com/products/processors-and-microcontrollers/arm-processors/s32g-vehicle-network-processors/s32g2-processors-for-vehicle-networking:S32G2)\n   and\n   [Renesas R-Car S4](https://www.renesas.com/jp/en/products/automotive-products/automotive-system-chips-socs/rtp8a779f0askb0sp2s-r-car-s4-reference-boardspider)\n2. Vehicle Head-Unit\n3. Vehicle's High Performance Computer\n4. Telecommunication Control Unit\n\nFWE was built and tested on 64-bit architectures. It has been tested on both ARM and X86 multicore\nbased machines, with a Linux Kernel version of 5.4 and above. The kernel module for ISO-TP\n(`can-isotp`) would need to be installed in addition for Kernels below 5.10.\n\nFWE was also tested on an EC2 Instance with the following details:\n\n- **Platform**: Ubuntu\n- **Platform Details**: Linux/UNIX\n- **Server**: AmazonEC2\n- **InstanceType**: c4.8xlarge\n- **AvailabilityZone**: us-east-1\n- **Architecture**: x86_64\n- **CpuOptions**: {'CoreCount': 18, 'ThreadsPerCore': 2}\n- **AMI name**: ubuntu-focal-20.04-amd64-server-20230112\n\n## AWS IoT FleetWise Client-Server Communication\n\nFWE depends on the [AWS SDK for C++](https://github.com/aws/aws-sdk-cpp) to send and receive data\nfrom and to AWS IoT FleetWise Server. All data sent to the AWS IoT FleetWise server is sent over an\nencrypted\n[TLS connection](https://docs.aws.amazon.com/iot/latest/developerguide/data-encryption.html) using\nMQTT, which is designed to make it secure by default while in transit. FWE uses MQTT quality of\nservice zero (QoS = 0).\n\n## Security\n\nSee [SECURITY](./SECURITY.md) for more information\n\n## License Summary and Build Dependencies\n\nFWE depends on the following open source libraries. Refer to the corresponding links for more\ninformation.\n\n- [AWS SDK for C++: v1.11.177](https://github.com/aws/aws-sdk-cpp)\n  - [Curl: v7.58.0](https://github.com/curl/curl)\n  - [OpenSSL: v1.1.1](https://github.com/openssl/openssl)\n  - [zlib: v1.2.11](https://github.com/madler/zlib)\n- [GoogleTest: v1.10.0](https://github.com/google/googletest)\n- [Google Benchmark: v1.6.1](https://github.com/google/benchmark)\n- [Protobuf: v3.21.12](https://github.com/protocolbuffers/protobuf)\n- [Boost: v1.78.0](https://github.com/boostorg/boost)\n- [JsonCpp: v1.9.5](https://github.com/open-source-parsers/jsoncpp)\n- [Snappy: v1.1.8](https://github.com/google/snappy)\n\nOptional: The following dependencies are only required when the option `FWE_FEATURE_GREENGRASSV2` is\nenabled.\n\n- [AWS IoT Device SDK for C++ v2: v1.30.0](https://github.com/aws/aws-iot-device-sdk-cpp-v2)\n\nOptional: The following dependencies are only required when the option\n`FWE_FEATURE_VISION_SYSTEM_DATA` is enabled.\n\n- [Amazon Ion: v1.1.2](https://github.com/amazon-ion/ion-c)\n\nOptional: The following dependencies are only required when the option `FWE_FEATURE_ROS2` is\nenabled.\n\n- [ROS2: Galactic](https://github.com/ros2/rclcpp)\n- [Cyclone DDS: 0.8.0](https://github.com/eclipse-cyclonedds/cyclonedds)\n- [Fast-CDR: v1.0.21](https://github.com/eProsima/Fast-CDR)\n\nSee [LICENSE](./LICENSE) for more information.\n\n## Getting Help\n\n[Contact AWS Support](https://aws.amazon.com/contact-us/) if you have any technical questions about\nFWE.\n\n## Metrics\n\nSee [Metrics](./docs/metrics.md) for details, which Edge specific metrics exist and how they can be\naccessed.\n\n## Resources\n\nThe following documents provide more information about FWE.\n\n1. [Change Log](./CHANGELOG.md) provides a summary of feature enhancements, updates, and resolved\n   and known issues.\n1. [Offboarding and Data Deletion](./docs/AWS-IoTFleetWiseOffboarding.md) provides a summary of the\n   steps needed on the client side to offboard from the service.\n1. [Edge Agent Developer Guide](./docs/dev-guide/edge-agent-dev-guide.md) provides step-by-step\n   instructions for building and running your Edge Agent.\n\nThe following documents provide more information about the cloud component of AWS IoT FleetWise.\n\n1. [AWS IoT FleetWise API Reference](https://docs.aws.amazon.com/iot-fleetwise/latest/APIReference/Welcome.html)\n   describes all the API operations for FleetWise\n", "release_dates": ["2024-02-15T13:15:25Z", "2023-11-27T05:01:07Z", "2023-09-25T12:42:10Z", "2023-08-02T15:00:44Z", "2023-06-12T12:47:55Z", "2023-05-11T16:37:19Z", "2023-03-06T09:42:16Z", "2023-01-11T10:07:53Z", "2022-11-28T12:48:32Z", "2022-11-03T23:20:02Z", "2022-09-27T14:27:27Z", "2022-08-30T18:29:02Z", "2022-08-03T17:57:51Z", "2022-02-24T19:59:27Z", "2022-02-24T16:16:13Z", "2022-02-08T16:06:41Z"]}, {"name": "aws-js-sns-message-validator", "description": null, "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon SNS Message Validator for JavaScript\n\n[![@awsforjs on Twitter](http://img.shields.io/badge/twitter-%40awsforjs-blue.svg?style=flat)](https://twitter.com/awsforjs)\n[![Build Status](https://img.shields.io/travis/aws/aws-js-sns-message-validator.svg?style=flat)](https://travis-ci.org/aws/aws-js-sns-message-validator)\n[![Apache 2 License](https://img.shields.io/github/license/aws/aws-js-sns-message-validator.svg?style=flat)](http://aws.amazon.com/apache-2-0/)\n\nThe **Amazon SNS Message Validator for Node.js** library allows you to validate\nthat incoming HTTP(S) POST messages are valid Amazon SNS notifications. This\nlibrary is standalone and does not depend on the AWS SDK for JavaScript.\n\n## Installation\n\nThe npm module's name is [`sns-validator`](https://www.npmjs.com/package/sns-validator). Install with npm or yarn:\n\n```\nnpm i sns-validator\n```\n\nor \n\n```\nyarn add sns-validator\n```\n\n## Basic Usage\n\nTo validate a message, you can instantiate a `MessageValidator` object and pass\nan SNS message and a callback to its `validate` method. The message should be\nthe result of calling `JSON.parse` on the body of the HTTP(S) message sent by\nSNS to your endpoint. The callback should take two arguments, the first being\nan error and the second being the successfully validated SNS message.\n\nThe message validator checks the `SigningCertURL`, `SignatureVersion`, and\n`Signature` to make sure they are valid and consistent with the message data.\n\n```javascript\nvar MessageValidator = require('sns-validator'),\n    validator = new MessageValidator();\n\nvalidator.validate(message, function (err, message) {\n    if (err) {\n        // Your message could not be validated.\n        return;\n    }\n\n    // message has been validated and its signature checked.\n});\n```\n\n## Installation\n\nThe SNS Message Validator relies on the Node crypto module and is only designed\nto work on a server, not in a browser. The validation performed is only\nnecessary when subscribing HTTP(S)\n\n## About Amazon SNS\n\n[Amazon Simple Notification Service (Amazon SNS)][sns] is a fast, fully-managed,\npush messaging service. Amazon SNS can deliver messages to email, mobile devices\n(i.e., SMS; iOS, Android and FireOS push notifications), Amazon SQS queues,and\n\u2014 of course \u2014 HTTP/HTTPS endpoints.\n\nWith Amazon SNS, you can setup topics to publish custom messages to subscribed\nendpoints. However, SNS messages are used by many of the other AWS services to\ncommunicate information asynchronously about your AWS resources. Some examples\ninclude:\n\n* Configuring Amazon Glacier to notify you when a retrieval job is complete.\n* Configuring AWS CloudTrail to notify you when a new log file has been written.\n* Configuring Amazon Elastic Transcoder to notify you when a transcoding job\n  changes status (e.g., from \"Progressing\" to \"Complete\")\n\nThough you can certainly subscribe your email address to receive SNS messages\nfrom service events like these, your inbox would fill up rather quickly. There\nis great power, however, in being able to subscribe an HTTP/HTTPS endpoint to\nreceive the messages. This allows you to program webhooks for your applications\nto easily respond to various events.\n\n## Handling Messages\n\n### Confirming a Subscription to a Topic\n\nIn order to handle a `SubscriptionConfirmation` message, you must use the\n`SubscribeURL` value in the incoming message:\n\n```javascript\nvar https = require('https'),\n    MessageValidator = require('sns-validator'),\n    validator = new MessageValidator();\n\nvalidator.validate(message, function (err, message) {\n    if (err) {\n        console.error(err);\n        return;\n    }\n\n    if (message['Type'] === 'SubscriptionConfirmation') {\n        https.get(message['SubscribeURL'], function (res) {\n          // You have confirmed your endpoint subscription\n        });\n    }\n});\n```\n\nIf an incoming message includes multibyte characters and its encoding is utf8,\nset the encoding to `validator`.\n\n```javascript\nvar MessageValidator = require('sns-validator'),\n    validator = new MessageValidator();\nvalidator.encoding = 'utf8';\n```\n\n### Receiving a Notification\n\nTo receive a notification, use the same code as the preceding example, but\ncheck for the `Notification` message type.\n\n```javascript\nif (message['Type'] === 'Notification') {\n    // Do whatever you want with the message body and data.\n    console.log(message['MessageId'] + ': ' + message['Message']);\n}\n```\n\nThe message body will be a string, and will hold whatever data was published\nto the SNS topic.\n\n### Unsubscribing\n\nUnsubscribing looks the same as subscribing, except the message type will be\n`UnsubscribeConfirmation`.\n\n```javascript\nif (message['Type'] === 'UnsubscribeConfirmation') {\n    // Unsubscribed in error? You can resubscribe by visiting the endpoint\n    // provided as the message's SubscribeURL field.\n    https.get(message['SubscribeURL'], function (res) {\n        // You have re-subscribed your endpoint.\n    });\n}\n```\n\n[sns]: http://aws.amazon.com/sns/\n[AWS SDK for JavaScript]: https://github.com/aws/aws-sdk-js\n", "release_dates": ["2022-08-19T17:58:34Z", "2015-10-19T22:58:53Z"]}, {"name": "aws-jupyter-proxy", "description": "A Jupyter server extension to proxy requests with AWS SigV4 authentication", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Jupyter Proxy\n\n![Build](https://github.com/aws/aws-jupyter-proxy/workflows/build/badge.svg)\n[![Version](https://img.shields.io/pypi/v/aws_jupyter_proxy.svg)](https://pypi.org/project/aws-jupyter-proxy/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\nA Jupyter server extension to proxy requests with AWS SigV4 authentication.\n\n## Overview\n\nThis server extension enables the usage of the [AWS JavaScript/TypeScript SDK](https://github.com/aws/aws-sdk-js) to write Jupyter frontend extensions without having to export AWS credentials to the browser.\n\nA single `/awsproxy` endpoint is added on the Jupyter server which receives incoming requests from the browser, uses the credentials on the server to add [SigV4](https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html) authentication to the request, and then proxies the request to the actual AWS service endpoint.\n\nAll requests are proxied back-and-forth as-is, e.g., a 4xx status code from the AWS service will be relayed back as-is to the browser.\n\nNOTE: This project is still under active development\n\n## Install\n\nInstalling the package from PyPI will install and enable the server extension on the Jupyter server.\n\n```bash\npip install aws-jupyter-proxy\n```\n\n## Usage\n\nUsing this requries no additional dependencies in the client-side code. Just use the regular AWS JavaScript/TypeScript SDK methods and add any dummy credentials and change the endpoint to the `/awsproxy` endpoint.\n\n```typescript\n    import * as AWS from 'aws-sdk';\n    import SageMaker from 'aws-sdk/clients/sagemaker';\n\n    // Reusable function to add the XSRF token header to a request\n    function addXsrfToken<D, E>(request: AWS.Request<D, E>) {\n      const cookie = document.cookie.match('\\\\b' + '_xsrf' + '=([^;]*)\\\\b');\n      const xsrfToken = cookie ? cookie[1] : undefined;\n      if (xsrfToken !== undefined) {\n        request.httpRequest.headers['X-XSRFToken'] = xsrfToken;\n      }\n    }\n\n    // These credentials are *not* used for the actual AWS service call but you have\n    // to provide any dummy credentials (Not real ones!)\n    AWS.config.secretAccessKey = 'IGNOREDIGNORE/IGNOREDIGNOREDIGNOREDIGNOR';\n    AWS.config.accessKeyId = 'IGNOREDIGNO';\n\n    // Change the endpoint in the client to the \"awsproxy\" endpoint on the Jupyter server.\n    const proxyEndpoint = 'http://localhost:8888/awsproxy';\n\n    const sageMakerClient = new SageMaker({\n        region: 'us-west-2',\n        endpoint: proxyEndpoint,\n    });\n\n    // Make the API call!\n    await sageMakerClient\n        .listNotebookInstances({\n            NameContains: 'jaipreet'\n        })\n        .on('build', addXsrfToken)\n        .promise();\n```\n\n### Usage with S3\n\nFor S3, use the `s3ForcePathStyle` parameter during the client initialization\n\n```typescript\n    import S3 from 'aws-sdk/clients/s3';\n\n    const s3Client = new S3({\n        region: 'us-west-2',\n        endpoint: proxyEndpoint,\n        s3ForcePathStyle: true,\n        s3DisableBodySigning:false // for https\n    });\n\n    await s3Client\n        .getObject({\n            Bucket: 'my-bucket',\n            Key: 'my-object'\n        })\n        .on('build', addXsrfToken)\n        .promise();\n```\n\n### Whitelisting\n\nOn the server, the `AWS_JUPYTER_PROXY_WHITELISTED_SERVICES` environment variable can be used to whitelist the set of services allowed to be proxied through. This is opt-in - Not specifying this \nenvironment variable will whitelist all services.\n\n```bash\nexport AWS_JUPYTER_PROXY_WHITELISTED_SERVICES=sagemaker,s3\njupyter-lab\n```\n\n## Development\n\nInstall all dev dependencies\n\n```bash\npip install -e \".[dev]\"\njupyter serverextension enable --py aws_jupyter_proxy --sys-prefix\n```\n\nRun unit tests using pytest\n\n```bash\npytest tests/unit\n```\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": ["2023-03-17T19:14:59Z", "2022-11-04T19:05:32Z", "2022-10-31T04:10:01Z", "2022-10-29T01:25:06Z", "2021-03-02T00:03:17Z", "2021-01-22T01:28:25Z", "2020-12-16T22:26:21Z"]}, {"name": "aws-k8s-tester", "description": "AWS Kubernetes tester, kubetest2 deployer implementation", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "\n# aws-k8s-tester\n\n[![Go Report Card](https://goreportcard.com/badge/github.com/aws/aws-k8s-tester)](https://goreportcard.com/report/github.com/aws/aws-k8s-tester)\n[![Godoc](http://img.shields.io/badge/go-documentation-blue.svg?style=flat-square)](https://pkg.go.dev/github.com/aws/aws-k8s-tester)\n[![Releases](https://img.shields.io/github/release/aws/aws-k8s-tester/all.svg?style=flat-square)](https://github.com/aws/aws-k8s-tester/releases)\n[![LICENSE](https://img.shields.io/github/license/aws/aws-k8s-tester.svg?style=flat-square)](https://github.com/aws/aws-k8s-tester/blob/master/LICENSE)\n\nhttps://github.com/kubernetes/enhancements/blob/master/keps/provider-aws/2313-aws-k8s-tester/README.md\n\n`aws-k8s-tester` is a set of utilities and libraries for \"testing\" Kubernetes on AWS.\n\n- Implements [`test-infra/kubetest2` interface](https://github.com/kubernetes/test-infra/tree/master/kubetest2).\n- Uses AWS CloudFormation for resource creation.\n- Supports automatic rollback and resource deletion.\n- Flexible add-on support via environmental variables.\n- Extensible as a Go package; `eks.Tester.Up` to create EKS.\n- Performance tests suites.\n\nThe main goal is to create \"temporary\" EC2 instances or EKS clusters for \"testing\" purposes:\n\n- Upstream conformance tests\n  - https://github.com/kubernetes/test-infra/blob/master/config/jobs/kubernetes/sig-cloud-provider/aws/eks/eks-periodics.yaml\n  - https://github.com/kubernetes/test-infra/pull/16890\n- CNI plugin conformance tests\n  - https://github.com/aws/amazon-vpc-cni-k8s/blob/master/scripts/lib/cluster.sh\n  - https://github.com/aws/amazon-vpc-cni-k8s/pull/875\n  - https://github.com/aws/amazon-vpc-cni-k8s/pull/878\n  - https://github.com/aws/amazon-vpc-cni-k8s/pull/951\n  - https://github.com/aws/amazon-vpc-cni-k8s/pull/957\n- AppMesh scalability testing\n  - https://github.com/aws/aws-app-mesh-controller-for-k8s/blob/master/scripts/lib/cluster.sh\n  - https://github.com/aws/aws-app-mesh-controller-for-k8s/pull/137\n\n\n## Install\n\nhttps://github.com/aws/aws-k8s-tester/releases\n\n\n## `aws-k8s-tester eks`\n\nMake sure AWS credential is located in your machine:\n\n```bash\n# confirm credential is valid\naws sts get-caller-identity --query Arn --output text\n```\n\nSee the following for more fields:\n- https://github.com/aws/aws-k8s-tester/blob/master/eksconfig/README.md\n- https://pkg.go.dev/github.com/aws/aws-k8s-tester/eksconfig?tab=doc\n- https://github.com/aws/aws-k8s-tester/blob/master/eksconfig/default.yaml\n\n```bash\n# easiest way, use the defaults\n# creates role, VPC, EKS cluster\nrm -rf /tmp/${USER}-test-eks-prod*\naws-k8s-tester eks create cluster --enable-prompt=true -p /tmp/${USER}-test-prod-eks.yaml\naws-k8s-tester eks delete cluster --enable-prompt=true -p /tmp/${USER}-test-prod-eks.yaml\n\n# advanced options can be set via environmental variables\n# e.g. node groups, managed node groups, add-ons\nrm -rf /tmp/${USER}-test-eks*\nAWS_K8S_TESTER_EKS_PARTITION=aws \\\nAWS_K8S_TESTER_EKS_REGION=us-west-2 \\\nAWS_K8S_TESTER_EKS_LOG_COLOR=true \\\nAWS_K8S_TESTER_EKS_S3_BUCKET_CREATE=true \\\nAWS_K8S_TESTER_EKS_S3_BUCKET_CREATE_KEEP=true \\\nAWS_K8S_TESTER_EKS_COMMAND_AFTER_CREATE_CLUSTER=\"aws eks describe-cluster --name GetRef.Name\" \\\nAWS_K8S_TESTER_EKS_COMMAND_AFTER_CREATE_ADD_ONS=\"aws eks describe-cluster --name GetRef.Name\" \\\nAWS_K8S_TESTER_EKS_PARAMETERS_ENCRYPTION_CMK_CREATE=true \\\nAWS_K8S_TESTER_EKS_PARAMETERS_ROLE_CREATE=true \\\nAWS_K8S_TESTER_EKS_PARAMETERS_VERSION=1.17 \\\nAWS_K8S_TESTER_EKS_PARAMETERS_VPC_CREATE=true \\\nAWS_K8S_TESTER_EKS_CLIENTS=5 \\\nAWS_K8S_TESTER_EKS_CLIENT_QPS=30 \\\nAWS_K8S_TESTER_EKS_CLIENT_BURST=20 \\\nAWS_K8S_TESTER_EKS_ADD_ON_NODE_GROUPS_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_NODE_GROUPS_ROLE_CREATE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_NODE_GROUPS_FETCH_LOGS=false \\\nAWS_K8S_TESTER_EKS_ADD_ON_NODE_GROUPS_ASGS='{\"GetRef.Name-ng-al2-cpu\":{\"name\":\"GetRef.Name-ng-al2-cpu\",\"remote-access-user-name\":\"ec2-user\",\"ami-type\":\"AL2_x86_64\",\"image-id\":\"\",\"image-id-ssm-parameter\":\"/aws/service/eks/optimized-ami/1.17/amazon-linux-2/recommended/image_id\",\"instance-types\":[\"c5.xlarge\"],\"volume-size\":40,\"asg-min-size\":2,\"asg-max-size\":2,\"asg-desired-capacity\":2,\"kubelet-extra-args\":\"\"},\"GetRef.Name-ng-al2-gpu\":{\"name\":\"GetRef.Name-ng-al2-gpu\",\"remote-access-user-name\":\"ec2-user\",\"ami-type\":\"AL2_x86_64_GPU\",\"image-id\":\"\",\"image-id-ssm-parameter\":\"/aws/service/eks/optimized-ami/1.17/amazon-linux-2-gpu/recommended/image_id\",\"instance-types\":[\"p3.8xlarge\"],\"volume-size\":40,\"asg-min-size\":1,\"asg-max-size\":1,\"asg-desired-capacity\":1,\"kubelet-extra-args\":\"\"},\"GetRef.Name-ng-bottlerocket\":{\"name\":\"GetRef.Name-ng-bottlerocket\",\"remote-access-user-name\":\"ec2-user\",\"ami-type\":\"BOTTLEROCKET_x86_64\",\"image-id\":\"\",\"image-id-ssm-parameter\":\"/aws/service/bottlerocket/aws-k8s-1.15/x86_64/latest/image_id\",\"ssm-document-cfn-stack-name\":\"GetRef.Name-install-bottlerocket\",\"ssm-document-name\":\"GetRef.Name-InstallBottlerocket\",\"ssm-document-create\":true,\"ssm-document-commands\":\"enable-admin-container\",\"ssm-document-execution-timeout-seconds\":3600,\"instance-types\":[\"c5.xlarge\"],\"volume-size\":40,\"asg-min-size\":2,\"asg-max-size\":2,\"asg-desired-capacity\":2}}' \\\nAWS_K8S_TESTER_EKS_ADD_ON_MANAGED_NODE_GROUPS_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_MANAGED_NODE_GROUPS_ROLE_CREATE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_MANAGED_NODE_GROUPS_FETCH_LOGS=false \\\nAWS_K8S_TESTER_EKS_ADD_ON_MANAGED_NODE_GROUPS_MNGS='{\"GetRef.Name-mng-al2-cpu\":{\"name\":\"GetRef.Name-mng-al2-cpu\",\"remote-access-user-name\":\"ec2-user\",\"release-version\":\"\",\"ami-type\":\"AL2_x86_64\",\"instance-types\":[\"c5.xlarge\"],\"volume-size\":40,\"asg-min-size\":2,\"asg-max-size\":2,\"asg-desired-capacity\":2},\"GetRef.Name-mng-al2-gpu\":{\"name\":\"GetRef.Name-mng-al2-gpu\",\"remote-access-user-name\":\"ec2-user\",\"release-version\":\"\",\"ami-type\":\"AL2_x86_64_GPU\",\"instance-types\":[\"p3.8xlarge\"],\"volume-size\":40,\"asg-min-size\":1,\"asg-max-size\":1,\"asg-desired-capacity\":1}}' \\\nAWS_K8S_TESTER_EKS_ADD_ON_FLUENTD_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_METRICS_SERVER_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_CONFORMANCE_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_APP_MESH_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_CSI_EBS_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_KUBERNETES_DASHBOARD_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_PROMETHEUS_GRAFANA_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_NLB_HELLO_WORLD_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_NLB_GUESTBOOK_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_ALB_2048_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_JOBS_PI_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_JOBS_ECHO_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_CRON_JOBS_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_CSRS_LOCAL_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_CONFIGMAPS_LOCAL_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_SECRETS_LOCAL_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_WORDPRESS_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_JUPYTER_HUB_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_CUDA_VECTOR_ADD_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_CLUSTER_LOADER_LOCAL_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_HOLLOW_NODES_LOCAL_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_STRESSER_LOCAL_ENABLE=true \\\naws-k8s-tester eks create cluster --enable-prompt=true -p /tmp/${USER}-test-eks.yaml\n\n<<COMMENT\n# to delete\naws-k8s-tester eks delete cluster --enable-prompt=true -p /tmp/${USER}-test-eks.yaml\n\n# run \"eks create config\" to check/edit configuration file first \naws-k8s-tester eks create config -p /tmp/${USER}-test-eks.yaml\n\n# run the following command with those envs overwrites configuration, and create\naws-k8s-tester eks create cluster --enable-prompt=true -p /tmp/${USER}-test-eks.yaml\nCOMMENT\n\n<<COMMENT\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text);\necho ${ACCOUNT_ID}\nCLUSTER_ARN=arn:aws:eks:us-west-2:${ACCOUNT_ID}:cluster/${USER}-test-eks\necho ${CLUSTER_ARN}\n\n# to assign a non-random cluster name\n# if empty, name is auto-generated\nAWS_K8S_TESTER_EKS_NAME=${USER}-test-eks \\\n\n# to create/delete a S3 bucket for test artifacts\nAWS_K8S_TESTER_EKS_S3_BUCKET_CREATE=true \\\n\n# to reuse an existing S3 bucket\nAWS_K8S_TESTER_EKS_S3_BUCKET_CREATE=false \\\nAWS_K8S_TESTER_EKS_S3_BUCKET_NAME=${BUCKET_NAME} \\\n\n# to automatically create EC2 key-pair\nAWS_K8S_TESTER_EKS_REMOTE_ACCESS_KEY_CREATE=true \\\n\n# to reuse an existing EC2 key-pair\nAWS_K8S_TESTER_EKS_REMOTE_ACCESS_KEY_CREATE=false \\\nAWS_K8S_TESTER_EKS_REMOTE_ACCESS_KEY_NAME=${KEY_NAME} \\\nAWS_K8S_TESTER_EKS_REMOTE_ACCESS_PRIVATE_KEY_PATH=${KEY_PATH} \\\n\n# to reuse an existing role for \"EKS cluster\"\nAWS_K8S_TESTER_EKS_PARAMETERS_ROLE_CREATE=false \\\nAWS_K8S_TESTER_EKS_PARAMETERS_ROLE_ARN=${CLUSTER_ROLE_ARN} \\\n\n# to reuse an existing VPC\nAWS_K8S_TESTER_EKS_PARAMETERS_VPC_CREATE=false \\\nAWS_K8S_TESTER_EKS_PARAMETERS_VPC_ID=${VPC_ID} \\\n\n# to reuse an existing role for \"Node Group\"\nAWS_K8S_TESTER_EKS_ADD_ON_NODE_GROUPS_ROLE_CREATE=false \\\nAWS_K8S_TESTER_EKS_ADD_ON_NODE_GROUPS_ROLE_ARN=${NG_ROLE_ARN} \\\n\n# to reuse an existing role for \"Managed Node Group\"\nAWS_K8S_TESTER_EKS_ADD_ON_MANAGED_NODE_GROUPS_ROLE_CREATE=false \\\nAWS_K8S_TESTER_EKS_ADD_ON_MANAGED_NODE_GROUPS_ROLE_ARN=${MNG_ROLE_ARN} \\\n\n# to reuse an existing role for \"Fargate\"\nAWS_K8S_TESTER_EKS_ADD_ON_FARGATE_ROLE_CREATE=false \\\nAWS_K8S_TESTER_EKS_ADD_ON_FARGATE_ROLE_ARN=${FARGATE_ROLE_ARN} \\\n\n# to user ${USER} in node groups\nAWS_K8S_TESTER_EKS_ADD_ON_NODE_GROUPS_ASGS={\\\"${USER}-test-eks-ng-al2-cpu\\\":{\\\"name\\\":\\\"${USER}-test-eks-ng-al2-cpu\\\",\\\"remote-access-user-name\\\":\\\"ec2-user\\\",\\\"ami-type\\\":\\\"AL2_x86_64\\\",\\\"image-id-ssm-parameter\\\":\\\"/aws/service/eks/optimized-ami/1.15/amazon-linux-2/recommended/image_id\\\",\\\"instance-types\\\":[\\\"c5.xlarge\\\"],\\\"volume-size\\\":40,\\\"asg-min-size\\\":1,\\\"asg-max-size\\\":1,\\\"asg-desired-capacity\\\":1,\\\"kubelet-extra-args\\\":\\\"\\\"},\\\"${USER}-test-eks-ng-bottlerocket\\\":{\\\"name\\\":\\\"${USER}-test-eks-ng-bottlerocket\\\",\\\"remote-access-user-name\\\":\\\"ec2-user\\\",\\\"ami-type\\\":\\\"BOTTLEROCKET_x86_64\\\",\\\"image-id-ssm-parameter\\\":\\\"/aws/service/bottlerocket/aws-k8s-1.15/x86_64/latest/image_id\\\",\\\"ssm-document-cfn-stack-name\\\":\\\"${USER}-install-bottle-rocket\\\",\\\"ssm-document-name\\\":\\\"${USER}InstallBottleRocket\\\",\\\"ssm-document-create\\\":true,\\\"ssm-document-commands\\\":\\\"enable-admin-container\\\",\\\"ssm-document-execution-timeout-seconds\\\":3600,\\\"instance-types\\\":[\\\"c5.xlarge\\\"],\\\"volume-size\\\":40,\\\"asg-min-size\\\":1,\\\"asg-max-size\\\":1,\\\"asg-desired-capacity\\\":1}} \\\nAWS_K8S_TESTER_EKS_ADD_ON_MANAGED_NODE_GROUPS_MNGS={\\\"${USER}-test-eks-mng-al2-cpu\\\":{\\\"name\\\":\\\"${USER}-test-eks-mng-al2-cpu\\\",\\\"remote-access-user-name\\\":\\\"ec2-user\\\",\\\"release-version\\\":\\\"\\\",\\\"ami-type\\\":\\\"AL2_x86_64\\\",\\\"instance-types\\\":[\\\"c5.xlarge\\\"],\\\"volume-size\\\":40,\\\"asg-min-size\\\":1,\\\"asg-max-size\\\":1,\\\"asg-desired-capacity\\\":1}} \\\n\n# to build ECR images for remote tester add-ons\ncd ${GOPATH}/src/github.com/aws/aws-k8s-tester\nmake docker-push ACCOUNT_ID=YOUR_ACCOUNT_ID TAG=latest\n\nAWS_K8S_TESTER_EKS_ADD_ON_CLUSTER_LOADER_REMOTE_ENABLE=true \\\nAWS_K8S_TESTER_EKS_ADD_ON_CLUSTER_LOADER_REMOTE_RUNS=1 \\\nAWS_K8S_TESTER_EKS_ADD_ON_CLUSTER_LOADER_REMOTE_REPOSITORY_ACCOUNT_ID=YOUR_ACCOUNT_ID \\\nAWS_K8S_TESTER_EKS_ADD_ON_CLUSTER_LOADER_REMOTE_REPOSITORY_NAME=aws/aws-k8s-tester \\\nAWS_K8S_TESTER_EKS_ADD_ON_CLUSTER_LOADER_REMOTE_REPOSITORY_IMAGE_TAG=latest \\\nCOMMENT\n```\n\nThis will create an EKS cluster with a worker node (takes about 20 minutes).\n\nOnce cluster is created, check cluster state using AWS CLI:\n\n```bash\naws eks describe-cluster \\\n  --name ${EKS_CLUSTER_NAME} \\\n  --query cluster.status\n\n\"ACTIVE\"\n```\n\nCluster states are persisted on disk and S3 bucket.\n\nEKS tester uses this file to record status.\n\n```bash\ncat /tmp/config.yaml\n\n# or\nless +FG /tmp/config.yaml\n```\n\n\n## `ec2-utils`\n\nMake sure AWS credential is located in your machine:\n\n```bash\n# confirm credential is valid\naws sts get-caller-identity --query Arn --output text\n```\n\nSee the following for more fields:\n- https://github.com/aws/aws-k8s-tester/blob/master/ec2config/README.md\n- https://pkg.go.dev/github.com/aws/aws-k8s-tester/ec2config?tab=doc\n- https://github.com/aws/aws-k8s-tester/blob/master/ec2config/default.yaml\n\n```bash\n# easiest way, use the defaults\n# creates role, VPC, EC2 ASG\nrm -rf /tmp/${USER}-test-ec2*\nec2-utils create instances --enable-prompt=true -p /tmp/${USER}-test-ec2.yaml\nec2-utils delete instances --enable-prompt=true -p /tmp/${USER}-test-ec2.yaml\n\n# advanced options can be set via environmental variables\nrm -rf /tmp/${USER}-test-ec2*\nAWS_K8S_TESTER_EC2_ON_FAILURE_DELETE=true \\\nAWS_K8S_TESTER_EC2_PARTITION=aws \\\nAWS_K8S_TESTER_EC2_REGION=us-west-2 \\\nAWS_K8S_TESTER_EC2_S3_BUCKET_CREATE=true \\\nAWS_K8S_TESTER_EC2_S3_BUCKET_CREATE_KEEP=true \\\nAWS_K8S_TESTER_EC2_REMOTE_ACCESS_KEY_CREATE=true \\\nAWS_K8S_TESTER_EC2_ASGS_FETCH_LOGS=true \\\nAWS_K8S_TESTER_EC2_ASGS='{\"GetRef.Name-al2-cpu\":{\"name\":\"GetRef.Name-al2-cpu\",\"remote-access-user-name\":\"ec2-user\",\"ami-type\":\"AL2_x86_64\",\"image-id-ssm-parameter\":\"/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2\",\"instance-types\":[\"c5.xlarge\"],\"volume-size\":40,\"asg-min-size\":1,\"asg-max-size\":1,\"asg-desired-capacity\":1},\"GetRef.Name-bottlerocket\":{\"name\":\"GetRef.Name-bottlerocket\",\"remote-access-user-name\":\"ec2-user\",\"ami-type\":\"BOTTLEROCKET_x86_64\",\"image-id-ssm-parameter\":\"/aws/service/bottlerocket/aws-k8s-1.15/x86_64/latest/image_id\",\"ssm-document-cfn-stack-name\":\"GetRef.Name-install-bottlerocket\",\"ssm-document-name\":\"GetRef.Name-install-bottlerocket\",\"ssm-document-create\":true,\"ssm-document-commands\":\"enable-admin-container\",\"ssm-document-execution-timeout-seconds\":3600,\"instance-types\":[\"c5.xlarge\"],\"volume-size\":40,\"asg-min-size\":1,\"asg-max-size\":1,\"asg-desired-capacity\":1}}' \\\nAWS_K8S_TESTER_EC2_ROLE_CREATE=true \\\nAWS_K8S_TESTER_EC2_VPC_CREATE=true \\\nec2-utils create instances --enable-prompt=true -p /tmp/${USER}-test-ec2.yaml\n\n<<COMMENT\n# to delete\nec2-utils delete instances --enable-prompt=true -p /tmp/${USER}-test-ec2.yaml\n\n# run \"ec2 create config\" to check/edit configuration file first \nec2-utils create config -p /tmp/${USER}-test-ec2.yaml\nec2-utils create instances -p /tmp/${USER}-test-ec2.yaml\n\n# run the following command with those envs overwrites configuration, and create\nec2-utils create instances --enable-prompt=true -p /tmp/${USER}-test-ec2.yaml\nCOMMENT\n\n<<COMMENT\n# to config a fixed name for EC2 ASG\nAWS_K8S_TESTER_EC2_NAME=${NAME} \\\n\n# to create/delete a S3 bucket for test artifacts\nAWS_K8S_TESTER_EC2_S3_BUCKET_CREATE=true \\\n\n# to reuse an existing S3 bucket\nAWS_K8S_TESTER_EC2_S3_BUCKET_CREATE=false \\\nAWS_K8S_TESTER_EC2_S3_BUCKET_NAME=${BUCKET_NAME} \\\n\n# to automatically create EC2 key-pair\nAWS_K8S_TESTER_EC2_REMOTE_ACCESS_KEY_CREATE=true \\\n\n# to reuse an existing EC2 key-pair\nAWS_K8S_TESTER_EC2_REMOTE_ACCESS_KEY_CREATE=false \\\nAWS_K8S_TESTER_EC2_REMOTE_ACCESS_KEY_NAME=${KEY_NAME} \\\nAWS_K8S_TESTER_EC2_REMOTE_ACCESS_PRIVATE_KEY_PATH=${KEY_PATH} \\\n\n# to reuse an existing role\nAWS_K8S_TESTER_EC2_ROLE_CREATE=false \\\nAWS_K8S_TESTER_EC2_ROLE_ARN=${ROLE_ARN} \\\n\n# to reuse an existing VPC\nAWS_K8S_TESTER_EC2_VPC_CREATE=false \\\nAWS_K8S_TESTER_EC2_VPC_ID=${VPC_ID} \\\n\n# to use ${USER}\nAWS_K8S_TESTER_EC2_ASGS={\\\"${USER}-test-ec2-al2-cpu\\\":{\\\"name\\\":\\\"${USER}-test-ec2-al2-cpu\\\",\\\"remote-access-user-name\\\":\\\"ec2-user\\\",\\\"ami-type\\\":\\\"AL2_x86_64\\\",\\\"image-id-ssm-parameter\\\":\\\"/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2\\\",,\\\"instance-types\\\":[\\\"c5.xlarge\\\"],\\\"volume-size\\\":40,\\\"asg-min-size\\\":1,\\\"asg-max-size\\\":1,\\\"asg-desired-capacity\\\":1},\\\"${USER}-test-ec2-bottlerocket\\\":{\\\"name\\\":\\\"${USER}-test-ec2-bottlerocket\\\",\\\"remote-access-user-name\\\":\\\"ec2-user\\\",\\\"ami-type\\\":\\\"BOTTLEROCKET_x86_64\\\",\\\"image-id-ssm-parameter\\\":\\\"/aws/service/bottlerocket/aws-k8s-1.15/x86_64/latest/image_id\\\",\\\"ssm-document-cfn-stack-name\\\":\\\"${USER}-install-bottlerocket\\\",\\\"ssm-document-name\\\":\\\"${USER}InstallBottleRocket\\\",\\\"ssm-document-create\\\":true,\\\"ssm-document-commands\\\":\\\"enable-admin-container\\\",\\\"ssm-document-execution-timeout-seconds\\\":3600,,\\\"instance-types\\\":[\\\"c5.xlarge\\\"],\\\"volume-size\\\":40,\\\"asg-min-size\\\":1,\\\"asg-max-size\\\":1,\\\"asg-desired-capacity\\\":1}} \\\nCOMMENT\n```\n\n\n## `eks-utils apis`\n\nInstall `eks-utils` from https://github.com/aws/aws-k8s-tester/releases.\n\n```\nAWS_K8S_TESTER_VERSION=${LATEST_RELEASE_VERSION}\n\nDOWNLOAD_URL=https://github.com/aws/aws-k8s-tester/releases/download\nrm -rf /tmp/aws-k8s-tester\nrm -rf /tmp/eks-utils\n\nif [[ \"${OSTYPE}\" == \"linux\"* ]]; then\n  curl -L ${DOWNLOAD_URL}/${AWS_K8S_TESTER_VERSION}/eks-utils-${AWS_K8S_TESTER_VERSION}-linux-amd64 -o /tmp/eks-utils\nelif [[ \"${OSTYPE}\" == \"darwin\"* ]]; then\n  curl -L ${DOWNLOAD_URL}/${AWS_K8S_TESTER_VERSION}/eks-utils-${AWS_K8S_TESTER_VERSION}-darwin-amd64 -o /tmp/eks-utils\nfi\n\nchmod +x /tmp/eks-utils\n/tmp/eks-utils version\n```\n\n`kube-apiserver-audit` logs will show:\n\n```json\n{\n    \"kind\": \"Event\",\n    \"apiVersion\": \"audit.k8s.io/v1\",\n    \"level\": \"Request\",\n    \"auditID\": \"b7ff399c-2d27-4f47-98b4-0b87c0ceb436\",\n    \"stage\": \"ResponseComplete\",\n    \"requestURI\": \"/api/v1/nodes?limit=30\",\n    \"verb\": \"list\",\n    \"user\": {\n        \"username\": \"system:unsecured\",\n        \"groups\": [\n            \"system:masters\",\n            \"system:authenticated\"\n        ]\n    },\n    \"sourceIPs\": [\n        \"127.0.0.1\"\n    ],\n    \"userAgent\": \"eks-utils/v0.0.0 (linux/amd64) kubernetes/$Format\",\n    \"objectRef\": {\n        \"resource\": \"nodes\",\n        \"apiVersion\": \"v1\"\n    },\n    \"responseStatus\": {\n        \"metadata\": {},\n        \"code\": 200\n    },\n    \"requestReceivedTimestamp\": \"2020-06-03T17:45:21.368826Z\",\n    \"stageTimestamp\": \"2020-06-03T17:45:21.372015Z\"\n}\n```\n\n`eks-utils apis` helps with API deprecation (e.g. https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.17.md#deprecations-and-removals).\n\n**WARNING**: `kubectl` internally converts API versions in the response (see [`kubernetes/issues#58131`](https://github.com/kubernetes/kubernetes/issues/58131#issuecomment-403829566)). Which means `kubectl get` output may have different API versions than the one persisted in `etcd`. Upstream Kubernetes recommends upgrading deprecated API with *get and put*:\n\n> the simplest approach is to get/put every object after upgrades. objects that don't need migration will no-op (they won't even increment resourceVersion in etcd). objects that do need migration will persist in the new preferred storage version\n\nWhich means there's no way in client-side to find all resources created with deprecated API groups. The only way to ensure API group upgrades is list all resources, and execute *get and put* with the latest API group version. If the resource has already latest API version, it will be no-op. Otherwise, it will upgrade to the latest API version.\n\n`eks-utils apis` will help with the list calls with proper pagination and generate *get and put* scripts for the cluster:\n\n```bash\n# to check supported API groups from current kube-apiserver\neks-utils apis \\\n  --kubeconfig /tmp/kubeconfig.yaml \\\n  supported\n\n# to write API upgrade/rollback scripts and YAML files in \"/tmp/eks-utils\"\n#\n# make sure to set proper \"--batch-limit\" and \"--batch-interval\"\n# to not overload EKS master; if it's set too high, it can affect\n# production workloads slowing down kube-apiserver\nrm -rf /tmp/eks-utils-resources\neks-utils apis \\\n  --kubeconfig /tmp/kubeconfig.yaml \\\n  --enable-prompt \\\n  deprecate \\\n  --batch-limit 10 \\\n  --batch-interval 2s \\\n  --dir /tmp/eks-utils-resources\n\n# this command does not apply or create any resources\n# it only lists the resources that need be upgraded\n\n# if there's any resources that needs upgrade,\n# it writes patched YAML file, original YAML file,\n# bash scripts to update and rollback\nfind /tmp/eks-utils-resources\n```\n\n## `etcd-utils k8s list`\n\n`etcd-utils k8s list` helps with API deprecation (e.g. https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.17.md#deprecations-and-removals).\n\n**WARNING**: `kubectl` internally converts API versions in the response (see [`kubernetes/issues#58131`](https://github.com/kubernetes/kubernetes/issues/58131#issuecomment-403829566)). Which means `kubectl get` output may have different API versions than the one persisted in `etcd` . Upstream Kubernetes recommends upgrading deprecated API with *get and put*:\n\n> the simplest approach is to get/put every object after upgrades. objects that don't need migration will no-op (they won't even increment resourceVersion in etcd). objects that do need migration will persist in the new preferred storage version\n\nTo minimize the impact of list calls, `etcd-utils k8s list` reads keys with leadership election and pagination; only a single worker can run at a time.\n\n```bash\n# to list all deployments with etcd pagination + k8s decoder\netcd-utils k8s \\\n  --endpoints http://localhost:2379 \\\n  list \\\n  --prefixes /registry/deployments \\\n  --output /tmp/etcd-utils-k8s-list.output.yaml\n\n# or \".json\"\n```\n", "release_dates": ["2022-02-01T23:34:41Z", "2021-10-15T23:20:55Z", "2021-10-13T20:53:45Z", "2021-10-06T17:04:18Z", "2021-09-15T21:56:58Z", "2021-07-19T07:47:59Z", "2021-06-03T02:05:46Z", "2021-04-26T18:39:16Z", "2021-02-03T00:54:35Z", "2021-01-26T05:31:33Z", "2021-01-25T22:48:57Z", "2020-11-13T04:42:29Z", "2020-11-11T18:35:44Z", "2020-10-20T17:25:18Z", "2020-09-13T01:29:49Z", "2020-09-10T18:25:29Z", "2020-09-05T00:01:39Z", "2020-07-21T00:52:54Z", "2020-07-17T20:25:24Z", "2020-07-17T20:09:46Z", "2020-07-14T22:12:50Z", "2020-07-11T03:34:22Z", "2020-07-10T22:30:51Z", "2020-07-09T21:11:24Z", "2020-07-07T22:33:59Z", "2020-06-29T16:39:34Z", "2020-06-15T19:52:59Z", "2020-06-15T03:19:15Z", "2020-06-14T06:46:35Z", "2020-06-13T06:51:43Z"]}, {"name": "aws-kinesisanalytics-flink-connectors", "description": "This library contains various Apache Flink connectors to connect to AWS data sources and sinks. ", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon Kinesis Flink Connectors\n\nThis repository contains various Apache Flink connectors to connect to [AWS Kinesis][kinesis] data sources and sinks. \n\n## Amazon Kinesis Data Firehose Producer for Apache Flink\nThis Producer allows Flink applications to push directly to [Kinesis Firehose][firehose].\n- [AWS Documentation][firehose-documentation]\n- [Issues][issues]\n\n### Quickstart\nConfigure and instantiate a `FlinkKinesisFirehoseProducer`:\n\n```java\nProperties config = new Properties();\noutputProperties.setProperty(ConsumerConfigConstants.AWS_REGION, region);\n\nFlinkKinesisFirehoseProducer<String> sink = new FlinkKinesisFirehoseProducer<>(streamName, new SimpleStringSchema(), config);\n```\n\n### Getting Started\nFollow the [example instructions][example] to create an end to end application:\n- Write data into a [Kinesis Data Stream][kds]\n- Process the streaming data using [Kinesis Data Analytics][kda]\n- Write the results to a [Kinesis Firehose][firehose] using the `FlinkKinesisFirehoseProducer`\n- Store the data in an [S3 Bucket][s3]\n\n### Building from Source\n1. You will need to install Java 1.8+ and Maven\n1. Clone the repository from Github\n1. Build using Maven from the project root directory: \n    1. `$ mvn clean install`\n\n### Flink Version Matrix\nFlink maintain backwards compatibility for the Sink interface used by the Firehose Producer. \nThis project is compatible with Flink 1.x, there is no guarantee it will support Flink 2.x should it release in the future. \n\nConnector Version | Flink Version | Release Date\n----------------- | ------------- | ------------\n2.1.0 | 1.x | Feb, 2023\n2.0.0 | 1.x | Jul, 2020\n1.1.0 | 1.x | Dec, 2019\n1.0.1 | 1.x | Dec, 2018\n1.0.0 | 1.x | Dec, 2018\n\n## License\n\nThis library is licensed under the Apache 2.0 License. \n\n[kinesis]: https://aws.amazon.com/kinesis\n[firehose]: https://aws.amazon.com/kinesis/data-firehose/\n[kds]: https://aws.amazon.com/kinesis/data-streams/\n[kda]: https://aws.amazon.com/kinesis/data-analytics/\n[s3]: https://aws.amazon.com/s3/\n[firehose-documentation]: https://docs.aws.amazon.com/kinesisanalytics/latest/java/how-sinks.html#sinks-firehose-create\n[issues]: https://github.com/aws/aws-kinesisanalytics-flink-connectors/issues\n[example]: https://docs.aws.amazon.com/kinesisanalytics/latest/java/get-started-exercise-fh.html", "release_dates": ["2023-02-20T11:45:55Z", "2020-07-30T15:57:55Z", "2020-06-17T20:15:39Z"]}, {"name": "aws-kinesisanalytics-runtime", "description": "This library contains the Kinesis Analytics stream processing runtime configuration classes. ", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Kinesis Analytics Flink Runtime\nThis library can be used to access externalised KDA Flink application properties at runtime. \nSee the [documentation][aws-documentation] for a guide on how to configure properties in your KDA application. \n\n- [AWS Documentation][aws-documentation]\n- [Issues][issues]\n\n## Quickstart\nAccess properties configured via the AWS Console/SDK using:\n\n```java\nProperties properties = KinesisAnalyticsRuntime.getApplicationProperties().get(\"app-group\");\nString inputStreamName = properties.getProperty(\"inputStreamName\");\nString outputStreamName = properties.getProperty(\"outputStreamName\");\n```\n\n## Building from Source\n1. You will need to install Java 1.8+ and Maven\n1. Clone the repository from Github\n1. Build using Maven from the project root directory: \n    1. `$ mvn clean install`\n\n## Flink Version Matrix\nThis project is compatible with Flink 1.x, there is no guarantee it will support Flink 2.x should it release in the future.\n\nConnector Version | Flink Version | Release Date\n----------------- | ------------- | ------------\n1.1.0 | 1.x | Dec, 2019\n1.0.1 | 1.x | Dec, 2018\n1.0.0 | 1.x | Dec, 2018 \n\n## License\n\nThis library is licensed under the Apache 2.0 License. \n\n[aws-documentation]: https://docs.aws.amazon.com/kinesisanalytics/latest/java/how-properties.html\n[issues]: https://github.com/aws/aws-kinesisanalytics-runtime/issues\n", "release_dates": []}, {"name": "aws-kms-xksproxy-api-spec", "description": "AWS KMS External Keystore (XKS) Proxy API specification", "language": null, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "![](https://github.com/aws/aws-kms-xksproxy-api-spec/actions/workflows/ci.yml/badge.svg)\n\n## AWS KMS External Key Store (XKS) Proxy API Specification\n\nThis repository contains the [AWS KMS External Keystore (XKS) Proxy API Specification](xks_proxy_api_spec.md\n). It is made available under the [Creative Commons Attribution-ShareAlike 4.0 International License](LICENSE).\n\n\nIf you discover a potential security issue, please follow [these](CONTRIBUTING.md#security-issue-notifications) guidelines.\n\nA sample XKS proxy implementing this specification is available at [aws-kms-xks-proxy](https://github.com/aws-samples/aws-kms-xks-proxy).\n\nA `curl` based test client that can be used to check if a specific XKS proxy implementation complies with this specification is available at [aws-kms-xksproxy-test-client](https://github.com/aws-samples/aws-kms-xksproxy-test-client).\n\nRead the AWS News blog on [AWS KMS External Key Stores](https://aws.amazon.com/blogs/aws/announcing-aws-kms-external-key-store-xks) to learn more about the XKS feature in AWS KMS.\n\n\n\n", "release_dates": []}, {"name": "aws-lakeformation-best-practices", "description": null, "language": null, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## AWS Lake Formation Best Practices\n\nA best practices guide for using Lake Formation.\n\nReturn to [Live Docs](https://aws.github.io/aws-lakeformation-best-practices/).\n\n## License Summary\n\nThe documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.\n\nThe sample code within this documentation is made available under the MIT-0 license. See the LICENSE-SAMPLECODE file.", "release_dates": []}, {"name": "aws-lambda-base-images", "description": null, "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS Lambda Base Container Images\n\nAWS provided base images for Lambda contain all the required components to run your functions packaged as container images on AWS Lambda.\nThese base images contain the Amazon Linux Base operating system, the runtime for a given language, dependencies and the Lambda Runtime Interface Client (RIC), which implements the Lambda [Runtime API](https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html).\nThe Lambda Runtime Interface Client allows your runtime to receive requests from and send requests to the Lambda service.\n\nTo learn more about how these images are used, check out the AWS documentation on how to [Create an image from an AWS base image for Lambda](https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#images-create-1).\n\n### Maintenance policy\n\nAWS will regularly provide security patches and other updates for these base images.\nThese images are similar to the AWS Lambda execution environment on the cloud to allow customers to easily packaging functions to the container image.\nHowever, we may choose to optimize the container images by changing the components or dependencies included.\nWhen deployed to AWS Lambda these images will be run as-is.\n\nThis is more of an *artifact store* than a Git repository, for reasons explained later. Please note that **branches other than `main` are regularly force-pushed, and content may disappear without warning**.\n\n## What we're doing here\n\nAs soon as new AWS Lambda base images are available, an automated process snapshots the layers and configuration used to create these images and force-pushes them to this repository.\n\nFor examples, please see other branches in this repository.\n\nCommitted alongside the Dockerfiles in the branches are the tarballs, which balloon the repository size. Thus, we force-push branches that contain the tarballs.\n\nAlthough we force-push the files away, the older versions of our images remain present on DockerHub and Amazon ECR.\n\n## Usage\n\n### Requirements\nTo re-create the AWS Lambda base images, make sure you have the following pre-requisites set up:\n- [git](https://git-scm.com/downloads)\n- [git-lfs](https://git-lfs.github.com/)\n- [docker](https://docs.docker.com/get-docker/)\n\n### Building an image\nFirst, clone this repository:\n```\ngit clone https://github.com/aws/aws-lambda-base-images\n```\n\nThen, checkout the branch relevant to the Lambda base image you want to build.\n\neg. to build the `nodejs18.x` image, start by checking out the `nodejs18.x` branch:\n```\ngit checkout nodejs18.x\n```\n\nFinally you can build your image as such:\n```\ndocker build -t nodejs18.x:local -f Dockerfile.nodejs18.x .\n```\n\nThis will use the Dockerfile at `Dockerfile.nodejs18.x` and tag the newly-built image as `nodejs18.x:local`.\n\n\n## Licence\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": []}, {"name": "aws-lambda-builders", "description": "Python library to compile, build & package AWS Lambda functions for several runtimes & framework", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Lambda Builders\n\n![Apache 2.0 License](https://img.shields.io/github/license/aws/aws-lambda-builders)\n![GitHub release (latest by date)](https://img.shields.io/github/v/release/aws/aws-lambda-builders)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/aws-lambda-builders)\n![pip](https://img.shields.io/badge/pip-aws--lambda--builders-9cf)\n\nLambda Builders is a Python library to compile, build and package AWS Lambda functions for several runtimes & \nframeworks.\n\nLambda Builders currently contains the following workflows\n\n* Java with Gradle\n* Java with Maven\n* Dotnet with amazon.lambda.tools\n* Python with Pip\n* Javascript with Npm\n* Typescript with esbuild\n* Ruby with Bundler\n* Go with Mod\n* Rust with Cargo\n\nIn Addition to above workflows, AWS Lambda Builders also supports *Custom Workflows* through a Makefile.\n\nLambda Builders is the brains behind the `sam build` command from [AWS SAM CLI](https://github.com/awslabs/aws-sam-cli)\n\n### Integrating with Lambda Builders\n\nLambda Builders is a Python library.\nIt additionally exposes a JSON-RPC 2.0 interface to use from other languages.\n\nIf you intend to integrate with Lambda Builders,\ncheck out [this section of the DESIGN DOCUMENT](DESIGN.md#builders-library).\n\n### Contributing\n\nIf you are a developer and interested in contributing, read the [DESIGN DOCUMENT](./DESIGN.md) to understand how this works.\n", "release_dates": ["2024-02-21T18:15:27Z", "2024-01-02T20:38:53Z", "2023-12-15T17:39:28Z", "2023-12-04T19:36:02Z", "2023-11-16T00:40:13Z", "2023-11-11T00:02:04Z", "2023-10-16T18:48:09Z", "2023-10-12T21:59:37Z", "2023-09-18T18:14:37Z", "2023-08-30T17:33:30Z", "2023-07-31T18:21:20Z", "2023-07-14T00:13:57Z", "2023-06-12T16:49:56Z", "2023-06-05T19:23:16Z", "2023-05-25T21:19:48Z", "2023-05-01T21:23:02Z", "2023-04-17T23:17:03Z", "2023-04-11T21:32:41Z", "2023-03-20T23:10:32Z", "2023-02-10T00:17:34Z", "2023-02-06T21:07:04Z", "2023-01-25T18:27:48Z", "2023-01-06T19:32:30Z", "2022-11-23T00:16:13Z", "2022-11-16T01:49:52Z", "2022-11-15T00:02:22Z", "2022-11-02T19:17:54Z", "2022-10-18T18:34:16Z", "2022-08-30T18:50:09Z", "2022-06-23T22:04:42Z"]}, {"name": "aws-lambda-dotnet", "description": "Libraries, samples and tools to help .NET Core developers develop AWS Lambda functions.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Lambda for .NET [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/aws/aws-lambda-dotnet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nRepository for the AWS NuGet packages and Blueprints to support writing AWS Lambda functions using .NET Core.\n\nFor a history of releases view the [release change log](RELEASE.CHANGELOG.md)\n\n## Table of Contents\n- [AWS Lambda for .NET Core ![Gitter](https://gitter.im/aws/aws-lambda-dotnet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)](#aws-lambda-for-net-core-img-srchttpsbadgesgitterimjoin20chatsvg-altgitter)\n  - [Table of Contents](#table-of-contents)\n  - [NuGet Packages](#nuget-packages)\n    - [Events](#events)\n    - [Amazon.Lambda.Tools](#amazonlambdatools)\n      - [Global Tool Migration](#global-tool-migration)\n        - [Migrating from DotNetCliToolReference](#migrating-from-dotnetclitoolreference)\n    - [Amazon.Lambda.Annotations](#amazonlambdaannotations)\n    - [Amazon.Lambda.AspNetCoreServer](#amazonlambdaaspnetcoreserver)\n    - [Amazon.Lambda.TestUtilities](#amazonlambdatestutilities)\n  - [Blueprints](#blueprints)\n    - [Dotnet CLI Templates](#dotnet-cli-templates)\n    - [Yeoman (Deprecated)](#yeoman-deprecated)\n  - [Getting Help](#getting-help)\n  - [Feedback and Contributing](#feedback-and-contributing)\n\n\n## NuGet Packages\nThis repo contains a number of different tools and libraries to support development of Lambda functions using .NET. These packages have individual README docs outlining specific information for that particular package. These packages are cataloged here.\n\n### Events\n\nThis packages in this folder contains classes that can be used as input types for Lambda functions that process various AWS events.\n\nThese are the packages and their README.md files:\n\n* [Amazon.Lambda.APIGatewayEvents](Libraries/src/Amazon.Lambda.APIGatewayEvents) - [README.md](Libraries/src/Amazon.Lambda.APIGatewayEvents/README.md)\n* [Amazon.Lambda.ApplicationLoadBalancerEvents](Libraries/src/Amazon.Lambda.ApplicationLoadBalancerEvents) - [README.md](Libraries/src/Amazon.Lambda.ApplicationLoadBalancerEvents/README.md)\n* [Amazon.Lambda.CloudWatchLogsEvents](Libraries/src/Amazon.Lambda.CloudWatchLogsEvents) - [README.md](Libraries/src/Amazon.Lambda.CloudWatchLogsEvents/README.md)\n* [Amazon.Lambda.CognitoEvents](Libraries/src/Amazon.Lambda.CognitoEvents) - [README.md](Libraries/src/Amazon.Lambda.CognitoEvents/README.md)\n* [Amazon.Lambda.ConfigEvents](Libraries/src/Amazon.Lambda.ConfigEvents) - [README.md](Libraries/src/Amazon.Lambda.ConfigEvents/README.md)\n* [Amazon.Lambda.DynamoDBEvents](Libraries/src/Amazon.Lambda.DynamoDBEvents) - [README.md](Libraries/src/Amazon.Lambda.DynamoDBEvents/README.md)\n* [Amazon.Lambda.LexEvents](Libraries/src/Amazon.Lambda.LexEvents) - [README.md](Libraries/src/Amazon.Lambda.LexEvents/README.md)\n* [Amazon.Lambda.KinesisAnalyticsEvents](Libraries/src/Amazon.Lambda.KinesisAnalyticsEvents) - [README.md](Libraries/src/Amazon.Lambda.KinesisAnalyticsEvents/README.md)\n* [Amazon.Lambda.KinesisEvents](Libraries/src/Amazon.Lambda.KinesisEvents) - [README.md](Libraries/src/Amazon.Lambda.KinesisEvents/README.md)\n* [Amazon.Lambda.KinesisFirehoseEvents](Libraries/src/Amazon.Lambda.KinesisFirehoseEvents) - [README.md](Libraries/src/Amazon.Lambda.KinesisFirehoseEvents/README.md)\n* [Amazon.Lambda.S3Events](Libraries/src/Amazon.Lambda.S3Events) - [README.md](Libraries/src/Amazon.Lambda.S3Events/README.md)\n* [Amazon.Lambda.SimpleEmailEvents](Libraries/src/Amazon.Lambda.SimpleEmailEvents) - [README.md](Libraries/src/Amazon.Lambda.SimpleEmailEvents/README.md)\n* [Amazon.Lambda.SNSEvents](Libraries/src/Amazon.Lambda.SNSEvents) - [README.md](Libraries/src/Amazon.Lambda.SNSEvents/README.md)\n* [Amazon.Lambda.SQSEvents](Libraries/src/Amazon.Lambda.SQSEvents) - [README.md](Libraries/src/Amazon.Lambda.SQSEvents/README.md)\n* [Amazon.Lambda.KafkaEvents](Libraries/src/Amazon.Lambda.KafkaEvents) - [README.md](Libraries/src/Amazon.Lambda.KafkaEvents/README.md)\n\n### Amazon.Lambda.Tools\n\nPackage adds commands to the dotnet cli that can be used to manage Lambda functions including deploying a function from the dotnet cli. \nFor more information see the [README.md](Libraries/src/Amazon.Lambda.Tools/README.md) file for Amazon.Lambda.Tools.\n\n#### Global Tool Migration\n\nAs of September 10th, 2018 Amazon.Lambda.Tools has migrated to be .NET Core [Global Tools](https://docs.microsoft.com/en-us/dotnet/core/tools/global-tools).\nAs part of the migration the version number was set to 3.0.0.0\n\nTo install Amazon.Lambda.Tools use the **dotnet tool install** command.\n```\ndotnet tool install -g Amazon.Lambda.Tools\n```\n\nTo update to the latest version of Amazon.Lambda.Tools use the **dotnet tool update** command.\n```\ndotnet tool update -g Amazon.Lambda.Tools\n```\n\n##### Migrating from DotNetCliToolReference\n\nTo migrate an existing project away from the older project tool, you need to edit your project file and remove the **DotNetCliToolReference** for the Amazon.Lambda.Tools package. For example, let's look at an existing Lambda project file.\n```xml\n<Project Sdk=\"Microsoft.NET.Sdk\">\n  <PropertyGroup>\n    <TargetFramework>netcoreapp2.1</TargetFramework>\n    <GenerateRuntimeConfigurationFiles>true</GenerateRuntimeConfigurationFiles>\n\n    <-- The new property indicating to AWS Toolkit for Visual Studio this is a Lambda project -->\n    <AWSProjectType>Lambda</AWSProjectType>\n  </PropertyGroup>\n  \n  <ItemGroup>\n    <-- This line needs to be removed -->\n    <DotNetCliToolReference Include=\"Amazon.Lambda.Tools\" Version=\"2.2.0\" />\n  </ItemGroup>\n\n  <ItemGroup>\n    <PackageReference Include=\"Amazon.Lambda.Core\" Version=\"1.0.0\" />\n    <PackageReference Include=\"Amazon.Lambda.Serialization.Json\" Version=\"2.1.0\" />\n  </ItemGroup>\n</Project>\n```\nTo migrate this project, you need to delete the **DotNetCliToolReference** element, including **Amazon.Lambda.Tools**. If you don't remove this line, the older project tool version of **Amazon.Lambda.Tools** will be used instead of an installed Global Tool.\n\nThe AWS Toolkit for Visual Studio before .NET Core 2.1 would look for the presence of **Amazon.Lambda.Tools** in the project file to determine whether to show the Lambda deployment menu item. Because we knew we were going to switch to Global Tools, and the reference to **Amazon.Lambda.Tools** in the project was going away, we added the **AWSProjectType** property to the project file. The current version of the AWS Toolkit for Visual Studio now looks for either the presence of **Amazon.Lambda.Tools** or the **AWSProjectType** set to **Lambda**. Make sure when removing the **DotNetCliToolReference** that your project file has the **AWSProjectType** property to continue deploying with the AWS Toolkit for Visual Studio.\n\n### Amazon.Lambda.Annotations\n\nThe Lambda Annotations library allows C# functions to use .NET attributes for a more idiomatic experience\nwriting Lambda functions. This includes dependency injection integration, simplified access to Lambda event\ninformation and automatic synchronization with CloudFormation template.\nFor more information see the [README.md](Libraries/src/Amazon.Lambda.Annotations/README.md) file for Amazon.Lambda.Annotations.\n\n### Amazon.Lambda.AspNetCoreServer\n\nPackage makes it easy to run ASP.NET Core Web API applications as Lambda functions.\nFor more information see the [README.md](Libraries/src/Amazon.Lambda.AspNetCoreServer/README.md) file for Amazon.Lambda.AspNetCoreServer.\n\n### Amazon.Lambda.TestUtilities\n\nPackage includes test implementation of the interfaces from Amazon.Lambda.Core and helper methods to help in locally testing.\nFor more information see the [README.md](Libraries/src/Amazon.Lambda.TestUtilities/README.md) file for Amazon.Lambda.TestUtilities.\n\n## Blueprints\n\nBlueprints in this repository are .NET Core Lambda functions that can used to get started. In Visual Studio the Blueprints are available when creating a new project and selecting the AWS Lambda Project.\n\n\n### Dotnet CLI Templates\n\nNew .NET Core projects can be created with the **dotnet new** command. By \ninstalling the **Amazon.Lambda.Templates** NuGet package the AWS Lamdba blueprints \ncan be created from the **dotnet new** command. To install the template execute the following command:\n```\ndotnet new -i \"Amazon.Lambda.Templates::*\"\n```\n\nThe ::* on the end of the command indicates the latest version of the NuGet package.\n\nTo see a list of the Lambda templates execute **dotnet new list --author AWS**\n\n```\n> dotnet new list --author AWS              \n\nTemplate Name                                                                         Short Name                                    Language  Tags                 \n------------------------------------------------------------------------------------  --------------------------------------------  --------  ---------------------\nEmpty Top-level Function                                                              lambda.EmptyTopLevelFunction                  [C#]      AWS/Lambda/Serverless\n\nLambda Annotations Framework (Preview)                                                serverless.Annotations                        [C#]      AWS/Lambda/Serverless\n\nLambda ASP.NET Core Minimal API                                                       serverless.AspNetCoreMinimalAPI               [C#]      AWS/Lambda/Serverless\n\nLambda ASP.NET Core Web API                                                           serverless.AspNetCoreWebAPI                   [C#],F#   AWS/Lambda/Serverless\n\nLambda ASP.NET Core Web API (.NET 6 Container Image)                                  serverless.image.AspNetCoreWebAPI             [C#],F#   AWS/Lambda/Serverless\n\nLambda ASP.NET Core Web Application with Razor Pages                                  serverless.AspNetCoreWebApp                   [C#]      AWS/Lambda/Serverless\n\nLambda Custom Runtime Function (.NET 7)                                               lambda.CustomRuntimeFunction                  [C#],F#   AWS/Lambda/Function  \n\nLambda Detect Image Labels                                                            lambda.DetectImageLabels                      [C#],F#   AWS/Lambda/Function\n\nLambda Empty Function                                                                 lambda.EmptyFunction                          [C#],F#   AWS/Lambda/Function\n\nLambda Empty Function (.NET 7 Container Image)                                        lambda.image.EmptyFunction                    [C#],F#   AWS/Lambda/Function\n\nLambda Empty Serverless                                                               serverless.EmptyServerless                    [C#],F#   AWS/Lambda/Serverless\n\nLambda Empty Serverless (.NET 7 Container Image)                                      serverless.image.EmptyServerless              [C#],F#   AWS/Lambda/Serverless\n\nLambda Function project configured for deployment using .NET 7's Native AOT feature.  lambda.NativeAOT                              [C#],F#   AWS/Lambda/Function\n\nLambda Giraffe Web App                                                                serverless.Giraffe                            F#        AWS/Lambda/Serverless\n\nLambda Simple Application Load Balancer Function                                      lambda.SimpleApplicationLoadBalancerFunction  [C#]      AWS/Lambda/Function\n\nLambda Simple DynamoDB Function                                                       lambda.DynamoDB                               [C#],F#   AWS/Lambda/Function\n\nLambda Simple Kinesis Firehose Function                                               lambda.KinesisFirehose                        [C#]      AWS/Lambda/Function\n\nLambda Simple Kinesis Function                                                        lambda.Kinesis                                [C#],F#   AWS/Lambda/Function\n\nLambda Simple S3 Function                                                             lambda.S3                                     [C#],F#   AWS/Lambda/Function\n\nLambda Simple SNS Function                                                            lambda.SNS                                    [C#]      AWS/Lambda/Function\n\nLambda Simple SQS Function                                                            lambda.SQS                                    [C#]      AWS/Lambda/Function\n\nLex Book Trip Sample                                                                  lambda.LexBookTripSample                      [C#]      AWS/Lambda/Function\n\nOrder Flowers Chatbot Tutorial                                                        lambda.OrderFlowersChatbot                    [C#]      AWS/Lambda/Function\n\nServerless Detect Image Labels                                                        serverless.DetectImageLabels                  [C#],F#   AWS/Lambda/Serverless\n\nServerless project configured for deployment using .NET 7's Native AOT feature.       serverless.NativeAOT                          [C#],F#   AWS/Lambda/Serverless\n\nServerless Simple S3 Function                                                         serverless.S3                                 [C#],F#   AWS/Lambda/Serverless\n\nServerless WebSocket API                                                              serverless.WebSocketAPI                       [C#]      AWS/Lambda/Serverless\n\nStep Functions Hello World                                                            serverless.StepFunctionsHelloWorld            [C#],F#   AWS/Lambda/Serverless\n```\n\nTo get details about a template, you can use the help command.\n\n**dotnet new lambda.EmptyFunction --help**\n\n```\nTemplate Instantiation Commands for .NET Core CLI.                                                                                          \n                                                                                                                                           \nLambda Empty Function (C#)                                                                                                                  \nAuthor: AWS                                                                                                                                 \nOptions:                                                                                                                                    \n  -p|--profile  The AWS credentials profile set in aws-lambda-tools-defaults.json and used as the default profile when interacting with AWS.\n                string - Optional                                                                                                           \n                                                                                                                                           \n  -r|--region   The AWS region set in aws-lambda-tools-defaults.json and used as the default region when interacting with AWS.              \n                string - Optional  \n```\n\nThe templates take two optional parameters to set the profile and region. These values are written to the aws-lambda-tools-default.json.\n\nTo create a function, run the following command\n\n```\ndotnet new lambda.EmptyFunction --name BlogFunction --profile default --region us-east-2\n```\n\n### Yeoman (Deprecated)\n\nThe Yeoman generators have been deprecated in favor of the new **dotnet new** templates. They will not be migrated from the older project.json based project system.\n\n## Getting Help\nTo learn more about the various packages in this repo, please reference our [Learning Resources](./Docs/Learning_Resources.md) document. In particular, please be sure to read through the official [Lambda Developer Guide](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html).\n\nIf those resources are not sufficient to answer your question or resolve your issue, please feel free to open an [issue](https://github.com/aws/aws-lambda-dotnet/issues/new/choose) on this repo.\n\n## Feedback and Contributing\nWe welcome community contributions to our codebase and tools! If you would like to contribute, please read through the [CONTRIBUTING.md](./CONTRIBUTING.md) document (our contribution guide) and check issues and open pull requests to ensure that the fix/feature you want to contribute is not already in development.\n", "release_dates": []}, {"name": "aws-lambda-go", "description": "Libraries, samples and tools to help Go developers develop AWS Lambda functions.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Lambda for Go \n\n[![tests][1]][2]\n[![build-lambda-zip][3]][4]\n[![Go Reference][5]][6]\n[![GoCard][7]][8]\n[![codecov][9]][10]\n\n[1]: https://github.com/aws/aws-lambda-go/workflows/tests/badge.svg\n[2]: https://github.com/aws/aws-lambda-go/actions?query=workflow%3Atests\n[3]: https://github.com/aws/aws-lambda-go/workflows/go%20get%20build-lambda-zip/badge.svg\n[4]: https://github.com/aws/aws-lambda-go/actions?query=workflow%3A%22go+get+build-lambda-zip%22\n[5]: https://pkg.go.dev/badge/github.com/aws/aws-lambda-go.svg\n[6]: https://pkg.go.dev/github.com/aws/aws-lambda-go\n[7]: https://goreportcard.com/badge/github.com/aws/aws-lambda-go\n[8]: https://goreportcard.com/report/github.com/aws/aws-lambda-go\n[9]: https://codecov.io/gh/aws/aws-lambda-go/branch/master/graph/badge.svg\n[10]: https://codecov.io/gh/aws/aws-lambda-go\n\nLibraries, samples, and tools to help Go developers develop AWS Lambda functions.\n\nTo learn more about writing AWS Lambda functions in Go, go to [the official documentation](https://docs.aws.amazon.com/lambda/latest/dg/go-programming-model.html)\n\n# Getting Started\n\n``` Go\n// main.go\npackage main\n\nimport (\n\t\"github.com/aws/aws-lambda-go/lambda\"\n)\n\nfunc hello() (string, error) {\n\treturn \"Hello \u03bb!\", nil\n}\n\nfunc main() {\n\t// Make the handler available for Remote Procedure Call by AWS Lambda\n\tlambda.Start(hello)\n}\n```\n\n# Building your function\n\nPreparing a binary to deploy to AWS Lambda requires that it is compiled for Linux and placed into a .zip file. When using the `provided`, `provided.al2`, or `provided.al2023` runtime, the executable within the .zip file should be named `bootstrap`. Lambda's default architecture is `x86_64`, so when cross compiling from a non-x86 environment, the executable should be built with `GOARCH=amd64`. Likewise, if the Lambda function will be [configured to use ARM](https://docs.aws.amazon.com/lambda/latest/dg/foundation-arch.html), the executable should built with `GOARCH=arm64`.\n\n``` shell\nGOOS=linux GOARCH=amd64 go build -o bootstrap main.go\nzip lambda-handler.zip bootstrap\n```\n\n## For developers on Linux\n\nOn Linux, the Go compiler's default behavior is to link the output executable to the system libc for some standard library functionality (for example, DNS lookups). If the build environment is using a Linux distribution with a GNU libc version newer than the deployment environment, the application when deployed to Lambda may fail with an error like ``/lib64/libc.so.6: version `GLIBC_X.YZ' not found``. \n\nMost Go applications do not require linking to the system libc. This behavior can be disabled by using the `CGO_ENABLED` environment variable.\n\n```\nCGO_ENABLED=0 go build -o bootstrap main.go\nzip lambda-handler.zip bootstrap\n```\n\nSee [Using CGO](#using-cgo)\n\n## For developers on Windows\n\nWindows developers may have trouble producing a zip file that marks the binary as executable on Linux. To create a .zip that will work on AWS Lambda, the `build-lambda-zip` tool may be helpful.\n\nGet the tool\n``` shell\ngo.exe install github.com/aws/aws-lambda-go/cmd/build-lambda-zip@latest\n```\n\nUse the tool from your `GOPATH`. If you have a default installation of Go, the tool will be in `%USERPROFILE%\\Go\\bin`. \n\nin cmd.exe:\n``` bat\nset GOOS=linux\nset GOARCH=amd64\nset CGO_ENABLED=0\ngo build -o bootstrap main.go\n%USERPROFILE%\\Go\\bin\\build-lambda-zip.exe -o lambda-handler.zip bootstrap\n```\n\nin Powershell:\n``` posh\n$env:GOOS = \"linux\"\n$env:GOARCH = \"amd64\"\n$env:CGO_ENABLED = \"0\"\ngo build -o bootstrap main.go\n~\\Go\\Bin\\build-lambda-zip.exe -o lambda-handler.zip bootstrap\n```\n\n## Using CGO\n\nFor applications that require CGO, the build environment must be using a GNU libc version installed compatible with the target Lambda runtime. Otherwise, execution may fail with errors like ``/lib64/libc.so.6: version `GLIBC_X.YZ' not found``.\n\n| Lambda runtime  | GLIBC version\n| ----- | ---\n| `provided.al2023` | 2.34\n| `provided.al2` | 2.26\n| `provided` and `go1.x` | 2.17\n\n\nAlternatively, Lambda supports container images as a deployment package alternative to .zip files. For more information, refer to the official documentation for [working with with container images](https://docs.aws.amazon.com/lambda/latest/dg/images-create.html).\n\n# Deploying your functions\n\nTo deploy your function, refer to the official documentation for [deploying using the AWS CLI, AWS Cloudformation, and AWS SAM](https://docs.aws.amazon.com/lambda/latest/dg/deploying-lambda-apps.html).\n\n# Event Integrations\n\nThe [event models](https://github.com/aws/aws-lambda-go/tree/master/events) can be used to model AWS event sources. The official documentation has [detailed walkthroughs](https://docs.aws.amazon.com/lambda/latest/dg/use-cases.html).\n\n", "release_dates": ["2024-01-29T21:20:10Z", "2024-01-18T18:21:56Z", "2024-01-08T18:09:26Z", "2023-12-20T00:40:37Z", "2023-12-11T19:25:09Z", "2023-05-04T20:12:20Z", "2023-04-12T23:06:38Z", "2023-03-22T21:35:26Z", "2023-03-22T20:36:15Z", "2023-03-02T20:44:16Z", "2023-01-09T19:06:08Z", "2022-12-22T23:40:00Z", "2022-12-05T18:39:48Z", "2022-11-16T19:50:19Z", "2022-07-31T08:58:34Z", "2022-07-28T20:40:48Z", "2022-07-19T02:27:32Z", "2022-06-30T05:22:45Z", "2022-05-19T00:38:25Z", "2022-04-25T19:35:01Z", "2022-04-13T23:38:53Z", "2022-03-29T21:39:23Z", "2022-01-07T00:56:19Z", "2021-12-10T13:05:17Z", "2021-09-27T18:07:10Z", "2021-07-30T08:47:31Z", "2021-07-15T01:19:06Z", "2021-05-22T03:19:14Z", "2021-03-03T17:13:34Z", "2020-12-30T23:54:38Z"]}, {"name": "aws-lambda-java-libs", "description": "Official mirror for interface definitions and helper classes for Java code running on the AWS Lambda platform.", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Lambda Java Support Libraries\nKey libraries for running Java on the AWS Lambda platform.\n\nFor issues and questions, you can start with our [FAQ](https://aws.amazon.com/lambda/faqs/)\nand the AWS questions and answer site [re:Post](https://repost.aws/tags/TA5uNafDy2TpGNjidWLMSxDw/aws-lambda)\n\nTo get started writing Lambda functions in Java, check out the official [developer guide](https://docs.aws.amazon.com/lambda/latest/dg/lambda-java.html).\n\nFor information on how to optimize your functions watch the re:Invent talk [Optimize your Java application on AWS Lambda](https://www.youtube.com/watch?v=sVJOJUD0fhQ).\n\n## Core Java Lambda interfaces - aws-lambda-java-core\n\nThis package defines the Lambda [Context](http://docs.aws.amazon.com/lambda/latest/dg/java-context-object.html) object\nas well as [interfaces](http://docs.aws.amazon.com/lambda/latest/dg/java-handler-using-predefined-interfaces.html) that Lambda accepts.\n\n- [Release Notes](aws-lambda-java-core/RELEASE.CHANGELOG.md)\n\nExample request handler\n\n```java\npublic class Handler implements RequestHandler<Map<String, String>, String>{\n @Override\n public String handleRequest(Map<String, String> event, Context context) {\n\n }\n}\n```\n\nExample request stream handler\n\n```java\npublic class HandlerStream implements RequestStreamHandler {\n  @Override\n  public void handleRequest(InputStream inputStream, OutputStream outputStream, Context context) throws IOException {\n\n  }\n}\n```\n\n```xml\n<dependency>\n <groupId>com.amazonaws</groupId>\n <artifactId>aws-lambda-java-core</artifactId>\n <version>1.2.3</version>\n</dependency>\n```\n\n## Java objects of Lambda event sources - aws-lambda-java-events\n\nThis package defines [event sources](http://docs.aws.amazon.com/lambda/latest/dg/intro-invocation-modes.html) that Lambda natively accepts.\nSee the [documentation](aws-lambda-java-events/README.md) for a list of currently supported event sources.\nUsing this library you can have Java objects which represent event sources.\n\nFor example an SQS event:\n\n```java\nimport com.amazonaws.services.lambda.runtime.events.SQSEvent;\n\npublic class SqsHandler implements RequestHandler<SQSEvent, String> {\n\n @Override\n public String handleRequest(SQSEvent event, Context context) {\n\n }\n}\n```\n\n- [Release Notes](aws-lambda-java-events/RELEASE.CHANGELOG.md)\n\n```xml\n<dependency>\n <groupId>com.amazonaws</groupId>\n <artifactId>aws-lambda-java-events</artifactId>\n <version>3.11.4</version>\n</dependency>\n```\n\n## Java Lambda JUnit Support - aws-lambda-java-tests\n\nThis package provides utils to ease Lambda Java testing. It uses the same Lambda serialisation logic and `aws-lambda-java-events` to inject events in your JUnit tests.\n\n- [Release Notes](aws-lambda-java-tests/RELEASE.CHANGELOG.md)\n\n```java\n@ParameterizedTest\n@Event(value = \"sqs/sqs_event.json\", type = SQSEvent.class)\npublic void testInjectSQSEvent(SQSEvent event) {\n        ...\n}\n```\n\n```xml\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-lambda-java-tests</artifactId>\n  <version>1.1.1</version>\n  <scope>test</scope>\n</dependency>\n```\n\n## aws-lambda-java-events-sdk-transformer\n\nThis package provides helper classes/methods to use alongside `aws-lambda-java-events` in order to transform\nLambda input event model objects into SDK-compatible output model objects.\nSee the [documentation](aws-lambda-java-events-sdk-transformer/README.md) for more information.\n\n- [Release Notes](aws-lambda-java-events-sdk-transformer/RELEASE.CHANGELOG.md)\n\n```xml\n<dependency>\n <groupId>com.amazonaws</groupId>\n <artifactId>aws-lambda-java-events-sdk-transformer</artifactId>\n <version>3.1.0</version>\n</dependency>\n```\n\n## Java Lambda Log4J2 support - aws-lambda-java-log4j2\n\nThis package defines the Lambda adapter to use with Log4J version 2.\nSee the [README](aws-lambda-java-log4j2/README.md) or the [official documentation](http://docs.aws.amazon.com/lambda/latest/dg/java-logging.html#java-wt-logging-using-log4j) for information on how to use the adapter.\n\n- [Release Notes](aws-lambda-java-log4j2/RELEASE.CHANGELOG.md)\n\n```xml\n<dependency>\n <groupId>com.amazonaws</groupId>\n <artifactId>aws-lambda-java-log4j2</artifactId>\n <version>1.6.0</version>\n</dependency>\n```\n\n## Java implementation of the Runtime Interface Client API - aws-lambda-java-runtime-interface-client\n\nThis package defines the Lambda Java Runtime Interface Client package, a Lambda Runtime component that starts the runtime and interacts with the Runtime API - i.e., it calls the API for invocation events, starts the function code, calls the API to return the response.\nThe purpose of this package is to allow developers to deploy their applications in Lambda under the form of Container Images. See the [README](aws-lambda-java-runtime-interface-client/README.md) for information on how to use the library.\n\n- [Release Notes](aws-lambda-java-runtime-interface-client/RELEASE.CHANGELOG.md)\n\n```xml\n<dependency>\n <groupId>com.amazonaws</groupId>\n <artifactId>aws-lambda-java-runtime-interface-client</artifactId>\n <version>2.4.0</version>\n</dependency>\n```\n\n## Java Lambda provided serialization support - aws-lambda-java-serialization\n\nThis package defines the Lambda serialization logic using in the `aws-lambda-java-runtime-client` library. It has no current standalone usage.\n\n- [Release Notes](aws-lambda-java-serialization/RELEASE.CHANGELOG.md)\n\n```xml\n<dependency>\n <groupId>com.amazonaws</groupId>\n <artifactId>aws-lambda-java-serialization</artifactId>\n <version>1.1.5</version>\n</dependency>\n```\n\n## Disclaimer of use\n\nEach of the supplied packages should be used without modification. Removing\ndependencies, adding conflicting dependencies, or selectively including classes\nfrom the packages can result in unexpected behavior.\n", "release_dates": []}, {"name": "aws-lambda-nodejs-runtime-interface-client", "description": null, "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS Lambda NodeJS Runtime Interface Client\n\nWe have open-sourced a set of software packages, Runtime Interface Clients (RIC), that implement the Lambda\n [Runtime API](https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html), allowing you to seamlessly extend your preferred\n  base images to be Lambda compatible.\nThe Lambda Runtime Interface Client is a lightweight interface that allows your runtime to receive requests from and send requests to the Lambda service.\n\nThe Lambda NodeJS Runtime Interface Client is vended through [npm](https://www.npmjs.com/package/aws-lambda-ric). \nYou can include this package in your preferred base image to make that base image Lambda compatible.\n\n## Requirements\nThe NodeJS Runtime Interface Client package currently supports NodeJS versions:\n - 16.x\n - 18.x\n - 20.x\n\n## Usage\n\n### Creating a Docker Image for Lambda with the Runtime Interface Client\nFirst step is to choose the base image to be used. The supported Linux OS distributions are:\n\n - Amazon Linux (2 and 2023)\n - Alpine\n - CentOS\n - Debian\n - Ubuntu\n\nThe Runtime Interface Client can be installed outside of the Dockerfile as a dependency of the function we want to run in Lambda (run the below command in your function directory to add the dependency to `package.json`):\n```shell script\nnpm install aws-lambda-ric --save\n```\nor inside the Dockerfile:\n```dockerfile\nRUN npm install aws-lambda-ric\n```\n\nNext step would be to copy your Lambda function code into the image's working directory.\n```dockerfile\n# Copy function code\nRUN mkdir -p ${FUNCTION_DIR}\nCOPY myFunction/* ${FUNCTION_DIR}\n\nWORKDIR ${FUNCTION_DIR}\n\n# If the dependency is not in package.json uncomment the following line\n# RUN npm install aws-lambda-ric\n\nRUN npm install\n```\n\nThe next step would be to set the `ENTRYPOINT` property of the Docker image to invoke the Runtime Interface Client and then set the `CMD` argument to specify the desired handler.\n\nExample Dockerfile (to keep the image light we used a multi-stage build):\n```dockerfile\n# Define custom function directory\nARG FUNCTION_DIR=\"/function\"\n\nFROM node:18-buster as build-image\n\n# Include global arg in this stage of the build\nARG FUNCTION_DIR\n\n# Install aws-lambda-cpp build dependencies\nRUN apt-get update && \\\n    apt-get install -y \\\n    g++ \\\n    make \\\n    cmake \\\n    unzip \\\n    libcurl4-openssl-dev\n\n# Copy function code\nRUN mkdir -p ${FUNCTION_DIR}\nCOPY myFunction/* ${FUNCTION_DIR}\n\nWORKDIR ${FUNCTION_DIR}\n\nRUN npm install\n\n# If the dependency is not in package.json uncomment the following line\n# RUN npm install aws-lambda-ric\n\n# Grab a fresh slim copy of the image to reduce the final size\nFROM node:18-buster-slim\n\n# Required for Node runtimes which use npm@8.6.0+ because\n# by default npm writes logs under /home/.npm and Lambda fs is read-only\nENV NPM_CONFIG_CACHE=/tmp/.npm\n\n# Include global arg in this stage of the build\nARG FUNCTION_DIR\n\n# Set working directory to function root directory\nWORKDIR ${FUNCTION_DIR}\n\n# Copy in the built dependencies\nCOPY --from=build-image ${FUNCTION_DIR} ${FUNCTION_DIR}\n\nENTRYPOINT [\"/usr/local/bin/npx\", \"aws-lambda-ric\"]\nCMD [\"app.handler\"]\n```\n\nExample NodeJS handler `app.js`:\n```js\n\"use strict\";\n\nexports.handler = async (event, context) => {\n    return 'Hello World!';\n}\n```\n\n### Local Testing\n\nTo make it easy to locally test Lambda functions packaged as container images we open-sourced a lightweight web-server, Lambda Runtime Interface Emulator (RIE), which allows your function packaged as a container image to accept HTTP requests. You can install the [AWS Lambda Runtime Interface Emulator](https://github.com/aws/aws-lambda-runtime-interface-emulator) on your local machine to test your function. Then when you run the image function, you set the entrypoint to be the emulator. \n\n*To install the emulator and test your Lambda function*\n\n1) From your project directory, run the following command to download the RIE from GitHub and install it on your local machine. \n\n```shell script\nmkdir -p ~/.aws-lambda-rie && \\\n    curl -Lo ~/.aws-lambda-rie/aws-lambda-rie https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie && \\\n    chmod +x ~/.aws-lambda-rie/aws-lambda-rie\n```\n2) Run your Lambda image function using the docker run command. \n\n```shell script\ndocker run -d -v ~/.aws-lambda-rie:/aws-lambda -p 9000:8080 \\\n    --entrypoint /aws-lambda/aws-lambda-rie \\\n    myfunction:latest \\\n        /usr/local/bin/npx aws-lambda-ric app.handler\n```\n\nThis runs the image as a container and starts up an endpoint locally at `http://localhost:9000/2015-03-31/functions/function/invocations`. \n\n3) Post an event to the following endpoint using a curl command: \n\n```shell script\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'\n```\n\nThis command invokes the function running in the container image and returns a response.\n\n*Alternately, you can also include RIE as a part of your base image. See the AWS documentation on how to [Build RIE into your base image](https://docs.aws.amazon.com/lambda/latest/dg/images-test.html#images-test-alternative).*\n\n\n## Development\n\n### Building the package\nClone this repository and run:\n\n```shell script\nmake init\nmake build\n```\n\n### Running tests\n\nMake sure the project is built:\n```shell script\nmake init build\n```\nThen,\n* to run unit tests: `make test`\n* to run integration tests: `make test-integ`\n* to run smoke tests: `make test-smoke`\n\n### Troubleshooting\n\nWhile running integration tests, you might encounter the Docker Hub rate limit error with the following body:\n```\nYou have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limits\n```\nTo fix the above issue, consider authenticating to a Docker Hub account by setting the Docker Hub credentials as below CodeBuild environment variables.\n```shell script\nDOCKERHUB_USERNAME=<dockerhub username>\nDOCKERHUB_PASSWORD=<dockerhub password>\n```\nRecommended way is to set the Docker Hub credentials in CodeBuild job by retrieving them from AWS Secrets Manager.\n## Security\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2023-11-09T10:23:09Z", "2023-06-26T09:22:21Z", "2023-05-15T11:24:01Z", "2021-09-29T19:38:45Z", "2021-06-09T21:46:02Z", "2020-12-01T16:51:19Z"]}, {"name": "aws-lambda-python-runtime-interface-client", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS Lambda Python Runtime Interface Client\n\nWe have open-sourced a set of software packages, Runtime Interface Clients (RIC), that implement the Lambda\n [Runtime API](https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html), allowing you to seamlessly extend your preferred\n  base images to be Lambda compatible.\nThe Lambda Runtime Interface Client is a lightweight interface that allows your runtime to receive requests from and send requests to the Lambda service.\n\nThe Lambda Python Runtime Interface Client is vended through [pip](https://pypi.org/project/awslambdaric). \nYou can include this package in your preferred base image to make that base image Lambda compatible.\n\n## Requirements\nThe Python Runtime Interface Client package currently supports Python versions:\n - 3.7.x up to and including 3.12.x\n\n## Usage\n\n### Creating a Docker Image for Lambda with the Runtime Interface Client\nFirst step is to choose the base image to be used. The supported Linux OS distributions are:\n\n - Amazon Linux 2\n - Alpine\n - CentOS\n - Debian\n - Ubuntu\n\n\nThen, the Runtime Interface Client needs to be installed. We provide both wheel and source distribution.\nIf the OS/pip version used does not support [manylinux2014](https://www.python.org/dev/peps/pep-0599/) wheels, you will also need to install the required build dependencies.\nAlso, your Lambda function code needs to be copied into the image.\n\n```dockerfile\n# Include global arg in this stage of the build\nARG FUNCTION_DIR\n\n# Install aws-lambda-cpp build dependencies\nRUN apt-get update && \\\n  apt-get install -y \\\n  g++ \\\n  make \\\n  cmake \\\n  unzip \\\n  libcurl4-openssl-dev\n\n# Copy function code\nRUN mkdir -p ${FUNCTION_DIR}\nCOPY app/* ${FUNCTION_DIR}\n\n# Install the function's dependencies\nRUN pip install \\\n    --target ${FUNCTION_DIR} \\\n        awslambdaric\n```\n\nThe next step would be to set the `ENTRYPOINT` property of the Docker image to invoke the Runtime Interface Client and then set the `CMD` argument to specify the desired handler.\n\nExample Dockerfile (to keep the image light we use a multi-stage build):\n```dockerfile\n# Define custom function directory\nARG FUNCTION_DIR=\"/function\"\n\nFROM public.ecr.aws/docker/library/python:buster as build-image\n\n# Include global arg in this stage of the build\nARG FUNCTION_DIR\n\n# Install aws-lambda-cpp build dependencies\nRUN apt-get update && \\\n  apt-get install -y \\\n  g++ \\\n  make \\\n  cmake \\\n  unzip \\\n  libcurl4-openssl-dev\n\n# Copy function code\nRUN mkdir -p ${FUNCTION_DIR}\nCOPY app/* ${FUNCTION_DIR}\n\n# Install the function's dependencies\nRUN pip install \\\n    --target ${FUNCTION_DIR} \\\n        awslambdaric\n\n\nFROM public.ecr.aws/docker/library/python:buster\n\n# Include global arg in this stage of the build\nARG FUNCTION_DIR\n# Set working directory to function root directory\nWORKDIR ${FUNCTION_DIR}\n\n# Copy in the built dependencies\nCOPY --from=build-image ${FUNCTION_DIR} ${FUNCTION_DIR}\n\nENTRYPOINT [ \"/usr/local/bin/python\", \"-m\", \"awslambdaric\" ]\nCMD [ \"app.handler\" ]\n```\n\nExample Python handler `app.py`:\n```python\ndef handler(event, context):\n    return \"Hello World!\"\n```\n\n### Local Testing\n\nTo make it easy to locally test Lambda functions packaged as container images we open-sourced a lightweight web-server, Lambda Runtime Interface Emulator (RIE), which allows your function packaged as a container image to accept HTTP requests. You can install the [AWS Lambda Runtime Interface Emulator](https://github.com/aws/aws-lambda-runtime-interface-emulator) on your local machine to test your function. Then when you run the image function, you set the entrypoint to be the emulator. \n\n*To install the emulator and test your Lambda function*\n\n1) From your project directory, run the following command to download the RIE from GitHub and install it on your local machine. \n\n```shell script\nmkdir -p ~/.aws-lambda-rie && \\\n    curl -Lo ~/.aws-lambda-rie/aws-lambda-rie https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie && \\\n    chmod +x ~/.aws-lambda-rie/aws-lambda-rie\n```\n2) Run your Lambda image function using the docker run command. \n\n```shell script\ndocker run -d -v ~/.aws-lambda-rie:/aws-lambda -p 9000:8080 \\\n    --entrypoint /aws-lambda/aws-lambda-rie \\\n    myfunction:latest \\\n        /usr/local/bin/python -m awslambdaric app.handler\n```\n\nThis runs the image as a container and starts up an endpoint locally at `http://localhost:9000/2015-03-31/functions/function/invocations`. \n\n3) Post an event to the following endpoint using a curl command: \n\n```shell script\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'\n```\n\nThis command invokes the function running in the container image and returns a response.\n\n*Alternately, you can also include RIE as a part of your base image. See the AWS documentation on how to [Build RIE into your base image](https://docs.aws.amazon.com/lambda/latest/dg/images-test.html#images-test-alternative).*\n\n\n## Development\n\n### Building the package\nClone this repository and run:\n\n```shell script\nmake init\nmake build\n```\n\n### Running tests\n\nMake sure the project is built:\n```shell script\nmake init build\n```\nThen,\n* to run unit tests: `make test`\n* to run integration tests: `make test-integ`\n* to run smoke tests: `make test-smoke`\n\n### Troubleshooting\nWhile running integration tests, you might encounter the Docker Hub rate limit error with the following body:\n```\nYou have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limits\n```\nTo fix the above issue, consider authenticating to a Docker Hub account by setting the Docker Hub credentials as below CodeBuild environment variables.\n```shell script\nDOCKERHUB_USERNAME=<dockerhub username>\nDOCKERHUB_PASSWORD=<dockerhub password>\n```\nRecommended way is to set the Docker Hub credentials in CodeBuild job by retrieving them from AWS Secrets Manager.\n## Security\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.", "release_dates": ["2024-02-13T16:52:52Z", "2024-02-12T12:56:00Z", "2023-10-30T17:36:07Z", "2023-08-31T12:32:03Z", "2023-08-22T14:10:42Z", "2023-08-16T13:47:08Z", "2021-09-29T20:18:05Z", "2021-08-23T12:36:54Z", "2021-06-28T18:25:09Z", "2021-06-09T21:55:34Z", "2021-06-01T12:31:36Z", "2020-12-01T18:26:01Z"]}, {"name": "aws-lambda-ruby-runtime-interface-client", "description": null, "language": "Ruby", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## AWS Lambda Ruby Runtime Interface Client\n\nWe have open-sourced a set of software packages, Runtime Interface Clients (RIC), that implement the Lambda\n [Runtime API](https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html), allowing you to seamlessly extend your preferred\n  base images to be Lambda compatible.\nThe Lambda Runtime Interface Client is a lightweight interface that allows your runtime to receive requests from and send requests to the Lambda service.\n\nThe Lambda Ruby Runtime Interface Client is vended through [rubygems](https://rubygems.org/gems/aws_lambda_ric). \nYou can include this package in your preferred base image to make that base image Lambda compatible.\n\n## Requirements\nThe Ruby Runtime Interface Client package currently supports Ruby versions:\n - 2.5.x up to and including 2.7.x\n \n## Usage\n\n### Creating a Docker Image for Lambda with the Runtime Interface Client\nFirst step is to choose the base image to be used. The supported Linux OS distributions are:\n\n - Amazon Linux 2\n - Alpine\n - CentOS\n - Debian\n - Ubuntu\n\nIn order to install the Runtime Interface Client, either add this line to your application's Gemfile:\n\n```ruby\ngem 'aws_lambda_ric'\n```\n\nAnd then execute:\n\n    $ bundle\n\nOr install it manually as:\n\n    $ gem install aws_lambda_ric\n\n\nNext step would be to copy your Lambda function code into the image's working directory.\n```dockerfile\n# Copy function code\nRUN mkdir -p ${FUNCTION_DIR}\nCOPY app.rb ${FUNCTION_DIR}\n\nWORKDIR ${FUNCTION_DIR}\n```\n\nThe next step would be to set the `ENTRYPOINT` property of the Docker image to invoke the Runtime Interface Client and then set the `CMD` argument to specify the desired handler.\n\nExample Dockerfile:\n```dockerfile\nFROM amazonlinux:latest\n\n# Define custom function directory\nARG FUNCTION_DIR=\"/function\"\n\n# Install ruby\nRUN amazon-linux-extras install -y ruby2.6\n\n# Install bundler\nRUN gem install bundler\n\n# Install the Runtime Interface Client\nRUN gem install aws_lambda_ric\n\n# Copy function code\nRUN mkdir -p ${FUNCTION_DIR}\nCOPY app.rb ${FUNCTION_DIR}\n\nWORKDIR ${FUNCTION_DIR}\n\nENTRYPOINT [\"/usr/local/bin/aws_lambda_ric\"]\nCMD [\"app.App::Handler.process\"]\n```\n\nExample Ruby handler `app.rb`:\n```ruby\nmodule App\n  class Handler\n    def self.process(event:, context:)\n      \"Hello World!\"\n    end\n  end\nend\n```\n\n### Local Testing\n\nTo make it easy to locally test Lambda functions packaged as container images we open-sourced a lightweight web-server, Lambda Runtime Interface Emulator (RIE), which allows your function packaged as a container image to accept HTTP requests. You can install the [AWS Lambda Runtime Interface Emulator](https://github.com/aws/aws-lambda-runtime-interface-emulator) on your local machine to test your function. Then when you run the image function, you set the entrypoint to be the emulator. \n\n*To install the emulator and test your Lambda function*\n\n1) From your project directory, run the following command to download the RIE from GitHub and install it on your local machine. \n\n```shell script\nmkdir -p ~/.aws-lambda-rie && \\\n    curl -Lo ~/.aws-lambda-rie/aws-lambda-rie https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie && \\\n    chmod +x ~/.aws-lambda-rie/aws-lambda-rie\n```\n2) Run your Lambda image function using the docker run command. \n\n```shell script\ndocker run -d -v ~/.aws-lambda-rie:/aws-lambda -p 9000:8080 \\\n    --entrypoint /aws-lambda/aws-lambda-rie \\\n    myfunction:latest \\\n        /usr/local/bin/aws_lambda_ric app.App::Handler.process\n```\n\nThis runs the image as a container and starts up an endpoint locally at `http://localhost:9000/2015-03-31/functions/function/invocations`. \n\n3) Post an event to the following endpoint using a curl command: \n\n```shell script\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'\n```\n\nThis command invokes the function running in the container image and returns a response.\n\n*Alternately, you can also include RIE as a part of your base image. See the AWS documentation on how to [Build RIE into your base image](https://docs.aws.amazon.com/lambda/latest/dg/images-test.html#images-test-alternative).*\n\n## Development\n\n### Building the package\nClone this repository and run:\n\n```shell script\nmake init\nmake build\n```\n\n### Running tests\n\nMake sure the project is built:\n```shell script\nmake init build\n```\nThen,\n* to run unit tests: `make test-unit`\n* to run integration tests: `make test-integ`\n* to run smoke tests: `make test-smoke`\n\n### Troubleshooting\nWhile running integration tests, you might encounter the Docker Hub rate limit error with the following body:\n```\nYou have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limits\n```\nTo fix the above issue, consider authenticating to a Docker Hub account by setting the Docker Hub credentials as below CodeBuild environment variables.\n```shell script\nDOCKERHUB_USERNAME=<dockerhub username>\nDOCKERHUB_PASSWORD=<dockerhub password>\n```\nRecommended way is to set the Docker Hub credentials in CodeBuild job by retrieving them from AWS Secrets Manager.\n\n## Security\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.", "release_dates": ["2021-09-29T20:20:02Z", "2020-12-18T08:56:52Z", "2020-12-01T22:06:51Z", "2020-12-01T17:42:26Z"]}, {"name": "aws-lambda-runtime-interface-emulator", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS Lambda Runtime Interface Emulator\n![GitHub release (latest by date)](https://img.shields.io/github/v/release/aws/aws-lambda-runtime-interface-emulator)\n![GitHub go.mod Go version](https://img.shields.io/github/go-mod/go-version/aws/aws-lambda-runtime-interface-emulator)\n![GitHub](https://img.shields.io/github/license/aws/aws-lambda-runtime-interface-emulator)\n\n\nThe Lambda Runtime Interface Emulator is a proxy for Lambda\u2019s Runtime and Extensions APIs, which allows customers to\nlocally test their Lambda function packaged as a container image. It is a lightweight web-server that converts\nHTTP requests to JSON events and maintains functional parity with the Lambda Runtime API in the cloud. It\nallows you to locally test your functions using familiar tools such as cURL and the Docker CLI (when testing\nfunctions packaged as container images). It also simplifies running your application on additional computes.\nYou can include the Lambda Runtime Interface Emulator in your container image to have it accept HTTP\nrequests instead of the JSON events required for deployment to Lambda. This component does not emulate\nLambda\u2019s orchestrator, or security and authentication configurations. You can get started by downloading and installing it on your local machine. When the Lambda Runtime API emulator is executed, a `/2015-03-31/functions/function/invocations` endpoint will be stood up within the container that you post data to it in order to invoke your function for testing.\n\n\n## Content\n* [Installing](#installing)\n* [Getting started](#getting-started)\n  * [Test an image with RIE included in the image](#test-an-image-with-rie-included-in-the-image)\n    * [To test your Lambda function with the emulator](#to-test-your-lambda-function-with-the-emulator)\n  * [Build RIE into your base image](#build-rie-into-your-base-image)\n    * [To build the emulator into your image](#to-build-the-emulator-into-your-image)\n  * [Test an image without adding RIE to the image](#test-an-image-without-adding-rie-to-the-image)\n    * [To test an image without adding RIE to the image](#to-test-an-image-without-adding-rie-to-the-image)\n* [How to configure](#how-to-configure)\n* [Level of support](#level-of-support)\n* [Security](#security)\n* [License](#license)\n\n\n## Installing\n\nInstructions for installing AWS Lambda Runtime Interface Emulator for your platform\n\n| Platform | Command to install |\n|---------|---------\n| macOS/Linux x86\\_64 | `mkdir -p ~/.aws-lambda-rie && curl -Lo ~/.aws-lambda-rie/aws-lambda-rie https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie && chmod +x ~/.aws-lambda-rie/aws-lambda-rie` |\n| macOS/Linux arm64 | `mkdir -p ~/.aws-lambda-rie && curl -Lo ~/.aws-lambda-rie/aws-lambda-rie https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie-arm64 && chmod +x ~/.aws-lambda-rie/aws-lambda-rie` |\n| Windows x86\\_64 | `Invoke-WebRequest -OutFile 'C:\\Program Files\\aws lambda\\aws-lambda-rie' https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie` |\n| Windows arm64 | `Invoke-WebRequest -OutFile 'C:\\Program Files\\aws lambda\\aws-lambda-rie' https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie-arm64` |\n\n\n## Getting started\n\nThere are a few ways you use the Runtime Interface Emulator (RIE) to locally test your function depending on the base image used.\n\n\n### Test an image with RIE included in the image\n\nThe AWS base images for Lambda include the runtime interface emulator. You can also follow these steps if you built the RIE into your alternative base image.\n\n#### To test your Lambda function with the emulator\n\n1. Build your image locally using the docker build command.\n\n    `docker build -t myfunction:latest .`\n\n2. Run your container image locally using the docker run command.\n\n    `docker run -p 9000:8080  myfunction:latest`\n\n    This command runs the image as a container and starts up an endpoint locally at `localhost:9000/2015-03-31/functions/function/invocations`.\n\n3. Post an event to the following endpoint using a curl command:\n\n    `curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'`\n\n    This command invokes the function running in the container image and returns a response.\n\n### Build RIE into your base image\n\nYou can build RIE into a base image. Download the RIE from GitHub to your local machine and update your Dockerfile to install RIE.\n\n#### To build the emulator into your image\n\n1. Create a script and save it in your project directory. Set execution permissions for the script file.\n\n    The script checks for the presence of the `AWS_LAMBDA_RUNTIME_API` environment variable, which indicates the presence of the runtime API. If the runtime API is present, the script runs [the runtime interface client](https://docs.aws.amazon.com/lambda/latest/dg/runtimes-images.html#runtimes-api-client). Otherwise, the script runs the runtime interface emulator.\n\n    The following example shows a typical script for a Node.js function.\n\n    ```sh\n    #!/bin/sh\n    if [ -z \"${AWS_LAMBDA_RUNTIME_API}\" ]; then\n      exec /usr/local/bin/aws-lambda-rie /usr/bin/npx aws-lambda-ric\n    else\n      exec /usr/bin/npx aws-lambda-ric\n    fi\n    ```\n\n2. Download the [runtime interface emulator](https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest) for your target architecture (`aws-lambda-rie` for x86\\_64 or `aws-lambda-rie-arm64` for arm64) from GitHub into your project directory.\n\n3. Install the emulator package and change `ENTRYPOINT` to run the new script by adding the following lines to your Dockerfile:\n\n    To use the default x86\\_64 architecture\n\n    ```dockerfile\n    ADD aws-lambda-rie /usr/local/bin/aws-lambda-rie\n    ENTRYPOINT [ \"/entry_script.sh\" ]\n    ```\n\n    To use the arm64 architecture:\n\n    ```dockerfile\n    ADD aws-lambda-rie-arm64 /usr/local/bin/aws-lambda-rie\n    ENTRYPOINT [ \"/entry_script.sh\" ]\n    ```\n\n4. Build your image locally using the docker build command.\n\n    ```sh\n    docker build -t myfunction:latest .\n    ```\n\n5. Run your image locally using the docker run command.\n\n    ```sh\n    docker run -p 9000:8080 myfunction:latest\n    ```\n\n### Test an image without adding RIE to the image\n\nYou install the runtime interface emulator to your local machine. When you run the container image, you set the entry point to be the emulator.\n\n#### To test an image without adding RIE to the image\n\n1. From your project directory, run the following command to download the RIE (x86-64 architecture) from GitHub and install it on your local machine.\n\n    ```sh\n    mkdir -p ~/.aws-lambda-rie && curl -Lo ~/.aws-lambda-rie/aws-lambda-rie \\\n    https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie \\\n    && chmod +x ~/.aws-lambda-rie/aws-lambda-rie\n    ```\n\n    To download the RIE for arm64 architecture, use the previous command with a different GitHub download url.\n\n    ```\n    https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie-arm64 \\\n    ```\n\n2. Run your Lambda image function using the docker run command.\n\n    ```sh\n    docker run -d -v ~/.aws-lambda-rie:/aws-lambda -p 9000:8080 myfunction:latest \\\n        --entrypoint /aws-lambda/aws-lambda-rie  <image entrypoint> <(optional) image command>\n    ```\n\n    This runs the image as a container and starts up an endpoint locally at `localhost:9000/2015-03-31/functions/function/invocations`.\n\n3. Post an event to the following endpoint using a curl command:\n\n    ```sh\n    curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'\n    ```\n\n    This command invokes the function running in the container image and returns a response.\n\n## How to configure\n\n`aws-lambda-rie` can be configured through Environment Variables within the local running Image.\nYou can configure your credentials by setting:\n* `AWS_ACCESS_KEY_ID`\n* `AWS_SECRET_ACCESS_KEY`\n* `AWS_SESSION_TOKEN`\n* `AWS_REGION`\n\nYou can configure timeout by setting `AWS_LAMBDA_FUNCTION_TIMEOUT` to the number of seconds you want your function to timeout in.\n\nThe rest of these Environment Variables can be set to match AWS Lambda's environment but are not required.\n* `AWS_LAMBDA_FUNCTION_VERSION`\n* `AWS_LAMBDA_FUNCTION_NAME`\n* `AWS_LAMBDA_FUNCTION_MEMORY_SIZE`\n\n## Level of support\n\nYou can use the emulator to test if your function code is compatible with the Lambda environment, executes successfully\nand provides the expected output. For example, you can mock test events from different event sources. You can also use\nit to test extensions and agents built into the container image against the Lambda Extensions API. This component\ndoes _not_ emulate the orchestration behavior of AWS Lambda. For example, Lambda has a network and security\nconfigurations that will not be emulated by this component.\n\n* You can use the emulator to test if your function code is compatible with the Lambda environment, runs successfully and provides the expected output.\n* You can also use it to test extensions and agents built into the container image against the Lambda Extensions API.\n* This component does _not_ emulate Lambda\u2019s orchestration, or security and authentication configurations.\n* The component does _not_ support X-ray and other Lambda integrations locally.\n* The component supports only Linux, for x86-64 and arm64 architectures.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2023-11-30T18:09:08Z", "2023-07-28T00:40:15Z", "2023-07-05T23:57:24Z", "2023-06-08T18:30:22Z", "2023-05-03T17:18:37Z", "2022-12-20T16:41:54Z", "2022-11-01T21:39:48Z", "2022-09-22T20:39:51Z", "2022-08-16T16:52:11Z", "2022-05-24T22:41:20Z", "2022-03-18T22:14:54Z", "2022-02-24T00:05:13Z", "2021-11-16T00:04:51Z", "2021-09-29T21:15:54Z", "2021-07-08T17:33:10Z", "2020-12-01T01:07:24Z"]}, {"name": "aws-lambda-snapstart-java-rules", "description": null, "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Lambda SnapStart Bug Scanner\n\nSnapStart Bug Scanner is the [SpotBugs](https://spotbugs.github.io/) plugin for helping AWS Lambda customers inspect\ntheir functions against potential bugs unique to AWS Lambda SnapStart environment.\n\n## How to use\n\nFollowing sections explain how to enable this plugin in your Gradle and Maven projects.\n\n### Gradle Builds\n\nAfter SpotBugs is [enabled in the Gradle project](https://spotbugs.readthedocs.io/en/latest/gradle.html) declaring a dependency on SnapStart bug scanner is sufficient. \n\nExample:\n\n```kotlin\nplugins {\n    id(\"com.github.spotbugs\") version \"4.7.3\"\n}\n\nspotbugs {\n    ignoreFailures.set(false)\n    showStackTraces.set(true)\n}\n\ndependencies {\n    spotbugs(\"com.github.spotbugs:spotbugs:4.7.3\")\n    spotbugsPlugins(\"software.amazon.lambda.snapstart:aws-lambda-snapstart-java-rules:0.2.1\")\n}\n```\n\nAfter updating the `build.gradle` file you can run `./gradlew check` to run the analysis and see the result.\n\n### Maven Builds\n\nAfter SpotBugs is [enabled in the Maven project](https://spotbugs.readthedocs.io/en/latest/maven.html) declaring a dependency on SnapStart bug scanner is sufficient.\n\nExample:\n\n```xml\n<build>\n    <plugins>\n        <plugin>\n            <groupId>com.github.spotbugs</groupId>\n            <artifactId>spotbugs-maven-plugin</artifactId>\n            <version>4.7.3.0</version>\n            <configuration>\n                <effort>Max</effort>\n                <threshold>medium</threshold>\n                <failOnError>true</failOnError>\n                <plugins>\n                    <plugin>\n                        <groupId>software.amazon.lambda.snapstart</groupId>\n                        <artifactId>aws-lambda-snapstart-java-rules</artifactId>\n                        <version>0.2.1</version>\n                    </plugin>\n                </plugins>\n            </configuration>\n        </plugin>\n    </plugins>\n</build>\n```\n\nAfter updating `pom.xml`  you can run `mvn compile && mvn spotbugs:spotbugs` to run the analysis and see results in `targets/spotbugsXml.xml` file. Also, you can run `mvn spotbugs:check` to see results on your terminal and `mvn spotbugs:gui` on SpotBug's graphical UI.\n\n## Bug Descriptions\n\n### SNAP_START: Detected handler state that is potentially not resilient to VM snapshot and restore operations. (AWS_LAMBDA_SNAP_START_BUG)\n\nOur analysis shows that AWS Lambda handler class initialization creates state that may not remain unique for the function \nwhen it uses SnapStart. Lambda functions that use SnapStart are  snapshotted at their initialized state and all execution \nenvironments created afterwards share the same initial state. This means that if the Lambda function relies on state that \nis not resilient to snapshot and restore operations, it might manifest an unexpected behavior by using SnapStart.\n\nThis tool helps provide an insight on possible cases where your code may not be fully compatible with \nsnapstart enabled. Please verify that your code maintains uniqueness with SnapStart. For best practices, follow the \nguidelines outlined in [SnapStart feature documentation](https://docs.aws.amazon.com/lambda/latest/dg/snapstart.html).\nFor more information on the tool and examples of scenarios that the tool helps identify, refer to the\n[SnapStart scanner GitHub documentation](https://github.com/aws/aws-lambda-snapstart-java-rules/wiki).\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2023-03-03T22:37:04Z"]}, {"name": "aws-lc", "description": "AWS-LC is a general-purpose cryptographic library maintained by the AWS Cryptography team for AWS and their customers. It \u0456s based on code from the Google BoringSSL project and the OpenSSL project.", "language": "C++", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# AWS libcrypto (AWS-LC)\n\nAWS-LC is a general-purpose cryptographic library maintained by the AWS Cryptography\nteam for AWS and their customers. It \u0456s based on code from the Google BoringSSL project\nand the OpenSSL project.\n\nAWS-LC contains portable C implementations of algorithms needed for TLS and common\napplications. For performance critical algorithms, optimized assembly versions are\nincluded for x86 and ARM.\n\n## Quickstart for Amazon Linux 2\n\nAWS-LC\u2019s libcrypto is a C library and needs a C compiler. AWS-LC's libssl is a\nC++ library and needs a C++ compiler.\n\nFork AWS-LC on GitHub and run the following commands to build AWS-LC with optimizations\nand debug info, run all tests, and install it:\n```bash\nsudo yum install cmake3 ninja-build clang perl golang\ngit clone https://github.com/${YOUR_GITHUB_ACCOUNT_NAME}/aws-lc.git\nmkdir aws-lc-build && cd aws-lc-build\ncmake3 -GNinja \\\n    -DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n    -DCMAKE_INSTALL_PREFIX=../aws-lc-install \\\n    ../aws-lc\nninja-build run_tests && ninja-build install\ncd ../aws-lc-install/\nls *\n```\nSee [Building.md](https://github.com/aws/aws-lc/blob/main/BUILDING.md) for more\ninformation about required dependencies and build options. If you\u2019re interested in\ngetting involved [open an issue](https://github.com/aws/aws-lc/issues/new/choose) to discuss your plan.\n[Contributing.md](https://github.com/aws/aws-lc/blob/main/CONTRIBUTING.md) has\ninfo for how to specifically make the change and get it reviewed by AWS-LC maintainers.\nIf you just want to use AWS-LC see our existing documentation in the public header\nfiles, if you\u2019re moving your application from OpenSSL see\n[Porting_to_AWS-LC.md](https://github.com/aws/aws-lc/blob/main/PORTING_TO_AWSLC.md)\nfor more information.\n\n## Why AWS-LC?\n\nAWS-LC's goal is to maintain a secure libcrypto that is compatible with software and\napplications used at AWS. AWS-LC also serves as the new home for the AWS Cryptography\nteam to publish open source contributions and enhancements that are submitted to\nother libcrypto projects.\n\n## AWS-LC features\n\n### API Compatibility\n\nAWS-LC is compatible with the majority of OpenSSL\u2019s APIs to make it easy to use with\nexisting applications. We\u2019re open to discussing adding missing functionality and\nunderstanding your use case in an [issue](https://github.com/aws/aws-lc/issues/new/choose).\n\n### Algorithm optimization support\n\nA portable C implementation of all algorithms is included and optimized assembly\nimplementations of select algorithms is included for some x86 and Arm CPUs. We\nuse [AWS Graviton processors](https://aws.amazon.com/ec2/graviton/) to test\nARMv8 optimizations and Intel CPUs to test x86 and x86-64 optimizations.\n\nThe [Intel Software Development Emulator](https://software.intel.com/content/www/us/en/develop/articles/intel-software-development-emulator.html)\nis used to run tests on many different x86 processors.\n\nIf you use another CPU and would like to make sure we test it or discuss adding\nan assembly optimized algorithm implementation, please open an issue to discuss\nadding it to our CI.\n\n## Platform Support\n\nAWS-LC correctness is tested on a variety of *platforms* (i.e., OS/CPU combinations).  \nThe following is an overview of the platforms we actively support or are \nknown to be of interest to our community. \n\nIf you use a platform not listed below and would like to request it be added to our CI,\nplease open an [issue](https://github.com/aws/aws-lc/issues/new/choose) for discussion.\nRegardless of our support level for a particular platform, we will gladly consider contributions that \nimprove or extend our support.\n\n### Supported Platforms\n\nThe following platforms are actively tested in our CI pipeline. A few of these platforms are tested across \nmultiple compilers or compiler versions. For each pull request, the proposed change is validated to confirm that it \nsuccessfully builds and tests pass for these platform. \nA more complete description of our test setup can be found in the \n[CI README](https://github.com/aws/aws-lc/blob/main/tests/ci/README.md).\n\n| OS      | CPU     | \n|---------|---------|\n| Linux   | x86     |\n| Linux   | x86-64  |\n| Linux   | aarch64 |\n| Windows | x86-64  |\n| macOS   | x86-64  |\n| macOS   | aarch64 |\n| Android | aarch64 |\n| Linux   | ppc     |\n| Linux   | ppc64   |\n| Linux   | ppc64le |\n\n### Other platforms\n\nThe platforms listed below are of interest to us or to our community. However, problems reported \nagainst them might not be prioritized for immediate action by our team. We welcome contributions \nthat improve the experience for consumers on these platforms.\n\n| OS        | CPU         |\n|-----------|-------------|\n| Android   | arm32       |\n| iOS       | aarch64     |\n| Linux     | arm32       |\n| Linux     | Loongarch64 | \n| Windows   | aarch64     |\n| OpenBSD   | x86-64      |\n| FreeBSD   | x86-64      |\n\n## AWS-LC safety mechanisms\n\n### Automated testing\n\nEvery change is tested with our\n[CI](https://github.com/aws/aws-lc/blob/main/tests/ci/README.md) that includes\npositive and negative unit tests, fuzz tests, Sanitizers\n([Address](https://clang.llvm.org/docs/AddressSanitizer.html),\n[Memory](https://clang.llvm.org/docs/MemorySanitizer.html),\n[Control flow integrity](https://clang.llvm.org/docs/ControlFlowIntegrity.html),\n[Thread](https://clang.llvm.org/docs/ThreadSanitizer.html), and\n[Undefined behavior](https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html)),\n[Valgrind](https://valgrind.org/), and Formal Verification.\n\n### Formal Verification\n\nPortions of AWS-LC have been formally verified in\n[AWS-LC Formal Verification](https://github.com/awslabs/aws-lc-verification),\nthe checks are run in AWS-LC\u2019s CI on every change. The algorithms that have been\nverified on certain platforms with caveats include:\n* SHA-2\n* HMAC\n* AES-KWP\n* ECDH & ECDSA with curve P-384\n* HKDF\n\n## Have a Question?\n\nWe use [GitHub Issues](https://github.com/aws/aws-lc/issues) for managing feature requests,\nbug reports, or questions about AWS-LC API usage.\n\nIf you think you might have found a security impacting issue, please instead\nfollow our [Security Notification Process](#security-issue-notifications).\n\n## Security issue notifications\n\nIf you discover a potential security issue in AWS-LC, we ask that you notify AWS\nSecurity via our\n[vulnerability reporting page](https://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public GitHub issue.\n\nIf you package or distribute AWS-LC, or use AWS-LC as part of a large multi-user service, you may be eligible for pre-notification of future AWS-LC releases. Please contact aws-lc-pre-notifications@amazon.com.\n", "release_dates": ["2024-02-26T21:18:35Z", "2024-01-24T18:12:05Z", "2024-01-23T20:34:12Z", "2024-01-17T15:54:21Z", "2024-01-10T18:15:11Z", "2024-01-10T01:04:11Z", "2023-12-19T00:23:10Z", "2023-12-19T00:26:38Z", "2023-12-05T14:20:41Z", "2023-11-22T14:48:43Z", "2023-11-10T17:36:38Z", "2023-11-10T17:37:54Z", "2023-11-10T17:37:10Z", "2023-11-08T22:09:52Z", "2023-11-08T20:39:47Z", "2023-11-08T20:51:53Z", "2023-11-02T21:18:30Z", "2023-10-26T19:38:28Z", "2023-10-26T14:09:03Z", "2023-09-22T18:10:47Z", "2023-08-28T23:39:24Z", "2023-08-14T12:04:20Z", "2023-08-14T12:05:23Z", "2023-08-02T17:46:32Z", "2023-08-01T21:06:22Z", "2023-07-14T08:18:13Z", "2023-07-07T19:45:55Z", "2023-06-20T19:35:15Z", "2023-05-19T18:29:00Z", "2023-04-21T21:35:16Z"]}, {"name": "aws-lc-rs", "description": "aws-lc-rs is a cryptographic library using AWS-LC for its cryptographic operations. The library strives to be API-compatible with the popular Rust library named ring.", "language": "Rust", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "[![aws-lc-rs](https://img.shields.io/badge/aws--lc--rs-crates.io-important)](https://crates.io/crates/aws-lc-rs)\n[![aws-lc-sys](https://img.shields.io/badge/aws--lc--sys-crates.io-important)](https://crates.io/crates/aws-lc-sys)\n[![aws-lc-fips-sys](https://img.shields.io/badge/aws--lc--fips--sys-crates.io-important)](https://crates.io/crates/aws-lc-fips-sys)\n\n# AWS Libcrypto for Rust\n[*aws-lc-rs*](aws-lc-rs/README.md) is a cryptographic library using [AWS-LC](https://github.com/aws/aws-lc) for its\ncryptographic operations.\nThis library strives to be API-compatible with the popular Rust library named\n[ring](https://github.com/briansmith/ring). It uses either the auto-generated [*aws-lc-sys*](aws-lc-sys/README.md) or [\n*aws-lc-fips-sys*](aws-lc-fips-sys/README.md) Foreign Function Interface (FFI) crates found in this\nrepository for invoking *AWS-LC*.\n\n## Crates\n\n### [aws-lc-rs](aws-lc-rs/README.md)\nA *ring*-compatible crypto library using the cryptographic operations provided by\n[*AWS-LC*](https://github.com/awslabs/aws-lc) using either *aws-lc-sys* or *aws-lc-fips-sys*.\n\n### [aws-lc-sys](aws-lc-sys/README.md)\n**Autogenerated** Low-level AWS-LC bindings for the Rust programming language.\nWe do not recommend directly relying on these bindings.\n\n### [aws-lc-fips-sys](aws-lc-fips-sys/README.md)\n**Autogenerated** Low-level AWS-LC bindings for the Rust programming language, providing FIPS support.\nWe do not recommend directly relying on these bindings. This crate provides bindings to\n[AWS-LC-FIPS 2.x](https://github.com/aws/aws-lc/tree/fips-2022-11-02), which has completed\nFIPS validation testing by an accredited lab and has been submitted to NIST for certification. The static build of AWS-LC-FIPS\nis used. (See [README](./aws-lc-fips-sys/README.md)).  \n\nRefer to the [NIST Cryptographic Module Validation Program's Modules In Progress List](https://csrc.nist.gov/Projects/cryptographic-module-validation-program/modules-in-process/Modules-In-Process-List)\nfor the latest status of the static or dynamic AWS-LC Cryptographic Module. A complete list of supported operating environments will be\nmade available in the vendor security policy once the validation certificate has been issued. We will also update our release notes\nand documentation to reflect any changes in FIPS certification status.\n\n## Build\n\nPlease see the [build instructions](aws-lc-rs/README.md#Build) in the aws-lc-rs crate.\n\n# Motivation\nRust developers increasingly need to deploy applications that meet US and Canadian government cryptographic\nrequirements. We evaluated how to deliver FIPS validated cryptography in idiomatic and performant Rust, built around our\nAWS-LC offering. We found that the popular ring (v0.16) library fulfilled much of the cryptographic needs in the Rust\ncommunity, but it did not meet the needs of developers with FIPS requirements. Our intention is to contribute a drop-in\nreplacement for ring that provides FIPS support and is compatible with the ring API. Rust developers with prescribed\ncryptographic requirements can seamlessly integrate aws-lc-rs into their applications and deploy them into AWS Regions.\n\n## Questions, Feedback and Contributing\n\n* [Submit an non-security Bug/Issue/Request](https://github.com/awslabs/aws-lc-rs/issues/new/choose)\n* [API documentation](https://docs.rs/aws-lc-rs/)\n* [Fork our repo](https://github.com/awslabs/aws-lc-rs/fork)\n\nWe use [GitHub Issues](https://github.com/awslabs/aws-lc-rs/issues/new/choose) for managing feature requests, bug reports, or questions about aws-lc-rs API usage.\n\nOtherwise, if you think you might have found a security impacting issue, please instead\nfollow our *Security Notification Process* below.\n\n## Security Notification Process\n\nIf you discover a potential security issue in *AWS-LC* or *aws-lc-rs*, we ask that you notify AWS\nSecurity via our\n[vulnerability reporting page](https://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public GitHub issue.\n\nIf you package or distribute *aws-lc-rs*, or use *aws-lc-rs* as part of a large multi-user service,\nyou may be eligible for pre-notification of future *aws-lc-rs* releases.\nPlease contact aws-lc-pre-notifications@amazon.com.\n\n## License\n\nThis library is licensed under the Apache-2.0 or the ISC License.\n", "release_dates": ["2024-02-14T20:16:38Z", "2024-01-26T22:12:55Z", "2024-01-26T21:35:14Z", "2023-11-20T22:04:02Z", "2023-11-17T00:26:20Z", "2023-11-16T21:54:08Z", "2023-10-27T19:24:52Z", "2023-08-31T17:43:19Z", "2023-07-10T18:40:17Z", "2023-06-23T18:06:12Z", "2023-05-24T20:21:11Z", "2023-05-12T20:56:41Z", "2023-04-25T20:04:28Z", "2023-04-19T17:43:05Z", "2023-04-19T16:44:58Z"]}, {"name": "aws-logging-dotnet", "description": ".NET Libraries for integrating Amazon CloudWatch Logs with popular .NET logging libraries", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Logging .NET\n\nThis repository contains plugins for popular .NET logging frameworks that integrate \nwith Amazon Web Services. The plugins use the \nAmazon CloudWatch Logs service to write log data to a configured \nlog group. The logs can be viewed and searched using the [AWS CloudWatch Console](https://console.aws.amazon.com/cloudwatch/).\n\nFor a history of releases view the [release change log](RELEASE.CHANGELOG.md)\n\n### AWS Lambda\n\nThese packages batch logging messages in a queue and send messages to CloudWatch Logs using a background thread. \nThe use of the background thread means that the messages are not guaranteed to be delivered when used in AWS Lambda.\nThe reason is because the background thread will be frozen once a Lambda event is processed and \nmay not ever be unfrozen if more Lambda events are not received for some time.\n\nWhen using Lambda it is recommended to use either the `ILambdaContext.Logger.LogLine` or the \n[Amazon.Lambda.Logging.AspNetCore](https://github.com/aws/aws-lambda-dotnet/tree/master/Libraries/src/Amazon.Lambda.Logging.AspNetCore) package.\n\n### Required IAM Permissions\n\nRegardless of the framework used, the following permissions must be allowed (via [IAM](https://aws.amazon.com/iam)) for the provided AWS credentials.\n\n```\nlogs:CreateLogGroup\nlogs:CreateLogStream\nlogs:PutLogEvents\nlogs:DescribeLogGroups\n```\n\nThe practice of granting least privilege access is recommended when setting up credentials. You can further reduce access by limiting permission scope to specific resources (such as a Log Stream) by referencing its ARN during policy creation.\n\nFor more information and a sample JSON policy template, please see [Amazon CloudWatch Logs and .NET Logging Frameworks](https://aws.amazon.com/blogs/developer/amazon-cloudwatch-logs-and-net-logging-frameworks/) on the AWS Developer Blog.\n\n### Optional IAM Permissions\n\nThe following [IAM](https://aws.amazon.com/iam) permissions are optional depending on the configured features of the logger.\n\n| Feature                                                                       | IAM Permission(s) for feature | Configuration Setting         |\n|-------------------------------------------------------------------------------|-------------------------------|-------------------------------|\n| [Set new log group retention policy](#setting-new-log-group-retention-policy) | `logs:PutRetentionPolicy`     | `NewLogGroupRetentionInDays`  |\n\n### Why can't the Log Stream name be configured?\n\nThese libraries use CloudWatch Logs' best practice of having the log stream name be generated. The name can be customized by adding \na suffix or prefix using the LogStreamNameSuffix and LogStreamNamePrefix configuration properties.\n\nGenerating the name ensures each process within an application has its own log stream to write to. When writing to log \nstream a marker is maintained to append more messages to the stream. When more then one process writes to the same stream\nthe marker maintained within the process goes out of sync. This generates errors and retries to post the log \nmessage causing performance issues.\n\nTo view logs across all log streams it is recommended to use the [Logs Insight](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html) feature for Cloud Watch Logs.\n\n### Setting new Log Group Retention Policy\n\nThese libraries support setting a [log retention policy](https://docs.aws.amazon.com/managedservices/latest/userguide/log-customize-retention.html) \non any CloudWatch Log Groups which they create.  This feature is enabled using the NewLogGroupRetentionInDays configuration property. \nThe DisableLogGroupCreation configuration property must not be set to true. Retention policies configured in this manner \nare only applied to _new_ Log Groups created directly by these libraries. By default no retention policy is applied to newly created Log Groups.\n\n\nNote that any value of NewLogGroupRetentionInDays which is not one supported by CloudWatch [which can be found here](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutRetentionPolicy.html#API_PutRetentionPolicy_RequestSyntax) - and listed below - is a configuration error which will result\nin a non-fatal error applying the policy. The application and logging will continue however no retention policy will be applied. \n \n```csharp\nnull, 1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1096, 1827, 2192, 2557, 2922, 3288, 3653\n```\n\n## Supported Logging Frameworks\n\n1. [NLog](#nlog)\n2. [Apache log4net](#apache-log4net)\n3. [ASP.NET Core Logging](#aspnet-core-logging)\n4. [Serilog](#serilog)\n\n### NLog\n\n* NuGet Package: [AWS.Logger.Nlog](https://www.nuget.org/packages/AWS.Logger.NLog/)\n\nNLog uses targets that can be configured to receive log messages. Targets can be configured either \nthrough a config file or through code. The default config file that NLog will automatically search for\nis **NLog.config**. Here is an example config file that configures the AWS Region and the CloudWatch Logs log group.\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n<nlog xmlns=\"http://www.nlog-project.org/schemas/NLog.xsd\"\n      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n\t  throwConfigExceptions=\"true\">\n  <extensions>\n    <add assembly=\"NLog.AWS.Logger\" />\n  </extensions>\n  <targets>\n    <target name=\"aws\" type=\"AWSTarget\" logGroup=\"NLog.ConfigExample\" region=\"us-east-1\"/>\n  </targets>\n  <rules>\n    <logger name=\"*\" minlevel=\"Info\" writeTo=\"aws\" />\n  </rules>\n</nlog>\n```\n\nThe AWS credentials will be found using the standard AWS SDK for .NET credentials search path. In this case\nit will look for a profile named default, search for environment variables or search for an instance profile on an \nEC2 instance. To use a specific AWS credential profile use the **profile** attribute on the target.\n\nHere is an example of performing the same configuration via code.\n\n```csharp\nvar config = new LoggingConfiguration();\n\nvar awsTarget = new AWSTarget()\n{\n    LogGroup = \"NLog.ProgrammaticConfigurationExample\",\n    Region = \"us-east-1\"\n};\nconfig.AddTarget(\"aws\", awsTarget);\n\nconfig.LoggingRules.Add(new LoggingRule(\"*\", LogLevel.Debug, awsTarget));\n\nLogManager.Configuration = config;\n```\n\nCheckout the [NLog samples](/samples/NLog) for examples on how you can use AWS and NLog together. \n\n### Apache log4net\n\n* NuGet Package: [AWS.Logger.Log4net](https://www.nuget.org/packages/AWS.Logger.Log4net/)\n\nLog4net configures appenders to receive log messages. Appenders can be configured either \nthrough a config file or through code. To use a config file add a file to your project. \nThe file can be named anything but for this example call it log4net.config. Make sure\nthat **Copy to Output Directory** is set to copy. Here is an example config file setting\nthe CloudWatch Log log group and the AWS Region. \n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n<log4net>\n  <appender name=\"AWS\" type=\"AWS.Logger.Log4net.AWSAppender,AWS.Logger.Log4net\">\n\n    <LogGroup>Log4net.ConfigExample</LogGroup>\n    <Region>us-east-1</Region>\n    \n    <layout type=\"log4net.Layout.PatternLayout\">\n      <conversionPattern value=\"%-4timestamp [%thread] %-5level %logger %ndc - %message%newline\" />\n    </layout>\n  </appender>\n\n  <root>\n    <level value=\"DEBUG\" />\n    <appender-ref ref=\"AWS\" />\n  </root>\n</log4net>\n```\n\nThe AWS credentials will be found using the standard AWS SDK for .NET credentials search path. In this case\nit will look for a profile named default, search for environment variables or search for an instance profile on an \nEC2 instance. To use a specific AWS credential profile add a **Profile** under the **appender** node.\n\nAdd the following code during the startup of the application to have log4net read the configuration file.\n```\n// log4net is configured in the log4net.config file which adds the AWS appender.\nXmlConfigurator.Configure(new System.IO.FileInfo(\"log4net.config\"));\n```\n\nHere is an example of performing the same configuration via code.\n\n```csharp\nstatic void ConfigureLog4net()\n{\n    Hierarchy hierarchy = (Hierarchy)LogManager.GetRepository();\n    PatternLayout patternLayout = new PatternLayout();\n\n    patternLayout.ConversionPattern = \"%-4timestamp [%thread] %-5level %logger %ndc - %message%newline\";\n    patternLayout.ActivateOptions();\n\n    AWSAppender appender = new AWSAppender();\n    appender.Layout = patternLayout;\n\n    // Set log group and region. Assume credentials will be found using the default profile or IAM credentials.\n    appender.LogGroup = \"Log4net.ProgrammaticConfigurationExample\";\n    appender.Region = \"us-east-1\";\n\n    appender.ActivateOptions();\n    hierarchy.Root.AddAppender(appender);\n\n    hierarchy.Root.Level = Level.All;\n    hierarchy.Configured = true;\n}\n\n```\n\nCheckout the [Log4net samples](/samples/Log4net) for examples of how you can use AWS and log4net together. \n\n### ASP.NET Core Logging\n\n* NuGet Package: [AWS.Logger.AspNetCore](https://www.nuget.org/packages/AWS.Logger.AspNetCore/)\n\nASP.NET Core introduced a new [logging framework](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/logging) that has providers configured to send logs to destinations. \nThe AWS.Logger.AspNetCore NuGet package provides a log provider which adds CloudWatch Logs as a destination for the logs.\n\n**Note:** Starting with version 2.0.0 of AWS.Logger.AspNetCore this library targets netstandard2.0 and the dependencies have been\nupgraded to the ASP.NET Core 2.1 versions. For older versions of .NET Core, which Microsoft has made end of life, use versions before 2.0.0.\n\nThe [WebSample](/samples/AspNetCore/WebSample) in this repository demonstrates how to configure\nthis provider.\n\nThe configuration is setup in the [appsettings.json](/samples/AspNetCore/WebSample/appsettings.json) file. In versions before 2.0.0 the `AWS.Logging`\nwas used as the configuration section root. Starting with 2.0.0 the library has switched to use the standard `Logging` configuration section root.\nFor backwards compatibility if the `Logging` section does not contain a `LogGroup` then the library will fallback to `AWS.Logging`.\n\n```json\n\"Logging\": {\n  \"Region\": \"us-east-1\",\n  \"LogGroup\": \"AspNetCore.WebSample\",\n  \"IncludeLogLevel\": true,\n  \"IncludeCategory\": true,\n  \"IncludeNewline\": true,\n  \"IncludeException\": true,\n  \"IncludeEventId\": false,\n  \"IncludeScopes\": false,\n  \"LogLevel\": {\n    \"Default\": \"Debug\",\n    \"System\": \"Information\",\n    \"Microsoft\": \"Information\"\n  }\n}\n```\n\nIn a typical ASP.NET Core application the `Program.cs` file contains a `CreateWebHostBuilder` method. To include AWS.Logger.AspNetCore\nadd a call to `ConfigureLogging` and in the `Action<ILoggingBuilder>` passed into ConfigureLogging call `AddAWSProvider`. This will look up the configuration\ninformation from the IConfiguration added to the dependency injection system.\n\n```csharp\npublic static IWebHostBuilder CreateWebHostBuilder(string[] args) =>\n    WebHost.CreateDefaultBuilder(args)\n        .ConfigureLogging(logging =>\n        {\n            logging.AddAWSProvider();\n\n            // When you need logging below set the minimum level. Otherwise the logging framework will default to Informational for external providers.\n            logging.SetMinimumLevel(LogLevel.Debug);\n        })\n        .UseStartup<Startup>();\n```\n\n### Serilog\n\n* NuGet Package: [AWS.Logger.SeriLog](https://www.nuget.org/packages/AWS.Logger.SeriLog/)\n\nSerilog can be configured with sinks to receive log messages either through a config file or through code. To use a config file with Serilog, follow the instructions [here](https://github.com/serilog/serilog/wiki/Configuration-Basics)\nto install the necessary extensions and NuGet packages. In the json file, make sure **AWS.Logger.SeriLog** is in the **Using** \narray. Set the **LogGroup** and **Region** under the **Serilog** node, and add **AWSSeriLog** as a sink under the **WriteTo** node. Here is an example.\n\n```json\n{\n  \"Serilog\": {\n    \"Using\": [\n      \"AWS.Logger.SeriLog\"\n    ],\n    \"LogGroup\": \"Serilog.ConfigExample\",\n    \"Region\": \"us-east-1\",\n    \"MinimumLevel\": \"Information\",\n    \"WriteTo\": [\n      {\n        \"Name\": \"AWSSeriLog\"\n      }\n    ]\n  }\n}\n```\n\nAdd the following code to configure the logger to read from the json file.\n\n```csharp\nvar configuration = new ConfigurationBuilder()\n.AddJsonFile(\"appsettings.json\")\n.Build();\n\nvar logger = new LoggerConfiguration()\n.ReadFrom.Configuration(configuration)\n.CreateLogger();\n```\n\nThe AWS Credentials will be found using the standard .NET credentials search path. It will search for a profile named default, environment variables, or an instance profile on an EC2 instance.\nIn order to use a profile other than default, add a **Profile** under the **Serilog** node. \n\nBelow is an example of doing the same configuration as above via code. The AWS sink can be added to the logger by using the WriteTo\nmethod. \n\n```csharp\nAWSLoggerConfig configuration = new AWSLoggerConfig(\"Serilog.ConfigExample\");\nconfiguration.Region = \"us-east-1\";\n\nvar logger = new LoggerConfiguration()\n.WriteTo.AWSSeriLog(configuration)\n.CreateLogger();\n```\n\nCheckout the [Serilog samples](/samples/Serilog) for examples of how you can use AWS and Serilog together.\n", "release_dates": []}, {"name": "aws-mobile-analytics-manager-net", "description": "The AWS SDK for .NET Mobile Analytics Manager makes it easy to gather usage information about mobile applications in the background and sync that data with AWS.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWSSDK.MobileAnalytics.MobileAnalyticsManager\n\n__After April 30, 2018, Amazon Mobile Analytics features will be provided only by Amazon Pinpoint. If you're new to Mobile Analytics, use Amazon Pinpoint instead. If you're currently using Mobile Analytics, see [Migrating from Amazon Mobile Analytics to Amazon Pinpoint](https://docs.aws.amazon.com/mobileanalytics/latest/ug/migrate.html). This repository is provided for existing customers.__\n\nThe AWS SDK for .NET Mobile Analytics Manager makes it easy to gather usage information about mobile applications in the background and\nsync that data with AWS.\n\nTo build the repository into a NuGet package for distribution, run the *full-build* msbuild target.\n\nIf you are looking for the Unity Mobile Analytics Manager, it is located in the Unity archive: https://github.com/aws/aws-sdk-unity-net\n", "release_dates": []}, {"name": "aws-msk-iam-auth", "description": "Enables developers to use AWS Identity and Access Management (IAM) to connect to their Amazon Managed Streaming for Apache Kafka (Amazon MSK) clusters.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon MSK Library for AWS Identity and Access Management \n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n## Introduction\nThe Amazon MSK Library for AWS Identity and Access Management enables developers to use \nAWS Identity and Access Management (IAM) to connect to their Amazon Managed Streaming for Apache Kafka (Amazon MSK) clusters. \nIt allows JVM based Apache Kafka clients to use AWS IAM for authentication and authorization against \nAmazon MSK clusters that have AWS IAM enabled as an authentication mechanism.\n\nThis library provides a new Simple Authentication and Security Layer (SASL) mechanism called `AWS_MSK_IAM`. This new \nSASL mechanism can be used by Kafka clients to authenticate against Amazon MSK clusters using AWS IAM.\n\n* [Amazon Managed Streaming for Apache Kafka][MSK]\n* [AWS Identity and Access Management][IAM]\n* [AWS IAM authentication and authorization for MSK ][MSK_IAM]\n\n## Building from source\nAfter you've downloaded the code from GitHub, you can build it using Gradle. Use this command:\n \n `gradle clean build`\n \nThe generated jar files can be found at: `build/libs/`.\n\nAn uber jar containing the library and all its relocated dependencies except the kafka client and `slf4j-api` can\n also be built. Use this command: \n\n`gradle clean shadowJar` \n\nThe generated uber jar file can also be found at: `build/libs/`. At runtime, the uber jar expects to find the kafka\n client library and the `sl4j-api` library on the classpath. \n\n## Validating secure dependencies\nTo ensure no security vulnerabilities in the dependency libraries, run the following.\n\n `gradle dependencyCheckAnalyze`\n\nIf the above reports any vulnerabilities, upgrade dependencies to use the respective latest versions.\n\n## Using the Amazon MSK Library for IAM Authentication\nThe recommended way to use this library is to consume it from maven central while building a Kafka client application.\n\n  ``` xml\n  <dependency>\n      <groupId>software.amazon.msk</groupId>\n      <artifactId>aws-msk-iam-auth</artifactId>\n      <version>2.0.3</version>\n  </dependency>\n  ```\nIf you want to use it with a pre-existing Kafka client, you could build the uber jar and place it in the Kafka client's\nclasspath.\n\n## Configuring a Kafka client to use AWS IAM with AWS_MSK_IAM mechanism\nYou can configure a Kafka client to use AWS IAM for authentication by adding the following properties to the client's \nconfiguration. \n\n```properties\n# Sets up TLS for encryption and SASL for authN.\nsecurity.protocol = SASL_SSL\n\n# Identifies the SASL mechanism to use.\nsasl.mechanism = AWS_MSK_IAM\n\n# Binds SASL client implementation.\nsasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required;\n\n# Encapsulates constructing a SigV4 signature based on extracted credentials.\n# The SASL client bound by \"sasl.jaas.config\" invokes this class.\nsasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler\n```\n\n## Configuring a Kafka client to use AWS IAM with SASL OAUTHBEARER mechanism\nYou can alternatively use SASL/OAUTHBEARER mechanism using IAM authentication by adding following configuration.\nFor more details on SASL/OAUTHBEARER mechanism, please read - [KIP-255](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=75968876)\n\n```properties\n# Sets up TLS for encryption and SASL for authN.\nsecurity.protocol=SASL_SSL\n# Identifies the SASL mechanism to use.\nsasl.mechanism=OAUTHBEARER\n# Binds SASL client implementation.\nsasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;\n# Encapsulates constructing a SigV4 signature based on extracted credentials.\n# The SASL client bound by \"sasl.jaas.config\" invokes this class.\nsasl.login.callback.handler.class=software.amazon.msk.auth.iam.IAMOAuthBearerLoginCallbackHandler\n```\n\nThis configuration finds IAM credentials using the [AWS Default Credentials Provider Chain][DefaultCreds]. To summarize,\nthe Default Credential Provider Chain looks for credentials in this order:\n\n1. Environment variables: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. \n1. Java system properties: aws.accessKeyId and aws.secretKey. \n1. Web Identity Token credentials from the environment or container.\n1. The default credential profiles file\u2013 typically located at ~/.aws/credentials (location can vary per platform), and shared by many of the AWS SDKs and by the AWS CLI.  \nYou can create a credentials file by using the aws configure command provided by the AWS CLI, or you can create it by editing the file with a text editor. For information about the credentials file format, see [AWS Credentials File Format][CredsFile].\n1. It can be used to load credentials from credential profiles other than the default one by setting the environment variable  \nAWS_PROFILE to the name of the alternate credential profile. Profiles can be used to load credentials from other sources such as AWS IAM Roles. See [AWS Credentials File Format][CredsFile] for more details.\n1. Amazon ECS container credentials\u2013 loaded from the Amazon ECS if the environment variable AWS_CONTAINER_CREDENTIALS_RELATIVE_URI is set. \n1. Instance profile credentials: used on EC2 instances, and delivered through the Amazon EC2 metadata service.\n\n### Specifying an alternate credential profile for a client\n\nIf the client wants to specify a particular credential profile as part of the client configuration rather than through \nthe environment variable AWS_PROFILE, they can pass in the name of the profile as a client configuration property:\n```properties\n# Binds SASL client implementation. Uses the specified profile name to look for credentials.\nsasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsProfileName=\"<Credential Profile Name>\";\n```\n#### Specifying a role based credential profile for a client\n\nSome clients may want to assume a role and use the role's temporary credentials to communicate with a MSK\n cluster. One way to do that is to create a credential profile for that role following the rules for \n [Using an IAM role in the CLI][RoleProfileCLI].\n They can then pass in the name of the credential profile as described [above](#specifying-an-alternate-credential-profile-for-a-client).\n \nAs an example, let's say a Kafka client is running on an Ec2 instance and the Kafka client wants to use an IAM role\n called `msk_client_role`. The Ec2 instance profile has permissions to assume the `msk_client_role` IAM role although\n  `msk_client_role` is not attached to the instance profile.\n\nIn such a case, we create a credential profile called `msk_client` that assumes the role `msk_client_role`. \nThe credential profile looks like:\n\n```\n[msk_client]\nrole_arn = arn:aws:iam::123456789012:role/msk_client_role\ncredential_source = Ec2InstanceMetadata\n```\nThe credential profile name `msk_client` is passed in as a client configuration property:\n```properties\nsasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsProfileName=\"msk_client\";\n```\n\nMany more examples of configuring credential profiles with IAM roles can be found in [Using an IAM role in the CLI][RoleProfileCLI]. \n\n### Specifying an AWS IAM Role for a client\nThe library supports another way to configure a client to assume an IAM role and use the role's temporary credentials.\nThe IAM role's ARN and optionally the session name for the client can be passed in as client configuration property:\n\n```properties\nsasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn=\"arn:aws:iam::123456789012:role/msk_client_role\" awsRoleSessionName=\"producer\"  awsStsRegion=\"us-west-2\";\n```\nIn this case, the `awsRoleArn` specifies the ARN for the IAM role the client should use and `awsRoleSessionName\n` specifies the session name that this particular client should use while assuming the IAM role. If the same IAM\n Role is used in multiple contexts, the session names can be used to differentiate between the different contexts.\nThe `awsRoleSessionName` is optional.\n \n `awsStsRegion` optionally specifies the regional endpoint of AWS STS to use \nwhile assuming the IAM role. If `awsStsRegion` is omitted the global endpoint for AWS STS is used by default. \nWhen the Kafka client is running in a VPC with an [STS interface VPC Endpoint][StsVpcE] [(AWS PrivateLink)][PrivateLink] to a regional endpoint of AWS STS and we want\n all STS traffic to go over that endpoint, we should set `awsStsRegion` to the region corresponding to the interface\n VPC Endpoint. It may also be necessary to configure the `sts_regional_endpoints` shared AWS config file setting, or \n the AWS_STS_REGIONAL_ENDPOINTS environment variable as per the [AWS STS Regionalized endpoints documentation.][StsRegionalEndpointsDoc]\n \nThe Default Credential Provider Chain must contain the permissions necessary to assume the client role.\nFor example, if the client is an EC2 instance, its instance profile should have permission to assume the\n `msk_client_role`.\n \n### Figuring out whether or not to use default credentials\n\nWhen you want the MSK client to connect to MSK using credentials not found in the [AWS Default Credentials Provider Chain][DefaultCreds], you can specify an `awsProfileName` containing the credential profile to use, or an `awsRoleArn` to indicate an IAM Role\u2019s ARN to assume using credentials in the Default Credential Provider Chain.  These parameters are optional, and if they are not set the MSK client will use credentials from the Default Credential Provider Chain. There is no need to specify them if you intend to use an IAM role associated with an AWS compute service, such as EC2 or ECS to authenticate to MSK.\n\n### Retries while getting credentials\nIn some scenarios the IAM credentials might be transiently unavailable. This will cause the connection to fail, which\nmight in some cases cause the client application to stop. \nSo, in version `1.1.3` the library retries loading the credentials when it gets an `SdkClientException` (which wraps\nmost `AWS SDK` client side exceptions). Since the retries do not impact the fault-free path and we had heard of user\nissues around random failures loading credentials (e.g.: [#59](https://github.com/aws/aws-msk-iam-auth/issues/59), maybe\n [#51](https://github.com/aws/aws-msk-iam-auth/issues/51) ), we decided to change the default behavior\n  to retry a maximum of `3` times. It exponentially backs off with full jitter upto a max-delay of `2000 ms`.\n   \nThe maximum number of retries and the maximum back off period can be set:\n```\nsasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required awsMaxRetries=\"7\" awsMaxBackOffTimeMs=\"500\";\n```\nThis sets the maximum number of retries to `7` and the maximum back off time to `500 ms`.\n\nThe retries can be turned off completely by setting `awsMaxRetries` to `\"0\"`.\n\n\n## Setting EKS Service Account\n \nIf your Kafka Client, Producer or Consumer, is running on EKS, you can use EKS service accounts to distribute IAM credentials. The [EKS service account documentation](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) is a good place to start. Following steps cover the scenario of cross account IAM access with EKS service account. Our set-up uses VPC peering for cross account network access, and managed EC2 node group on EKS side. Each step below is linked to AWS documentation for more details and troubleshooting:\n1. Create two AWS accounts one for MSK cluster, let's say it has AWS accountId 'A', and one for EKS cluster, let's say it has AWS accountId 'B'.\n2. [Create VPCs](https://docs.aws.amazon.com/directoryservice/latest/admin-guide/gsg_create_vpc.html) in Account 'A' and Account 'B' with different CIDR blocks. \n3. [Set-up VPC Peering](https://docs.aws.amazon.com/vpc/latest/peering/create-vpc-peering-connection.html) between the two VPCs that were created in the step 1.\n4. [Create a new MSK cluster](https://docs.aws.amazon.com/msk/latest/developerguide/create-cluster.html) in the account A with IAM auth enabled.\n5. [Create a new EKS cluster](https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html) in the account B with `--with-oidc` [flag](https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html) to use AWS Identity and Access Management (IAM) roles for service accounts.\n6. [Update security groups](https://docs.aws.amazon.com/quicksight/latest/user/vpc-security-groups.html) for MSK and EKS clusters to allow traffic from each other's CIDR.\n7. [Create an IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html) in the account 'A' which delegates accesss to account 'B'. Attach MSK cluster access policy to this role.\n8. [Create an IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html) in the account 'B' which assumes the role delegated from the account 'A'.\n9. Create a new namespace in the EKS cluster and [create a new service account](https://docs.aws.amazon.com/eks/latest/userguide/associate-service-account-role.html) in that namespace with role created in the step 8.\n10. All apps created under this namespace with the service account from the step 9 will have MSK cluster access. \n\nWith console access to your EKS containers, as in the [EKS example](https://docs.aws.amazon.com/eks/latest/userguide/sample-deployment.html). You can connect, and download the [latest version of Kafka](https://kafka.apache.org/downloads) on the container. It comes with the [kafka CLI](https://kafka.apache.org/quickstart), that you can use for validation. As an example, you can [set-up your config file](#configuring-a-kafka-client-to-use-aws-iam) and use the following command to test topic creation with IAM auth. \n\n```\n./kafka-topics.sh --bootstrap-server <borker-name>:9098 --create --topic test-topic --partitions 1 --replication-factor 3   --command-config <config_file>\n```\n \n## Troubleshooting\n\n### IAMClientCallbackHandler could not be found\n\nA Kafka client configured to use `AWS_MSK_IAM` may see an error that the `IAMClientCallbackHandler` cannot be found:\n\n```\nException in thread \"main\" org.apache.kafka.common.config.ConfigException: Invalid value \nsoftware.amazon.msk.auth.iam.IAMClientCallbackHandler for configuration sasl.client.callback.handler.class: \nClass software.amazon.msk.auth.iam.IAMClientCallbackHandler could not be found.\n```\n\nThat means that this `aws-msk-iam-auth` library is not on the classpath of the Kafka client. Please add the `aws-msk-iam-auth` library \nto the classpath and try again.\n\n\n### Finding out which identity is being used\n\nYou may receive an `Access denied` error and there may be some doubt as to which credential is being exactly used. The \ncredential may be sourced from a role ARN, EC2 instance profile, credential profile etc. This may be particularly so\n when cross account access is being attempted. \n If the client side logging is set to `DEBUG` and the client configuration property includes `awsDebugCreds` set to\n  `true`:\n ```properties\nsasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required awsDebugCreds=true;\n```\nthe client library will print a debug log of the form:\n```\nDEBUG The identity of the credentials is {UserId: ARID4JIC6BCC6OK5OFDFGS:test124,Account: 12345678,Arn: arn:aws:sts::12345678:assumed-role/kada/test124} (software.amazon.msk.auth.iam.internals.MSKCredentialProvider)```\n```\nThe log line provides the IAM Account, IAM user id and the ARN of the IAM Principal corresponding to the credential\n being used. \n The `awsDebugCreds=true` parameter can be combined with any of the other parameters such as `awsRoleArn`,\n  `awsRoleSessionName`. \n  \n Please note that the log level should also be set to `DEBUG` for this information to be logged. \n It is not recommended to run with `awsDebugCreds=true` since it makes an additional remote call.\n\n### Failed Authentication: Too many connects\n\nYou may receive an error, indicating that authentication has failed due to `Too many connects`, similar to:\n```\n ERROR org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-x] Connection to node 3 (...) failed\n authentication due to: [446c81dc-9ab3-4d4b-b174-4ecd9baa406c]: Too many connects\n```\n\nThis is a sign that one or more IAM clients are trying to connect to a particular broker too many times per second and\n the broker is protecting itself.\n\nSetting the `reconnect.backoff.ms` to at least `1000` should help clients backoff and retry\nconnections such that the broker does not need to reject new connections because of the connection rate. \n\nThe broker type determines the limit on the rate of new IAM connections per broker. Please note the limit is not about\n the total number of connections per broker but the rate of new IAM connections per\n broker. See the [limits page for MSK][MSKLimits] for the limit on the rate of new IAM connections per broker for\n  different broker types.\n\n### Kafka Connect: Unsupported callback type\nWhile using the library from a Kafka Connect client, you may see an error of the form:\n```\n[Producer clientId=connector-...] Failed authentication with BROKER (An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: Exception while evaluating challenge [Caused by javax.security.auth.callback.UnsupportedCallbackException: Unsupported callback type:\n```\nThis most commonly occurs when two different class loaders are used to load different classes used by the library. \nIt can happen when the `aws-msk-iam-auth` library is placed on the plugin path for Kafka Connect. Since the \nlibrary is actually used by the Kafka producer and consumer clients and not the Kafka Connect plugin itself, \nit should be placed in a location that is on the classpath but outside the plugin path. This should ensure that Kafka\n Connect's `PluginClassLoader` is not used to load classes for the `aws-msk-iam-auth` library.\n\n### Dependency mismatch\nIf you are building the library from source using `gradle build` and copying it over to a Kafka client on that or\n another machine, there is a chance that some dependencies may not be available on the Kafka client machine. In that\n  case, you could instead generate and use the uber jar that packages all the necessary runtime dependencies by \n  running `gradle shadowJar`.\n\n## Details\nThis library introduces a new SASL mechanism called `AWS_MSK_IAM`. The `IAMLoginModule` is used to register the\n `IAMSaslClientProvider` as a `Provider` for the `AWS_MSK_IAM` mechanism. The `IAMSaslClientProvider` is used to\n generate a new `IAMSaslClient` every time a new connection to a Kafka broker is opened or an existing connection\n  is re-authenticated.  \n\nThe `IAMSaslClient` is used to perform the actual SASL authentication for a client. It evaluates challenges and creates\n responses that can be used to authenticate a Kafka client to a Kafka broker configured with AWS IAM authentication\n . Its initial response contains an authentication payload that includes a signature generated using the client's\n  credentials. The `IAMClientCallbackHandler` is used to extract the client credentials that are then used for\n   generating the signature.\n \n The authentication payload and the signature are generated by the `AWS4SignedPayloadGenerator` class based on the\n  parameters specified in `AuthenticationRequestParams`. The authentication payload consists of a JSON object:\n  \n```json\n{\n    \"version\" : \"2020_10_22\",\n    \"host\" : \"<broker address>\",\n    \"user-agent\": \"<user agent string from the client>\",\n    \"action\": \"kafka-cluster:Connect\",\n    \"x-amz-algorithm\" : \"<algorithm>\",\n    \"x-amz-credential\" : \"<clientAWSAccessKeyID>/<date in yyyyMMdd format>/<region>/kafka-cluster/aws4_request\",\n    \"x-amz-date\" : \"<timestamp in yyyyMMdd'T'HHmmss'Z' format>\",\n    \"x-amz-security-token\" : \"<clientAWSSessionToken if any>\",\n    \"x-amz-signedheaders\" : \"host\",\n    \"x-amz-expires\" : \"<expiration in seconds>\",\n    \"x-amz-signature\" : \"<AWS SigV4 signature computed by the client>\"\n}\n``` \n## Generating the authentication payload\nPlease note that all the keys in the authentication payload are in lowercase.\n\nThe values of the following keys in the authentication payload are fixed for a client:\n*  `\"version\"` currently always has the value `\"2020_10_22\"`\n* `\"user-agent\"` is a string passed in by the client library to describe the client. The simplest user agent is `\"<name\n of client library>\"`. However, more details can be added to the user agent as well `\"<name of client library\n /<version of client library>/<os and version>/<version of language>\"`\n\nThe values of the remaining keys will be generated as part of calculating the signature of the authentication payload\n. The signature is calculated by following the rules for generating [presigned urls][PreSigned]. Although, this library\n  uses the `AWS4Signer` from the [AWS SDK for Java][AwsSDK] to generate the signature we outline the steps followed\n in calculating the signature and generating the authentication payload from it.\n  \n The inputs to this calculation are \n 1. The AWS Credentials that will be used to sign the authentication payload. This has 3 parts: the client `AWSAccessKeyId`,\n  the client `AWSSecretyKeyId` and the optional client `SessionToken`.\n 1. The hostname of the Kafka broker to which the client wishes to connect.\n 1. The AWS region in which the Kafka broker exists.\n 1. The timestamp when the connection is being established.\n \nThe steps in the calculation are:\n1. Generate a canonical request based on the inputs.\n1. Generate a string to sign based on the canonical request.\n1. Calculate the signature based on the string to sign and the inputs.\n1. Put it all together in the authentication payload.\n \n### Generate a Canonical Request\nWe start by generating a canonical request with an empty payload based on the inputs.\nThe canonical request in this case has the following form\n```\n\"GET\\n\"+\n\"/\\n\"+\n<CanonicalQueryString>+\"\\n\"+\n<CanonicalHeaders>+\"\\n\"+\n<SignedHeaders>+\"\\n\"+\n<HashedPayload>\n```\n\n#### Canonical Query String\n`<CanonicalQueryString>` specifies the authentication parameters as URI-encoded query string parameters. We URI-encode\n query parameter names and values  individually. We also sort the parameters in the canonical query string\n  alphabetically by key name. The sorting occurs after encoding. \nThe `<CanonicalQueryString>` can be calculated by:\n```\nUriEncode(\"Action\")+\"=\"+UriEncode(\"kafka-cluster:Connect\")+\"&\"+\nUriEncode(\"X-Amz-Algorithm\")+\"=\"+UriEncode(\"AWS4-HMAC-SHA256\") + \"&\" +\nUriEncode(\"X-Amz-Credential\")+\"=\"+UriEncode(\"<clientAWSAccessKeyID>/<timestamp in yyyyMMdd format>/<AWS region>/kafka-cluster/aws4_request\") + \"&\" +\nUriEncode(\"X-Amz-Date\")+\"=\"+UriEncode(\"<timestamp in yyyyMMdd'T'HHmmss'Z' format>\") + \"&\" +\nUriEncode(\"X-Amz-Expires\")+\"=\"+UriEncode(\"900\") + \"&\" +\nUriEncode(\"X-Amz-Security-Token\")+\"=\"+UriEncode(\"<client Session Token>\") + \"&\" +\nUriEncode(\"X-Amz-SignedHeaders\")+\"=\"+UriEncode(\"host\")\n```\nThe exact definition of URIEncode from generating [presigned urls][PreSigned] maybe found [later](#UriEncode).\n\nThe query string parameters are in order:\n* `\"Action\"`: Always has the value `\"kafka-cluster:Connect\"` \n* `\"X-Amz-Algorithm\"`: Describes the algorithm used to calculate the signature. Currently it is `\"AWS4-HMAC-SHA256\"`\n* `\"X-Amz-Credential\"`: Contains the access key ID, timestamp in `yyyyMMdd` format, the scope of the credential and\n the constant string `aws4_request`. The scope is defined as the AWS region of the Kafka broker and the name of the \n service (\"kafka-cluster\" in this case). For example if the broker is in `us-west-2` region, the scope is `us-west-2/kafka-cluster`.\n  This scope will be used again later to calculate the signature in [String To Sign](#string-to-sign) and must \n  match the one  used to calculate the signature. \n* `\"X-Amz-Date\"`: The date and time format must follow the ISO 8601 standard, and must be formatted with the\n \"yyyyMMddTHHmmssZ\" format. The date and time must be in UTC.\n* `\"X-Amz-Expires\"` :  Provides the time period, in seconds, for which the generated presigned URL is valid. We\n recommend 900 seconds.\n* `\"X-Amz-Security-Token\"`: The session token if it is specified as part of the AWS Credential. Otherwise this query\n parameter is skipped.\n* `\"X-Amz-Signedheaders\"` is currently always set to `host`.\n\n#### Canonical Header\n`<CanonicalHeaders>` is a list of request headers with their values.  Header names must be in lowercase. \nIndividual header name and value pairs are separated by the newline character (`\"\\n\"`). In this case there is just one\n header. So `<CanonicalHeaders>` is calculated by:\n ```\n\"host\"+\":\"+\"<broker host name>\"+\"\\n\"\n```\n\n#### Signed Headers\n`<SignedHeaders>` is an alphabetically sorted, semicolon-separated list of lowercase request header names. In this\n case there is just one header. So `<SignedHeaders>` is calculated by:\n ```\n\"host\"\n```\n#### Hashed Payload\nSince the payload is empty the `<HashedPayload>` is calculated as\n```\nHex(SHA256Hash(\"\"))\n```\nwhere \n* `Hex` is a function to do lowercase base 16 encoding.\n* `SHA256Hash` is a Secure Hash Algorithm (SHA) cryptographic hash function.\n\n### String To Sign\nFrom the canonical string, we derive the string that will be used to sign the authentication payload.\nThe String to Sign is calculated as:\n```\n\"AWS4-HMAC-SHA256\" + \"\\n\" +\n\"<timestamp in yyyyMMdd format>\" + \"\\n\" +\n<Scope> + \"\\n\" +\nHex(SHA256Hash(<CanonicalRequest>))\n```\nwhere \n* `Hex` is a function to do lowercase base 16 encoding.\n* `SHA256Hash` is a Secure Hash Algorithm (SHA) cryptographic hash function.\n\nThe `<Scope>` is defined as the AWS region of the Kafka broker and the name of the service (\"kafka-cluster\" in this\n case). For example if the broker is in `us-west-2` region, the scope is `\"us-west-2/kafka-cluster\"`. It must be the same scope\nas was defined for the `\"X-Amz-Credential\"` query parameter while generating the [canonical query string](#canonical-query-string).\n\n### Calculate Signature\nThe signature is calculated by:\n```\nDateKey              = HMAC-SHA256(\"AWS4\"+\"<client AWSSecretAccessKey>\", \"<timestanp in YYYYMMDD>\")\nDateRegionKey        = HMAC-SHA256(<DateKey>, \"<aws-region>\")\nDateRegionServiceKey = HMAC-SHA256(<DateRegionKey>, \"kafka-cluster\")\nSigningKey           = HMAC-SHA256(<DateRegionServiceKey>, \"aws4_request\")\nSignature            = Hex(HMAC-SHA256(<SigningKey>, <StringToSign>))\n```\nwhere\n* `Hex` is a function to do lowercase base 16 encoding.\n* `HMAC-SHA256` is a function that computes HMAC by using the SHA256 algorithm with the signing key provided. \n\nThe `<Signature>` is the final signature.\n\n## Putting it all together\nAs mentioned earlier, the authentication payload is a json object with certain keys. All the keys are in lower case.\nThe following keys in the authentication payload json are constant:\n* `\"version\"` \n* `\"user-agent\"` \n\nAll the query parameters calculated earlier are added to the authentication payload json. The keys are lower case\n strings and the values are the ones calculated [earlier](#canonical-query-string) but the values are not uri encoded:\n* `\"action\"`\n* `\"x-amz-algorithm\"`\n* `\"x-amz-credential\"`\n* `\"x-amz-date\"`\n* `\"x-amz-expires\"`\n* `\"x-amz-security-token\"`\n* `\"x-amz-signedheaders\"`\n\nThe host header is added to the authentication payload json\n* `\"host\"` and its value is set to the hostname of the broker being connected.\n\nThe `<Signature>` calculated in the [previous step](#calculate-signature) is added to the authentication payload json as\n* `\"x-amz-signature\"` and its value is set to`<Signature>` \n\nThis finally yields the authentication payload that looks like\n```json\n{\n    \"version\" : \"2020_10_22\",\n    \"host\" : \"<broker address>\",\n    \"user-agent\": \"<user agent string from the client>\",\n    \"action\": \"kafka-cluster:Connect\",\n    \"x-amz-algorithm\" : \"<algorithm>\",\n    \"x-amz-credential\" : \"<clientAWSAccessKeyID>/<date in yyyyMMdd format>/<region>/kafka-cluster/aws4_request\",\n    \"x-amz-date\" : \"<timestamp in yyyyMMdd'T'HHmmss'Z' format>\",\n    \"x-amz-security-token\" : \"<clientAWSSessionToken if any>\",\n    \"x-amz-signedheaders\" : \"host\",\n    \"x-amz-expires\" : \"<expiration in seconds>\",\n    \"x-amz-signature\" : \"<AWS SigV4 signature computed by the client>\"\n}\n``` \n## Message Exchange with Kafka Broker\n\nThis authentication payload is sent as the first message from the client to the Kafka broker. The kafka broker then\n responds with a challenge. We expect a non-empty response from the broker if authentication using AWS IAM succeeded.\nThe authentication response is a json object that may be logged:\n\n```json\n{\n  \"version\" : \"2020_10_22\",\n  \"request-id\" : \"<broker generated request id>\"\n}\n```\nThe `request-id` which is generated by the broker can be useful for debugging issues with AWS IAM authentication\n between the client and the broker.\n\n\n\n \n### UriEncode\nSnipped from the detailed rules for generating [presigned urls][PreSigned].\nURI encode every byte. UriEncode() must enforce the following rules:\n\n* URI encode every byte except the unreserved characters: 'A'-'Z', 'a'-'z', '0'-'9', '-', '.', '_', and '~'.\n* The space character is a reserved character and must be encoded as \"%20\" (and not as \"+\").\n* Each URI encoded byte is formed by a '%' and the two-digit hexadecimal value of the byte.\n* Letters in the hexadecimal value must be uppercase, for example \"%1A\".\n* Encode the forward slash character, '/', everywhere except in the object key name. For example, if the object key\n name is photos/Jan/sample.jpg, the forward slash in the key name is not encoded.\n\nThe following is an example UriEncode() function in Java.\n\n```java\npublic class UriEncode {\npublic static String UriEncode(CharSequence input, boolean encodeSlash) {\n          StringBuilder result = new StringBuilder();\n          for (int i = 0; i < input.length(); i++) {\n              char ch = input.charAt(i);\n              if ((ch >= 'A' && ch <= 'Z') || (ch >= 'a' && ch <= 'z') || (ch >= '0' && ch <= '9') || ch == '_' || ch == '-' || ch == '~' || ch == '.') {\n                  result.append(ch);\n              } else if (ch == '/') {\n                  result.append(encodeSlash ? \"%2F\" : ch);\n              } else {\n                  result.append(toHexUTF8(ch));\n              }\n          }\n          return result.toString();\n      } \n}\n```\n   \n## Release Notes\n\n### Release 2.0.3\n- Upgrade AWS SKD version\n- Fix SSO OIDC error\n\n### Release 2.0.2\n- Add refreshing of credentials before Oauth token generation\n\n### Release 2.0.1\n- Enable STS region support to set regional endpoints\n\n### Release 2.0.0\n- Add SASL/OAUTHBEARER mechanism with IAM\n\n### Release 1.1.9\n- Bug fix to revert backward incompatible change for STS regional endpoint\n- Add support for cross-region and cross-account connection to MSK\n\n### Release 1.1.8 (Deprecated)\n- Add support for STS regional endpoint\n- Update AWS SDK dependencies to the latest to address the security vulnerabilities\n\n### Release 1.1.7\n- Add support to pass session credentials to an STS role credential provider\n- Add support for external id for role-based authentication\n\n### Release 1.1.6\n- Update dependencies to address the following security vulnerability\n  * CVE-2022-41915\n- Add support for explicit access key and secret in `sasl.jaas.config`\n\n### Release 1.1.5\n\n- Update dependencies to address the following security vulnerabilities.\n  * CVE-2022-42003\n  * CVE-2022-42004\n- Add support for multi-classloader environments, such as Apache Flink ([#36](https://github.com/aws/aws-msk-iam-auth/issues/36))\n\n### Release 1.1.4\n\n- Update dependencies to address the following security vulnerabilities.\n  * CVE-2021-37136\n  * CVE-2021-37137\n  * CVE-2022-24823\n  * CVE-2021-43797\n  * CVE-2021-38153\n  * CVE-2020-36518\n- Specifically, build and test against Kafka 2.8.\n\n### Release 1.1.3\n\n- Add retries if loading credential fails with client side errors.\n- If AWS STS is not accessible for identifying the credential when `awsDebugCreds=true`, do not fail the connection.\n- Update Troubleshooting section in README.\n\n### Release 1.1.2\n\n- Update log4j version in test dependencies to CVE-2021-44832\n- Allow users to debug the credentials being used\n\n### Release 1.1.1\n* Enable support for STS regional endpoints when configured to assume a role (thanks dvuple@)\n* Additional logging to log the classes and classloaders for `IAMClientCallbackHandler` and `AWSCredentialsCallback\n` classes.\n* README updates to start a section on troubleshooting.\n* In the uber jar do not relocate the awsk sdk v2 modules.\n* In the uber jar, stop shadowing `slf4j-api`.\n\n### Release 1.1.0\n* Add support for credential profiles based on AWS Single Sign-On (SSO).\n* Add support for clients using IAM Roles without using credential profiles.\n* Bug fix for credential profiles with IAM Roles. \n### Release 1.0.0\n* First version of the Amazon MSK Library for IAM Authentication\n\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n\n[MSK]: https://aws.amazon.com/msk/\n[IAM]: https://aws.amazon.com/iam/\n[MSK_IAM]: https://docs.aws.amazon.com/msk/latest/developerguide/iam-access-control.html\n[DefaultCreds]: https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html\n[CredsFile]: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html\n[PreSigned]: https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html\n[AwsSDK]: https://github.com/aws/aws-sdk-java\n[RoleProfileCLI]: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-role.html\n[MSKLimits]: https://docs.aws.amazon.com/msk/latest/developerguide/limits.html\n[StsVpcE]: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_sts_vpce.html\n[PrivateLink]: https://aws.amazon.com/privatelink/?privatelink-blogs.sort-by=item.additionalFields.createdDate&privatelink-blogs.sort-order=desc\n[StsRegionalEndpointsDoc]: https://docs.aws.amazon.com/sdkref/latest/guide/feature-sts-regionalized-endpoints.html\n", "release_dates": ["2024-01-17T16:28:57Z", "2023-12-14T15:31:53Z", "2023-12-04T13:10:00Z", "2023-11-09T19:10:56Z", "2023-08-21T10:08:51Z", "2023-08-10T20:25:58Z", "2023-06-20T19:54:10Z", "2023-02-02T16:04:59Z", "2022-11-02T05:44:56Z", "2022-05-18T15:14:03Z", "2022-03-04T07:58:59Z", "2022-02-02T10:45:27Z", "2021-09-07T21:24:42Z", "2021-06-25T13:49:43Z", "2021-05-05T13:15:29Z"]}, {"name": "aws-msk-iam-sasl-signer-go", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS MSK IAM SASL Signer for Go\n\n[![Go Build status](https://github.com/aws/aws-msk-iam-sasl-signer-go/actions/workflows/go.yml/badge.svg?branch=main)](https://github.com/aws/aws-msk-iam-sasl-signer-go/actions/workflows/go.yml) [![Apache V2 License](https://img.shields.io/badge/license-Apache%20V2-blue.svg)](https://github.com/aws/aws-msk-iam-sasl-signer-go/blob/main/LICENSE.txt)\n[![Security Scan](https://github.com/aws/aws-msk-iam-sasl-signer-go/actions/workflows/securityscan.yml/badge.svg?branch=main)](https://github.com/aws/aws-msk-iam-sasl-signer-go/actions/workflows/securityscan.yml)\n\n`aws-msk-iam-sasl-signer-go` is the AWS MSK IAM SASL Signer for Go programming language.\n\nThe AWS MSK IAM SASL Signer for Go requires a minimum version of `Go 1.17`.\n\nCheck out the [release notes](https://github.com/aws/aws-msk-iam-sasl-signer-go/blob/main/CHANGELOG.md) for information about the latest bug\nfixes, updates, and features added to the library.\n\nJump To:\n* [Getting Started](#getting-started)\n* [Getting Help](#getting-help)\n* [Contributing](#feedback-and-contributing)\n* [More Resources](#resources)\n\n\n## Getting started\nTo get started working with the AWS MSK IAM SASL Signer for Go with your Kafka client library please follow below code sample -\n\n###### Add Dependencies\n```sh\n$ go get github.com/aws/aws-msk-iam-sasl-signer-go\n```\n\n###### Write Code\n\nFor example, you can use the signer library to generate IAM default credentials based OAUTH token with [IBM sarama library](https://github.com/IBM/sarama) as below -\n\n```go\npackage main\n\nimport (\n  \"context\"\n  \"crypto/tls\"\n  \"log\"\n  \"os\"\n  \"os/signal\"\n  \"time\"\n  \n  \"github.com/aws/aws-msk-iam-sasl-signer-go/signer\"\n  \"github.com/IBM/sarama\"\n)\n\nvar (\n  kafkaBrokers = []string{\"<your_msk_bootstrap_string>\"}\n  KafkaTopic = \"<your topic name>\"\n  enqueued int\n)\n\ntype MSKAccessTokenProvider struct {\n}\n\nfunc (m *MSKAccessTokenProvider) Token() (*sarama.AccessToken, error) {\n  token, _, err := signer.GenerateAuthToken(context.TODO(), \"<region>\")\n  return &sarama.AccessToken{Token: token}, err}\n\nfunc main() {\n  sarama.Logger = log.New(os.Stdout, \"[sarama] \", log.LstdFlags)\n  producer, err := setupProducer()\n  if err != nil {\n    panic(err)\n  } else {\n    log.Println(\"Kafka AsyncProducer up and running!\")\n  }\n\n  // Trap SIGINT to trigger a graceful shutdown.\n  signals := make(chan os.Signal, 1)\n  signal.Notify(signals, os.Interrupt)\n\n  produceMessages(producer, signals)\n\n  log.Printf(\"Kafka AsyncProducer finished with %d messages produced.\", enqueued)\n}\n\n// setupProducer will create a AsyncProducer and returns it\nfunc setupProducer() (sarama.AsyncProducer, error){\n  // Set the SASL/OAUTHBEARER configuration\n  config := sarama.NewConfig()\n  config.Net.SASL.Enable = true\n  config.Net.SASL.Mechanism = sarama.SASLTypeOAuth\n  config.Net.SASL.TokenProvider = &MSKAccessTokenProvider{}\n\n  tlsConfig := tls.Config{}\n  config.Net.TLS.Enable = true\n  config.Net.TLS.Config = &tlsConfig\n  return sarama.NewAsyncProducer(kafkaBrokers, config)\n}\n\n// produceMessages will send 'testing 123' to KafkaTopic each second, until receive a os signal to stop e.g. control + c\n// by the user in terminal\nfunc produceMessages(producer sarama.AsyncProducer, signals chan os.Signal) {\n  for {\n    time.Sleep(time.Second)\n    message := &sarama.ProducerMessage{Topic: KafkaTopic, Value: sarama.StringEncoder(\"testing 123\")}\n    select {\n    case producer.Input() <- message:\n      enqueued++\n      log.Println(\"New Message produced\")\n    case <-signals:\n      producer.AsyncClose() // Trigger a shutdown of the producer.\n      return\n    }\n  }\n}\n```\n\nConsumer -\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"crypto/tls\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\t\"os/signal\"\n\n\t\"github.com/aws/aws-msk-iam-sasl-signer-go/signer\"\n\t\"github.com/IBM/sarama\"\n)\n\nvar (\n  kafkaBrokers = []string{\"<your_msk_bootstrap_string>\"}\n  KafkaTopic = \"<your topic name>\"\n)\n\ntype MSKAccessTokenProvider struct {\n}\n\nfunc (m *MSKAccessTokenProvider) Token() (*sarama.AccessToken, error) {\n\ttoken, _, err := signer.GenerateAuthToken(context.TODO(), \"<region>\")\n\treturn &sarama.AccessToken{Token: token}, err\n}\n\nfunc main() {\n\tsarama.Logger = log.New(os.Stdout, \"[sarama] \", log.LstdFlags)\n\tconsumer, err := setUpConsumer()\n\tif err != nil {\n\t\tpanic(err)\n\t} else {\n\t\tlog.Println(\"Kafka Consumer is up and running!\")\n\t}\n\n\tdefer func() {\n\t\tif err := consumer.Close(); err != nil {\n\t\t\tlog.Printf(\"Error closing consumer: %w\", err)\n\t\t}\n\t}()\n\n\tconsumeMessages(consumer)\n}\n\nfunc setUpConsumer() (sarama.Consumer, error) {\n\t// Set the SASL/OAUTHBEARER configuration\n\tconfig := sarama.NewConfig()\n\tconfig.Net.SASL.Enable = true\n\tconfig.Net.SASL.Mechanism = sarama.SASLTypeOAuth\n\tconfig.Net.SASL.TokenProvider = &MSKAccessTokenProvider{}\n\n\ttlsConfig := tls.Config{}\n\tconfig.Net.TLS.Enable = true\n\tconfig.Net.TLS.Config = &tlsConfig\n\treturn sarama.NewConsumer(kafkaBrokers, config)\n}\n\nfunc consumeMessages(consumer sarama.Consumer) {\n\tpartitions, err := consumer.Partitions(KafkaTopic)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to retrieve partitions for topic %s: %v\", KafkaTopic, err)\n\t}\n\n\tconsumers := make(chan *sarama.ConsumerMessage)\n\terrors := make(chan *sarama.ConsumerError)\n\n\t// Create a partition consumer and goroutine for each partition\n\tfor _, partition := range partitions {\n\t\tpartitionConsumer, err := consumer.ConsumePartition(KafkaTopic, partition, sarama.OffsetNewest)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Failed to create partition consumer for topic %s, partition %d: %v\", KafkaTopic, partition, err)\n\t\t}\n\n\t\tgo func(KafkaTopic string, partitionConsumer sarama.PartitionConsumer) {\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase consumerError := <-partitionConsumer.Errors():\n\t\t\t\t\terrors <- consumerError\n\n\t\t\t\tcase msg := <-partitionConsumer.Messages():\n\t\t\t\t\tconsumers <- msg\n\t\t\t\t}\n\t\t\t}\n\t\t}(KafkaTopic, partitionConsumer)\n\t}\n\n\tsignals := make(chan os.Signal, 1)\n\tsignal.Notify(signals, os.Interrupt)\n\n\tmsgCount := 0\n\n\tdoneCh := make(chan struct{})\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase msg := <-consumers:\n\t\t\t\tmsgCount++\n\t\t\t\tfmt.Println(\"Received message : \", string(msg.Key), string(msg.Value))\n\t\t\tcase consumerError := <-errors:\n\t\t\t\tmsgCount++\n\t\t\t\tfmt.Println(\"Received consumerError \", string(consumerError.Topic), string(consumerError.Partition), consumerError.Err)\n\t\t\t\tdoneCh <- struct{}{}\n\t\t\tcase <-signals:\n\t\t\t\tfmt.Println(\"Interrupt is detected\")\n\t\t\t\tdoneCh <- struct{}{}\n\t\t\t}\n\t\t}\n\t}()\n\n\t<-doneCh\n\tfmt.Println(\"Processed\", msgCount, \"messages\")\n}\n\n\n```\n\n* To use IAM credentials from a named profile, update the Token() function: \n```go\nfunc (t *MSKAccessTokenProvider) Token() (*sarama.AccessToken, error) {\n\ttoken, _, err := signer.GenerateAuthTokenFromProfile(context.TODO(), \"<region>\", \"<namedProfile>\")\n\treturn &sarama.AccessToken{Token: token}, err\n}\n```\n\n* To use IAM credentials by assuming a IAM Role using sts, update the Token() function:\n\n```go\nfunc (t *MSKAccessTokenProvider) Token() (*sarama.AccessToken, error) {\n        token, _, err := signer.GenerateAuthTokenFromRole(context.TODO(), \"<region>\", \"<my-role-arn>\", \"my-sts-session-name\")\n        return &sarama.AccessToken{Token: token}, err\n}\n```\n* To use IAM credentials from a credentials provider, update the Token() function:\n```go\nfunc (t *MSKAccessTokenProvider) Token() (*sarama.AccessToken, error) {\n        token, _, err := signer.GenerateAuthTokenFromCredentialsProvider(context.TODO(), \"<region>\", <MyCredentialsProvider>)\n        return &sarama.AccessToken{Token: token}, err\n}\n```\n\n\n###### Compile and Execute\n```sh\n$ go build\n$ go run .\n```\n\n###### Test\n```sh\n$ cd signer\n$ go test\n```\n\n## Troubleshooting\n### Finding out which identity is being used\nYou may receive an `Access denied` error and there may be some doubt as to which credential is being exactly used. The credential may be sourced from a role ARN, EC2 instance profile, credential profile etc.\nYou can set the field `AwsDebugCreds` set to true before getting the token:\n\n```go\n    signer.AwsDebugCreds = true\n```\nthe client library will print a debug log of the form:\n```\nCredentials Identity: {UserId: ABCD:test124, Account: 1234567890, Arn: arn:aws:sts::1234567890:assumed-role/abc/test124}\n```\n\nThe log line provides the IAM Account, IAM user id and the ARN of the IAM Principal corresponding to the credential being used.\n\nPlease note that the log level should also be set to DEBUG for this information to be logged. It is not recommended to run with AwsDebugCreds=true since it makes an additional remote call.\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues\nfor tracking bugs and feature requests.\n\n* Ask us a [question](https://github.com/aws/aws-msk-iam-sasl-signer-go/discussions/new?category=q-a) or open a [discussion](https://github.com/aws/aws-msk-iam-sasl-signer-go/discussions/new?category=general).\n* If you think you may have found a bug, please open an [issue](https://github.com/aws/aws-msk-iam-sasl-signer-go/issues/new/choose).\n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n\nThis repository provides a pluggable library with any Go Kafka client for SASL/OAUTHBEARER mechanism. For more information about SASL/OAUTHBEARER mechanism please go to [KIP 255](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=75968876).\n\n### Opening Issues\n\nIf you encounter a bug with the AWS MSK IAM SASL Signer for Go we would like to hear about it.\nSearch the [existing issues][Issues] and see\nif others are also experiencing the same issue before opening a new issue. Please\ninclude the version of AWS MSK IAM SASL Signer for Go, Go language, and OS you\u2019re using. Please\nalso include reproduction case when appropriate.\n\nThe GitHub issues are intended for bug reports and feature requests. For help\nand questions with using AWS MSK IAM SASL Signer for Go, please make use of the resources listed\nin the [Getting Help](#getting-help) section.\nKeeping the list of open issues lean will help us respond in a timely manner.\n\n## Feedback and contributing\n\nThe AWS MSK IAM SASL Signer for Go will use GitHub [Issues] to track feature requests and issues with the library. In addition, we'll use GitHub [Projects] to track large tasks spanning multiple pull requests, such as refactoring the library's internal request lifecycle. You can provide feedback to us in several ways.\n\n**GitHub issues**. To provide feedback or report bugs, file GitHub [Issues] on the library. This is the preferred mechanism to give feedback so that other users can engage in the conversation, +1 issues, etc. Issues you open will be evaluated, and included in our roadmap for the GA launch.\n\n**Contributing**. You can open pull requests for fixes or additions to the AWS MSK IAM SASL Signer for Go. All pull requests must be submitted under the Apache 2.0 license and will be reviewed by a team member before being merged in. Accompanying unit tests, where possible, are appreciated.\n\n## Resources\n\n[Service Documentation](https://docs.aws.amazon.com/msk/latest/developerguide/getting-started.html) - Use this\ndocumentation to learn how to interface with AWS MSK.\n\n[Issues] - Report issues, submit pull requests, and get involved\n  (see [Apache 2.0 License][license])\n\n[Dep]: https://github.com/golang/dep\n[Issues]: https://github.com/aws/aws-msk-iam-sasl-signer-go/issues\n[Projects]: https://github.com/aws/aws-msk-iam-sasl-signer-go/projects\n[CHANGELOG]: https://github.com/aws/aws-msk-iam-sasl-signer-go/blob/main/CHANGELOG.md\n[design]: https://github.com/aws/aws-msk-iam-sasl-signer-go/blob/main/DESIGN.md\n[license]: http://aws.amazon.com/apache2.0/\n", "release_dates": ["2023-11-08T20:51:26Z"]}, {"name": "aws-msk-iam-sasl-signer-js", "description": null, "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS MSK IAM SASL Signer for JavaScript\n\n[![Build status](https://github.com/aws/aws-msk-iam-sasl-signer-js/actions/workflows/ci.yml/badge.svg)](https://github.com/aws/aws-msk-iam-sasl-signer-js/actions/workflows/ci.yml) \n[![Apache V2 License](https://img.shields.io/badge/license-Apache%20V2-blue.svg)](https://github.com/aws/aws-msk-iam-sasl-signer-js/blob/main/LICENSE)\n[![Security Scan](https://github.com/aws/aws-msk-iam-sasl-signer-js/actions/workflows/securityscan.yml/badge.svg?branch=main)](https://github.com/aws/aws-msk-iam-sasl-signer-js/actions/workflows/securityscan.yml)\n\n`aws-msk-iam-sasl-signer-js` is the AWS MSK IAM SASL Signer for JavaScript programming language.\n\nThe AWS MSK IAM SASL Signer for JavaScript is compatible with Node.js version 14.x and later.\n\nCheck out the [release notes](https://github.com/aws/aws-msk-iam-sasl-signer-js/blob/main/CHANGELOG.md) for information about the latest bug\nfixes, updates, and features added to the library.\n\nJump To:\n* [Getting Started](#getting-started)\n* [Getting Help](#getting-help)\n* [Contributing](#feedback-and-contributing)\n* [More Resources](#resources)\n\n\n## Getting started\nTo get started working with the AWS MSK IAM SASL Signer for JavaScript with your Kafka client library please follow below code sample -\n\n###### Add Dependencies\n ```sh\n $ npm install https://github.com/aws/aws-msk-iam-sasl-signer-js\n ```\n\n###### Write Code\n\nFor example, you can use the signer library to generate IAM based OAUTH token with tulios/kafkajs library as below -\n\n ```js\nconst { Kafka } = require('kafkajs')\nconst { generateAuthToken } = require('aws-msk-iam-sasl-signer-js')\n\nasync function oauthBearerTokenProvider(region) {\n    // Uses AWS Default Credentials Provider Chain to fetch credentials\n    const authTokenResponse = await generateAuthToken({ region });\n    return {\n        value: authTokenResponse.token\n    }\n}\n\nconst run = async () => {\n    const kafka = new Kafka({\n        clientId: 'my-app',\n        brokers: ['kafka1:9092', 'kafka2:9092'],\n        ssl: true,\n        sasl: {\n            mechanism: 'oauthbearer',\n            oauthBearerProvider: () => oauthBearerTokenProvider('us-east-1')\n        }\n    })\n\n    const producer = kafka.producer()\n    const consumer = kafka.consumer({ groupId: 'test-group' })\n\n    // Producing\n    await producer.connect()\n    await producer.send({\n        topic: 'test-topic',\n        messages: [\n            { value: 'Hello KafkaJS user!' },\n        ],\n    })\n\n    // Consuming\n    await consumer.connect()\n    await consumer.subscribe({ topic: 'test-topic', fromBeginning: true })\n\n    await consumer.run({\n        eachMessage: async ({ topic, partition, message }) => {\n            console.log({\n                partition,\n                offset: message.offset,\n                value: message.value.toString(),\n            })\n        },\n    })\n}\n\nrun().catch(console.error)\n ```\n\n## More examples of generating auth token\n\n### Specifying an alternate credential profile for a client\n\n```js\nconst authTokenResponse = await generateAuthTokenFromProfile({\n    region: \"AWS region\",\n    awsProfileName: \"<Credential Profile Name>\"\n});\n```\n\n### Specifying a role based credential profile for a client\n\n```js\nconst authTokenResponse = await generateAuthTokenFromRole({\n    region: \"AWS region\",\n    awsRoleArn: \"<IAM Role ARN>\",\n    awsRoleSessionName: \"<Optional session name>\"\n});\n```\n\n### Specifying AWS Credential Provider for a client\n\n```js\nconst authTokenResponse = await generateAuthTokenFromCredentialsProvider({\n    region: \"AWS region\",\n    awsCredentialsProvider: fromNodeProviderChain()\n});\n```\n\nFind [more examples](https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/modules/_aws_sdk_credential_providers.html) of creating credentials provider using AWS SDK for JavaScript v3.\n\n## Troubleshooting\n### Finding out which identity is being used\nYou may receive an `Access denied` error and there may be some doubt as to which credential is being exactly used. The credential may be sourced from a role ARN, EC2 instance profile, credential profile etc.\nIf the client side logging is set to DEBUG and the client configuration property includes `logger`, and `awsDebugCreds` set to true:\n\n```js\nconst authTokenResponse = await generateAuthToken({\n    region: \"AWS region\",\n    logger: console,\n    awsDebugCreds: true\n});\n```\nthe client library will print a debug log of the form:\n```\nCredentials Identity: {UserId: ABCD:test124, Account: 1234567890, Arn: arn:aws:sts::1234567890:assumed-role/abc/test124}\n```\n\nThe log line provides the IAM Account, IAM user id and the ARN of the IAM Principal corresponding to the credential being used.\nThe awsDebugCreds=true parameter can be combined with any of the above token generation function.\n\nPlease note that the log level should also be set to DEBUG for this information to be logged. It is not recommended to run with awsDebugCreds=true since it makes an additional remote call.\n\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues\nfor tracking bugs and feature requests.\n\n* Ask us a [question](https://github.com/aws/aws-msk-iam-sasl-signer-js/discussions/new?category=q-a) or open a [discussion](https://github.com/aws/aws-msk-iam-sasl-signer-js/discussions/new?category=general).\n* If you think you may have found a bug, please open an [issue](https://github.com/aws/aws-msk-iam-sasl-signer-js/issues/new/choose).\n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n\nThis repository provides a pluggable library with any JavaScript Kafka client for SASL/OAUTHBEARER mechanism. For more information about SASL/OAUTHBEARER mechanism please go to [KIP 255](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=75968876).\n\n### Opening Issues\n\nIf you encounter a bug with the AWS MSK IAM SASL Signer for JavaScript we would like to hear about it.\nSearch the [existing issues][Issues] and see\nif others are also experiencing the same issue before opening a new issue. Please\ninclude the version of AWS MSK IAM SASL Signer for JavaScript, Node.js version, and OS you\u2019re using. Please\nalso include reproduction case when appropriate.\n\nThe GitHub issues are intended for bug reports and feature requests. For help\nand questions with using AWS MSK IAM SASL Signer for JavaScript, please make use of the resources listed\nin the [Getting Help](#getting-help) section.\nKeeping the list of open issues lean will help us respond in a timely manner.\n\n## Feedback and contributing\n\nThe AWS MSK IAM SASL Signer for JavaScript will use GitHub [Issues] to track feature requests and issues with the library. In addition, we'll use GitHub [Projects] to track large tasks spanning multiple pull requests, such as refactoring the library's internal request lifecycle. You can provide feedback to us in several ways.\n\n**GitHub issues**. To provide feedback or report bugs, file GitHub [Issues] on the library. This is the preferred mechanism to give feedback so that other users can engage in the conversation, +1 issues, etc. Issues you open will be evaluated, and included in our roadmap for the GA launch.\n\n**Contributing**. You can open pull requests for fixes or additions to the AWS MSK IAM SASL Signer for JavaScript. All pull requests must be submitted under the Apache 2.0 license and will be reviewed by a team member before being merged in. Accompanying unit tests, where possible, are appreciated.\n\n## Resources\n\n[Developer Guide](https://aws.github.io/aws-msk-iam-sasl-signer-js/docs/) - Use this document to learn how to get started and\nuse the AWS MSK IAM SASL Signer for JavaScript.\n\n[Service Documentation](https://docs.aws.amazon.com/msk/latest/developerguide/getting-started.html) - Use this\ndocumentation to learn how to interface with AWS MSK.\n\n[Issues] - Report issues, submit pull requests, and get involved\n(see [Apache 2.0 License][license])\n\n[Issues]: https://github.com/aws/aws-msk-iam-sasl-signer-js/issues\n[Projects]: https://github.com/aws/aws-msk-iam-sasl-signer-js/projects\n[CHANGELOG]: https://github.com/aws/aws-msk-iam-sasl-signer-js/blob/main/CHANGELOG.md\n[license]: http://aws.amazon.com/apache2.0/", "release_dates": ["2023-11-08T19:40:46Z"]}, {"name": "aws-msk-iam-sasl-signer-net", "description": null, "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS MSK IAM SASL Signer for .NET\n \n[![Build status](https://github.com/aws/aws-msk-iam-sasl-signer-net/actions/workflows/build.yml/badge.svg)](https://github.com/aws/aws-msk-iam-sasl-signer-net/actions/workflows/build.yml) \n[![Apache V2 License](https://img.shields.io/badge/license-Apache%20V2-blue.svg)](https://github.com/aws/aws-msk-iam-sasl-signer-net/blob/main/LICENSE.txt)\n[![Security Scan](https://github.com/aws/aws-msk-iam-sasl-signer-net/actions/workflows/securityscan.yml/badge.svg?branch=main)](https://github.com/aws/aws-msk-iam-sasl-signer-net/actions/workflows/securityscan.yml)\n\n`aws-msk-iam-sasl-signer-net` is the AWS MSK IAM SASL Signer for .NET. \n\nThis libary vends encoded IAM v4 signatures which can be used as IAM Auth tokens to authenticate against an MSK cluster. \n \nThe AWS MSK IAM SASL Signer for .NET has a target framework of [netstandard2.0](https://learn.microsoft.com/en-us/dotnet/standard/net-standard?tabs=net-standard-2-0)\n \nCheck out the [release notes](https://github.com/aws/aws-msk-iam-sasl-signer-net/blob/main/CHANGELOG.md) for information about the latest bug\nfixes, updates, and features added to the library.\n \nJump To:\n* [Getting Started](#getting-started)\n* [Getting Help](#getting-help)\n* [Feedback and Contributing](#contributing)\n* [More Resources](#resources)\n \n \n## <a name=\"getting-started\"></a> Getting started\nTo get started working with the AWS MSK IAM SASL Signer for .NET with your Kafka client library please follow below code sample -\n \n###### Add Dependencies\n \n AWS MSK IAM SASL SIGNER is distribured via NuGet. We provide the package [AWS.MSK.Auth](https://www.nuget.org/packages/AWS.MSK.Auth/) which can be imported via NuGet in your development environment. \n \n###### Write Code\n \nFor example, you can use the signer library to generate IAM based OAUTH token with [confluent-kafka-dotnet](https://github.com/confluentinc/confluent-kafka-dotnet) library as below -\n \n ```cs\n    var producerConfig = new ProducerConfig\n    {\n        BootstrapServers = < BOOTSTRAP - SERVER - HERE >,\n        SecurityProtocol = SecurityProtocol.SaslSsl,\n        SaslMechanism = SaslMechanism.OAuthBearer\n    };\n\n    AWSMSKAuthTokenGenerator mskAuthTokenGenerator = new AWSMSKAuthTokenGenerator();\n\n    //Callback to handle OAuth bearer token refresh. It fetches the OAUTH Token from the AWSMSKAuthTokenGenerator class. \n    void OauthCallback(IClient client, string cfg)\n    {\n        try\n        {\n            var (token, expiryMs) = await mskAuthTokenGenerator.GenerateAuthTokenAsync(Amazon.RegionEndpoint.USEast1);\n            client.OAuthBearerSetToken(token, expiryMs, \"DummyPrincipal\");\n        }\n        catch (Exception e)\n        {\n            client.OAuthBearerSetTokenFailure(e.ToString());\n        }\n    }\n\n    var producer = new ProducerBuilder<string, string>(producerConfig)\n                        .SetOAuthBearerTokenRefreshHandler(OauthCallback).Build();\n            try\n            {\n                var deliveryReport = await producer.ProduceAsync(\"test-topic\", new Message<string, string> { Value = \"Hello from .NET\" });\n\n                Console.WriteLine($\"Produced message to {deliveryReport.TopicPartitionOffset}\");\n            }\n            catch (ProduceException<string, string> e)\n            {\n                Console.WriteLine($\"failed to deliver message: {e.Message} [{e.Error.Code}]\");\n            }\n ```\n \n## More examples of generating auth token\n \n### Specifying an alternate credential profile for a client\n \n```cs\nAWSMSKAuthTokenGenerator mskAuthTokenGenerator = new AWSMSKAuthTokenGenerator();\nvar (token, expiryMs) = await mskAuthTokenGenerator.GenerateAuthTokenFromProfileAsync(Amazon.RegionEndpoint.USEast1, \"profileName\");\n```\n \n### Specifying a role based credential for a client\n \n```cs\nAWSMSKAuthTokenGenerator mskAuthTokenGenerator = new AWSMSKAuthTokenGenerator();\nvar (token, expiryMs) = await mskAuthTokenGenerator.GenerateAuthTokenFromRoleAsync(Amazon.RegionEndpoint.USEast1, \"roleName\", \"roleSessioName\");\n```\n\nNote that roleSessionName is optional here. A default name is used if not specified. This uses the default token expiry, and creates a new STS client for every invocation. \nFor higher configurability, use the method mentioned below which takes a credentials provider as an input. This allows you to bring your own credentials for signing the request. \n \n### <a name=\"credential-provider-method\"></a> Specifying AWS Credential Provider for a client\n \n```cs\nAWSMSKAuthTokenGenerator mskAuthTokenGenerator = new AWSMSKAuthTokenGenerator();\nvar (token, expiryMs) = await mskAuthTokenGenerator.GenerateAuthTokenFromCredentialsProviderAsyc(Amazon.RegionEndpoint.USEast1, () => new BasicAWSCredentials(\"secretKey\", \"accessKey\"));\n```\n \n## <a name=\"troubleshooting\"></a> Troubleshooting\n\n### <a name=\"debug-creds\"></a> Finding out which identity is being used\n\nWhen using default credentials, You may receive an Access denied error and there may be some doubt as to which credential is being exactly used. The credential may be sourced from a role ARN, EC2 instance profile, credential profile etc.\n\nYou can set the optional parameter awsDebugCreds set to true before getting the token in such cases. \n\n```cs\nvar (token, expiryMs) = mskAuthTokenGenerator.GenerateAuthTokenAsync(Amazon.RegionEndpoint.USEast1, awsDebugCreds:true);\n\n```\n\nThe client library will print a debug log of the form:\n\n```\n\"Credentials Identity: UserId: ABCD:test124, Account: 1234567890, Arn: arn:aws:sts::1234567890:assumed-role/abc/test124\"\n```\n \n## <a name=\"getting-help\"></a> Getting Help\n \nPlease use these community resources for getting help. We use the GitHub issues\nfor tracking bugs and feature requests.\n \n* Ask us a [question](https://github.com/aws/aws-msk-iam-sasl-signer-net/discussions/new?category=q-a) or open a [discussion](https://github.com/aws/aws-msk-iam-sasl-signer-net/discussions/new?category=general).\n* If you think you may have found a bug, please open an [issue](https://github.com/aws/aws-msk-iam-sasl-signer-net/issues/new/choose).\n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n \nThis repository provides a pluggable library with any .NET Kafka client for SASL/OAUTHBEARER mechanism. For more information about SASL/OAUTHBEARER mechanism please go to [KIP 255](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=75968876).\n \n### Opening Issues\n \nIf you encounter a bug with the AWS MSK IAM SASL Signer for .NET we would like to hear about it.\nSearch the [existing issues][Issues] and see\nif others are also experiencing the same issue before opening a new issue. Please\ninclude the version of AWS MSK IAM SASL Signer for .NET, and OS you\u2019re using. Please\nalso include reproduction case when appropriate.\n \nThe GitHub issues are intended for bug reports and feature requests. For help\nand questions with using AWS MSK IAM SASL Signer for .NET, please make use of the resources listed\nin the [Getting Help](#getting-help) section.\nKeeping the list of open issues lean will help us respond in a timely manner.\n \n## <a name=\"contributing\"></a> Feedback and contributing\n \nThe AWS MSK IAM SASL Signer for .NET will use GitHub [Issues] to track feature requests and issues with the library. In addition, we'll use GitHub [Projects] to track large tasks spanning multiple pull requests, such as refactoring the library's internal request lifecycle. You can provide feedback to us in several ways.\n \n**GitHub issues**. To provide feedback or report bugs, file GitHub [Issues] on the library. This is the preferred mechanism to give feedback so that other users can engage in the conversation, +1 issues, etc. Issues you open will be evaluated, and included in our roadmap for the GA launch.\n \n**Contributing**. You can open pull requests for fixes or additions to the AWS MSK IAM SASL Signer for .NET. All pull requests must be submitted under the Apache 2.0 license and will be reviewed by a team member before being merged in. Accompanying unit tests, where possible, are appreciated.\n \n## <a name=\"resources\"></a> Resources\n \n[Developer Guide](https://aws.github.io/aws-msk-iam-sasl-signer-net/docs/) - Use this document to learn how to get started and\nuse the AWS MSK IAM SASL Signer for .NET.\n \n[Service Documentation](https://docs.aws.amazon.com/msk/latest/developerguide/getting-started.html) - Use this\ndocumentation to learn how to interface with AWS MSK.\n \n[Issues] - Report issues, submit pull requests, and get involved\n(see [Apache 2.0 License][license])\n \n[Issues]: https://github.com/aws/aws-msk-iam-sasl-signer-net/issues\n[Projects]: https://github.com/aws/aws-msk-iam-sasl-signer-net/projects\n[CHANGELOG]: https://github.com/aws/aws-msk-iam-sasl-signer-net/blob/main/CHANGELOG.md\n[license]: http://aws.amazon.com/apache2.0/", "release_dates": ["2023-11-10T20:43:16Z"]}, {"name": "aws-msk-iam-sasl-signer-python", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2024-01-24T18:22:17Z", "2023-11-10T17:21:35Z"]}, {"name": "aws-mwaa-local-runner", "description": "This repository provides a command line interface (CLI) utility that replicates an Amazon Managed Workflows for Apache Airflow (MWAA) environment locally.", "language": "Shell", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# About aws-mwaa-local-runner\n\nThis repository provides a command line interface (CLI) utility that replicates an Amazon Managed Workflows for Apache Airflow (MWAA) environment locally.\n\n*Please note: MWAA/AWS/DAG/Plugin issues should be raised through AWS Support or the Airflow Slack #airflow-aws channel.  Issues here should be focused on this local-runner repository.*\n\n\n## About the CLI\n\nThe CLI builds a Docker container image locally that\u2019s similar to a MWAA production image. This allows you to run a local Apache Airflow environment to develop and test DAGs, custom plugins, and dependencies before deploying to MWAA.\n\n## What this repo contains\n\n```text\ndags/\n  example_lambda.py\n  example_dag_with_taskflow_api.py    \n  example_redshift_data_execute_sql.py\ndocker/\n  config/\n    airflow.cfg\n    constraints.txt\n    mwaa-base-providers-requirements.txt\n    webserver_config.py\n    .env.localrunner\n  script/\n    bootstrap.sh\n    entrypoint.sh\n    systemlibs.sh\n    generate_key.sh\n  docker-compose-local.yml\n  docker-compose-resetdb.yml\n  docker-compose-sequential.yml\n  Dockerfile\nplugins/\n  README.md\nrequirements/  \n  requirements.txt\n.gitignore\nCODE_OF_CONDUCT.md\nCONTRIBUTING.md\nLICENSE\nmwaa-local-env\nREADME.md\nVERSION\n```\n\n## Prerequisites\n\n- **macOS**: [Install Docker Desktop](https://docs.docker.com/desktop/).\n- **Linux/Ubuntu**: [Install Docker Compose](https://docs.docker.com/compose/install/) and [Install Docker Engine](https://docs.docker.com/engine/install/).\n- **Windows**: Windows Subsystem for Linux (WSL) to run the bash based command `mwaa-local-env`. Please follow [Windows Subsystem for Linux Installation (WSL)](https://docs.docker.com/docker-for-windows/wsl/) and [Using Docker in WSL 2](https://code.visualstudio.com/blogs/2020/03/02/docker-in-wsl2), to get started.\n\n## Get started\n\n```bash\ngit clone https://github.com/aws/aws-mwaa-local-runner.git\ncd aws-mwaa-local-runner\n```\n\n### Step one: Building the Docker image\n\nBuild the Docker container image using the following command:\n\n```bash\n./mwaa-local-env build-image\n```\n\n**Note**: it takes several minutes to build the Docker image locally.\n\n### Step two: Running Apache Airflow\n\n#### Local runner\n\nRuns a local Apache Airflow environment that is a close representation of MWAA by configuration.\n\n```bash\n./mwaa-local-env start\n```\n\nTo stop the local environment, Ctrl+C on the terminal and wait till the local runner and the postgres containers are stopped.\n\n### Step three: Accessing the Airflow UI\n\nBy default, the `bootstrap.sh` script creates a username and password for your local Airflow environment.\n\n- Username: `admin`\n- Password: `test`\n\n#### Airflow UI\n\n- Open the Apache Airlfow UI: <http://localhost:8080/>.\n\n### Step four: Add DAGs and supporting files\n\nThe following section describes where to add your DAG code and supporting files. We recommend creating a directory structure similar to your MWAA environment.\n\n#### DAGs\n\n1. Add DAG code to the `dags/` folder.\n2. To run the sample code in this repository, see the `example_dag_with_taskflow_api.py` file.\n\n#### Requirements.txt\n\n1. Add Python dependencies to `requirements/requirements.txt`.  \n2. To test a requirements.txt without running Apache Airflow, use the following script:\n\n```bash\n./mwaa-local-env test-requirements\n```\n\nLet's say you add `aws-batch==0.6` to your `requirements/requirements.txt` file. You should see an output similar to:\n\n```bash\nInstalling requirements.txt\nCollecting aws-batch (from -r /usr/local/airflow/dags/requirements.txt (line 1))\n  Downloading https://files.pythonhosted.org/packages/5d/11/3aedc6e150d2df6f3d422d7107ac9eba5b50261cf57ab813bb00d8299a34/aws_batch-0.6.tar.gz\nCollecting awscli (from aws-batch->-r /usr/local/airflow/dags/requirements.txt (line 1))\n  Downloading https://files.pythonhosted.org/packages/07/4a/d054884c2ef4eb3c237e1f4007d3ece5c46e286e4258288f0116724af009/awscli-1.19.21-py2.py3-none-any.whl (3.6MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.6MB 365kB/s \n...\n...\n...\nInstalling collected packages: botocore, docutils, pyasn1, rsa, awscli, aws-batch\n  Running setup.py install for aws-batch ... done\nSuccessfully installed aws-batch-0.6 awscli-1.19.21 botocore-1.20.21 docutils-0.15.2 pyasn1-0.4.8 rsa-4.7.2\n```\n\n3. To package the necessary WHL files for your requirements.txt without running Apache Airflow, use the following script:\n\n```bash\n./mwaa-local-env package-requirements\n```\n\nFor example usage see [Installing Python dependencies using PyPi.org Requirements File Format Option two: Python wheels (.whl)](https://docs.aws.amazon.com/mwaa/latest/userguide/best-practices-dependencies.html#best-practices-dependencies-python-wheels).\n\n#### Custom plugins\n\n- There is a directory at the root of this repository called plugins. \n- In this directory, create a file for your new custom plugin.\n- Add any Python dependencies to `requirements/requirements.txt`.\n\n**Note**: this step assumes you have a DAG that corresponds to the custom plugin. For example usage [MWAA Code Examples](https://docs.aws.amazon.com/mwaa/latest/userguide/sample-code.html).\n\n#### Startup script\n\n- There is a sample shell script `startup.sh` located in a directory at the root of this repository called `startup_script`.\n- If there is a need to run additional setup (e.g. install system libraries, setting up environment variables), please modify the `startup.sh` script.\n- To test a `startup.sh` without running Apache Airflow, use the following script:\n\n```bash\n./mwaa-local-env test-startup-script\n```\n\n## What's next?\n\n- Learn how to upload the requirements.txt file to your Amazon S3 bucket in [Installing Python dependencies](https://docs.aws.amazon.com/mwaa/latest/userguide/working-dags-dependencies.html).\n- Learn how to upload the DAG code to the dags folder in your Amazon S3 bucket in [Adding or updating DAGs](https://docs.aws.amazon.com/mwaa/latest/userguide/configuring-dag-folder.html).\n- Learn more about how to upload the plugins.zip file to your Amazon S3 bucket in [Installing custom plugins](https://docs.aws.amazon.com/mwaa/latest/userguide/configuring-dag-import-plugins.html).\n\n## FAQs\n\nThe following section contains common questions and answers you may encounter when using your Docker container image.\n\n### Can I test execution role permissions using this repository?\n\n- You can setup the local Airflow's boto with the intended execution role to test your DAGs with AWS operators before uploading to your Amazon S3 bucket. To setup aws connection for Airflow locally see [Airflow | AWS Connection](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.html)\nTo learn more, see [Amazon MWAA Execution Role](https://docs.aws.amazon.com/mwaa/latest/userguide/mwaa-create-role.html).\n- You can set AWS credentials via environment variables set in the `docker/config/.env.localrunner` env file. To learn more about AWS environment variables, see [Environment variables to configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html) and [Using temporary security credentials with the AWS CLI](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html#using-temp-creds-sdk-cli). Simply set the relevant environment variables in `.env.localrunner` and `./mwaa-local-env start`.\n\n### How do I add libraries to requirements.txt and test install?\n\n- A `requirements.txt` file is included in the `/requirements` folder of your local Docker container image. We recommend adding libraries to this file, and running locally.\n\n### What if a library is not available on PyPi.org?\n\n- If a library is not available in the Python Package Index (PyPi.org), add the `--index-url` flag to the package in your `requirements/requirements.txt` file. To learn more, see [Managing Python dependencies in requirements.txt](https://docs.aws.amazon.com/mwaa/latest/userguide/best-practices-dependencies.html).\n\n## Troubleshooting\n\nThe following section contains errors you may encounter when using the Docker container image in this repository.\n\n### My environment is not starting\n\n- If you encountered [the following error](https://issues.apache.org/jira/browse/AIRFLOW-3678): `process fails with \"dag_stats_table already exists\"`, you'll need to reset your database using the following command:\n\n```bash\n./mwaa-local-env reset-db\n```\n\n- If you are moving from an older version of local-runner you may need to run the above reset-db command, or delete your `./db-data` folder. Note, too, that newer Airflow versions have newer provider packages, which may require updating your DAG code.\n\n### Fernet Key InvalidToken\n\nA Fernet Key is generated during image build (`./mwaa-local-env build-image`) and is durable throughout all\ncontainers started from that image. This key is used to [encrypt connection passwords in the Airflow DB](https://airflow.apache.org/docs/apache-airflow/stable/security/secrets/fernet.html).\nIf changes are made to the image and it is rebuilt, you may get a new key that will not match the key used when\nthe Airflow DB was initialized, in this case you will need to reset the DB (`./mwaa-local-env reset-db`).\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.\n", "release_dates": ["2021-05-26T17:23:02Z", "2021-04-27T05:48:24Z"]}, {"name": "aws-network-policy-agent", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# aws-network-policy-agent\nAmazon EKS Network Policy Agent is a daemonset that is responsible for enforcing configured network policies on the cluster. Network policy support is a feature of the [Amazon VPC CNI](https://github.com/aws/amazon-vpc-cni-k8s). \n\n[Network Policy Controller](https://github.com/aws/amazon-network-policy-controller-k8s/) resolves the configured network policies and publishes the resolved endpoints via Custom CRD (`PolicyEndpoints`) resource. Network Policy agent derives the endpoints from PolicyEndpoint resources and enforces them via eBPF probes attached to pod's host Veth interface.\n\nStarting with Amazon VPC CNI v1.14.0, Network Policy agent will be automatically installed. Review the instructions in the [EKS User Guide](https://docs.aws.amazon.com/eks/latest/userguide/cni-network-policy.html).\n\n## Getting Started\nYou\u2019ll need a Kubernetes cluster version 1.25+ to run against. You can use [KIND](https://sigs.k8s.io/kind) to get a local cluster for testing, or run against a remote cluster.\n\n**Note:** Your controller will automatically use the current context in your kubeconfig file (i.e. whatever cluster `kubectl cluster-info` shows).\n\n## Prerequisites \n - You need to install [Network Policy Controller](https://github.com/aws/amazon-network-policy-controller-k8s/) in your cluster before you can enable the feature in VPC CNI. When you create a new Amazon EKS cluster, the controller will be automatically installed in EKS control plane.\n - Network Policy Agent expects the BPF FS (`/sys/fs/bpf`) to be mounted. If you rely on EKS AMIs, all v1.27+ EKS AMIs will mount BPF FS by default. For v1.25 and v1.26 clusters, EKS AMIs above version https://github.com/awslabs/amazon-eks-ami/releases/tag/v20230703 will mount the BPF FS by default.\n - PolicyEndpoint CRD needs to be installed in the cluster. Installing Network Policy Controller will automatically install the CRD.\n\n## Setup\nDownload the latest version of the [yaml](https://github.com/aws/amazon-vpc-cni-k8s/tree/release-1.14/config) and apply it to the cluster.\n\nPlease refer to [EKS User Guide](https://docs.aws.amazon.com/eks/latest/userguide/cni-network-policy.html) on how to enable the feature.\n\n### Network Policy Agent Configuration flags\n---\n\n#### `enable-network-policy`\n\nType: Boolean\n\nDefault: false\n\nSet this flag to `true` to enable the Network Policy feature support.\n\n#### `enable-policy-event-logs`\n\nType: Boolean\n\nDefault: false\n\nSet this flag to `true` to enable the collection & logging of policy decision logs.\n\n> Notice: Enabling this feature requires one CPU core per node.\n\n#### `enable-cloudwatch-logs`\n\nType: Boolean\n\nDefault: false\n\nNetwork Policy Agent provides an option to stream policy decision logs to Cloudwatch. For EKS clusters, the policy logs will be located under `/aws/eks/<cluster-name>/cluster/` and for self-managed K8S clusters, the logs will be placed under `/aws/k8s-cluster/cluster/`. By default, Network Policy Agent will log policy decision information for individual flows to a file on the local node (`/var/run/aws-routed-eni/network-policy-agent.log`).\n\nThis feature requires to also enable the `enable-policy-event-logs` flag.\n\nThis feature requires you to provide relevant Cloudwatch permissions to `aws-node` pod via the below policy.\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:DescribeLogGroups\",\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n```\n\n#### `enable-ipv6`\n\nType: Boolean\n\nDefault: false\n\nNetwork Policy agent can operate in either IPv4 or IPv6 mode. Setting this flag to `true` in the manifest will configure it in IPv6 mode.\n\n**Note:** VPC CNI by default creates an egress only IPv4 interface for IPv6 pods and this network interface will not be secured by the Network policy feature. Network policies will only be enforced on the Pod's primary interface (i.e.,) `eth0`. If you want to block the egress IPv4 access, please disable the interface creation via [ENABLE_V4_EGRESS](https://github.com/aws/amazon-vpc-cni-k8s#enable_v4_egress-v1151) flag in VPC CNI. \n\n## Network Policy Agent CLI\nThe Amazon VPC CNI plugin for Kubernetes installs eBPF SDK collection of tools on the nodes. You can use the eBPF SDK tools to identify issues with network policies. For example, the following command lists the programs that are running on the node.\n\n**Note:**: To run this CLI, you can use any method to connect to the node. CLI binary is located at `/opt/cni/bin`.\n\n**Usage**:\n\n```\n./aws-eks-na-cli ebpf -h\nDump all ebpf related data\n\nUsage:\n  aws-eks-na-cli ebpf [flags]\n  aws-eks-na-cli ebpf [command]\n\nAliases:\n  ebpf, ebpf\n\nAvailable Commands:\n  dump-maps       Dump all ebpf maps related data\n  loaded-ebpfdata Dump all ebpf related data\n  maps            Dump all ebpf maps related data\n  progs           Dump all ebpf program related data\n```\n\n- Load all eBPF programs managed by Network Policy Agent\n\n```\n   ./aws-eks-na-cli ebpf progs\n\nExample:\n\n./aws-eks-na-cli ebpf progs\nPrograms currently loaded : \nType : 26 ID : 6 Associated maps count : 1\n========================================================================================\nType : 26 ID : 8 Associated maps count : 1\n========================================================================================\nType : 3 ID : 57 Associated maps count : 3\n========================================================================================\n```\n\n- Load all eBPF maps managed by Network Policy Agent\n  \n```\n   ./aws-eks-na-cli ebpf maps\n\nExample:\n\n./aws-eks-na-cli ebpf maps\nMaps currently loaded : \nType : 2 ID : 45\nKeysize 4 Valuesize 98 MaxEntries 1\n========================================================================================\nType : 9 ID : 201\nKeysize 16 Valuesize 1 MaxEntries 65536\n========================================================================================\n```\n\n- Print Map contents by ID\n  \n```\n   ./aws-eks-na-cli ebpf dump-maps <Map-ID>\n  \nExample:\n\n./aws-eks-na-cli ebpf dump-maps 40\nKey : IP/Prefixlen - 192.168.61.236/32 \nValue : \nProtocol -  254\nStartPort -  0\nEndport -  0\n*******************************\nKey : IP/Prefixlen - 0.0.0.0/0 \nValue : \nProtocol -  254\nStartPort -  0\nEndport -  0\n*******************************\n```\n\n- Load all eBPF related programs and maps managed by Network Policy Agent\n  \n```\n   ./aws-eks-na-cli ebpf loaded-ebpfdata\n\nExample:\n./aws-eks-na-cli ebpf loaded-ebpfdata\npinPathName: busybox-deployment-77948c5466-default_handle_egress\nPinPath:  /sys/fs/bpf/globals/aws/programs/busybox-deployment-77948c5466-default_handle_egress\nPod Identifier : busybox-deployment-77948c5466-default  Direction : egress \nProg FD:  9\nAssociated Maps -> \nMap Name:  \nMap ID:  224\nMap Name:  egress_map\nMap ID:  225\n========================================================================================\npinPathName:  busybox-deployment-77948c5466-default_handle_ingress\nPinPath:  /sys/fs/bpf/globals/aws/programs/busybox-deployment-77948c5466-default_handle_ingress\nPod Identifier : busybox-deployment-77948c5466-default  Direction : ingress \nProg FD:  13\nAssociated Maps -> \nMap Name:  \nMap ID:  224\nMap Name:  ingress_map\nMap ID:  226\n========================================================================================\n```\n\n## Contributing\n\nSee [CONTRIBUTING](CONTRIBUTING.md) for more information.\n\n### How it works\nThis project aims to follow the Kubernetes [Operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/).\n\nIt uses [Controllers](https://kubernetes.io/docs/concepts/architecture/controller/),\nwhich provide a reconcile function responsible for synchronizing resources until the desired state is reached on the cluster.\n\n### Modifying the API definitions\nIf you are editing the API definitions, generate the manifests such as CRs or CRDs using:\n\n```sh\nmake manifests\n```\n\n**NOTE:** Run `make --help` for more information on all potential `make` targets\n\nMore information can be found via the [Kubebuilder Documentation](https://book.kubebuilder.io/introduction.html)\n\n## License\n\nCopyright 2023.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n## Security Disclosures \n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues. Instead, please follow the\ninstructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).", "release_dates": ["2024-02-20T18:06:02Z", "2024-01-04T18:33:06Z", "2023-11-20T18:22:15Z", "2023-11-03T17:55:28Z", "2023-10-13T17:52:02Z", "2023-09-08T17:47:00Z", "2023-08-29T17:16:39Z", "2023-08-09T19:34:31Z"]}, {"name": "aws-nitro-enclaves-acm", "description": "AWS Certificate Manager for Nitro Enclaves allows the use of public and private SSL/TLS certificates with web applications and web servers running on Amazon EC2 instances with AWS Nitro Enclaves.", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Certificate Manager for Nitro Enclaves\n\nThis is a PKCS#11 provider intended to be executed within the confines of a\nNitro Enclave.\n\nDevelopment is aided by Docker containers that can be used to build and test\nrun the PKCS#11 provider as a `p11-kit` module. These containers are designed to\nbe mostly transparent to the developer, and employed via the omnitool at\n`tools/devtool`.\n\n## How to install and setup\n\nThe user guide for the ACM for Nitro Enclaves can be found at https://docs.aws.amazon.com/enclaves/latest/user/nitro-enclave-refapp.html.\n\n# Managed tokens\n\nEach token can store an end-entity private key and its associated ACM certificate chain. Up to 128 SSL/TLS X.509 ACM certificates can be managed via provisioned tokens by the nitro-enclaves-acm service.\nConfiguration options can be found in the `/etc/nitro_enclaves/acm.yaml` post service installation.\n\n## Design Overview\n\nACM for Nitro Enclaves is a PKCS#11 provider (i.e. a dynamic library exposing the\nPKCS#11 API). The `p11-kit` client and server are used to transport crypto\noperation calls from the parent instance to the enclave, where they are handled\nby this provider via the AWS cryptographic library.\n\nHere is the general flow of a parent instance crypto operation:\n\n```\n        [parent instance]                 |            [enclave]\n                                          |\n    OpenSSL client (e.g. nginx)           |\n                |                         |\n                v                         |\n              OpenSSL                     |\n                |                         |\n                v                         |\n        OpenSSL PKCS#11 Engine            |\n                |                         |\n                v                         |\n          p11-kit client ------- vsock channel ---> p11-kit server\n                                          |              |\n                                          |              v\n                                          |    ACM for Nitro Enclaves module\n                                          |              |\n                                          |              v\n                                          |\t        AWS libcrypto\n```\n\n## Dependencies\n\n| name                       | version              | link                                              |\n|----------------------------|----------------------|---------------------------------------------------|\n| aws-lc                     | v1.12.0              | https://github.com/awslabs/aws-lc/                |\n| aws-nitro-enclaves-sdk     | v0.4.1               | https://github.com/aws/aws-nitro-enclaves-sdk-c   |\n| s2n-tls                    | v1.3.46              | https://github.com/aws/s2n-tls.git                |\n| aws-c-common               | v0.8.0               | https://github.com/awslabs/aws-c-common           |\n| aws-c-io                   | v0.11.0              | https://github.com/awslabs/aws-c-io               |\n| aws-c-compression          | v0.2.14              | https://github.com/awslabs/aws-c-compression      |\n| aws-c-http                 | v0.6.19              | https://github.com/awslabs/aws-c-http             |\n| aws-c-cal                  | v0.5.18              | https://github.com/awslabs/aws-c-cal              |\n| aws-c-auth                 | v0.6.15              | https://github.com/awslabs/aws-c-auth             |\n| aws-c-sdkutils             | v0.1.2               | https://github.com/awslabs/aws-c-sdkutils         |\n| aws-nitro-enclaves-nsm-api | v0.4.0               | https://github.com/aws/aws-nitro-enclaves-nsm-api |\n| json-c                     | json-c-0.16-20220414 | https://github.com/json-c/json-c                  |\n\n`devtool` sets up two containers: one for emulating the enclave environment,\nand another for emulating the parent instance environment.\n\nIf using Docker is not an option, have a look at the Dockerfile for a full list\nof packages needed to build and run the ACM for Nitro Enclaves module. Additionally,\nthe `devtool` source (it's just a BASH script) may provide useful details on what\nenvironment setup is required prior to building and/or running.\n\n## Components\n\nACM for Nitro Enclaves has a few different components, some meant to be run inside the enclave,\nothers inside the parent instance:\n- enclave-side components:\n  - `p11ne-srv` - the AWS for NE RPC server, used to query the state of the pkcs#11 enclave\n                  device, and to provision its database;\n  - `libvtok_p11.so` - the PKCS#11 provider implementation;\n- parent-instance-side components:\n  - `p11ne-client` - the ACM for NE RPC client, providing a low-level interface to\n                     the ACM for NE RPC server;\n  - `p11ne-cli` - a user-facing CLI tool that can be used to manage the\n                  ACM for NE enclave (e.g. provision a PKCS#11 token);\n  - `p11ne-db`- a user-facing CLI tool that can be used to pack a private key and\n                its associated certificate (or certificate chain) in a database format\n\t\t\t\tfor provisioning a PKCS#11 token\n\n## Building\n\nUse `devtool` to build any ACM for NE component, by invoking `devtool build <component>`.\n\nE.g. building the PKCS#11 provider:\n\n```bash\ntools/devtool build libvtok_p11.so\n```\n\nBuilding the (development version of) ACM for NE enclave image (EIF):\n\n```bash\ntools/devtool build dev-image\n```\n\nSee `devtool help` for more build options.\n\n## Testing in the development environment\n\n`devtool` uses development containers to simulate both the enclave and\nparent instance environments. The communication channel between `p11-kit\nclient` and `p11-kit server` is emulated via a Unix socket, bind-mounted into\nboth container environments (parent and enclave).\n\n**Note**: The emulated enclave environment differs substantially from the\nproduction enclave, and it is only to be used for testing the PKCS#11 API\nfunctionality of the ACM for Nitro Enclaves module. Most notably, attestation and token\nprovisioning are both missing from the emulated environment.\n\nFirst, the enclave container needs to be running:\n\n```bash\ntools/devtool simulate-enclave\n```\n\nThis will start `p11-kit server` with the ACM for Nitro Enclaves module loaded (the\nmodule is first built if unavailable). The server is run in foreground mode, so\nthe pkcs#11 provider module log will show up at `stderr`.\n\nWith the enclave environment up and running, the parent environment can be\nstarted:\n\n```bash\ntools/devtool simulate-parent\n```\n\nThis will spin up a container with p11-kit configured to access the remote\nmodule exposed by the enclave container via a Unix socket.\n`devtool simulate-parent` starts a BASH shell, so the user can manually test /\ninspect the functionality of the ACM for Nitro Enclaves module; for instance, via\nrunning `openssl` manually, directed to use the PKCS#11 engine and a URI\npointing to the pkcs#11 provider module token:\n\n```bash\nopenssl pkeyutl -keyform engine -engine pkcs11 -sign -inkey \\\n\t\"pkcs11:model=p11ne-token;manufacturer=Amazon;serial=EVT00;token=my-token-label;id=%52;type=private\" \\\n\t-in hello.txt -out test.sig\n```\n\nThe `tests` directory contains integration tests that can be executed to\nvalidate the PKCS#11 module functionality using openssl or OpenSC pkcs11-tool.\n\nBuild the testhelper binary:\n```bash\n$ cd tests/helpers && cargo build --release\n\n$ cd - && cp build/target/release/testhelpers ./tests\n```\nAfter this, the test suite can be executed via the command:\n```bash\n$ ./tests/testtool openssl --kms-key-id <your-kms-key-id> --kms-region <your-kms-key-region>\n\n```\nThe above cryptographic test suite is applicable when using real enclaves on EC2 instances\nwhere an instance role and a KMS key has already been setup accordingly for provisioning the\ntest pkcs#11 token with the private keys.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n## Security issue notifications\n\nIf you discover a potential security issue in ACM for Nitro Enclaves, we ask that you notify AWS\nSecurity via our\n[vulnerability reporting page](https://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public GitHub issue.\n", "release_dates": ["2023-09-01T14:43:35Z", "2022-09-20T10:53:20Z"]}, {"name": "aws-nitro-enclaves-cli", "description": "Tooling for Nitro Enclave Management", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![msrv]\n\n[msrv]: https://img.shields.io/badge/MSRV-1.60.0-blue\n\n## Nitro Enclaves Command Line Interface (Nitro CLI)\n\nThis repository contains a collection of tools and commands used for managing the lifecycle of enclaves. The Nitro CLI needs to be installed on the parent instance, and it can be used to start, manage, and terminate enclaves.  \n\n### Prerequisites\n  1. A working docker setup, follow https://docs.docker.com/install/overview/ for details of how to install docker on your host, including how to run it as non-root.\n  2. Install gcc, make, git, llvm-dev, libclang-dev, clang.\n\n### Driver information\n  The Nitro Enclaves kernel driver is available in the upstream Linux kernel starting with the v5.10 kernel for x86_64 and starting with the v5.16 kernel for arm64. The codebase from the 'drivers/virt/nitro_enclaves' directory in this GitHub repository is similar to the one merged into the upstream Linux kernel.\n\n  The Nitro Enclaves kernel driver is currently available in the following distro kernels:\n\n  - x86_64\n      - Amazon Linux 2 v4.14 kernel starting with kernel-4.14.198-152.320.amzn2.x86_64\n      - Amazon Linux 2 v5.4 kernel starting with kernel-5.4.68-34.125.amzn2.x86_64\n      - Amazon Linux 2 v5.10+ kernels (e.g. kernel-5.10.29-27.128.amzn2.x86_64)\n      - Amazon Linux 2022 v5.10+ kernels (e.g. kernel-5.10.75-82.359.amzn2022.x86_64)\n      - CentOS Stream v4.18+ kernels starting with kernel-4.18.0-257.el8.x86_64\n      - Fedora v5.10+ kernels (e.g. kernel-5.10.12-200.fc33.x86_64)\n      - openSUSE Tumbleweed v5.10+ kernels (e.g. kernel-default-5.10.1-1.1.x86_64)\n      - Red Hat Enterprise Linux v4.18+ kernels starting with kernel-4.18.0-305.el8.x86_64\n      - SUSE Linux Enterprise Server v5.14+ kernels starting with kernel-default-5.14.21-150400.22.1.x86_64\n      - Ubuntu v5.4 kernel starting with linux-aws 5.4.0-1030-aws x86_64\n      - Ubuntu v5.8 kernel starting with linux-aws 5.8.0-1017-aws x86_64\n      - Ubuntu v5.11+ kernels (e.g. linux-aws 5.11.0-1006-aws x86_64)\n\n  - aarch64\n      - Amazon Linux 2 v4.14 kernel starting with kernel-4.14.252-195.483.amzn2.aarch64\n      - Amazon Linux 2 v5.4 kernel starting with kernel-5.4.156-83.273.amzn2.aarch64\n      - Amazon Linux 2 v5.10+ kernels starting with kernel-5.10.75-79.358.amzn2.aarch64\n      - Amazon Linux 2022 v5.10+ kernels starting with kernel-5.10.75-82.359.amzn2022.aarch64\n      - CentOS Stream v4.18 kernel starting with kernel-4.18.0-358.el8.aarch64\n      - CentOS Stream v5.14+ kernels starting with kernel-5.14.0-24.el9.aarch64\n      - Fedora v5.16+ kernels (e.g. kernel-5.16.5-200.fc35.aarch64)\n      - Red Hat Enterprise Linux v4.18+ kernels starting with kernel-4.18.0-372.9.1.el8.aarch64\n      - Ubuntu v5.4 kernel starting with linux-aws 5.4.0-1064-aws aarch64\n      - Ubuntu v5.13+ kernels starting with linux-aws 5.13.0-1012-aws aarch64\n\n  The following packages need to be installed or updated to have the Nitro Enclaves kernel driver available in the mentioned distros:\n\n  - Amazon Linux 2 - \"kernel\" (amzn2-core) for the v4.14 kernel, \"kernel\" (amzn2extra-kernel-5.4) for the v5.4 kernel, \"kernel\" (amzn2extra-kernel-5.10) for the v5.10 kernel\n  - Amazon Linux 2022 - \"kernel\" for the v5.10+ kernels\n  - CentOS Stream - \"kernel\" for the v4.18+ kernels\n  - Fedora - \"kernel\" for the v5.10+ kernels\n  - openSUSE Tumbleweed - \"kernel-default\" for the v5.10+ kernels\n  - Red Hat Enterprise Linux - \"kernel\" for the v4.18+ kernels\n  - SUSE Linux Enterprise Server - \"kernel-default\" for the v5.14+ kernels\n  - Ubuntu - \"linux-aws\" and \"linux-modules-extra-aws\" for the v5.4, v5.8 and v5.11+ kernels\n\n  Out-of-tree driver build can be done using the Makefile in the 'drivers/virt/nitro_enclaves' directory.\n\n### How to install (GitHub sources):\n  1. Clone the repository.\n  2. Set NITRO_CLI_INSTALL_DIR to the desired location, by default everything will be installed in build/install\n  3. Run 'make nitro-cli && make vsock-proxy && make install'.\n  4. [Rerun after reboot] Source the script ${NITRO_CLI_INSTALL_DIR}/etc/profile.d/nitro-cli-env.sh.\n  5. [Rerun after reboot] Preallocate resources for the enclaves(s). \n     For example, to configure 2 vCPUs and 256 Mib for enclave use:\n     `nitro-cli-config -i -m 256 -t 2`\n  6. [Optional] You could add ${NITRO_CLI_INSTALL_DIR}/etc/profile.d/nitro-cli-env.sh in your local shell configuration.\n  7. You are now ready to go.\n\n  A set of steps options to install on distros the Nitro CLI from GitHub sources can be found in the [docs](docs) directory:\n  - [CentOS Stream 8](docs/centos_stream_8_how_to_install_nitro_cli_from_github_sources.md)\n  - [Fedora 34](docs/fedora_34_how_to_install_nitro_cli_from_github_sources.md)\n  - [RHEL 8.4](docs/rhel_8.4_how_to_install_nitro_cli_from_github_sources.md)\n  - [Ubuntu 20.04](docs/ubuntu_20.04_how_to_install_nitro_cli_from_github_sources.md)\n\n### How to use Nitro Enclaves CLI\n  The user guide for the Nitro Enclaves CLI can be found at https://docs.aws.amazon.com/enclaves/latest/user/nitro-enclave-cli.html.\n\n  Ensure that your EC2 instance was created with enclave support enabled and that your system (*and container if applicable*) has read/write access to `/dev/nitro_enclaves`.\n\n  Ensure that your Linux system (*and container if applicable*) has Linux hugepages available.\n\n  The AWS Nitro Enclaves CLI package is currently available for:\n  - Amazon Linux 2 - https://docs.aws.amazon.com/enclaves/latest/user/nitro-enclave-cli-install.html\n  - openSUSE and SUSE Linux Enterprise Server - https://build.opensuse.org/package/show/Cloud:Tools/aws-nitro-enclaves-cli\n  - Windows - https://docs.aws.amazon.com/enclaves/latest/user/nitro-enclave-cli-install-win.html\n\n#### Enclave disk size\n  The enclaves do not have access to a physical disk, just a RAM filesystem.\n  One can configure the disk space by changing memory size or by using kernel command line arguments.\n\n  The [`init.c`](https://github.com/aws/aws-nitro-enclaves-sdk-bootstrap/tree/main/init/init.c) file keeps the default configuration for each volume. The below example shows\n  the default options for `/tmp`.\n  ```\n  { OpMount, .mount = { \"tmpfs\", \"/tmp\", \"tmpfs\", MS_NODEV | MS_NOSUID | MS_NOEXEC } },\n  ```\n  To modify the memory allocated to this volume, another parameter is needed\n  ```\n  { OpMount, .mount = { \"tmpfs\", \"/tmp\", \"tmpfs\", MS_NODEV | MS_NOSUID | MS_NOEXEC, \"size=100%\" } },\n  ```\n  Note that the parameter `size` specifies only the maximum allocated size.\n  After modifying the configuration, the file needs to be recompiled using `make init` and moved to\n  `/usr/share/nitro_enclaves/blobs/init`.\n\n## License\n  This library is licensed under the Apache 2.0 License.\n\n## Source-code components\n  The components of the Nitro Enclaves CLI are organized as follows (all paths are relative to the Nitro Enclaves CLI's root directory):\n\n  - 'blobs': Binary blobs providing pre-compiled components needed for the building of enclave images:\n      - 'blobs/aarch64/Image': Kernel image\n      - 'blobs/aarch64/Image.config': Kernel config\n      - 'blobs/aarch64/cmdline': Kernel boot command line\n      - 'blobs/aarch64/init': Init process executable\n      - 'blobs/aarch64/linuxkit': LinuxKit-based user-space environment\n      - 'blobs/aarch64/nsm.ko': The driver which enables the Nitro Secure Module (NSM) component inside the enclave\n      - 'blobs/x86_64/bzImage': Kernel image\n      - 'blobs/x86_64/bzImage.config': Kernel config\n      - 'blobs/x86_64/cmdline': Kernel boot command line\n      - 'blobs/x86_64/init': Init process executable\n      - 'blobs/x86_64/linuxkit': LinuxKit-based user-space environment\n      - 'blobs/x86_64/nsm.ko': The driver which enables the Nitro Secure Module (NSM) component inside the enclave\n      - The enclave kernel is based on the v4.14 Amazon Linux kernel - https://github.com/amazonlinux/linux/tree/amazon-4.14.y/master\n      - The source code for the init process and the NSM kernel driver can be found in the following GitHub repository - https://github.com/aws/aws-nitro-enclaves-sdk-bootstrap\n\n  - 'build': An automatically-generated directory which stores the build output for various components (the CLI, the command executer etc.)\n\n  - 'bootstrap': Various useful scripts for CLI environment configuration, namely:\n      - 'allocator.yaml': Configuration file for enclave memory and CPUs reservation\n      - 'env.sh': A script which inserts the pre-built Nitro Enclaves kernel module, adds the CLI binary directory to $PATH and sets the blobs directory\n      - 'nitro-cli-config': A script which can build, configure and install the Nitro Enclaves kernel module, as well as configure the memory and CPUs available for enclave launches (depending on the operation, root privileges may be required)\n      - 'nitro-enclaves-allocator': Configuration script for enclave memory and CPUs reservation\n      - 'nitro-enclaves-allocator.service': Configuration service for enclave memory and CPUs reservation\n\n  - 'docs': Useful documentation\n\n  - 'drivers': The source code of the kernel modules used by the CLI in order to control enclave behavior, containing:\n      - 'drivers/virt/nitro_enclaves': The Nitro Enclaves driver used by the Nitro CLI\n\n  - 'eif_loader': The source code for the EIF loader, a module which ensures that an enclave has booted successfully\n\n  - 'enclave_build': A tool which builds EIF files starting from a Docker image and pre-existing binary blobs (such as those from 'blobs')\n\n  - 'examples': Basic examples of enclaves. One example is the hello world enclave.\n\n  - 'include': The header files exposed by the Nitro Enclaves kernel module used by the Nitro CLI\n\n  - 'samples': A collection of CLI-related sample applications. One sample is the command executer - an application that enables a parent instance to issue commands to an enclave (such as transferring a file, executing an application on the enclave etc.)\n\n  - 'src': The Nitro CLI implementation, divided into 3 components:\n      - The implementation of the background enclave process: 'src/enclave_proc'\n      - The implementation of the CLI, which takes user commands and communicates with enclave processes: 'src/*.rs'\n      - A common module used by both the CLI and the enclave process: 'src/common'\n\n  - 'tests': Various unit and integration tests for the CLI\n\n  - 'tools': Various useful configuration files used for CLI and EIF builds\n\n  - 'vsock_proxy': The implementation of the Vsock - TCP proxy application, which is used to allow an enclave to communicate with an external service through the parent instance\n\n  - 'ci_entrypoint.sh': The script which launches the CLI continuous integration tests\n\n  - 'scripts/run_tests.sh': The continuous integration test suite for the CLI across all supported platforms\n\n## Security issue notifications\n\nIf you discover a potential security issue in nitro-cli, we ask that you notify AWS\nSecurity via our\n[vulnerability reporting page](https://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public GitHub issue.\n", "release_dates": ["2024-01-31T16:42:22Z", "2023-03-07T10:50:45Z", "2022-10-28T09:33:04Z", "2022-03-08T17:05:03Z", "2021-11-19T09:27:56Z", "2021-08-27T09:51:22Z", "2021-07-07T05:41:44Z", "2021-04-29T08:20:43Z", "2020-04-13T11:50:41Z"]}, {"name": "aws-nitro-enclaves-image-format", "description": "This library provides the definition of the enclave image format (EIF) file used in AWS Nitro Enclaves.", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## aws-nitro-enclaves-image-format\n\n[![status]][actions] [![version]][crates.io] [![docs]][docs.rs] ![msrv]\n\n[status]: https://img.shields.io/github/workflow/status/aws/aws-nitro-enclaves-image-format/Rust/main\n[actions]: https://github.com/aws/aws-nitro-enclaves-image-format/actions?query=branch%3Amain\n[version]: https://img.shields.io/crates/v/aws-nitro-enclaves-image-format.svg\n[crates.io]: https://crates.io/crates/aws-nitro-enclaves-image-format\n[docs]: https://img.shields.io/docsrs/aws-nitro-enclaves-image-format\n[docs.rs]: https://docs.rs/aws-nitro-enclaves-image-format\n[msrv]: https://img.shields.io/badge/MSRV-1.58.1-blue\n\nThis library provides the definition of the enclave image format (EIF) file.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "aws-nitro-enclaves-k8s-device-plugin", "description": "Nitro Enclaves Kubernetes Device Plugin", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Introduction\n\nThe Nitro Enclaves [Device Plugin](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) gives your pods and containers the ability to access the [Nitro Enclaves device driver](https://docs.kernel.org/virt/ne_overview.html). The device plugin works with both [Amazon EKS](https://aws.amazon.com/eks/) and self-managed Kubernetes nodes.\n\n[AWS Nitro Enclaves](https://aws.amazon.com/ec2/nitro/nitro-enclaves/) is an [Amazon EC2](https://aws-content-sandbox.aka.amazon.com/ec2/) capability that enables customers to create isolated compute environments to further protect and securely process highly sensitive data within their EC2 instances.\n\n# Prerequisites\nTo utilize this device plugin, you will need:\n\n  - A configured Kubernetes cluster.\n  - At least one enclave-enabled node available in the cluster. An enclave-enabled node is an EC2 instance with the **EnclaveOptions** parameter set to **true**. For more information on creating an enclaving an enclave-enabled node, review the using [Nitro Enclaves with EKS user guide](https://docs.aws.amazon.com/enclaves/latest/user/kubernetes.html).\n\nTo build the plugin, you will need:\n  - Docker\n\n# Usage\nTo deploy the device plugin to your Kubernetes cluster, use the following command:\n```\nkubectl -f apply https://raw.githubusercontent.com/aws/aws-nitro-enclaves-k8s-device-plugin/main/aws-nitro-enclaves-k8s-ds.yaml\n```\n\nAfter deploying the device plugin, use labelling to enable the device plugin on a particular node:\n```\nkubectl label node <node-name> aws-nitro-enclaves-k8s-dp=enabled\n```\n\nTo see list of the nodes that have plugin enabled, use the following command:\n```\nkubectl get nodes --show-labels | grep aws-nitro-enclaves-k8s-dp=enabled\n```\n\nTo disable the plugin on a particular node, use the following command:\n```\nkubectl label node <node-name> aws-nitro-enclaves-k8s-dp-\n```\n\n# Building the Device Plugin\nTo build the device plugin from its sources, use the following command:\n\n```\n./scripts/build.sh\n````\n\nAfter successfully running the script, the device plugin will be built as a Docker image with the name `aws-nitro-enclaves-k8s-device-plugin`.\n\n# Running Nitro Enclaves in a Kubernetes Cluster\n\nThere is a guide available on how to run Nitro Enclaves in EKS clusters. See this [link](https://github.com/aws/aws-nitro-enclaves-with-k8s) to learn more.\n\n# License\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2022-11-28T18:07:55Z"]}, {"name": "aws-nitro-enclaves-nsm-api", "description": "This provides a library for interacting with the Nitro Secure Module, which provides Nitro Enclaves with attestation capability. ", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Nitro Secure Module library\n\n[![version]][crates.io]\n[![docs]][docs.rs]\n![msrv]\n\n[version]: https://img.shields.io/crates/v/aws-nitro-enclaves-nsm-api.svg\n[crates.io]: https://crates.io/crates/aws-nitro-enclaves-nsm-api\n[docs]: https://img.shields.io/docsrs/aws-nitro-enclaves-nsm-api\n[docs.rs]: https://docs.rs/aws-nitro-enclaves-nsm-api\n[msrv]: https://img.shields.io/badge/MSRV-1.63.0-blue\n\nThis is a collection of helpers which Nitro Enclaves userland\napplications can use to communicate with a connected NitroSecureModule (NSM) device.\n\nVarious operations can be requested such as:\n- PCR query and manipulation\n- Attestation\n- Entropy\n\n## Prerequisites\nAn up-to-date RUST toolchain (v1.63.0 or later)\n\n## How To Build\n1. Clone the repository\n2. Execute `make nsm-api-stable`\n\n## How to Test\n\n# Prerequisites\nTo run the tests it's required to build the command-executor tool, as follows:\n```\nmake command-executor\n```\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n## Security issue notifications\n\nIf you discover a potential security issue in the Nitro Enclaves NSM API, we ask that you notify AWS\nSecurity via our\n[vulnerability reporting page](https://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public GitHub issue.\n", "release_dates": ["2023-07-26T10:28:08Z", "2022-04-18T11:38:41Z", "2022-03-30T09:47:33Z"]}, {"name": "aws-nitro-enclaves-samples", "description": "Provides samples that can help developers get started with Nitro Enclaves.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Nitro Enclaves Samples\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "aws-nitro-enclaves-sdk-bootstrap", "description": "This project builds the kernel, nsm driver and bootstrap process for AWS Nitro Enclaves.", "language": "C", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Bootstrap for AWS Nitro Enclaves\n\nThis project builds the kernel, nsm driver and bootstrap process for AWS Nitro Enclaves.\n\nThe referenced kernel in the 'linux-url' file is an example that can be used for building an enclave image. The kernel corresponding to the enclave image blobs in https://github.com/aws/aws-nitro-enclaves-cli/tree/main/blobs is based on the v4.14 Amazon Linux kernel - https://github.com/amazonlinux/linux/tree/amazon-4.14.y/master; it is different than the kernel mentioned in 'linux-url'.\n\n### Prerequisites\n\nThe kernel download step requires setting up gpg2 with the kernel developer\nkeys. Instructions are available [here](https://www.kernel.org/category/signatures.html).\n\nFor Debian / Ubuntu systems, install build prequisites:\n```\nsudo apt-get install build-essential libncurses-dev bison flex libssl-dev libelf-dev gnupg2\n```\n\nFor Amazon Linux 2 / Fedora / RHEL / CentOS install build prequisites:\n```\nsudo yum group install \"Development Tools\" \n```\n\n## Build\n\nThe project can be built inside a Docker container to avoid installing toolchains and other packages\non your local device.\n\nFor example, to build for aarch64 run:\n\n```\ndocker build --build-arg BUILD_ARCH=aarch64 .\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "aws-nitro-enclaves-sdk-c", "description": "This repo provides a C API for AWS Nitro Enclaves, including a KMS SDK that integrates it with attestation.", "language": "C", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Nitro Enclaves SDK for C\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n## Dependencies\n| name                       | version              | link                                              |\n|----------------------------|----------------------|---------------------------------------------------|\n| aws-lc                     | v1.12.0              | https://github.com/awslabs/aws-lc/                |\n| s2n-tls                    | v1.3.46              | https://github.com/aws/s2n-tls.git                |\n| aws-c-common               | v0.8.0               | https://github.com/awslabs/aws-c-common           |\n| aws-c-sdkutils             | v0.1.2               | https://github.com/awslabs/aws-c-sdkutils         |\n| aws-c-io                   | v0.11.0              | https://github.com/awslabs/aws-c-io               |\n| aws-c-compression          | v0.2.14              | https://github.com/awslabs/aws-c-compression      |\n| aws-c-http                 | v0.7.6               | https://github.com/awslabs/aws-c-http             |\n| aws-c-cal                  | v0.5.18              | https://github.com/awslabs/aws-c-cal              |\n| aws-c-auth                 | v6.15.0              | https://github.com/awslabs/aws-c-auth             |\n| aws-nitro-enclaves-nsm-api | v0.4.0               | https://github.com/aws/aws-nitro-enclaves-nsm-api |\n| json-c                     | json-c-0.16-20220414 | https://github.com/json-c/json-c                  |\n\n## Building\n\n### Linux - Using containers:\nThe simplest way to use this SDK is by using one of the available containers as a base:\n```\ndocker build -f containers/Dockerfile.al2 --target builder -t aws-nitro-enclaves-sdk-c .\n```\n\n### Windows\nNote that this SDK is currently not supported on Windows.  Only the client side sample application (kmstool_instance) is supported on Windows.\n\n## Samples\n * [kmstool](docs/kmstool.md)\n * [kmstool-enclave-cli](docs/kmstool.md#kmstool-enclave-cli)\n\n## Security issue notifications\n\nIf you discover a potential security issue in the Nitro Enclaves SDK for C, we ask that you notify AWS\nSecurity via our\n[vulnerability reporting page](https://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public GitHub issue.\n", "release_dates": ["2023-08-07T15:23:06Z", "2023-07-26T11:43:05Z", "2023-05-02T13:53:08Z", "2022-11-03T17:36:20Z", "2022-04-20T11:30:41Z"]}, {"name": "aws-nitro-enclaves-with-k8s", "description": "Tools and guides for using AWS Nitro Enclaves with Amazon EKS.", "language": "Shell", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Nitro Enclaves with Kubernetes\n\nThis repository contains a collection of tools that can be used to build and run [AWS Nitro Enclaves](https://docs.aws.amazon.com/enclaves/latest/user/nitro-enclave.html) applications with [Amazon Elastic Kubernetes Service (EKS)](https://aws.amazon.com/eks/).\n\nThe userguide for AWS Nitro Enclaves with Kubernetes (K8s) can be found [here](https://docs.aws.amazon.com/enclaves/latest/user/kubernetes.html).\n\n# Overview\n\nThere are two NE (Nitro Enclaves) applications in this repository which can be built and deployed in a **Kubernetes** [deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/).\n\n- [hello](https://github.com/aws/aws-nitro-enclaves-cli/tree/main/examples/x86_64/hello): A simple application that prints periodically on the Nitro Enclave debug console\n- [kmstool](https://github.com/aws/aws-nitro-enclaves-sdk-c/blob/main/docs/kmstool.md): A kms application that is able to connect to KMS from inside the Nitro Enclave and decrypt an encrypted KMS message received from the outside world\n\nDependencies like `docker`, `jq`, `eksctl` and `kubectl` are required for configuring, building and deploying various NE applications.\n\nUse the `enclavectl` tool from this repository to create EKS clusters, build NE applications and do deployments.\n\nTo add the tool to your **$PATH** variable, use:\n\n```bash\nsource env.sh\n```\n\nSee `enclavectl help` for all the supported options.\n\nThe default settings for `enclavectl` are stored in the local `settings.json` file.\n\nIn this file the following input can be provided:\n- AWS region\n- Instance type\n- EKS cluster name\n- EKS nodegroup name\n- EKS nodegroup desired capacity\n- K8s version\n- CPUs per node to reserve for Nitro Enclaves\n- Memory per node to reserve for Nitro Enclaves\n\nHere is an example of a configuration file:\n```bash\n{\n  \"region\" : \"eu-central-1\",\n  \"instance_type\" : \"m5.2xlarge\",\n  \"eks_cluster_name\" : \"eks-ne-cluster\",\n  \"eks_worker_node_name\" : \"eks-ne-nodegroup\",\n  \"eks_worker_node_capacity\" : \"1\",\n  \"k8s_version\" : \"1.22\",\n  \"node_enclave_cpu_limit\": 2,\n  \"node_enclave_memory_limit_mib\": 768\n}\n```\n\n## Building and running the hello example\n\n1) Adapt the configuration and apply it to the project:\n```bash\nenclavectl configure --file settings.json\n```\nAfter finishing, the tool confirms a successful configuration like below\n\n```bash\n[enclavectl] Configuration finished successfully.\n```\n\n2) Create a Nitro Enclave aware EKS cluster. This will use the `EnclaveOptions=true` parameter in the EC2 launch template that shall be used on the cluster nodegroup:\n\n```\nenclavectl setup\n```\nThis high-level command consists of three internal steps:\n- Generates a basic EC2 Launch Template for Nitro Enclaves and UserData\n- Creates an EKS cluster with a managed node-group of configured capacity\n- Deploys the [Nitro Enclaves K8s Device plugin](https://github.com/aws/aws-nitro-enclaves-k8s-device-plugin): This plugin enables Kubernetes [pods](https://kubernetes.io/docs/concepts/workloads/pods/) to access Nitro Enclaves device driver. As part of this step, the plugin is deployed as a [daemonset](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) to the cluster.\n\n3) Build the hello enclave application\n\nUsually, applications run on EKS clusters in containers. A Nitro Enclave applications need one more step for running in an enclave - it needs to be packaged in an **Enclave Image File (EIF)**. To get more information about building **EIFs**, please take a look at this [user guide](https://docs.aws.amazon.com/enclaves/latest/user/building-eif.html).\n\nTo trigger a build, use:\n```bash\nenclavectl build --image hello\n```\n\nThis phase makes use of a builder docker container which builds the targeted application if it is present in the `container` directory and packages it in a ready-to-deploy container.\nAll application deliverables, including the Nitro Enclave EIF, are put in the `container/bin/` folder.\n\n4) Push the hello enclave application to a remote repository\nFor deploying, a docker repository shall be required. We will be using [Amazon Elastic Container Registry (ECR)](https://aws.amazon.com/ecr/) for this purpose.\n```bash\nenclavectl push --image hello\n```\n\nUnless it has already been created, this also creates a repository under your private ECR and pushes the **hello** image to it.\nFor the subsequent uses, the command will always use the previously created repository.\n\n5) Deploy the hello enclave application\n\nThis command does necessary pre-initialization (if exists) for the application before deployment and generates deployment specification.\n```bash\nenclavectl run --image hello --prepare-only\n```\n\nTo see the contents of the deployment specification, use:\n```bash\ncat hello_deployment.yaml\n```\n\nFinally, to start the actual deployment, use:\n```bash\nkubectl apply -f hello_deployment.yaml\n```\n\nThe above steps can be done via a single command:\n```bash\nenclavectl run --image hello\n```\n\n6) Check the application logs\n\nTo check deployment status of the hello application, use\n```bash\nkubectl get pods --selector app=hello --watch\n```\n\nAfter a while the command is expected to report a similar output like below after a short while:\n\n```bash\nNAME                               READY   STATUS              RESTARTS   AGE\nhello-deployment-7bf5d9b79-qv8vm   0/1     Pending             0          2s\nhello-deployment-7bf5d9b79-qv8vm   0/1     Pending             0          27s\nhello-deployment-7bf5d9b79-qv8vm   0/1     ContainerCreating   0          27s\nhello-deployment-7bf5d9b79-qv8vm   1/1     Running             0          30s\n```\n\nWhen you ensure that the application is running, press \"Ctrl + C\" to terminate `kubectl` and check the logs:\n```bash\nkubectl logs hello-deployment-7bf5d9b79-qv8vm\n```\nYou can find the `<deployment name>` from your terminal logs. After successful execution of this command, you will see output like this below.\n\n```bash\n[   1] Hello from the enclave side!\n[   2] Hello from the enclave side!\n[   3] Hello from the enclave side!\n[   4] Hello from the enclave side!\n[   5] Hello from the enclave side!\n```\nThe application keeps printing \"Hello from the enclave side!\" message every 5 seconds.\n\n7) Stop the application logs\n\nTo clear the previous deployment, use:\n```bash\nenclavectl stop --image hello\n```\nThis function not only executes `kubectl -f delete hello_deployment.yaml` in the background, but also uninitalizes resources if any were initialized after `enclavectl run` command.\n\n## Building and running the kmstool example\n\n[KMS Tool](https://github.com/aws/aws-nitro-enclaves-sdk-c/blob/main/docs/kmstool.md) is an example application that uses is able to connect to KMS and decrypt an encrypted KMS message.\n\n**NOTE**: The user would be required to create a role which is associated with the EC2 instance that has permissions to access the KMS service in order to create a key, encrypt a message and decrypt the message inside the enclave. This is the way and the recommended way Nitro Enclaves are used today by users. More information in the doc listed at the beginning of the section.\n\nFor this demo application in EKS, we already have a role associated with the instance but those permissions do not apply to the Kubernetes containers. In order to resolve this, we require a [service account](https://docs.aws.amazon.com/eks/latest/userguide/service-accounts.html) that has all the required permissions with KMS in order to issue a successful KMS Decrypt from inside the Nitro Enclave.\n\nAs an important note, AWS currently supports one enclave per EC2 instance. Before moving on, please ensure you stopped the previous `hello` deployment.\n\nTo run KMS example, please follow the similar steps below as you did for the `hello` application.\n\n```bash\nenclavectl build --image kms\nenclavectl push --image kms\nenclavectl run --image kms\nkubectl get pods --selector app=kms --watch\n```\n\nAnd check the logs to see that the enclave has decrypted the message:\n```bash\nkubectl logs <kms-deployment-name>\n```\nAfter successful execution of this command, you will see output like this below. (User specific data has been truncated)\n```bash\n[kms-example] Creating a KMS key...\n[kms-example] Encrypting message...\n[kms-example] ******************************\n[kms-example] KMS Key ARN: arn:aws:kms:[...]\n[kms-example] Account ID: [...]\n[kms-example] Unencrypted message: Hello, KMS\\!\n[kms-example] Ciphertext: AQICAHg7LT9PYQzAhL3hhzA4N15Lsok7f4DEEPGiNf8fyUM+5QHHy85xZXBek7uFPtNX+vJyAAAAZDBiBgkqhkiG9w0BBwagVTBTAgEAME4GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMspJf9GEN1DqaJ55sAgEQgCGorI4UgmAAwmhfgsuXIud/PcTKwt8K8L/aPyj8Hq6KIVo=\n[kms-example] ******************************\nStart allocating memory...\nStarted enclave with enclave-cid: 18, memory: 128 MiB, cpu-ids: [1, 5]\n[kms-example] Requesting from the enclave to decrypt message...\n[kms-example] ------------------------\n[kms-example] > Got response from the enclave!\n[kms-example] Object = { \"Status\": \"Ok\" } Object = { \"Status\": \"Ok\", \"Message\": \"HelloKMS\" }\n[kms-example] ------------------------\nSuccessfully terminated enclave i-[...]-enc[...]\n```\n\n## Creating your own example application\n\nTo quickly create your own application within this tutorial, you need to perform a few more steps. All application specific data is stored under the `container` folder. The `hello` can be\na good example to see what kind of files are required for your application. To see more information, please check this [document](./container/README.md).\n\n## Cleaning up AWS resources\nIf you followed this tutorial partially or entirely, it must have created some AWS resources. To clean them up, use:\n\n```bash\nenclavectl cleanup\n```\n\n## Security issue notifications\n\nIf you discover a potential security issue, we ask that you notify AWS Security via our [vulnerability reporting page](https://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public GitHub issue.\n", "release_dates": ["2022-11-28T18:27:37Z"]}, {"name": "aws-node-termination-handler", "description": "Gracefully handle EC2 instance shutdown within Kubernetes", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<h1>AWS Node Termination Handler</h1>\n\n<h4>Gracefully handle EC2 instance shutdown within Kubernetes</h4>\n\n<p>\n  <a href=\"https://github.com/kubernetes/kubernetes/releases\">\n    <img src=\"https://img.shields.io/badge/Kubernetes-%3E%3D%201.22-brightgreen\" alt=\"kubernetes\">\n  </a>\n  <a href=\"https://golang.org/doc/go1.21\">\n    <img src=\"https://img.shields.io/github/go-mod/go-version/aws/aws-node-termination-handler?color=blueviolet\" alt=\"go-version\">\n  </a>\n  <a href=\"https://opensource.org/licenses/Apache-2.0\">\n    <img src=\"https://img.shields.io/badge/License-Apache%202.0-ff69b4.svg\" alt=\"license\">\n  </a>\n  <a href=\"https://codecov.io/gh/aws/aws-node-termination-handler\">\n    <img src=\"https://img.shields.io/codecov/c/github/aws/aws-node-termination-handler\" alt=\"build-status\">\n  </a>\n  <a href=\"https://gallery.ecr.aws/aws-ec2/aws-node-termination-handler\">\n    <img src=\"https://img.shields.io/docker/pulls/amazon/aws-node-termination-handler\" alt=\"docker-pulls\">\n  </a>\n    <a href=\"https://github.com/aws/aws-node-termination-handler/workflows\">\n    <img src=\"https://img.shields.io/github/workflow/status/aws/aws-node-termination-handler/Build%20and%20Test?label=Builds%20%26%20Tests\">\n  </a>\n</p>\n\n<div>\n<hr>\n</div>\n\n\n## Project Summary\n\nThis project ensures that the Kubernetes control plane responds appropriately to events that can cause your EC2 instance to become unavailable, such as [EC2 maintenance events](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instances-status-check_sched.html), [EC2 Spot interruptions](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html), [ASG Scale-In](https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html#as-lifecycle-scale-in), ASG AZ Rebalance, and EC2 Instance Termination via the API or Console.  If not handled, your application code may not stop gracefully, take longer to recover full availability, or accidentally schedule work to nodes that are going down.\n\nThe aws-node-termination-handler (NTH) can operate in two different modes: Instance Metadata Service (IMDS) or the Queue Processor.\n\nThe aws-node-termination-handler **[Instance Metadata Service](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html) Monitor** will run a small pod on each host to perform monitoring of IMDS paths like `/spot` or `/events` and react accordingly to drain and/or cordon the corresponding node.\n\nThe aws-node-termination-handler **Queue Processor** will monitor an SQS queue of events from Amazon EventBridge for ASG lifecycle events, EC2 status change events, Spot Interruption Termination Notice events, and Spot Rebalance Recommendation events. When NTH detects an instance is going down, we use the Kubernetes API to cordon the node to ensure no new work is scheduled there, then drain it, removing any existing work. The termination handler **Queue Processor** requires AWS IAM permissions to monitor and manage the SQS queue and to query the EC2 API.\n\nYou can run the termination handler on any Kubernetes cluster running on AWS, including self-managed clusters and those created with Amazon [Elastic Kubernetes Service](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html). If you're using [EKS managed node groups](https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html), you don't need the aws-node-termination-handler.\n\n## Major Features\n\nBoth modes (IMDS and Queue Processor) monitor for events affecting your EC2 instances, but each supports different types of events. Both modes have the following:\n\n- Helm installation and event configuration support\n- Webhook feature to send shutdown or restart notification messages\n- Unit & integration tests\n\n### Instance Metadata Service Processor\nMust be deployed as a Kubernetes **DaemonSet**.\n\n- Monitors EC2 Instance Metadata for:\n   - [Spot Instance Termination Notifications](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-instance-termination-notices.html)\n   - [Scheduled Events](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instances-status-check_sched.html)\n   - [Instance Rebalance Recommendations](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/rebalance-recommendations.html)\n\n### Queue Processor\nMust be deployed as a Kubernetes **Deployment**. Also requires some **additional infrastructure setup** (including SQS queue, EventBridge rules).\n\n- Monitors an SQS Queue for:\n   - Spot Instance Termination Notifications\n   - Scheduled Events (via AWS Health)\n   - Instance Rebalance Recommendations\n   - ASG Termination Lifecycle Hooks to handle the following:\n     - [ASG Scale-In](https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html)\n     - [Availability Zone Rebalance](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#:~:text=are%20replaced%20first.-,Availability%20Zone%20rebalancing,-Amazon%20EC2%20Auto)\n     - [Unhealthy Instances](https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html), and more\n   - [Instance State Change events](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instance-state-changes.html)\n\n### Which one should I use?\n|                    Feature                    | IMDS Processor | Queue Processor |\n| :-------------------------------------------: | :------------: | :-------------: |\n| Spot Instance Termination Notifications (ITN) |       \u2705        |        \u2705        |\n|               Scheduled Events                |       \u2705        |        \u2705        |\n|       Instance Rebalance Recommendation       |       \u2705        |        \u2705        |\n|        AZ Rebalance Recommendation            |       \u274c        |        \u2705        |\n|        ASG Termination Lifecycle Hooks        |       \u274c        |        \u2705        |\n|         Instance State Change Events          |       \u274c        |        \u2705        |\n\n### Kubernetes Compatibility\n\n|                                      NTH Release                                      | K8s v1.28 | K8s v1.27 | K8s v1.26 | K8s v1.25 | K8s v1.24 | K8s v1.23 | K8s v1.22 | K8s v1.21 |\n| :-----------------------------------------------------------------------------------: | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n|  [v1.20.0](https://github.com/aws/aws-node-termination-handler/releases/tag/v1.20.0)  |     \u2705    |     \u2705    |     \u2705    |     \u2705    |    \u2705     |    \u2705    |    \u2705     |    \u274c    |\n|  [v1.19.0](https://github.com/aws/aws-node-termination-handler/releases/tag/v1.19.0)  |     \u274c    |     \u274c    |     \u274c    |     \u274c    |    \u274c     |    \u2705    |    \u2705     |    \u2705    |\n|  [v1.18.3](https://github.com/aws/aws-node-termination-handler/releases/tag/v1.18.3)  |     \u274c    |     \u274c    |     \u274c    |     \u274c    |    \u274c     |    \u2705    |    \u2705     |    \u2705    |\n|  [v1.18.2](https://github.com/aws/aws-node-termination-handler/releases/tag/v1.18.2)  |     \u274c    |     \u274c    |     \u274c    |     \u274c    |    \u274c     |    \u2705    |    \u2705     |    \u2705    |\n|  [v1.18.1](https://github.com/aws/aws-node-termination-handler/releases/tag/v1.18.1)  |     \u274c    |     \u274c    |     \u274c    |     \u274c    |    \u274c     |    \u2705    |    \u2705     |    \u2705    |\n|  [v1.18.0](https://github.com/aws/aws-node-termination-handler/releases/tag/v1.18.0)  |     \u274c    |     \u274c    |     \u274c    |     \u274c    |    \u274c     |    \u2705    |    \u2705     |    \u2705    |\n\n\n## Installation and Configuration\n\nThe aws-node-termination-handler can operate in two different modes: IMDS Processor and Queue Processor. The `enableSqsTerminationDraining` helm configuration key or the `ENABLE_SQS_TERMINATION_DRAINING` environment variable are used to enable the Queue Processor mode of operation. If `enableSqsTerminationDraining` is set to true, then IMDS paths will NOT be monitored. If the `enableSqsTerminationDraining` is set to false, then IMDS Processor Mode will be enabled. Queue Processor Mode and IMDS Processor Mode cannot be run at the same time.\n\nIMDS Processor Mode allows for a fine-grained configuration of IMDS paths that are monitored. There are currently 3 paths supported that can be enabled or disabled by using the following helm configuration keys:\n - `enableSpotInterruptionDraining`\n - `enableRebalanceMonitoring`\n - `enableScheduledEventDraining`\n\nBy default, IMDS mode will only Cordon in response to a Rebalance Recommendation event (all other events are Cordoned and Drained). Cordon is the default for a rebalance event because it's not known if an ASG is being utilized and if that ASG is configured to replace the instance on a rebalance event. If you are using an ASG w/ rebalance recommendations enabled, then you can set the `enableRebalanceDraining` flag to true to perform a Cordon and Drain when a rebalance event is received.\n\nRebalance Recommendation is an early indicator to notify the Spot Instances that they can be interrupted soon. Node Termination Handler supports AZ Rebalance Recommendation only in Queue Processor mode using ASG Lifecycle Hooks. For AZ rebalances the instances are just terminated, using Lifecycle Hooks and EventBridge rule for `EC2 Instance-terminate Lifecycle Action` we can handle OD Instances.\n\nThe `enableSqsTerminationDraining` must be set to false for these configuration values to be considered.\n\nThe Queue Processor Mode does not allow for fine-grained configuration of which events are handled through helm configuration keys. Instead, you can modify your Amazon EventBridge rules to not send certain types of events to the SQS Queue so that NTH does not process those events. All events when operating in Queue Processor mode are Cordoned and Drained unless the `cordon-only` flag is set to true.\n\nThe `enableSqsTerminationDraining` flag turns on Queue Processor Mode. When Queue Processor Mode is enabled, IMDS mode will be disabled, even if you explicitly enabled any of the IMDS configuration keys. NTH cannot respond to queue events AND monitor IMDS paths. In this case, it is safe to disable IMDS for the NTH pod.\n\n<details opened>\n<summary>AWS Node Termination Handler - IMDS Processor</summary>\n<br>\n\n### Installation and Configuration\n\nThe termination handler DaemonSet installs into your cluster a [ServiceAccount](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/), [ClusterRole](https://kubernetes.io/docs/reference/access-authn-authz/rbac/), [ClusterRoleBinding](https://kubernetes.io/docs/reference/access-authn-authz/rbac/), and a [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/). All four of these Kubernetes constructs are required for the termination handler to run properly.\n\n#### Pod Security Admission\n\nWhen using Kubernetes [Pod Security Admission](https://kubernetes.io/docs/concepts/security/pod-security-admission/) it is recommended to assign the `[privileged](https://kubernetes.io/docs/concepts/security/pod-security-standards/#privileged)` level.\n\n#### Kubectl Apply\n\nYou can use kubectl to directly add all of the above resources with the default configuration into your cluster.\n\n```\nkubectl apply -f https://github.com/aws/aws-node-termination-handler/releases/download/v1.20.0/all-resources.yaml\n```\n\nFor a full list of releases and associated artifacts see our [releases page](https://github.com/aws/aws-node-termination-handler/releases).\n\n#### Helm\n\nThe easiest way to configure the various options of the termination handler is via [helm](https://helm.sh/). The chart for this project is hosted in [helm/aws-node-termination-handler](https://gallery.ecr.aws/aws-ec2/helm/aws-node-termination-handler)\n\nTo get started you need to authenticate your helm client\n\n```\naws ecr-public get-login-password \\\n  --region us-east-1 | helm registry login \\\n  --username AWS \\\n  --password-stdin public.ecr.aws\n```\n\nOnce that is complete you can install the termination handler. We've provided some sample setup options below. Make sure to replace CHART_VERSION with the version you want to install.\n\nZero Config:\n\n```sh\nhelm upgrade --install aws-node-termination-handler \\\n  --namespace kube-system \\\n  oci://public.ecr.aws/aws-ec2/helm/aws-node-termination-handler --version $CHART_VERSION\n```\n\nEnabling Features:\n\n```\nhelm upgrade --install aws-node-termination-handler \\\n  --namespace kube-system \\\n  --set enableSpotInterruptionDraining=\"true\" \\\n  --set enableRebalanceMonitoring=\"true\" \\\n  --set enableScheduledEventDraining=\"false\" \\\n  oci://public.ecr.aws/aws-ec2/helm/aws-node-termination-handler --version $CHART_VERSION\n```\n\nThe `enable*` configuration flags above enable or disable IMDS monitoring paths.\n\nRunning Only On Specific Nodes:\n\n```\nhelm upgrade --install aws-node-termination-handler \\\n  --namespace kube-system \\\n  --set nodeSelector.lifecycle=spot \\\n  oci://public.ecr.aws/aws-ec2/helm/aws-node-termination-handler --version $CHART_VERSION\n```\n\nWebhook Configuration:\n\n```\nhelm upgrade --install aws-node-termination-handler \\\n  --namespace kube-system \\\n  --set webhookURL=https://hooks.slack.com/services/YOUR/SLACK/URL \\\n  oci://public.ecr.aws/aws-ec2/helm/aws-node-termination-handler --version $CHART_VERSION\n```\n\nAlternatively, pass Webhook URL as a Secret:\n\n```\nWEBHOOKURL_LITERAL=\"webhookurl=https://hooks.slack.com/services/YOUR/SLACK/URL\"\n\nkubectl create secret -n kube-system generic webhooksecret --from-literal=$WEBHOOKURL_LITERAL\n```\n```\nhelm upgrade --install aws-node-termination-handler \\\n  --namespace kube-system \\\n  --set webhookURLSecretName=webhooksecret \\\n  oci://public.ecr.aws/aws-ec2/helm/aws-node-termination-handler --version $CHART_VERSION\n```\n\nFor a full list of configuration options see our [Helm readme](https://github.com/aws/aws-node-termination-handler/blob/v1.20.0/config/helm/aws-node-termination-handler#readme).\n\n</details>\n\n\n<details closed>\n<summary>AWS Node Termination Handler - Queue Processor (requires AWS IAM Permissions)</summary>\n\n<br>\n\n### Infrastructure Setup\n\nThe termination handler requires some infrastructure prepared before deploying the application. In a multi-cluster environment, you will need to repeat the following steps for each cluster.\n\nYou'll need the following AWS infrastructure components:\n\n1. Amazon Simple Queue Service (SQS) Queue\n2. AutoScaling Group Termination Lifecycle Hook\n3. Instance Tagging\n4. Amazon EventBridge Rule\n5. IAM Role for the aws-node-termination-handler Queue Processing Pods\n\nOptional AWS infrastructure components:\n1. AutoScaling Group Launch Lifecycle Hook\n\n#### 1. Create an SQS Queue:\n\nHere is the AWS CLI command to create an SQS queue to hold termination events from ASG and EC2, although this should really be configured via your favorite infrastructure-as-code tool like CloudFormation (template [here](docs/cfn-template.yaml)) or Terraform:\n\n```\n## Queue Policy\nQUEUE_POLICY=$(cat <<EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Id\": \"MyQueuePolicy\",\n    \"Statement\": [{\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"Service\": [\"events.amazonaws.com\", \"sqs.amazonaws.com\"]\n        },\n        \"Action\": \"sqs:SendMessage\",\n        \"Resource\": [\n            \"arn:aws:sqs:${AWS_REGION}:${ACCOUNT_ID}:${SQS_QUEUE_NAME}\"\n        ]\n    }]\n}\nEOF\n)\n\n## make sure the queue policy is valid JSON\necho \"$QUEUE_POLICY\" | jq .\n\n## Save queue attributes to a temp file\ncat << EOF > /tmp/queue-attributes.json\n{\n  \"MessageRetentionPeriod\": \"300\",\n  \"Policy\": \"$(echo $QUEUE_POLICY | sed 's/\\\"/\\\\\"/g' | tr -d -s '\\n' \" \")\",\n  \"SqsManagedSseEnabled\": \"true\"\n}\nEOF\n\naws sqs create-queue --queue-name \"${SQS_QUEUE_NAME}\" --attributes file:///tmp/queue-attributes.json\n```\n\nIf you are sending Lifecycle termination events from ASG directly to SQS, instead of through EventBridge, then you will also need to create an IAM service role to give Amazon EC2 Auto Scaling access to your SQS queue. Please follow [these linked instructions to create the IAM service role: link.](https://docs.aws.amazon.com/autoscaling/ec2/userguide/configuring-lifecycle-hook-notifications.html#sqs-notifications)\nNote the ARNs for the SQS queue and the associated IAM role for Step 2.\n\nThere are some caveats when using [server side encryption with SQS](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-server-side-encryption.html):\n* using [SSE-KMS](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-sse-existing-queue.html) with a [customer managed key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#key-mgmt) requires [changing the KMS key policy](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-troubleshooting.html#eb-sqs-encrypted) to allow EventBridge to publish events to SQS.\n* using [SSE-KMS](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-sse-existing-queue.html) with an [AWS managed key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#key-mgmt) is not supported as the KMS key policy can't be updated to allow EventBridge to publish events to SQS.\n* using [SSE-SQS](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-sqs-sse-queue.html) doesn't require extra setup and works out of the box as SQS queues without encryption at rest.\n\n#### 2. Create an ASG Termination Lifecycle Hook:\n\nHere is the AWS CLI command to create a termination lifecycle hook on an existing ASG when using EventBridge, although this should really be configured via your favorite infrastructure-as-code tool like CloudFormation or Terraform:\n\n```\naws autoscaling put-lifecycle-hook \\\n  --lifecycle-hook-name=my-k8s-term-hook \\\n  --auto-scaling-group-name=my-k8s-asg \\\n  --lifecycle-transition=autoscaling:EC2_INSTANCE_TERMINATING \\\n  --default-result=CONTINUE \\\n  --heartbeat-timeout=300\n```\n\nIf you want to avoid using EventBridge and instead send ASG Lifecycle events directly to SQS, instead use the following command, using the ARNs from Step 1:\n\n```\naws autoscaling put-lifecycle-hook \\\n  --lifecycle-hook-name=my-k8s-term-hook \\\n  --auto-scaling-group-name=my-k8s-asg \\\n  --lifecycle-transition=autoscaling:EC2_INSTANCE_TERMINATING \\\n  --default-result=CONTINUE \\\n  --heartbeat-timeout=300 \\\n  --notification-target-arn <your queue ARN here> \\\n  --role-arn <your SQS access role ARN here>\n```\n\n#### 3. Tag the Instances:\n\nBy default the aws-node-termination-handler will only manage terminations for instances tagged with `key=aws-node-termination-handler/managed`.\nThe value of the key does not matter.\n\nTo tag ASGs and propagate the tags to your instances (recommended):\n```\naws autoscaling create-or-update-tags \\\n  --tags ResourceId=my-auto-scaling-group,ResourceType=auto-scaling-group,Key=aws-node-termination-handler/managed,Value=,PropagateAtLaunch=true\n```\n\nTo tag an individual EC2 instance:\n```\naws ec2 create-tags \\\n    --resources i-1234567890abcdef0 \\\n    --tags 'Key=\"aws-node-termination-handler/managed\",Value='\n```\n\nTagging your EC2 instances in this way is helpful if you only want aws-node-termination-handler to manage the lifecycle of instances in certain ASGs. For example, if your account also has other ASGs that do not contain Kubernetes nodes, this tagging mechanism will ensure that NTH does not manage the lifecycle of any instances in those non-Kubernetes ASGs.\n\nHowever, if the only ASGs in your account are for your Kubernetes cluster, then you can turn off the tag check by setting the flag `--check-tag-before-draining=false` or environment variable `CHECK_TAG_BEFORE_DRAINING=false`.\n\nYou can also control what resources NTH manages by adding the resource ARNs to your Amazon EventBridge rules.\n\nTake a look at the docs on how to [create rules that only manage certain ASGs](https://docs.aws.amazon.com/autoscaling/ec2/userguide/cloud-watch-events.html), and read about all the [supported ASG events](https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-event-reference.html).\n\n#### 4. Create Amazon EventBridge Rules\n\nYou may skip this step if sending events from ASG to SQS directly.\n\nIf we use ASG with capacity-rebalance enabled on ASG, then we do not need Spot and Rebalance events enabled with EventBridge. ASG will send a termination lifecycle hook for spot interrruptions while it's launching a new instance and for Rebalance events ASG will send a termination lifecycle hook after it brings a new node in the ASG.\n\nIf we use ASG without capacity-rebalance enabled, then spot interruptions will cause a termination lifecycle hook after the interruption occurs but not while launching the new instance.\n\nHere are AWS CLI commands to create Amazon EventBridge rules so that ASG termination events, Spot Interruptions, Instance state changes, Rebalance Recommendations, and AWS Health Scheduled Changes are sent to the SQS queue created in the previous step. This should really be configured via your favorite infrastructure-as-code tool like CloudFormation (template [here](docs/cfn-template.yaml)) or Terraform:\n\n```\naws events put-rule \\\n  --name MyK8sASGTermRule \\\n  --event-pattern \"{\\\"source\\\":[\\\"aws.autoscaling\\\"],\\\"detail-type\\\":[\\\"EC2 Instance-terminate Lifecycle Action\\\"]}\"\n\naws events put-targets --rule MyK8sASGTermRule \\\n  --targets \"Id\"=\"1\",\"Arn\"=\"arn:aws:sqs:us-east-1:123456789012:MyK8sTermQueue\"\n\naws events put-rule \\\n  --name MyK8sSpotTermRule \\\n  --event-pattern \"{\\\"source\\\": [\\\"aws.ec2\\\"],\\\"detail-type\\\": [\\\"EC2 Spot Instance Interruption Warning\\\"]}\"\n\naws events put-targets --rule MyK8sSpotTermRule \\\n  --targets \"Id\"=\"1\",\"Arn\"=\"arn:aws:sqs:us-east-1:123456789012:MyK8sTermQueue\"\n\naws events put-rule \\\n  --name MyK8sRebalanceRule \\\n  --event-pattern \"{\\\"source\\\": [\\\"aws.ec2\\\"],\\\"detail-type\\\": [\\\"EC2 Instance Rebalance Recommendation\\\"]}\"\n\naws events put-targets --rule MyK8sRebalanceRule \\\n  --targets \"Id\"=\"1\",\"Arn\"=\"arn:aws:sqs:us-east-1:123456789012:MyK8sTermQueue\"\n\naws events put-rule \\\n  --name MyK8sInstanceStateChangeRule \\\n  --event-pattern \"{\\\"source\\\": [\\\"aws.ec2\\\"],\\\"detail-type\\\": [\\\"EC2 Instance State-change Notification\\\"]}\"\n\naws events put-targets --rule MyK8sInstanceStateChangeRule \\\n  --targets \"Id\"=\"1\",\"Arn\"=\"arn:aws:sqs:us-east-1:123456789012:MyK8sTermQueue\"\n\naws events put-rule \\\n  --name MyK8sScheduledChangeRule \\\n  --event-pattern \"{\\\"source\\\": [\\\"aws.health\\\"],\\\"detail-type\\\": [\\\"AWS Health Event\\\"],\\\"detail\\\": {\\\"service\\\": [\\\"EC2\\\"],\\\"eventTypeCategory\\\": [\\\"scheduledChange\\\"]}}\"\n\naws events put-targets --rule MyK8sScheduledChangeRule \\\n  --targets \"Id\"=\"1\",\"Arn\"=\"arn:aws:sqs:us-east-1:123456789012:MyK8sTermQueue\"\n```\n\n#### 5. Create an IAM Role for the Pods\n\nThere are many different ways to allow the aws-node-termination-handler pods to assume a role:\n\n1. [Amazon EKS IAM Roles for Service Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html)\n2. [IAM Instance Profiles for EC2](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html)\n3. [Kiam](https://github.com/uswitch/kiam)\n4. [kube2iam](https://github.com/jtblin/kube2iam)\n\nIAM Policy for aws-node-termination-handler Deployment:\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"autoscaling:CompleteLifecycleAction\",\n                \"autoscaling:DescribeAutoScalingInstances\",\n                \"autoscaling:DescribeTags\",\n                \"ec2:DescribeInstances\",\n                \"sqs:DeleteMessage\",\n                \"sqs:ReceiveMessage\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n```\n\n#### 1. Handle ASG Instance Launch Lifecycle Notifications (optional):\n\nNTH can monitor for new instances launched by an ASG and notify the ASG when the instance is available in the EKS cluster.\n\nNTH will need to receive notifications of new instance launches within the ASG.  We can add a lifecycle hook to the ASG that will send instance launch notifications via EventBridge:\n\n```\naws autoscaling put-lifecycle-hook \\\n  --lifecycle-hook-name=my-k8s-launch-hook \\\n  --auto-scaling-group-name=my-k8s-asg \\\n  --lifecycle-transition=autoscaling:EC2_INSTANCE_LAUNCHING \\\n  --default-result=\"ABANDON\" \\\n  --heartbeat-timeout=300\n```\n\nAlternatively, ASG can send the instance launch notification directly to an SQS Queue:\n\n```\naws autoscaling put-lifecycle-hook \\\n  --lifecycle-hook-name=my-k8s-launch-hook \\\n  --auto-scaling-group-name=my-k8s-asg \\\n  --lifecycle-transition=autoscaling:EC2_INSTANCE_LAUNCHING \\\n  --default-result=\"ABANDON\" \\\n  --heartbeat-timeout=300 \\\n  --notification-target-arn <your queue ARN here> \\\n  --role-arn <your SQS access role ARN here>\n```    \n\nWhen NTH receives a launch notification, it will periodically check for a node backed by the EC2 instance to join the cluster and for the node to have a status of 'ready.' Once a node becomes ready, NTH will complete the lifecycle hook, prompting the ASG to proceed with terminating the previous instance. If the lifecycle hook is not completed before the timeout, the ASG will take the default action. If the default action is 'ABANDON', the new instance will be terminated, and the notification process will be repeated with another new instance.\n\n### Installation\n\n#### Pod Security Admission\n\nWhen using Kubernetes [Pod Security Admission](https://kubernetes.io/docs/concepts/security/pod-security-admission/) it is recommended to assign the `[baseline](https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline)` level.\n\n#### Helm\n\nThe easiest way to configure the various options of the termination handler is via [helm](https://helm.sh/). The chart for this project is hosted in [helm/aws-node-termination-handler](https://gallery.ecr.aws/aws-ec2/helm/aws-node-termination-handler)\n\nTo get started you need to authenticate your helm client\n\n```\naws ecr-public get-login-password \\\n     --region us-east-1 | helm registry login \\\n     --username AWS \\\n     --password-stdin public.ecr.aws\n```\n\nOnce that is complete you can install the termination handler. We've provided some sample setup options below. Make sure to replace CHART_VERSION with the version you want to install.\n\nMinimal Config:\n\n```sh\nhelm upgrade --install aws-node-termination-handler \\\n  --namespace kube-system \\\n  --set enableSqsTerminationDraining=true \\\n  --set queueURL=https://sqs.us-east-1.amazonaws.com/0123456789/my-term-queue \\\n  oci://public.ecr.aws/aws-ec2/helm/aws-node-termination-handler --version $CHART_VERSION\n```\n\nWebhook Configuration:\n\n```\nhelm upgrade --install aws-node-termination-handler \\\n  --namespace kube-system \\\n  --set enableSqsTerminationDraining=true \\\n  --set queueURL=https://sqs.us-east-1.amazonaws.com/0123456789/my-term-queue \\\n  --set webhookURL=https://hooks.slack.com/services/YOUR/SLACK/URL \\\n  oci://public.ecr.aws/aws-ec2/helm/aws-node-termination-handler --version $CHART_VERSION\n```\n\nAlternatively, pass Webhook URL as a Secret:\n\n```\nWEBHOOKURL_LITERAL=\"webhookurl=https://hooks.slack.com/services/YOUR/SLACK/URL\"\n\nkubectl create secret -n kube-system generic webhooksecret --from-literal=$WEBHOOKURL_LITERAL\n```\n```\nhelm upgrade --install aws-node-termination-handler \\\n  --namespace kube-system \\\n  --set enableSqsTerminationDraining=true \\\n  --set queueURL=https://sqs.us-east-1.amazonaws.com/0123456789/my-term-queue \\\n  --set webhookURLSecretName=webhooksecret \\\n  oci://public.ecr.aws/aws-ec2/helm/aws-node-termination-handler --version $CHART_VERSION\n```\n\nFor a full list of configuration options see our [Helm readme](https://github.com/aws/aws-node-termination-handler/blob/v1.20.0/config/helm/aws-node-termination-handler#readme).\n\n#### Single Instance vs Multiple Replicas\n\nThe Helm chart, by default, will deploy a single instance of Amazon Node Termination Handler. With the minimizing of resource usage, a single instance still provides good responsiveness in processing SQS messages.\n\n**When should multiple instances of Amazon Node Termination Handler be used?**\n\n* Responsiveness: Amazon Node Termination Handler may be taking longer than desired to process certain events, potentially in processing numerous concurrent events or taking too long to drain Pods. The deployment of multiple Amazon Node Termination Handler instances may help.\n\n* Availability: The deployment of multiple Amazon Node Termination Handler instances provides mitigation in the case that Amazon Node Termination Handler itself is drained. Replica Amazon Node Termination Handlers will process SQS messages, avoiding a delay until the Deployment can start another instance. \n\n**Notes**\n\n* Running multiple instances of Amazon Node Termination Handler will not load balance responding to events. Each instance will greedily consume and respond to events.\n* Logs from multiple instances of Amazon Node Termination Handler are not aggregated.\n* Multiple instances of Amazon Node Termination Handler may respond to the same event, if it takes longer than 20s to process. This is not an error case, only the first response will have an affect.\n\n#### Kubectl Apply\n\nQueue Processor needs an **SQS queue URL** to function; therefore, manifest changes are **REQUIRED** before using kubectl to directly add all of the above resources into your cluster.\n\nMinimal Config:\n\n```\ncurl -L https://github.com/aws/aws-node-termination-handler/releases/download/v1.20.0/all-resources-queue-processor.yaml -o all-resources-queue-processor.yaml\n<open all-resources-queue-processor.yaml and update QUEUE_URL value>\nkubectl apply -f ./all-resources-queue-processor.yaml\n```\n\nFor a full list of releases and associated artifacts see our [releases page](https://github.com/aws/aws-node-termination-handler/releases).\n\n</details>\n\n\n<details close>\n<summary>Use with Kiam</summary>\n<br>\n\n## Use with Kiam\n\nIf you are using IMDS mode which defaults to `hostNetworking: true`, or if you are using queue-processor mode, then this section does not apply. The configuration below only needs to be used if you are explicitly changing NTH IMDS mode to `hostNetworking: false` .\n\nTo use the termination handler alongside [Kiam](https://github.com/uswitch/kiam) requires some extra configuration on Kiam's end.\nBy default Kiam will block all access to the metadata address, so you need to make sure it passes through the requests the termination handler relies on.\n\nTo add a whitelist configuration, use the following fields in the Kiam Helm chart values:\n\n```\nagent.whiteListRouteRegexp: '^\\/latest\\/meta-data\\/(spot\\/instance-action|events\\/maintenance\\/scheduled|instance-(id|type)|public-(hostname|ipv4)|local-(hostname|ipv4)|placement\\/availability-zone)|\\/latest\\/dynamic\\/instance-identity\\/document$'\n```\nOr just pass it as an argument to the kiam agents:\n\n```\nkiam agent --whitelist-route-regexp='^\\/latest\\/meta-data\\/(spot\\/instance-action|events\\/maintenance\\/scheduled|instance-(id|type)|public-(hostname|ipv4)|local-(hostname|ipv4)|placement\\/availability-zone)|\\/latest\\/dynamic\\/instance-identity\\/document$'\n```\n\n## Metadata endpoints\nThe termination handler relies on the following metadata endpoints to function properly:\n\n```\n/latest/dynamic/instance-identity/document\n/latest/meta-data/spot/instance-action\n/latest/meta-data/events/recommendations/rebalance\n/latest/meta-data/events/maintenance/scheduled\n/latest/meta-data/instance-id\n/latest/meta-data/instance-life-cycle\n/latest/meta-data/instance-type\n/latest/meta-data/public-hostname\n/latest/meta-data/public-ipv4\n/latest/meta-data/local-hostname\n/latest/meta-data/local-ipv4\n/latest/meta-data/placement/availability-zone\n```\n\n</details>\n\n## Building\nFor build instructions please consult [BUILD.md](./BUILD.md).\n\n## Metrics\nAvailable Prometheus metrics:\n\n| Metric name    | Description                                                        |                              \n| -------------- | -------------------------------------------------------------------|\n| `actions`      | Number of actions                                                  |\n| `actions_node` | Number of actions per node (Deprecated: Use actions metric instead)|\n| `events_error` | Number of errors in events processing                              |\n\n\n## Communication\n* If you've run into a bug or have a new feature request, please open an [issue](https://github.com/aws/aws-node-termination-handler/issues/new).\n* You can also chat with us in the [Kubernetes Slack](https://kubernetes.slack.com) in the `#provider-aws` channel\n* Check out the open source [Amazon EC2 Spot Instances Integrations Roadmap](https://github.com/aws/ec2-spot-instances-integrations-roadmap) to see what we're working on and give us feedback!\n\n##  Contributing\nContributions are welcome! Please read our [guidelines](https://github.com/aws/aws-node-termination-handler/blob/main/CONTRIBUTING.md) and our [Code of Conduct](https://github.com/aws/aws-node-termination-handler/blob/main/CODE_OF_CONDUCT.md)\n\n## License\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2023-06-21T21:31:48Z", "2023-01-26T20:07:36Z", "2023-01-17T19:51:14Z", "2022-12-14T20:41:29Z", "2022-12-13T20:53:47Z", "2022-11-28T17:16:43Z", "2022-11-21T23:22:01Z", "2022-10-20T19:06:47Z", "2022-09-14T15:47:30Z", "2022-08-31T21:28:25Z", "2022-08-24T19:48:24Z", "2022-08-18T15:14:36Z", "2022-05-31T16:25:28Z", "2022-05-18T19:21:06Z", "2022-05-11T20:38:21Z", "2022-04-20T17:52:39Z", "2022-04-07T14:54:26Z", "2022-03-09T15:57:20Z", "2022-02-10T14:14:40Z", "2022-01-31T22:33:54Z", "2021-10-27T18:42:22Z", "2021-10-15T18:28:52Z", "2021-08-19T22:40:08Z", "2021-07-28T21:33:22Z", "2021-06-08T14:35:11Z", "2021-04-12T18:37:09Z", "2021-03-26T18:41:27Z", "2021-03-02T00:38:37Z", "2021-01-05T16:26:38Z", "2020-12-23T22:02:41Z"]}, {"name": "aws-northstar", "description": "NorthStar is an open source design system with reusable React components for rapidly prototyping intuitive, meaningful and accessible user experience. It simplifies your work and ensures consistent, predictable user experience at scale for your customers. With NorthStar, you can focus on innovation and do more with less.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# NorthStar - Prototyping Design System\n\n[![CodeBuild Status](https://codebuild.us-west-2.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiTm9JSkpnb2IrV09tcGcwU2E5c0diN1lJaVdkbEQza0FoeVVpWnljb3hSZHBEb09mS1pCMmwva3R5SXlYa2Fqem1NVVUrNGJaSFVMTCt5R1VKZ1hDRm5FPSIsIml2UGFyYW1ldGVyU3BlYyI6Ijkyd2FEMFc5cmlmN3hxUnYiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=main)](https://codebuild.us-west-2.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiTm9JSkpnb2IrV09tcGcwU2E5c0diN1lJaVdkbEQza0FoeVVpWnljb3hSZHBEb09mS1pCMmwva3R5SXlYa2Fqem1NVVUrNGJaSFVMTCt5R1VKZ1hDRm5FPSIsIml2UGFyYW1ldGVyU3BlYyI6Ijkyd2FEMFc5cmlmN3hxUnYiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=main)\n[![Github Action Workflow - CI](https://github.com/aws/aws-northstar/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/aws/aws-northstar/actions/workflows/ci.yml)\n[![NorthStar UI NPM latest version](https://img.shields.io/npm/v/@aws-northstar/ui)](https://www.npmjs.com/package/@aws-northstar/ui)\n[![Coverage Status](https://coveralls.io/repos/github/aws/aws-northstar/badge.svg?branch=main)](https://coveralls.io/github/aws/aws-northstar?branch=main)\n[![GitHub Release Date](https://img.shields.io/github/release-date/aws/aws-northstar)](https://github.com/aws/aws-northstar/releases)\n[![NPM Download](https://img.shields.io/npm/dw/@aws-northstar/ui)](https://www.npmjs.com/package/@aws-northstar/ui)\n![NPM type definitions](https://img.shields.io/npm/types/aws-northstar)\n[![Github license](https://img.shields.io/npm/l/aws-northstar)](https://github.com/aws/aws-northstar/blob/main/LICENSE)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/aws/aws-northstar.svg)](https://github.com/aws/aws-northstar/pulls)\n[![GitHub issues](https://img.shields.io/github/issues/aws/aws-northstar)](https://github.com/aws/aws-northstar/issues)\n\nNorthStar is an open source design system with reusable React components for rapidly prototyping intuitive, meaningful and accessible user experience. It simplifies your work and ensures consistent, predictable user experience at scale for your customers. With NorthStar, you can focus on innovation and do more with less.\n\nCheck out our [documentation website](https://aws.github.io/aws-northstar) for more details.\n\n## Cloudscape Design System and NorthStar v2\n\nOn July 19, 2022, AWS released [Cloudscape Design System](https://cloudscape.design/) as open source. Cloudscape is a solution for building intuitive user experiences. It offers guidelines to create web applications, along with the design resources and front-end components to streamline implementation. \n\nNorthStar v2 (**@aws-northstar/ui**) improves upon the previous version by leveraging [Cloudscape Design System](https://cloudscape.design/) and with updates to the existing components with new features that make the development experience even better.\n\n## NorthStar Legacy\n\nNorthStar legacy (**aws-northstar**), released in October 2020, was built using [Material UI v4](https://v4.mui.com/) as its base and provided approximately 50 components for building prototyping user experience. **This version entered maintenance on April 1, 2023.** \n\nDuring the maintenance phase, NorthStar legacy will only receive critical bug fixes and security patches. New features will be exclusively developed for NorthStar v2. **On April 1, 2024, support will end for NorthStar legacy.** \n\nRefer to [the Migration tabs in the documentation website](https://aws.github.io/aws-northstar/?path=/story/migration-migratingfromlegacy--page) for more information on how to migrate NorthStar legacy to v2.  \n\n## Development\n\nContribution guide are available at the [Contributing Guidelines](https://github.com/aws/aws-northstar/blob/main/CONTRIBUTING.md).\n\n### Folder Structure\n\nThis monorepo hosts source code for both NorthStar legacy and NorthStar v2. \n\n| Path                                  |                                                |\n| ------------------------------------- | ---------------------------------------------- |\n| **packages/legacy**                   | Source code for NorthStar legacy               |\n| **packages/ui**                       | Source code for NorthStar v2                   |\n| **packages/examples/legacy**          | Source code for NorthStar legacy demo app      |\n| **packages/examples/ui**              | Source code for NorthStar v2 demo app          |\n\n### Prerequisites\n\n* [git-secrets](https://github.com/awslabs/git-secrets#installing-git-secrets)\n\n### Commands\n\nIn the project directory, you can run:\n\n#### `yarn storybook`\n\nRuns storybook to navigate all the components on NorthStarv v2. \n\nOpen [http://localhost:6006](http://localhost:6006) to view it in the browser. The page will reload if you make edits.\n\n**It is recommended to use storybook as development environment.**\n\n#### `yarn lint:fix`\n\nFix lint problems automatically\n\n#### `yarn check:all`\n\nCheck all the tests passed, code built, storybook built, documentation built\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n## Changelog\n\nCheck out the [Changelog](https://github.com/aws/aws-northstar/releases)\n\n", "release_dates": ["2023-10-30T23:19:09Z", "2023-09-27T02:09:06Z", "2023-09-22T07:47:56Z", "2023-09-21T02:40:21Z", "2023-09-13T06:47:50Z", "2023-09-12T06:27:16Z", "2023-07-12T08:13:06Z", "2023-06-30T03:17:39Z", "2023-06-29T21:27:50Z", "2023-06-29T08:31:06Z", "2023-06-29T03:46:51Z", "2023-06-23T07:29:45Z", "2023-05-25T07:17:16Z", "2023-05-11T00:55:05Z", "2023-05-11T00:11:46Z", "2023-05-03T05:37:55Z", "2023-05-03T02:39:36Z", "2023-04-24T09:32:22Z", "2023-04-20T03:09:16Z", "2023-03-31T09:56:49Z", "2023-03-31T09:57:01Z", "2023-03-31T07:18:20Z", "2022-10-23T22:39:03Z", "2022-10-20T04:42:51Z", "2022-08-10T23:34:28Z", "2022-07-25T11:07:15Z", "2022-07-19T03:04:37Z", "2022-03-03T05:55:07Z", "2022-03-03T01:28:31Z", "2022-03-01T01:44:06Z"]}, {"name": "aws-ofi-nccl", "description": "This is a plugin which lets EC2 developers use libfabric as network provider while running NCCL applications.", "language": "C", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS OFI NCCL\n\nAWS OFI NCCL is a plug-in which enables EC2 developers to use\n[libfabric](https://github.com/ofiwg/libfabric) as a network provider while\nrunning [NVIDIA's NCCL](https://github.com/NVIDIA/nccl) based applications.\n\n## Overview\n\nMachine learning frameworks running on top of NVIDIA GPUs use a library called\n[NCCL](https://developer.nvidia.com/nccl) which provides standard collective\ncommunication routines for an arbitrary number of GPUs installed across single\nor multiple nodes.\n\nThis project implements a plug-in which maps NCCLs connection-oriented\ntransport APIs to [libfabric's](https://ofiwg.github.io/libfabric/)\nconnection-less reliable interface. This allows NCCL applications to take\nbenefit of libfabric's transport layer services like reliable message support\nand operating system bypass.\n\n## Getting Started\n\nThe best way to build the plugin is to start with the latest [release\npackage](https://github.com/aws/aws-ofi-nccl/releases).  The plugin\ndevelopers highly discourage customers from building directly from the\nHEAD of a GitHub branch, as releases go through more extensive testing\nthan the pre-commit testing on git branches. More information about installing\nthe plugin from a released tarball can be found in [INSTALL.md](INSTALL.md).\n\nVersion numbers that end in `-aws` have only been tested on Amazon Web\nServices Elastic Compute Cloud (EC2) instances and the Elastic Fabric\nAdapter (EFA) network transport.  Customers using other networks may\nexperience unexpected issues with these releases, but we welcome bug\nreports if that is the case.\n\n## Basic Requirements\n\nThe plugin is regularly tested on the following operating systems:\n\n* Amazon Linux 2\n* Ubuntu 20.04 LTS and 22.04 LTS\n\nOther operating systems are likely to work; there is very little\ndistribution-specific code in the plugin.\n\nTo build the plugin, you need to have\n[Libfabric](http://github.com/ofiwg/libfabric/) and\n[HWLOC](https://www.open-mpi.org/projects/hwloc/) installed prior to\nbuilding the plugin., If you want to run the included multi-node\ntests, you also need  an MPI Implementation installed.  Each release of the\nplugin has a list of dependency versions in the top-level README.md\nfile.\n\nThe plugin does not require NCCL to be pre-installed, but obviously a\nNCCL installation is required to use the plugin.  As of NCCL 2.4.8,\nit is possible to use the same plugin build across multiple versions\nof NCCL (such as those installed per-package with Conda-like environments).\n\nMost Libfabric providers should work with the plugin, possibly through\na utility provider.  The plugin generally requires Reliable datagram\nendpoints (`FI_EP_RDM`) with tagged messaging (`FI_TAGGED`, `FI_MSG`).\nThis is similar to the requirements of most MPI implementations and a\ngenerally tested path in Libfabric.  For GPUDirect RDMA support, the\nplugin also requires `FI_HMEM` support, as well as RDMA support.\n\n## Getting Help\n\nIf you have any issues in building or using the package or if you think you may\nhave found a bug, please open an\n[issue](https://github.com/aws/aws-ofi-nccl/issues).\n\n## Contributing\n\nReporting issues and sending pull requests are always welcome. To learn how you\ncan contribute, please look at our\n[contributing guidelines](CONTRIBUTING.md#contributing-guidelines).\n\n## License\n\nThis library is licensed under the [Apache 2.0 License](LICENSE).\n", "release_dates": ["2024-02-25T21:40:21Z", "2024-02-19T17:48:40Z", "2023-12-04T20:44:38Z", "2023-10-05T19:26:30Z", "2023-08-25T17:58:16Z", "2023-07-29T00:31:35Z", "2023-07-25T22:27:38Z", "2023-07-21T22:54:18Z", "2023-04-22T01:29:16Z", "2023-01-26T21:59:04Z", "2022-07-14T02:28:49Z", "2022-07-14T02:27:01Z", "2022-05-12T22:13:40Z", "2022-05-12T22:13:07Z", "2022-03-11T06:28:57Z", "2022-03-11T06:27:27Z", "2022-02-03T22:09:39Z", "2022-02-03T22:08:13Z", "2021-10-26T23:22:50Z", "2021-10-26T23:20:58Z", "2021-06-20T16:35:33Z", "2021-04-14T18:55:47Z", "2021-04-14T18:53:53Z", "2020-11-02T21:54:06Z", "2020-08-22T22:54:40Z", "2020-08-22T01:23:13Z", "2020-05-29T20:51:43Z", "2020-05-29T20:45:55Z", "2020-01-13T20:09:07Z", "2019-02-23T02:01:28Z"]}, {"name": "aws-ops-wheel", "description": "The AWS Ops Wheel is a randomizer that biases for options that haven\u2019t come up recently; you can also outright cheat and specify the next result to be generated.", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Introduction\nThe AWS Ops Wheel is a tool that simulates a random selection from a group of participants that weights away from participants recently chosen. For any group, the selection can also be rigged to suggest a particular participant that will be selected in a blatantly obvious (and sometimes hilarious) way.\n\nGet your own in just a few clicks by starting here: [![Launch the Wheel](https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png)](https://us-west-2.console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/create/review?filter=active&templateURL=https:%2F%2Fs3-us-west-2.amazonaws.com%2Faws-ops-wheel%2Fcloudformation-template.yml&stackName=AWSOpsWheel)\n\nOr, simply set up a CloudFormation stack using the S3 template url: https://s3-us-west-2.amazonaws.com/aws-ops-wheel/cloudformation-template.yml\n\n**We are aware of an [issue](/../../issues/3) where you can only run this stack in us-west-2 if you launch from this template, we are working on removing this limitation. This limitation does not apply if you build your own stack using the Development Guide below.**\n\nThe endpoint will then be in the CloudFormation Stack Output messages.\n\n## ScreenShots\n### Wheels Table\n![Wheels Table](screenshots/wheels_table.png)\n### Participants Table\n![Participants Table](screenshots/participants_table.png)\n### Wheel (pre-spin)\n![Participants Table](screenshots/wheel_pre_spin.png)\n### Wheel (post-spin)\n![Participants Table](screenshots/wheel_post_spin.png)\n\n# User Guide\n## Concepts\n**Wheel**\n  A group of participants that can be selected from. Users can get a suggestion of a participant from a wheel that is weighted away from recently-chosen participants.\n\n**Participant**\n  A member of a wheel identified by a name, which must be unique, and also a follow-through url when they are chosen. Participants all start with a weight of 1.0.\n\n## Operations\n### Wheel Operations\n- Create a new wheel\n- Edit an existing wheel\n- Delete a wheel\n- Spin the wheel and suggest a participant\n  - ***Notes:*** This does not adjust weighting, so if you're unhappy with the result, you can spin again.\n- Proceed: Accept the suggested participant\n- Reset: Restart all participants to equal weights as 1.0\n\n### Participant Operations\n***Notes:*** Participants aren't shared between wheels\n\n- Add a participant to a wheel\n\t- This requires a name and url that will be opened in a new browser tab when the participant is chosen. A participant begin with a weight of 1.0 which will always be the average weight for all participants.\n- Edit a participant's name and/or url\n- Delete a specific participant from the wheel\n- Rig a specific participant to be selected next\n    - This doesn't change any weighting, but actually bypasses the suggestion algorithm to always suggest the participant until told to proceed.\n    - After proceeding, weights are adjusted as if the participant had been selected normally.\n    - The rigging can be hidden (deceptive) or non-hidden (comical).\n\n### Authentication and User management\nAWS Ops Wheel is protected by Amazon Cognito authentication. It uses [Cognito User Pools](http://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html) to manage users that have access to the deployed application.\nBy default, during the initial deployment phase, it creates an `admin` user with a random password that is sent to the email address provided to the `run` script.\nDuring the first attempt to login to the AWS Ops Wheel, the `admin` user will be asked to change the random password to a new one.\n\nIf you need to add more users that have access to the wheel application, you can add them using AWS Cognito web console or using the AWS Cli.\n\n### The Weighting algorithm\n\nAssumption: `total_weight == number_of_participants == len(wheel)`. This is because we only redistribute weights among participants and all participants start with a weight of 1.0. The below is the algorithm in python pseudo-code:\n\n```python\ndef suggest_participant(wheel):\n    target_number =  len(wheel) * random()  # Get a random floating point number between 0 and the total_weight\n    participant = None\n    for participant in wheel:\n        target_number -= participant.weight\n        if target_number <= 0:\n            break\n    return participant\n\ndef select_participant(chosen, wheel):\n    # When there is only one participant in the wheel, the selected participant's weight remains intact.\n    # Otherwise, the remaining participant(s) get a slice of the selected participant's weight. That participant will not be chosen on next spin unless it's rigged.\n\t if len(wheel) > 1:\n\t    weight_slice = chosen.weight / (len(wheel) - 1)\n\t    for participant in wheel:\n\t        if participant == chosen:\n\t            participant.weight = 0\n\t        else:\n\t            participant.weight += weight_slice\n```\n\n# Development Guide\n\n***Notes:*** The development tools are currently only written to be Linux/OSX compatible\n\n## Development Dependencies\n\n- NodeJS 6.10+\n- Python 3\n\t- boto3\n\t- pyaml\n  - pytest\n  - pytest-cov\n  - moto\n- AWSCLI 1.11+\n- An AWS Account you have administrator privileges on\n\n\n## A dedicated IAM User (Optional, but highly-recommended)\n- You should create a dedicated IAM User for ``AWS Ops Wheel`` development\n\n### Create a custom IAM Policy for the User\n\n- Go to the [AWS Create Policy Wizard](https://console.aws.amazon.com/iam/home?region=us-west-2#/policies$new?step=edit)\n- Go to the `JSON` tab and paste in the content of our [policy configuration](https://raw.githubusercontent.com/aws/aws-ops-wheel/master/cloudformation/awsopswheel-create-policy.json)\n- Click `Review Policy`\n- Give it an identifying name (we'll need it for the next step) like *AWSOpsWheelDevelopment*\n\n### Create an IAM user with the policy attached\n\n- Got to the [AWS Create User Wizard](https://console.aws.amazon.com/iam/home?region=us-west-2#/users$new?step=details)\n- Give it a descriptive name like *AWSOpsWheelDevelopmentUser* and check the `Programmatic access` checkbox.  **Note:** It doesn't need to be the same as the name of the policy, but it might help keep your things organized\n- Click `Next: Permissions`\n- Switch to the `Attach existing policies directly` tab and filter on the name you used during the *Create custom Policy* step\n- Click the checkbox next to the policy and click `Next: Review`\n- Click `Create user`\n- On the next page, save the Access key ID and the Secret access key (visible by clicking `Show`) for use in the `AWS Cli Configuration` step.  **Note**: This will be the only opportunity to copy the Secret Access Key for this Access Key ID.  if you don't copy the secret access key now, you'll need to create a new access-key, secret-key pair for the user.\n\n\n## AWS Cli Configuration\nFor the purpose of our work, we will use AWS Cli to simplify management of the resources.\nLater we will add support for the `Launch Stack` button which will be displayed on the GitHub Repo page.\n\nIn `$HOME/.aws/config` add in your credentials configuration and default region, replacing with your IAM user's credentials (or your own access key and secret key if you didn't follow our highly-recommended best-practice).  **Note**: The region can be whatever region you choose, but you should definitely set a default region.  We chose us-west-2 since we're in Seattle and it's close by.\n\n```\n[default]\naws_access_key_id = ACCESS_KEY\naws_secret_access_key = SECRET_KEY\nregion = us-west-2\n```\n\n\n\n## Test the code\n\nCurrently we have unit tests for the API and the UI. \n\nTo run the API unit tests: \n* If you haven't already, go to the ``<PATH_TO_YOUR_WORKSPACE>`` directory and install the required dependencies using:\n  ```\n  pip install -r requirements.txt\n  ```\n* Go to the ``<PATH_TO_YOUR_WORKSPACE>/api`` directory and run:\n  ```\n  pytest --verbose --cov-report term-missing --cov ./ -s\n  ```\n  * If you see this error `NoRegionError: You must specify a region. `, export the region environment variable as follows:\n    `export AWS_DEFAULT_REGION=us-west-2`\n\nTo run the UI unit tests, go to the ``<PATH_TO_YOUR_WORKSPACE>/ui`` directory and run:\n\n```\nnpm run test\n```\n\n## Build and deploy the code\n\nGo to the ``<PATH_TO_YOUR_WORKSPACE>`` directory and run:\n\n```\n$ ./run \\\n  --suffix <SUFFIX, optional with default value as no suffix, so stack name will be 'AWSOpsWheel'> \\\n  --email <EMAIL_ADDRESS, required only during initial stack creation> \\\n  --no-clean <CLEAN_BUILD_DIRECTORY, optional with default value as False. Note that do not clean the build directory before building or remove the deploy working directory>\n```\n\nThis will:\n\n- Create a `./build` directory with all of the build artifacts\n- Package the build artifacts up into a zip file with name based on a hash of the contents and upload it to S3 for lambda deployment\n- Compile the Service CloudFormation Template:\n    - Create the lambda functions for all of the routes in the API\n    - Add policies for lambda functions to be called by the gateway's functions\n    - Create/update the DynamoDB Tables\n    - Create the lambda execution IAM role\n    - Create the swagger configuration for API Gateway that points the paths to their functions\n- Deploy the template directly to CloudFormation through update or create, depending on if it's a new stack\n\n## Start Local Dev Server\nGo to the ``<PATH_TO_YOUR_WORKSPACE>/ui`` directory and run:\n\n```\nnpm run start\n```\n\n# Miscellaneous\n## Import Participant data from .csv file\nTo populate Participant data from .csv file to one of your wheels you can use a tool that is in `utils` folder.\nAll parameters are required.\n\n```\n$ <PATH_TO_YOUR_WORKSPACE>/utils/wheel_feeder.py \\\n  --wheel-url <https://<your_api_gateway>.amazonaws.com> \\\n  --wheel-id <TARGET_WHEEL_ID> \\\n  --csv-file-path <PATH_TO_CSV_FILE> \\\n  --cognito-user-pool-id <COGNITO_USER_POOL_ID> \\\n  --cognito-client-id <COGNITO_CLIENT_ID>\n```\n\n## List Stacks\nTo list all Stacks that are currently provisioned (or have been in the past):\n\n```\n$ aws cloudformation list-stacks\n```\n\n## Delete Stack\n\nTo delete existing stack:\n\n```\n$ aws cloudformation delete-stack [--suffix SUFFIX_NAME]\n```\n\n## Set up continuous deployment\n\nCreate continuous deployment resources:\n```\naws cloudformation create-stack --stack-name AWSOpsWheel --template-body file://cloudformation/continuous-deployment.yml --parameters ParameterKey=AdminEmail,ParameterValue=example@example.com --capabilities CAPABILITY_NAMED_IAM\n\naws cloudformation wait stack-create-complete --stack-name AWSOpsWheel\n```\nMake sure you have your preferred [CodeCommit access](https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up.html) configured.\n\nThe following assumes that you are using the AWS CLI Credential Helper.\n\nPush to the newly created git repository:\n```\ngit config --global credential.helper '!aws codecommit credential-helper $@'\n\ngit config --global credential.UseHttpPath true\n\ngit remote add app `aws cloudformation describe-stacks --stack-name AWSOpsWheel --query 'Stacks[0].Outputs[?OutputKey==\\`RepositoryCloneUrl\\`].OutputValue' --output text`\n\ngit push app master\n```\n\nWait for the pipeline to finish deploying:\n```\naws cloudformation describe-stacks --stack-name AWSOpsWheel --query 'Stacks[0].Outputs[?OutputKey==`PipelineConsoleUrl`].OutputValue' --output text\n```\n\nGet the URL of the newly deployed application:\n```\naws cloudformation describe-stacks --stack-name AWSOpsWheel-application --query 'Stacks[0].Outputs[?OutputKey==`Endpoint`].OutputValue' --output text\n```\n\n## Wheel Customization\nTo change how fast wheels spin, modify `EASE_OUT_FRAMES` and `LINEAR_FRAMES` in `wheel.jsx`. \nLower values correspond to faster spinning.\n", "release_dates": []}, {"name": "aws-panorama-cli", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# PanoramaDevPythonCLI\n\nThe purpose of this package is to provide a CLI tool to facilitate Panorama developer creating a local application as well as being able to upload the application to the cloud using the CLI.\n\n## Dependencies\n\nYou will need Docker and AWS CLI installed on your machine.\nDocker is required for building a package and AWS CLI is needed for downloading a model from S3 and packaging the application to Panorama Cloud.\n\n##### Docker Setup\n\nhttps://docs.docker.com/get-docker/\n\nSince Panorama CLI builds ARM Docker images, it needs these extra steps on Linux to build cross platform images. Installing Docker Desktop on Mac should automatically handle cross platform builds.\n\nOn Debian based distros\n```Shell\n$ sudo apt-get install qemu binfmt-support qemu-user-static\n$ sudo docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n```\n\nOn CentOS/RHEL based distros\n```Shell\n$ sudo yum install qemu binfmt-support qemu-user-static\n$ sudo docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n```\n\n##### AWS CLI Setup\n\nhttps://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\n\nAWS CLI version should be >=2.3.0 for v2 and >=1.21.0 for v1\n\n## Install\n\nPanorama CLI is only supported on Linux and macOS right now.\n\n```Shell\n$ pip3 install panoramacli\n```\n\n## Upgrade\n\nTo upgrade to the latest release\n\n```Shell\n$ pip3 install --upgrade panoramacli\n```\n\n## Commands\n\nBasic structure of the commands is as follows\n\n```Shell\n$ panorama-cli <command> [options and parameters]\n```\n\nTo view help documentation, use one of the following\n\n```Shell\n$ panorama-cli -h\n$ panorama-cli <command> -h\n```\n\n### Deploying a sample application\n\nInstructions for downloading and deploying a sample application can be found at https://docs.aws.amazon.com/panorama/latest/dev/gettingstarted-deploy.html\n\n### Related Github Repositories\n\nDeveloper Guide - https://github.com/awsdocs/aws-panorama-developer-guide\n\nSample Applications - https://github.com/aws-samples/aws-panorama-samples\n\n### Panorama Docs\n\nhttps://docs.aws.amazon.com/panorama/\n\n### Application creation flow example\n\nThis is an example of a sample app which has two node packages. people_counter package has core logic for counting the number of people, call_node has the model which people_counter package uses. We will also add an abstract camera to the application which can be linked to a real camera from the console while deploying the application. \n\n```Shell\n$ panorama-cli init-project --name example_project\nSuccessfully created the project skeleton at <path>\n\n$ cd example_project\n\n$ panorama-cli create-package --name people_counter\n\n$ panorama-cli create-package --name call_node --type Model\n```\n\n#### Application Structure\n\nAt this point, the application structure looks as follows.\n`graph.json` under `graphs` directory lists down all the packages, nodes and edges in this application. Nodes and Edges are the way to define an application graph in Panorama.\n`package.json` in each package has details about the package and the assets it uses. Interface definitions for the package need to be defined in this as well.\nModel package `call-node` has a `descriptor.json` which needs to have the metadata required for compiling the model. More about this in the models section.\nIn `people_counter` package which is the default i.e container type, all the implementation related files go into the `src` directory and `descriptor.json` has details about which command and file to use when the container is launched. More about container package management later.\n`assets` directory is where all the assets reside. Developer is not expected to make any changes in this directory.\n\n```Shell\n\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 graphs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 example_project\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 graph.json\n\u2514\u2500\u2500 packages\n    \u251c\u2500\u2500 accountXYZ-call_node-1.0\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 descriptor.json\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 package.json\n    \u2514\u2500\u2500 accountXYZ-people_counter-1.0\n     \u00a0\u00a0 \u251c\u2500\u2500 Dockerfile\n     \u00a0\u00a0 \u251c\u2500\u2500 descriptor.json\n     \u00a0\u00a0 \u251c\u2500\u2500 package.json\n     \u00a0\u00a0 \u2514\u2500\u2500 src\n```\n\n#### Setting up Cameras for Panorama\n\nPanorama has a concept of Abstract Camera Package which the developers can use while developing their apps. This abstract camera package can be overriden and linked to an actual camera in the developer's Panorama account while deploying.\n\nLet's add an abstract camera to this application by running the following command.\n\n```\n$ panorama-cli add-panorama-package --type camera --name front_door_camera\n```\n\nThis command defines an abstract camera package in the `packages` section and adds the following snippet in the `nodes` section of the `graph.json`. You can modify the title and description to be more relevant to the use case.\n\n```JSON\n{\n    \"name\": \"front_door_camera\",\n    \"interface\": \"panorama::abstract_rtsp_media_source.rtsp_v1_interface\",\n    \"overridable\": true,\n    \"launch\": \"onAppStart\",\n    \"decorator\": {\n        \"title\": \"Camera front_door_camera\",\n        \"description\": \"Default description for camera front_door_camera\"\n    }\n}\n```\n\n`rtsp_v1_interface` is the defined interface for an abstract camera and it has an output port called `video_out` which can be used to forward the camera output to another node. More details about connecting this camera are discussed in the [app graph section below](#defining-interfaces-and-app-graph)\n\n#### Preparing a model for Panorama\n\nRaw models are compiled using Sagemaker Neo on Panorama Cloud before being deployed onto the device. All models for this reason are paired with a `descriptor.json` which has the required meta deta for compiling the raw model on the cloud.\n\nDetails about using Sagemaker Neo to compile models can be found at https://docs.aws.amazon.com/sagemaker/latest/dg/neo-job-compilation.html\n\nAll the model info that is used to compile models on Sagemaker Neo are part of the `descriptor.json`. Values used in this example are specific to the squeezenet model that is being used in this example.\n\nSince call_node has the model in this example, edit `packages/accountXYZ-call-node-1.0/descriptor.json` and add the following snippet into it.\n```JSON\n{\n    \"mlModelDescriptor\": {\n        \"envelopeVersion\": \"2021-01-01\",\n        \"framework\": \"PYTORCH\",\n        \"inputs\": [\n            {\n                \"name\": \"data\",\n                \"shape\": [\n                    1,\n                    3,\n                    224,\n                    224\n                ]\n            }\n        ]\n    }\n}\n```\n\nNow we can add the model by passing in the path to the descriptor file which we just updated.\n\nIf you want to download the model from S3 and then add it pass `--model-s3-uri` as shown below. Otherwise just use `--model-local-path` to pass the local model path instead.\n\n`--packages-path` can be used to pass all the packges where this model is being used and after downloading the model, DX CLI automatically adds the downloaded model into assets section of all the specified packages.\n\n```Shell\n$ panorama-cli add-raw-model --model-asset-name callable_squeezenet --model-s3-uri s3://<s3_bucket_path>/raw_models/squeezenet1_0.tar.gz --descriptor-path packages/accountXYZ-call_node-1.0/descriptor.json --packages-path packages/accountXYZ-call_node-1.0\ndownload: s3://<s3_bucket_path>/squeezenet1_0.tar.gz to assets/callable_squeezenet.tar.gz\nSuccessfully downloaded compiled artifacts (s3://<s3_bucket_path>/squeezenet1_0.tar.gz) to ./assets/callable_squeezenet.tar.gz\n{\n    \"name\": \"callable_squeezenet\",\n    \"implementations\": [\n        {\n            \"type\": \"model\",\n            \"assetUri\": \"fd1aef48acc3350a5c2673adacffab06af54c3f14da6fe4a8be24cac687a386e.tar.gz\",\n            \"descriptorUri\": \"df53d7bc3666089c2f93d23f5b4d168d2f36950de42bd48da5fdcafd9dbac41a.json\"\n        }\n    ]\n}\n```\n\nIf you make any updates to your model or `desriptor.json` file after running this command, just re-run the command with the same `--model-asset-name` and the old asset will be updated with the new assets.\n\n#### Writing code and building a container\n\npeople_counter package has the core logic to count the number of people, so let's create a file called `people_counter_main.py` at `packages/accountXYZ-people_counter-1.0/src` and add the relevant code to that.\nEdit `packages/accountXYZ-people_counter-1.0/descriptor.json` to have the following content\n```JSON\n{\n    \"runtimeDescriptor\":\n    {\n        \"envelopeVersion\": \"2021-01-01\",\n        \"entry\":\n        {\n            \"path\": \"python3\",\n            \"name\": \"/panorama/people_counter_main.py\"\n        }\n    }\n}\n```\ndescriptor.json basically provides the path for the command that needs to run and the path to the file that needs to be executed once the container starts.\n\nWe can now build the package using the following command to create a container asset.\n```Shell\n$ panorama-cli build-container --container-asset-name people_counter_container_binary --package-path packages/accountXYZ-people-counter-package-1.0\n```\n\nIf you make any updates to your code or `desriptor.json` or `Dockerfile` file after building a container, just re-run the command with the same `--container-asset-name` and the old assets will be updated with the new assets.\n\n#### Defining interfaces and app graph\n\nLet's take a look at how the `package.json` looks for people_counter package after running build-container\n\n```JSON\n{\n    \"nodePackage\": {\n        \"envelopeVersion\": \"2021-01-01\",\n        \"name\": \"people_counter\",\n        \"version\": \"1.0\",\n        \"description\": \"Default description for package people_counter\",\n        \"assets\": [\n            {\n                \"name\": \"people_counter_container_binary\",\n                \"implementations\": [\n                    {\n                        \"type\": \"container\",\n                        \"assetUri\": \"4e3a68e5fc0be9b7f3d8540a0a7d9855d6baae0b6dfc280b68431fd90b1e2c90.tar.gz\",\n                        \"descriptorUri\": \"15545511b51d390a0a252537a41719498efd04f707deae17c6618d544e40e996.json\"\n                    }\n                ]\n            }\n        ],\n        \"interfaces\": [\n            {\n                \"name\": \"people_counter_container_binary_interface\",\n                \"category\": \"business_logic\",\n                \"asset\": \"people_counter_container_binary\",\n                \"inputs\": [\n                    {\n                        \"name\": \"video_in\",\n                        \"type\": \"media\"\n                    }\n                ],\n                \"outputs\": [\n                    {\n                        \"name\": \"video_out\",\n                        \"type\": \"media\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n```\n\nA new asset named `people_counter_container_binary` has been added under assets and a new interface named `people_counter_container_binary_interface` has been defined. In Panorama, interfaces are a way to programtically interact with a package and each interface is linked to an asset. For example, `people_counter_container_binary_interface` has an asset field which points to `people_counter_container_binary`. That means that we are defining an interface to that asset. In this case, since our asset is a container with your code in it, all the inputs your code expects can be part of the inputs under interfaces. In this example, the code just expects one input which is a video stream. If output of your code needs to be consumed by another asset, that can be part of the ouputs. Similarly, a new interface was added to the call-node package when we can `add-raw-model` command. In that case, interface was linked to the model asset which we added using that command.\n\nAt this point, `graph.json` under the graphs directory looks like this\n\n```JSON\n{\n    \"nodeGraph\": {\n        \"envelopeVersion\": \"2021-01-01\",\n        \"packages\": [\n            {\n                \"name\": \"accountXYZ::people_counter\",\n                \"version\": \"1.0\"\n            },\n            {\n                \"name\": \"accountXYZ::call_node\",\n                \"version\": \"1.0\"\n            },\n            {\n                \"name\": \"panorama::abstract_rtsp_media_source\",\n                \"version\": \"1.0\"\n            }\n        ],\n        \"nodes\": [\n            {\n                \"name\": \"front_door_camera\",\n                \"interface\": \"panorama::abstract_rtsp_media_source.rtsp_v1_interface\",\n                \"overridable\": true,\n                \"launch\": \"onAppStart\",\n                \"decorator\": {\n                    \"title\": \"Camera front_door_camera\",\n                    \"description\": \"Default description for camera front_door_camera\"\n                }\n            },\n            {\n                \"name\": \"callable_squeezenet\",\n                \"interface\": \"accountXYZ::call_node.callable_squeezenet_interface\"\n            },\n            {\n                \"name\": \"people_counter_container_binary_node\",\n                \"interface\": \"accountXYZ::people_counter.people_counter_container_binary_interface\",\n                \"overridable\": false,\n                \"launch\": \"onAppStart\"\n            }\n        ],\n        \"edges\": []\n    }\n}\n```\n\n`packages` section here has all the packages that are part of this application and we can see that `nodes` section has some nodes defined already. To be able to use any package, we need to define a corresponding nodes in the `graph.json` for all the interfaces that are part of the package. `people_counter_container_binary_node` node is linked to `people_counter_container_binary_interface` interface from people_counter package which we just looked at and similarly `callable_squeezenet` node is linked to `callable_squeezenet_interface` interface from the call_node package. We already discussed the `front_door_camera` node in [setting up cameras section](#setting-up-cameras-for-panorama)\n\n\nNext thing we will do is set up the edges for the application graph. `people_counter_container_binary_interface` had one input `video_in` as part of the interface definition and that was the video input to the code in that package. We can connect that input to the camera node's output by adding the following edge under the `edges` section.\n\n```JSON\n\"edges\": [\n            {\n                \"producer\": \"front_door_camera_node.video_out\",\n                \"consumer\": \"people_counter_container_binary_node.video_in\"\n            },\n        ]\n```\n\n#### Registering and Uploading all local packages in the Cloud\n\nWhen the application is ready, use the following command to upload all the packages to the cloud\n```Shell\n$ panorama-cli package-application\n```\n\n#### Deploying the application\n\nAfter packaging the application, you can now use the graph.json from the package to start a deployment from the cloud!\n\n\n### Panorama Application Concepts\n\n#### Container Package Basics\n\n##### Handling implementation related files\n\nThis is a directory tree of how an example container package. All the implementation related files for this package go into the `src` directory. In this package, `people_counter_main.py` has the logic for processing the frames from the camera and `people_counter_main.py` depends on `blueprint.csv` and `requirements.json` for some of its functionality and therefore those are under the `src` directory as well. If the application requires multiple `.py` files then all those will be under the `src` directory as well.\n\n```Shell\naccountXYZ-people_counter-1.0\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 descriptor.json\n\u251c\u2500\u2500 package.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 blueprint.csv\n    \u251c\u2500\u2500 people_counter_main.py\n    \u2514\u2500\u2500 requirements.json\n```\n\nLet's now take a look at the Dockerfile provided as part of the package\n\n```dockerfile\nFROM public.ecr.aws/panorama/panorama-application\nCOPY src /panorama\n```\n\nIn the second line, we are basically copying all the contents of the `src` directory into the `/panorama` directory of the Docker image. Therefore, its important to note that when `people_counter_main.py` accesses other files which were originally part of the `src` directory, they are actually under `/panorama` when the application is running on the Panorama Appliance.\n\n##### Handling dynamic data\n\nSince all the containers run in read-only mode on the Panorama Appliance, its not possible to create new files at all paths. To facilitate this, Panorama base image(i.e first line in Dockerfile) has two directories `/opt/aws/panorama/logs` and `/opt/aws/panorama/storage` which are empty. During runtime, these directories are mounted to the device file system and allow the developer to store new files and data dynamically.\n\n`/opt/aws/panorama/logs` is the location to store all the logs and all files created in the directory are uploaded to CloudWatch for the account which was used to provision the device.\n`/opt/aws/panorama/storage` is a good location to store all the dynamic info that the application might need.\nWhen the device re-starts, all the memory locations are deleted but the data under these two directories is persistent and therefore should contain all the context for the application to function from where it left off on a reboot.\n\n##### Installing dependencies\n\nThe image(`public.ecr.aws/panorama/panorama-application`) provided by Panorama is a ARM64v8 Ubuntu image with just Panorama base software installed so all the additional dependencies for the application must be installed separately. For example, add the following line to the Dockerfile to install OpenCV and boto3. This Dockerfile fetches the latest `panorama-application` image by default, to use a specific version of the image, refer to https://gallery.ecr.aws/panorama/panorama-application and tag the image in the Dockerfile with the version you're planning to use.\n\n```dockerfile\nFROM public.ecr.aws/panorama/panorama-application\nCOPY src /panorama\nRUN pip3 install opencv-python boto3\n```\n\n#### Overriding Abstract Cameras\n\nWe added an Abstract Camera in [setting up cameras section](#setting-up-cameras-for-panorama). If you're deploying an app through Panorama console, you will be automatically promted to replace the abstract camera node with a data source in your account. This section mentions how to create an `override.json` which can be used to replace abstract camera with a real camera while deploying applications from command line.\n\nCreate a Camera node using the following command. `--output-package-name` is the name of the camera package and `--node-name` specifies the node name under this package. We are keeping the names for both of these as `my_rtsp_camera` to keep it simple. Update `--template-parameters` with the correct Username, Password and StreamUrl.\n\n```Shell\n$ aws panorama create-node-from-template-job --template-type RTSP_CAMERA_STREAM --output-package-name my_rtsp_camera --output-package-version \"1.0\" --node-name my_rtsp_camera --template-parameters '{\"Username\":\"admin\",\"Password\":\"123456\",\"StreamUrl\": \"rtsp://<url>\"}'\n```\n\nThis command will return an output like\n\n```Shell\n{\n    \"JobId\": \"d1d81752-d8ab-4131-8e48-8cf638685e71\"\n}\n```\n\nLet's make sure camera was created successfully by running the following command using the job id from above.\n\n```Shell\n$ aws panorama describe-node-from-template-job --job-id d1d81752-d8ab-4131-8e48-8cf638685e71\n{\n    \"JobId\": \"d1d81752-d8ab-4131-8e48-8cf638685e71\",\n    \"Status\": \"SUCCEEDED\",\n    \"CreatedTime\": \"2021-10-11T13:22:51.284000-07:00\",\n    \"LastUpdatedTime\": \"2021-10-11T13:22:51.284000-07:00\",\n    \"OutputPackageName\": \"my_rtsp_camera\",\n    \"OutputPackageVersion\": \"1.0\",\n    \"NodeName\": \"my_rtsp_camera_node\",\n    \"NodeDescription\": \"my_rtsp_camera_node\",\n    \"TemplateType\": \"RTSP_CAMERA_STREAM\",\n    \"TemplateParameters\": {\n        \"Password\": \"SAVED_AS_SECRET\",\n        \"StreamUrl\": \"rtsp://<url>\",\n        \"Username\": \"SAVED_AS_SECRET\"\n    }\n}\n```\n\nSince the status says succeeded, we are now ready to use this camera. Now, let's create an `override.json` for this camera. Here we are replacing `front_door_camera` node which we created in [setting up cameras section](#setting-up-cameras-for-panorama) with the newly created `my_rtsp_camera`\n\n```JSON\n{\n  \"nodeGraphOverrides\" : {\n    \"envelopeVersion\" : \"2021-01-01\",\n    \"packages\" : [\n        {\n            \"name\" : \"accountXYZ::my_rtsp_camera\",\n            \"version\" : \"1.0\"\n        }\n    ],\n    \"nodes\" : [\n        {\n            \"name\" : \"my_rtsp_camera_node\",\n            \"interface\" : \"accountXYZ::my_rtsp_camera.my_rtsp_camera\",\n            \"overridable\" : true,\n            \"overrideMandatory\" : false,\n            \"launch\" : \"onAppStart\"\n        }\n    ],\n    \"nodeOverrides\" : [\n        {\n            \"replace\" : \"front_door_camera\",\n            \"with\" : [\n                {\n                    \"name\" : \"my_rtsp_camera_node\"\n                }\n            ]\n        }\n    ]\n}\n}\n```\n\n##### Processing streams from multiple cameras together\n\nIf you want to process streams from two or more cameras together, you can also replace the `front_door_camera` node with multiple cameras. For processing two streams together for example, create a second camera using the steps mentioned above and use the following override file.\n\n```JSON\n{\n  \"nodeGraphOverrides\" : {\n    \"envelopeVersion\" : \"2021-01-01\",\n    \"packages\" : [\n        {\n            \"name\" : \"accountXYZ::my_rtsp_camera\",\n            \"version\" : \"1.0\"\n        },\n        {\n            \"name\" : \"accountXYZ::my_other_camera\",\n            \"version\" : \"1.0\"\n        }\n    ],\n    \"nodes\" : [\n        {\n            \"name\" : \"my_rtsp_camera_node\",\n            \"interface\" : \"accountXYZ::my_rtsp_camera.my_rtsp_camera\",\n            \"overridable\" : true,\n            \"overrideMandatory\" : false,\n            \"launch\" : \"onAppStart\"\n        },\n        {\n            \"name\" : \"my_other_camera_node\",\n            \"interface\" : \"accountXYZ::my_other_camera.my_other_camera\",\n            \"overridable\" : true,\n            \"overrideMandatory\" : false,\n            \"launch\" : \"onAppStart\"\n        }\n    ],\n    \"nodeOverrides\" : [\n        {\n            \"replace\" : \"front_door_camera\",\n            \"with\" : [\n                {\n                    \"name\" : \"my_rtsp_camera_node\"\n                },\n                {\n                    \"name\": \"my_other_camera_node\"\n                }\n            ]\n        }\n    ]\n}\n}\n```\n\n#### Using multiple cameras\n\nAt the end of last section, we looked at how to modify the override file we want to process streams from  multiple cameras together. This section speaks about using multiple camera streams in the same application and processing them separately. This is useful when multiple cameras are being used for different purposes.\n\nLet's take the `package.json` of people_counter package from the [defining interfaces section](#defining-interfaces-and-app-graph) and modify it to have two video inputs.\n\n```JSON\n{\n    \"nodePackage\": {\n        \"envelopeVersion\": \"2021-01-01\",\n        \"name\": \"people_counter\",\n        \"version\": \"1.0\",\n        \"description\": \"Default description for package people_counter\",\n        \"assets\": [\n            {\n                \"name\": \"people_counter_container_binary\",\n                \"implementations\": [\n                    {\n                        \"type\": \"container\",\n                        \"assetUri\": \"4e3a68e5fc0be9b7f3d8540a0a7d9855d6baae0b6dfc280b68431fd90b1e2c90.tar.gz\",\n                        \"descriptorUri\": \"15545511b51d390a0a252537a41719498efd04f707deae17c6618d544e40e996.json\"\n                    }\n                ]\n            }\n        ],\n        \"interfaces\": [\n            {\n                \"name\": \"people_counter_container_binary_interface\",\n                \"category\": \"business_logic\",\n                \"asset\": \"people_counter_container_binary\",\n                \"inputs\": [\n                    {\n                        \"name\": \"video_in_1\",\n                        \"type\": \"media\"\n                    },\n                    {\n                        \"name\": \"video_in_2\",\n                        \"type\": \"media\"\n                    }\n                ],\n                \"outputs\": [\n                    {\n                        \"name\": \"video_out\",\n                        \"type\": \"media\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n```\n\nLet's add another camera to the application by using the following command.\n\n```\n$ panorama-cli add-panorama-package --type camera --name back_door_camera\n```\n\nA node named `back_door_camera` will be added into the `nodes` section of `graph.json` and let us connect both the cameras to the video inputs defined above in the `edges` section as shown below.\n\n```JSON\n{\n    \"nodeGraph\": {\n        \"envelopeVersion\": \"2021-01-01\",\n        \"packages\": [\n            {\n                \"name\": \"accountXYZ::people_counter\",\n                \"version\": \"1.0\"\n            },\n            {\n                \"name\": \"accountXYZ::call_node\",\n                \"version\": \"1.0\"\n            },\n            {\n                \"name\": \"panorama::abstract_rtsp_media_source\",\n                \"version\": \"1.0\"\n            },\n        ],\n        \"nodes\": [\n            {\n                \"name\": \"front_door_camera\",\n                \"interface\": \"panorama::abstract_rtsp_media_source.rtsp_v1_interface\",\n                \"overridable\": true,\n                \"launch\": \"onAppStart\",\n                \"decorator\": {\n                    \"title\": \"Camera front_door_camera\",\n                    \"description\": \"Default description for camera front_door_camera\"\n                }\n            },\n            {\n                \"name\": \"callable_squeezenet\",\n                \"interface\": \"accountXYZ::call_node.callable_squeezenet_interface\"\n            },\n            {\n                \"name\": \"people_counter_container_binary_node\",\n                \"interface\": \"accountXYZ::people_counter.people_counter_container_binary_interface\",\n                \"overridable\": false,\n                \"launch\": \"onAppStart\"\n            },\n            {\n                \"name\": \"back_door_camera\",\n                \"interface\": \"panorama::abstract_rtsp_media_source.rtsp_v1_interface\",\n                \"overridable\": true,\n                \"launch\": \"onAppStart\",\n                \"decorator\": {\n                    \"title\": \"Camera back_door_camera\",\n                    \"description\": \"Default description for camera back_door_camera\"\n                }\n            }\n        ],\n        \"edges\": [\n            {\n                \"producer\": \"front_door_camera_node.video_out\",\n                \"consumer\": \"people_counter_container_binary_node.video_in_1\"\n            },\n            {\n                \"producer\": \"back_door_camera_node.video_out\",\n                \"consumer\": \"people_counter_container_binary_node.video_in_2\"\n            },\n        ]\n    }\n}\n```\n\n#### Viewing output on a HDMI\n\nIn this people counter example application, if we also want to draw bounding boxes around people and view those processed frames on a screen, we can do that as well by adding a Data Sink node. Data Sink node forwards the input it receives to the HDMI port.\n\nLike the abstract camera package, Panorama also provides a data sink package and we can create a data_sink using the following command.\n\n```\n$ panorama-cli add-panorama-package --type data_sink --name data_sink_node\n```\n\nThis command adds the following node in the `nodes` section of `graph.json`\n\n```\n{\n    \"name\": \"data_sink_node\",\n    \"interface\": \"panorama::hdmi_data_sink.hdmi0\",\n    \"overridable\": false,\n    \"launch\": \"onAppStart\"\n}\n```\n\nWe can now connect this `data_sink_node` to the output of `people_counter_container_binary_node` in the `edges` section. At this point, `graph.json` looks as follows.\n\n```JSON\n{\n    \"nodeGraph\": {\n        \"envelopeVersion\": \"2021-01-01\",\n        \"packages\": [\n            {\n                \"name\": \"accountXYZ::people_counter\",\n                \"version\": \"1.0\"\n            },\n            {\n                \"name\": \"accountXYZ::call_node\",\n                \"version\": \"1.0\"\n            },\n            {\n                \"name\": \"panorama::abstract_rtsp_media_source\",\n                \"version\": \"1.0\"\n            },\n            {\n                \"name\": \"panorama::hdmi_data_sink\",\n                \"version\": \"1.0\"\n            }\n        ],\n        \"nodes\": [\n            {\n                \"name\": \"front_door_camera\",\n                \"interface\": \"panorama::abstract_rtsp_media_source.rtsp_v1_interface\",\n                \"overridable\": true,\n                \"launch\": \"onAppStart\",\n                \"decorator\": {\n                    \"title\": \"Camera front_door_camera\",\n                    \"description\": \"Default description for camera front_door_camera\"\n                }\n            },\n            {\n                \"name\": \"callable_squeezenet\",\n                \"interface\": \"accountXYZ::call_node.callable_squeezenet_interface\"\n            },\n            {\n                \"name\": \"people_counter_container_binary_node\",\n                \"interface\": \"accountXYZ::people_counter.people_counter_container_binary_interface\",\n                \"overridable\": false,\n                \"launch\": \"onAppStart\"\n            },\n            {\n                \"name\": \"data_sink_node\",\n                \"interface\": \"panorama::hdmi_data_sink.hdmi0\",\n                \"overridable\": false,\n                \"launch\": \"onAppStart\"\n            }\n        ],\n        \"edges\": [\n            {\n                \"producer\": \"front_door_camera_node.video_out\",\n                \"consumer\": \"people_counter_container_binary_node.video_in\"\n            },\n            {\n                \"producer\": \"people_counter_container_binary_node.video_out\",\n                \"consumer\": \"data_sink_node.video_in\"\n            }\n        ]\n    }\n}\n```\n", "release_dates": []}, {"name": "aws-parallelcluster", "description": "AWS ParallelCluster is an AWS supported Open Source cluster management tool to deploy and manage HPC clusters in the AWS cloud.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "AWS ParallelCluster - HPC for the Cloud\n=======================================\n\n[![PyPI Version](https://img.shields.io/pypi/v/aws-parallelcluster)](https://pypi.org/project/aws-parallelcluster/)\n[![Spack Version](https://img.shields.io/spack/v/aws-parallelcluster)](https://spack.readthedocs.io/en/latest/package_list.html#aws-parallelcluster)\n[![Conda Verseion](https://img.shields.io/conda/vn/conda-forge/aws-parallelcluster)](https://anaconda.org/conda-forge/aws-parallelcluster)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![codecov](https://codecov.io/gh/aws/aws-parallelcluster/branch/develop/graph/badge.svg)](https://codecov.io/gh/aws/aws-parallelcluster)\n[![ParallelCluster CI](https://github.com/aws/aws-parallelcluster/workflows/ParallelCluster%20CI/badge.svg)](https://github.com/aws/aws-parallelcluster/actions)\n\nAWS ParallelCluster is an AWS supported Open Source cluster management tool that makes it easy for you to deploy and\nmanage High Performance Computing (HPC) clusters in the AWS cloud.\nBuilt on the Open Source CfnCluster project, AWS ParallelCluster enables you to quickly build an HPC compute environment in AWS.\nIt automatically sets up the required compute resources and a shared filesystem and offers a variety of batch schedulers such as AWS Batch and Slurm.\nAWS ParallelCluster facilitates both quick start proof of concepts (POCs) and production deployments.\nYou can build higher level workflows, such as a Genomics portal that automates the entire DNA sequencing workflow, on top of AWS ParallelCluster.\n\nQuick Start\n-----------\n**IMPORTANT**: you will need an **Amazon EC2 Key Pair** to be able to complete the following steps.\nPlease see the [Official AWS Guide](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html).\n\nFirst, prepare a Python Virtual Environment for ParallelCluster, note ParallelCluster >= 3.0.0 requires Python >= 3.7.\n```\npython3 -m pip install --upgrade pip\npython3 -m pip install --user --upgrade virtualenv\npython3 -m virtualenv ~/hpc-ve\nsource ~/hpc-ve/bin/activate\n```\n\nMake sure you have installed the [AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html):\n\n```\n$ pip3 install awscli\n```\n\n[Node.js](https://nodejs.org/en/) is required by AWS CDK library used by ParallelCluster.\nPlease reference installation instructions [in the AWS CDK documentaton](https://docs.aws.amazon.com/cdk/v2/guide/work-with.html#work-with-prerequisites).\n\nThen you can install AWS ParallelCluster:\n\n```\n$ pip3 install aws-parallelcluster\n```\n\nNext, configure your aws credentials and default region:\n\n```\n$ aws configure\nAWS Access Key ID [None]: YOUR_KEY\nAWS Secret Access Key [None]: YOUR_SECRET\nDefault region name [us-east-1]:\nDefault output format [None]:\n```\n\nThen, run ``pcluster configure``. A list of valid options will be displayed for each\nconfiguration parameter. Type an option number and press ``Enter`` to select a specific option,\nor just press ``Enter`` to accept the default option.\n\n```\n$ pcluster configure --config /dir/cluster-config.yaml\nINFO: Configuration file /dir/cluster-config.yaml will be written.\nPress CTRL-C to interrupt the procedure.\n\n\nAllowed values for AWS Region ID:\n1. eu-north-1\n...\n15. us-west-1\n16. us-west-2\nAWS Region ID [us-east-1]:\n```\n\nBe sure to select a region containing the EC2 key pair you wish to use. You can also import a public key using\n[these instructions](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#how-to-generate-your-own-key-and-import-it-to-aws).\n\nDuring the process you will be asked to set up your networking environment. The wizard will offer you the choice of\nusing an existing VPC or creating a new one on the fly.\n\n```\nAutomate VPC creation? (y/n) [n]:\n```\n\nEnter ``n`` if you already have a VPC suitable for the cluster. Otherwise you can let ``pcluster configure``\ncreate a VPC for you. The same choice is given for the subnet: you can select a valid subnet ID for\nboth the head node and compute nodes, or you can let ``pcluster configure`` set up everything for you.\nThe same choice is given for the subnet configuration: you can select a valid subnet ID for both\nthe head node and compute nodes, or you can let pcluster configure set up everything for you.\nIn the latter case, just select the configuration you prefer.\n\n```\nAutomate Subnet creation? (y/n) [y]: y\nAllowed values for Network Configuration:\n1. Head node in a public subnet and compute fleet in a private subnet\n2. Head node and compute fleet in the same public subnet\n```\n\n\nAt the end of the process a message like this one will be shown:\n\n```\nConfiguration file written to /dir/conf_file\nYou can edit your configuration file or simply run 'pcluster create-cluster --cluster-name cluster-name --cluster-configuration /dir/cluster-config.yaml' to create your cluster.\n```\n\n\nNow you can create your first cluster:\n\n```\n$ pcluster create-cluster --cluster-name myfirstcluster --cluster-configuration /dir/cluster-config.yaml\n```\n\n\nAfter the cluster finishes creating, log in:\n\n```\n$ pcluster ssh --cluster-name myfirstcluster\n```\n\nYou can view the running compute hosts:\n\n```\n$ sinfo\n```\n\nFor more information on any of these steps see the [Getting Started Guide](https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3.html).\n\nDocumentation\n-------------\n\nWe've been working hard to greatly improve the [Documentation](https://docs.aws.amazon.com/parallelcluster/latest/ug/), it's now published in 10 languages, one of the many benefits of being hosted on AWS Docs. Of most interest to new users is\nthe [Getting Started Guide](https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3.html).\n\nIf you have changes you would like to see in the docs, please either submit feedback using the feedback link at the bottom\nof each page or create an issue or pull request for the project at:\nhttps://github.com/awsdocs/aws-parallelcluster-user-guide.\n\nIssues\n------\n\n[![GitHub issues](https://img.shields.io/github/issues/aws/aws-parallelcluster.svg)](https://github.com/aws/aws-parallelcluster/issues)\n[![GitHub closed issues](https://img.shields.io/github/issues-closed-raw/aws/aws-parallelcluster.svg)](https://github.com/aws-parallelcluster/issues?q=is%3Aissue+is%3Aclosed)\n\nPlease open a GitHub issue for any feedback or issues:\nhttps://github.com/aws/aws-parallelcluster/issues.  There is also an active AWS\nHPC forum which may be helpful: https://repost.aws/tags/TAbl-DsTlyQMe0T2i-d5Rr8g/aws-parallel-cluster.\n\nChanges\n-------\n\n### CfnCluster to AWS ParallelCluster\nIn Version `2.0.0`, we changed the name of CfnCluster to AWS ParallelCluster. With that name change we released several new features, which you can read about in the [Change Log](https://github.com/aws/aws-parallelcluster/blob/develop/CHANGELOG.md#200).\n", "release_dates": ["2023-12-19T17:40:23Z", "2023-10-13T19:36:53Z", "2023-09-22T20:15:10Z", "2023-08-30T12:11:03Z", "2023-07-05T14:22:06Z", "2023-05-22T15:51:39Z", "2023-03-28T20:11:55Z", "2023-02-20T11:50:13Z", "2023-01-13T07:38:34Z", "2022-12-22T14:10:12Z", "2022-12-02T12:18:30Z", "2022-12-03T00:49:00Z", "2022-11-15T01:36:29Z", "2022-11-16T13:54:19Z", "2022-11-02T15:06:12Z", "2022-10-03T08:59:38Z", "2022-07-27T17:48:42Z", "2022-05-13T16:46:17Z", "2022-05-16T19:57:36Z", "2022-04-19T13:27:31Z", "2022-04-20T15:38:20Z", "2022-03-01T18:29:17Z", "2022-03-02T14:40:20Z", "2022-02-10T19:01:53Z", "2022-01-17T13:49:58Z", "2021-12-20T17:02:32Z", "2021-11-05T18:24:50Z", "2021-11-03T17:56:35Z", "2021-10-27T14:23:30Z", "2021-09-10T15:51:35Z"]}, {"name": "aws-parallelcluster-cookbook", "description": "The Chef cookbook used to build and bootstrap AWS ParallelCluster", "language": "Ruby", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "AWS ParallelCluster Cookbook\n============================\n\n[![codecov](https://codecov.io/gh/aws/aws-parallelcluster-cookbook/branch/develop/graph/badge.svg)](https://codecov.io/gh/aws/aws-parallelcluster-cookbook)\n[![Build Status](https://github.com/aws/aws-parallelcluster-cookbook/actions/workflows/ci.yml/badge.svg?event=push)](https://github.com/aws/aws-parallelcluster-cookbook/actions)\n\nThis repo contains the AWS ParallelCluster Chef cookbook used in AWS ParallelCluster.\n\n# Code structure\n\nThe root folder of the cookbook repository can be considered the main cookbook, since it contains two files: `Berksfile` and `metadata.rb`.\nThese files are used by the CLI (build image time) and `user-data.sh` code to [vendor](https://docs.chef.io/workstation/berkshelf/#berks-vendor)\nthe other sub-cookbooks and third party cookbooks.\n\nThe main cookbook does not contain any recipe, attribute or library. They are distributed in the functional cookbooks under the `cookbooks` folder\ndefined as follows:\n- `aws-parallelcluster-entrypoints` is the cookbook to define the external interface, contains recipes called by AMI builder, cluster setup, cluster update, etc. \n  The recipes in this cookbook are called directly by the CLI or CI/CD. It orchestrates invocation of recipes/resources from other cookbooks;\n- `aws-parallelcluster-platform` OS packages and system configuration (directories, users, services, drivers);\n- `aws-parallelcluster-environment` AWS services configuration and usage, such as shared file systems, directory service and network interfaces;\n- `aws-parallelcluster-computefleet` slurm specific scaling logic, compute fleet scripts and daemons;\n- `aws-parallelcluster-awsbatch` files required to support AWS Batch as scheduler;\n- `aws-parallelcluster-slurm` files required to support Slurm as a scheduler and its dependencies (Munge, MySQL for accounting, etc), it depends by `aws-parallelcluster-computefleet`;\n\nFinally, some common code, such as source/script directories, usernames, package installer etc., are located in a cookbook \nwhich every other cookbook depend on, that is `aws-parallelcluster-shared`.\nEach cookbook hosts recipes and resources, attributes, functions, files and templates belonging to its functional area.\n\nEvery cookbook contains ChefSpec and Kitchen tests for its code.\nHowever, the code in a cookbook might require that some other code from a different cookbook not listed among dependencies,\nto be executed as a prerequisite (test setup phase). For this reason `aws-parallelcluster-tests` cookbook must depend on every other cookbook.\n\nThe `test` folder contains Python unit tests files and Kitchen [environment](https://docs.chef.io/environments/) files.\n\nThe `kitchen` folder contains utility files used when running Inspec tests locally.\n\nThe `cookbooks/third-party` folder contains cookbooks from marketplace. They must be regularly updated and should not be modified by hand.\nThey have been pre-downloaded and stored in our repository to avoid contacting Chef Marketplace at AMI build time and cluster creation.\nYou can find more information about them in the `cookbooks/third-party/THIRD-PARTY-LICENSES.txt` file.\n\n# Development\n\n## About ChefSpec Tests\n\n[ChefSpec](https://github.com/chefspec/chefspec) is a unit testing framework for testing Chef cookbooks. \nIt is very fast, and we use it to verify recipes with multiple branches (e.g. HeadNode vs ComputeNode) work as expected.\nThey don't need virtual machines or cloud servers. They can be executed locally by executing:\n\n```\ncd cookbooks/aws-parallelcluster-platform\n# run all the ChefSpec tests in a cookbook\nchef exec rspec\n# run a specific ChefSpec test\nchef exec rspec ./spec/unit/recipes/sudo_config_spec.rb \n```\n\nThey are automatically executed as GitHub actions, see definition in `.github/ci.yml`.\n\n## About Kitchen Tests\n\nKitchen is used to automatically test cookbooks across any combination of platforms and test suites.\nIt requires cinc-workstation to be installed on your environment:\n\n`brew install --cask cinc-workstation` on MacOS\n\nor\n\n`curl -L https://omnitruck.cinc.sh/install.sh | sudo bash -s -- -P cinc-workstation -v 23`\n\nMake sure you have set a locale in your local shell environment, by exporting the `LC_ALL` and `LANG` variables, \nfor example by adding to your `.bashrc` or `.zshrc` the following and sourcing the file:\n\n```\nexport LC_ALL=en_US.UTF-8\nexport LANG=en_US.UTF-8\n```\n\nTo speed up the transfer of files when kitchens are run on ec2 instances,\nthe [transport](https://docs.chef.io/workstation/config_yml_kitchen/#transport-settings) selected is [kitchen-transport-speedy](https://github.com/criteo/kitchen-transport-speedy).\nTo install `kitchen-transport-speedy` in the kitchen embedded ruby environment please run: `/opt/cinc-workstation/embedded/bin/gem install kitchen-transport-speedy`.\n\nIn order to test on docker containers, you also need `docker` installed on your environment.\nPlease note that not all the tests can run on docker so in any case we need to validate our recipes on EC2.\nYou can use `on_docker?` condition to skip execution of some recipes steps, resource actions or controls execution on docker.\nPlease look at \"Known issues with docker\" section of the README for specific issues (e.g. when running kitchen tests on non `amd64` architectures).\n\n### Kitchen tests helpers\n\n`kitchen.docker.sh` and `kitchen.ec2.sh` help you run kitchen tests virtually without any further environment setup.\nThey are wrappers of the `kitchen` command, so you can pass them all the options exposed by it. See `kitchen --help` for more details. \n\nYou must do some initial setup on your AWS account in order to be able to use defaults from `kitchen.ec2.sh`. \nDefault values are the following. Take a look at comments at the top of the script in order to understand how to use it.\n\n```\n: \"${KITCHEN_AWS_REGION:=${AWS_DEFAULT_REGION:-eu-west-1}}\"\n: \"${KITCHEN_KEY_NAME:=kitchen}\"\n: \"${KITCHEN_SSH_KEY_PATH:=\"~/.ssh/${KITCHEN_KEY_NAME}-${KITCHEN_AWS_REGION}.pem\"}\"\n: \"${KITCHEN_AVAILABILITY_ZONE:=a}\"\n: \"${KITCHEN_ARCHITECTURE:=x86_64}\"\n```\n\nBoth scripts can be run as follows:\n\n```\nkitchen.<ec2|docker>.sh <context> <kitchen parameters>\n```\n\n`<context>` is your test context, like `environment-config` or `platform-install`.\nFor example `./kitchen.docker.sh platform-install test nvidia` will execute `kitchen test` command, executing all the\ntests for which the name starts with `nvidia` prefix, in `cookbooks/aws-parallelcluster-platform` directory on docker.\n\nIt is important to keep in mind that the parameter after the kitchen action is a pattern,\nso it's important to choose the appropriate naming for kitchen tests suites.\nFor instance, we can use `nvidia-<context>` for nvidia-related tests, so that they can be run separately or together.\nHowever, we should not have `nvidia` and `nvidia-something` tests, as we wouldn't be able to run only the first one on all OSes.\n\nExamples of submission are:\n```\n# Run supervisord kitchen test from file kitchen.platform-install.yml in cookbooks/aws-parallelcluster-platform directory,\n# for all OSes (concurrency 5) and log level debug\n# Note that in this case \"supervisord\" is a pattern, so all the tests starting with \"supervisord\" string in that yaml file will be executed.\n./kitchen.docker.sh platform-install test supervisord -c 5 -l debug\n\n# Run converge phase only of kitchen from file kitchen.environment-config.yml in cookbooks/aws-parallelcluster-environment directory, for alinux2 only.\n# This is useful when you want to test recipe execution only.\n# Once you have executed the converge step, you can for example execute multiple times the verify step, to validate the tests you are writing.\n./kitchen.ec2.sh environment-config converge efa-alinux2\n\n# Run verify phase only from file kitchen.platform-config.yml in cookbooks/aws-parallelcluster-platform directory,\n# useful if you're modifing the test logic without touching the recipes code.\n./kitchen.ec2.sh platform-config verify sudo -c 5\n\n# Login to the instance created with the converge step\n./kitchen.ec2.sh platform-config login sudo-alinux2\n```\n\nA context must have the format `$subject-$phase`. \n\nSupported phases are:\n- `install` (on EC2 it defaults to a bare base AMI)\n- `config` (on EC2 it defaults to a ParallelCluster official AMI)\n\nIt will use `kitchen.${context}.yml` in the specific cookbook, i.e. in `cookbooks/aws-parallelcluster-$subject` dir.\n\nYou can override default values by setting environment variables in a `.kitchen.env.sh` file to be created in the cookbook root folder.\nExample of `.kitchen.env.sh` file:\n\n```\nexport KITCHEN_KEY_NAME=your-key  # ED25519 key type (required for Ubuntu 22)\nexport KITCHEN_SSH_KEY_PATH=/path/your-key.pem\nexport KITCHEN_AWS_REGION=eu-west-1\nexport KITCHEN_SUBNET_ID=subnet-xxx\nexport KITCHEN_SECURITY_GROUP_ID=sg-your-group\nexport KITCHEN_INSTANCE_TYPE=t2.large\nexport KITCHEN_IAM_PROFILE=test-kitchen  # required for tests with lifecycle hooks\n```\n\n### Kitchen tests definition\n\nThe different `kitchen.${context}.yml` files in the functional cookbooks contain a list of Inspec tests\nfor the different recipes and resources. \n\nEvery test specifies:\n- the `run_list` that is the list of recipes to be executed as preparatory steps as part of the `kitchen converge` phase:\n  - the `recipe[aws-parallelcluster-tests::setup]` is a utility recipe that should be added to every test to prepare the environment\n    and automatically execute resources and recipes listed as dependencies in the `dependencies` attributes.\n  - the `recipe[aws-parallelcluster-tests::test_resource]` is a utility recipe to simplify testing of the custom resource defined in the\n    `resource` attribute. Please check `test_resource` content to see which parameters you can pass to it.\n- the `verifier` with the list of controls to execute as part of the `kitchen verify` phase, it's possible to use regex here,\n  it can accept regular expressions in format `/regex/`.\n- the node `attributes` that will be propagated in the test environment.\n  - `resource` is a reserved attribute, used by `test_resource` recipe mentioned before.\n  - `dependencies` is a reserved attribute, used by `setup` recipe mentioned before.\n  - `cluster` structure permits to pass specific parameters to the test to simulate environment condition\n    (i.e. dna.json configuration that should come from the CLI when executing the recipes in a real cluster)\n\nExample of test definition:\n```\n- name: system_authentication\n  run_list:\n    - recipe[aws-parallelcluster-tests::setup]\n    - recipe[aws-parallelcluster-tests::test_resource]\n  verifier:\n    controls:\n      - /tag:config_system_authentication/\n  attributes:\n    resource: system_authentication:configure\n    dependencies:\n      - resource:system_authentication:setup\n    cluster:\n      directory_service:\n        enabled: \"true\"\n      node_type: HeadNode\n```\n\nWhen you execute a test like this with `kitchen test` command it will execute the recipes or resources actions specified in the `run_list`,\nincluding `dependencies`, will set `cluster` attributes in the environment and at the end will execute the `verify` steps by executing\nthe listed `controls`. \nThe `kitchen test` command will execute all the steps and will destroy the instance at the end. If you want to preserve the instance\nyou can execute the step one by one, check `kitchen help` for more details.\n\n\n### Kitchen tests as GitHub actions and as part of CI/CD \n\nAs you can see in the `.github/workflows/dokken-system-tests.yml` we are executing both install and config recipes as GitHub actions. \nWe execute install steps in the `Kitchen Test Install` (to simulate AMI build) and then re-using the container to validate the config steps, \nin the `Kitchen Test Config`.\n\nIn our daily CI/CD we build an AMI (calling the `aws-parallelcluster-entrypoints::install` recipe) and then execute kitchen tests on top of it.\n\nBoth CI/CD and GitHub actions use the `kitchen.validate-config.yml` file in the root folder to validate the config steps.\nIf you look at it, you can see it runs all the `inspec_tests` from all the cookbooks by executing the \ncontrols matching the `/tag:config/` regex.\n```\nverifier:\n  inspec_tests:\n    - cookbooks/aws-parallelcluster-awsbatch/test\n    - cookbooks/aws-parallelcluster-platform/test\n    - cookbooks/aws-parallelcluster-environment/test\n    - cookbooks/aws-parallelcluster-computefleet/test\n    ...\n  controls:\n    - /tag:config/\n```\n\nThis means that if you want a specific control to be executed as part of the CI/CD or GitHub action you should\nuse `tag:config_` as prefix in the control name.\n\nNote that not all the Inspec tests can run on GitHub and on the CI/CD because, as you can see in the `kitchen.validate-config.yml`,\nin this case the `run_list` is defined as follows:\n```\n_run_list: &_run_list\n  - recipe[aws-parallelcluster-tests::setup]\n  - recipe[aws-parallelcluster-entrypoints::init]\n  - recipe[aws-parallelcluster-entrypoints::config]\n  - recipe[aws-parallelcluster-entrypoints::finalize]\n  - recipe[aws-parallelcluster-tests::tear_down]\n```\nwithout any attribute specified, so this could not match with the `run_list` or the `attributes` specified in the test definition.\nSo before adding the `tag:config_` prefix to a control name, please be sure that it can run even without specific setup.\n\nIf you want to execute some kitchen tests at the end of the build-image process, to validate that a created AMI\ncontains what we expect, you can use the `tag:install_` as prefix in the control name. They will be automatically executed by \nImage builder at the end of the build-image process (search for `tag:install_` in the CLI code to understand the details behind this mechanism).\nPlease note that if this test fails the build of the image will fail as well.\n\nIf you want to execute some kitchen tests as part of validate phase the build-image process without causing the build image to fail\nyou can use the `tag:testami_` as prefix. These tests will be executed when the image has already been created\n(search for `tag:testami_` in the CLI code to understand the details behind this mechanism).\n\nPlease note that a test suite name can contain multiple tags (for instance, `tag:install_tag:config_`), the code search for them\nwith a regex so it's not required to have them as prefix.\n\n### Save and reuse Docker image\n\nWhen you set the environment variable `KITCHEN_SAVE_IMAGE=true`, a successful `kitchen verify` phase will lead to \nthe Docker image being committed with the tag `pcluster-${PHASE}/${INSTANCE_NAME}`.\n\nFor instance, if you successfully run\n```\n./kitchen.docker.sh platform-install test directories-alinux2\n```\nan image with tag `pcluster-install/directories-alinux2:latest` will be saved.\n\nTo use it in a later Kitchen test, `export KITCHEN_${PLATFORM}_IMAGE=<your_image>`.\n\nFor instance, to reuse the image from the example above, set `KITCHEN_ALINUX2_IMAGE=pcluster-install/directories-alinux2`.\n\nWe are using this approach to re-use the docker image created by the `Kitchen Test Install` in the following `Kitchen Test Config` phase\nas part of the GitHub actions.\n\n\n### Save and reuse EC2 image\n\nThe procedure described above also applies to EC2, with minor differences.\n\n1. To keep the EC2 instance running while the image is being cooked, refrain from using `kitchen test` \n   or `kitchen destroy` commands. Opt for `kitchen verify` and destroy the instance once the AMI is ready.\n2. Set `KITCHEN_${PLATFORM}_AMI=<ami_id>` to reuse the AMI.\n   For instance, `KITCHEN_ALINUX2_AMI=ami-nnnnnnnnnnnnn`.\n\nThis is useful when you need a long list of dependencies to be installed in the AMI (e.g. Slurm recipes) to verify configuration steps. \n\n### Kitchen lifecycle hooks\n\nKitchen [lifecycle hooks](https://kitchen.ci/docs/reference/lifecycle-hooks/) allow running commands \nbefore and/or after any phase of Kitchen tests (create, converge, verify, or destroy).\n\nWe leverage this feature in Kitchen tests to create/destroy AWS resources (see `kitchen.global.yaml` file. \n\nFor each phase, a generic run script executes custom \n`${THIS_DIR}/${KITCHEN_COOKBOOK_PATH}/test/hooks/${KITCHEN_PHASE}/${KITCHEN_SUITE_NAME}/${KITCHEN_HOOK}.sh` script, if it exists.\n\n__Example.__ \n\n`network_interfaces` Kitchen test suite in the `aws-parallelcluster-environment` cookbook requires a network interface to be attached to the node. \n- `cookbooks/aws-parallelcluster-environment/test/hooks/config/network_interfaces/post_create.sh`: creates ENI and attaches it to the instance\n- `cookbooks/aws-parallelcluster-environment/test/hooks/config/network_interfaces/pre_destroy.sh`: detaches and deletes ENI.\n\n### Use variables from lifecycle hooks as resource properties\n\nIn the `kitchen.global.yaml` we're configuring an [environment](https://docs.chef.io/environments/).\n\nIn the environment file (i.e. `test/environments/kitchen.rb`), for every value to pass and for every OS, \nyou have to define a line like: `'<suite_name>-<variable_name>/<platform>' => 'placeholder'`. For instance:\n```\ndefault_attributes 'kitchen_hooks' => {\n  'ebs_mount-vol_array/alinux2' => 'placeholder',\n  ...\n}\n```\n\nThese environment variables will be available to the kitchen tests as node attributes:\n`node['kitchen_hooks']['ebs_mount-vol_array/alinux2']`. \n\nTo permit to use these environment variables as parameters attributes you have to use th `FROM-HOOK`\nkeyword in the test suite definition.\ne.g. `resource: 'manage_ebs:mount {\"shared_dir_array\" : [\"shared_dir\"], \"vol_array\" : \"FROM_HOOK-<suite_name>-<variable_name>\"}'`\n\nThis value will be automatically replaced, searching for the `<suite_name>-<variable_name>/<platform>` in the environment.\nYou can find all the details of this mechanism in the `test_resource.rb`.\n\nNote: the value of the property to be replaced must be a string even if it's an array.\nIt's up to the post_create script to define an array in the environment.\n\n### Known issues with docker\n\n#### Running kitchen tests on non `amd64` architectures\n\nRunning locally kitchen tests on system with CPU architecture other than `amd64` (i.e. Apple Silicon that have `arm64`)\nmay run in a known **dokken** issue (tracked as https://github.com/test-kitchen/kitchen-dokken/issues/288).\n\nAll tests will fail with messages containing errors such as:\n\n```\n[qemu-x86_64: Could not open '/lib64/ld-linux-x86-64.so.2](https://stackoverflow.com/questions/71040681/qemu-x86-64-could-not-open-lib64-ld-linux-x86-64-so-2-no-such-file-or-direc)\n```\n\nTo work around the issue, please ensure that the `cinc-workstation` version is `>= 23`, as it's the first one that has a\ndokken version that features platform support.\n\nProviding the correct platform configuration in `./kitchen.docker.yml` :\n\n```\n---\ndriver:\n  name: dokken\n  platform: linux/amd64\n  pull_platform_image: false # Use the local images, prevent pull of docker images from Docker Hub,\n  chef_version: 18 # Chef version aligned with the one used to build the images\n  chef_image: cincproject/cinc\n...\n```\n\nis required but not enough if images for different CPU architectures already are present in the local docker cache.\nLocal images of different architectures should be removed in order to work around the issue, then in subsequent\nexecutions dokken will pull the ones for the specified platform and use those, since there are no other than those for\nthe correct architecture available locally.\n\nHere are some examples to clean up local docker containers and images:\n\n```\n# removes running containers that may have been left dangling by previous\n# executions of <your test prefix> test\ndocker rm \\\n  $(docker container stop \\\n    $(docker container ls -q --filter name='<your test prefix>*'))\n\n# remove images from offending <your test prefix>\n# you may want also to remove all dokken images\n# (and safely remove all images, since subsequent executions will pull the\n# required ones)\ndocker rmi \\\n  $(docker images --format '{{.Repository}}:{{.Tag}}' \\\n  | grep '<your test prefix>')\n```\n\n#### kitchen tests fail in `docker_config_creds` with NPE\n\n**dokken** expects that `~/.docker/config.json` contains an `\"auths\"` key, fails in `docker_config_creds` with NPE\notherwise, this issue is tracked in upstream as: https://github.com/test-kitchen/kitchen-dokken/issues/290\n\n### Known issues with EC2\n\n#### Ubuntu22\n\nOn Ubuntu22, `kitchen create` keeps trying to connect to the instance via ssh indefinitely.\nIf you interrupt it and try to run `kitchen verify`, you see authentication failures. \n\nThis happens because Ubuntu22 does not accept authentication via RSA key. You need to re-create a key pair \nusing `ED25519` key type.\n\n### Known issues with Berks\n\n#### Kitchen doesn't see your changes\n\nIf Kitchen doesn't detect your changes, try\n```\nberks shelf uninstall ${COOKBOOK_NAME}\n```\n\n## About python tests\n\nPython tests are configured in `tox.ini` file, including paths to python files.\nIf you move python files around, you need to fix python path accordingly.\n", "release_dates": ["2023-12-19T17:40:31Z", "2023-10-13T19:37:21Z", "2023-09-22T20:15:43Z", "2023-08-30T12:11:16Z", "2023-07-05T14:22:19Z", "2023-05-22T15:51:47Z", "2023-03-28T20:12:50Z", "2023-02-20T11:50:16Z", "2023-01-13T07:38:49Z", "2022-12-22T14:10:29Z", "2022-12-02T12:18:37Z", "2022-12-03T00:49:12Z", "2022-11-16T13:57:33Z", "2022-11-15T01:36:47Z", "2022-11-02T15:06:47Z", "2022-10-03T09:00:10Z", "2022-07-27T17:49:05Z", "2022-05-16T19:57:51Z", "2022-05-13T16:46:45Z", "2022-04-20T15:38:32Z", "2022-04-19T13:27:44Z", "2022-03-02T14:41:09Z", "2022-03-01T18:29:25Z", "2022-02-10T19:02:05Z", "2022-01-17T13:50:20Z", "2021-12-20T17:02:14Z", "2021-11-05T18:27:17Z", "2021-11-03T17:57:00Z", "2021-10-27T14:24:52Z", "2021-09-10T15:52:04Z"]}, {"name": "aws-parallelcluster-node", "description": "aws-parallelcluster-node is the python package installed on the Amazon EC2 instances launched as part of AWS ParallelCluster", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "AWS ParallelCluster Node\n========================\n\n[![PyPI Version](https://img.shields.io/pypi/v/aws-parallelcluster-node)](https://pypi.org/project/aws-parallelcluster-node/)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![codecov](https://codecov.io/gh/aws/aws-parallelcluster-node/branch/develop/graph/badge.svg)](https://codecov.io/gh/aws/aws-parallelcluster-node)\n[![ParallelCluster CI](https://github.com/aws/aws-parallelcluster-node/workflows/ParallelCluster%20CI/badge.svg)](https://github.com/aws/aws-parallelcluster-node/actions)\n\nThis repo contains the aws-parallelcluster-node package installed on the Amazon EC2 instances launched\nas part of AWS ParallelCluster.", "release_dates": ["2023-12-19T17:40:37Z", "2023-10-13T19:37:47Z", "2023-09-22T20:16:00Z", "2023-08-30T12:11:31Z", "2023-07-05T14:22:34Z", "2023-05-22T15:51:53Z", "2023-03-28T20:13:36Z", "2023-02-20T11:50:19Z", "2023-01-13T07:39:00Z", "2022-12-22T14:10:48Z", "2022-12-03T00:49:26Z", "2022-12-02T12:18:44Z", "2022-11-16T13:58:24Z", "2022-11-15T01:36:58Z", "2022-11-02T15:07:12Z", "2022-10-03T09:00:25Z", "2022-07-27T17:49:32Z", "2022-05-16T19:58:38Z", "2022-05-13T16:46:53Z", "2022-04-19T13:28:01Z", "2022-04-20T15:38:38Z", "2022-03-01T18:29:34Z", "2022-03-02T14:41:29Z", "2022-02-10T19:02:16Z", "2022-01-17T13:50:39Z", "2021-12-20T17:01:56Z", "2021-11-05T18:28:12Z", "2021-11-03T17:57:17Z", "2021-10-27T14:25:30Z", "2021-09-10T15:53:02Z"]}, {"name": "aws-parallelcluster-ui", "description": null, "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "AWS ParallelCluster UI\n================================\nThis project is a front-end for [AWS ParallelCluster](https://github.com/aws/aws-parallelcluster)\n\nQuickly and easily create HPC cluster in AWS using AWS ParallelCluster UI. This UI uses the AWS ParallelCluster 3.x API to Create, Update and Delete Clusters as well as access, view logs, and build Amazon Machine Images (AMI's).\n\n## Install\nSee [Official documentation](https://docs.aws.amazon.com/parallelcluster/latest/ug/install-pcui-v3.html) to install ParallelCluster UI.\n## Development\n\nSee [Development guide](DEVELOPMENT.md) to setup a local environment.\n\n## Security\n\nSee [Security Issue Notifications](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## Contributing\n\nPlease refer to our [Contributing Guidelines](CONTRIBUTING.md) before reporting bugs or feature requests.\n\nPlease refer to our [Project Guidelines](PROJECT_GUIDELINES.md) before diving into the code.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n[![PCUI ADRs](https://aws.github.io/aws-parallelcluster-ui/log4brains/badge.svg)](https://aws.github.io/aws-parallelcluster-ui/log4brains/)\n", "release_dates": ["2024-02-08T21:16:55Z", "2023-12-21T12:06:28Z", "2023-10-24T08:54:01Z", "2023-06-07T08:11:10Z", "2023-05-16T12:19:44Z", "2023-04-17T15:19:19Z", "2023-02-13T15:08:38Z"]}, {"name": "aws-pdk", "description": "The AWS PDK provides building blocks for common patterns together with development tools to manage and build your projects.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<a href=\"https://aws.github.io/aws-pdk/getting_started/migration_guide.html\"><img src=\"docs/content/assets/images/rebrand_banner.png\" width=\"1024px\"></a>\n\n# Getting started\n\n## What is the AWS PDK?\n\nThe AWS Project Development Kit (AWS PDK) provides building blocks for common patterns together with development tools to manage and build your projects.\n\nThe AWS PDK lets you define your projects programatically via the expressive power of type safe constructs available in one of 3 languages (typescript, python or java). This approach yields many benefits, including:\n\n- Ability to set up new projects within seconds, with all boilerplate already pre-configured.\n- Receive updates to previously bootstrapped projects when new versions become available i.e: updated dependenies or lint configurations.\n- Build polyglot monorepos, with build caching, cross-language build dependencies, dependency visualization and much more.\n- Leverage codified patterns which vend project and infrastructure (CDK) code.\n\nThe AWS PDK is built on top of [Projen](https://github.com/projen/projen) and as such all constructs that you compose together need to be defined via a [projenrc](https://projen.io/programmatic-api.html) file.\n\n## Why use the AWS PDK?\n\nIt's much easier to show than explain! Here is some PDK code (within projenrc file) that creates a Polyglot monorepo, with a React Website pre-configured with Cognito Auth and pre-integrated with a Smithy Type Safe Api.\n\n```ts\nimport { CloudscapeReactTsWebsiteProject } from \"@aws/pdk/cloudscape-react-ts-website\";\nimport { InfrastructureTsProject } from \"@aws/pdk/infrastructure\";\nimport { MonorepoTsProject } from \"@aws/pdk/monorepo\";\nimport {\n    DocumentationFormat,\n    Language,\n    Library,\n    ModelLanguage,\n    TypeSafeApiProject,\n} from \"@aws/pdk/type-safe-api\";\nimport { javascript } from \"projen\";\n\nconst monorepo = new MonorepoTsProject({\n    name: \"my-project\",\n    packageManager: javascript.NodePackageManager.PNPM,\n    projenrcTs: true,\n});\n\nconst api = new TypeSafeApiProject({\n    parent: monorepo,\n    outdir: \"packages/api\",\n    name: \"myapi\",\n    infrastructure: {\n        language: Language.TYPESCRIPT,\n    },\n    model: {\n        language: ModelLanguage.SMITHY,\n        options: {\n        smithy: {\n            serviceName: {\n            namespace: \"com.aws\",\n            serviceName: \"MyApi\",\n            },\n        },\n        },\n    },\n    runtime: {\n        languages: [Language.TYPESCRIPT],\n    },\n    documentation: {\n        formats: [DocumentationFormat.HTML_REDOC],\n    },\n    library: {\n        libraries: [Library.TYPESCRIPT_REACT_QUERY_HOOKS],\n    },\n    handlers: {\n        languages: [Language.TYPESCRIPT],\n    },\n});\n\nconst website = new CloudscapeReactTsWebsiteProject({\n    parent: monorepo,\n    outdir: \"packages/website\",\n    name: \"website\",\n    typeSafeApi: api,\n});\n\nnew InfrastructureTsProject({\n    parent: monorepo,\n    outdir: \"packages/infra\",\n    name: \"infra\",\n    cloudscapeReactTsWebsite: website,\n    typeSafeApi: api,\n});\n\nmonorepo.synth();\n```\n\nThis code (also available in Python and Java), produces all the source code, packages and infrastructure needed to deploy a fully-operable application in the AWS cloud. All that's left to do is build and deploy it!\n\nFrom this ~70 lines of code above, the AWS PDK produces the following packages on your behalf:\n\n- `monorepo`: Root level project that manages interdependencies between projects within the Monorepo, provides build caching and dependency visualziation.\n- `api/model`: A project that allows you to define your API using Smithy (or OpenAPI) IDL.\n- `api/generated/documentation`: A project that automatically creates API documentation in a variety of formats.\n- `api/generated/infrastructure`: A project that automatically creates API infrastructure constructs in a type-safe manner.\n- `api/generated/libraries`: A project that automatically generates a react hooks library that can be used to call your API from a React based website.\n- `api/generated/runtime`: A project that contains server bindings for handlers to ensure type safety.\n- `api/handlers`: A project that automatically creates handler stubs, preconfigured with type-safety and a variety of value added features based on your defined API's.\n- `website`: A project which creates a React based website built using [Cloudscape](https://cloudscape.design/) that comes pre-integrated with Cognito Auth and your created API. This provides you with the ability to call your API securely.\n- `infra`: A project which sets up all CDK related infrastructure needed to deploy your application. It also comes pre-configured to generate a diagram based on your CDK code everytime you build.\n\n### Bootstrapped Source\n\n<img src=\"docs/content/assets/images/boilerplate_source.png\" width=\"800\" />\n\n### Generated Website\n\n<img src=\"docs/content/assets/images/website_screenshot.png\" width=\"800\" />\n\n### Generated Diagram\n\n<img src=\"docs/content/assets/images/generated_diagram.png\" width=\"800\" />\n\nAs you can see, the AWS PDK provides you with valuable time savings so you can focus on working on what matters most to your project.\n\n## Developing with the AWS PDK\n\nPlease refer to the full documentation website.\n\nhttps://aws.github.io/aws-pdk\n\n## Contributing to the AWS PDK\n\nhttps://aws.github.io/aws-pdk/contributing/index.html\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2024-02-27T01:54:29Z", "2024-02-23T01:06:28Z", "2024-02-22T10:06:06Z", "2024-02-15T01:50:07Z", "2024-02-13T01:18:20Z", "2024-02-09T03:24:07Z", "2024-02-01T04:40:54Z", "2024-02-01T03:59:22Z", "2024-01-30T00:29:26Z", "2024-01-25T03:37:59Z", "2024-01-23T07:09:10Z", "2024-01-23T01:01:12Z", "2024-01-17T00:31:03Z", "2024-01-10T02:58:42Z", "2023-12-28T02:09:42Z", "2023-12-19T01:14:58Z", "2023-12-15T00:37:20Z", "2023-12-11T06:07:08Z", "2023-12-05T00:48:25Z", "2023-11-30T04:35:41Z", "2023-11-17T06:02:09Z", "2023-11-14T02:43:00Z", "2023-11-10T03:12:38Z", "2023-11-10T01:56:44Z", "2023-11-08T06:38:36Z", "2023-11-08T01:15:38Z", "2023-11-06T05:44:29Z", "2023-11-06T03:37:54Z", "2023-11-06T02:48:59Z", "2023-11-06T00:20:11Z"]}, {"name": "aws-php-sns-message-validator", "description": "Amazon SNS message validation for PHP", "language": "PHP", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon SNS Message Validator for PHP\n\n[![@awsforphp on Twitter](http://img.shields.io/badge/twitter-%40awsforphp-blue.svg?style=flat)](https://twitter.com/awsforphp)\n[![Total Downloads](https://img.shields.io/packagist/dt/aws/aws-php-sns-message-validator.svg?style=flat)](https://packagist.org/packages/aws/aws-php-sns-message-validator)\n[![Build Status](https://img.shields.io/travis/aws/aws-php-sns-message-validator.svg?style=flat)](https://travis-ci.org/aws/aws-php-sns-message-validator)\n[![Apache 2 License](https://img.shields.io/packagist/l/aws/aws-php-sns-message-validator.svg?style=flat)](http://aws.amazon.com/apache-2-0/)\n\nThe **Amazon SNS Message Validator for PHP** library allows you to validate that\nincoming HTTP(S) POST messages are valid Amazon SNS notifications. This library\nis standalone and does not depend on the AWS SDK for PHP or Guzzle; however, it\ndoes require PHP 5.4+ and that the OpenSSL PHP extension is installed.\n\nJump To:\n* [Basic Usage](_#Basic-Usage_)\n* [Installation](_#Installation_)\n* [About Amazon SNS](_#About-Amazon-SNS_)\n* [Handling Messages](_#Handling-Messages_)\n* [Testing Locally](_#Testing-Locally_)\n* [Contributing](_#Contributing_)\n\n## Basic Usage\n\nTo validate a message, you can instantiate a `Message` object from the POST\ndata using the `Message::fromRawPostData`. This reads the raw POST data from\nthe [`php://input` stream][php-input], decodes the JSON data, and validates\nthe message's type and structure.\n\nNext, you must create an instance of `MessageValidator`, and then use either\nthe `isValid()` or `validate()`, methods to validate the message. The\nmessage validator checks the `SigningCertURL`, `SignatureVersion`, and\n`Signature` to make sure they are valid and consistent with the message data.\n\n```php\n<?php\n\nrequire 'vendor/autoload.php';\n\nuse Aws\\Sns\\Message;\nuse Aws\\Sns\\MessageValidator;\n \n$message = Message::fromRawPostData();\n \n// Validate the message\n$validator = new MessageValidator();\nif ($validator->isValid($message)) {\n   // do something with the message\n}\n```\n\n## Installation\n\nThe SNS Message Validator can be installed via [Composer][].\n\n    $ composer require aws/aws-php-sns-message-validator\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues for tracking bugs and feature requests and have limited bandwidth to address them.\n\n* Ask a question on [StackOverflow](https://stackoverflow.com/) and tag it with [`aws-php-sdk`](http://stackoverflow.com/questions/tagged/aws-php-sdk)\n* Come join the AWS SDK for PHP [gitter](https://gitter.im/aws/aws-sdk-php)\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home/)\n* If it turns out that you may have found a bug, please [open an issue](https://github.com/aws/aws-php-sns-message-validator/issues/new/choose)\n\n## About Amazon SNS\n\n[Amazon Simple Notification Service (Amazon SNS)][sns] is a fast, fully-managed,\npush messaging service. Amazon SNS can deliver messages to email, mobile devices\n(i.e., SMS; iOS, Android and FireOS push notifications), Amazon SQS queues,and\n\u2014 of course \u2014 HTTP/HTTPS endpoints.\n\nWith Amazon SNS, you can setup topics to publish custom messages to subscribed\nendpoints. However, SNS messages are used by many of the other AWS services to\ncommunicate information asynchronously about your AWS resources. Some examples\ninclude:\n\n* Configuring Amazon Glacier to notify you when a retrieval job is complete.\n* Configuring AWS CloudTrail to notify you when a new log file has been written.\n* Configuring Amazon Elastic Transcoder to notify you when a transcoding job\n  changes status (e.g., from \"Progressing\" to \"Complete\")\n\nThough you can certainly subscribe your email address to receive SNS messages\nfrom service events like these, your inbox would fill up rather quickly. There\nis great power, however, in being able to subscribe an HTTP/HTTPS endpoint to\nreceive the messages. This allows you to program webhooks for your applications\nto easily respond to various events.\n\n## Handling Messages\n\n### Confirming a Subscription to a Topic\n\nIn order to handle a `SubscriptionConfirmation` message, you must use the\n`SubscribeURL` value in the incoming message:\n\n```php\nuse Aws\\Sns\\Message;\nuse Aws\\Sns\\MessageValidator;\nuse Aws\\Sns\\Exception\\InvalidSnsMessageException;\n\n// Instantiate the Message and Validator\n$message = Message::fromRawPostData();\n$validator = new MessageValidator();\n\n// Validate the message and log errors if invalid.\ntry {\n   $validator->validate($message);\n} catch (InvalidSnsMessageException $e) {\n   // Pretend we're not here if the message is invalid.\n   http_response_code(404);\n   error_log('SNS Message Validation Error: ' . $e->getMessage());\n   die();\n}\n\n// Check the type of the message and handle the subscription.\nif ($message['Type'] === 'SubscriptionConfirmation') {\n   // Confirm the subscription by sending a GET request to the SubscribeURL\n   file_get_contents($message['SubscribeURL']);\n}\n```\n\n### Receiving a Notification\n\nTo receive a notification, use the same code as the preceding example, but\ncheck for the `Notification` message type.\n\n```php\nif ($message['Type'] === 'Notification') {\n   // Do whatever you want with the message body and data.\n   echo $message['MessageId'] . ': ' . $message['Message'] . \"\\n\";\n}\n```\n\nThe message body will be a string, and will hold whatever data was published\nto the SNS topic.\n\n### Unsubscribing\n\nUnsubscribing looks the same as subscribing, except the message type will be\n`UnsubscribeConfirmation`.\n\n```php\nif ($message['Type'] === 'UnsubscribeConfirmation') {\n    // Unsubscribed in error? You can resubscribe by visiting the endpoint\n    // provided as the message's SubscribeURL field.\n    file_get_contents($message['SubscribeURL']);\n}\n```\n\n## Testing Locally\n\nOne challenge of using webhooks in a web application is testing the integration\nwith the service. Testing integrations with SNS notifications can be fairly easy\nusing tools like [ngrok][] and [PHP's built-in webserver][php-server]. One of\nour blog posts, [*Testing Webhooks Locally for Amazon SNS*][blogpost], illustrates\na good technique for testing.\n\n> **NOTE:** The code samples in the blog post are specific to the message\n> validator in Version 2 of the SDK, but can be easily adapted to using this\n> version.\n\n### Special Thank You\n\nA special thanks goes out to [Julian Vidal][] who helped create the [initial\nimplementation][] in Version 2 of the [AWS SDK for PHP][].\n\n[php-input]: http://php.net/manual/en/wrappers.php.php#wrappers.php.input\n[composer]: https://getcomposer.org/\n[source code]: https://github.com/aws/aws-php-sns-message-validator/archive/master.zip\n[sns]: http://aws.amazon.com/sns/\n[ngrok]: https://ngrok.com/\n[php-server]: http://www.php.net/manual/en/features.commandline.webserver.php\n[blogpost]: http://blogs.aws.amazon.com/php/post/Tx2CO24DVG9CAK0/Testing-Webhooks-Locally-for-Amazon-SNS\n[Julian Vidal]: https://github.com/poisa\n[initial implementation]: https://github.com/aws/aws-sdk-php/tree/2.8/src/Aws/Sns/MessageValidator\n[AWS SDK for PHP]: https://github.com/aws/aws-sdk-php\n\n## Contributing\n\nWe work hard to provide a high-quality and useful SDK for our AWS services, and we greatly value feedback and contributions from our community. Please review our [contributing guidelines](./CONTRIBUTING.md) before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.", "release_dates": ["2023-12-19T17:04:34Z", "2022-08-22T19:51:42Z", "2022-04-22T19:29:03Z", "2019-11-08T16:47:08Z", "2018-07-04T00:38:25Z", "2017-09-27T23:08:10Z", "2017-08-29T17:48:08Z", "2017-07-12T21:44:58Z", "2015-09-03T21:12:46Z", "2015-07-06T22:55:26Z", "2015-06-30T23:09:53Z"]}, {"name": "aws-proton-public-roadmap", "description": "This is the public roadmap for AWS Proton", "language": null, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## AWS Proton\n\nThis is the public roadmap for AWS Proton.\n\nAWS Proton is an application delivery service, available at aws.amazon.com/proton\n\n\n## Introduction\n\nThis is the public roadmap for AWS Proton. We want to use this roadmap to help our customers understand upcoming features that can be tried and give feedback on them.\n\nWe are interested in all kinds of feedback on our upcoming features. Please comment on whether or not each idea would help start using AWS Proton in your organization and standardize infrastructure.\n\nThis is an experimental public roadmap for a new service. We will continue to improve how we manage and organize this roadmap over time, based on input from customers that engage in it. If you have any suggestion on how to improve this process, please open an issue with your comments!\n\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n\n## FAQs\n**Q: Why did you create this?**  \nWe want to make sure that we continue to build and deliver features that are important for our customers. We have a large roadmap and we want to rely on customer input to prioritize the features, as well as to design and deliver them in a way that will work\n\n**Q: What is AWS Proton?**  \nAWS Proton is an application delivery service. See aws.amazon.com/proton.\n\n**Q: Why are there no dates on your roadmap?**  \nBecause job zero is security and operational stability, we can't provide specific target dates for features.\n\n**Q: Will you build everything here?**  \nWe intend to investigate and priorotize all features here. We can't commit to delivering any feature or in any specific timeline.\n\n**Q: Is this roadmap prioritized?**  \nNo. Any item here is being investigated or worked on, but we can't commit to a priority list at this point.\n\n**Q: How can I provide feedback or ask for more information?**  \nPlease open an issue!\n\n**Q: How can I request a feature be added to the roadmap?**  \nPlease open an issue!  You can read about how to contribute [here](/CONTRIBUTING.md). Community submitted issues will be tagged \"Proposed\" and will be reviewed by the team.\n\n**Q: What does each of the column mean?**\n* \"Researching\" means that we have received a request and are at some stage of finding out more about it - whether it is understanding more of the problem, deciding if this is something we can solve, looking for a chance to prioritize it, or defining the product experience for it. Note that items in \"researching\" could be there for a relatively long time as we make our way through analyzing them\n* \"Working on it\" means that we are in active development, and we have an internal release date for it (even though we can't publizice it). Some of these are small features that will get done soon, some of them are larger projects and will ake a while. This column also includes features in the technical design phase\n* \"Just shipped\" means that it was part of a recent release, somewhere in the last few months\n* \"Past releases\" includes all elements of the public roadmap that shipped beyond 1-2 months ago\n\n\n## License Summary\n\nThe documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.\n\nThe sample code within this documentation is made available under the MIT-0 license. See the LICENSE-SAMPLECODE file.\n", "release_dates": []}, {"name": "aws-refcocog-adv", "description": null, "language": "Jupyter Notebook", "license": {"key": "cc-by-4.0", "name": "Creative Commons Attribution 4.0 International", "spdx_id": "CC-BY-4.0", "url": "https://api.github.com/licenses/cc-by-4.0", "node_id": "MDc6TGljZW5zZTI1"}, "readme": "##RefCOCOg-Adv: A Referring Expresion Dataset\n\nThis folder contains adversarial annotations for part of the test images from [RefCOCOg dataset](http://bvisionweb1.cs.unc.edu/licheng/referit/data/refcocog.zip).\n\n\n##Prepare Images:\nDownload \"mscoco\" into the ``Images`` folder, which can be from [mscoco](http://mscoco.org/dataset/#overview)\nBounding box annotations are also from mscoco dataset\n\n\n##Dataset Summary:\n- In RefCOCOg-adv dataset we have have 976 unique images from RefCOCOg test dataset. In total 3704 referring expressions were annotated with an average length of 11.3.\n\n#Example Annotation of RefCOCOG-adv:\nrefcocog_adv_annotations.json contains a dict of advesarial referential expressions, where each annotation is\n\n```\n{\n    'shuffle': 'to giraffes in hat next red white fence over shirt a a a two and woman leaning', \n    'hit_id': '3K3G488TR2FROXACUQOSDKLYTK65Q8', \n    'hit_type_id': '3QH037L2CMDD4RRYVNJWX0CT0B0AWC', \n    'adj_noun': 'giraffes white hat shirt woman next fence red', \n    'GT_bbox_number': '3', \n    'most_confused_bbox_GT_desc': 'A woman in a red shirt, feeding giraffes through a fence.', \n    'raw_str': 'two giraffes leaning over a fence next to a woman in a red shirt and white hat.', \n    'noun_adj': ' giraffes fence woman red shirt white', \n    'batch_id': 'batch18', \n    'bbox_candidates_map': '{\"1\": [373.0, 78.04, 266.56, 348.96], \"2\": [0.96, 0.24, 294.58, 398.21], \"3\": [62.37, 0.14, 329.79, 292.73], \"4\": [455.08, 177.19, 50.35, 235.29]}', \n    'adj': 'next red white', \n    'most_confused_bbox_number': '1', \n    'assigns': [{'dropdownselect': '2'}, {'dropdownselect': '1'}, {'dropdownselect': '1'}], \n    'must_words': 'giraffes,fence,woman,red,shirt,white', \n    'bbox_html_dropdown': \"<option value='0'>Select the Box here</option> <option value='1'>1</option> <option value='2'>2</option> <option value='3'>3</option> <option value='4'>4</option> \", \n    'shuffle_pos': 'two shirt leaning over a fence white to a giraffes in a next woman red hat', \n    'refID': '64809', \n    'stage': 'stage3', \n    'noun': 'giraffes fence woman shirt hat', \n    'imgID': '254291', \n    'annID': '595839', \n    'img_file_name': 'COCO_train2014_000000254291.jpg'\n}\n```\n\n#From the above example\n```\nshuffle, adj_noun, noun_adj, noun, must_words, shuffle_pos are paramters used to perturb data for our adversarial data creation\nraw_str and GT_bbox_number are from RefCOCOg dataset\nbbox_candidates: from mscoco\nmost_confused_bbox_GT_desc: from our annotation\n```\n\n\n## License\n\nThis library is licensed under the CC-BY-4.0 License.\n\n\n", "release_dates": []}, {"name": "aws-rfdk", "description": "The Render Farm Deployment Kit on AWS is a library for use with the AWS Cloud Development Kit that helps you define your render farm cloud infrastructure as code.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Render Farm Deployment Kit on AWS (RFDK)\n\nThe Render Farm Deployment Kit on AWS is an open-source library for use with the\n[AWS Cloud Development Kit](https://github.com/aws/aws-cdk) that is designed to help you\ndeploy, configure, and maintain your render farm infrastructure in the cloud.\n\nIt offers high-level object-oriented abstractions to define render farm infrastructure\nusing the power of Python and Typescript.\n\nThe RFDK is available in:\n- Javascript, Typescript ([Node.js >= 18.0.0](https://nodejs.org/download/release/latest-v18.x/) officially supported, [Node.js >= 14.15.0](https://nodejs.org/download/release/latest-v14.x/) unofficially supported)\n  - We recommend using an [Active LTS Release](https://nodejs.org/en/about/releases/)\n- Python ([Python >= 3.6](https://www.python.org/downloads/))\n\nNote: Language version compatibility is the greater of those listed above and\nthe versions listed in the [AWS CDK](https://github.com/aws/aws-cdk/blob/master/README.md).\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to the RFDK are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to the RFDK can be found\n[in CONTRIBUTING.md](https://github.com/aws/aws-rfdk/blob/release/CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": ["2023-12-22T18:14:50Z", "2023-03-29T16:14:37Z", "2022-12-16T15:39:07Z", "2022-08-30T18:45:16Z", "2022-06-20T18:47:27Z", "2022-04-01T17:30:40Z", "2022-01-06T19:44:36Z", "2021-11-22T18:08:02Z", "2021-10-25T21:51:57Z", "2021-08-05T22:17:50Z", "2021-07-09T21:44:22Z", "2021-06-18T15:37:49Z", "2021-06-16T20:24:20Z", "2021-06-01T02:24:18Z", "2021-05-18T03:58:29Z", "2021-05-11T21:54:26Z", "2021-04-21T20:55:58Z", "2021-04-06T14:42:50Z", "2021-03-25T22:17:34Z", "2021-03-15T16:46:52Z", "2021-03-01T22:36:51Z", "2021-01-29T03:04:54Z", "2021-01-26T15:46:23Z", "2021-01-08T22:00:24Z", "2020-12-16T15:21:43Z", "2020-11-30T15:28:04Z", "2020-11-12T19:02:44Z", "2020-10-29T17:22:51Z", "2020-10-16T21:24:00Z", "2020-10-13T16:31:08Z"]}, {"name": "aws-rndr-engine-for-openssl", "description": "A Random Number Generation Engine for OpenSSL making use of the Arm instruction RNDR.", "language": "C", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS RNDR Engine for OpenSSL\nA Random Number Generation Engine for OpenSSL making use of the Arm instruction\nRNDR.\n\n## Build Requirements\n* CMake\n* OpenSSL Development files (ie openssl-devel/libssl-dev)\n  * The development files' versions must match the version of OpenSSL which\n    will run the engine.\n* C compiler\n* OpenSSL\n* Perl\n* Target Host of Arm 64 CPU\n\n## Test and Run Requirements\n* OpenSSL\n* Host running on Arm 64 CPU which has access to RNDR and RNDRRS instructions\n\n## Installation\nRun once:\n```\nmkdir build\ncd build\ncmake ../\n```\n\n### Quick Install\n```\nmake\nmake install\n```\n\n### Configuring Build\n```\ncmake ../\n```\nRun `cmake --help` for details regarding configuration options.\n\nSome useful configuration options are:\n```\n  -DCMAKE_INSTALL_PREFIX=DIR    install library to specified directory prefix\n  -DCMAKE_INSTALL_LIBDIR=DIR    install library to specified directory\n  -DOPENSSL_ROOT_DIR=DIR        set destination for OpenSSL root directory\n  -DCMAKE_C_FLAGS=FLAGS         set additional CFLAGS for compilation\n```\n\n<details>\n    <summary>Installing to a non-default engine location</summary>\n\nEngine libraries (`eng_rndr.so`) are installed by default to\n`${CMAKE_INSTALL_LIBDIR}` where `${CMAKE_INSTALL_LIBDIR}` usually refers to\n`/usr/local/lib/`. This location can be overwritten in the\nconfigurations using `-DCMAKE_INSTALL_PREFIX=DIR`.\n\ni.e. To install the engine library to `/usr/lib/aarch64-linux-gnu/engines-1.1/`\n```\ncmake -DCMAKE_INSTALL_PREFIX=/usr/lib/aarch64-linux-gnu/engines-1.1/ ../\n```\n</details>\n### Make\n```\nmake\n```\nGenerated shared library files `libeng_rndr.so*` will be located in `./build`.\n\n### Testing\nVerify that random number generation functions for the engine work.\n```\nmake test ARGS=\"-V\"\n```\nThe output will generate test run messages.\n```\nLoading test...\nRunning 'sanity_check_rndr_bytes'...\nTest succeeded\nRunning 'sanity_check_rndrrs_bytes'...\nTest succeeded\n```\nTest the engine built successfully and can be installed\n```\nopenssl engine -t -c src/.libs/libeng_rndr.so\n```\nThis will generate the engines details and availability.\n```\n(eng_rndr) Arm RNDR engine\nLoaded: (rndr) Arm RNDR engine\n [RAND]\n     [ available ]\n```\nTest random number generating using the engine.\n```\nopenssl rand -engine build/libeng_rndr.so -hex 10\n```\nThis will display the randomly generated 10 hex numbers.\n```\nengine \"rndr\" set.\n01c1269d93d9f01ebff4\n```\n\n### Installation\nInstallation may require root privileges. To install, run:\n```\nmake install\n```\n<details>\n    <summary>Environment Variable</summary>\n\nSet `export OPENSSL_ENGINES=INSTALLATION_DIR` environment variable in shell\nstartup files. This will allow openssl to find the RNDR engine.\n\nIf using OpenSSL 1.0.2, the engine will be called `eng_rndr`. If using OpenSSL\n1.1.1 or above the engine will be called `libeng_rndr`.\n\nVerify installation works\n`openssl engine -t -c libeng_rndr`\n```\n...\n(rndr) Arm RNDR engine\n [RAND]\n     [ available ]\n```\n</details>\n\n<details>\n    <summary>Dynamic Engine Installation</summary>\n\nAfter installing update `openssl.cnf` to contain the following.\n\n```\nopenssl_conf = openssl_def\n\n[openssl_def]\nengines = engine_section\n\n[engine_section]\neng_rndr = eng_rndr_section\n\n[eng_rndr_section]\nengine_id = libeng_rndr\ndynamic_path = <PATH TO INSTALLED libeng_rndr.so>\ninit = 0\n```\n\nVerify installation works\n`openssl engine -t -c`\n```\n...\n(rndr) Arm RNDR engine\n [RAND]\n     [ available ]\n```\n</details>\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": []}, {"name": "aws-s3-accessgrants-plugin-java-v2", "description": null, "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS S3 ACCESS GRANTS PLUGIN FOR AWS JAVA SDK 2.0\n\nAWS S3 ACCESS GRANTS PLUGIN provides the functionality to enable S3 customers to configure S3 ACCESS GRANTS as a permission layer on top of the S3 Clients.\n\nS3 ACCESS GRANTS is a feature from S3 that allows its customers to configure fine-grained access permissions for the data in their buckets.\n\n### Things to Know\n\n---\n\n* AWS SDK Java 2.0 is built on Java 8\n* [maven] is used as the build and dependency management system\n\n### Contributions\n\n---\n* Use [GitHub flow](https://docs.github.com/en/get-started/quickstart/github-flow) to commit/review/collaborate on changes\n* After a PR is approved/merged, please delete the PR branch both remotely and locally\n\n### Building From Source\n\n---\nOnce you check out the code from GitHub, you can build it using the following commands.\n\nLinux:\n\n```./mvnw clean install```\n\nWindows:\n\n```./mvnw.cmd clean install```\n### USING THE PLUGIN\n\n---\n\nThe recommended way to use the S3 ACCESS GRANTS PLUGIN for Java in your project is to consume it from Maven Central\n\n\n```\n <dependency>\n    <groupId>software.amazon.s3.accessgrants</groupId>\n    <artifactId>aws-s3-accessgrants-java-plugin</artifactId>\n    <version>replace with latest version</version>\n</dependency>\n```\n\nCreate a S3AccessGrantsPlugin object and choose if you want to enable fallback.\n1.  If enableFallback option is set to false we will fallback only in case the operation/API is not supported by Access Grants.\n2.  If enableFallback is set to true then we will fall back every time we are not able to get the credentials from Access Grants, no matter the reason.\n\n```\nS3AccessGrantsPlugin accessGrantsPlugin = S3AccessGrantsPlugin.builder().enableFallback(true).build();\n```\n\nWhile building S3 client you have to provide a credentialsProvider object which contains credentials that have access to get credentials from Access Grants.\nNote - We only support IAM credentials with this release.\n\n````\nS3Client s3Client = S3Client.builder()\n                    .addPlugin(accessGrantsPlugin)\n                    .credentialsProvider(credentialsProvider)\n                    .region(REGION)\n                    .build();\n````\n\nUsing this S3Client to make API calls, you should be able to use Access Grants to get access to your resources.\n\n### Turn on cross-region access\n\nThe plugin by default does not support cross-region access of S3 Buckets/data. \nIn order to turn on the cross-region support, please configure S3Client and Access Grants Plugin to support cross-region access.\n\n```\n S3AccessGrantsPlugin accessGrantsPlugin =\n                S3AccessGrantsPlugin.builder().enableCrossRegionAccess(Boolean.TRUE).build();\n\n        S3Client s3Client =\n                S3Client.builder()\n                        .crossRegionAccessEnabled(true)\n                        .credentialsProvider(credentialsProvider)\n                        .addPlugin(accessGrantsPlugin)\n                        .region(S3AccessGrantsIntegrationTestsUtils.TEST_REGION)\n                        .build();\n```\n\n#### NOTE - \nIf cross-region access setting is turned on for either the S3 Client or the plugin (but not both), you might experience bucket region mismatch errors.\n\n### Cross-account support\n\nThe plugin makes S3 head bucket requests to determine bucket location. \nIn case of cross-account access S3 expects s3:ListBucket permission for the requesting account on the requested bucket. Please add the necessary permission if the plugin will be used for cross-account access.\n\n### Turn on metrics\n\nThe plugin integrates with the Metrics publisher specified on the S3 Clients and does not require any separate metrics publisher to be defined during the plugin creation.\n\n\n```\n\nMetricPublisher metricPublisher = CloudWatchMetricPublisher.builder().namespace(\"S3AccessGrantsPlugin\").cloudWatchClient(CloudWatchAsyncClient.builder().region(S3AccessGrantsIntegrationTestsUtils.TEST_REGION).credentialsProvider(credentialsProvider).build()).build();\n\nS3Client s3Client = S3Client.builder()\n                    .credentialsProvider(credentialsProvider)\n                    .addPlugin(accessGrantsPlugin)\n                    .region(S3AccessGrantsIntegrationTestsUtils.TEST_REGION)\n                    .overrideConfiguration(config -> config.addMetricPublisher(metricPublisher))\n                    .build();\n            \n```\n\n### Change logging level\n\nTurning on the AWS SDK level logging should turn on the logging for the S3 Access grants plugin. You can also control the logging for the plugin specifically by adding the below config to your log4j.properties file.\n\n```\nlogger.s3accessgrants.name = software.amazon.awssdk.s3accessgrants\nlogger.s3accessgrants.level = debug\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.", "release_dates": []}, {"name": "aws-sagemaker-edge-quick-device-setup", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "| :rotating_light: **ALERT**: We are retiring Amazon Sagemaker Edge on April 26th, 2024. Use this [step-by-step guide]( https://docs.aws.amazon.com/sagemaker/latest/dg/edge-eol.html) to learn about how to continue deploying your models to edge devices. :rotating_light:|\n| --- |\n\n| &#9888; **WARNING**: This tool is meant for development/testing use only. We don't recommend the use of this tool for production! &#9888; |\n| --- |\n\n# aws-sagemaker-egde-quick-device-setup\n\nThis package provides a command line interface to easily onboard device with [SageMaker Edge](https://aws.amazon.com/sagemaker/edge/). Run the cli on the device you would like to provision as it will create all the necessary artifacts on the device.\n\nJump to:\n\n- [Installation ](#installation)\n- [Configuration ](#configuration)\n- [Permissions ](#permissions)\n- [Basic Commands ](#basic-commands)\n- [Sample ](#sample)\n- [Getting Help](#getting-help)\n- [More Resources](#more-resource)\n\n\n\nThis README is for aws-sagemaker-edge-quick-device-setup version 0.0.1\n\nInstallation\n------------\n\n`aws-sagemaker-edge-quick-device-setup` is written in golang. You can also generate the binary directly from the source using\n\n`go build` \n\nOptionally there is a build script to generate binaries and shasums for the relevant OS/architecture combination\n\n\nWe support out of the the box distributions for known OS and architectures. Check out [Releases](https://github.com/aws/aws-sagemaker-edge-quick-device-setup/releases) for latest distributions.\n\n\n`bash ./build.sh {OS} {ARCH}`\n\nConfiguration\n-------------\n\nBefore using the cli, you need to configure your AWS credentials. Go to https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html to learn about how to configure aws credentials.\nSince the required IAM policy is fairly permissive and the credentials are only neede during setup, we recommend to use [temporary credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html).\n\n\nPermissions\n-----------\n\nIn order to invoke the CLI to create required resources in cloud the user/role must have required permission. You can create/attach a policy containing the following permissions.\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:GetRole\",\n                \"iam:AttachRolePolicy\",\n                \"iam:CreatePolicy\",\n                \"iam:PassRole\",\n                \"iam:GetPolicy\",\n                \"iam:CreateRole\",\n                \"iam:ListAttachedRolePolicies\",\n                \"iot:GetPolicy\",\n                \"iot:CreateThing\",\n                \"iot:AttachPolicy\",\n                \"iot:AttachThingPrincipal\",\n                \"iot:DescribeThing\",\n                \"iot:CreatePolicy\",\n                \"iot:CreateThingType\",\n                \"iot:CreateKeysAndCertificate\",\n                \"iot:DescribeThingType\",\n                \"iot:DescribeEndpoint\",\n                \"s3:CreateBucket\",\n                \"sagemaker:DescribeDeviceFleet\",\n                \"sagemaker:RegisterDevices\",\n                \"sagemaker:UpdateDevices\",\n                \"sagemaker:CreateDeviceFleet\",\n                \"sagemaker:DescribeDevice\",\n                \"sagemaker:AddTags\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::sagemaker-edge-release-store-*/*\"\n        }\n    ]\n}\n```\n\nAlso attach ` SagemakerFullAccess` policy to the user/role.\n\nBasic Commands\n--------------\n\nThe CLI command has the following structure: (Replace OS , ARCH with os, architecture of choice)\n\n```\n   # If built from source, change into bin directory of repo to access binary.\n   $ cd bin/\n   $ aws-sagemaker-edge-quick-device-setup-{OS}-{ARCH} --[options]\n   # Eg: for linux\n   $ aws-sagemaker-edge-quick-device-setup-linux-amd64 --options\n```\n\nFollowing are all the arguments supported by the cli. The important ones \n```\n  -accelerator string\n        Name of accelerator (optional).\n  -account string\n        AWS AccountId (required).\n  -agentDirectory string\n        Local path to store agent (default \"/home/ubuntu/aws-sagemaker-edge-quick-device-setup/aws-sagemaker-edge-quick-device-setup/demo-agent\")\n  -arch string\n        Name of device architecture (optional with distribution binary).\n  -deviceFleet string\n        Name of the device fleet (required).\n  -deviceFleetBucket string\n        Bucket to store device related data (optional/autogenerated).\n  -deviceFleetRole string\n        Name of the role for the device fleet (optional/autogenerated).\n  -deviceName string\n        Name of the device (required).\n  -dist\n        Print distribution information.\n  -enableDB\n        Enable DB library for metrics backup and deployment with agent binary.\n  -enableDeployment\n        Enable deployment library with agent binary.\n  -iotThingName string\n        IOT thing name for the device (optional/autogenerated).\n  -iotThingType string\n        Iot thing type for the device (optional/autogenerated).\n  -os string\n        Name of operating system (optional with distribution binary).\n  -region string\n        AWS Region. (default \"us-west-2\")\n  -s3FolderPrefix string\n        S3 prefix to store captured data (optional/autogenerated).\n  -version\n        Print the version of aws-sagemaker-edge-quick-device-setup\n```\n\nTo view help documentation, use one of the following:\n\n```\n   $ aws-sagemaker-edge-quick-device-setup-{OS}-{ARCH} --help\n```\n\nTo get the version of the cli:\n\n```\n   $ aws-sagemaker-edge-quick-device-setup-{OS}-{ARCH} --version\n```\n\nSample\n------\n\nIf your device is linux amd64(x86_64). You could use one of the pre built distribution [aws-sagemaker-edge-quick-device-setup-linux-amd64](https://github.com/aws/aws-sagemaker-edge-quick-device-setup/releases/download/v0.0.1/aws-sagemaker-edge-quick-device-setup-linux-amd64) to setup the device. For distributions OS and architecture are hardcoded into the binaries.\n\n**NOTE**: `deviceName` and `deviceFleet` are expected to be lower case. If upper case names are given, the tool converts them to lower case equivalent.\n\n```\n   $ aws-sagemaker-edge-quick-device-setup-{OS}-{ARCH} --deviceFleet test-fleet --deviceName test-device --account AWS_ACCOUNT_ID\n```\n\nGetting Help\n------------\n\nThe best way to interact with our team is through GitHub. You can [open\nan issue](https://github.com/aws/aws-sagemaker-edge-quick-device-setup/issues/new/choose) and\nchoose from one of our templates for guidance, bug reports, or feature\nrequests.\n\n\nPlease check for open similar\n[issues](https://github.com/aws/aws-sagemaker-edge-quick-device-setup/issues/) before opening\nanother one.\n\nMore Resources\n--------------\n\n-  [Changelog](https://github.com/aws/aws-cli/blob/develop/CHANGELOG.rst)\n   [Reference](https://docs.aws.amazon.com/cli/latest/reference/)\n-  [Amazon Web Services Discussion\n   Forums](https://forums.aws.amazon.com/)\n-  [AWS Support](https://console.aws.amazon.com/support/home#/)\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2022-10-27T00:57:10Z", "2022-08-18T23:14:47Z", "2022-04-18T22:28:36Z", "2021-12-20T23:29:36Z"]}, {"name": "aws-sam-build-images", "description": "AWS SAM build images", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS SAM build images\n\nAWS SAM build container images for all supported runtimes to build and deploy [serverless applications](https://aws.amazon.com/serverless/) on AWS.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "aws-sam-cli", "description": "CLI tool to build, test, debug, and deploy Serverless applications using AWS SAM", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<p align=\"center\">\n</p>\n\n# AWS SAM CLI\n\n![Apache 2.0 License](https://img.shields.io/github/license/aws/aws-sam-cli)\n![SAM CLI Version](https://img.shields.io/github/release/aws/aws-sam-cli.svg?label=CLI%20Version)\n![Install](https://img.shields.io/badge/brew-aws/tap/aws--sam--cli-orange)\n![pip](https://img.shields.io/badge/pip-aws--sam--cli-9cf)\n\n[Installation](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html) | [Blogs](https://serverlessland.com/blog?tag=AWS%20SAM) | [Videos](https://serverlessland.com/video?tag=AWS%20SAM) | [AWS Docs](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html) | [Roadmap](https://github.com/aws/aws-sam-cli/wiki/SAM-CLI-Roadmap) | [Try It Out](https://s12d.com/jKo46elk) | [Slack Us](https://join.slack.com/t/awsdevelopers/shared_invite/zt-yryddays-C9fkWrmguDv0h2EEDzCqvw)\n\nThe AWS Serverless Application Model (SAM) CLI is an open-source CLI tool that helps you develop serverless applications containing [Lambda functions](https://aws.amazon.com/lambda/), [Step Functions](https://aws.amazon.com/step-functions/), [API Gateway](https://aws.amazon.com/api-gateway/), [EventBridge](https://aws.amazon.com/eventbridge/), [SQS](https://aws.amazon.com/sqs/), [SNS](https://aws.amazon.com/sns/) and more. Some of the features it provides are:\n\n* **Initialize serverless applications** in minutes with AWS-provided infrastructure templates with `sam init`\n* **Compile, build, and package** Lambda functions with provided runtimes and with custom Makefile workflows, for zip and image types of Lambda functions with `sam build`\n* **Locally test** a Lambda function and API Gateway easily in a Docker container with `sam local` commands on SAM and CDK applications\n* **Sync and test your changes in the cloud** with `sam sync` in your developer environments\n* **Deploy** your SAM and CloudFormation templates using `sam deploy`\n* Quickly **create pipelines** with prebuilt templates with popular CI/CD systems using `sam pipeline init`\n* **Tail CloudWatch logs and X-Ray traces** with `sam logs` and `sam traces`\n\n## Recent blogposts and workshops\n\n* **Speeding up incremental changes with AWS SAM Accelerate and Nested Stacks** - [Read blogpost here](https://s12d.com/wt1ajjwB).\n\n* **Develop Node projects with SAM CLI using esbuild** - and use SAM Accelerate on Typescript projects. [Read blogpost here](https://s12d.com/5Aa6u0o7).\n\n* **Speed up development with SAM Accelerate** - quickly test your changes in the cloud. [Read docs here](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/accelerate.html).\n\n* **AWS Serverless Developer Experience Workshop: A day in a life of a developer** - [This advanced workshop](https://s12d.com/aws-sde-workshop) provides you with an immersive experience as a serverless developer, with hands-on experience building a serverless solution using AWS SAM and SAM CLI.\n\n* **The Complete SAM Workshop**- [This workshop](https://s12d.com/jKo46elk) is a great way to experience the power of SAM and SAM CLI.\n\n* **Getting started with CI/CD? SAM pipelines can help you get started** - [This workshop](https://s12d.com/_JQ48d5T) walks you through the basics.\n\n* **Get started with Serverless Application development using SAM CLI** - [This workshop](https://s12d.com/Tq9ZE-Br) walks you through the basics.\n\n## Get Started\n\nTo get started with building SAM-based applications, use the SAM CLI. SAM CLI provides a Lambda-like execution\nenvironment that lets you locally build, test, debug, and deploy [AWS serverless](https://aws.amazon.com/serverless/) applications.\n\n* [Install SAM CLI](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html)\n* [Build & Deploy a \"Hello World\" Web App](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-quick-start.html)\n* [Install AWS Toolkit](https://aws.amazon.com/getting-started/tools-sdks/#IDE_and_IDE_Toolkits) to use SAM with your favorite IDEs\n* [Tutorials and Workshops](https://serverlessland.com/learn)\n* **Powertools for AWS Lambda** is a developer toolkit to implement Serverless best practices and increase developer velocity. Available for [Python](https://awslabs.github.io/aws-lambda-powertools-python), [Java](https://github.com/awslabs/aws-lambda-powertools-java), [TypeScript](https://github.com/awslabs/aws-lambda-powertools-typescript) and [.NET](https://github.com/awslabs/aws-lambda-powertools-dotnet).\n\n**Next Steps:** Learn to build a more complex serverless application.\n\n* [Extract text from images and store it in a database](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-example-s3.html) using Amazon S3 and Amazon Rekognition services.\n* [Detect when records are added to a database](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-example-ddb.html) using Amazon DynamoDB database and asynchronous stream processing.\n* [Explore popular patterns](https://serverlessland.com/patterns)\n\n## What is this Github repository? \ud83d\udcbb\n\nThis Github repository contains source code for SAM CLI. Here is the development team talking about this code:\n\n> SAM CLI code is written in Python. Source code is well documented, very modular, with 95% unit test coverage.\nIt uses this awesome Python library called Click to manage the command line interaction and uses Docker to run Lambda functions locally.\nWe think you'll like the code base. Clone it and run `make pr` or `./Make -pr` on Windows!\n\n## Related Repositories and Resources\n\n* **SAM Transform** [Open source template specification](https://github.com/aws/serverless-application-model/) that provides shorthand syntax for CloudFormation\n* **SAM CLI application templates** Get started quickly with [predefined application templates](https://github.com/aws/aws-sam-cli-app-templates/blob/master/README.md) for all supported runtimes and languages, used by `sam init`\n* **Lambda Builders** [Lambda builder tools](https://github.com/aws/aws-lambda-builders) for supported runtimes and custom build workflows, used by `sam build`\n* **Build and local emulation images for CI/CD tools** [Build container images](https://gallery.ecr.aws/sam/) to use with CI/CD tasks\n\n## Contribute to SAM\n\nWe love our contributors \u2764\ufe0f We have over 100 contributors who have built various parts of the product.\nRead this [testimonial from @ndobryanskyy](https://www.lohika.com/aws-sam-my-exciting-first-open-source-experience/) to learn\nmore about what it was like contributing to SAM.\n\nDepending on your interest and skill, you can help build the different parts of the SAM project;\n\n**Enhance the SAM Specification**\n\nMake pull requests, report bugs, and share ideas to improve the full SAM template specification.\nSource code is located on Github at [aws/serverless-application-model](https://github.com/aws/serverless-application-model).\nRead the [SAM Specification Contributing Guide](https://github.com/aws/serverless-application-model/blob/master/CONTRIBUTING.md)\nto get started.\n\n**Strengthen SAM CLI**\n\nAdd new commands, enhance existing ones, report bugs, or request new features for the SAM CLI.\nSource code is located on Github at [aws/aws-sam-cli](https://github.com/aws/aws-sam-cli). Read the [SAM CLI Contributing Guide](https://github.com/aws/aws-sam-cli/blob/develop/CONTRIBUTING.md) to\nget started.\n\n**Update SAM Developer Guide**\n\n[SAM Developer Guide](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/index.html) provides a comprehensive getting started guide and reference documentation.\nSource code is located on Github at [awsdocs/aws-sam-developer-guide](https://github.com/awsdocs/aws-sam-developer-guide).\nRead the [SAM Documentation Contribution Guide](https://github.com/awsdocs/aws-sam-developer-guide/blob/master/CONTRIBUTING.md) to get\nstarted.\n\n### Join the SAM Community on Slack\n\n[Join the SAM developers channel (#samdev)](https://join.slack.com/t/awsdevelopers/shared_invite/zt-yryddays-C9fkWrmguDv0h2EEDzCqvw) on Slack to collaborate with fellow community members and the AWS SAM team.\n", "release_dates": ["2024-02-22T18:09:36Z", "2024-02-06T19:54:33Z", "2024-01-23T21:46:19Z", "2024-01-10T19:44:14Z", "2024-01-04T01:31:09Z", "2023-12-12T19:36:58Z", "2023-12-06T20:10:03Z", "2023-11-16T22:09:26Z", "2023-11-14T22:56:24Z", "2023-11-09T19:39:41Z", "2023-10-31T00:10:19Z", "2023-10-18T17:47:45Z", "2023-10-03T18:54:05Z", "2023-09-05T18:53:38Z", "2023-08-29T20:53:42Z", "2023-08-10T18:03:25Z", "2023-07-27T18:52:03Z", "2023-07-20T22:43:51Z", "2023-07-19T22:23:58Z", "2023-07-18T18:06:56Z", "2023-07-06T20:55:55Z", "2023-06-26T21:40:35Z", "2023-06-22T18:54:16Z", "2023-06-20T18:37:55Z", "2023-06-07T03:30:09Z", "2023-06-06T21:39:28Z", "2023-05-31T18:10:18Z", "2023-05-16T21:58:21Z", "2023-05-11T19:29:30Z", "2023-04-27T20:17:24Z"]}, {"name": "aws-sam-cli-app-templates", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SAM CLI Application Templates\n\nThis repository contains the application templates used in the AWS SAM CLI for `sam init` calls.\n\n## Contributing\n\nWe welcome issue reports and pull requests to help improve these application templates.\n\n### Testing a template locally\n\nYou can create a buildable and deployable project folder from a local template using the below command. The `<project_name>` can be whatever you want your project to be called, and `<path/to/init_template>` should point to the existing folder that contains the template.\n\nCommand: `sam init --no-input --location <path/to/init_template> --name <project_name>`\n\nFor example: `samdev init --no-input --location /home/myuser/code/aws-sam-cli-app-templates/dotnet8/hello-native-aot --name beauNativeAotTest`\n\n### Testing updated `sam init` locally\n\nTo test an updated workflow of `sam init`, including changes to the prompts to create your new template, you need to push your changes from your fork so they can be downloaded by the SAM CLI. You'll also need a local copy of SAM CLI's source code to modify to point to your fork. Follow SAM CLI's main [DEVELOPMENT_GUIDE.md](https://github.com/aws/aws-sam-cli/blob/develop/DEVELOPMENT_GUIDE.md) to get a local version of SAM CLI setup. With your fork pushed, you can update the [samcli/commands/init/init_templates.py](https://github.com/aws/aws-sam-cli/blob/49fb8f9ad60d1daee67ebc8045266c965a125b3c/samcli/commands/init/init_templates.py#L38-L42) file in your local SAM CLI like below. Make sure to update all three variables.\n\n```\nAPP_TEMPLATES_REPO_COMMIT = <the commit hash>\nMANIFEST_URL = (\n    f\"https://raw.githubusercontent.com/<your_github_id>/aws-sam-cli-app-templates/{APP_TEMPLATES_REPO_COMMIT}/manifest-v2.json\"\n)\nAPP_TEMPLATES_REPO_URL = \"https://github.com/<your_github_id>/aws-sam-cli-app-templates\"\n```\n\nExample\n```\nAPP_TEMPLATES_REPO_COMMIT = \"abcdefg1234567d6e78f319e189d39593c628b7f\"\nMANIFEST_URL = (\n    f\"https://raw.githubusercontent.com/aws/aws-sam-cli-app-templates/abcdefg1234567d6e78f319e189d39593c628b7f/manifest-v2.json\"\n)\nAPP_TEMPLATES_REPO_URL = \"https://github.com/cool-developer-xyz/aws-sam-cli-app-templates\"\n```\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "aws-sam-cli-pipeline-init-templates", "description": null, "language": "Shell", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# AWS SAM CLI Pipeline Init Templates\n\nThis repository contains the pipeline init templates used in the AWS SAM CLI for `sam pipeline` commands.\n\n## Getting Started\n\nTo get started with SAM-based pipelines, use the SAM CLI.\n\n* [Introducing AWS SAM Pipelines](https://aws.amazon.com/blogs/compute/introducing-aws-sam-pipelines-automatically-generate-deployment-pipelines-for-serverless-applications/)\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## Test\n\n```sh\n# Recommend: run these in a Python virtual environment\npip install aws-sam-cli==1.27.2\npip install pytest~=6.0\npytest tests/\n```\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.\n", "release_dates": []}, {"name": "aws-sdk", "description": "Landing page for the AWS SDKs on GitHub", "language": null, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## aws/sdk\n\nWelcome to the AWS SDKs landing page! This page provides\nlinks and information about the AWS SDKs on GitHub. This repository is also used\nto track feature requests and issues that affect multiple AWS SDKs.\n\n## Online Resources\n[AWS Developer Center](https://aws.amazon.com/developer/)\nRead the latest AWS developer news,\ndig into our tools, and share your ideas with the community worldwide.\n\n[AWS Developer Blog](https://aws.amazon.com/blogs/developer/)\nLearn about SDK software announcements, guides, and how-tos.\n\n[AWS Tools](https://aws.amazon.com/tools/)\nCheck out examples, language-specific developer guides, and getting started guides \nfor developing and managing applications on AWS.\n\n## AWS SDKs on GitHub\n* [AWS SDK for C++](https://github.com/aws/aws-sdk-cpp)\n* [AWS CLI](https://github.com/aws/aws-cli)\n* Go\n  * [AWS SDK for Go v2](https://github.com/aws/aws-sdk-go-v2)\n  * [AWS SDK for Go](https://github.com/aws/aws-sdk-go)\n* Java\n  * [AWS SDK for Java, v2](https://github.com/aws/aws-sdk-java-v2)\n  * [AWS SDK for Java](https://github.com/aws/aws-sdk-java) (not recommended for new projects)\n* JavaScript\n  * [AWS SDK for JavaScript v3](https://github.com/aws/aws-sdk-js-v3)\n  * [AWS SDK for JavaScript](https://github.com/aws/aws-sdk-js)\n* [AWS SDK for .NET](https://github.com/aws/dotnet)\n* [AWS SDK for PHP](https://github.com/aws/aws-sdk-php)\n* Python\n  * [AWS SDK for Python](https://github.com/boto/boto3)\n  * [AWS SDK for Python, legacy](https://github.com/boto/boto) (not recommended for new projects)\n  * [botocore](https://github.com/boto/botocore)\n* [AWS SDK for Ruby](https://github.com/aws/aws-sdk-ruby)\n\n## Security\n\nSee the [security section](CONTRIBUTING.md#security-issue-notifications) of the contributing guide for more information.\n\n## License\n\nThis project is dual licensed under the MIT-0 and the CC-BY-SA-4.0 licenses.\nSee the [LICENSE](LICENSE) file.\n", "release_dates": []}, {"name": "aws-sdk-cpp", "description": "AWS SDK for C++", "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for C++\nThe AWS SDK for C++ provides a modern C++ (version C++ 11 or later) interface for Amazon Web Services (AWS). It is meant to be performant and fully functioning with low- and high-level SDKs, while minimizing dependencies and providing platform portability (Windows, OSX, Linux, and mobile).\n\nAWS SDK for C++ is now in General Availability and recommended for production use. We invite our customers to join the development efforts by submitting pull requests and sending us feedback and ideas via GitHub Issues.\n\n## __Jump To:__\n* [Change log](https://github.com/aws/aws-sdk-cpp/tags)\n* [API Docs](https://sdk.amazonaws.com/cpp/api/LATEST/index.html)\n* [Getting Started](#getting-started)\n* [Getting Help](#getting-help)\n* [Using the SDK and Other Topics](#using-the-sdk-and-other-topics)\n\n# Getting Started\n\n## Building the SDK:\n\n### Minimum Requirements:\n* Visual Studio 2015 or later\n* OR GNU Compiler Collection (GCC) 4.9 or later\n* OR Clang 3.3 or later\n* 4GB of RAM (required for building some of the larger clients; SDK build may fail on EC2 instance types t2.micro, t2.small, and other small instance types due to insufficient memory)\n* Supported platforms\n  * Amazon Linux\n  * Windows\n  * Mac\n\n### Building From Source:\n\n#### To create an **out-of-source build**:\n1. Install CMake and the relevant build tools for your platform. Ensure these are available in your executable path.\n2. Clone this repository with submodules\n\n    ```sh\n    git clone --recurse-submodules https://github.com/aws/aws-sdk-cpp\n    ```\n\n3. Create your build directory. Replace `<BUILD_DIR>` with your build directory name:\n4. Build the project:\n\n    ```sh\n   cd <BUILD_DIR>\n   cmake <path-to-root-of-this-source-code> \\\n    -DCMAKE_BUILD_TYPE=Debug \\\n    -DCMAKE_INSTALL_PREFIX=<path-to-install> \\\n    -DBUILD_ONLY=\"s3\"\n   cmake --build . --config=Debug\n   cmake --install . --config=Debug\n   ```\n\n   **_NOTE:_** BUILD_ONLY is an optional flag used to list only the services you are using. Building the whole SDK can take a long time. Also, check out the list of [CMake parameters](./docs/CMake_Parameters.md)\n\n#### Other Dependencies:\nTo compile in Linux, you must have the header files for libcurl, libopenssl. The packages are typically available in your package manager.\n\nDebian based Linux distributions example:\n   `sudo apt-get install libcurl-dev`\n\nRPM based Linux distributions example:\n   `sudo [yum|dnf|zypper] install libcurl-devel`\n\n### Building for MacOS\n\nBuilding for macOS is largely the same as building on a *nix system except for how the system consumes the curl dependency and compilers.\n\nYou must install the [xcode command line tools](https://mac.install.guide/commandlinetools/4.html). This is required for Apple clang and gcc. This also installs libcurl as well.\n\n> :warning: If you are using macOS Sonoma, there is a [known issue](https://github.com/aws/aws-sdk-cpp/issues/2804) where using libcurl version 8.4.0 on macOS can lead to issues. [This issue is being tracked with curl and Apple](https://github.com/curl/curl/issues/12525). In the meanwhile, please use an updated version of [curl from Homebrew](https://formulae.brew.sh/formula/curl). You can include this in your project via the CMAKE_PREFIX_PATH.\n>\n> ```\n> cmake -DCMAKE_PREFIX_PATH=\"/opt/homebrew/opt/curl/\" \\\n>  -DAUTORUN_UNIT_TESTS=OFF \\\n>  -DBUILD_ONLY=\"s3\" \\\n>  -DCMAKE_INSTALL_PREFIX=\"~/sdk-install\" \\\n>  ..\n> ```\n\n### Building for Android\nTo build for Android, add `-DTARGET_ARCH=ANDROID` to your CMake command line. Currently, we support Android APIs from 19 to 28 with Android NDK 19c, and we are using the built-in CMake toolchain file supplied by Android NDK, assuming you have the appropriate environment variables (ANDROID_NDK) set.\n\n##### Android on Windows\nBuilding for Android on Windows requires some additional setup. In particular, you will need to run CMake from a Visual Studio developer command prompt (2015 or higher). Additionally, you will need 'git' and 'patch' in your path. If you have Git installed on a Windows system, then the patch is likely found in a sibling directory (.../Git/usr/bin/). Once you've verified these requirements, your CMake command line will change slightly to use nmake:\n\n   ```sh\n   cmake -G \"NMake Makefiles\" `-DTARGET_ARCH=ANDROID` <other options> ..\n   ```\n\nNmake builds targets in a serial fashion. To make things quicker, we recommend installing JOM as an alternative to nmake and then changing the CMake invocation to:\n\n   ```sh\n   cmake -G \"NMake Makefiles JOM\" `-DTARGET_ARCH=ANDROID` <other options> ..\n   ```\n\n### Building aws-sdk-cpp - Using vcpkg\n\nYou can download and install aws-sdk-cpp using the [vcpkg](https://github.com/Microsoft/vcpkg) dependency manager:\n\n    git clone https://github.com/Microsoft/vcpkg.git\n    cd vcpkg\n    ./bootstrap-vcpkg.sh\n    ./vcpkg integrate install\n    ./vcpkg install aws-sdk-cpp\n\nThe aws-sdk-cpp port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.\n\n# Maintenance and support for SDK major versions\n\nFor information about maintenance and support for SDK major versions and our underlying dependencies, see the following in the AWS SDKs and Tools Shared Configuration and Credentials Reference Guide\n\n* [AWS SDKs and Tools Maintenance Policy](https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html)\n* [AWS SDKs and Tools Version Support Matrix](https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html)\n\n\n# Getting Help\n\nThe best way to interact with our team is through GitHub. You can open a [discussion](https://github.com/aws/aws-sdk-cpp/discussions/new/choose) for guidance questions or an [issue](https://github.com/aws/aws-sdk-cpp/issues/new/choose) for bug reports or feature requests.\n\nYou may also find help on community resources such as [StackOverFlow](https://stackoverflow.com/) with the tag [#aws-sdk-cpp](https://stackoverflow.com/questions/tagged/aws-sdk-cpp). If you have a support plan with [AWS Support](https://aws.amazon.com/premiumsupport/), you can also create a new support case.\n\nPlease make sure to check out our resources too before opening an issue:\n* [Developer Guide](https://docs.aws.amazon.com/sdk-for-cpp/v1/developer-guide/welcome.html) and [API reference](http://sdk.amazonaws.com/cpp/api/LATEST/index.html)\n* [Changelog](./CHANGELOG.md) for recent breaking changes.\n* [Contribution](./CONTRIBUTING.md) guide.\n* [samples repo](https://github.com/awsdocs/aws-doc-sdk-examples/tree/master/cpp).\n\n\n# Using the SDK and Other Topics\n* Other docs for how to build the sdk\n  * [CMake Parameters](./docs/CMake_Parameters.md)\n  * [Add as CMake external project](./docs/CMake_External_Project.md)\n  * [Building for Docker](https://github.com/aws/aws-sdk-cpp/tree/master/CI/docker-file) (To build for Docker, ensure your container meets the [minimum requirements](#minimum-requirements))\n  * [Building on an EC2 instance](https://github.com/aws/aws-sdk-cpp/wiki/Building-the-SDK-from-source-on-EC2)\n* SDK usage\n  * [API Docs](https://sdk.amazonaws.com/cpp/api/LATEST/index.html)\n  * [Using the SDK](./docs/SDK_usage_guide.md)\n  * [Credentials Providers](./docs/Credentials_Providers.md)\n  * [Client Configuration Parameters](./docs/ClientConfiguration_Parameters.md)\n  * [Service Client](./docs/Service_Client.md)\n  * [Memory Management](./docs/Memory_Management.md)\n  * [Advanced Topics](./docs/Advanced_topics.md)\n* [Coding Standards](./docs/CODING_STANDARDS.md)\n* [License](./LICENSE)\n* [Code of Conduct](./CODE_OF_CONDUCT.md)\n", "release_dates": []}, {"name": "aws-sdk-go", "description": "AWS SDK for the Go programming language.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for Go\n\n[![API Reference](https://img.shields.io/badge/api-reference-blue.svg)](https://docs.aws.amazon.com/sdk-for-go/api) [![Join the chat at https://gitter.im/aws/aws-sdk-go](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/aws/aws-sdk-go?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Build status](https://github.com/aws/aws-sdk-go/actions/workflows/go.yml/badge.svg?branch=main)](https://github.com/aws/aws-sdk-go/actions/workflows/go.yml) [![Apache V2 License](https://img.shields.io/badge/license-Apache%20V2-blue.svg)](https://github.com/aws/aws-sdk-go/blob/main/LICENSE.txt)\n\naws-sdk-go is the v1 AWS SDK for the Go programming language.\n\nWe [announced](https://aws.amazon.com/blogs/developer/announcing-end-of-support-for-aws-sdk-for-go-v1-on-july-31-2025) the upcoming **end-of-support for AWS SDK for Go (v1)**. We recommend that you migrate to [AWS SDK for Go v2](https://aws.github.io/aws-sdk-go-v2/docs/). For dates, additional details, and information on how to migrate, please refer to the linked announcement.\n\nJump To:\n* [Getting Started](#Getting-Started)\n* [Quick Examples](#Quick-Examples)\n* [Getting Help](#Getting-Help)\n* [Contributing](#Contributing)\n* [More Resources](#Resources)\n\n## Getting Started\n\n### Installing\nUse `go get` to retrieve the SDK to add it to your project's Go module dependencies.\n\n\tgo get github.com/aws/aws-sdk-go\n\nTo update the SDK use `go get -u` to retrieve the latest version of the SDK.\n\n\tgo get -u github.com/aws/aws-sdk-go\n\n## Quick Examples \n\n### Complete SDK Example\n\nThis example shows a complete working Go file which will upload a file to S3\nand use the Context pattern to implement timeout logic that will cancel the\nrequest if it takes too long. This example highlights how to use sessions,\ncreate a service client, make a request, handle the error, and process the\nresponse.\n\n```go\n  package main\n\n  import (\n  \t\"context\"\n  \t\"flag\"\n  \t\"fmt\"\n  \t\"os\"\n  \t\"time\"\n\n  \t\"github.com/aws/aws-sdk-go/aws\"\n  \t\"github.com/aws/aws-sdk-go/aws/awserr\"\n  \t\"github.com/aws/aws-sdk-go/aws/request\"\n  \t\"github.com/aws/aws-sdk-go/aws/session\"\n  \t\"github.com/aws/aws-sdk-go/service/s3\"\n  )\n\n  // Uploads a file to S3 given a bucket and object key. Also takes a duration\n  // value to terminate the update if it doesn't complete within that time.\n  //\n  // The AWS Region needs to be provided in the AWS shared config or on the\n  // environment variable as `AWS_REGION`. Credentials also must be provided\n  // Will default to shared config file, but can load from environment if provided.\n  //\n  // Usage:\n  //   # Upload myfile.txt to myBucket/myKey. Must complete within 10 minutes or will fail\n  //   go run withContext.go -b mybucket -k myKey -d 10m < myfile.txt\n  func main() {\n  \tvar bucket, key string\n  \tvar timeout time.Duration\n\n  \tflag.StringVar(&bucket, \"b\", \"\", \"Bucket name.\")\n  \tflag.StringVar(&key, \"k\", \"\", \"Object key name.\")\n  \tflag.DurationVar(&timeout, \"d\", 0, \"Upload timeout.\")\n  \tflag.Parse()\n\n  \t// All clients require a Session. The Session provides the client with\n \t// shared configuration such as region, endpoint, and credentials. A\n \t// Session should be shared where possible to take advantage of\n \t// configuration and credential caching. See the session package for\n \t// more information.\n  \tsess := session.Must(session.NewSession())\n\n \t// Create a new instance of the service's client with a Session.\n \t// Optional aws.Config values can also be provided as variadic arguments\n \t// to the New function. This option allows you to provide service\n \t// specific configuration.\n  \tsvc := s3.New(sess)\n\n  \t// Create a context with a timeout that will abort the upload if it takes\n  \t// more than the passed in timeout.\n  \tctx := context.Background()\n  \tvar cancelFn func()\n  \tif timeout > 0 {\n  \t\tctx, cancelFn = context.WithTimeout(ctx, timeout)\n  \t}\n  \t// Ensure the context is canceled to prevent leaking.\n  \t// See context package for more information, https://golang.org/pkg/context/\n\tif cancelFn != nil {\n  \t\tdefer cancelFn()\n\t}\n\n  \t// Uploads the object to S3. The Context will interrupt the request if the\n  \t// timeout expires.\n  \t_, err := svc.PutObjectWithContext(ctx, &s3.PutObjectInput{\n  \t\tBucket: aws.String(bucket),\n  \t\tKey:    aws.String(key),\n  \t\tBody:   os.Stdin,\n  \t})\n  \tif err != nil {\n  \t\tif aerr, ok := err.(awserr.Error); ok && aerr.Code() == request.CanceledErrorCode {\n  \t\t\t// If the SDK can determine the request or retry delay was canceled\n  \t\t\t// by a context the CanceledErrorCode error code will be returned.\n  \t\t\tfmt.Fprintf(os.Stderr, \"upload canceled due to timeout, %v\\n\", err)\n  \t\t} else {\n  \t\t\tfmt.Fprintf(os.Stderr, \"failed to upload object, %v\\n\", err)\n  \t\t}\n  \t\tos.Exit(1)\n  \t}\n\n  \tfmt.Printf(\"successfully uploaded file to %s/%s\\n\", bucket, key)\n  }\n```\n\n### Overview of SDK's Packages\n\nThe SDK is composed of two main components, SDK core, and service clients.\nThe SDK core packages are all available under the aws package at the root of\nthe SDK. Each client for a supported AWS service is available within its own\npackage under the service folder at the root of the SDK.\n\n  * aws - SDK core, provides common shared types such as Config, Logger,\n    and utilities to make working with API parameters easier.\n\n      * awserr - Provides the error interface that the SDK will use for all\n        errors that occur in the SDK's processing. This includes service API\n        response errors as well. The Error type is made up of a code and message.\n        Cast the SDK's returned error type to awserr.Error and call the Code\n        method to compare returned error to specific error codes. See the package's\n        documentation for additional values that can be extracted such as RequestID.\n\n      * credentials - Provides the types and built in credentials providers\n        the SDK will use to retrieve AWS credentials to make API requests with.\n        Nested under this folder are also additional credentials providers such as\n        stscreds for assuming IAM roles, and ec2rolecreds for EC2 Instance roles.\n\n      * endpoints - Provides the AWS Regions and Endpoints metadata for the SDK.\n        Use this to lookup AWS service endpoint information such as which services\n        are in a region, and what regions a service is in. Constants are also provided\n        for all region identifiers, e.g UsWest2RegionID for \"us-west-2\".\n\n      * session - Provides initial default configuration, and load\n        configuration from external sources such as environment and shared\n        credentials file.\n\n      * request - Provides the API request sending, and retry logic for the SDK.\n        This package also includes utilities for defining your own request\n        retryer, and configuring how the SDK processes the request.\n\n  * service - Clients for AWS services. All services supported by the SDK are\n    available under this folder.\n\n### How to Use the SDK's AWS Service Clients\n\nThe SDK includes the Go types and utilities you can use to make requests to\nAWS service APIs. Within the service folder at the root of the SDK you'll find\na package for each AWS service the SDK supports. All service clients follow common pattern of creation and usage.\n\nWhen creating a client for an AWS service you'll first need to have a Session\nvalue constructed. The Session provides shared configuration that can be shared\nbetween your service clients. When service clients are created you can pass\nin additional configuration via the aws.Config type to override configuration\nprovided by in the Session to create service client instances with custom\nconfiguration.\n\nOnce the service's client is created you can use it to make API requests the\nAWS service. These clients are safe to use concurrently.\n\n### Configuring the SDK\n\nIn the AWS SDK for Go, you can configure settings for service clients, such\nas the log level and maximum number of retries. Most settings are optional;\nhowever, for each service client, you must specify a region and your credentials.\nThe SDK uses these values to send requests to the correct AWS region and sign\nrequests with the correct credentials. You can specify these values as part\nof a session or as environment variables.\n\nSee the SDK's [configuration guide][config_guide] for more information.\n\nSee the [session][session_pkg] package documentation for more information on how to use Session\nwith the SDK.\n\nSee the [Config][config_typ] type in the [aws][aws_pkg] package for more information on configuration\noptions.\n\n[config_guide]: https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html\n[session_pkg]: https://docs.aws.amazon.com/sdk-for-go/api/aws/session/\n[config_typ]: https://docs.aws.amazon.com/sdk-for-go/api/aws/#Config\n[aws_pkg]: https://docs.aws.amazon.com/sdk-for-go/api/aws/\n\n### Configuring Credentials\n\nWhen using the SDK you'll generally need your AWS credentials to authenticate\nwith AWS services. The SDK supports multiple methods of supporting these\ncredentials. By default the SDK will source credentials automatically from\nits default credential chain. See the session package for more information\non this chain, and how to configure it. The common items in the credential\nchain are the following:\n\n  * Environment Credentials - Set of environment variables that are useful\n    when sub processes are created for specific roles.\n\n  * Shared Credentials file (~/.aws/credentials) - This file stores your\n    credentials based on a profile name and is useful for local development.\n\n  * EC2 Instance Role Credentials - Use EC2 Instance Role to assign credentials\n    to application running on an EC2 instance. This removes the need to manage\n    credential files in production.\n\nCredentials can be configured in code as well by setting the Config's Credentials\nvalue to a custom provider or using one of the providers included with the\nSDK to bypass the default credential chain and use a custom one. This is\nhelpful when you want to instruct the SDK to only use a specific set of\ncredentials or providers.\n\nThis example creates a credential provider for assuming an IAM role, \"myRoleARN\"\nand configures the S3 service client to use that role for API requests.\n\n```go\n  // Initial credentials loaded from SDK's default credential chain. Such as\n  // the environment, shared credentials (~/.aws/credentials), or EC2 Instance\n  // Role. These credentials will be used to to make the STS Assume Role API.\n  sess := session.Must(session.NewSession())\n\n  // Create the credentials from AssumeRoleProvider to assume the role\n  // referenced by the \"myRoleARN\" ARN.\n  creds := stscreds.NewCredentials(sess, \"myRoleArn\")\n\n  // Create service client value configured for credentials\n  // from assumed role.\n  svc := s3.New(sess, &aws.Config{Credentials: creds})\n```\n\nSee the [credentials][credentials_pkg] package documentation for more information on credential\nproviders included with the SDK, and how to customize the SDK's usage of\ncredentials.\n\nThe SDK has support for the shared configuration file (~/.aws/config). This\nsupport can be enabled by setting the environment variable, \"AWS_SDK_LOAD_CONFIG=1\",\nor enabling the feature in code when creating a Session via the\nOption's SharedConfigState parameter.\n\n```go\n  sess := session.Must(session.NewSessionWithOptions(session.Options{\n      SharedConfigState: session.SharedConfigEnable,\n  }))\n```\n\n[credentials_pkg]: https://docs.aws.amazon.com/sdk-for-go/api/aws/credentials\n\n### Configuring AWS Region\n\nIn addition to the credentials you'll need to specify the region the SDK\nwill use to make AWS API requests to. In the SDK you can specify the region\neither with an environment variable, or directly in code when a Session or\nservice client is created. The last value specified in code wins if the region\nis specified multiple ways.\n\nTo set the region via the environment variable set the \"AWS_REGION\" to the\nregion you want to the SDK to use. Using this method to set the region will\nallow you to run your application in multiple regions without needing additional\ncode in the application to select the region.\n\n    AWS_REGION=us-west-2\n\nThe endpoints package includes constants for all regions the SDK knows. The\nvalues are all suffixed with RegionID. These values are helpful, because they\nreduce the need to type the region string manually.\n\nTo set the region on a Session use the aws package's Config struct parameter\nRegion to the AWS region you want the service clients created from the session to\nuse. This is helpful when you want to create multiple service clients, and\nall of the clients make API requests to the same region.\n\n```go\n  sess := session.Must(session.NewSession(&aws.Config{\n      Region: aws.String(endpoints.UsWest2RegionID),\n  }))\n```\n\nSee the [endpoints][endpoints_pkg] package for the AWS Regions and Endpoints metadata.\n\nIn addition to setting the region when creating a Session you can also set\nthe region on a per service client bases. This overrides the region of a\nSession. This is helpful when you want to create service clients in specific\nregions different from the Session's region.\n\n```go\n  svc := s3.New(sess, &aws.Config{\n      Region: aws.String(endpoints.UsWest2RegionID),\n  })\n```\n\nSee the [Config][config_typ] type in the [aws][aws_pkg] package for more information and additional\noptions such as setting the Endpoint, and other service client configuration options.\n\n[endpoints_pkg]: https://docs.aws.amazon.com/sdk-for-go/api/aws/endpoints/\n\n### Making API Requests\n\nOnce the client is created you can make an API request to the service.\nEach API method takes a input parameter, and returns the service response\nand an error. The SDK provides methods for making the API call in multiple ways.\n\nIn this list we'll use the S3 ListObjects API as an example for the different\nways of making API requests.\n\n  * ListObjects - Base API operation that will make the API request to the service.\n\n  * ListObjectsRequest - API methods suffixed with Request will construct the\n    API request, but not send it. This is also helpful when you want to get a\n    presigned URL for a request, and share the presigned URL instead of your\n    application making the request directly.\n\n  * ListObjectsPages - Same as the base API operation, but uses a callback to\n    automatically handle pagination of the API's response.\n\n  * ListObjectsWithContext - Same as base API operation, but adds support for\n    the Context pattern. This is helpful for controlling the canceling of in\n    flight requests. See the Go standard library context package for more\n    information. This method also takes request package's Option functional\n    options as the variadic argument for modifying how the request will be\n    made, or extracting information from the raw HTTP response.\n\n  * ListObjectsPagesWithContext - same as ListObjectsPages, but adds support for\n    the Context pattern. Similar to ListObjectsWithContext this method also\n    takes the request package's Option function option types as the variadic\n    argument.\n\nIn addition to the API operations the SDK also includes several higher level\nmethods that abstract checking for and waiting for an AWS resource to be in\na desired state. In this list we'll use WaitUntilBucketExists to demonstrate\nthe different forms of waiters.\n\n  * WaitUntilBucketExists. - Method to make API request to query an AWS service for\n    a resource's state. Will return successfully when that state is accomplished.\n\n  * WaitUntilBucketExistsWithContext - Same as WaitUntilBucketExists, but adds\n    support for the Context pattern. In addition these methods take request\n    package's WaiterOptions to configure the waiter, and how underlying request\n    will be made by the SDK.\n\nThe API method will document which error codes the service might return for\nthe operation. These errors will also be available as const strings prefixed\nwith \"ErrCode\" in the service client's package. If there are no errors listed\nin the API's SDK documentation you'll need to consult the AWS service's API\ndocumentation for the errors that could be returned.\n\n```go\n  ctx := context.Background()\n\n  result, err := svc.GetObjectWithContext(ctx, &s3.GetObjectInput{\n      Bucket: aws.String(\"my-bucket\"),\n      Key: aws.String(\"my-key\"),\n  })\n  if err != nil {\n      // Cast err to awserr.Error to handle specific error codes.\n      aerr, ok := err.(awserr.Error)\n      if ok && aerr.Code() == s3.ErrCodeNoSuchKey {\n          // Specific error code handling\n      }\n      return err\n  }\n\n  // Make sure to close the body when done with it for S3 GetObject APIs or\n  // will leak connections.\n  defer result.Body.Close()\n\n  fmt.Println(\"Object Size:\", aws.Int64Value(result.ContentLength))\n```\n\n### API Request Pagination and Resource Waiters\n\nPagination helper methods are suffixed with \"Pages\", and provide the\nfunctionality needed to round trip API page requests. Pagination methods\ntake a callback function that will be called for each page of the API's response.\n\n```go\n   objects := []string{}\n   err := svc.ListObjectsPagesWithContext(ctx, &s3.ListObjectsInput{\n       Bucket: aws.String(myBucket),\n   }, func(p *s3.ListObjectsOutput, lastPage bool) bool {\n       for _, o := range p.Contents {\n           objects = append(objects, aws.StringValue(o.Key))\n       }\n       return true // continue paging\n   })\n   if err != nil {\n       panic(fmt.Sprintf(\"failed to list objects for bucket, %s, %v\", myBucket, err))\n   }\n\n   fmt.Println(\"Objects in bucket:\", objects)\n```\n\nWaiter helper methods provide the functionality to wait for an AWS resource\nstate. These methods abstract the logic needed to check the state of an\nAWS resource, and wait until that resource is in a desired state. The waiter\nwill block until the resource is in the state that is desired, an error occurs,\nor the waiter times out. If a resource times out the error code returned will\nbe request.WaiterResourceNotReadyErrorCode.\n\n```go\n  err := svc.WaitUntilBucketExistsWithContext(ctx, &s3.HeadBucketInput{\n      Bucket: aws.String(myBucket),\n  })\n  if err != nil {\n      aerr, ok := err.(awserr.Error)\n      if ok && aerr.Code() == request.WaiterResourceNotReadyErrorCode {\n          fmt.Fprintf(os.Stderr, \"timed out while waiting for bucket to exist\")\n      }\n      panic(fmt.Errorf(\"failed to wait for bucket to exist, %v\", err))\n  }\n  fmt.Println(\"Bucket\", myBucket, \"exists\")\n```    \n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues\nfor tracking bugs and feature requests.\n\n* Ask a question on [StackOverflow](http://stackoverflow.com/) and tag it with the [`aws-sdk-go`](http://stackoverflow.com/questions/tagged/aws-sdk-go) tag.\n* Come join the AWS SDK for Go community chat on [gitter](https://gitter.im/aws/aws-sdk-go).\n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n* If you think you may have found a bug, please open an [issue](https://github.com/aws/aws-sdk-go/issues/new/choose).\n\nThis SDK implements AWS service APIs. For general issues regarding the AWS services and their limitations, you may also take a look at the [Amazon Web Services Discussion Forums](https://forums.aws.amazon.com/).\n\n### Opening Issues\n\nIf you encounter a bug with the AWS SDK for Go we would like to hear about it.\nSearch the [existing issues](https://github.com/aws/aws-sdk-go/issues) and see\nif others are also experiencing the issue before opening a new issue. Please\ninclude the version of AWS SDK for Go, Go language, and OS you\u2019re using. Please\nalso include reproduction case when appropriate.\n\nThe GitHub issues are intended for bug reports and feature requests. For help\nand questions with using AWS SDK for Go please make use of the resources listed\nin the [Getting Help](https://github.com/aws/aws-sdk-go#getting-help) section.\nKeeping the list of open issues lean will help us respond in a timely manner.\n\n## Contributing\n\nWe work hard to provide a high-quality and useful SDK for our AWS services, and we greatly value feedback and contributions from our community. Please review our [contributing guidelines](./CONTRIBUTING.md) before submitting any [issues] or [pull requests][pr] to ensure we have all the necessary information to effectively respond to your bug report or contribution.\n\n## Maintenance and support for SDK major versions\n\nFor information about maintenance and support for SDK major versions and our underlying dependencies, see the following in the AWS SDKs and Tools Shared Configuration and Credentials Reference Guide:\n\n* [AWS SDKs and Tools Maintenance Policy](https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html)\n* [AWS SDKs and Tools Version Support Matrix](https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html)\n\n### Go version support policy\n\nThe v2 SDK follows the upstream [release policy](https://go.dev/doc/devel/release#policy)\nwith an additional six months of support for the most recently deprecated\nlanguage version.\n\n**AWS reserves the right to drop support for unsupported Go versions earlier to\naddress critical security issues.**\n\n## Resources\n\n[Developer guide](https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/welcome.html) - This document\nis a general introduction on how to configure and make requests with the SDK.\nIf this is your first time using the SDK, this documentation and the API\ndocumentation will help you get started. This document focuses on the syntax\nand behavior of the SDK. The [Service Developer Guide](https://aws.amazon.com/documentation/) \nwill help you get started using specific AWS services.\n\n[SDK API Reference Documentation](https://docs.aws.amazon.com/sdk-for-go/api/) - Use this\ndocument to look up all API operation input and output parameters for AWS\nservices supported by the SDK. The API reference also includes documentation of\nthe SDK, and examples how to using the SDK, service client API operations, and\nAPI operation require parameters.\n\n[Service Documentation](https://aws.amazon.com/documentation/) - Use this\ndocumentation to learn how to interface with AWS services. These guides are\ngreat for getting started with a service, or when looking for more \ninformation about a service. While this document is not required for coding, \nservices may supply helpful samples to look out for.\n\n[SDK Examples](https://github.com/aws/aws-sdk-go/tree/main/example) -\nIncluded in the SDK's repo are several hand crafted examples using the SDK\nfeatures and AWS services.\n\n[Forum](https://forums.aws.amazon.com/forum.jspa?forumID=293) - Ask questions, get help, and give feedback\n\n[Issues][issues] - Report issues, submit pull requests, and get involved\n  (see [Apache 2.0 License][license])\n\n\n[issues]: https://github.com/aws/aws-sdk-go/issues\n[pr]: https://github.com/aws/aws-sdk-go/pulls\n[license]: http://aws.amazon.com/apache2.0/\n", "release_dates": ["2024-03-01T19:19:31Z", "2024-02-29T19:18:20Z", "2024-02-28T19:26:43Z", "2024-02-27T19:19:38Z", "2024-02-26T19:31:47Z", "2024-02-23T19:23:25Z", "2024-02-22T19:33:03Z", "2024-02-21T19:22:31Z", "2024-02-20T19:18:07Z", "2024-02-19T19:25:05Z", "2024-02-16T19:29:33Z", "2024-02-15T19:32:44Z", "2024-02-14T19:27:57Z", "2024-02-13T19:29:04Z", "2024-02-12T19:28:59Z", "2024-02-09T19:21:34Z", "2024-02-08T19:27:23Z", "2024-02-07T19:28:11Z", "2024-02-06T19:31:07Z", "2024-02-05T19:24:59Z", "2024-02-02T19:30:19Z", "2024-02-01T19:21:53Z", "2024-01-31T19:41:03Z", "2024-01-30T19:30:14Z", "2024-01-29T19:34:57Z", "2024-01-26T19:29:31Z", "2024-01-25T19:39:57Z", "2024-01-24T19:31:56Z", "2024-01-23T20:57:54Z", "2024-01-22T19:32:28Z"]}, {"name": "aws-sdk-go-v2", "description": "AWS SDK for the Go programming language. ", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for Go v2\n\n[![Go Build status](https://github.com/aws/aws-sdk-go-v2/actions/workflows/go.yml/badge.svg?branch=main)](https://github.com/aws/aws-sdk-go-v2/actions/workflows/go.yml)[![Codegen Build status](https://github.com/aws/aws-sdk-go-v2/actions/workflows/codegen.yml/badge.svg?branch=main)](https://github.com/aws/aws-sdk-go-v2/actions/workflows/codegen.yml) [![SDK Documentation](https://img.shields.io/badge/SDK-Documentation-blue)](https://aws.github.io/aws-sdk-go-v2/docs/) [![Migration Guide](https://img.shields.io/badge/Migration-Guide-blue)](https://aws.github.io/aws-sdk-go-v2/docs/migrating/) [![API Reference](https://img.shields.io/badge/api-reference-blue.svg)](https://pkg.go.dev/mod/github.com/aws/aws-sdk-go-v2) [![Apache V2 License](https://img.shields.io/badge/license-Apache%20V2-blue.svg)](https://github.com/aws/aws-sdk-go-v2/blob/main/LICENSE.txt)\n\n`aws-sdk-go-v2` is the v2 AWS SDK for the Go programming language.\n\nThe v2 SDK requires a minimum version of `Go 1.20`.\n\nCheck out the [release notes](https://github.com/aws/aws-sdk-go-v2/blob/main/CHANGELOG.md) for information about the latest bug\nfixes, updates, and features added to the SDK.\n\nJump To:\n* [Getting Started](#getting-started)\n* [Getting Help](#getting-help)\n* [Contributing](#feedback-and-contributing)\n* [More Resources](#resources)\n\n## Maintenance and support for SDK major versions\n\nFor information about maintenance and support for SDK major versions and their underlying dependencies, see the\nfollowing in the AWS SDKs and Tools Shared Configuration and Credentials Reference Guide:\n\n* [AWS SDKs and Tools Maintenance Policy](https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html)\n* [AWS SDKs and Tools Version Support Matrix](https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html)\n\n### Go version support policy\n\nThe v2 SDK follows the upstream [release policy](https://go.dev/doc/devel/release#policy)\nwith an additional six months of support for the most recently deprecated\nlanguage version.\n\n**AWS reserves the right to drop support for unsupported Go versions earlier to\naddress critical security issues.**\n\n## Getting started\nTo get started working with the SDK setup your project for Go modules, and retrieve the SDK dependencies with `go get`.\nThis example shows how you can use the v2 SDK to make an API request using the SDK's [Amazon DynamoDB] client.\n\n###### Initialize Project\n```sh\n$ mkdir ~/helloaws\n$ cd ~/helloaws\n$ go mod init helloaws\n```\n###### Add SDK Dependencies\n```sh\n$ go get github.com/aws/aws-sdk-go-v2/aws\n$ go get github.com/aws/aws-sdk-go-v2/config\n$ go get github.com/aws/aws-sdk-go-v2/service/dynamodb\n```\n\n###### Write Code\nIn your preferred editor add the following content to `main.go`\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n\n    \"github.com/aws/aws-sdk-go-v2/aws\"\n    \"github.com/aws/aws-sdk-go-v2/config\"\n    \"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\n)\n\nfunc main() {\n    // Using the SDK's default configuration, loading additional config\n    // and credentials values from the environment variables, shared\n    // credentials, and shared configuration files\n    cfg, err := config.LoadDefaultConfig(context.TODO(), config.WithRegion(\"us-west-2\"))\n    if err != nil {\n        log.Fatalf(\"unable to load SDK config, %v\", err)\n    }\n\n    // Using the Config value, create the DynamoDB client\n    svc := dynamodb.NewFromConfig(cfg)\n\n    // Build the request with its input parameters\n    resp, err := svc.ListTables(context.TODO(), &dynamodb.ListTablesInput{\n        Limit: aws.Int32(5),\n    })\n    if err != nil {\n        log.Fatalf(\"failed to list tables, %v\", err)\n    }\n\n    fmt.Println(\"Tables:\")\n    for _, tableName := range resp.TableNames {\n        fmt.Println(tableName)\n    }\n}\n```\n\n###### Compile and Execute\n```sh\n$ go run .\nTables:\ntableOne\ntableTwo\n```\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues\nfor tracking bugs and feature requests.\n\n* Ask us a [question](https://github.com/aws/aws-sdk-go-v2/discussions/new?category=q-a) or open a [discussion](https://github.com/aws/aws-sdk-go-v2/discussions/new?category=general).\n* If you think you may have found a bug, please open an [issue](https://github.com/aws/aws-sdk-go-v2/issues/new/choose).\n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n\nThis SDK implements AWS service APIs. For general issues regarding the AWS services and their limitations, you may also take a look at the [Amazon Web Services Discussion Forums](https://forums.aws.amazon.com/).\n\n### Opening Issues\n\nIf you encounter a bug with the AWS SDK for Go we would like to hear about it.\nSearch the [existing issues][Issues] and see\nif others are also experiencing the same issue before opening a new issue. Please\ninclude the version of AWS SDK for Go, Go language, and OS you\u2019re using. Please\nalso include reproduction case when appropriate.\n\nThe GitHub issues are intended for bug reports and feature requests. For help\nand questions with using AWS SDK for Go please make use of the resources listed\nin the [Getting Help](#getting-help) section.\nKeeping the list of open issues lean will help us respond in a timely manner.\n\n## Feedback and contributing\n\nThe v2 SDK will use GitHub [Issues] to track feature requests and issues with the SDK. In addition, we'll use GitHub [Projects] to track large tasks spanning multiple pull requests, such as refactoring the SDK's internal request lifecycle. You can provide feedback to us in several ways.\n\n**GitHub issues**. To provide feedback or report bugs, file GitHub [Issues] on the SDK. This is the preferred mechanism to give feedback so that other users can engage in the conversation, +1 issues, etc. Issues you open will be evaluated, and included in our roadmap for the GA launch.\n\n**Contributing**. You can open pull requests for fixes or additions to the AWS SDK for Go 2.0. All pull requests must be submitted under the Apache 2.0 license and will be reviewed by an SDK team member before being merged in. Accompanying unit tests, where possible, are appreciated.\n\n## Resources\n\n[SDK Developer Guide](https://aws.github.io/aws-sdk-go-v2/docs/) - Use this document to learn how to get started and\nuse the AWS SDK for Go V2.\n\n[SDK Migration Guide](https://aws.github.io/aws-sdk-go-v2/docs/migrating/) - Use this document to learn how to migrate to V2 from the AWS SDK for Go.\n\n[SDK API Reference Documentation](https://pkg.go.dev/mod/github.com/aws/aws-sdk-go-v2) - Use this\ndocument to look up all API operation input and output parameters for AWS\nservices supported by the SDK. The API reference also includes documentation of\nthe SDK, and examples how to using the SDK, service client API operations, and\nAPI operation require parameters.\n\n[Service Documentation](https://aws.amazon.com/documentation/) - Use this\ndocumentation to learn how to interface with AWS services. These guides are\ngreat for getting started with a service, or when looking for more\ninformation about a service. While this document is not required for coding,\nservices may supply helpful samples to look out for.\n\n[Forum](https://forums.aws.amazon.com/forum.jspa?forumID=293) - Ask questions, get help, and give feedback\n\n[Issues] - Report issues, submit pull requests, and get involved\n  (see [Apache 2.0 License][license])\n\n[Dep]: https://github.com/golang/dep\n[Issues]: https://github.com/aws/aws-sdk-go-v2/issues\n[Projects]: https://github.com/aws/aws-sdk-go-v2/projects\n[CHANGELOG]: https://github.com/aws/aws-sdk-go-v2/blob/main/CHANGELOG.md\n[Amazon DynamoDB]: https://aws.amazon.com/dynamodb/\n[design]: https://github.com/aws/aws-sdk-go-v2/blob/main/DESIGN.md\n[license]: http://aws.amazon.com/apache2.0/\n", "release_dates": ["2024-03-01T19:23:32Z", "2024-02-29T19:27:04Z", "2024-02-28T19:24:19Z", "2024-02-27T19:25:08Z", "2024-02-26T19:28:10Z", "2024-02-23T19:29:31Z", "2024-02-22T19:40:59Z", "2024-02-21T19:28:06Z", "2024-02-20T19:23:58Z", "2024-02-19T19:23:45Z", "2024-02-16T19:25:18Z", "2024-02-15T19:26:02Z", "2024-02-14T19:25:07Z", "2024-02-13T19:25:35Z", "2024-02-12T19:24:27Z", "2024-02-09T19:25:37Z", "2024-02-08T19:21:33Z", "2024-02-07T19:22:17Z", "2024-02-06T19:26:07Z", "2024-02-05T19:22:20Z", "2024-02-02T19:23:18Z", "2024-02-01T19:26:28Z", "2024-01-31T19:23:17Z", "2024-01-30T19:22:21Z", "2024-01-29T19:29:08Z", "2024-01-26T19:26:04Z", "2024-01-25T19:34:28Z", "2024-01-24T19:27:18Z", "2024-01-23T20:48:47Z", "2024-01-22T19:29:40Z"]}, {"name": "aws-sdk-java", "description": "The official AWS SDK for Java 1.x. The AWS SDK for Java 2.x is available here: https://github.com/aws/aws-sdk-java-v2/", "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for Java\n\nThe **AWS SDK for Java** enables Java developers to easily work with [Amazon Web Services][aws] and\nbuild scalable solutions with Amazon S3, Amazon DynamoDB, Amazon Glacier, and more. You can get\nstarted in minutes using ***Maven*** or by downloading a [single zip file][install-jar].\n\n## End-of-Support Announcement ##\n\nWe [announced][deprecation-announcement-post] the upcoming end-of-support for AWS SDK for Java (v1). We recommend that you migrate to \n[AWS SDK for Java v2][sdk-v2-dev-guide]. For dates, additional details, and information on how to migrate, please refer \nto the linked announcement. \n\n\n## Release Notes ##\nChanges to the SDK beginning with version 1.12.1 (June 2021) are tracked in [CHANGELOG.md][changes-file].\n\nChanges in the _retired_ 1.11.x series of the SDK, beginning with version 1.11.82, \nare listed in the [CHANGELOG-1.11.x.md](./changelogs/CHANGELOG-1.11.x.md) file.\n\n## Getting Started\n\n* [SDK Homepage][sdk-website]\n* [API Docs][docs-api]\n* [Developer Guide][docs-guide]\n* [Issues][sdk-issues]\n* [SDK Blog][blog]\n* [Getting Help](#getting-help)\n\n#### Sign up for AWS ####\n\nBefore you begin, you need an AWS account. Please see the [Sign Up for AWS][docs-signup] section of\nthe developer guide for information about how to create an AWS account and retrieve your AWS\ncredentials.\n\n#### Minimum requirements ####\n\nTo run the SDK you will need **Java 1.7+**. For more information about the requirements and optimum\nsettings for the SDK, please see the [Installing a Java Development Environment][docs-java-env]\nsection of the developer guide.\n\n#### Install the SDK ####\n\nThe recommended way to use the AWS SDK for Java in your project is to consume it from Maven. Import\nthe [aws-java-sdk-bom][] and specify the SDK Maven modules that your project needs in the\ndependencies.\n\n##### Importing the BOM #####\n\n```xml\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>com.amazonaws</groupId>\n      <artifactId>aws-java-sdk-bom</artifactId>\n      <version>1.12.671</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n```\n\n##### Using the SDK Maven modules #####\n\n```xml\n<dependencies>\n  <dependency>\n    <groupId>com.amazonaws</groupId>\n    <artifactId>aws-java-sdk-ec2</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>com.amazonaws</groupId>\n    <artifactId>aws-java-sdk-s3</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>com.amazonaws</groupId>\n    <artifactId>aws-java-sdk-dynamodb</artifactId>\n  </dependency>\n</dependencies>\n```\n\nSee the [Set up the AWS SDK for Java][docs-setup] section of the developer guide for more\ninformation about installing the SDK through other means.\n\n## Features\n\n* Provides easy-to-use HTTP clients for all supported AWS services, regions, and authentication\n    protocols.\n\n* Client-Side Data Encryption for Amazon S3 - Helps improve the security of storing application data\n    in Amazon S3.\n\n* Amazon DynamoDB Object Mapper - Uses Plain Old Java Object (POJOs) to store and retrieve Amazon\n    DynamoDB data.\n\n* Amazon S3 Transfer Manager - With a simple API, achieve enhanced the throughput, performance, and\n    reliability by using multi-threaded Amazon S3 multipart calls.\n\n* Amazon SQS Client-Side Buffering - Collect and send SQS requests in asynchronous batches,\n    improving application and network performance.\n\n* Automatically uses [IAM Instance Profile Credentials][aws-iam-credentials] on configured Amazon\n    EC2 instances.\n\n* And more!\n\n## Building From Source\n\nOnce you check out the code from GitHub, you can build it using Maven. To disable the GPG-signing\nin the build, use:\n\n```sh\nmvn clean install -Dgpg.skip=true\n```\n\n## Getting Help\nGitHub [issues][sdk-issues] is the preferred channel to interact with our team. Also check these community resources for getting help:\n\n* Ask a question on [StackOverflow][stack-overflow] and tag it with `aws-java-sdk`\n* Articulate your feature request or upvote existing ones on our [Issues][features] page\n* Take a look at the [blog] for plenty of helpful walkthroughs and tips\n* Open a case via the [AWS Support Center][support-center] in the [AWS console][console]\n* If it turns out that you may have found a bug, please open an [issue][sdk-issues]\n\n## Maintenance and Support for SDK Major Versions\nFor information about maintenance and support for SDK major versions and their underlying dependencies, see the following in the AWS SDKs and Tools Reference Guide:\n\n* [AWS SDKs and Tools Maintenance Policy][maintenance-policy]\n* [AWS SDKs and Tools Version Support Matrix][version-matrix]\n\n## Supported Minor Versions\n\n* **1.12.x** - Recommended.\n\n* **1.11.x** - No longer supported, but migration to 1.12.x should require no code changes.\n\n## AWS SDK for Java 2.x\nA version 2.x of the SDK is generally available. It is a major rewrite of the 1.x code base, built on top of Java 8+ and adds several frequently requested features. These include support for non-blocking I/O, improved start-up performance, automatic iteration over paginated responses and the ability to plug in a different HTTP implementation at run time.\n\nFor more information see the [AWS SDK for Java 2.x Developer Guide][sdk-v2-dev-guide] or check the project repository in https://github.com/aws/aws-sdk-java-v2.\n\n## Maintenance and Support for Java Versions\n\nThe AWS Java SDK version 1 (v1) supports Java versions from 7 to 16. The Java 17 version introduces strong encapsulation of internal Java elements, which is not backwards-compatible with the Java SDK v1. \nThis may cause issues for certain use-cases of the SDK. If you plan to use Java 17+, we recommend that you migrate to\n[AWS SDK for Java 2.x][aws-sdk-for-java-2x] that fully supports Java 8, Java 11, and Java 17 Long-Term Support(LTS) releases.\n\nIf you are experiencing issues with Java 17+ and unable to migrate to AWS SDK for Java v2 at this time, below are the workarounds that you might find helpful.\nPlease keep in mind that these workarounds may not work in the future \nversions of Java. See [JEP 403: Strongly Encapsulate JDK Internals][jep-403]\nand [Breaking Encapsulation][jep-break-encapsulation]\nfor more details.\n\n**Error: com.amazonaws.AmazonServiceException: Unable to unmarshall\nexception response with the unmarshallers provided caused by java.lang.\nreflect.InaccessibleObjectException**\n\n- use JVM option `--add-opens java.base/java.lang=ALL-UNNAMED` at JVM startup\n\n**WARNING: Illegal reflective access by com.amazonaws.util.XpathUtils**\n\n- use JVM option `--add-opens=java.xml/com.sun.org.apache.xpath.internal=ALL-UNNAMED` at JVM startup\n\n[aws-iam-credentials]: https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/java-dg-roles.html\n[aws]: https://aws.amazon.com/\n[blog]: https://aws.amazon.com/blogs/developer/category/java/\n[docs-api]: https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html\n[docs-guide]: https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/welcome.html\n[docs-guide-source]: https://github.com/awsdocs/aws-java-developer-guide\n[docs-java-env]: https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-install.html#installing-a-java-development-environment\n[docs-signup]: https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/signup-create-iam-user.html\n[docs-setup]: https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-install.html\n[install-jar]: https://sdk-for-java.amazonwebservices.com/latest/aws-java-sdk.zip\n[sdk-issues]: https://github.com/aws/aws-sdk-java/issues\n[sdk-license]: https://aws.amazon.com/apache2.0/\n[sdk-website]: https://aws.amazon.com/sdkforjava\n[aws-java-sdk-bom]: https://github.com/aws/aws-sdk-java/tree/master/aws-java-sdk-bom\n[release-notes-catalog]: https://aws.amazon.com/releasenotes/Java?browse=1\n[changes-file]: ./CHANGELOG.md\n[stack-overflow]: https://stackoverflow.com/questions/tagged/aws-java-sdk\n[features]: https://github.com/aws/aws-sdk-java/issues?q=is%3Aopen+is%3Aissue+label%3A%22feature-request%22\n[support-center]: https://console.aws.amazon.com/support/\n[console]: https://console.aws.amazon.com\n[jackson-deserialization-gadget]: https://medium.com/@cowtowncoder/on-jackson-cves-dont-panic-here-is-what-you-need-to-know-54cd0d6e8062\n[sdk-v2-dev-guide]: https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/welcome.html\n[maintenance-policy]: https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html\n[version-matrix]: https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html\n[jep-break-encapsulation]: https://openjdk.org/jeps/261#Breaking-encapsulation\n[jep-403]: https://openjdk.org/jeps/403\n[aws-sdk-for-java-2x]: https://github.com/aws/aws-sdk-java-v2\n[deprecation-announcement-post]: https://aws.amazon.com/blogs/developer/announcing-end-of-support-for-aws-sdk-for-java-v1-x-on-december-31-2025\n", "release_dates": []}, {"name": "aws-sdk-java-archetype", "description": null, "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "aws-sdk-java-archetype\n======================\n\nThe AWS SDK for Java Maven archetype allows you to easily create new Java projects configured with the [AWS SDK for Java](http://aws.amazon.com/sdkforjava/) and some sample code to help you find your way around the SDK.\n\nStarting a project from the archetype is easy:\n<pre>\nmvn archetype:generate \\\n     -DarchetypeGroupId=com.amazonaws \\\n     -DarchetypeArtifactId=aws-java-sdk-archetype\n</pre>\n\nWhen you run the Maven <code>archetype:generate</code> goal, you'll be prompted for some basic Maven values for your new project <i>(<a href=\"http://maven.apache.org/guides/mini/guide-naming-conventions.html\">groupId, artifactId, version</a>)</i>.\n\n<pre>\n[INFO] Generating project in Interactive mode\n[INFO] Archetype [com.amazonaws:aws-java-sdk-archetype:1.0.0] found in catalog local\nDefine value for property 'groupId': : com.foo   \nDefine value for property 'artifactId': : my-aws-java-project\nDefine value for property 'version':  1.0-SNAPSHOT: : \nDefine value for property 'package':  com.foo: : \n</pre>\n\nWhen the <code>archetype:generate</code> goal completes, you'll have a new Maven Java project, already configured with a dependency on the AWS SDK for Java and some sample code in the project to help you get started with the SDK.\n\nBefore you run the sample code, you'll need to fill in your AWS security credentials.  This sample loads your credentials from the default credentials profiles file <i>(~/.aws/credentials)</i>.  If you've already set up your profile credentials file, then you're ready to run the sample.  If you haven't set up your <code>~/.aws/credentials</code> file yet, here's what you should start with:\n<pre>\n[default]\naws_access_key_id     = <your AWS access key>\naws_secret_access_key = <your AWS secret access key>\n</pre>\n\nWhen your AWS security credentials are configured, you're ready to compile and run the new project.  The sample project's POM file is configured so that you can easily compile, jar, and run the project by executing <code>mvn package exec:java</code>.  \n", "release_dates": []}, {"name": "aws-sdk-java-v2", "description": "The official AWS SDK for Java - Version 2", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for Java 2.0\n![Build Status](https://codebuild.us-west-2.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiTFJSRXBBN1hkU1ZEQzZ4M1hoaWlFUExuNER3WjNpVllSQ09Qam1YdFlTSDNTd3RpZzNia3F0VkJRUTBwZlQwR1BEelpSV2dWVnp4YTBCOFZKRzRUR004PSIsIml2UGFyYW1ldGVyU3BlYyI6ImdHdEp1UHhKckpDRmhmQU4iLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=master)\n[![Maven](https://img.shields.io/maven-central/v/software.amazon.awssdk/s3.svg?label=Maven)](https://search.maven.org/search?q=g:%22software.amazon.awssdk%22%20AND%20a:%22s3%22)\n[![Gitter](https://badges.gitter.im/aws/aws-sdk-java-v2.svg)](https://gitter.im/aws/aws-sdk-java-v2?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge) \n[![codecov](https://codecov.io/gh/aws/aws-sdk-java-v2/branch/master/graph/badge.svg)](https://codecov.io/gh/aws/aws-sdk-java-v2)\n<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n[![All Contributors](https://img.shields.io/badge/all_contributors-106-orange.svg?style=flat-square)](#contributors-)\n<!-- ALL-CONTRIBUTORS-BADGE:END -->\n\nThe **AWS SDK for Java 2.0** is a rewrite of 1.0 with some great new features. As with version 1.0,\nit enables you to easily work with [Amazon Web Services][aws] but also includes features like\nnon-blocking IO and pluggable HTTP implementation to further customize your applications. You can\nget started in minutes using ***Maven*** or any build system that supports MavenCentral as an\nartifact source.\n\n* [SDK Homepage][sdk-website]\n* [1.11 to 2.0 Changelog](docs/LaunchChangelog.md)\n* [Best Practices](docs/BestPractices.md)\n* [Sample Code](#sample-code)\n* [API Docs][docs-api]\n* [Developer Guide][docs-guide] ([source][docs-guide-source])\n* [Maven Archetypes](archetypes/README.md)\n* [Issues][sdk-issues]\n* [SDK Blog][blog]\n* [Giving Feedback](#giving-feedback)\n\n## Getting Started\n\n#### Sign up for AWS ####\n\nBefore you begin, you need an AWS account. Please see the [Sign Up for AWS][docs-signup] section of\nthe developer guide for information about how to create an AWS account and retrieve your AWS\ncredentials.\n\n#### Minimum requirements ####\n\nTo run the SDK you will need **Java 1.8+**. For more information about the requirements and optimum\nsettings for the SDK, please see the [Installing a Java Development Environment][docs-java-env]\nsection of the developer guide.\n\n## Using the SDK\n\nThe recommended way to use the AWS SDK for Java in your project is to consume it from Maven Central. \n\n#### Importing the BOM ####\n\nTo automatically manage module versions (currently all modules have the same version, but this may not always be the case) we recommend you use the [Bill of Materials][bom] import as follows:\n\n```xml\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>software.amazon.awssdk</groupId>\n      <artifactId>bom</artifactId>\n      <version>2.25.1</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n```\n\nThen individual modules may omit the `version` from their dependency statement:\n\n```xml\n<dependencies>\n  <dependency>\n    <groupId>software.amazon.awssdk</groupId>\n    <artifactId>ec2</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>software.amazon.awssdk</groupId>\n    <artifactId>s3</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>software.amazon.awssdk</groupId>\n    <artifactId>dynamodb</artifactId>\n  </dependency>\n</dependencies>\n```\n#### Individual Services ####\n\nAlternatively you can add dependencies for the specific services you use only:\n\n```xml\n<dependency>\n  <groupId>software.amazon.awssdk</groupId>\n  <artifactId>ec2</artifactId>\n  <version>2.25.1</version>\n</dependency>\n<dependency>\n  <groupId>software.amazon.awssdk</groupId>\n  <artifactId>s3</artifactId>\n  <version>2.25.1</version>\n</dependency>\n```\n\n#### Whole SDK ####\n\nYou can import the whole SDK into your project (includes *ALL* services). Please note that it is recommended to only import the modules you need.\n\n```xml\n<dependency>\n  <groupId>software.amazon.awssdk</groupId>\n  <artifactId>aws-sdk-java</artifactId>\n  <version>2.25.1</version>\n</dependency>\n```\n\nSee the [Set up the AWS SDK for Java][docs-setup] section of the developer guide for more usage information.\n\n## New Features for 2.0\n\n* Provides a way to plug in your own HTTP implementation.\n\n* Provides first class support for non-blocking IO in Async clients.\n\n## Building From Source\n\nOnce you check out the code from GitHub, you can build it using the following commands.\n\nLinux:\n\n```sh\n./mvnw clean install\n\n# Skip tests, checkstyles, findbugs, etc for quick build\n./mvnw clean install -P quick\n\n# Build a specific service module\n./mvnw clean install -pl :s3 -P quick --am\n```\n\nWindows:\n```sh\n./mvnw.cmd clean install\n```\n\n## Sample Code\nYou can find sample code for v2 in the following places:\n\n* [aws-doc-sdk-examples] repo.\n* Integration tests in this repo. They are located in the `it` directory under each service module, eg: [s3-integration-tests]\n\n## Maintenance and Support for SDK Major Versions\nFor information about maintenance and support for SDK major versions and their underlying dependencies, see the following in the AWS SDKs and Tools Reference Guide:\n\n* [AWS SDKs and Tools Maintenance Policy][maintenance-policy]\n* [AWS SDKs and Tools Version Support Matrix][version-matrix]\n\n## Maintenance and Support for Java Versions\n\nWe maintain full support on Long-Term Support(LTS) releases: Java 8, Java 11, Java 17, and Java 21.\n\n## Giving Feedback\nWe need your help in making this SDK great. Please participate in the community and contribute to this effort by submitting issues, participating in discussion forums and submitting pull requests through the following channels:\n\n* Submit [issues][sdk-issues] - this is the **preferred** channel to interact with our team\n* Articulate your feature request or upvote existing ones on our [Issues][features] page\n\n[aws-iam-credentials]: http://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/java-dg-roles.html\n[aws]: http://aws.amazon.com/\n[blog]: https://aws.amazon.com/blogs/developer/category/java/\n[docs-api]: https://sdk.amazonaws.com/java/api/latest/overview-summary.html\n[docs-guide]: http://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/welcome.html\n[docs-guide-source]: https://github.com/awsdocs/aws-java-developer-guide-v2\n[docs-java-env]: http://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/setup-install.html##java-dg-java-env\n[docs-signup]: http://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/signup-create-iam-user.html\n[docs-setup]: http://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/setup-install.html\n[sdk-issues]: https://github.com/aws/aws-sdk-java-v2/issues\n[sdk-license]: http://aws.amazon.com/apache2.0/\n[sdk-website]: http://aws.amazon.com/sdkforjava\n[aws-java-sdk-bom]: https://github.com/aws/aws-sdk-java-v2/tree/master/bom\n[stack-overflow]: http://stackoverflow.com/questions/tagged/aws-java-sdk\n[gitter]: https://gitter.im/aws/aws-sdk-java-v2\n[features]: https://github.com/aws/aws-sdk-java-v2/issues?q=is%3Aopen+is%3Aissue+label%3A%22feature-request%22\n[support-center]: https://console.aws.amazon.com/support/\n[console]: https://console.aws.amazon.com\n[bom]: http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22software.amazon.awssdk%22%20AND%20a%3A%22bom%22\n[aws-doc-sdk-examples]: https://github.com/awsdocs/aws-doc-sdk-examples/tree/master/javav2\n[s3-integration-tests]: https://github.com/aws/aws-sdk-java-v2/tree/master/services/s3/src/it/java/software/amazon/awssdk/services/s3\n[maintenance-policy]: https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html\n[version-matrix]: https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html\n\n## Contributors \u2728\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sullis\"><img src=\"https://avatars.githubusercontent.com/u/30938?v=4?s=100\" width=\"100px;\" alt=\"sullis\"/><br /><sub><b>sullis</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=sullis\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/abrooksv\"><img src=\"https://avatars.githubusercontent.com/u/8992246?v=4?s=100\" width=\"100px;\" alt=\"Austin Brooks\"/><br /><sub><b>Austin Brooks</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=abrooksv\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ktoso\"><img src=\"https://avatars.githubusercontent.com/u/120979?v=4?s=100\" width=\"100px;\" alt=\"Konrad `ktoso` Malawski\"/><br /><sub><b>Konrad `ktoso` Malawski</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=ktoso\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/andrewhop\"><img src=\"https://avatars.githubusercontent.com/u/41167468?v=4?s=100\" width=\"100px;\" alt=\"Andrew Hopkins\"/><br /><sub><b>Andrew Hopkins</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=andrewhop\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/adamthom-amzn\"><img src=\"https://avatars.githubusercontent.com/u/61852529?v=4?s=100\" width=\"100px;\" alt=\"Adam Thomas\"/><br /><sub><b>Adam Thomas</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=adamthom-amzn\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sworisbreathing\"><img src=\"https://avatars.githubusercontent.com/u/1486524?v=4?s=100\" width=\"100px;\" alt=\"Steven Swor\"/><br /><sub><b>Steven Swor</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=sworisbreathing\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Carey-AWS\"><img src=\"https://avatars.githubusercontent.com/u/61763083?v=4?s=100\" width=\"100px;\" alt=\"Carey Burgess\"/><br /><sub><b>Carey Burgess</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=Carey-AWS\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/anuraaga\"><img src=\"https://avatars.githubusercontent.com/u/198344?v=4?s=100\" width=\"100px;\" alt=\"Anuraag Agrawal\"/><br /><sub><b>Anuraag Agrawal</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=anuraaga\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jeffalder\"><img src=\"https://avatars.githubusercontent.com/u/49817386?v=4?s=100\" width=\"100px;\" alt=\"jeffalder\"/><br /><sub><b>jeffalder</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=jeffalder\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dotbg\"><img src=\"https://avatars.githubusercontent.com/u/367403?v=4?s=100\" width=\"100px;\" alt=\"Boris\"/><br /><sub><b>Boris</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=dotbg\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/notdryft\"><img src=\"https://avatars.githubusercontent.com/u/2608594?v=4?s=100\" width=\"100px;\" alt=\"Guillaume Corr\u00e9\"/><br /><sub><b>Guillaume Corr\u00e9</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=notdryft\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/hyandell\"><img src=\"https://avatars.githubusercontent.com/u/477715?v=4?s=100\" width=\"100px;\" alt=\"Henri Yandell\"/><br /><sub><b>Henri Yandell</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=hyandell\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/rschmitt\"><img src=\"https://avatars.githubusercontent.com/u/3725049?v=4?s=100\" width=\"100px;\" alt=\"Ryan Schmitt\"/><br /><sub><b>Ryan Schmitt</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=rschmitt\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/SomayaB\"><img src=\"https://avatars.githubusercontent.com/u/23043132?v=4?s=100\" width=\"100px;\" alt=\"Somaya\"/><br /><sub><b>Somaya</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=SomayaB\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/steven-aerts\"><img src=\"https://avatars.githubusercontent.com/u/1381633?v=4?s=100\" width=\"100px;\" alt=\"Steven Aerts\"/><br /><sub><b>Steven Aerts</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=steven-aerts\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/skwslide\"><img src=\"https://avatars.githubusercontent.com/u/1427510?v=4?s=100\" width=\"100px;\" alt=\"Steven Wong\"/><br /><sub><b>Steven Wong</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=skwslide\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/telendt\"><img src=\"https://avatars.githubusercontent.com/u/85191?v=4?s=100\" width=\"100px;\" alt=\"Tomasz Elendt\"/><br /><sub><b>Tomasz Elendt</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=telendt\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Sarev0k\"><img src=\"https://avatars.githubusercontent.com/u/8388574?v=4?s=100\" width=\"100px;\" alt=\"Will Erickson\"/><br /><sub><b>Will Erickson</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=Sarev0k\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/madgnome\"><img src=\"https://avatars.githubusercontent.com/u/279528?v=4?s=100\" width=\"100px;\" alt=\"Julien Hoarau\"/><br /><sub><b>Julien Hoarau</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=madgnome\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/SEOKHYOENCHOI\"><img src=\"https://avatars.githubusercontent.com/u/42906668?v=4?s=100\" width=\"100px;\" alt=\"SEOKHYOENCHOI\"/><br /><sub><b>SEOKHYOENCHOI</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=SEOKHYOENCHOI\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/adriannistor\"><img src=\"https://avatars.githubusercontent.com/u/3051958?v=4?s=100\" width=\"100px;\" alt=\"adriannistor\"/><br /><sub><b>adriannistor</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=adriannistor\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/alicesun16\"><img src=\"https://avatars.githubusercontent.com/u/56938110?v=4?s=100\" width=\"100px;\" alt=\"Xian Sun \"/><br /><sub><b>Xian Sun </b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=alicesun16\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ascheja\"><img src=\"https://avatars.githubusercontent.com/u/3932118?v=4?s=100\" width=\"100px;\" alt=\"Andreas Scheja\"/><br /><sub><b>Andreas Scheja</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=ascheja\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/antegocanva\"><img src=\"https://avatars.githubusercontent.com/u/43571020?v=4?s=100\" width=\"100px;\" alt=\"Anton Egorov\"/><br /><sub><b>Anton Egorov</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=antegocanva\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/roexber\"><img src=\"https://avatars.githubusercontent.com/u/7964627?v=4?s=100\" width=\"100px;\" alt=\"roexber\"/><br /><sub><b>roexber</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=roexber\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/brharrington\"><img src=\"https://avatars.githubusercontent.com/u/1289028?v=4?s=100\" width=\"100px;\" alt=\"brharrington\"/><br /><sub><b>brharrington</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=brharrington\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/chrisradek\"><img src=\"https://avatars.githubusercontent.com/u/14189820?v=4?s=100\" width=\"100px;\" alt=\"Christopher Radek\"/><br /><sub><b>Christopher Radek</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=chrisradek\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/zakkak\"><img src=\"https://avatars.githubusercontent.com/u/1435395?v=4?s=100\" width=\"100px;\" alt=\"Foivos\"/><br /><sub><b>Foivos</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=zakkak\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/superwese\"><img src=\"https://avatars.githubusercontent.com/u/954116?v=4?s=100\" width=\"100px;\" alt=\"Frank Wesemann\"/><br /><sub><b>Frank Wesemann</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=superwese\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sperka\"><img src=\"https://avatars.githubusercontent.com/u/157324?v=4?s=100\" width=\"100px;\" alt=\"Gergely Varga\"/><br /><sub><b>Gergely Varga</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=sperka\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/GuillermoBlasco\"><img src=\"https://avatars.githubusercontent.com/u/1889971?v=4?s=100\" width=\"100px;\" alt=\"Guillermo\"/><br /><sub><b>Guillermo</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=GuillermoBlasco\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/rce\"><img src=\"https://avatars.githubusercontent.com/u/4427896?v=4?s=100\" width=\"100px;\" alt=\"Henry Heikkinen\"/><br /><sub><b>Henry Heikkinen</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=rce\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/joschi\"><img src=\"https://avatars.githubusercontent.com/u/43951?v=4?s=100\" width=\"100px;\" alt=\"Jochen Schalanda\"/><br /><sub><b>Jochen Schalanda</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=joschi\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/josephlbarnett\"><img src=\"https://avatars.githubusercontent.com/u/13838924?v=4?s=100\" width=\"100px;\" alt=\"Joe Barnett\"/><br /><sub><b>Joe Barnett</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=josephlbarnett\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/seratch\"><img src=\"https://avatars.githubusercontent.com/u/19658?v=4?s=100\" width=\"100px;\" alt=\"Kazuhiro Sera\"/><br /><sub><b>Kazuhiro Sera</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=seratch\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ChaithanyaGK\"><img src=\"https://avatars.githubusercontent.com/u/28896513?v=4?s=100\" width=\"100px;\" alt=\"Krishna Chaithanya Ganta\"/><br /><sub><b>Krishna Chaithanya Ganta</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=ChaithanyaGK\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/leepa\"><img src=\"https://avatars.githubusercontent.com/u/9469?v=4?s=100\" width=\"100px;\" alt=\"Lee Packham\"/><br /><sub><b>Lee Packham</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=leepa\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/MatteCarra\"><img src=\"https://avatars.githubusercontent.com/u/11074527?v=4?s=100\" width=\"100px;\" alt=\"Matteo Carrara\"/><br /><sub><b>Matteo Carrara</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=MatteCarra\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mscharp\"><img src=\"https://avatars.githubusercontent.com/u/1426929?v=4?s=100\" width=\"100px;\" alt=\"Michael Scharp\"/><br /><sub><b>Michael Scharp</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=mscharp\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/miguelrjim\"><img src=\"https://avatars.githubusercontent.com/u/1420241?v=4?s=100\" width=\"100px;\" alt=\"Miguel Jimenez\"/><br /><sub><b>Miguel Jimenez</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=miguelrjim\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Helmsdown\"><img src=\"https://avatars.githubusercontent.com/u/1689115?v=4?s=100\" width=\"100px;\" alt=\"Russell Bolles\"/><br /><sub><b>Russell Bolles</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=Helmsdown\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/scheerer\"><img src=\"https://avatars.githubusercontent.com/u/4659?v=4?s=100\" width=\"100px;\" alt=\"Russell Scheerer\"/><br /><sub><b>Russell Scheerer</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=scheerer\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/scotty-g\"><img src=\"https://avatars.githubusercontent.com/u/7861050?v=4?s=100\" width=\"100px;\" alt=\"Scott\"/><br /><sub><b>Scott</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=scotty-g\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ueokande\"><img src=\"https://avatars.githubusercontent.com/u/534166?v=4?s=100\" width=\"100px;\" alt=\"Shin'ya Ueoka\"/><br /><sub><b>Shin'ya Ueoka</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=ueokande\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sushilamazon\"><img src=\"https://avatars.githubusercontent.com/u/42008398?v=4?s=100\" width=\"100px;\" alt=\"sushilamazon\"/><br /><sub><b>sushilamazon</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=sushilamazon\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/tomliu4uber\"><img src=\"https://avatars.githubusercontent.com/u/22459891?v=4?s=100\" width=\"100px;\" alt=\"tomliu4uber\"/><br /><sub><b>tomliu4uber</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=tomliu4uber\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/musketyr\"><img src=\"https://avatars.githubusercontent.com/u/660405?v=4?s=100\" width=\"100px;\" alt=\"Vladimir Orany\"/><br /><sub><b>Vladimir Orany</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=musketyr\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Xinyu-Hu\"><img src=\"https://avatars.githubusercontent.com/u/31017838?v=4?s=100\" width=\"100px;\" alt=\"Xinyu Hu\"/><br /><sub><b>Xinyu Hu</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=Xinyu-Hu\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/frosforever\"><img src=\"https://avatars.githubusercontent.com/u/1630422?v=4?s=100\" width=\"100px;\" alt=\"Yosef Fertel\"/><br /><sub><b>Yosef Fertel</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=frosforever\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/denyskonakhevych\"><img src=\"https://avatars.githubusercontent.com/u/5894907?v=4?s=100\" width=\"100px;\" alt=\"Denys Konakhevych\"/><br /><sub><b>Denys Konakhevych</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=denyskonakhevych\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/alexw91\"><img src=\"https://avatars.githubusercontent.com/u/3596374?v=4?s=100\" width=\"100px;\" alt=\"Alex Weibel\"/><br /><sub><b>Alex Weibel</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=alexw91\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/rccarper\"><img src=\"https://avatars.githubusercontent.com/u/51676630?v=4?s=100\" width=\"100px;\" alt=\"Ryan Carper\"/><br /><sub><b>Ryan Carper</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=rccarper\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/JonathanHenson\"><img src=\"https://avatars.githubusercontent.com/u/3926469?v=4?s=100\" width=\"100px;\" alt=\"Jonathan M. Henson\"/><br /><sub><b>Jonathan M. Henson</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=JonathanHenson\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/debora-ito\"><img src=\"https://avatars.githubusercontent.com/u/476307?v=4?s=100\" width=\"100px;\" alt=\"Debora N. Ito\"/><br /><sub><b>Debora N. Ito</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=debora-ito\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bretambrose\"><img src=\"https://avatars.githubusercontent.com/u/341314?v=4?s=100\" width=\"100px;\" alt=\"Bret Ambrose\"/><br /><sub><b>Bret Ambrose</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=bretambrose\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/cenedhryn\"><img src=\"https://avatars.githubusercontent.com/u/26603446?v=4?s=100\" width=\"100px;\" alt=\"Anna-Karin Salander\"/><br /><sub><b>Anna-Karin Salander</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=cenedhryn\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/joviegas\"><img src=\"https://avatars.githubusercontent.com/u/70235430?v=4?s=100\" width=\"100px;\" alt=\"John Viegas\"/><br /><sub><b>John Viegas</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=joviegas\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dagnir\"><img src=\"https://avatars.githubusercontent.com/u/261310?v=4?s=100\" width=\"100px;\" alt=\"Dongie Agnir\"/><br /><sub><b>Dongie Agnir</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=dagnir\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/millems\"><img src=\"https://avatars.githubusercontent.com/u/24903526?v=4?s=100\" width=\"100px;\" alt=\"Matthew Miller\"/><br /><sub><b>Matthew Miller</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=millems\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bmaizels\"><img src=\"https://avatars.githubusercontent.com/u/36682168?v=4?s=100\" width=\"100px;\" alt=\"Benjamin Maizels\"/><br /><sub><b>Benjamin Maizels</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=bmaizels\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Quanzzzz\"><img src=\"https://avatars.githubusercontent.com/u/51490885?v=4?s=100\" width=\"100px;\" alt=\"Quan Zhou\"/><br /><sub><b>Quan Zhou</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=Quanzzzz\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/zoewangg\"><img src=\"https://avatars.githubusercontent.com/u/33073555?v=4?s=100\" width=\"100px;\" alt=\"Zoe Wang\"/><br /><sub><b>Zoe Wang</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=zoewangg\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/varunnvs92\"><img src=\"https://avatars.githubusercontent.com/u/17261531?v=4?s=100\" width=\"100px;\" alt=\"Varun Nandi\"/><br /><sub><b>Varun Nandi</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=varunnvs92\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/shorea\"><img src=\"https://avatars.githubusercontent.com/u/11096681?v=4?s=100\" width=\"100px;\" alt=\"Andrew Shore\"/><br /><sub><b>Andrew Shore</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=shorea\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kiiadi\"><img src=\"https://avatars.githubusercontent.com/u/4661536?v=4?s=100\" width=\"100px;\" alt=\"Kyle Thomson\"/><br /><sub><b>Kyle Thomson</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=kiiadi\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/spfink\"><img src=\"https://avatars.githubusercontent.com/u/20525381?v=4?s=100\" width=\"100px;\" alt=\"Sam Fink\"/><br /><sub><b>Sam Fink</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=spfink\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bondj\"><img src=\"https://avatars.githubusercontent.com/u/4749778?v=4?s=100\" width=\"100px;\" alt=\"Jonathan Bond\"/><br /><sub><b>Jonathan Bond</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=bondj\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ajs139\"><img src=\"https://avatars.githubusercontent.com/u/9387176?v=4?s=100\" width=\"100px;\" alt=\"ajs139\"/><br /><sub><b>ajs139</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=ajs139\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://imdewey.com\"><img src=\"https://avatars.githubusercontent.com/u/44629464?v=4?s=100\" width=\"100px;\" alt=\"Dewey Nguyen\"/><br /><sub><b>Dewey Nguyen</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=duy3101\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dleen\"><img src=\"https://avatars.githubusercontent.com/u/1297964?v=4?s=100\" width=\"100px;\" alt=\"David Leen\"/><br /><sub><b>David Leen</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=dleen\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://16lim21.github.io\"><img src=\"https://avatars.githubusercontent.com/u/53011962?v=4?s=100\" width=\"100px;\" alt=\"Michael Li\"/><br /><sub><b>Michael Li</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=16lim21\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Bennett-Lynch\"><img src=\"https://avatars.githubusercontent.com/u/11811448?v=4?s=100\" width=\"100px;\" alt=\"Bennett Lynch\"/><br /><sub><b>Bennett Lynch</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=Bennett-Lynch\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://bandism.net/\"><img src=\"https://avatars.githubusercontent.com/u/22633385?v=4?s=100\" width=\"100px;\" alt=\"Ikko Ashimine\"/><br /><sub><b>Ikko Ashimine</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=eltociear\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jamieliu.me\"><img src=\"https://avatars.githubusercontent.com/u/35614552?v=4?s=100\" width=\"100px;\" alt=\"Jamie Liu\"/><br /><sub><b>Jamie Liu</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=jamieliu386\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/guillepb10\"><img src=\"https://avatars.githubusercontent.com/u/28654665?v=4?s=100\" width=\"100px;\" alt=\"guillepb10\"/><br /><sub><b>guillepb10</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=guillepb10\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/lorenznickel/\"><img src=\"https://avatars.githubusercontent.com/u/29959150?v=4?s=100\" width=\"100px;\" alt=\"Lorenz Nickel\"/><br /><sub><b>Lorenz Nickel</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=LorenzNickel\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/erin889\"><img src=\"https://avatars.githubusercontent.com/u/38885911?v=4?s=100\" width=\"100px;\" alt=\"Erin Yang\"/><br /><sub><b>Erin Yang</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=erin889\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.theguardian.com/profile/roberto-tyley\"><img src=\"https://avatars.githubusercontent.com/u/52038?v=4?s=100\" width=\"100px;\" alt=\"Roberto Tyley\"/><br /><sub><b>Roberto Tyley</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=rtyley\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://alvinsee.com/\"><img src=\"https://avatars.githubusercontent.com/u/1531158?v=4?s=100\" width=\"100px;\" alt=\"Alvin See\"/><br /><sub><b>Alvin See</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=alvinsee\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ron1\"><img src=\"https://avatars.githubusercontent.com/u/1318509?v=4?s=100\" width=\"100px;\" alt=\"ron1\"/><br /><sub><b>ron1</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=ron1\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/srsaikumarreddy\"><img src=\"https://avatars.githubusercontent.com/u/24988810?v=4?s=100\" width=\"100px;\" alt=\"Sai Kumar Reddy Chandupatla\"/><br /><sub><b>Sai Kumar Reddy Chandupatla</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=srsaikumarreddy\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/davidh44\"><img src=\"https://avatars.githubusercontent.com/u/70000000?v=4?s=100\" width=\"100px;\" alt=\"David Ho\"/><br /><sub><b>David Ho</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=davidh44\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.berrycloud.co.uk\"><img src=\"https://avatars.githubusercontent.com/u/1552612?v=4?s=100\" width=\"100px;\" alt=\"Thomas Turrell-Croft\"/><br /><sub><b>Thomas Turrell-Croft</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=thomasturrell\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/stevenshan\"><img src=\"https://avatars.githubusercontent.com/u/3723174?v=4?s=100\" width=\"100px;\" alt=\"Steven Shan\"/><br /><sub><b>Steven Shan</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=stevenshan\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/barryoneill\"><img src=\"https://avatars.githubusercontent.com/u/885049?v=4?s=100\" width=\"100px;\" alt=\"Barry O'Neill\"/><br /><sub><b>Barry O'Neill</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=barryoneill\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/akiesler\"><img src=\"https://avatars.githubusercontent.com/u/4186292?v=4?s=100\" width=\"100px;\" alt=\"Andy Kiesler\"/><br /><sub><b>Andy Kiesler</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=akiesler\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.youtube.com/CodigoMorsa\"><img src=\"https://avatars.githubusercontent.com/u/21063181?v=4?s=100\" width=\"100px;\" alt=\"Martin\"/><br /><sub><b>Martin</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=martinKindall\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/paulolieuthier\"><img src=\"https://avatars.githubusercontent.com/u/1238157?v=4?s=100\" width=\"100px;\" alt=\"Paulo Lieuthier\"/><br /><sub><b>Paulo Lieuthier</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=paulolieuthier\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.inulogic.fr\"><img src=\"https://avatars.githubusercontent.com/u/88554524?v=4?s=100\" width=\"100px;\" alt=\"S\u00e9bastien Crocquesel\"/><br /><sub><b>S\u00e9bastien Crocquesel</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=scrocquesel\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dave-fn\"><img src=\"https://avatars.githubusercontent.com/u/21349334?v=4?s=100\" width=\"100px;\" alt=\"David Negrete\"/><br /><sub><b>David Negrete</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=dave-fn\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/StephenFlavin\"><img src=\"https://avatars.githubusercontent.com/u/14975957?v=4?s=100\" width=\"100px;\" alt=\"Stephen Flavin\"/><br /><sub><b>Stephen Flavin</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=StephenFlavin\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://applin.ca\"><img src=\"https://avatars.githubusercontent.com/u/16511950?v=4?s=100\" width=\"100px;\" alt=\"Olivier L Applin\"/><br /><sub><b>Olivier L Applin</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=L-Applin\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/breader124\"><img src=\"https://avatars.githubusercontent.com/u/36669019?v=4?s=100\" width=\"100px;\" alt=\"Adrian Chlebosz\"/><br /><sub><b>Adrian Chlebosz</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=breader124\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.buymeacoffee.com/chadwilson\"><img src=\"https://avatars.githubusercontent.com/u/29788154?v=4?s=100\" width=\"100px;\" alt=\"Chad Wilson\"/><br /><sub><b>Chad Wilson</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=chadlwilson\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ManishDait\"><img src=\"https://avatars.githubusercontent.com/u/90558243?v=4?s=100\" width=\"100px;\" alt=\"Manish Dait\"/><br /><sub><b>Manish Dait</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=ManishDait\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.dekies.de\"><img src=\"https://avatars.githubusercontent.com/u/858827?v=4?s=100\" width=\"100px;\" alt=\"Dennis Kieselhorst\"/><br /><sub><b>Dennis Kieselhorst</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=deki\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/psnilesh\"><img src=\"https://avatars.githubusercontent.com/u/12656997?v=4?s=100\" width=\"100px;\" alt=\"Nilesh PS\"/><br /><sub><b>Nilesh PS</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=psnilesh\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/swar8080\"><img src=\"https://avatars.githubusercontent.com/u/17691679?v=4?s=100\" width=\"100px;\" alt=\"Steven Swartz\"/><br /><sub><b>Steven Swartz</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=swar8080\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/michaeldimchuk\"><img src=\"https://avatars.githubusercontent.com/u/22773297?v=4?s=100\" width=\"100px;\" alt=\"Michael Dimchuk\"/><br /><sub><b>Michael Dimchuk</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=michaeldimchuk\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/faucct\"><img src=\"https://avatars.githubusercontent.com/u/5202503?v=4?s=100\" width=\"100px;\" alt=\"Nikita Sokolov\"/><br /><sub><b>Nikita Sokolov</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=faucct\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/anirudh9391\"><img src=\"https://avatars.githubusercontent.com/u/15699250?v=4?s=100\" width=\"100px;\" alt=\"Anirudh\"/><br /><sub><b>Anirudh</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=anirudh9391\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sugmanue\"><img src=\"https://avatars.githubusercontent.com/u/108146565?v=4?s=100\" width=\"100px;\" alt=\"Manuel Sugawara\"/><br /><sub><b>Manuel Sugawara</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=sugmanue\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/haydenbaker\"><img src=\"https://avatars.githubusercontent.com/u/26096419?v=4?s=100\" width=\"100px;\" alt=\"Hayden Baker\"/><br /><sub><b>Hayden Baker</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=haydenbaker\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/gosar\"><img src=\"https://avatars.githubusercontent.com/u/5666661?v=4?s=100\" width=\"100px;\" alt=\"Jaykumar Gosar\"/><br /><sub><b>Jaykumar Gosar</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=gosar\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/graebm\"><img src=\"https://avatars.githubusercontent.com/u/24399397?v=4?s=100\" width=\"100px;\" alt=\"Michael Graeb\"/><br /><sub><b>Michael Graeb</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=graebm\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mgrundie-r7\"><img src=\"https://avatars.githubusercontent.com/u/103498312?v=4?s=100\" width=\"100px;\" alt=\"Michael Grundie\"/><br /><sub><b>Michael Grundie</b></sub></a><br /><a href=\"https://github.com/aws/aws-sdk-java-v2/commits?author=mgrundie-r7\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!\n", "release_dates": ["2024-03-01T19:48:50Z", "2024-02-29T20:26:11Z", "2024-02-28T20:25:32Z", "2024-02-27T20:26:35Z", "2024-02-26T20:24:13Z", "2024-02-23T20:42:39Z", "2024-02-22T20:36:43Z", "2024-02-21T22:43:03Z", "2024-02-20T20:29:41Z", "2024-02-16T20:30:09Z", "2024-02-15T20:33:36Z", "2024-02-14T20:40:40Z", "2024-02-13T20:58:09Z", "2024-02-12T20:22:16Z", "2024-02-08T19:45:19Z", "2024-02-07T19:45:28Z", "2024-02-06T19:47:01Z", "2024-02-05T20:46:26Z"]}, {"name": "aws-sdk-js", "description": "AWS SDK for JavaScript in the browser and Node.js", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for JavaScript\n\n[![NPM version](https://img.shields.io/npm/v/aws-sdk.svg)](https://www.npmjs.com/package/aws-sdk)\n[![NPM downloads](https://img.shields.io/npm/dm/aws-sdk.svg)](https://www.npmjs.com/package/aws-sdk)\n[![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.svg)](https://gitter.im/aws/aws-sdk-js)\n\n[![Build Status](https://travis-ci.org/aws/aws-sdk-js.svg?branch=master)](https://travis-ci.org/aws/aws-sdk-js)\n[![Coverage Status](https://codecov.io/gh/aws/aws-sdk-js/branch/master/graph/badge.svg)](https://codecov.io/gh/aws/aws-sdk-js)\n[![Known Vulnerabilities](https://snyk.io/test/github/aws/aws-sdk-js/badge.svg)](https://snyk.io/test/github/aws/aws-sdk-js)\n\n## Version 3.x Now Available\nThe [version 3.x](https://github.com/aws/aws-sdk-js-v3) of the AWS SDK for JavaScript is generally available.\nFor more information see the [Developer Guide](https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/)\nor [API Reference](https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/index.html).\n\nFor release notes, see the [CHANGELOG](https://github.com/aws/aws-sdk-js/blob/master/CHANGELOG.md). Prior to v2.4.8, release notes can be found at https://aws.amazon.com/releasenotes/?tag=releasenotes%23keywords%23javascript\n\n## Version 2.x Support\nWe are formalizing our plans to make the *Maintenance Announcement (Phase 2)* for **AWS SDK for JavaScript v2** in early 2024.\nPlease refer to the [AWS SDKs and Tools maintenance policy][aws-sdks-maintenance-policy] for further details.\n\n[**AWS SDK for JavaScript v3**][aws-sdk-js-v3] is the latest and recommended version, \nwhich has been GA since December 2020. Here is [why and how you should use\n**AWS SDK for JavaScript v3**][v3-recommended-blog]. You can try our experimental\nmigration scripts in [aws-sdk-js-codemod][aws-sdk-js-codemod] to migrate\nyour application from v2 to v3.\n\nTo get help with your migration, please follow our general guidelines to \n[open an issue][v2-new-issue] and choose [guidance][open-issue-v2-guidance].\nTo give feedback on and report issues in the v3 repo, please refer to \n[Giving feedback and contributing][v3-contributing].\n\nWatch this README and the [AWS Developer Tools Blog][aws-devtools-blog]\nfor updates and announcements regarding the maintenance plans and timelines.\n\nA maintenance mode message may be emitted by this package on startup. \nTo suppress this message, use an environment variable:\n\n```sh\nAWS_SDK_JS_SUPPRESS_MAINTENANCE_MODE_MESSAGE=1 node my_program.js\n```\n\nor a JavaScript setting as follows:\n```js\nvar SDK = require('aws-sdk');\nrequire('aws-sdk/lib/maintenance_mode_message').suppress = true;\n```\n\n[v2-new-issue]: https://github.com/aws/aws-sdk-js/issues/new/choose\n[v3-recommended-blog]: https://aws.amazon.com/blogs/developer/why-and-how-you-should-use-aws-sdk-for-javascript-v3-on-node-js-18/\n[v3-contributing]: https://github.com/aws/aws-sdk-js-v3#giving-feedback-and-contributing\n[aws-sdk-js-v3]: https://github.com/aws/aws-sdk-js-v3\n[aws-devtools-blog]: https://aws.amazon.com/blogs/developer/\n[aws-sdks-maintenance-policy]: https://docs.aws.amazon.com/sdkref/latest/guide/maint-policy.html\n[open-issue-v2-guidance]: https://github.com/aws/aws-sdk-js/issues/new?assignees=&labels=guidance%2C+needs-triage&template=---questions---help.md&title=\n[aws-sdk-js-codemod]: https://www.npmjs.com/package/aws-sdk-js-codemod\n\n## Table of Contents:\n* [Getting Started](#getting-Started)\n* [Getting Help](#getting-help)\n* [Contributing](#contributing)\n\n## Getting Started\n\n## How To Install\n\n### In the Browser\n\nTo use the SDK in the browser, simply add the following script tag to your\nHTML pages:\n\n    <script src=\"https://sdk.amazonaws.com/js/aws-sdk-2.1569.0.min.js\"></script>\n\nYou can also build a custom browser SDK with your specified set of AWS services.\nThis can allow you to reduce the SDK's size, specify different API versions of\nservices, or use AWS services that don't currently support CORS if you are\nworking in an environment that does not enforce CORS. To get started:\n\nhttp://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/building-sdk-for-browsers.html\n\nThe AWS SDK is also compatible with [browserify](http://browserify.org).\n\nFor browser-based web, mobile and hybrid apps, you can use [AWS Amplify Library](https://aws.github.io/aws-amplify/?utm_source=aws-js-sdk&utm_campaign=browser) which extends the AWS SDK and provides an easier and declarative interface.\n\n### In Node.js\n\nThe preferred way to install the AWS SDK for Node.js is to use the\n[npm](http://npmjs.org) package manager for Node.js. Simply type the following\ninto a terminal window:\n\n```sh\nnpm install aws-sdk\n```\n\n### In React Native\nTo use the SDK in a react native project, first install the SDK using npm:\n\n```sh\nnpm install aws-sdk\n```\n\nThen within your application, you can reference the react native compatible version of the SDK with the following:\n\n```javascript\nvar AWS = require('aws-sdk/dist/aws-sdk-react-native');\n```\n\nAlternatively, you can use [AWS Amplify Library](https://aws.github.io/aws-amplify/media/react_native_guide?utm_source=aws-js-sdk&utm_campaign=react-native) which extends AWS SDK and provides React Native UI components and CLI support to work with AWS services.\n\n### Using Bower\n\nYou can also use [Bower](http://bower.io) to install the SDK by typing the\nfollowing into a terminal window:\n\n```sh\nbower install aws-sdk-js\n```\n\n## Usage with TypeScript\nThe AWS SDK for JavaScript bundles TypeScript definition files for use in TypeScript projects and to support tools that can read `.d.ts` files.\nOur goal is to keep these TypeScript definition files updated with each release for any public api.\n\n### Pre-requisites\nBefore you can begin using these TypeScript definitions with your project, you need to make sure your project meets a few of these requirements:\n\n * Use latest version of TypeScript. We recommend 4.x+\n * Includes the TypeScript definitions for node. You can use npm to install this by typing the following into a terminal window:\n\n    ```sh\n    npm install --save-dev @types/node\n    ```\n\n * If you are targeting at es5 or older ECMA standards, your `tsconfig.json` has to include `'es5'` and `'es2015.promise'` under `compilerOptions.lib`.\n See [tsconfig.json](https://github.com/aws/aws-sdk-js/blob/master/ts/tsconfig.json) for an example.\n\n### In the Browser\nTo use the TypeScript definition files with the global `AWS` object in a front-end project, add the following line to the top of your JavaScript file:\n\n```javascript\n/// <reference types=\"aws-sdk\" />\n```\n\nThis will provide support for the global `AWS` object.\n\n### In Node.js\nTo use the TypeScript definition files within a Node.js project, simply import `aws-sdk` as you normally would.\n\nIn a TypeScript file:\n\n```javascript\n// import entire SDK\nimport AWS from 'aws-sdk';\n// import AWS object without services\nimport AWS from 'aws-sdk/global';\n// import individual service\nimport S3 from 'aws-sdk/clients/s3';\n```\n\n**NOTE:** You need to add `\"esModuleInterop\": true` to compilerOptions of your `tsconfig.json`. If not possible, use like `import * as AWS from 'aws-sdk'`.\n\nIn a JavaScript file:\n\n```javascript\n// import entire SDK\nvar AWS = require('aws-sdk');\n// import AWS object without services\nvar AWS = require('aws-sdk/global');\n// import individual service\nvar S3 = require('aws-sdk/clients/s3');\n```\n\n### With React\n\nTo create React applications with AWS SDK, you can use [AWS Amplify Library](https://aws.github.io/aws-amplify/media/react_guide?utm_source=aws-js-sdk&utm_campaign=react) which provides React components and CLI support to work with AWS services.\n\n### With Angular\nDue to the SDK's reliance on node.js typings, you may encounter compilation \n[issues](https://github.com/aws/aws-sdk-js/issues/1271) when using the\ntypings provided by the SDK in an Angular project created using the Angular CLI.\n\nTo resolve these issues, either add `\"types\": [\"node\"]` to the project's `tsconfig.app.json`\nfile, or remove the `\"types\"` field entirely.\n\n[AWS Amplify Library](https://aws.github.io/aws-amplify/media/angular_guide?utm_source=aws-js-sdk&utm_campaign=angular) provides Angular components and CLI support to work with AWS services.\n\n### Known Limitations\nThere are a few known limitations with the bundled TypeScript definitions at this time:\n\n * Service client typings reflect the latest `apiVersion`, regardless of which `apiVersion` is specified when creating a client.\n * Service-bound parameters use the `any` type.\n\n# Getting Help\n\nThe best way to interact with our team is through GitHub. \nYou can [open an issue](https://github.com/aws/aws-sdk-js/issues/new/choose) and choose from one of our templates for \n[bug reports](https://github.com/aws/aws-sdk-js/issues/new?assignees=&labels=bug%2C+needs-triage&template=---bug-report.md&title=), \n[feature requests](https://github.com/aws/aws-sdk-js/issues/new?assignees=&labels=feature-request&template=---feature-request.md&title=) \nor [guidance](https://github.com/aws/aws-sdk-js/issues/new?assignees=&labels=guidance%2C+needs-triage&template=---questions---help.md&title=). \nYou may also find help on community resources such as [StackOverFlow](https://stackoverflow.com/questions/tagged/aws-sdk-js) with the tag #aws-sdk-js.\nIf you have a support plan with [AWS Support](https://aws.amazon.com/premiumsupport/), you can also create a new support case.\n\nPlease make sure to check out our resources too before opening an issue:\n* Our [Developer Guide](https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/welcome.html) and [API reference](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/)\n* Our [Changelog](https://github.com/aws/aws-sdk-js/blob/master/CHANGELOG.md) for recent changes.\n* Our [code examples](https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/sdk-code-samples.html).\n\nPlease see [SERVICES.md](https://github.com/aws/aws-sdk-js/blob/master/SERVICES.md) for a list of supported services.\n\n# Maintenance and support for SDK major versions\nFor information about maintenance and support for SDK major versions and their underlying dependencies, see the following in the [AWS SDKs and Tools Shared Configuration and Credentials Reference Guide](https://docs.aws.amazon.com/credref/latest/refdocs/overview.html):\n* [AWS SDKs and Tools Maintenance Policy](https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html)\n* [AWS SDKs and Tools Version Support Matrix](https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html)\n\n\n\n# Contributing\nWe welcome community contributions and pull requests. See [CONTRIBUTING.md](https://github.com/aws/aws-sdk-js/blob/master/CONTRIBUTING.md) for information on how to set up a development environment and submit code.\n\n## License\n\nThis SDK is distributed under the\n[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0),\nsee LICENSE.txt and NOTICE.txt for more information.\n", "release_dates": ["2024-02-29T19:29:34Z", "2024-02-28T19:24:34Z", "2024-02-27T19:30:46Z", "2024-02-26T19:30:49Z", "2024-02-23T19:35:05Z", "2024-02-22T19:40:03Z", "2024-02-21T19:30:06Z", "2024-02-20T19:28:02Z", "2024-02-19T19:28:20Z", "2024-02-16T19:27:58Z", "2024-02-15T19:29:59Z", "2024-02-14T19:29:19Z", "2024-02-13T19:30:14Z", "2024-02-12T19:31:34Z", "2024-02-09T19:29:25Z", "2024-02-08T19:27:22Z", "2024-02-07T19:29:28Z", "2024-02-06T19:26:41Z", "2024-02-05T19:25:02Z", "2024-02-02T19:28:43Z", "2024-02-01T20:36:28Z", "2024-01-31T19:25:55Z", "2024-01-30T19:30:04Z", "2024-01-29T19:33:14Z", "2024-01-26T19:29:25Z", "2024-01-25T19:36:34Z", "2024-01-24T19:30:02Z", "2024-01-23T20:53:30Z", "2024-01-22T22:13:19Z", "2024-01-19T19:37:30Z"]}, {"name": "aws-sdk-js-codemod", "description": "Codemod scripts to update AWS SDK for JavaScript APIs.", "language": "TypeScript", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# aws-sdk-js-codemod\n\nThis repository contains a collection of codemod scripts for use with\n[JSCodeshift][jscodeshift] that help update [AWS SDK for JavaScript][aws-sdk-js]\nAPIs.\n\nThe `aws-sdk-js-codemod` CLI is a lightweight wrapper over jscodeshift.\nIt processes `--help`, `--version` and `--transform` options before passing them\ndownstream.\n\nYou can provide names of the custom transforms instead of a local path or url:\n\n     v2-to-v3  Converts AWS SDK for JavaScript APIs in a Javascript/TypeScript\n               codebase from version 2 (v2) to version 3 (v3).\n\nPlease review the code change thoroughly for required functionality before deploying it to production.\nIf the transformation is not complete or is incorrect, please report the issue on GitHub.\n\n## Prerequisites\n\nTo use aws-sdk-js-codemod, please install [Node.js][install-nodejs].\n\n## Usage\n\n- Optionally execute dry-run for the transform, and print transformed files on stdout:\n  ```console\n  npx aws-sdk-js-codemod@latest --dry --print -t v2-to-v3 PATH...\n  ```\n- Run transform, and make changes to files:\n  ```console\n  npx aws-sdk-js-codemod@latest -t v2-to-v3 PATH...\n  ```\n\n## Example\n\n```console\n$ cat example.ts\nimport AWS from \"aws-sdk\";\nconst client = new AWS.DynamoDB();\nconst response = await client.listTables({}).promise();\n\n$ npx aws-sdk-js-codemod@latest -t v2-to-v3 example.ts\n\n$ cat example.ts\nimport { DynamoDB } from \"@aws-sdk/client-dynamodb\";\nconst client = new DynamoDB();\nconst response = await client.listTables({});\n```\n\nFor a summary of supported transformations, check [TRANSFORMATIONS.md](TRANSFORMATIONS.md).\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.\n\n[aws-sdk-js]: https://aws.amazon.com/sdk-for-javascript/\n[install-nodejs]: https://nodejs.dev/learn/how-to-install-nodejs\n[jscodeshift]: https://github.com/facebook/jscodeshift\n", "release_dates": ["2024-03-03T04:44:27Z", "2024-03-01T23:05:27Z", "2024-02-29T22:53:40Z", "2024-02-27T05:58:33Z", "2024-02-22T20:38:30Z", "2024-02-22T00:54:19Z", "2024-02-20T20:55:54Z", "2024-02-19T21:23:39Z", "2023-12-19T18:51:36Z", "2023-11-13T21:46:45Z", "2023-11-13T17:52:46Z", "2023-11-13T17:08:09Z", "2023-11-12T12:00:28Z", "2023-11-12T11:28:17Z", "2023-11-12T10:56:41Z", "2023-11-12T03:09:17Z", "2023-11-12T02:15:50Z", "2023-11-11T20:15:04Z", "2023-11-10T19:33:28Z", "2023-11-07T21:07:18Z", "2023-10-31T18:37:49Z", "2023-10-31T17:38:01Z", "2023-10-27T19:27:12Z", "2023-10-27T06:34:58Z", "2023-10-26T22:24:33Z", "2023-10-26T20:28:16Z", "2023-10-26T16:13:05Z", "2023-10-25T23:01:16Z", "2023-10-24T00:36:36Z", "2023-10-23T20:02:29Z"]}, {"name": "aws-sdk-js-crypto-helpers", "description": "AWS Cryptographic helpers for Javascript and Node.js", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS SDK JS Crypto Helpers\n\nAWS Cryptographic Helpers for Javascript and Node.js\n\n[Security issue notifications](./CONTRIBUTING.md#security-issue-notifications)\n\n## Scope\n\nThis repository collects cryptographic helper packages. We have designed it to gather packages that implement simple primitives for the browser or Node.js. More information about AWS Crypto Tools can be found [here](https://docs.aws.amazon.com/aws-crypto-tools/index.html?id=docs_gateway#lang/en_us)\n\n## Project Status\n\nThis project is still in its early stages. Please send us your feedback. We might make breaking changes in future releases while the SDK is still in developer preview.\n\n## Getting started\n\nLet\u2019s walk through setting up a project that requires a cryptographically secure random value. The following steps use npm as an example. They assume you have node.js and npm already installed.\n\n1. Create a new node.js project.\n2. In the project, run: `npm install --save @aws-crypto/random-source-node@preview`\n3. Create a new file called index.js, require the function, and then use it to get a random value.\n\n```javascript\nconst { randomValues } = require(\"@aws-crypto/random-source-node\");\nasync function example() {\n  try {\n    const rand = await randomValues(32);\n    console.log(rand.length);\n  } catch (err) {\n    console.error(err);\n  }\n}\nexample();\n```\n\n## Crypto Helper Package Index\n\nEach package has readme details.\n\n- [crc32](packages/crc32)\n- [random-source-browser](packages/random-source-browser)\n- [random-source-node](packages/random-source-node)\n- [random-source-universal](packages/random-source-universal)\n- [sha256-browser](packages/sha256-browser)\n- [sha256-js](packages/sha256-js)\n- [sha256-universal](packages/sha256-universal)\n- [supports-web-crypto](packages/supports-web-crypto)\n\n## Testing\n\nTo run the tests in every package.\n\n```\nnpm install\nnpm test\n```\n\n## Feedback\n\nWe welcome your feedback! If you have comments, questions, or suggestions, open a GitHub issue.\nWe are actively monitoring issues and will respond to feedback as we prepare for our GA launch.\n\n## Contributing\n\nWe welcome your contributions! To fix a problem, or add to an existing package: create a pull request.\nYou must submit all pull requests under the Apache 2.0 license. They will be reviewed by a team member prior to merging.\nWe would appreciate, but do not require, unit tests for all significant contributions. See [Contributing](CONTRIBUTING.md) for more information.\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": []}, {"name": "aws-sdk-js-dist-tools", "description": "Please see aws/aws-sdk-js for distribution tools", "language": "JavaScript", "license": null, "readme": "# This repository has moved.\n\nThe distribution tools for the AWS SDK for JavaScript have now moved into\nthe \"dist-tools\" directory of the [AWS SDK for JavaScript][sdk].\n\nPlease note that this repository is deprecated and will be removed. You can\nupdate your scripts to use the new repository by visiting the\n[\"Building the SDK for Use in the Browser\"][guide] section of the\nSDK user guide.\n\n[sdk]: https://github.com/aws/aws-sdk-js\n[guide]: http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-building.html\n", "release_dates": []}, {"name": "aws-sdk-js-v3", "description": "Modularized AWS SDK for JavaScript.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for JavaScript v3\n\n![Build Status](https://codebuild.us-west-2.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiMmtFajZWQmNUbEhidnBKN1VncjRrNVI3d0JUcFpGWUd3STh4T3N3Rnljc1BMaEIrYm9HU2t4YTV1RlE1YmlnUG9XM3luY0Ftc2tBc0xTeVFJMkVOa24wPSIsIml2UGFyYW1ldGVyU3BlYyI6IlBDMDl6UEROK1dlU1h1OWciLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=main)\n[![codecov](https://codecov.io/gh/aws/aws-sdk-js-v3/branch/main/graph/badge.svg)](https://codecov.io/gh/aws/aws-sdk-js-v3)\n[![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg)](https://github.com/prettier/prettier)\n\nThe **AWS SDK for JavaScript v3** is a rewrite of v2 with some great new features.\nAs with version 2, it enables you to easily work with [Amazon Web Services](https://aws.amazon.com/),\nbut has a [modular architecture](https://aws.amazon.com/blogs/developer/modular-packages-in-aws-sdk-for-javascript/) with a separate package for each service.\nIt also includes many frequently requested features, such as a [first-class TypeScript support](https://aws.amazon.com/blogs/developer/first-class-typescript-support-in-modular-aws-sdk-for-javascript/)\nand a [new middleware stack](https://aws.amazon.com/blogs/developer/middleware-stack-modular-aws-sdk-js/).\nFor more details, visit blog post on [general availability of Modular AWS SDK for JavaScript](https://aws.amazon.com/blogs/developer/modular-aws-sdk-for-javascript-is-now-generally-available/).\n\nTo get started with JavaScript SDK version 3, visit our\n[Developer Guide](https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/welcome.html)\nor [API Reference](https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/).\n\nIf you are starting a new project with AWS SDK for JavaScript v3, then you can refer\n[aws-sdk-js-notes-app](https://github.com/aws-samples/aws-sdk-js-notes-app) which shows examples of calling\nmultiple AWS Services in a note taking application.\nIf you are migrating from v2 to v3, then you can visit our [self-guided workshop](https://github.com/aws-samples/aws-sdk-js-v3-workshop)\nwhich builds as basic version of note taking application using AWS SDK for JavaScript v2\nand provides step-by-step migration instructions to v3.\n\nTo test your universal JavaScript code in Node.js, browser and react-native environments,\nvisit our [code samples repo](https://github.com/aws-samples/aws-sdk-js-tests).\n\n# Table of Contents\n\n1. [Getting Started](#getting-started)\n1. [New Features](#new-features)\n   1. [Modularized packages](#modularized-packages)\n   1. [API consistency changes](#api-changes)\n      1. [Configuration](#configuration)\n      1. [Middleware Stack](#middleware)\n   1. [How to upgrade](#other-changes)\n1. [High Level Concepts in V3](#high-level-concepts)\n   1. [Generated Packages](#generated-code)\n   1. [Streams](#streams)\n   1. [Paginators](#paginators)\n   1. [Abort Controller](#abort-controller)\n   1. [Middleware Stack](#middleware-stack)\n1. [Working with the SDK in Lambda](#working-with-the-sdk-in-lambda)\n1. [Install from Source](#install-from-source)\n1. [Giving feedback and contributing](#giving-feedback-and-contributing)\n1. [Release Cadence](#release-cadence)\n1. [Node.js versions](#nodejs-versions)\n1. [Stability of Modular Packages](#stability-of-modular-packages)\n1. [Known Issues](#known-issues)\n   1. [Functionality requiring AWS Common Runtime (CRT)](#functionality-requiring-aws-common-runtime-crt)\n\n## Getting Started\n\nLet\u2019s walk through setting up a project that depends on DynamoDB from the SDK and makes a simple service call. The following steps use yarn as an example. These steps assume you have Node.js and yarn already installed.\n\n1. Create a new Node.js project.\n2. Inside of the project, run: `yarn add @aws-sdk/client-dynamodb`. Adding packages results in update in lock file, [yarn.lock](https://yarnpkg.com/getting-started/qa/#should-lockfiles-be-committed-to-the-repository) or [package-lock.json](https://docs.npmjs.com/configuring-npm/package-lock-json). You **should** commit your lock file along with your code to avoid potential breaking changes.\n\n3. Create a new file called index.js, create a DynamoDB service client and send a request.\n\n```javascript\nconst { DynamoDBClient, ListTablesCommand } = require(\"@aws-sdk/client-dynamodb\");\n\n(async () => {\n  const client = new DynamoDBClient({ region: \"us-west-2\" });\n  const command = new ListTablesCommand({});\n  try {\n    const results = await client.send(command);\n    console.log(results.TableNames.join(\"\\n\"));\n  } catch (err) {\n    console.error(err);\n  }\n})();\n```\n\nIf you want to use non-modular (v2-like) interfaces, you can import client with only the service name (e.g DynamoDB), and call the operation name directly from the client:\n\n```javascript\nconst { DynamoDB } = require(\"@aws-sdk/client-dynamodb\");\n\n(async () => {\n  const client = new DynamoDB({ region: \"us-west-2\" });\n  try {\n    const results = await client.listTables({});\n    console.log(results.TableNames.join(\"\\n\"));\n  } catch (err) {\n    console.error(err);\n  }\n})();\n```\n\nIf you use tree shaking to reduce bundle size, using non-modular interface will increase the bundle size as compared to using modular interface.\n\n<!-- Uncomment when numbers are available for gamma clients\nIn our workshop code, a lambda with DynamoDBClient and a command takes ~18kB while DynamoDB takes ~26 kB ([details](https://github.com/aws-samples/aws-sdk-js-v3-workshop/blob/dc3ad778b04dfe3f8f277dca67162da79c937eca/Exercise1/backend/README.md#reduce-bundle-size-by-just-importing-dynamodb)) -->\n\nIf you are consuming modular AWS SDK for JavaScript on react-native environments, you will need\nto add and import following polyfills in your react-native application:\n\n- [react-native-get-random-values](https://www.npmjs.com/package/react-native-get-random-values)\n- [react-native-url-polyfill](https://www.npmjs.com/package/react-native-url-polyfill)\n\n```js\nimport \"react-native-get-random-values\";\nimport \"react-native-url-polyfill/auto\";\nimport { DynamoDB } from \"@aws-sdk/client-dynamodb\";\n```\n\n## New features\n\n### Modularized packages\n\nThe SDK is now split up across multiple packages. The 2.x version of the SDK contained support for every service. This made it very easy to use multiple services in a project. Due to the limitations around reducing the size of the SDK when only using a handful of services or operations, many customers requested having separate packages for each service client. We have also split up the core parts of the SDK so that service clients only pull in what they need. For example, a service sends responses in JSON will no longer need to also have an XML parser as a dependency.\n\nFor those that were already importing services as sub-modules from the v2 SDK, the import statement doesn\u2019t look too different. Here\u2019s an example of importing the AWS Lambda service in v2 of the SDK, and the v3 SDK:\n\n```javascript\n// import the Lambda client constructor in v2 of the SDK\nconst Lambda = require(\"aws-sdk/clients/lambda\");\n\n// import the Lambda client constructor in v3 SDK\nconst { Lambda } = require(\"@aws-sdk/client-lambda\");\n```\n\nIt is also possible to import both versions of the Lambda client by changing the variable name the Lambda constructor is stored in.\n\n### API changes\n\nWe\u2019ve made several public API changes to improve consistency, make the SDK easier to use, and remove deprecated or confusing APIs. The following are some of the big changes included in the new AWS SDK for JavaScript v3.\n\n#### Configuration\n\nIn version 2.x of the SDK, service configuration could be passed to individual client constructors.\nHowever, these configurations would first be merged automatically into a copy of the global SDK configuration: `AWS.config`.\n\nAlso, calling `AWS.config.update({/* params */})` only updated configuration for service clients instantiated after the update call was made, not any existing clients.\n\nThis behavior was a frequent source of confusion, and made it difficult to add configuration to the global object that only affects a subset of service clients in a forward-compatible way.\nIn v3, there is no longer a global configuration managed by the SDK.\nConfiguration must be passed to each service client that is instantiated.\nIt is still possible to share the same configuration across multiple clients but that configuration will not be automatically merged with a global state.\n\n#### Middleware\n\nVersion 2.x of the SDK allows modifying a request throughout multiple stages of a request\u2019s lifecycle by attaching event listeners to a request.\nSome feedback we received frequently was that it can be difficult to debug what went wrong during a request\u2019s lifecycle.\nWe\u2019ve switched to using a middleware stack to control the lifecycle of an operation call now.\nThis gives us a few benefits. Each middleware in the stack calls the next middleware after making any changes to the request object.\nThis also makes debugging issues in the stack much easier since you can see exactly which middleware have been called leading up to an error.\nHere\u2019s an example of logging requests using middleware:\n\n```javascript\nconst client = new DynamoDB({ region: \"us-west-2\" });\n\nclient.middlewareStack.add(\n  (next, context) => async (args) => {\n    console.log(\"AWS SDK context\", context.clientName, context.commandName);\n    console.log(\"AWS SDK request input\", args.input);\n    const result = await next(args);\n    console.log(\"AWS SDK request output:\", result.output);\n    return result;\n  },\n  {\n    name: \"MyMiddleware\",\n    step: \"build\",\n    override: true,\n  }\n);\n\nawait client.listTables({});\n```\n\nIn the above example, we\u2019re adding a middleware to our DynamoDB client\u2019s middleware stack.\nThe first argument is a function that accepts next, the next middleware in the stack to call, and context, an object that contains some information about the operation being called.\nIt returns a function that accepts args, an object that contains the parameters passed to the operation and the request, and returns the result from calling the next middleware with args.\n\n#### Other Changes\n\nIf you are looking for a breakdown of the API changes from AWS SDK for JavaScript v2 to v3,\nwe have them listed in [UPGRADING.md](https://github.com/aws/aws-sdk-js-v3/blob/main/UPGRADING.md).\n\n## Working with the SDK in Lambda\n\n### General Info\n\nThe Lambda provided AWS SDK is set to a specific minor version, and **NOT** the latest version. To check the minor version used by Lambda please refer to [Lambda runtimes doc page](https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html).\nIf you wish to use the latest / different version of the SDK from the one provided by lambda, we recommend that you [bundle and minify](https://aws.amazon.com/blogs/compute/optimizing-node-js-dependencies-in-aws-lambda/) your project, or [upload it as a Lambda layer](https://aws.amazon.com/blogs/compute/using-lambda-layers-to-simplify-your-development-process/).\n\nThe performance of the AWS SDK for JavaScript v3 on node 18 has improved from v2 as seen in the [performance benchmarking](https://aws.amazon.com/blogs/developer/reduce-lambda-cold-start-times-migrate-to-aws-sdk-for-javascript-v3/)\n\n### Best practices\n\nWhen using Lambda we should use a single SDK client per service, per region, and initialize it outside of the handler's codepath. This is done to optimize for Lambda's [container reuse](https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtime-environment.html).\n\nThe API calls themselves should be made from within the handler's codepath.\nThis is done to ensure that API calls are signed at the very last step of Lambda's execution cycle, after the Lambda is \"hot\" to avoid signing time skew.\n\nExample:\n\n```javascript\nimport { STSClient, GetCallerIdentityCommand } from \"@aws-sdk/client-sts\";\n\nconst client = new STSClient({}); // SDK Client initialized outside the handler\n\nexport const handler = async (event) => {\n  const response = {\n    statusCode: 200,\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n  };\n\n  try {\n    const results = await client.send(new GetCallerIdentityCommand({})); // API operation made from within the handler\n    const responseBody = {\n      userId: results.UserId,\n    };\n\n    response.body = JSON.stringify(responseBody);\n  } catch (err) {\n    console.log(\"Error:\", err);\n    response.statusCode = 500;\n    response.body = JSON.stringify({\n      message: \"Internal Server Error\",\n    });\n  }\n\n  return response;\n};\n```\n\n## Install from Source\n\nAll clients have been published to NPM and can be installed as described above. If you want to play with latest clients, you can build from source as follows:\n\n1. Clone this repository to local by:\n\n   ```\n   git clone https://github.com/aws/aws-sdk-js-v3.git\n   ```\n\n1. Under the repository root directory, run following command to link and build the whole library, the process may take several minutes:\n\n   ```\n   yarn && yarn test:all\n   ```\n\n   For more information, please refer to [contributing guide](https://github.com/aws/aws-sdk-js-v3/blob/main/CONTRIBUTING.md#setup-and-testing).\n\n1. After the repository is successfully built, change directory to the client that you want to install, for example:\n\n   ```\n   cd clients/client-dynamodb\n   ```\n\n1. Pack the client:\n\n   ```\n   yarn pack .\n   ```\n\n   `yarn pack` will create an archive file in the client package folder, e.g. `aws-sdk-client-dynamodb-v3.0.0.tgz`.\n\n1. Change directory to the project you are working on and move the archive to the location to store the vendor packages:\n\n   ```\n   mv path/to/aws-sdk-js-v3/clients/client-dynamodb/aws-sdk-client-dynamodb-v3.0.0.tgz ./path/to/vendors/folder\n   ```\n\n1. Install the package to your project:\n\n   ```\n   yarn add ./path/to/vendors/folder/aws-sdk-client-dynamodb-v3.0.0.tgz\n   ```\n\n## Giving feedback and contributing\n\nYou can provide feedback to us in several ways. Both positive and negative feedback is appreciated.\nIf you do, please feel free to [open an issue](https://github.com/aws/aws-sdk-js-v3/issues/new/choose) on our GitHub repository.\nOur GitHub issues page also includes work we know still needs to be done to reach full feature parity with v2 SDK.\n\n### Feedback\n\n**GitHub issues**. Customers who are comfortable giving public feedback can open a GitHub issue in the new repository.\nThis is the preferred mechanism to give feedback so that other customers can engage in the conversation, +1 issues, etc.\nIssues you open will be evaluated, and included in our roadmap for the GA launch.\n\n**Gitter channel**. For informal discussion or general feedback, you may join the [Gitter chat](https://gitter.im/aws/aws-sdk-js-v3).\nThe Gitter channel is also a great place to get help with v3 from other developers. JS SDK team doesn't\ntrack the discussion daily, so feel free to open a GitHub issue if your question is not answered there.\n\n### Contributing\n\nYou can open pull requests for fixes or additions to the new AWS SDK for JavaScript v3.\nAll pull requests must be submitted under the Apache 2.0 license and will be reviewed by an SDK team member prior to merging.\nAccompanying unit tests are appreciated. See [Contributing](CONTRIBUTING.md) for more information.\n\n## High Level Concepts\n\nThis is an introduction to some of the high level concepts behind AWS SDK for JavaScript (v3)\nwhich are shared between services and might make your life easier. Please consult the user\nguide and API reference for service specific details.\n\n#### Terminology:\n\n**Bare-bones clients/commands**: This refers to a modular way of consuming individual operations on JS SDK clients. It results in less code being imported and thus more performant. It is otherwise equivalent to the aggregated clients/commands.\n\n```javascript\n// this imports a bare-bones version of S3 that exposes the .send operation\nimport { S3Client } from \"@aws-sdk/client-s3\"\n\n// this imports just the getObject operation from S3\nimport { GetObjectCommand } from \"@aws-sdk/client-s3\"\n\n//usage\nconst bareBonesS3 = new S3Client({...});\nawait bareBonesS3.send(new GetObjectCommand({...}));\n```\n\n**Aggregated clients/commands**: This refers to a way of consuming clients that contain all operations on them. Under the hood this calls the bare-bones commands. This imports all commands on a particular client and results in more code being imported and thus less performant. This is 1:1 with v2's style.\n\n```javascript\n// this imports an aggregated version of S3 that exposes the .send operation\nimport { S3 } from \"@aws-sdk/client-s3\"\n\n// No need to import an operation as all operations are already on the S3 prototype\n\n//usage\nconst aggregatedS3 = new S3({...});\nawait aggregatedS3.getObject({...}));\n```\n\n### Generated Code\n\nThe v3 codebase is generated from internal AWS models that AWS services expose. We use [smithy-typescript](https://github.com/awslabs/smithy-typescript) to generate all code in the `/clients` subdirectory. These packages always have a prefix of `@aws-sdk/client-XXXX` and are one-to-one with AWS services and service operations. You should be importing `@aws-sdk/client-XXXX` for most usage.\n\nClients depend on common \"utility\" code in `/packages`. The code in `/packages` is manually written and outside of special cases (like credentials or abort controller) is generally not very useful alone.\n\nLastly we have higher level libraries in `/lib`. These are javascript specific libraries that wrap client operations to make them easier to work with. Popular examples are `@aws-sdk/lib-dynamodb` which [simplifies working with items in Amazon DynamoDB](https://github.com/aws/aws-sdk-js-v3/blob/main/lib/lib-dynamodb/README.md) or `@aws-sdk/lib-storage` which exposes the `Upload` function and [simplifies parallel uploads in S3's multipartUpload](https://github.com/aws/aws-sdk-js-v3/blob/main/lib/lib-storage/README.md).\n\n1. `/packages`. This sub directory is where most manual code updates are done. These are published to NPM under `@aws-sdk/XXXX` and have no special prefix.\n1. `/clients`. This sub directory is code generated and depends on code published from `/packages` . It is 1:1 with AWS services and operations. Manual edits should generally not occur here. These are published to NPM under `@aws-sdk/client-XXXX`.\n1. `/lib`. This sub directory depends on generated code published from `/clients`. It wraps existing AWS services and operations to make them easier to work with in Javascript. These are published to NPM under `@aws-sdk/lib-XXXX`\n\n### Streams\n\nCertain command outputs include streams, which have different implementations in\nNode.js and browsers. For convenience, a set of stream handling methods will be\nmerged (`Object.assign`) to the output stream object, as defined in\n[SdkStreamMixin][serde-code-url].\n\nOutput types having this feature will be indicated by the `WithSdkStreamMixin<T, StreamKey>`\n[wrapper type][serde-code-url], where `T` is the original output type\nand `StreamKey` is the output property key having a stream type specific to\nthe runtime environment.\n\n[serde-code-url]: https://github.com/aws/aws-sdk-js-v3/blob/main/packages/types/src/serde.ts\n\nHere is an example using `S3::GetObject`.\n\n```js\nimport { S3 } from \"@aws-sdk/client-s3\";\n\nconst client = new S3({});\n\nconst getObjectResult = await client.getObject({\n  Bucket: \"...\",\n  Key: \"...\",\n});\n\n// env-specific stream with added mixin methods.\nconst bodyStream = getObjectResult.Body;\n\n// one-time transform.\nconst bodyAsString = await bodyStream.transformToString();\n\n// throws an error on 2nd call, stream cannot be rewound.\nconst __error__ = await bodyStream.transformToString();\n```\n\nNote that these methods will read the stream in order to collect it,\nso **you must save the output**. The methods cannot be called more than once\non a stream.\n\n### Paginators\n\nMany AWS operations return paginated results when the response object is too large to return in a single response. In AWS SDK for JavaScript v2, the response contains a token you can use to retrieve the next page of results. You then need to write additional functions to process pages of results.\n\nIn AWS SDK for JavaScript v3 we\u2019ve improved pagination using async generator functions, which are similar to generator functions, with the following differences:\n\n- When called, async generator functions return an object, an async generator whose methods (`next`, `throw`, and `return`) return promises for `{ `value`, `done` }`, instead of directly returning `{ `value`, `done` }`. This automatically makes the returned async generator objects async iterators.\n- await expressions and `for await (x of y)` statements are allowed.\n- The behavior of `yield*` is modified to support delegation to async iterables.\n\nThe Async Iterators were added in the ES2018 iteration of JavaScript. They are supported by Node.js 10.x+ and by all modern browsers, including Chrome 63+, Firefox 57+, Safari 11.1+, and Edge 79+. If you\u2019re using TypeScript v2.3+, you can compile Async Iterators to older versions of JavaScript.\n\nAn async iterator is much like an iterator, except that its `next()` method returns a promise for a `{ `value`, `done` }` pair. As an implicit aspect of the Async Iteration protocol, the next promise is not requested until the previous one resolves. This is a simple, yet a very powerful pattern.\n\n#### Example Pagination Usage\n\nIn v3, the clients expose paginateOperationName APIs that are written using async generators, allowing you to use async iterators in a for await..of loop. You can perform the paginateListTables operation from `@aws-sdk/client-dynamodb` as follows:\n\n```javascript\nconst {\n  DynamoDBClient,\n  paginateListTables,\n} = require(\"@aws-sdk/client-dynamodb\");\n\n...\nconst paginatorConfig = {\n  client: new DynamoDBClient({}),\n  pageSize: 25\n};\nconst commandParams = {};\nconst paginator = paginateListTables(paginatorConfig, commandParams);\n\nconst tableNames = [];\nfor await (const page of paginator) {\n  // page contains a single paginated output.\n  tableNames.push(...page.TableNames);\n}\n...\n\n```\n\nOr simplified:\n\n```javascript\n...\nconst client = new DynamoDBClient({});\n\nconst tableNames = [];\nfor await (const page of paginateListTables({ client }, {})) {\n    // page contains a single paginated output.\n    tableNames.push(...page.TableNames);\n}\n...\n```\n\n### Abort Controller\n\nIn v3, we support the AbortController interface which allows you to abort requests as and when desired.\n\nThe [AbortController Interface](https://dom.spec.whatwg.org/#interface-abortcontroller) provides an `abort()` method that toggles the state of a corresponding AbortSignal object. Most APIs accept an AbortSignal object, and respond to `abort()` by rejecting any unsettled promise with an \u201cAbortError\u201d.\n\n```javascript\n// Returns a new controller whose signal is set to a newly created AbortSignal object.\nconst controller = new AbortController();\n\n// Returns the AbortSignal object associated with controller.\nconst signal = controller.signal;\n\n// Invoking this method will set controller\u2019s AbortSignal's aborted flag\n// and signal to any observers that the associated activity is to be aborted.\ncontroller.abort();\n```\n\n#### AbortController Usage\n\nIn JavaScript SDK v3, we added an implementation of WHATWG AbortController interface in `@aws-sdk/abort-controller`. To use it, you need to send `AbortController.signal` as `abortSignal` in the httpOptions parameter when calling `.send()` operation on the client as follows:\n\n```javascript\nconst { AbortController } = require(\"@aws-sdk/abort-controller\");\nconst { S3Client, CreateBucketCommand } = require(\"@aws-sdk/client-s3\");\n\n...\n\nconst abortController = new AbortController();\nconst client = new S3Client(clientParams);\n\nconst requestPromise = client.send(new CreateBucketCommand(commandParams), {\n  abortSignal: abortController.signal,\n});\n\n// The abortController can be aborted any time.\n// The request will not be created if abortSignal is already aborted.\n// The request will be destroyed if abortSignal is aborted before response is returned.\nabortController.abort();\n\n// This will fail with \"AbortError\" as abortSignal is aborted.\nawait requestPromise;\n```\n\nFor a full pagination deep dive please check out our [blog post](https://aws.amazon.com/blogs/developer/pagination-using-async-iterators-in-modular-aws-sdk-for-javascript/).\n\n#### AbortController Example\n\nThe following code snippet shows how to upload a file using S3's putObject API in the browser with support to abort the upload. First, create a controller using the `AbortController()` constructor, then grab a reference to its associated AbortSignal object using the AbortController.signal property. When the `PutObjectCommand` is called with `.send()` operation, pass in AbortController.signal as abortSignal in the httpOptions parameter. This will allow you to abort the PutObject operation by calling `abortController.abort()`.\n\n```javascript\nconst abortController = new AbortController();\nconst abortSignal = abortController.signal;\n\nconst uploadBtn = document.querySelector('.upload');\nconst abortBtn = document.querySelector('.abort');\n\nuploadBtn.addEventListener('click', uploadObject);\n\nabortBtn.addEventListener('click', function() {\n  abortController.abort();\n  console.log('Upload aborted');\n});\n\nconst uploadObject = async (file) => {\n  ...\n  const client = new S3Client(clientParams);\n  try {\n    await client.send(new PutObjectCommand(commandParams), { abortSignal });\n  } catch(e) {\n    if (e.name === \"AbortError\") {\n      uploadProgress.textContent = 'Upload aborted: ' + e.message;\n    }\n    ...\n  }\n}\n```\n\nFor a full abort controller deep dive please check out our [blog post](https://aws.amazon.com/blogs/developer/abortcontroller-in-modular-aws-sdk-for-javascript/).\n\n### Middleware Stack\n\nThe AWS SDK for JavaScript (v3) maintains a series of asynchronous actions. These series include actions that serialize input parameters into the data over the wire and deserialize response data into JavaScript objects. Such actions are implemented using functions called middleware and executed in a specific order. The object that hosts all the middleware including the ordering information is called a Middleware Stack. You can add your custom actions to the SDK and/or remove the default ones.\n\nWhen an API call is made, SDK sorts the middleware according to the step it belongs to and its priority within each step. The input parameters pass through each middleware. An HTTP request gets created and updated along the process. The HTTP Handler sends a request to the service, and receives a response. A response object is passed back through the same middleware stack in reverse, and is deserialized into a JavaScript object.\n\nA middleware is a higher-order function that transfers user input and/or HTTP request, then delegates to \u201cnext\u201d middleware. It also transfers the result from \u201cnext\u201d middleware. A middleware function also has access to context parameter, which optionally contains data to be shared across middleware.\n\nFor example, you can use middleware to log or modify a request:\n\n```javascript\nconst { S3 } = require(\"@aws-sdk/client-s3\");\nconst client = new S3({ region: \"us-west-2\" });\n\n// Middleware added to client, applies to all commands.\nclient.middlewareStack.add(\n  (next, context) => async (args) => {\n    args.request.headers[\"x-amz-meta-foo\"] = \"bar\";\n    console.log(\"AWS SDK context\", context.clientName, context.commandName);\n    console.log(\"AWS SDK request input\", args.input);\n    const result = await next(args);\n    console.log(\"AWS SDK request output:\", result.output);\n    return result;\n  },\n  {\n    step: \"build\",\n    name: \"addFooMetadataMiddleware\",\n    tags: [\"METADATA\", \"FOO\"],\n    override: true,\n  }\n);\n\nawait client.putObject(params);\n```\n\nSpecifying the absolute location of your middleware\nThe example above adds middleware to `build` step of middleware stack. The middleware stack contains five steps to manage a request\u2019s lifecycle:\n\n- The **initialize** lifecycle step initializes an API call. This step typically adds default input values to a command. The HTTP request has not yet been constructed.\n- The **serialize** lifecycle step constructs an HTTP request for the API call. Example of typical serialization tasks include input validation and building an HTTP request from user input. The downstream middleware will have access to serialized HTTP request object in callback\u2019s parameter `args.request`.\n- The **build** lifecycle step builds on top of serialized HTTP request. Examples of typical build tasks include injecting HTTP headers that describe a stable aspect of the request, such as `Content-Length` or a body checksum. Any request alterations will be applied to all retries.\n- The **finalizeRequest** lifecycle step prepares the request to be sent over the wire. The request in this stage is semantically complete and should therefore only be altered to match the recipient\u2019s expectations. Examples of typical finalization tasks include request signing, performing retries and injecting hop-by-hop headers.\n- The **deserialize** lifecycle step deserializes the raw response object to a structured response. The upstream middleware have access to deserialized data in next callbacks return value: `result.output`.\n  Each middleware must be added to a specific step. By default each middleware in the same step has undifferentiated order. In some cases, you might want to execute a middleware before or after another middleware in the same step. You can achieve it by specifying its `priority`.\n\n```javascript\nclient.middlewareStack.add(middleware, {\n  name: \"MyMiddleware\",\n  step: \"initialize\",\n  priority: \"high\", // or \"low\".\n  override: true, // provide both a name and override=true to avoid accidental middleware duplication.\n});\n```\n\nFor a full middleware stack deep dive please check out our [blog post](https://aws.amazon.com/blogs/developer/middleware-stack-modular-aws-sdk-js/).\n\n## Release Cadence\n\nOur releases usually happen once per weekday. Each release increments the\nminor version, e.g. 3.200.0 -> 3.201.0.\n\n## <a id=\"nodejs-versions\"></a> Node.js versions\n\nv3.201.0 and higher requires Node.js >= 14.\n\nv3.46.0 to v3.200.0 requires Node.js >= 12.\n\nEarlier versions require Node.js >= 10.\n\n## Stability of Modular Packages\n\n| Package name                | containing folder | API controlled by | stability     |\n| --------------------------- | ----------------- | ----------------- | ------------- |\n| @aws-sdk/client-\\* Commands | clients           | AWS service teams | public/stable |\n| @aws-sdk/client-\\* Clients  | clients           | AWS SDK JS team   | public/stable |\n| @aws-sdk/lib-\\*             | lib               | AWS SDK JS team   | public/stable |\n| @aws-sdk/\\*-signer          | packages          | AWS SDK JS team   | public/stable |\n| @aws-sdk/middleware-stack   | packages          | AWS SDK JS team   | public/stable |\n| remaining @aws-sdk/\\*       | packages          | AWS SDK JS team   | internal      |\n\nPublic interfaces are marked with the `@public` annotation in source code and appear\nin our [API Reference](https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/).\n\nAdditional notes:\n\n- `@internal` does not mean a package or interface is constantly changing\n  or being actively worked on. It means it is subject to change without any\n  notice period. The changes are included in the release notes.\n- public interfaces such as client configuration are also subject to change\n  in exceptional cases. We will try to undergo a deprecation period with\n  an advance notice.\n\nAll supported interfaces are provided at the package level, e.g.:\n\n```js\nimport { S3Client } from \"@aws-sdk/client-s3\"; // Yes, do this.\n\nimport { S3Client } from \"@aws-sdk/client-s3/dist-cjs/S3Client\"; // No, don't do this.\n```\n\nDo not import from a deep path in any package, since the file structure may change, and\nin the future packages may include the `exports` metadata in `package.json` preventing\naccess to the file structure.\n\n## Known Issues\n\n### Functionality requiring AWS Common Runtime (CRT)\n\nThis SDK has optional functionality that requires the [AWS Common Runtime (CRT)](https://docs.aws.amazon.com/sdkref/latest/guide/common-runtime.html)\nbindings to be included as a dependency with your application. This functionality includes:\n\n- [Amazon S3 Multi-Region Access Points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPoints.html)\n- [Amazon S3 Object Integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)\n\nIf the required AWS Common Runtime components are not installed you will receive an error like:\n\n```console\nCannot find module '@aws-sdk/signature-v4-crt'\n...\nPlease check whether you have installed the \"@aws-sdk/signature-v4-crt\" package explicitly.\nYou must also register the package by calling [require(\"@aws-sdk/signature-v4-crt\");]\nor an ESM equivalent such as [import \"@aws-sdk/signature-v4-crt\";].\nFor more information please go to\nhttps://github.com/aws/aws-sdk-js-v3#functionality-requiring-aws-common-runtime-crt\"\n```\n\nindicating that the required dependency is missing to use the associated functionality. To install this dependency follow\nthe provided [instructions](#installing-the-aws-common-runtime-crt-dependency).\n\n#### Installing the AWS Common Runtime (CRT) Dependency\n\nYou can install the CRT dependency with different commands depending on the package management tool you are using.\nIf you are using NPM:\n\n```console\nnpm install @aws-sdk/signature-v4-crt\n```\n\nIf you are using Yarn:\n\n```console\nyarn add @aws-sdk/signature-v4-crt\n```\n\nAdditionally, load the signature-v4-crt package by importing it.\n\n```js\nrequire(\"@aws-sdk/signature-v4-crt\");\n// or ESM\nimport \"@aws-sdk/signature-v4-crt\";\n```\n\nOnly the import statement is needed. The implementation then registers itself with `@aws-sdk/signature-v4-multi-region`\nand becomes available for its use. You do not need to use any imported objects directly.\n\n#### Related issues\n\n1. [S3 Multi-Region Access Point(MRAP) is not available unless with additional dependency](https://github.com/aws/aws-sdk-js-v3/issues/2822)\n", "release_dates": ["2024-03-01T20:01:50Z", "2024-02-29T20:06:15Z", "2024-02-28T20:00:57Z", "2024-02-27T20:06:26Z", "2024-02-26T20:05:19Z", "2024-02-23T20:09:17Z", "2024-02-22T20:16:05Z", "2024-02-21T20:02:59Z", "2024-02-20T19:59:34Z", "2024-02-19T20:02:19Z", "2024-02-16T20:03:02Z", "2024-02-15T20:08:49Z", "2024-02-14T20:08:10Z", "2024-02-13T20:06:37Z", "2024-02-12T20:01:12Z", "2024-02-09T20:05:27Z", "2024-02-08T20:00:34Z", "2024-02-07T20:03:29Z", "2024-02-06T20:02:20Z", "2024-02-05T20:03:22Z", "2024-02-02T20:01:30Z", "2024-02-01T20:04:03Z", "2024-01-31T21:06:40Z", "2024-01-30T22:27:47Z", "2024-01-30T20:04:10Z", "2024-01-29T23:07:10Z", "2024-01-26T20:07:10Z", "2024-01-25T20:12:18Z", "2024-01-24T20:09:55Z", "2024-01-23T21:28:39Z"]}, {"name": "aws-sdk-mobile-analytics-js", "description": "Amazon Mobile Analytics JavaScript SDK", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon Mobile Analytics SDK for JavaScript\n\n**Developer Preview:** We welcome developer feedback on this project. You can reach us by creating an issue on the \nGitHub repository or posting to the Amazon Mobile Analytics forums:\n* https://github.com/aws/aws-sdk-mobile-analytics-js\n* https://forums.aws.amazon.com/forum.jspa?forumID=174\n\nIntroduction\n============\nThe Mobile Analytics SDK for JavaScript allows JavaScript enabled applications to create and submit events for analysis in the AWS Console and through Auto Export to S3 and Redshift. The library uses the browser's local storage API to create a local cache for the data, allowing your web application to batch and record events even when the app is offline.\n\n## Setup\n\n1. Download and include the AWS JavaScript SDK (minimum version 2.1.18):\n  * http://aws.amazon.com/sdk-for-browser/\n\n2. Download and include the Amazon Mobile Analytics SDK for JavaScript:\n  * [/dist/aws-sdk-mobile-analytics.min.js](https://raw.githubusercontent.com/aws/aws-sdk-mobile-analytics-js/master/dist/aws-sdk-mobile-analytics.min.js)\n\n<pre class=\"prettyprint\">\n    &lt;script src=\"/js/aws-sdk.min.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"/js/aws-sdk-mobile-analytics.min.js\"&gt;&lt;/script&gt;\n</pre>\n\n## Usage\n\n**Step 1.** Log in to the [Amazon Mobile Analytics management console](https://console.aws.amazon.com/mobileanalytics/home/?region=us-east-1) and create a new app. Be sure to note your App Id and Cognito Identity Pool Id.\n* https://console.aws.amazon.com/mobileanalytics/home/?region=us-east-1\n\n**Step 2.** Initialize the credentials provider using a Cognito Identity Pool ID. This is necessary for the AWS SDK to manage authentication to the Amazon Mobile Analytics REST API.\n\n<pre class=\"prettyprint\">\n    AWS.config.region = 'us-east-1';\n    AWS.config.credentials = new AWS.CognitoIdentityCredentials({\n        IdentityPoolId: COGNITO_IDENTITY_POOL_ID   //Required e.g. 'us-east-1:12345678-c1ab-4122-913b-22d16971337b'\n    });\n</pre>\n\n**Step 3.** Instantiate the Mobile Analytics Manager, including your App ID generated in Step 1, above. Session events will be automatically recorded and the client will batch and automatically submit events to Amazon Mobile Analytics every 10 seconds.\n\n<pre class=\"prettyprint\">\n    var options = {\n        appId : MOBILE_ANALYTICS_APP_ID   //Required e.g. 'c5d69c75a92646b8953126437d92c007'\n    };\n    mobileAnalyticsClient = new AMA.Manager(options);\n</pre>\n\nTo manually force an event submission you can call:\n<pre class=\"prettyprint\">\n    mobileAnalyticsClient.submitEvents();\n</pre>\n\n## Additional Options\n### Custom Events\nYou can optionally add custom events to capture additional information you find valuable.\n\n<pre class=\"prettyprint\">\n    mobileAnalyticsClient.recordEvent('CUSTOM EVENT NAME', {\n            'ATTRIBUTE_1_NAME': 'ATTRIBUTE_1_VALUE',\n            'ATTRIBUTE_2_NAME': 'ATTRIBUTE_2_VALUE'\n            /* ... */\n        }, {\n            'METRIC_1_NAME': 1,\n            'METRIC_2_NAME': 99.3\n            /* ... */\n        });\n</pre>\n\n\n### Session Settings\nBy default a session lasts 10 minutes. You can override this default setting when initializing the Mobile Analytics Manager by including \"sessionLength\" in the \"options\" object.\n\n<pre class=\"prettyprint\">\n    var options = {\n        appId : MOBILE_ANALYTICS_APP_ID, \n        sessionLength: 300000            //Session Length in milliseconds.  This will evaluate to 5min.\n    };\n    mobileAnalyticsClient = new AMA.Manager(options);\n</pre>\n\nA session's timeout can also be updated to allow for continuation of a session.\n\n<pre class=\"prettyprint\">\n    //This will set the current session to expire in 5 seconds from now.\n    mobileAnalyticsClient.resetSessionTimeout(5000); \n    \n    //This will reset the current session's expiration time using the time specified during initialization. \n    //If the default setting was used (10 minutes) then the session will expire 10 minutes from now. \n    mobileAnalyticsClient.resetSessionTimeout();\n</pre>\n\n### Record Monetization Event\nYou can record monetization events to enable reports such as Average Revenue Per User (ARPU) and more.\n\n<pre class=\"prettyprint\">\n    mobileAnalyticsClient.recordMonetizationEvent(\n        {\n            productId : PRODUCT_ID,   //Required e.g. 'My Example Product'\n            price : PRICE,            //Required e.g. 1.99\n            quantity : QUANTITY,      //Required e.g. 1\n            currency : CURRENCY_CODE  //Optional ISO currency code e.g. 'USD'\n        }, \n        {/* Custom Attributes */}, \n        {/* Custom Metrics */}\n    );\n</pre>\n\n### Add App Details to Events\nAdditional app and environment details can be added to the \"options\" object when initializing the SDK. These details will be captured and applied to all events and can be useful if using Auto Export for custom analysis of your data.\n\n<pre class=\"prettyprint\">\n    var options = {\n        appId : MOBILE_ANALYTICS_APP_ID,       //Required e.g. 'c5d69c75a92646b8953126437d92c007'\n        appTitle : APP_TITLE,                  //Optional e.g. 'Example App'\n        appVersionName : APP_VERSION_NAME,     //Optional e.g. '1.4.1'\n        appVersionCode : APP_VERSION_CODE,     //Optional e.g. '42'\n        appPackageName : APP_PACKAGE_NAME,     //Optional e.g. 'com.amazon.example'\n        make : DEVICE_MAKE,                    //Optional e.g. 'Amazon'\n        model : DEVICE_MODEL,                  //Optional e.g. 'KFTT'\n        platform : DEVICE_PLATFORM,            //Optional valid values: 'Android', 'iPhoneOS', 'WindowsPhone', 'Blackberry', 'Windows', 'MacOS', 'Linux'\n        platformVersion : DEVICE_PLATFORM_VER  //Optional e.g. '4.4'\n    };\n    mobileAnalyticsClient = new AMA.Manager(options);\n</pre>\n\nPlease note, if device details are not specified Amazon Mobile Analytics will make best efforts to determine these values based on the User-Agent header value. It is always better to specify these values during initialization if they are available. \n\n### Further Documentation\nFurther documentation and advanced configurations can be found here:\n\nhttps://aws.github.io/aws-sdk-mobile-analytics-js/doc/AMA.Manager.html\n\n## Network Configuration\nThe Amazon Mobile Analytics JavaScript SDK will make requests to the following endpoints\n* For Event Submission: \"https://mobileanalytics.us-east-1.amazonaws.com\"\n* For Cognito Authentication: \"https://cognito-identity.us-east-1.amazonaws.com\"\n  * This endpoint may change based on which region your Identity Pool was created in.\n \nFor most frameworks you can whitelist both domains by whitelisting all AWS endpoints with \"*.amazonaws.com\".\n\n## Change Log\n**v0.9.2:**\n* Bug Fixes:\n    * Fixed data loss issue on migration from previous versions\n\n**v0.9.1:**\n* Updated Dependency: aws-sdk-js v2.2.37\n* Increase base delay between retries from 30ms to 3s\n* Allow passing of configurations to the low level client via clientOptions attribute\n* Local events from different apps are stored in different locations\n* Improved retry strategies\n* CorrectClockSkew is enabled by default for Sigv4 Signatures [per Issue#7](https://github.com/aws/aws-sdk-mobile-analytics-js/issues/7)\n* Bug Fixes:\n  * Fixed timer from being killed in cases where multiple submissions happened in under a second\n  * Fixed duplicate batch re-submission to the Mobile Analytics service\n  * Fixed delayed auto-submission of first _session.start event\n  * Fixed Safari throwing exception when in private browsing mode\n\n**v0.9.0:**\n* Initial release. Developer preview.\n", "release_dates": ["2016-07-19T20:37:35Z", "2016-02-22T22:16:43Z", "2015-03-19T21:50:30Z"]}, {"name": "aws-sdk-net", "description": "The official AWS SDK for .NET. For more information on the AWS SDK for .NET, see our web site:", "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for .NET [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/aws/aws-sdk-net?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\nThe **AWS SDK for .NET** enables .NET developers to easily work with [Amazon Web Services][aws] and build scalable solutions with Amazon S3, Amazon DynamoDB, Amazon Glacier, and more.\n\n* [API Docs][docs-api]\n* [AWS .NET Developer Blog][dotnet-blog]\n* [SDK Homepage][sdk-website]\n* [SDK Developer Guide](https://docs.aws.amazon.com/sdk-for-net)\n* [Forum][sdk-forum]\n* [GitHub Issues][sdk-issues]\n* [SDK Samples](https://github.com/awsdocs/aws-doc-sdk-examples/tree/main/dotnetv3)\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the [GitHub issues][sdk-issues] for tracking bugs and feature requests and have limited bandwidth to address them.\n* Ask a question on [StackOverflow](http://stackoverflow.com/) and tag it with aws-sdk-net\n* Come join the AWS .NET community chat on [gitter](https://gitter.im/aws/aws-sdk-net)\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home)\n* If it turns out that you may have found a bug, please open an [issue][sdk-issues]\n\n### Opening Issues\n\nIf you encounter a bug with AWS SDK for .NET we would like to hear about it. Search the existing [issues](https://github.com/aws/aws-sdk-net/issues?q=is%3Aissue) and try to make sure your problem doesn\u2019t already exist before opening a new issue. It\u2019s helpful if you include the version of AWS SDK .NET and the OS you\u2019re using. Please include a stack trace and reduced repro case when appropriate, too.\n\nThe [GitHub issues][sdk-issues] are intended for bug reports and feature requests. For help and questions with using AWS SDK for .NET please make use of the resources listed in the Getting Help section. There are limited resources available for handling issues and by keeping the list of open issues clean we can respond in a timely manner.\n\n## SDK Change Log\n\nThe change log for the SDK can be found in the [SDK.CHANGELOG.ALL.md](https://github.com/aws/aws-sdk-net/blob/master/changelogs/SDK.CHANGELOG.ALL.md) file. Change logs divided up by year can be found in the [changelogs folder](https://github.com/aws/aws-sdk-net/tree/master/changelogs).\n\n## Maintenance and support for SDK major versions\n\nFor information about maintenance and support for SDK major versions and their underlying dependencies, see the following in the AWS SDKs and Tools Shared Configuration and Credentials Reference Guide:\n\n* [AWS SDKs and Tools Maintenance Policy](https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html)\n* [AWS SDKs and Tools Version Support Matrix](https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html)\n\n## Modularization\n\nWith version 3 of the AWS SDK for .NET the SDK has been modularized. This means a separate NuGet package is created for each service as well as a core project. To use this branch compile the solution in the **sdk** folder that matches the desired platform and then include the assemblies for the services needed as well as the core assembly.\n\n## Versioning\n\nThe AWS SDK for .NET uses a 4 part versioning scheme following the pattern of `w.x.y.z`. \n\n* **w** - Incremented for code breaking changes.\n* **x** - Incremented for binary breaking changes. In particular this is used to identitfy what versions of AWSSDK.Core are binary compatible with the service packages. For example if AWSSDK.SQS version 3.3.1.0 came out today when the current version of AWSSDK.Core is 3.3.17.5 then AWSSDK.SQS 3.3.1.0 would be compatible with all versions of AWSSDK.Core starting from 3.3.17.5 up to but not including a future 3.4.0.0.\n* **y** - Incremented for a new SDK feature, like new credential management, or an AWS service update.\n* **z** - Incremented for a bug fix or for service packages to update to the latest AWSSDK.Core to pull in latest bug fixes.\n\nThe SDK assemblies are strongly named which requires consumers of the SDK to recompile every time the `AssemblyVersion` attribute is incremented. To avoid forced recompilations the `AssemblyVersion` only contains the `w.x` portion of the version. The full `w.x.y.z` version number is set in the `AssemblyFileVersion` attribute which is not part of the strong name.\n\n### Internal Namespace\n\nClasses and interfaces with `Internal` in the namespace name are logically internal to the SDK but are often marked with a `public` access modifier, generally to allow the service-specific packages to use shared functionality in the Core package. Classes and interfaces in these namespaces are subject to modification or removal outside of versioning scheme described above. If you find yourself relying on `Internal` functionality directly, consider [opening a Feature Request](https://github.com/aws/aws-sdk-net/issues/new/choose) for your use case if one does not already exist.\n\n## Code Analyzers\n\nEach service package includes a code analyzer that's automatically included when installing from NuGet. These analyzers are created based on the rules from the service model, and will generate a warning if you use a property value that's not valid (for example, shorter than the minimum length expected by the service).\n\nStarting with the 3.7.200 versions of the AWS SDK for .NET NuGet packages, the analyzers target [.NET Standard 2.0](https://learn.microsoft.com/en-us/dotnet/standard/net-standard) and support Visual Studio 2019 RTM and later (announcement: https://github.com/aws/aws-sdk-net/issues/2998).\n\nUsers on Visual Studio 2017 (or earlier) can still use new versions of the SDK, but there'll be a new warning (`CS8032`) about analyzers failing to run. If the build system is configured to treat warnings as errors, this new warning will need to be suppressed or ignored (using [WarningsNotAsErrors](https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/compiler-options/errors-warnings#warningsaserrors-and-warningsnotaserrors)).\n\n## Unity Support\n\nStarting with version 3.5 of the AWS SDK for .NET, projects using Unity 2018.1 or later should target the .NET Standard 2.0 release of the SDK. You can find additional information in the developer guide: [Unity support](https://docs.aws.amazon.com/sdk-for-net/latest/developer-guide/unity-special.html) and [Migrating your Unity application](https://docs.aws.amazon.com/sdk-for-net/latest/developer-guide/net-dg-v35.html#net-dg-v35-changes-unity).\n\nYou can find the archive for _**legacy**_ Unity support at https://github.com/aws/aws-sdk-unity-net.\n\n## Functionality requiring AWS Common Runtime (CRT)\n\nThis SDK has optional functionality that requires the [AWS Common Runtime (CRT)](https://docs.aws.amazon.com/sdkref/latest/guide/common-runtime.html)\nbindings to be included as a dependency with your application. This functionality includes:\n* [Amazon S3 Multi-Region Access Points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPoints.html)\n* [Amazon S3 Object Integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)\n* Amazon EventBridge Global Endpoints\n\nIf the required AWS Common Runtime components are not installed you will receive an error like `Attempting to make a request that requires an implementation of AWS Signature V4a. Add a reference to the AWSSDK.Extensions.CrtIntegration NuGet package to your project to include the AWS Signature V4a signer.`,\nindicating that the required dependency is missing to use the associated functionality. To install this dependency follow\nthe provided [instructions](#installing-the-aws-common-runtime-crt-dependency).\n\n### Installing the AWS Common Runtime (CRT) Dependency\n\nAdd a reference to the NuGet package [AWSSDK.Extensions.CrtIntegration](https://www.nuget.org/packages/AWSSDK.Extensions.CrtIntegration/) to your project.\n\n## Tests\n\n**Important:** Do not run the integration tests on a production account.\n\nIntegration tests can be found in the **AWSSDK.IntegrationTests** project. These test assume that a default profile has been \nconfigured for credentials. For information about setting up a default profile read the [Developer Guide][credentials-management].\n\nThe tests are designed to create and delete the resources needed for testing but it is important to keep your data safe. Do not run\nthese tests on accounts that contain production data or resources. Since AWS resources are created and deleted during the running \nof these tests, charges can occur. To reduce charges occurred by running the tests the test focus on AWS resources that have minimal cost.\n\nUnit tests can be found in the **AWSSDK.UnitTests** project.\n\n### NuGet Packages\n\n* [AWSSDK.AccessAnalyzer](https://www.nuget.org/packages/AWSSDK.AccessAnalyzer/)\n\t* Introducing AWS IAM Access Analyzer, an IAM feature that makes it easy for AWS customers to ensure that their resource-based policies provide only the intended access to resources outside their AWS accounts.\n* [AWSSDK.Account](https://www.nuget.org/packages/AWSSDK.Account/)\n\t* This release of the Account Management API enables customers to manage the alternate contacts for their AWS accounts. For more information, see https://docs.aws.amazon.com/accounts/latest/reference/accounts-welcome.html\n* [AWSSDK.ACMPCA](https://www.nuget.org/packages/AWSSDK.ACMPCA/)\n\t* AWS Certificate Manager (ACM) Private Certificate Authority (CA) is a managed private CA service that helps you easily and securely manage the lifecycle of your private certificates. ACM Private CA provides you a highly-available private CA service without the upfront investment and ongoing maintenance costs of operating your own private CA. ACM Private CA extends ACM's certificate management capabilities to private certificates, enabling you to manage public and private certificates centrally.\n* [AWSSDK.AlexaForBusiness](https://www.nuget.org/packages/AWSSDK.AlexaForBusiness/)\n\t* Alexa for Business is now generally available for production use. Alexa for Business makes it easy for you to use Alexa in your organization. The Alexa for Business SDK gives you APIs to manage Alexa devices, enroll users, and assign skills at scale. For more information about Alexa for Business, go to https://aws.amazon.com/alexaforbusiness\n* [AWSSDK.Amplify](https://www.nuget.org/packages/AWSSDK.Amplify/)\n\t* Amplify is a fully managed continuous deployment and hosting service for modern web apps.\n* [AWSSDK.AmplifyBackend](https://www.nuget.org/packages/AWSSDK.AmplifyBackend/)\n\t* (New Service) The Amplify Admin UI offers an accessible way to develop app backends and manage app content. We recommend that you use the Amplify Admin UI to manage the backend of your Amplify app.\n* [AWSSDK.AmplifyUIBuilder](https://www.nuget.org/packages/AWSSDK.AmplifyUIBuilder/)\n\t* This release introduces the actions and data types for the new Amplify UI Builder API. The Amplify UI Builder API provides a programmatic interface for creating and configuring user interface (UI) component libraries and themes for use in Amplify applications.\n* [AWSSDK.APIGateway](https://www.nuget.org/packages/AWSSDK.APIGateway/)\n\t* Amazon API Gateway helps developers deliver robust, secure and scalable mobile and web application backends. Amazon API Gateway allows developers to securely connect mobile and web applications to APIs that run on AWS Lambda, Amazon EC2, or other publicly addressable web services that are hosted outside of AWS.\n* [AWSSDK.ApiGatewayManagementApi](https://www.nuget.org/packages/AWSSDK.ApiGatewayManagementApi/)\n\t* This is the initial SDK release for the Amazon API Gateway Management API, which allows you to directly manage runtime aspects of your APIs. This release makes it easy to send data directly to clients connected to your WebSocket-based APIs.\n* [AWSSDK.ApiGatewayV2](https://www.nuget.org/packages/AWSSDK.ApiGatewayV2/)\n\t* This is the initial SDK release for the Amazon API Gateway v2 APIs. This SDK will allow you to manage and configure APIs in Amazon API Gateway; this first release provides the capabilities that allow you to programmatically setup and manage WebSocket APIs end to end.\n* [AWSSDK.AppConfig](https://www.nuget.org/packages/AWSSDK.AppConfig/)\n\t* Introducing AWS AppConfig, a new service that enables customers to quickly deploy validated configurations to applications of any size in a controlled and monitored fashion.\n* [AWSSDK.AppConfigData](https://www.nuget.org/packages/AWSSDK.AppConfigData/)\n\t* AWS AppConfig Data is a new service that allows you to retrieve configuration deployed by AWS AppConfig. See the AppConfig user guide for more details on getting started. https://docs.aws.amazon.com/appconfig/latest/userguide/what-is-appconfig.html\n* [AWSSDK.AppFabric](https://www.nuget.org/packages/AWSSDK.AppFabric/)\n\t* Initial release of AWS AppFabric for connecting SaaS applications for better productivity and security.\n* [AWSSDK.Appflow](https://www.nuget.org/packages/AWSSDK.Appflow/)\n\t* Amazon AppFlow is a fully managed integration service that securely transfers data between AWS services and SaaS applications. This update releases the first version of Amazon AppFlow APIs and SDK.\n* [AWSSDK.AppIntegrationsService](https://www.nuget.org/packages/AWSSDK.AppIntegrationsService/)\n\t* The Amazon AppIntegrations service (in preview release) enables you to configure and reuse connections to external applications.\n* [AWSSDK.ApplicationAutoScaling](https://www.nuget.org/packages/AWSSDK.ApplicationAutoScaling/)\n\t* Application Auto Scaling is a general purpose Auto Scaling service for supported elastic AWS resources. With Application Auto Scaling, you can automatically scale your AWS resources, with an experience similar to that of Amazon EC2 Auto Scaling.\n* [AWSSDK.ApplicationCostProfiler](https://www.nuget.org/packages/AWSSDK.ApplicationCostProfiler/)\n\t* APIs for AWS Application Cost Profiler.\n* [AWSSDK.ApplicationDiscoveryService](https://www.nuget.org/packages/AWSSDK.ApplicationDiscoveryService/)\n\t* AWS Application Discovery Service helps Systems Integrators quickly and reliably plan application migration projects by automatically identifying applications running in your data center, their associated dependencies, and their performance profile.\n* [AWSSDK.ApplicationInsights](https://www.nuget.org/packages/AWSSDK.ApplicationInsights/)\n\t* CloudWatch Application Insights detects errors and exceptions from logs, including .NET custom application logs, SQL Server logs, IIS logs, and more, and uses a combination of built-in rules and machine learning, such as dynamic baselining, to identify common problems. You can then easily drill into specific issues with CloudWatch Automatic Dashboards that are dynamically generated. These dashboards contain the most recent alarms, a summary of relevant metrics, and log snippets to help you identify root cause.\n* [AWSSDK.AppMesh](https://www.nuget.org/packages/AWSSDK.AppMesh/)\n\t* AWS App Mesh is a service mesh that makes it easy to monitor and control communications between microservices of an application. AWS App Mesh APIs are available for preview in eu-west-1, us-east-1, us-east-2, and us-west-2 regions.\n* [AWSSDK.AppRegistry](https://www.nuget.org/packages/AWSSDK.AppRegistry/)\n\t* AWS Service Catalog AppRegistry provides a repository of your applications, their resources, and the application metadata that you use within your enterprise.\n* [AWSSDK.AppRunner](https://www.nuget.org/packages/AWSSDK.AppRunner/)\n\t* AWS App Runner is a service that provides a fast, simple, and cost-effective way to deploy from source code or a container image directly to a scalable and secure web application in the AWS Cloud.\n* [AWSSDK.AppStream](https://www.nuget.org/packages/AWSSDK.AppStream/)\n\t* Amazon AppStream is a fully managed, secure application streaming service that allows you to stream desktop applications from AWS to a web browser.\n* [AWSSDK.AppSync](https://www.nuget.org/packages/AWSSDK.AppSync/)\n\t* AWS AppSync is an enterprise-level, fully managed GraphQL service with real-time data synchronization and offline programming features.\n* [AWSSDK.ARCZonalShift](https://www.nuget.org/packages/AWSSDK.ARCZonalShift/)\n\t* Amazon Route 53 Application Recovery Controller Zonal Shift is a new service that makes it easy to shift traffic away from an Availability Zone in a Region. See the developer guide for more information: https://docs.aws.amazon.com/r53recovery/latest/dg/what-is-route53-recovery.html\n* [AWSSDK.Artifact](https://www.nuget.org/packages/AWSSDK.Artifact/)\n\t* This is the initial SDK release for AWS Artifact. AWS Artifact provides on-demand access to compliance and third-party compliance reports. This release includes access to List and Get reports, along with their metadata. This release also includes access to AWS Artifact notifications settings.\n* [AWSSDK.Athena](https://www.nuget.org/packages/AWSSDK.Athena/)\n\t* This release adds support for Amazon Athena. Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.\n* [AWSSDK.AuditManager](https://www.nuget.org/packages/AWSSDK.AuditManager/)\n\t* AWS Audit Manager helps you continuously audit your AWS usage to simplify how you manage risk and compliance. This update releases the first version of the AWS Audit Manager APIs and SDK.\n* [AWSSDK.AugmentedAIRuntime](https://www.nuget.org/packages/AWSSDK.AugmentedAIRuntime/)\n\t* This release adds support for Amazon Augmented AI, which makes it easy to build workflows for human review of machine learning predictions.\n* [AWSSDK.AutoScaling](https://www.nuget.org/packages/AWSSDK.AutoScaling/)\n\t* Auto Scaling helps you maintain application availability and allows you to scale your capacity up or down automatically according to conditions you define.\n* [AWSSDK.AutoScalingPlans](https://www.nuget.org/packages/AWSSDK.AutoScalingPlans/)\n\t* AWS Auto Scaling enables you to quickly discover all of the scalable resources underlying your application and set up application scaling in minutes using built-in scaling recommendations.\n* [AWSSDK.AWSHealth](https://www.nuget.org/packages/AWSSDK.AWSHealth/)\n\t* The AWS Health API serves as the primary source for you to receive personalized information related to your AWS infrastructure, guiding your through scheduled changes, and accelerating the troubleshooting of issues impacting your AWS resources and accounts.\n* [AWSSDK.AWSMarketplaceCommerceAnalytics](https://www.nuget.org/packages/AWSSDK.AWSMarketplaceCommerceAnalytics/)\n\t* The AWS Marketplace Commerce Analytics service allows marketplace partners to programmatically request business intelligence data from AWS Marketplace. This service provides the same data that was previously only available through the AWS Marketplace Management Portal, but offers the data in a fully-machine-readable format and available in fine-grained data sets rather than large reports.\n* [AWSSDK.AWSMarketplaceMetering](https://www.nuget.org/packages/AWSSDK.AWSMarketplaceMetering/)\n\t* The AWS Marketplace Metering Service enables sellers to price their products along new pricing dimensions. After a integrating their product with the AWS Marketplace Metering Service, that product will emit an hourly record capturing the usage of any single pricing dimension. Buyers can easily subscribe to software priced by this new dimension on the AWS Marketplace website and only pay for what they use.\n* [AWSSDK.AWSSupport](https://www.nuget.org/packages/AWSSDK.AWSSupport/)\n\t* The AWS Support API provides methods for creating and managing AWS Support cases and for retrieving the results of AWS Trusted Advisor checks.\n* [AWSSDK.B2bi](https://www.nuget.org/packages/AWSSDK.B2bi/)\n\t* This is the initial SDK release for AWS B2B Data Interchange.\n* [AWSSDK.Backup](https://www.nuget.org/packages/AWSSDK.Backup/)\n\t* AWS Backup is a fully managed backup service that makes it easy to centralize and automate the back up of data across AWS services in the cloud as well as on-premises.\n* [AWSSDK.BackupGateway](https://www.nuget.org/packages/AWSSDK.BackupGateway/)\n\t* Initial release of AWS Backup gateway which enables you to centralize and automate protection of on-premises VMware and VMware Cloud on AWS workloads using AWS Backup.\n* [AWSSDK.BackupStorage](https://www.nuget.org/packages/AWSSDK.BackupStorage/)\n\t* This is the first public release of AWS Backup Storage. We are exposing some previously-internal APIs for use by external services. These APIs are not meant to be used directly by customers.\n* [AWSSDK.Batch](https://www.nuget.org/packages/AWSSDK.Batch/)\n\t* AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. \n* [AWSSDK.BCMDataExports](https://www.nuget.org/packages/AWSSDK.BCMDataExports/)\n\t* Users can create, read, update, delete Exports of billing and cost management data.  Users can get details of Export Executions and details of Tables for exporting.  Tagging support is provided for Exports\n* [AWSSDK.Bedrock](https://www.nuget.org/packages/AWSSDK.Bedrock/)\n\t* Model Invocation logging added to enable or disable logs in customer account. Model listing and description support added. Provisioned Throughput feature added. Custom model support added for creating custom models. Also includes list, and delete functions for custom model.\n* [AWSSDK.BedrockAgent](https://www.nuget.org/packages/AWSSDK.BedrockAgent/)\n\t* This release introduces Agents for Amazon Bedrock\n* [AWSSDK.BedrockAgentRuntime](https://www.nuget.org/packages/AWSSDK.BedrockAgentRuntime/)\n\t* This release introduces Agents for Amazon Bedrock Runtime\n* [AWSSDK.BedrockRuntime](https://www.nuget.org/packages/AWSSDK.BedrockRuntime/)\n\t* Run Inference: Added support to run the inference on models.  Includes set of APIs for running inference in streaming and non-streaming mode.\n* [AWSSDK.BillingConductor](https://www.nuget.org/packages/AWSSDK.BillingConductor/)\n\t* This is the initial SDK release for AWS Billing Conductor. The AWS Billing Conductor is a customizable billing service, allowing you to customize your billing data to match your desired business structure.\n* [AWSSDK.Braket](https://www.nuget.org/packages/AWSSDK.Braket/)\n\t* Amazon Braket general availability with Device and Quantum Task operations.\n* [AWSSDK.Budgets](https://www.nuget.org/packages/AWSSDK.Budgets/)\n\t* AWS Budget service will provide create/get/list/update/delete budgets for cost management. \n* [AWSSDK.CertificateManager](https://www.nuget.org/packages/AWSSDK.CertificateManager/)\n\t* AWS Certificate Manager (ACM) is an AWS service that makes it easier for you to deploy secure SSL based websites and applications on the AWS platform.\n* [AWSSDK.Chatbot](https://www.nuget.org/packages/AWSSDK.Chatbot/)\n\t* This release adds support for AWS Chatbot. You can now monitor, operate, and troubleshoot your AWS resources with interactive ChatOps using the AWS SDK.\n* [AWSSDK.Chime](https://www.nuget.org/packages/AWSSDK.Chime/)\n\t* The Amazon Chime API (application programming interface) is designed for administrators to use to perform key tasks, such as creating and managing Amazon Chime accounts and users.\n* [AWSSDK.ChimeSDKIdentity](https://www.nuget.org/packages/AWSSDK.ChimeSDKIdentity/)\n\t* The Amazon Chime SDK Identity APIs allow software developers to create and manage unique instances of their messaging applications.\n* [AWSSDK.ChimeSDKMediaPipelines](https://www.nuget.org/packages/AWSSDK.ChimeSDKMediaPipelines/)\n\t* For Amazon Chime SDK meetings, the Amazon Chime Media Pipelines SDK allows builders to capture audio, video, and content share streams. You can also capture meeting events, live transcripts, and data messages. The pipelines save the artifacts to an Amazon S3 bucket that you designate.\n* [AWSSDK.ChimeSDKMeetings](https://www.nuget.org/packages/AWSSDK.ChimeSDKMeetings/)\n\t* The Amazon Chime SDK Meetings APIs allow software developers to create meetings and attendees for interactive audio, video, screen and content sharing in custom meeting applications which use the Amazon Chime SDK.\n* [AWSSDK.ChimeSDKMessaging](https://www.nuget.org/packages/AWSSDK.ChimeSDKMessaging/)\n\t* The Amazon Chime SDK Messaging APIs allow software developers to send and receive messages in custom messaging applications.\n* [AWSSDK.ChimeSDKVoice](https://www.nuget.org/packages/AWSSDK.ChimeSDKVoice/)\n\t* Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See https://docs.aws.amazon.com/chime-sdk/latest/dg/sdk-available-regions.html for more details.\n* [AWSSDK.CleanRooms](https://www.nuget.org/packages/AWSSDK.CleanRooms/)\n\t* Initial release of AWS Clean Rooms\n* [AWSSDK.CleanRoomsML](https://www.nuget.org/packages/AWSSDK.CleanRoomsML/)\n\t* Public Preview SDK release of AWS Clean Rooms ML APIs\n* [AWSSDK.Cloud9](https://www.nuget.org/packages/AWSSDK.Cloud9/)\n\t* Adds support for creating and managing AWS Cloud9 development environments. AWS Cloud9 is a cloud-based integrated development environment (IDE) that you use to write, run, and debug code.\n* [AWSSDK.CloudControlApi](https://www.nuget.org/packages/AWSSDK.CloudControlApi/)\n\t* Initial release of the SDK for AWS Cloud Control API\n* [AWSSDK.CloudDirectory](https://www.nuget.org/packages/AWSSDK.CloudDirectory/)\n\t* Cloud Directory (CD) is a multi-tenant, hierarchical data store for use by other AWS services to store directory data for AWS resources, including both metadata about resources and policy data governing resources.\n* [AWSSDK.CloudFormation](https://www.nuget.org/packages/AWSSDK.CloudFormation/)\n\t* AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.\n* [AWSSDK.CloudFront](https://www.nuget.org/packages/AWSSDK.CloudFront/)\n\t* Amazon CloudFront is a content delivery web service. It integrates with other Amazon Web Services products to give developers and businesses an easy way to distribute content to end users with low latency, high data transfer speeds, and no minimum usage commitments.\n* [AWSSDK.CloudFrontKeyValueStore](https://www.nuget.org/packages/AWSSDK.CloudFrontKeyValueStore/)\n\t* This release adds support for CloudFront KeyValueStore, a globally managed key value datastore associated with CloudFront Functions.\n* [AWSSDK.CloudHSM](https://www.nuget.org/packages/AWSSDK.CloudHSM/)\n\t* The AWS CloudHSM service helps you meet corporate, contractual and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) appliances within the AWS cloud. With CloudHSM, you control the encryption keys and cryptographic operations performed by the HSM.\n* [AWSSDK.CloudHSMV2](https://www.nuget.org/packages/AWSSDK.CloudHSMV2/)\n\t* CloudHSM provides hardware security modules for protecting sensitive data and cryptographic keys within an EC2 VPC, and enable the customer to maintain control over key access and use. This is a second-generation of the service that will improve security, lower cost and provide better customer usability.\n* [AWSSDK.CloudSearch](https://www.nuget.org/packages/AWSSDK.CloudSearch/)\n\t* Amazon CloudSearch is a managed service in the AWS Cloud that makes it simple and cost-effective to set up, manage, and scale a search solution for your website or application.\n* [AWSSDK.CloudSearchDomain](https://www.nuget.org/packages/AWSSDK.CloudSearchDomain/)\n\t* Amazon CloudSearch Domain encapsulates a collection of data you want to search, the search instances that process your search requests, and a configuration that controls how your data is indexed and searched.\n* [AWSSDK.CloudTrail](https://www.nuget.org/packages/AWSSDK.CloudTrail/)\n\t* AWS CloudTrail is a web service that records AWS API calls for your account and delivers log files to you. The recorded information includes the identity of the API caller, the time of the API call, the source IP address of the API caller, the request parameters, and the response elements returned by the AWS service.\n* [AWSSDK.CloudTrailData](https://www.nuget.org/packages/AWSSDK.CloudTrailData/)\n\t* Add CloudTrail Data Service to enable users to ingest activity events from non-AWS sources into CloudTrail Lake.\n* [AWSSDK.CloudWatch](https://www.nuget.org/packages/AWSSDK.CloudWatch/)\n\t* Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms.\n* [AWSSDK.CloudWatchEvents](https://www.nuget.org/packages/AWSSDK.CloudWatchEvents/)\n\t* Amazon CloudWatch Events helps you to respond to state changes in your AWS resources. When your resources change state they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to targets to take action. You can also use rules to take action on a pre-determined schedule.\n* [AWSSDK.CloudWatchEvidently](https://www.nuget.org/packages/AWSSDK.CloudWatchEvidently/)\n\t* Introducing Amazon CloudWatch Evidently. This is the first public release of Amazon CloudWatch Evidently.\n* [AWSSDK.CloudWatchLogs](https://www.nuget.org/packages/AWSSDK.CloudWatchLogs/)\n\t* Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms.\n* [AWSSDK.CloudWatchRUM](https://www.nuget.org/packages/AWSSDK.CloudWatchRUM/)\n\t* This is the first public release of CloudWatch RUM\n* [AWSSDK.CodeArtifact](https://www.nuget.org/packages/AWSSDK.CodeArtifact/)\n\t* Added support for AWS CodeArtifact.\n* [AWSSDK.CodeBuild](https://www.nuget.org/packages/AWSSDK.CodeBuild/)\n\t* AWS CodeBuild is a fully-managed build service in the cloud. AWS CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy.\n* [AWSSDK.CodeCatalyst](https://www.nuget.org/packages/AWSSDK.CodeCatalyst/)\n\t* This release adds operations that support customers using the AWS Toolkits and Amazon CodeCatalyst, a unified software development service that helps developers develop, deploy, and maintain applications in the cloud. For more information, see the documentation.\n* [AWSSDK.CodeCommit](https://www.nuget.org/packages/AWSSDK.CodeCommit/)\n\t* AWS CodeCommit is a fully-managed source control service that makes it easy for companies to host secure and highly scalable private Git repositories.\n* [AWSSDK.CodeDeploy](https://www.nuget.org/packages/AWSSDK.CodeDeploy/)\n\t* AWS CodeDeploy is a service that automates code deployments. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.\n* [AWSSDK.CodeGuruProfiler](https://www.nuget.org/packages/AWSSDK.CodeGuruProfiler/)\n\t* (New Service) Amazon CodeGuru Profiler analyzes application CPU utilization and latency characteristics to show you where you are spending the most cycles in your application. This analysis is presented in an interactive flame graph that helps you easily understand which paths consume the most resources, verify that your application is performing as expected, and uncover areas that can be optimized further.\n* [AWSSDK.CodeGuruReviewer](https://www.nuget.org/packages/AWSSDK.CodeGuruReviewer/)\n\t* This is the preview release of Amazon CodeGuru Reviewer.\n* [AWSSDK.CodeGuruSecurity](https://www.nuget.org/packages/AWSSDK.CodeGuruSecurity/)\n\t* Initial release of Amazon CodeGuru Security APIs\n* [AWSSDK.CodePipeline](https://www.nuget.org/packages/AWSSDK.CodePipeline/)\n\t* AWS CodePipeline is a continuous delivery service for fast and reliable application updates.\n* [AWSSDK.CodeStar](https://www.nuget.org/packages/AWSSDK.CodeStar/)\n\t* AWS CodeStar is a cloud-based service for creating, managing, and working with software development projects on AWS. An AWS CodeStar project creates and integrates AWS services for your project development toolchain. AWS CodeStar also manages the permissions required for project users.\n* [AWSSDK.CodeStarconnections](https://www.nuget.org/packages/AWSSDK.CodeStarconnections/)\n\t* Public beta for Bitbucket Cloud support in AWS CodePipeline through integration with AWS CodeStar connections.\n* [AWSSDK.CodeStarNotifications](https://www.nuget.org/packages/AWSSDK.CodeStarNotifications/)\n\t* This release adds a notification manager for events in repositories, build projects, deployments, and pipelines. You can now configure rules and receive notifications about events that occur for resources. Each notification includes a status message as well as a link to the resource (repository, build project, deployment application, or pipeline) whose event generated the notification.\n* [AWSSDK.CognitoIdentity](https://www.nuget.org/packages/AWSSDK.CognitoIdentity/)\n\t* Amazon Cognito is a service that makes it easy to save user data, such as app preferences or game state, in the AWS Cloud without writing any backend code or managing any infrastructure. With Amazon Cognito, you can focus on creating great app experiences instead of having to worry about building and managing a backend solution to handle identity management, network state, storage, and sync.\n* [AWSSDK.CognitoIdentityProvider](https://www.nuget.org/packages/AWSSDK.CognitoIdentityProvider/)\n\t* You can create a user pool in Amazon Cognito Identity to manage directories and users. You can authenticate a user to obtain tokens related to user identity and access policies. This API reference provides information about user pools in Amazon Cognito Identity, which is a new capability that is available as a beta.\n* [AWSSDK.CognitoSync](https://www.nuget.org/packages/AWSSDK.CognitoSync/)\n\t* Amazon Cognito is a service that makes it easy to save user data, such as app preferences or game state, in the AWS Cloud without writing any backend code or managing any infrastructure. With Amazon Cognito, you can focus on creating great app experiences instead of having to worry about building and managing a backend solution to handle identity management, network state, storage, and sync.\n* [AWSSDK.Comprehend](https://www.nuget.org/packages/AWSSDK.Comprehend/)\n\t* Amazon Comprehend is an AWS service for gaining insight into the content of text and documents. It can be used to determine the topics contained in your documents, the topics they discuss, the  predominant sentiment expressed in them, the predominant language used, and more. For more information, go to the Amazon Comprehend product page. To get started, see the Amazon Comprehend Developer Guide.\n* [AWSSDK.ComprehendMedical](https://www.nuget.org/packages/AWSSDK.ComprehendMedical/)\n\t* The first release of Comprehend Medical includes two APIs, detectPHI and detectEntities. DetectPHI extracts PHI from your clinical text, and detectEntities extracts entities such as medication, medical conditions, or anatomy. DetectEntities also extracts attributes (e.g. dosage for medication) and identifies contextual traits (e.g. negation) for each entity.\n* [AWSSDK.ComputeOptimizer](https://www.nuget.org/packages/AWSSDK.ComputeOptimizer/)\n\t* Initial release of AWS Compute Optimizer. AWS Compute Optimizer recommends optimal AWS Compute resources to reduce costs and improve performance for your workloads.\n* [AWSSDK.ConfigService](https://www.nuget.org/packages/AWSSDK.ConfigService/)\n\t* AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance.\n* [AWSSDK.Connect](https://www.nuget.org/packages/AWSSDK.Connect/)\n\t* Amazon Connect is a self-service, cloud-based contact center service that makes it easy for any business to deliver better customer service at lower cost.\n* [AWSSDK.ConnectCampaignService](https://www.nuget.org/packages/AWSSDK.ConnectCampaignService/)\n\t* Added Amazon Connect high volume outbound communications SDK.\n* [AWSSDK.ConnectCases](https://www.nuget.org/packages/AWSSDK.ConnectCases/)\n\t* This release adds APIs for Amazon Connect Cases. Cases allows your agents to quickly track and manage customer issues that require multiple interactions, follow-up tasks, and teams in your contact center.  For more information, see https://docs.aws.amazon.com/cases/latest/APIReference/Welcome.html\n* [AWSSDK.ConnectContactLens](https://www.nuget.org/packages/AWSSDK.ConnectContactLens/)\n\t* Contact Lens for Amazon Connect analyzes conversations, both real-time and post-call. The ListRealtimeContactAnalysisSegments API returns a list of analysis segments for a real-time analysis session.\n* [AWSSDK.ConnectParticipant](https://www.nuget.org/packages/AWSSDK.ConnectParticipant/)\n\t* This release adds 5 new APIs: CreateParticipantConnection, DisconnectParticipant, GetTranscript, SendEvent, and SendMessage. For Amazon Connect chat, you can use them to programmatically perform participant actions on the configured Amazon Connect instance. Learn more here: https://docs.aws.amazon.com/connect-participant/latest/APIReference/Welcome.html\n* [AWSSDK.ConnectWisdomService](https://www.nuget.org/packages/AWSSDK.ConnectWisdomService/)\n\t* Released Amazon Connect Wisdom, a feature of Amazon Connect, which provides real-time recommendations and search functionality in general availability (GA).  For more information, see https://docs.aws.amazon.com/wisdom/latest/APIReference/Welcome.html.\n* [AWSSDK.ControlTower](https://www.nuget.org/packages/AWSSDK.ControlTower/)\n\t* This release contains the first SDK for AWS Control Tower. It introduces  a new set of APIs: EnableControl, DisableControl, GetControlOperation, and ListEnabledControls.\n* [AWSSDK.CostAndUsageReport](https://www.nuget.org/packages/AWSSDK.CostAndUsageReport/)\n\t* The AWS Cost and Usage Report Service API allows you to enable and disable the Cost and Usage report, as well as modify the report name, the data granularity, and the delivery preferences.\n* [AWSSDK.CostExplorer](https://www.nuget.org/packages/AWSSDK.CostExplorer/)\n\t* The AWS Cost Explorer API gives customers programmatic access to AWS cost and usage information, allowing them to perform adhoc queries and build interactive cost management applications that leverage this dataset.\n* [AWSSDK.CostOptimizationHub](https://www.nuget.org/packages/AWSSDK.CostOptimizationHub/)\n\t* This release launches Cost Optimization Hub, a new AWS Billing and Cost Management feature that helps you consolidate and prioritize cost optimization recommendations across your AWS Organizations member accounts and AWS Regions, so that you can get the most out of your AWS spend.\n* [AWSSDK.CustomerProfiles](https://www.nuget.org/packages/AWSSDK.CustomerProfiles/)\n\t* This is the first release of Amazon Connect Customer Profiles, a unified customer profile for your Amazon Connect contact center.\n* [AWSSDK.DatabaseMigrationService](https://www.nuget.org/packages/AWSSDK.DatabaseMigrationService/)\n\t* AWS Database Migration Service (AWS DMS) can migrate your data to and from most widely used commercial and open-source databases such as Oracle, PostgreSQL, Microsoft SQL Server, Amazon Aurora, MariaDB, and MySQL. The service supports homogeneous migrations such as Oracle to Oracle, and also heterogeneous migrations between different database platforms, such as Oracle to MySQL or MySQL to Amazon Aurora.\n* [AWSSDK.DataExchange](https://www.nuget.org/packages/AWSSDK.DataExchange/)\n\t* Introducing AWS Data Exchange, a service that makes it easy for AWS customers to securely create, manage, access, and exchange data sets in the cloud.\n* [AWSSDK.DataPipeline](https://www.nuget.org/packages/AWSSDK.DataPipeline/)\n\t* AWS Data Pipeline is a managed extract-transform-load (ETL) service that helps you reliably and cost-effectively move and process data across your on-premise data stores and AWS services.\n* [AWSSDK.DataSync](https://www.nuget.org/packages/AWSSDK.DataSync/)\n\t* AWS DataSync simplifies, automates, and accelerates moving and replicating data between on-premises storage and AWS services over the network.\n* [AWSSDK.DataZone](https://www.nuget.org/packages/AWSSDK.DataZone/)\n\t* Initial release of Amazon DataZone\n* [AWSSDK.DAX](https://www.nuget.org/packages/AWSSDK.DAX/)\n\t* Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement - from milliseconds to microseconds - even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.\n* [AWSSDK.Detective](https://www.nuget.org/packages/AWSSDK.Detective/)\n\t* This is the initial release of Amazon Detective.\n* [AWSSDK.DeviceFarm](https://www.nuget.org/packages/AWSSDK.DeviceFarm/)\n\t* AWS Device Farm is an app testing service that enables you to test your Android and Fire OS apps on real, physical phones and tablets that are hosted by AWS. The service allows you to upload your own tests or use built-in, script-free compatibility tests.\n* [AWSSDK.DevOpsGuru](https://www.nuget.org/packages/AWSSDK.DevOpsGuru/)\n\t* (New Service) Amazon DevOps Guru is available in public preview. It's a fully managed service that uses machine learning to analyze your operational solutions to help you find and troubleshoot issues.\n* [AWSSDK.DirectConnect](https://www.nuget.org/packages/AWSSDK.DirectConnect/)\n\t* AWS Direct Connect makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.\n* [AWSSDK.DirectoryService](https://www.nuget.org/packages/AWSSDK.DirectoryService/)\n\t* AWS Directory Service is a managed service that allows you to connect your AWS resources with an existing on-premises Microsoft Active Directory or to set up a new, stand-alone directory in the AWS Cloud.\n* [AWSSDK.DLM](https://www.nuget.org/packages/AWSSDK.DLM/)\n\t* Amazon Data Lifecycle Manager manages lifecycle of your AWS resources.\n* [AWSSDK.DocDB](https://www.nuget.org/packages/AWSSDK.DocDB/)\n\t* Amazon DocumentDB is a fast, reliable, and fully managed MongoDB compatible database service.\n* [AWSSDK.DocDBElastic](https://www.nuget.org/packages/AWSSDK.DocDBElastic/)\n\t* Launched Amazon DocumentDB Elastic Clusters. You can now use the SDK to create, list, update and delete Amazon DocumentDB Elastic Cluster resources\n* [AWSSDK.Drs](https://www.nuget.org/packages/AWSSDK.Drs/)\n\t* Introducing AWS Elastic Disaster Recovery (AWS DRS), a new service that minimizes downtime and data loss with fast, reliable recovery of on-premises and cloud-based applications using affordable storage, minimal compute, and point-in-time recovery.\n* [AWSSDK.DynamoDBv2](https://www.nuget.org/packages/AWSSDK.DynamoDBv2/)\n\t* Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale.\n* [AWSSDK.EBS](https://www.nuget.org/packages/AWSSDK.EBS/)\n\t* This release introduces the EBS direct APIs for Snapshots: 1. ListSnapshotBlocks, which lists the block indexes and block tokens for blocks in an Amazon EBS snapshot. 2. ListChangedBlocks, which lists the block indexes and block tokens for blocks that are different between two snapshots of the same volume/snapshot lineage. 3. GetSnapshotBlock, which returns the data in a block of an Amazon EBS snapshot.\n* [AWSSDK.EC2](https://www.nuget.org/packages/AWSSDK.EC2/)\n\t* Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers.\n* [AWSSDK.EC2InstanceConnect](https://www.nuget.org/packages/AWSSDK.EC2InstanceConnect/)\n\t* Amazon EC2 Instance Connect is a simple and secure way to connect to your instances using Secure Shell (SSH). With EC2 Instance Connect, you can control SSH access to your instances using AWS Identity and Access Management (IAM) policies as well as audit connection requests with AWS CloudTrail events. In addition, you can leverage your existing SSH keys or further enhance your security posture by generating one-time use SSH keys each time an authorized user connects.\n* [AWSSDK.ECR](https://www.nuget.org/packages/AWSSDK.ECR/)\n\t* Amazon EC2 Container Registry (Amazon ECR) is a managed AWS Docker registry service. Customers can use the familiar Docker CLI to push, pull, and manage images.\n* [AWSSDK.ECRPublic](https://www.nuget.org/packages/AWSSDK.ECRPublic/)\n\t* Supports Amazon Elastic Container Registry (Amazon ECR) Public, a fully managed registry that makes it easy for a developer to publicly share container software worldwide for anyone to download.\n* [AWSSDK.ECS](https://www.nuget.org/packages/AWSSDK.ECS/)\n\t* Amazon EC2 Container Service is a highly scalable, high performance container management service that supports Docker containers and allows you to easily run distributed applications on a managed cluster of Amazon EC2 instances.\n* [AWSSDK.EKS](https://www.nuget.org/packages/AWSSDK.EKS/)\n\t* Amazon Elastic Container Service for Kubernetes (Amazon EKS) is a fully managed service that makes it easy to deploy, manage, and scale containerized applications using Kubernetes on AWS.Amazon EKS runs Kubernetes management infrastructure for you across multiple AWS availability zones to eliminate a single point of failure. Amazon EKS is certified Kubernetes conformant so you can use existing tooling and plugins from partners and the Kubernetes community. Applications running on any standard Kubernetes environment are fully compatible and can be easily migrated to Amazon EKS.\n* [AWSSDK.EKSAuth](https://www.nuget.org/packages/AWSSDK.EKSAuth/)\n\t* This release adds support for EKS Pod Identity feature. EKS Pod Identity makes it easy for customers to obtain IAM permissions for their applications running in the EKS clusters.\n* [AWSSDK.ElastiCache](https://www.nuget.org/packages/AWSSDK.ElastiCache/)\n\t* ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based databases.\n* [AWSSDK.ElasticBeanstalk](https://www.nuget.org/packages/AWSSDK.ElasticBeanstalk/)\n\t* AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\n* [AWSSDK.ElasticFileSystem](https://www.nuget.org/packages/AWSSDK.ElasticFileSystem/)\n\t* Amazon Elastic File System (Amazon EFS) is a file storage service for Amazon Elastic Compute Cloud (Amazon EC2) instances.\n* [AWSSDK.ElasticInference](https://www.nuget.org/packages/AWSSDK.ElasticInference/)\n\t* Amazon Elastic Inference allows customers to attach Elastic Inference Accelerators to Amazon EC2 and Amazon ECS tasks, thus providing low-cost GPU-powered acceleration and reducing the cost of running deep learning inference. This release allows customers to add or remove tags for their Elastic Inference Accelerators.\n* [AWSSDK.ElasticLoadBalancing](https://www.nuget.org/packages/AWSSDK.ElasticLoadBalancing/)\n\t* Elastic Load Balancing automatically distributes incoming application traffic across multiple compute instances in the cloud.\n* [AWSSDK.ElasticLoadBalancingV2](https://www.nuget.org/packages/AWSSDK.ElasticLoadBalancingV2/)\n\t* Elastic Load Balancing automatically distributes incoming application traffic across multiple compute instances in the cloud.\n* [AWSSDK.ElasticMapReduce](https://www.nuget.org/packages/AWSSDK.ElasticMapReduce/)\n\t* Amazon Elastic MapReduce (Amazon EMR) is a web service that makes it easy to quickly and cost-effectively process vast amounts of data.\n* [AWSSDK.Elasticsearch](https://www.nuget.org/packages/AWSSDK.Elasticsearch/)\n\t* Use the Amazon Elasticsearch configuration API to create, configure, and manage Elasticsearch domains.\n* [AWSSDK.ElasticTranscoder](https://www.nuget.org/packages/AWSSDK.ElasticTranscoder/)\n\t* Amazon Elastic Transcoder is media transcoding in the cloud. It is designed to be a highly scalable, easy to use and a cost effective way for developers and businesses to convert (or 'transcode') media files from their source format into versions that will playback on devices like smartphones, tablets and PCs.\n* [AWSSDK.EMRContainers](https://www.nuget.org/packages/AWSSDK.EMRContainers/)\n\t* This release adds support for Amazon EMR on EKS, a simple way to run Spark on Kubernetes.\n* [AWSSDK.EMRServerless](https://www.nuget.org/packages/AWSSDK.EMRServerless/)\n\t* This release adds support for Amazon EMR Serverless, a serverless runtime environment that simplifies running analytics applications using the latest open source frameworks such as Apache Spark and Apache Hive.\n* [AWSSDK.EntityResolution](https://www.nuget.org/packages/AWSSDK.EntityResolution/)\n\t* AWS Entity Resolution can effectively match a source record from a customer relationship management (CRM) system with a source record from a marketing system containing campaign information.\n* [AWSSDK.EventBridge](https://www.nuget.org/packages/AWSSDK.EventBridge/)\n\t* Amazon EventBridge is a serverless event bus service that makes it easy to connect your applications with data from a variety of sources, including AWS services, partner applications, and your own applications.\n* [AWSSDK.Finspace](https://www.nuget.org/packages/AWSSDK.Finspace/)\n\t* This is the initial SDK release for the management APIs for Amazon FinSpace. Amazon FinSpace is a data management and analytics service for the financial services industry (FSI).\n* [AWSSDK.FinSpaceData](https://www.nuget.org/packages/AWSSDK.FinSpaceData/)\n\t* This is the initial SDK release for the data APIs for Amazon FinSpace. Amazon FinSpace is a data management and analytics application for the financial services industry (FSI).\n* [AWSSDK.FIS](https://www.nuget.org/packages/AWSSDK.FIS/)\n\t* Initial release of AWS Fault Injection Simulator, a managed service that enables you to perform fault injection experiments on your AWS workloads\n* [AWSSDK.FMS](https://www.nuget.org/packages/AWSSDK.FMS/)\n\t* This release is the initial release version for AWS Firewall Manager, a new AWS service that makes it easy for customers to centrally configure WAF rules across all their resources (ALBs and CloudFront distributions) and across accounts.\n* [AWSSDK.ForecastQueryService](https://www.nuget.org/packages/AWSSDK.ForecastQueryService/)\n\t* Amazon Forecast is a fully managed machine learning service that makes it easy for customers to generate accurate forecasts using their historical time-series data\n* [AWSSDK.ForecastService](https://www.nuget.org/packages/AWSSDK.ForecastService/)\n\t* Amazon Forecast is a fully managed machine learning service that makes it easy for customers to generate accurate forecasts using their historical time-series data\n* [AWSSDK.FraudDetector](https://www.nuget.org/packages/AWSSDK.FraudDetector/)\n\t* Amazon Fraud Detector is a fully managed service that makes it easy to identify potentially fraudulent online activities such as online payment fraud and the creation of fake accounts. Amazon Fraud Detector uses your data, machine learning (ML), and more than 20 years of fraud detection expertise from Amazon to automatically identify potentially fraudulent online activity so you can catch more fraud faster.\n* [AWSSDK.FreeTier](https://www.nuget.org/packages/AWSSDK.FreeTier/)\n\t* This is the initial SDK release for the AWS Free Tier GetFreeTierUsage API\n* [AWSSDK.FSx](https://www.nuget.org/packages/AWSSDK.FSx/)\n\t* Amazon FSx provides fully-managed third-party file systems optimized for a variety of enterprise and compute-intensive workloads.\n* [AWSSDK.GameLift](https://www.nuget.org/packages/AWSSDK.GameLift/)\n\t* Amazon GameLift Service is a managed AWS service for developers who need a scalable, server-based solution for multiplayer games.\n* [AWSSDK.Glacier](https://www.nuget.org/packages/AWSSDK.Glacier/)\n\t* Amazon Glacier is a secure, durable, and extremely low-cost storage service for data archiving and online backup.\n* [AWSSDK.GlobalAccelerator](https://www.nuget.org/packages/AWSSDK.GlobalAccelerator/)\n\t* AWS Global Accelerator is a network layer service that helps you improve the availability and performance of the applications that you offer to your global customers. Global Accelerator uses the AWS global network to direct internet traffic from your users to your applications running in AWS Regions. Global Accelerator creates a fixed entry point for your applications through static anycast IP addresses, and routes user traffic to the optimal endpoint based on performance, application health and routing policies that you can configure. Global Accelerator supports the following features at launch: static anycast IP addresses, support for TCP and UDP, support for Network Load Balancers, Application Load Balancers and Elastic-IP address endpoints,  continuous health checking, instant regional failover, fault isolating Network Zones, granular traffic controls, and client affinity.\n* [AWSSDK.Glue](https://www.nuget.org/packages/AWSSDK.Glue/)\n\t* AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. You can create and run an ETL job with a few clicks in the AWS Management Console. You simply point AWS Glue to your data stored on AWS, and AWS Glue discovers your data and stores the associated metadata (e.g. table definition and schema) in the AWS Glue Data Catalog. Once cataloged, your data is immediately searchable, queryable, and available for ETL. AWS Glue generates the code to execute your data transformations and data loading processes. AWS Glue generates Python code that is entirely customizable, reusable, and portable. Once your ETL job is ready, you can schedule it to run on AWS Glue's fully managed, scale-out Spark environment. AWS Glue provides a flexible scheduler with dependency resolution, job monitoring, and alerting. AWS Glue is serverless, so there is no infrastructure to buy, set up, or manage. It automatically provisions the environment needed to complete the job, and customers pay only for the compute resources consumed while running ETL jobs. With AWS Glue, data can be available for analytics in minutes.\n* [AWSSDK.GlueDataBrew](https://www.nuget.org/packages/AWSSDK.GlueDataBrew/)\n\t* This is the initial SDK release for AWS Glue DataBrew. DataBrew is a visual data preparation tool that enables users to clean and normalize data without writing any code.\n* [AWSSDK.Greengrass](https://www.nuget.org/packages/AWSSDK.Greengrass/)\n\t* AWS Greengrass is software that lets you run local compute, messaging, and device state synchronization for connected devices in a secure way. With AWS Greengrass, connected devices can run AWS Lambda functions, keep device data in sync, and communicate with other devices securely even when not connected to the Internet. Using AWS Lambda, Greengrass ensures your IoT devices can respond quickly to local events, operate with intermittent connections, and minimize the cost of transmitting IoT data to the cloud.\n* [AWSSDK.GreengrassV2](https://www.nuget.org/packages/AWSSDK.GreengrassV2/)\n\t* AWS IoT Greengrass V2 is a new major version of AWS IoT Greengrass. This release adds several updates such as modular components, continuous deployments, and improved ease of use.\n* [AWSSDK.GroundStation](https://www.nuget.org/packages/AWSSDK.GroundStation/)\n\t* AWS Ground Station is a fully managed service that enables you to control satellite communications, downlink and process satellite data, and scale your satellite operations efficiently and cost-effectively without having to build or manage your own ground station infrastructure.\n* [AWSSDK.GuardDuty](https://www.nuget.org/packages/AWSSDK.GuardDuty/)\n\t* Enable Amazon GuardDuty to continuously monitor and process AWS data sources to identify threats to your AWS accounts and workloads. You can add customization by uploading additional threat intelligence lists and IP safe lists. You can list security findings, suspend, and disable the service.\n* [AWSSDK.HealthLake](https://www.nuget.org/packages/AWSSDK.HealthLake/)\n\t* This release introduces Amazon HealthLake (preview), a HIPAA-eligible service that enables healthcare and life sciences customers to store, transform, query, and analyze health data in the AWS Cloud.\n* [AWSSDK.Honeycode](https://www.nuget.org/packages/AWSSDK.Honeycode/)\n\t* Introducing Amazon Honeycode - a fully managed service that allows you to quickly build mobile and web apps for teams without programming.\n* [AWSSDK.IAMRolesAnywhere](https://www.nuget.org/packages/AWSSDK.IAMRolesAnywhere/)\n\t* IAM Roles Anywhere allows your workloads such as servers, containers, and applications to obtain temporary AWS credentials and use the same IAM roles and policies that you have configured for your AWS workloads to access AWS resources.\n* [AWSSDK.IdentityManagement](https://www.nuget.org/packages/AWSSDK.IdentityManagement/)\n\t* AWS Identity and Access Management (IAM) enables you to securely control access to AWS services and resources for your users. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources.\n* [AWSSDK.IdentityStore](https://www.nuget.org/packages/AWSSDK.IdentityStore/)\n\t* AWS Single Sign-On (SSO) Identity Store service provides an interface to retrieve all of your users and groups. It enables entitlement management per user or group for AWS SSO and other IDPs.\n* [AWSSDK.Imagebuilder](https://www.nuget.org/packages/AWSSDK.Imagebuilder/)\n\t* This is the first release of EC2 Image Builder, a service that provides a managed experience for automating the creation of EC2 AMIs.\n* [AWSSDK.ImportExport](https://www.nuget.org/packages/AWSSDK.ImportExport/)\n\t* AWS Import/Export accelerates moving large amounts of data into and out of the AWS cloud using portable storage devices for transport.\n* [AWSSDK.Inspector](https://www.nuget.org/packages/AWSSDK.Inspector/)\n\t* Amazon Inspector identifies security issues in your application deployments.\n* [AWSSDK.Inspector2](https://www.nuget.org/packages/AWSSDK.Inspector2/)\n\t* This release adds support for the new Amazon Inspector API. The new Amazon Inspector can automatically discover and scan Amazon EC2 instances and Amazon ECR container images for software vulnerabilities and unintended network exposure, and report centralized findings across multiple AWS accounts.\n* [AWSSDK.InspectorScan](https://www.nuget.org/packages/AWSSDK.InspectorScan/)\n\t* This release adds support for the new Amazon Inspector Scan API. The new Inspector Scan API can synchronously scan SBOMs adhering to the CycloneDX v1.5 format.\n* [AWSSDK.InternetMonitor](https://www.nuget.org/packages/AWSSDK.InternetMonitor/)\n\t* CloudWatch Internet Monitor is a a new service within CloudWatch that will help application developers and network engineers continuously monitor internet performance metrics such as availability and performance between their AWS-hosted applications and end-users of these applications\n* [AWSSDK.IoT](https://www.nuget.org/packages/AWSSDK.IoT/)\n\t* AWS IoT allows you to leverage AWS to build your Internet of Things.\n* [AWSSDK.IoT1ClickDevicesService](https://www.nuget.org/packages/AWSSDK.IoT1ClickDevicesService/)\n\t* AWS IoT 1-Click makes it easy for customers to incorporate simple ready-to-use IoT devices into their workflows. These devices can trigger AWS Lambda functions that implement business logic. In order to build applications using AWS IoT 1-Click devices, programmers can use the AWS IoT 1-Click Devices API and the AWS IoT 1-Click Projects API. Learn more at https://aws.amazon.com/documentation/iot-1-click/\n* [AWSSDK.IoT1ClickProjects](https://www.nuget.org/packages/AWSSDK.IoT1ClickProjects/)\n\t* AWS IoT 1-Click makes it easy for customers to incorporate simple ready-to-use IoT devices into their workflows. These devices can trigger AWS Lambda functions that implement business logic. In order to build applications using AWS IoT 1-Click devices, programmers can use the AWS IoT 1-Click Devices API and the AWS IoT 1-Click Projects API. Learn more at https://aws.amazon.com/documentation/iot-1-click/\n* [AWSSDK.IoTAnalytics](https://www.nuget.org/packages/AWSSDK.IoTAnalytics/)\n\t* AWS IoT Analytics is a fully-managed service that makes it easy to run and operationalize sophisticated analytics on massive volumes of IoT data without having to worry about the cost and complexity typically required to build an IoT analytics platform.\n* [AWSSDK.IotData](https://www.nuget.org/packages/AWSSDK.IotData/)\n\t* AWS IoT-Data enables secure, bi-directional communication between Internet-connected things (such as sensors, actuators, embedded devices, or smart appliances) and the AWS cloud. It implements a broker for applications and things to publish messages over HTTP (Publish) and retrieve, update, and delete thing shadows. A thing shadow is a persistent representation of your things and their state in the AWS cloud.\n* [AWSSDK.IoTDeviceAdvisor](https://www.nuget.org/packages/AWSSDK.IoTDeviceAdvisor/)\n\t* AWS IoT Core Device Advisor is fully managed test capability for IoT devices. Device manufacturers can use Device Advisor to test their IoT devices for reliable and secure connectivity with AWS IoT.\n* [AWSSDK.IoTEvents](https://www.nuget.org/packages/AWSSDK.IoTEvents/)\n\t* The AWS IoT Events service allows customers to monitor their IoT devices and sensors to detect failures or changes in operation and to trigger actions when these events occur\n* [AWSSDK.IoTEventsData](https://www.nuget.org/packages/AWSSDK.IoTEventsData/)\n\t* The AWS IoT Events service allows customers to monitor their IoT devices and sensors to detect failures or changes in operation and to trigger actions when these events occur\n* [AWSSDK.IoTFleetHub](https://www.nuget.org/packages/AWSSDK.IoTFleetHub/)\n\t* AWS IoT Fleet Hub, a new feature of AWS IoT Device Management that provides a web application for monitoring and managing device fleets connected to AWS IoT at scale.\n* [AWSSDK.IoTFleetWise](https://www.nuget.org/packages/AWSSDK.IoTFleetWise/)\n\t* General availability (GA) for AWS IoT Fleetwise. It adds AWS IoT Fleetwise to AWS SDK. For more information, see https://docs.aws.amazon.com/iot-fleetwise/latest/APIReference/Welcome.html.\n* [AWSSDK.IoTJobsDataPlane](https://www.nuget.org/packages/AWSSDK.IoTJobsDataPlane/)\n\t* This release adds support for new the service called Iot Jobs. This client is built for the device SDK to use Iot Jobs Device specific APIs.\n* [AWSSDK.IoTRoboRunner](https://www.nuget.org/packages/AWSSDK.IoTRoboRunner/)\n\t* AWS IoT RoboRunner is a new service that makes it easy to build applications that help multi-vendor robots work together seamlessly. See the IoT RoboRunner developer guide for more details on getting started. https://docs.aws.amazon.com/iotroborunner/latest/dev/iotroborunner-welcome.html\n* [AWSSDK.IoTSecureTunneling](https://www.nuget.org/packages/AWSSDK.IoTSecureTunneling/)\n\t* This release adds support for IoT Secure Tunneling to remote access devices behind restricted firewalls.\n* [AWSSDK.IoTSiteWise](https://www.nuget.org/packages/AWSSDK.IoTSiteWise/)\n\t* AWS IoT SiteWise is a managed service that makes it easy to collect, store, organize and monitor data from industrial equipment at scale. You can use AWS IoT SiteWise to model your physical assets, processes and facilities, quickly compute common industrial performance metrics, and create fully managed web applications to help analyze industrial equipment data, prevent costly equipment issues, and reduce production inefficiencies.\n* [AWSSDK.IoTThingsGraph](https://www.nuget.org/packages/AWSSDK.IoTThingsGraph/)\n\t* Initial Release.\n* [AWSSDK.IoTTwinMaker](https://www.nuget.org/packages/AWSSDK.IoTTwinMaker/)\n\t* AWS IoT TwinMaker makes it faster and easier to create, visualize and monitor digital twins of real-world systems like buildings, factories and industrial equipment to optimize operations. Learn more: https://docs.aws.amazon.com/iot-twinmaker/latest/apireference/Welcome.html (New Service) (Preview)\n* [AWSSDK.IoTWireless](https://www.nuget.org/packages/AWSSDK.IoTWireless/)\n\t* AWS IoT for LoRaWAN enables customers to setup a private LoRaWAN network by connecting their LoRaWAN devices and gateways to the AWS cloud without managing a LoRaWAN Network Server.\n* [AWSSDK.IVS](https://www.nuget.org/packages/AWSSDK.IVS/)\n\t* Introducing Amazon Interactive Video Service - a managed live streaming solution that is quick and easy to set up, and ideal for creating interactive video experiences.\n* [AWSSDK.Ivschat](https://www.nuget.org/packages/AWSSDK.Ivschat/)\n\t* Adds new APIs for IVS Chat, a feature for building interactive chat experiences alongside an IVS broadcast.\n* [AWSSDK.IVSRealTime](https://www.nuget.org/packages/AWSSDK.IVSRealTime/)\n\t* Initial release of the Amazon Interactive Video Service RealTime API.\n* [AWSSDK.Kafka](https://www.nuget.org/packages/AWSSDK.Kafka/)\n\t* Amazon Managed Streaming for Kafka (Amazon MSK). Amazon MSK is a service that you can use to easily build, monitor, and manage Apache Kafka clusters in the cloud.\n* [AWSSDK.KafkaConnect](https://www.nuget.org/packages/AWSSDK.KafkaConnect/)\n\t* This is the initial SDK release for Amazon Managed Streaming for Apache Kafka Connect (MSK Connect).\n* [AWSSDK.Kendra](https://www.nuget.org/packages/AWSSDK.Kendra/)\n\t* It is a preview launch of Amazon Kendra. Amazon Kendra is a managed, highly accurate and easy to use enterprise search service that is powered by machine learning.\n* [AWSSDK.KendraRanking](https://www.nuget.org/packages/AWSSDK.KendraRanking/)\n\t* Introducing Amazon Kendra Intelligent Ranking, a new set of Kendra APIs that leverages Kendra semantic ranking capabilities to improve the quality of search results from other search services (i.e. OpenSearch, ElasticSearch, Solr).\n* [AWSSDK.KeyManagementService](https://www.nuget.org/packages/AWSSDK.KeyManagementService/)\n\t* AWS Key Management Service (KMS) is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data, and uses Hardware Security Modules (HSMs) to protect the security of your keys.\n* [AWSSDK.Keyspaces](https://www.nuget.org/packages/AWSSDK.Keyspaces/)\n\t* This release adds support for data definition language (DDL) operations\n* [AWSSDK.Kinesis](https://www.nuget.org/packages/AWSSDK.Kinesis/)\n\t* Amazon Kinesis is a fully managed, cloud-based service for real-time processing of large, distributed data streams.\n* [AWSSDK.KinesisAnalytics](https://www.nuget.org/packages/AWSSDK.KinesisAnalytics/)\n\t* Amazon Kinesis Analytics is a fully managed service for continuously querying streaming data using standard SQL.\n* [AWSSDK.KinesisAnalyticsV2](https://www.nuget.org/packages/AWSSDK.KinesisAnalyticsV2/)\n\t* Amazon Kinesis Data Analytics now supports Java-based stream processing applications, in addition to the previously supported SQL. Now, you can use your own Java code in Amazon Kinesis Data Analytics to build and run stream processing applications. This new capability also comes with an update to the previous Amazon Kinesis Data Analytics APIs to enable support for different runtime environments and more.\n* [AWSSDK.KinesisFirehose](https://www.nuget.org/packages/AWSSDK.KinesisFirehose/)\n\t* Amazon Kinesis Firehose is a fully managed service for ingesting data streams directly into AWS data services such as Amazon S3 and Amazon Redshift.\n* [AWSSDK.KinesisVideo](https://www.nuget.org/packages/AWSSDK.KinesisVideo/)\n\t* Announcing Amazon Kinesis Video Streams, a fully managed video ingestion and storage service. Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS for machine learning, analytics, and processing. You can also stream other time-encoded data like RADAR and LIDAR signals using Kinesis Video Streams.\n* [AWSSDK.KinesisVideoArchivedMedia](https://www.nuget.org/packages/AWSSDK.KinesisVideoArchivedMedia/)\n\t* Announcing Amazon Kinesis Video Streams, a fully managed video ingestion and storage service. Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS for machine learning, analytics, and processing. You can also stream other time-encoded data like RADAR and LIDAR signals using Kinesis Video Streams.\n* [AWSSDK.KinesisVideoMedia](https://www.nuget.org/packages/AWSSDK.KinesisVideoMedia/)\n\t* Announcing Amazon Kinesis Video Streams, a fully managed video ingestion and storage service. Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS for machine learning, analytics, and processing. You can also stream other time-encoded data like RADAR and LIDAR signals using Kinesis Video Streams.\n* [AWSSDK.KinesisVideoSignalingChannels](https://www.nuget.org/packages/AWSSDK.KinesisVideoSignalingChannels/)\n\t* Announcing support for WebRTC in Kinesis Video Streams, as fully managed capability. You can now use simple APIs to enable your connected devices, web, and mobile apps with real-time two-way media streaming capabilities.\n* [AWSSDK.KinesisVideoWebRTCStorage](https://www.nuget.org/packages/AWSSDK.KinesisVideoWebRTCStorage/)\n\t* Amazon Kinesis Video Streams offers capabilities to stream video and audio in real-time via WebRTC to the cloud for storage, playback, and analytical processing. Customers can use our enhanced WebRTC SDK and cloud APIs to enable real-time streaming, as well as media ingestion to the cloud.\n* [AWSSDK.LakeFormation](https://www.nuget.org/packages/AWSSDK.LakeFormation/)\n\t* Lake Formation: (New Service) AWS Lake Formation is a fully managed service that makes it easier for customers to build, secure and manage data lakes.  AWS Lake Formation simplifies and automates many of the complex manual steps usually required to create data lakes including collecting, cleaning and cataloging data and securely making that data available for analytics and machine learning.\n* [AWSSDK.Lambda](https://www.nuget.org/packages/AWSSDK.Lambda/)\n\t* AWS Lambda is a compute service that runs your code in response to events and automatically manages the compute resources for you, making it easy to build applications that respond quickly to new information.\n* [AWSSDK.LaunchWizard](https://www.nuget.org/packages/AWSSDK.LaunchWizard/)\n\t* AWS Launch Wizard is a service that helps reduce the time it takes to deploy applications to the cloud while providing a guided deployment experience.\n* [AWSSDK.Lex](https://www.nuget.org/packages/AWSSDK.Lex/)\n\t* Amazon Lex is a service for building conversational interactions into any application using voice or text.\n* [AWSSDK.LexModelBuildingService](https://www.nuget.org/packages/AWSSDK.LexModelBuildingService/)\n\t* Amazon Lex is a service for building conversational interfaces into any application using voice and text.\n* [AWSSDK.LexModelsV2](https://www.nuget.org/packages/AWSSDK.LexModelsV2/)\n\t* This release adds support for Amazon Lex V2 APIs for model building.\n* [AWSSDK.LexRuntimeV2](https://www.nuget.org/packages/AWSSDK.LexRuntimeV2/)\n\t* This release adds support for Amazon Lex V2 APIs for runtime, including Streaming APIs for conversation management.\n* [AWSSDK.LicenseManager](https://www.nuget.org/packages/AWSSDK.LicenseManager/)\n\t* AWS License Manager makes it easier to manage licenses in AWS and on premises when customers run applications using existing licenses from a variety of software vendors including Microsoft, SAP, Oracle, and IBM. AWS License Manager automatically tracks and controls license usage once administrators have created and enforced rules that emulate the terms of their licensing agreements. The capabilities of AWS License Manager are available through SDK and Tools, besides the management console and CLI.\n* [AWSSDK.LicenseManagerLinuxSubscriptions](https://www.nuget.org/packages/AWSSDK.LicenseManagerLinuxSubscriptions/)\n\t* AWS License Manager now offers cross-region, cross-account tracking of commercial Linux subscriptions on AWS. This includes subscriptions purchased as part of EC2 subscription-included AMIs, on the AWS Marketplace, or brought to AWS via Red Hat Cloud Access Program.\n* [AWSSDK.LicenseManagerUserSubscriptions](https://www.nuget.org/packages/AWSSDK.LicenseManagerUserSubscriptions/)\n\t* This release supports user based subscription for Microsoft Visual Studio Professional and Enterprise on EC2.\n* [AWSSDK.Lightsail](https://www.nuget.org/packages/AWSSDK.Lightsail/)\n\t* An extremely simplified VM creation and management service.\n* [AWSSDK.LocationService](https://www.nuget.org/packages/AWSSDK.LocationService/)\n\t* Initial release of Amazon Location Service. A new geospatial service providing capabilities to render maps, geocode/reverse geocode, track device locations, and detect geofence entry/exit events.\n* [AWSSDK.LookoutEquipment](https://www.nuget.org/packages/AWSSDK.LookoutEquipment/)\n\t* This release introduces support for Amazon Lookout for Equipment.\n* [AWSSDK.LookoutforVision](https://www.nuget.org/packages/AWSSDK.LookoutforVision/)\n\t* This release introduces support for Amazon Lookout for Vision.\n* [AWSSDK.LookoutMetrics](https://www.nuget.org/packages/AWSSDK.LookoutMetrics/)\n\t* Amazon Lookout for Metrics is now generally available. You can use Lookout for Metrics to monitor your data for anomalies. For more information, see the Amazon Lookout for Metrics Developer Guide.\n* [AWSSDK.MachineLearning](https://www.nuget.org/packages/AWSSDK.MachineLearning/)\n\t* Amazon Machine Learning is a service that makes it easy for developers of all skill levels to use machine learning technology.\n* [AWSSDK.Macie2](https://www.nuget.org/packages/AWSSDK.Macie2/)\n\t* This release introduces a new major version of the Amazon Macie API. You can use this version of the API to develop tools and applications that interact with the new Amazon Macie.\n* [AWSSDK.MainframeModernization](https://www.nuget.org/packages/AWSSDK.MainframeModernization/)\n\t* AWS Mainframe Modernization service is a managed mainframe service and set of tools for planning, migrating, modernizing, and running mainframe workloads on AWS\n* [AWSSDK.ManagedBlockchain](https://www.nuget.org/packages/AWSSDK.ManagedBlockchain/)\n\t* (New Service) Amazon Managed Blockchain is a fully managed service that makes it easy to create and manage scalable blockchain networks using popular open source frameworks.\n* [AWSSDK.ManagedBlockchainQuery](https://www.nuget.org/packages/AWSSDK.ManagedBlockchainQuery/)\n\t* Amazon Managed Blockchain (AMB) Query provides serverless access to standardized, multi-blockchain datasets with developer-friendly APIs.\n* [AWSSDK.ManagedGrafana](https://www.nuget.org/packages/AWSSDK.ManagedGrafana/)\n\t* Initial release of the SDK for Amazon Managed Grafana API.\n* [AWSSDK.MarketplaceAgreement](https://www.nuget.org/packages/AWSSDK.MarketplaceAgreement/)\n\t* The AWS Marketplace Agreement Service provides an API interface that helps AWS Marketplace sellers manage their agreements, including listing, filtering, and viewing details about their agreements.\n* [AWSSDK.MarketplaceCatalog](https://www.nuget.org/packages/AWSSDK.MarketplaceCatalog/)\n\t* This is the first release for the AWS Marketplace Catalog service which allows you to list, describe and manage change requests on your published entities on AWS Marketplace. \n* [AWSSDK.MarketplaceDeployment](https://www.nuget.org/packages/AWSSDK.MarketplaceDeployment/)\n\t* AWS Marketplace Deployment is a new service that provides essential features that facilitate the deployment of software, data, and services procured through AWS Marketplace.\n* [AWSSDK.MarketplaceEntitlementService](https://www.nuget.org/packages/AWSSDK.MarketplaceEntitlementService/)\n\t* AWS Marketplace Entitlement Service enables AWS Marketplace sellers to determine the capacity purchased by their customers.\n* [AWSSDK.MediaConnect](https://www.nuget.org/packages/AWSSDK.MediaConnect/)\n\t* This is the initial release for AWS Elemental MediaConnect, an ingest and transport service for live video. This new AWS service allows broadcasters and content owners to send high-value live content into the cloud, securely transmit it to partners for distribution, and replicate it to multiple destinations around the globe.\n* [AWSSDK.MediaConvert](https://www.nuget.org/packages/AWSSDK.MediaConvert/)\n\t* AWS Elemental MediaConvert is a file-based video conversion service that transforms media into formats required for traditional broadcast and for internet streaming to multi-screen devices.\n* [AWSSDK.MediaLive](https://www.nuget.org/packages/AWSSDK.MediaLive/)\n\t* AWS Elemental MediaLive is a video service that lets you easily create live outputs for broadcast and streaming delivery.\n* [AWSSDK.MediaPackage](https://www.nuget.org/packages/AWSSDK.MediaPackage/)\n\t* AWS Elemental MediaPackage is a just-in-time video packaging and origination service that lets you format highly secure and reliable live outputs for a variety of devices.\n* [AWSSDK.MediaPackageV2](https://www.nuget.org/packages/AWSSDK.MediaPackageV2/)\n\t* Adds support for the MediaPackage Live v2 API\n* [AWSSDK.MediaPackageVod](https://www.nuget.org/packages/AWSSDK.MediaPackageVod/)\n\t* AWS Elemental MediaPackage now supports Video-on-Demand (VOD) workflows. These new features allow you to easily deliver a vast library of source video Assets stored in your own S3 buckets using a small set of simple to set up Packaging Configurations and Packaging Groups.\n* [AWSSDK.MediaStore](https://www.nuget.org/packages/AWSSDK.MediaStore/)\n\t* AWS Elemental MediaStore is an AWS storage service optimized for media. It gives you the performance, consistency, and low latency required to deliver live and on-demand video content. AWS Elemental MediaStore acts as the origin store in your video workflow.\n* [AWSSDK.MediaStoreData](https://www.nuget.org/packages/AWSSDK.MediaStoreData/)\n\t* AWS Elemental MediaStore Data is an AWS storage service optimized for media. It gives you the performance, consistency, and low latency required to deliver live and on-demand video content. AWS Elemental MediaStore Data acts as the origin store in your video workflow.\n* [AWSSDK.MediaTailor](https://www.nuget.org/packages/AWSSDK.MediaTailor/)\n\t* AWS Elemental MediaTailor is a personalization and monetization service that allows scalable server-side ad insertion. The service enables you to serve targeted ads to viewers while maintaining broadcast quality in over-the-top (OTT) video applications. This SDK allows user access to the AWS Elemental MediaTailor configuration interface.\n* [AWSSDK.MedicalImaging](https://www.nuget.org/packages/AWSSDK.MedicalImaging/)\n\t* General Availability (GA) release of AWS Health Imaging, enabling customers to store, transform, and analyze medical imaging data at petabyte-scale.\n* [AWSSDK.MemoryDB](https://www.nuget.org/packages/AWSSDK.MemoryDB/)\n\t* AWS MemoryDB  SDK now supports all APIs for newly launched MemoryDB service.\n* [AWSSDK.Mgn](https://www.nuget.org/packages/AWSSDK.Mgn/)\n\t* Add new service - Application Migration Service.\n* [AWSSDK.MigrationHub](https://www.nuget.org/packages/AWSSDK.MigrationHub/)\n\t* AWS Migration Hub provides a single location to track migrations across multiple AWS and partner solutions. Using Migration Hub allows you to choose the AWS and partner migration tools that best fit your needs, while providing visibility into the status of your entire migration portfolio. Migration Hub also provides key metrics and progress for individual applications, regardless of which tools are being used to migrate them. For example, you might use AWS Database Migration Service, AWS Server Migration Service, and partner migration tools to migrate an application comprised of a database, virtualized web servers, and a bare metal server. Using Migration Hub will provide you with a single screen that shows the migration progress of all the resources in the application. This allows you to quickly get progress updates across all of your migrations, easily identify and troubleshoot any issues, and reduce the overall time and effort spent on your migration projects. Migration Hub is available to all AWS customers at no additional charge. You only pay for the cost of the migration tools you use, and any resources being consumed on AWS. \n* [AWSSDK.MigrationHubConfig](https://www.nuget.org/packages/AWSSDK.MigrationHubConfig/)\n\t* AWS Migration Hub Config Service allows you to get and set the Migration Hub home region for use with AWS Migration Hub and Application Discovery Service\n* [AWSSDK.MigrationHubOrchestrator](https://www.nuget.org/packages/AWSSDK.MigrationHubOrchestrator/)\n\t* Introducing AWS MigrationHubOrchestrator. This is the first public release of AWS MigrationHubOrchestrator.\n* [AWSSDK.MigrationHubRefactorSpaces](https://www.nuget.org/packages/AWSSDK.MigrationHubRefactorSpaces/)\n\t* This is the initial SDK release for AWS Migration Hub Refactor Spaces\n* [AWSSDK.MigrationHubStrategyRecommendations](https://www.nuget.org/packages/AWSSDK.MigrationHubStrategyRecommendations/)\n\t* AWS SDK for Migration Hub Strategy Recommendations. It includes APIs to start the portfolio assessment, import portfolio data for assessment, and to retrieve recommendations. For more information, see the AWS Migration Hub documentation at https://docs.aws.amazon.com/migrationhub/index.html\n* [AWSSDK.Mobile](https://www.nuget.org/packages/AWSSDK.Mobile/)\n\t* AWS Mobile Hub is an integrated experience designed to help developers build, test, configure and release cloud-based applications for mobile devices using Amazon Web Services. AWS Mobile Hub provides a console and API for developers, allowing them to quickly select desired features and integrate them into mobile applications. Features include NoSQL Database, Cloud Logic, Messaging and Analytics. With AWS Mobile Hub, you pay only for the underlying services that Mobile Hub provisions based on the features you choose in the Mobile Hub console.\n* [AWSSDK.MobileAnalytics](https://www.nuget.org/packages/AWSSDK.MobileAnalytics/)\n\t* Amazon Mobile Analytics is a service that lets you simply and cost effectively collect and analyze your application usage data. In addition to providing usage summary charts that are available for quick reference, Amazon Mobile Analytics enables you to set up automatic export of your data to Amazon S3 for use with other data analytics tools such as Amazon Redshift, Amazon Elastic MapReduce (EMR), Extract, Transform and Load (ETL) software, or your own data warehouse.\n* [AWSSDK.MQ](https://www.nuget.org/packages/AWSSDK.MQ/)\n\t* This is the initial SDK release for Amazon MQ. Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud.\n* [AWSSDK.MTurk](https://www.nuget.org/packages/AWSSDK.MTurk/)\n\t* Amazon MTurk is a web service that provides an on-demand, scalable, human workforce to complete jobs that humans can do better than computers, for example, recognizing objects in photos.\n* [AWSSDK.MWAA](https://www.nuget.org/packages/AWSSDK.MWAA/)\n\t* (New Service) Amazon MWAA is a managed service for Apache Airflow that makes it easy for data engineers and data scientists to execute data processing workflows in the cloud.\n* [AWSSDK.Neptune](https://www.nuget.org/packages/AWSSDK.Neptune/)\n\t* Amazon Neptune is a fast, reliable graph database service that makes it easy to build and run applications that work with highly connected datasets. Neptune supports popular graph models Property Graph and W3C's Resource Description Frame (RDF), and their respective query languages Apache TinkerPop Gremlin 3.3.2 and SPARQL 1.1. \n* [AWSSDK.Neptunedata](https://www.nuget.org/packages/AWSSDK.Neptunedata/)\n\t* Allows customers to execute data plane actions like bulk loading graphs, issuing graph queries using Gremlin and openCypher directly from the SDK.\n* [AWSSDK.NeptuneGraph](https://www.nuget.org/packages/AWSSDK.NeptuneGraph/)\n\t* This is the initial SDK release for Amazon Neptune Analytics\n* [AWSSDK.NetworkFirewall](https://www.nuget.org/packages/AWSSDK.NetworkFirewall/)\n\t* (New Service) AWS Network Firewall is a managed network layer firewall service that makes it easy to secure your virtual private cloud (VPC) networks and block malicious traffic.\n* [AWSSDK.NetworkManager](https://www.nuget.org/packages/AWSSDK.NetworkManager/)\n\t* This is the initial SDK release for AWS Network Manager.\n* [AWSSDK.NetworkMonitor](https://www.nuget.org/packages/AWSSDK.NetworkMonitor/)\n\t* CloudWatch Network Monitor is a new service within CloudWatch that will help network administrators and operators continuously monitor network performance metrics such as round-trip-time and packet loss between their AWS-hosted applications and their on-premises locations.\n* [AWSSDK.NimbleStudio](https://www.nuget.org/packages/AWSSDK.NimbleStudio/)\n\t* Amazon Nimble Studio is a virtual studio service that empowers visual effects, animation, and interactive content teams to create content securely within a scalable, private cloud service.\n* [AWSSDK.OAM](https://www.nuget.org/packages/AWSSDK.OAM/)\n\t* Amazon CloudWatch Observability Access Manager is a new service that allows configuration of the CloudWatch cross-account observability feature.\n* [AWSSDK.Omics](https://www.nuget.org/packages/AWSSDK.Omics/)\n\t* Amazon Omics is a new, purpose-built service that can be used by healthcare and life science organizations to store, query, and analyze omics data. The insights from that data can be used to accelerate scientific discoveries and improve healthcare.\n* [AWSSDK.OpenSearchServerless](https://www.nuget.org/packages/AWSSDK.OpenSearchServerless/)\n\t* Publish SDK for Amazon OpenSearch Serverless\n* [AWSSDK.OpenSearchService](https://www.nuget.org/packages/AWSSDK.OpenSearchService/)\n\t* Updated Configuration APIs for Amazon OpenSearch Service (successor to Amazon Elasticsearch Service)\n* [AWSSDK.OpsWorks](https://www.nuget.org/packages/AWSSDK.OpsWorks/)\n\t* AWS OpsWorks is an application management service that makes it easy to deploy and operate applications of all shapes and sizes. You can define the application's architecture and the specification of each component including package installation, software configuration and resources such as storage.\n* [AWSSDK.OpsWorksCM](https://www.nuget.org/packages/AWSSDK.OpsWorksCM/)\n\t* AWS OpsWorks for Chef Automate gives customers a single tenant Chef Automate server. The Chef Automate server is fully managed by AWS and supports automatic backup, restore and upgrade operations.\n* [AWSSDK.Organizations](https://www.nuget.org/packages/AWSSDK.Organizations/)\n\t* AWS Organizations is a web service that enables you to consolidate your multiple AWS accounts into an organization and centrally manage your accounts and their resources.\n* [AWSSDK.OSIS](https://www.nuget.org/packages/AWSSDK.OSIS/)\n\t* Initial release for OpenSearch Ingestion\n* [AWSSDK.Outposts](https://www.nuget.org/packages/AWSSDK.Outposts/)\n\t* This is the initial release for AWS Outposts, a fully managed service that extends AWS infrastructure, services, APIs, and tools to customer sites. AWS Outposts enables you to launch and run EC2 instances and EBS volumes locally at your on-premises location. This release introduces new APIs for creating and viewing Outposts. \n* [AWSSDK.Panorama](https://www.nuget.org/packages/AWSSDK.Panorama/)\n\t* General availability for AWS Panorama. AWS SDK for Panorama includes APIs to manage your devices and nodes, and deploy computer vision applications to the edge. For more information, see the AWS Panorama documentation at http://docs.aws.amazon.com/panorama\n* [AWSSDK.PaymentCryptography](https://www.nuget.org/packages/AWSSDK.PaymentCryptography/)\n\t* Initial release of AWS Payment Cryptography Control Plane service for creating and managing cryptographic keys used during card payment processing.\n* [AWSSDK.PaymentCryptographyData](https://www.nuget.org/packages/AWSSDK.PaymentCryptographyData/)\n\t* Initial release of AWS Payment Cryptography DataPlane Plane service for performing cryptographic operations typically used during card payment processing.\n* [AWSSDK.PcaConnectorAd](https://www.nuget.org/packages/AWSSDK.PcaConnectorAd/)\n\t* The Connector for AD allows you to use a fully-managed AWS Private CA as a drop-in replacement for your self-managed enterprise CAs without local agents or proxy servers. Enterprises that use AD to manage Windows environments can reduce their private certificate authority (CA) costs and complexity.\n* [AWSSDK.Personalize](https://www.nuget.org/packages/AWSSDK.Personalize/)\n\t* Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications.\n* [AWSSDK.PersonalizeEvents](https://www.nuget.org/packages/AWSSDK.PersonalizeEvents/)\n\t* Introducing Amazon Personalize  - a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications.\n* [AWSSDK.PersonalizeRuntime](https://www.nuget.org/packages/AWSSDK.PersonalizeRuntime/)\n\t* Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications.\n* [AWSSDK.PI](https://www.nuget.org/packages/AWSSDK.PI/)\n\t* Performance Insights is a feature of Amazon Relational Database Service (RDS) that helps you quickly assess the load on your database, and determine when and where to take action. You can use the SDK to retrieve Performance Insights data and integrate your monitoring solutions.\n* [AWSSDK.Pinpoint](https://www.nuget.org/packages/AWSSDK.Pinpoint/)\n\t* Amazon Pinpoint makes it easy to run targeted campaigns to improve user engagement. Pinpoint helps you understand your users behavior, define who to target, what messages to send, when to deliver them, and tracks the results of the campaign.\n* [AWSSDK.PinpointEmail](https://www.nuget.org/packages/AWSSDK.PinpointEmail/)\n\t* This is the first release of the Amazon Pinpoint Email API. You can use this API to configure and send transactional email from your Amazon Pinpoint account to specific email addresses. Unlike campaign-based email that you send from Amazon Pinpoint, you don't have to create segments and campaigns in order to send transactional email. \n* [AWSSDK.PinpointSMSVoice](https://www.nuget.org/packages/AWSSDK.PinpointSMSVoice/)\n\t* With Amazon Pinpoint Voice, you can use text-to-speech technology to deliver personalized voice messages to your customers. Amazon Pinpoint Voice is a way to deliver transactional messages -- such as one-time passwords and appointment confirmations to customers.\n* [AWSSDK.PinpointSMSVoiceV2](https://www.nuget.org/packages/AWSSDK.PinpointSMSVoiceV2/)\n\t* Amazon Pinpoint now offers a version 2.0 suite of SMS and voice APIs, providing increased control over sending and configuration. This release is a new SDK for sending SMS and voice messages called PinpointSMSVoiceV2.\n* [AWSSDK.Pipes](https://www.nuget.org/packages/AWSSDK.Pipes/)\n\t* AWS introduces new Amazon EventBridge Pipes which allow you to connect sources (SQS, Kinesis, DDB, Kafka, MQ) to Targets (14+ EventBridge Targets) without any code, with filtering, batching, input transformation, and an optional Enrichment stage (Lambda, StepFunctions, ApiGateway, ApiDestinations)\n* [AWSSDK.Polly](https://www.nuget.org/packages/AWSSDK.Polly/)\n\t* Amazon Polly is a service that turns text into lifelike speech, making it easy to develop applications that use high-quality speech to increase engagement and accessibility.\n* [AWSSDK.Pricing](https://www.nuget.org/packages/AWSSDK.Pricing/)\n\t* We launched new service, Price List Service.\n* [AWSSDK.Private5G](https://www.nuget.org/packages/AWSSDK.Private5G/)\n\t* This is the initial SDK release for AWS Private 5G. AWS Private 5G is a managed service that makes it easy to deploy, operate, and scale your own private mobile network at your on-premises location.\n* [AWSSDK.PrometheusService](https://www.nuget.org/packages/AWSSDK.PrometheusService/)\n\t* (New Service) Amazon Managed Service for Prometheus is a fully managed Prometheus-compatible monitoring service that makes it easy to monitor containerized applications securely and at scale.\n* [AWSSDK.Proton](https://www.nuget.org/packages/AWSSDK.Proton/)\n\t* This is the initial SDK release for AWS Proton\n* [AWSSDK.QBusiness](https://www.nuget.org/packages/AWSSDK.QBusiness/)\n\t* Amazon Q - a generative AI powered application that your employees can use to ask questions and get answers from knowledge spread across disparate content repositories, summarize reports, write articles, take actions, and much more - all within their company's connected content repositories.\n* [AWSSDK.QConnect](https://www.nuget.org/packages/AWSSDK.QConnect/)\n\t* Amazon Q in Connect, an LLM-enhanced evolution of Amazon Connect Wisdom. This release adds generative AI support to Amazon Q Connect QueryAssistant and GetRecommendations APIs.\n* [AWSSDK.QLDB](https://www.nuget.org/packages/AWSSDK.QLDB/)\n\t* Introduces operations needed for managing Amazon QLDB ledgers. This includes the ability to create, delete, modify, and describe Amazon QLDB ledgers. This also includes the ability to cryptographically verify documents and export the journal in a ledger.\n* [AWSSDK.QLDBSession](https://www.nuget.org/packages/AWSSDK.QLDBSession/)\n\t* Amazon QLDB introduces the SendCommand API to interact with data in Amazon QLDB ledgers.\n* [AWSSDK.QuickSight](https://www.nuget.org/packages/AWSSDK.QuickSight/)\n\t* Amazon QuickSight is a fully managed, serverless, cloud business intelligence system that allows you to extend data and insights to every user in your organization. The first release of APIs for Amazon QuickSight introduces embedding and user/group management capabilities. The get-dashboard-embed-url API allows you to obtain an authenticated dashboard URL that can be embedded in application domains whitelisted for QuickSight dashboard embedding. User APIs allow you to programmatically expand and manage your QuickSight deployments while group APIs allow easier permissions management for resources within QuickSight.\n* [AWSSDK.RAM](https://www.nuget.org/packages/AWSSDK.RAM/)\n\t* AWS Resource Access Manager (AWS RAM) enables you to share your resources with any AWS account or through AWS Organizations.\n* [AWSSDK.RDS](https://www.nuget.org/packages/AWSSDK.RDS/)\n\t* Amazon Relational Database Service (Amazon RDS) is a web service that makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while managing time-consuming database management tasks, freeing you up to focus on your applications and business.\n* [AWSSDK.RDSDataService](https://www.nuget.org/packages/AWSSDK.RDSDataService/)\n\t* Amazon RDS Data Service API is an HTTP endpoint to run SQL statements on an Amazon Aurora Serverless DB cluster.\n* [AWSSDK.RecycleBin](https://www.nuget.org/packages/AWSSDK.RecycleBin/)\n\t* This release adds support for Recycle Bin.\n* [AWSSDK.Redshift](https://www.nuget.org/packages/AWSSDK.Redshift/)\n\t* Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse solution that makes it simple and cost-effective to efficiently analyze all your data using your existing business intelligence tools.\n* [AWSSDK.RedshiftDataAPIService](https://www.nuget.org/packages/AWSSDK.RedshiftDataAPIService/)\n\t* The Amazon Redshift Data API is generally available. This release enables querying Amazon Redshift data and listing various database objects.\n* [AWSSDK.RedshiftServerless](https://www.nuget.org/packages/AWSSDK.RedshiftServerless/)\n\t* Add new API operations for Amazon Redshift Serverless, a new way of using Amazon Redshift without needing to manually manage provisioned clusters. The new operations let you interact with Redshift Serverless resources, such as create snapshots, list VPC endpoints, delete resource policies, and more.\n* [AWSSDK.Rekognition](https://www.nuget.org/packages/AWSSDK.Rekognition/)\n\t* AWS Rekognition service does image processing and concept recognition, face detection and identification, face verification, similar face search and face clustering.\n* [AWSSDK.Repostspace](https://www.nuget.org/packages/AWSSDK.Repostspace/)\n\t* Initial release of AWS re:Post Private\n* [AWSSDK.ResilienceHub](https://www.nuget.org/packages/AWSSDK.ResilienceHub/)\n\t* Initial release of AWS Resilience Hub, a managed service that enables you to define, validate, and track the resilience of your applications on AWS\n* [AWSSDK.ResourceExplorer2](https://www.nuget.org/packages/AWSSDK.ResourceExplorer2/)\n\t* This is the initial SDK release for AWS Resource Explorer. AWS Resource Explorer lets your users search for and discover your AWS resources across the AWS Regions in your account.\n* [AWSSDK.ResourceGroups](https://www.nuget.org/packages/AWSSDK.ResourceGroups/)\n\t* AWS Resource Groups lets you search and group AWS resources from multiple services based on their tags.\n* [AWSSDK.ResourceGroupsTaggingAPI](https://www.nuget.org/packages/AWSSDK.ResourceGroupsTaggingAPI/)\n\t* Resource Groups Tagging APIs can help you organize your resources and enable you to simplify resource management, access management, and cost allocation.\n* [AWSSDK.RoboMaker](https://www.nuget.org/packages/AWSSDK.RoboMaker/)\n\t* (New Service) AWS RoboMaker is a service that makes it easy to develop, simulate, and deploy intelligent robotics applications at scale. \n* [AWSSDK.Route53](https://www.nuget.org/packages/AWSSDK.Route53/)\n\t* Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service.\n* [AWSSDK.Route53Domains](https://www.nuget.org/packages/AWSSDK.Route53Domains/)\n\t* Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service.\n* [AWSSDK.Route53RecoveryCluster](https://www.nuget.org/packages/AWSSDK.Route53RecoveryCluster/)\n\t* Amazon Route 53 Application Recovery Controller's routing control - Routing Control Data Plane APIs help you update the state (On/Off) of the routing controls to reroute traffic across application replicas in a 100% available manner.\n* [AWSSDK.Route53RecoveryControlConfig](https://www.nuget.org/packages/AWSSDK.Route53RecoveryControlConfig/)\n\t* Amazon Route 53 Application Recovery Controller's routing control - Routing Control Configuration APIs help you create and delete clusters, control panels, routing controls and safety rules. State changes (On/Off) of routing controls are not part of configuration APIs.\n* [AWSSDK.Route53RecoveryReadiness](https://www.nuget.org/packages/AWSSDK.Route53RecoveryReadiness/)\n\t* Amazon Route 53 Application Recovery Controller's readiness check capability continually monitors resource quotas, capacity, and network routing policies to ensure that the recovery environment is scaled and configured to take over when needed.\n* [AWSSDK.Route53Resolver](https://www.nuget.org/packages/AWSSDK.Route53Resolver/)\n\t* This is the first release of the Amazon Route 53 Resolver API. Customers now have the ability to create and manage Amazon Route 53 Resolver endpoints and Amazon Route 53 Resolver rules.\n* [AWSSDK.S3](https://www.nuget.org/packages/AWSSDK.S3/)\n\t* Amazon Simple Storage Service (Amazon S3), provides developers and IT teams with secure, durable, highly-scalable object storage.\n* [AWSSDK.S3Control](https://www.nuget.org/packages/AWSSDK.S3Control/)\n\t* Add support for new S3 Block Public Access account-level APIs. The Block Public Access settings allow account owners to prevent public access to S3 data via bucket/object ACLs or bucket policies.\n* [AWSSDK.S3Outposts](https://www.nuget.org/packages/AWSSDK.S3Outposts/)\n\t* Amazon S3 on Outposts expands object storage to on-premises AWS Outposts environments, enabling you to store and retrieve objects using S3 APIs and features.\n* [AWSSDK.SageMaker](https://www.nuget.org/packages/AWSSDK.SageMaker/)\n\t* Amazon SageMaker is a fully-managed service that enables data scientists and developers to quickly and easily build, train, and deploy machine learning models, at scale.\n* [AWSSDK.SagemakerEdgeManager](https://www.nuget.org/packages/AWSSDK.SagemakerEdgeManager/)\n\t* Amazon SageMaker Edge Manager makes it easy to optimize, secure, monitor, and maintain  machine learning (ML) models across fleets of edge devices such as smart cameras, smart speakers, and robots.\n* [AWSSDK.SageMakerFeatureStoreRuntime](https://www.nuget.org/packages/AWSSDK.SageMakerFeatureStoreRuntime/)\n\t* This release adds support for Amazon SageMaker Feature Store, which makes it easy for customers to create, version, share, and manage curated data for machine learning (ML) development.\n* [AWSSDK.SageMakerGeospatial](https://www.nuget.org/packages/AWSSDK.SageMakerGeospatial/)\n\t* This release provides Amazon SageMaker geospatial APIs to build, train, deploy and visualize geospatial models.\n* [AWSSDK.SageMakerMetrics](https://www.nuget.org/packages/AWSSDK.SageMakerMetrics/)\n\t* This release introduces support SageMaker Metrics APIs.\n* [AWSSDK.SageMakerRuntime](https://www.nuget.org/packages/AWSSDK.SageMakerRuntime/)\n\t* Amazon SageMaker is a fully-managed service that enables data scientists and developers to quickly and easily build, train, and deploy machine learning models, at scale.\n* [AWSSDK.SavingsPlans](https://www.nuget.org/packages/AWSSDK.SavingsPlans/)\n\t* This is the first release of Savings Plans, a new flexible pricing model that offers low prices on Amazon EC2 and AWS Fargate usage.\n* [AWSSDK.Scheduler](https://www.nuget.org/packages/AWSSDK.Scheduler/)\n\t* AWS introduces the new Amazon EventBridge Scheduler. EventBridge Scheduler is a serverless scheduler that allows you to create, run, and manage tasks from one central, managed service.\n* [AWSSDK.Schemas](https://www.nuget.org/packages/AWSSDK.Schemas/)\n\t* This release introduces support for Amazon EventBridge schema registry, making it easy to discover and write code for events in EventBridge.\n* [AWSSDK.SecretsManager](https://www.nuget.org/packages/AWSSDK.SecretsManager/)\n\t* AWS Secrets Manager enables you to easily create and manage the secrets that you use in your customer-facing apps. Instead of embedding credentials into your source code, you can dynamically query Secrets Manager from your app whenever you need credentials. You can automatically and frequently rotate your secrets without having to deploy updates to your apps. All secret values are encrypted when they're at rest with AWS KMS, and while they're in transit with HTTPS and TLS.\n* [AWSSDK.SecurityHub](https://www.nuget.org/packages/AWSSDK.SecurityHub/)\n\t* AWS Security Hub provides you with a comprehensive view of your security state within AWS and your compliance with the security industry standards and best practices. Security Hub collects security data from across AWS accounts, services, and supported third-party partners and helps you analyze your security trends and identify the highest priority security issues.\n* [AWSSDK.SecurityLake](https://www.nuget.org/packages/AWSSDK.SecurityLake/)\n\t* Amazon Security Lake automatically centralizes security data from cloud, on-premises, and custom sources into a purpose-built data lake stored in your account. Security Lake makes it easier to analyze security data, so you can improve the protection of your workloads, applications, and data\n* [AWSSDK.SecurityToken](https://www.nuget.org/packages/AWSSDK.SecurityToken/)\n\t* The AWS Security Token Service (AWS STS) enables you to provide trusted users with temporary credentials that provide controlled access to your AWS resources.\n* [AWSSDK.ServerlessApplicationRepository](https://www.nuget.org/packages/AWSSDK.ServerlessApplicationRepository/)\n\t* First release of the AWS Serverless Application Repository SDK.\n* [AWSSDK.ServerMigrationService](https://www.nuget.org/packages/AWSSDK.ServerMigrationService/)\n\t* AWS Server Migration Service (SMS) is an agentless service which makes it easier and faster for you to migrate thousands of on-premises workloads to AWS.\n* [AWSSDK.ServiceCatalog](https://www.nuget.org/packages/AWSSDK.ServiceCatalog/)\n\t* AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.\n* [AWSSDK.ServiceDiscovery](https://www.nuget.org/packages/AWSSDK.ServiceDiscovery/)\n\t* AWS Cloud Map lets you configure public DNS, private DNS, or HTTP namespaces that your microservice applications run in. When an instance of the service becomes available, you can call the AWS Cloud Map API to register the instance with AWS Cloud Map. For public or private DNS namespaces, AWS Cloud Map automatically creates DNS records and an optional health check. Clients that submit public or private DNS queries, or HTTP requests, for the service receive an answer that contains up to eight healthy records.\n* [AWSSDK.ServiceQuotas](https://www.nuget.org/packages/AWSSDK.ServiceQuotas/)\n\t* Service Quotas enables you to view and manage your quotas for AWS services from a central location.\n* [AWSSDK.Shield](https://www.nuget.org/packages/AWSSDK.Shield/)\n\t* AWS Shield protects web applications from large and sophisticated DDoS attacks at Layer 3, 4 and 7. In addition AWS Shield provides visibility in attacks, and access to 24X7 DDoS Response Team.\n* [AWSSDK.Signer](https://www.nuget.org/packages/AWSSDK.Signer/)\n\t* With code signing for IoT, you can sign code that you create for any IoT device that is supported by Amazon Web Services (AWS). Code signing is available through Amazon FreeRTOS and AWS IoT Device Management, and integrated with AWS Certificate Manager (ACM).\n* [AWSSDK.SimpleDB](https://www.nuget.org/packages/AWSSDK.SimpleDB/)\n\t* Amazon SimpleDB is a highly available, scalable, and flexible non-relational data store that enables you to store and query data items using web services requests.\n* [AWSSDK.SimpleEmail](https://www.nuget.org/packages/AWSSDK.SimpleEmail/)\n\t* Amazon SES is an outbound-only email-sending service that provides an easy, cost-effective way for you to send email.\n* [AWSSDK.SimpleEmailV2](https://www.nuget.org/packages/AWSSDK.SimpleEmailV2/)\n\t* This is the first release of version 2 of the Amazon SES API. You can use this API to configure your Amazon SES account, and to send email. This API extends the functionality that exists in the previous version of the Amazon SES API.\n* [AWSSDK.SimpleNotificationService](https://www.nuget.org/packages/AWSSDK.SimpleNotificationService/)\n\t* Amazon Simple Notification Service (Amazon SNS) is a fast, flexible, fully managed push messaging service. Amazon SNS makes it simple and cost-effective to push notifications to Apple, Google, Fire OS, and Windows devices, as well as Android devices in China with Baidu Cloud Push.  You can also use SNS to push notifications to internet connected smart devices, as well as other distributed services.\n* [AWSSDK.SimpleSystemsManagement](https://www.nuget.org/packages/AWSSDK.SimpleSystemsManagement/)\n\t* Amazon EC2 Simple Systems Manager (SSM) enables you to manage a number of administrative and configuration tasks on your instances.\n* [AWSSDK.SimpleWorkflow](https://www.nuget.org/packages/AWSSDK.SimpleWorkflow/)\n\t* Amazon SWF helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\n* [AWSSDK.SimSpaceWeaver](https://www.nuget.org/packages/AWSSDK.SimSpaceWeaver/)\n\t* AWS SimSpace Weaver is a new service that helps customers build spatial simulations at new levels of scale - resulting in virtual worlds with millions of dynamic entities. See the AWS SimSpace Weaver developer guide for more details on how to get started. https://docs.aws.amazon.com/simspaceweaver\n* [AWSSDK.Snowball](https://www.nuget.org/packages/AWSSDK.Snowball/)\n\t* Amazon Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of the AWS cloud\n* [AWSSDK.SnowDeviceManagement](https://www.nuget.org/packages/AWSSDK.SnowDeviceManagement/)\n\t* AWS Snow Family customers can remotely monitor and operate their connected AWS Snowcone devices.\n* [AWSSDK.SQS](https://www.nuget.org/packages/AWSSDK.SQS/)\n\t* Amazon Simple Queue Service (SQS) is a fast, reliable, scalable, fully managed message queuing service. SQS makes it simple and cost-effective to decouple the components of a cloud application.\n* [AWSSDK.SSMContacts](https://www.nuget.org/packages/AWSSDK.SSMContacts/)\n\t* AWS Systems Manager Incident Manager enables faster resolution of critical application availability and performance issues, management of contacts and post incident analysis\n* [AWSSDK.SSMIncidents](https://www.nuget.org/packages/AWSSDK.SSMIncidents/)\n\t* AWS Systems Manager Incident Manager enables faster resolution of critical application availability and performance issues, management of contacts and post-incident analysis\n* [AWSSDK.SsmSap](https://www.nuget.org/packages/AWSSDK.SsmSap/)\n\t* AWS Systems Manager for SAP provides simplified operations and management of SAP applications such as SAP HANA. With this release, SAP customers and partners can automate and simplify their SAP system administration tasks such as backup/restore of SAP HANA.\n* [AWSSDK.SSO](https://www.nuget.org/packages/AWSSDK.SSO/)\n\t* This is an initial release of AWS Single Sign-On (SSO) end-user access. This release adds support for accessing AWS accounts assigned in AWS SSO using short term credentials.\n* [AWSSDK.SSOAdmin](https://www.nuget.org/packages/AWSSDK.SSOAdmin/)\n\t* This is an initial release of AWS Single Sign-On (SSO) Access Management APIs. This release adds support for SSO operations which could be used for managing access to AWS accounts.\n* [AWSSDK.SSOOIDC](https://www.nuget.org/packages/AWSSDK.SSOOIDC/)\n\t* This is an initial release of AWS Single Sign-On OAuth device code authorization service.\n* [AWSSDK.StepFunctions](https://www.nuget.org/packages/AWSSDK.StepFunctions/)\n\t* AWS Step Functions is a web service that enables you to coordinate a network of computing resources across distributed components using state machines.\n* [AWSSDK.StorageGateway](https://www.nuget.org/packages/AWSSDK.StorageGateway/)\n\t* The AWS Storage Gateway is a service connecting an on-premises software appliance with cloud-based storage to provide seamless and secure integration between an organization's on-premises IT environment and AWS's storage infrastructure.\n* [AWSSDK.SupplyChain](https://www.nuget.org/packages/AWSSDK.SupplyChain/)\n\t* This release includes APIs CreateBillOfMaterialsImportJob and GetBillOfMaterialsImportJob.\n* [AWSSDK.SupportApp](https://www.nuget.org/packages/AWSSDK.SupportApp/)\n\t* This is the initial SDK release for the AWS Support App in Slack.\n* [AWSSDK.Synthetics](https://www.nuget.org/packages/AWSSDK.Synthetics/)\n\t* Introducing CloudWatch Synthetics. This is the first public release of CloudWatch Synthetics.\n* [AWSSDK.Textract](https://www.nuget.org/packages/AWSSDK.Textract/)\n\t* Amazon Textract enables you to add document text detection and analysis to your applications. You provide a document image to the Amazon Textract API, and the service detects the document text. Amazon Textract works with formatted text and can detect words and lines of words that are located close to each other. It can also analyze a document for items such as related text, tables, key-value pairs, and selection elements.\n* [AWSSDK.TimestreamQuery](https://www.nuget.org/packages/AWSSDK.TimestreamQuery/)\n\t* (New Service) Amazon Timestream is a fast, scalable, fully managed, purpose-built time series database that makes it easy to store and analyze trillions of time series data points per day.\n* [AWSSDK.TimestreamWrite](https://www.nuget.org/packages/AWSSDK.TimestreamWrite/)\n\t* (New Service) Amazon Timestream is a fast, scalable, fully managed, purpose-built time series database that makes it easy to store and analyze trillions of time series data points per day.\n* [AWSSDK.Tnb](https://www.nuget.org/packages/AWSSDK.Tnb/)\n\t* This is the initial SDK release for AWS Telco Network Builder (TNB). AWS Telco Network Builder is a network automation service that helps you deploy and manage telecom networks.\n* [AWSSDK.TranscribeService](https://www.nuget.org/packages/AWSSDK.TranscribeService/)\n\t* Amazon Transcribe Public Preview Release\n* [AWSSDK.Transfer](https://www.nuget.org/packages/AWSSDK.Transfer/)\n\t* AWS Transfer for SFTP is a fully managed service that enables transfer of secure data over the internet into and out of Amazon S3. SFTP is deeply embedded in data exchange workflows across different industries such as financial services, healthcare, advertising, and retail, among others.\n* [AWSSDK.Translate](https://www.nuget.org/packages/AWSSDK.Translate/)\n\t* Public preview release of Amazon Translate and the Amazon Translate Developer Guide. For more information, see the Amazon Translate Developer Guide.\n* [AWSSDK.TrustedAdvisor](https://www.nuget.org/packages/AWSSDK.TrustedAdvisor/)\n\t* AWS Trusted Advisor introduces new APIs to enable you to programmatically access Trusted Advisor best practice checks, recommendations, and prioritized recommendations. Trusted Advisor APIs enable you to integrate Trusted Advisor with your operational tools to automate your workloads.\n* [AWSSDK.VerifiedPermissions](https://www.nuget.org/packages/AWSSDK.VerifiedPermissions/)\n\t* GA release of Amazon Verified Permissions.\n* [AWSSDK.VoiceID](https://www.nuget.org/packages/AWSSDK.VoiceID/)\n\t* Released the Amazon Voice ID SDK, for usage with the Amazon Connect Voice ID feature released for Amazon Connect.\n* [AWSSDK.VPCLattice](https://www.nuget.org/packages/AWSSDK.VPCLattice/)\n\t* General Availability (GA) release of Amazon VPC Lattice\n* [AWSSDK.WAF](https://www.nuget.org/packages/AWSSDK.WAF/)\n\t* AWS WAF (Web Application Firewall) protects web applications from attack by allowing customers to block bad actors and provides filters against common web exploits like SQL injection.\n* [AWSSDK.WAFRegional](https://www.nuget.org/packages/AWSSDK.WAFRegional/)\n\t* AWS WAF (Web Application Firewall) Regional protects web applications from attack via ALB load balancer and provides API to associate it with a WAF WebACL.\n* [AWSSDK.WAFV2](https://www.nuget.org/packages/AWSSDK.WAFV2/)\n\t* This release introduces new set of APIs (wafv2) for AWS WAF. Major changes include single set of APIs for creating/updating resources in global and regional scope, and rules are configured directly into web ACL instead of being referenced. The previous APIs (waf and waf-regional) are now referred as AWS WAF Classic. For more information visit: https://docs.aws.amazon.com/waf/latest/APIReference/Welcome.html\n* [AWSSDK.WellArchitected](https://www.nuget.org/packages/AWSSDK.WellArchitected/)\n\t* This is the first release of AWS Well-Architected Tool API support, use to review your workload and compare against the latest AWS architectural best practices.\n* [AWSSDK.WorkDocs](https://www.nuget.org/packages/AWSSDK.WorkDocs/)\n\t* Amazon WorkDocs is a fully managed, secure enterprise storage and sharing service with strong administrative controls and feedback capabilities that improve user productivity.\n* [AWSSDK.WorkLink](https://www.nuget.org/packages/AWSSDK.WorkLink/)\n\t* This is the initial SDK release for Amazon WorkLink. Amazon WorkLink is a fully managed, cloud-based service that enables secure, one-click access to internal websites and web apps from mobile phones. With Amazon WorkLink, employees can access internal websites as seamlessly as they access any other website. IT administrators can manage users, devices, and domains by enforcing their own security and access policies via the AWS Console or the AWS SDK.\n* [AWSSDK.WorkMail](https://www.nuget.org/packages/AWSSDK.WorkMail/)\n\t* Today, Amazon WorkMail released an administrative SDK and enabled AWS CloudTrail integration. With the administrative SDK, you can natively integrate WorkMail with your existing services. The SDK enables programmatic user, resource, and group management through API calls. This means your existing IT tools and workflows can now automate WorkMail management, and third party applications can streamline WorkMail migrations and account actions.\n* [AWSSDK.WorkMailMessageFlow](https://www.nuget.org/packages/AWSSDK.WorkMailMessageFlow/)\n\t* This release allows customers to access email messages as they flow to and from Amazon WorkMail.\n* [AWSSDK.WorkSpaces](https://www.nuget.org/packages/AWSSDK.WorkSpaces/)\n\t* Amazon WorkSpaces is a managed desktop computing service in the cloud.\n* [AWSSDK.WorkSpacesThinClient](https://www.nuget.org/packages/AWSSDK.WorkSpacesThinClient/)\n\t* Initial release of Amazon WorkSpaces Thin Client\n* [AWSSDK.WorkSpacesWeb](https://www.nuget.org/packages/AWSSDK.WorkSpacesWeb/)\n\t* This is the initial SDK release for Amazon WorkSpaces Web. Amazon WorkSpaces Web is a low-cost, fully managed WorkSpace built to deliver secure web-based workloads and software-as-a-service (SaaS) application access to users within existing web browsers.\n* [AWSSDK.XRay](https://www.nuget.org/packages/AWSSDK.XRay/)\n\t* AWS X-Ray helps developers analyze and debug distributed applications. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors.\n\n### Code Generator\n\nAll low-level service clients are created using the code generator found in the **generator** folder. The code generator \nuses the service models defined in the **generator\\ServiceModels** folder.\n\n[nuget-info]: https://nuget.org/\n[aws]: http://aws.amazon.com/\n[sdk-website]: http://aws.amazon.com/sdkfornet\n[sdk-forum]: http://developer.amazonwebservices.com/connect/forum.jspa?forumID=61\n[sdk-source]: https://github.com/aws/aws-sdk-net\n[sdk-issues]: https://github.com/aws/aws-sdk-net/issues\n[sdk-license]: http://aws.amazon.com/apache2.0/\n[docs-api]: http://docs.aws.amazon.com/sdkfornet/v3/apidocs/Index.html\n[docs-signup]: http://docs.aws.amazon.com/sdk-for-net/latest/developer-guide/net-dg-setup.html\n[aws-iam-credentials]: http://docs.aws.amazon.com/sdk-for-net/latest/developer-guide/net-dg-hosm.html\n[docs-guide]: http://docs.aws.amazon.com/sdk-for-net/latest/developer-guide/welcome.html\n[credentials-management]: http://docs.aws.amazon.com/sdk-for-net/latest/developer-guide/net-dg-config-creds.html\n[dotnet-blog]: http://blogs.aws.amazon.com/net/\n[github-aws-sdk-net-v2]: https://github.com/aws/aws-sdk-net/tree/aws-sdk-net-v2\n\n\n", "release_dates": []}, {"name": "aws-sdk-net-extensions-cognito", "description": "An extension library to assist in the Amazon Cognito User Pools authentication process", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![.NET on AWS Banner](./logo.png \".NET on AWS\")\n\n## Amazon Cognito Authentication Extension Library\n\n[![nuget](https://img.shields.io/nuget/v/Amazon.Extensions.CognitoAuthentication.svg)](https://www.nuget.org/packages/Amazon.Extensions.CognitoAuthentication/)\n\n[Amazon.Extensions.CognitoAuthentication](https://www.nuget.org/packages/Amazon.Extensions.CognitoAuthentication/) simplifies the authentication process of [Amazon Cognito User Pools](https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html) for .NET developers.\n\nIt allows you to use various authentication methods for Amazon Cognito User Pools with only a few short method calls, and makes the process intuitive.\n\n[Learn more about Amazon Cognito User Pools.](https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-getting-started.html)\n\nThis library targets the .NET Standard 2.0 and introduces the following dependencies:\n\n* [AWSSDK.CognitoIdentity](https://www.nuget.org/packages/AWSSDK.CognitoIdentity/)\n* [AWSSDK.CognitoIdentityProvider](https://www.nuget.org/packages/AWSSDK.CognitoIdentityProvider/)\n\n\n# Getting Started\n\nTo take advantage of this library, set up an AWS account and install the AWS SDK for .NET as described in [Getting Started with the AWS SDK for .NET](https://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/net-dg-setup.html).\n\nWhile this library is in development, you will need to build it manually.\n\nCreate a new project in Visual Studio and add the Amazon Cognito Authentication Extension Library as a reference to the project.\n\nUsing the library to make calls to the Amazon Cognito Identity Provider API from the AWS SDK for .NET is as simple as creating the necessary **CognitoAuthentication** objects and calling the appropriate **AmazonCognitoIdentityProviderClient** methods. The principal Amazon Cognito authentication objects are:\n\n- **CognitoUserPool** objects store information about a user pool, including the poolID, clientID, and other pool attributes.\n- **CognitoUser** objects contain a user\u2019s username, the pool they are associated with, session information, and other user properties.\n- **CognitoDevice** objects include device information, such as the device key.\n\n## Authenticating with Secure Remote Protocol (SRP)\n\nInstead of implementing hundreds of lines of cryptographic methods yourself, you now only need to create the necessary **AmazonCognitoIdentityProviderClient**, **CognitoUserPool**, **CognitoUser**, and **InitiateSrpAuthRequest** objects and then call **StartWithSrpAuthAsync**:\n\n\n```csharp\nusing Amazon.Runtime;\nusing Amazon.CognitoIdentityProvider;\nusing Amazon.Extensions.CognitoAuthentication;\n\npublic async void AuthenticateWithSrpAsync()\n{\n    var provider = new AmazonCognitoIdentityProviderClient(new AnonymousAWSCredentials(), FallbackRegionFactory.GetRegionEndpoint());\n    var userPool = new CognitoUserPool(\"poolID\", \"clientID\", provider);\n    var user = new CognitoUser(\"username\", \"clientID\", userPool, provider);\n\n    var password = \"userPassword\";\n\n    AuthFlowResponse authResponse = await user.StartWithSrpAuthAsync(new InitiateSrpAuthRequest()\n    {\n        Password = password\n    }).ConfigureAwait(false);\n}\n```\n\nThe **AuthenticationResult** property of the **AuthFlowResponse** object contains the user\u2019s session tokens if the user was successfully authenticated. If more challenge responses are required, this field is null and the **ChallengeName** property describes the next challenge, such as multi-factor authentication. You would then call the appropriate method to continue the authentication flow. \n\n## Authenticating with Multiple Forms of Authentication\n\nContinuing the authentication flow with challenges, such as with **NewPasswordRequired** and **Multi-Factor Authentication (MFA)**, is simpler as well. \n\nThe following code shows one way to check the challenge type and get appropriate responses for MFA and NewPasswordRequired challenges. This processing might be necessary as the authentication flow proceeds, depending on the properties of the **AuthFlowResponse** object that was retrieved earlier.\n\n```csharp\nwhile (authResponse.AuthenticationResult == null)\n{\n    if (authResponse.ChallengeName == ChallengeNameType.NEW_PASSWORD_REQUIRED)\n    {\n        Console.WriteLine(\"Enter your desired new password:\");\n        string newPassword = Console.ReadLine();\n\n        authResponse = \n            await user.RespondToNewPasswordRequiredAsync(new RespondToNewPasswordRequiredRequest()\n            {\n                SessionID = authResponse.SessionID,\n                NewPassword = newPassword\n            }).ConfigureAwait(false);\n    }\n    else if (authResponse.ChallengeName == ChallengeNameType.SMS_MFA)\n    {\n        Console.WriteLine(\"Enter the MFA Code sent to your device:\");\n        string mfaCode = Console.ReadLine();\n\n        authResponse = await user.RespondToSmsMfaAuthAsync(new RespondToSmsMfaRequest()\n        {\n                SessionID = authResponse.SessionID,\n                MfaCode = mfaCode\n        }).ConfigureAwait(false);\n        }\n        else\n        {\n            Console.WriteLine(\"Unrecognized authentication challenge.\");\n            break;\n        }\n}\n```\n\n[Learn more about Amazon Cognito User Pool Authentication Flow.](https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-authentication-flow.html)\n\n## Authenticating with Different Levels of Authentication\n\nAfter a user is authenticated by using the Amazon Cognito Authentication Extension Library, you can then allow them to access specific AWS resources.\n\nTo allow users to access specific AWS resources, you must create an identity pool through the **Amazon Cognito Federated Identities** console.\n\nYou can also specify different roles for both unauthenticated and authenticated users so that they can access different resources. \nThese roles can be changed in the IAM console where you can add or remove permissions in the **Action** field of the role\u2019s attached policy. Then, using the appropriate identity pool, user pool, and Amazon Cognito user information, calls can be made to different AWS resources.\n\nThe following code shows how a user, who was authenticated with SRP, can access various S3 buckets as permitted by the associated identity pool\u2019s role.\n\n```csharp\nusing Amazon;\nusing Amazon.Runtime;\nusing Amazon.S3;\nusing Amazon.S3.Model;\nusing Amazon.CognitoIdentity;\nusing Amazon.CognitoIdentityProvider;\nusing Amazon.Extensions.CognitoAuthentication;\n\npublic async void GetS3BucketsAsync()\n{\n    var provider = new AmazonCognitoIdentityProviderClient(new AnonymousAWSCredentials(),\n                                                            FallbackRegionFactory.GetRegionEndpoint());\n    var userPool = new CognitoUserPool(\"poolID\", \"clientID\", provider);\n    var user = new CognitoUser(\"username\", \"clientID\", userPool, provider);\n\n    var password = \"userPassword\";\n\n    await user.StartWithSrpAuthAsync(new InitiateSrpAuthRequest()\n    {\n        Password = password\n    }).ConfigureAwait(false);\n\n    var credentials = \n        user.GetCognitoAWSCredentials(\"identityPoolID\", RegionEndpoint.<YourIdentityPoolRegion>);\n\n    using (var client = new AmazonS3Client(credentials))\n    {\n        ListBucketsResponse response = \n            await client.ListBucketsAsync(new ListBucketsRequest()).ConfigureAwait(false);\n\n        foreach (S3Bucket bucket in response.Buckets)\n        {\n            Console.WriteLine(bucket.BucketName);\n        }\n    }\n}\n```\n\n## Authenticating using a Refresh Token from a Previous Session\n\nAccess and ID tokens provided by Cognito are only valid for one hour but the refresh token can be configured to be valid for much longer. Below is an example of how to retrieve new Access and ID tokens using a refresh token which is still valid.\n\nSee [here](https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html) to learn more about using the tokens returned by Amazon Cognito.\n\n```csharp\nusing Amazon;\nusing Amazon.Runtime;\nusing Amazon.CognitoIdentity;\nusing Amazon.CognitoIdentityProvider;\nusing Amazon.Extensions.CognitoAuthentication;\n\npublic async Task GetCredsFromRefreshAsync(string refreshToken, string deviceKey)\n{\n    using var provider = new AmazonCognitoIdentityProviderClient();\n    var userPool = new CognitoUserPool(\"poolID\", \"clientID\", provider);\n\n    var user = new CognitoUser(\"username\", \"clientID\", userPool, provider, \"clientSecret\")\n    {\n        SessionTokens = new CognitoUserSession(null, null, refreshToken, DateTime.UtcNow, DateTime.UtcNow.AddHours(1))\n    };\n\n    // If the user pool is configured to track and remember user devices, it must be attached to the user before initiating the flow:\n    // user.Device = new CognitoDevice(new DeviceType { DeviceKey = deviceKey }, user);\n\n    var authResponse = await user.StartWithRefreshTokenAuthAsync(new InitiateRefreshTokenAuthRequest\n    {\n        AuthFlowType = AuthFlowType.REFRESH_TOKEN_AUTH\n    });\n}\n```\n\n## Other Forms of Authentication\n\nIn addition to SRP, NewPasswordRequired, MFA and Refresh the Amazon Cognito Authentication Extension Library offers an easier authentication flow for the following:\n\n- **Custom** \u2013 Begins with a call to StartWithCustomAuthAsync(InitiateCustomAuthRequest customRequest)\n- **AdminNoSRP** \u2013 Begins with a call to StartWithAdminNoSrpAuth(InitiateAdminNoSrpAuthRequest adminAuthRequest)\n\n# Getting Help\n\nWe use the [GitHub issues](https://github.com/aws/aws-sdk-net-extensions-cognito/issues) for tracking bugs and feature requests and have limited bandwidth to address them.\n\nIf you think you may have found a bug, please open an [issue](https://github.com/aws/aws-sdk-net-extensions-cognito/issues/new)\n\n# Contributing\n\nWe welcome community contributions and pull requests. See\n[CONTRIBUTING](./CONTRIBUTING.md) for information on how to set up a development\nenvironment and submit code.\n\n# Additional Resources\n\n[AWS .NET GitHub Home Page](https://github.com/aws/dotnet)  \nGitHub home for .NET development on AWS. You'll find libraries, tools, and resources to help you build .NET applications and services on AWS.\n\n[AWS Developer Center - Explore .NET on AWS](https://aws.amazon.com/developer/language/net/)  \nFind .NET code samples, step-by-step guides, videos, blog content, tools, and information about live events all in one place. \n\n[AWS Developer Blog - .NET](https://aws.amazon.com/blogs/developer/category/programing-language/dot-net/)  \nCome and see what .NET developers at AWS are up to! Learn about new .NET software announcements, guides, and how-to's.\n\n[@dotnetonaws](https://twitter.com/dotnetonaws)\nFollow us on twitter!\n\n# License\n\nLibraries in this repository are licensed under the Apache 2.0 License. \n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.\n", "release_dates": []}, {"name": "aws-sdk-pandas", "description": "pandas on AWS - Easy integration with Athena, Glue, Redshift, Timestream, Neptune, OpenSearch, QuickSight, Chime, CloudWatchLogs, DynamoDB, EMR, SecretManager, PostgreSQL, MySQL, SQLServer and S3 (Parquet, CSV, JSON and EXCEL).", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for pandas (awswrangler)\n\nAWS Data Wrangler is now **AWS SDK for pandas (awswrangler)**.  We\u2019re changing the name we use when we talk about the library, but everything else will stay the same.  You\u2019ll still be able to install using `pip install awswrangler` and you won\u2019t need to change any of your code.  As part of this change, we\u2019ve moved the library from AWS Labs to the main AWS GitHub organisation but, thanks to the GitHub\u2019s redirect feature, you\u2019ll still be able to access the project by its old URLs until you update your bookmarks.  Our documentation has also moved to [aws-sdk-pandas.readthedocs.io](https://aws-sdk-pandas.readthedocs.io), but old bookmarks will redirect to the new site.\n\n*Pandas on AWS*\n\nEasy integration with Athena, Glue, Redshift, Timestream, OpenSearch, Neptune, QuickSight, Chime, CloudWatchLogs, DynamoDB, EMR, SecretManager, PostgreSQL, MySQL, SQLServer and S3 (Parquet, CSV, JSON and EXCEL).\n\n![AWS SDK for pandas](docs/source/_static/logo2.png?raw=true \"AWS SDK for pandas\")\n![tracker](https://d3tiqpr4kkkomd.cloudfront.net/img/pixel.png?asset=GVOYN2BOOQ573LTVIHEW)\n\n> An [AWS Professional Service](https://aws.amazon.com/professional-services/) open source initiative | aws-proserve-opensource@amazon.com\n\n[![PyPi](https://img.shields.io/pypi/v/awswrangler)](https://pypi.org/project/awswrangler/)\n[![Conda](https://img.shields.io/conda/vn/conda-forge/awswrangler)](https://anaconda.org/conda-forge/awswrangler)\n[![Python Version](https://img.shields.io/pypi/pyversions/awswrangler.svg)](https://pypi.org/project/awswrangler/)\n[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\n[![Checked with mypy](http://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)\n![Static Checking](https://github.com/aws/aws-sdk-pandas/workflows/Static%20Checking/badge.svg?branch=main)\n[![Documentation Status](https://readthedocs.org/projects/aws-sdk-pandas/badge/?version=latest)](https://aws-sdk-pandas.readthedocs.io/?badge=latest)\n\n| Source | Downloads | Installation Command |\n|--------|-----------|----------------------|\n| **[PyPi](https://pypi.org/project/awswrangler/)**  | [![PyPI Downloads](https://img.shields.io/pypi/dm/awswrangler)](https://pypi.org/project/awswrangler/) | `pip install awswrangler` |\n| **[Conda](https://anaconda.org/conda-forge/awswrangler)** | [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/awswrangler.svg)](https://anaconda.org/conda-forge/awswrangler) | `conda install -c conda-forge awswrangler` |\n\n> \u26a0\ufe0f **Starting version 3.0, optional modules must be installed explicitly:**<br>\n\u27a1\ufe0f`pip install 'awswrangler[redshift]'`\n\nPowered By [<img src=\"https://arrow.apache.org/img/arrow.png\" width=\"200\">](https://arrow.apache.org/powered_by/)\n\n## Table of contents\n\n- [Quick Start](#quick-start)\n- [At Scale](#at-scale)\n- [Read The Docs](#read-the-docs)\n- [Getting Help](#getting-help)\n- [Community Resources](#community-resources)\n- [Logging](#logging)\n- [Who uses AWS SDK for pandas?](#who-uses-aws-sdk-for-pandas)\n\n## Quick Start\n\nInstallation command: `pip install awswrangler`\n\n> \u26a0\ufe0f **Starting version 3.0, optional modules must be installed explicitly:**<br>\n\u27a1\ufe0f`pip install 'awswrangler[redshift]'`\n\n```py3\nimport awswrangler as wr\nimport pandas as pd\nfrom datetime import datetime\n\ndf = pd.DataFrame({\"id\": [1, 2], \"value\": [\"foo\", \"boo\"]})\n\n# Storing data on Data Lake\nwr.s3.to_parquet(\n    df=df,\n    path=\"s3://bucket/dataset/\",\n    dataset=True,\n    database=\"my_db\",\n    table=\"my_table\"\n)\n\n# Retrieving the data directly from Amazon S3\ndf = wr.s3.read_parquet(\"s3://bucket/dataset/\", dataset=True)\n\n# Retrieving the data from Amazon Athena\ndf = wr.athena.read_sql_query(\"SELECT * FROM my_table\", database=\"my_db\")\n\n# Get a Redshift connection from Glue Catalog and retrieving data from Redshift Spectrum\ncon = wr.redshift.connect(\"my-glue-connection\")\ndf = wr.redshift.read_sql_query(\"SELECT * FROM external_schema.my_table\", con=con)\ncon.close()\n\n# Amazon Timestream Write\ndf = pd.DataFrame({\n    \"time\": [datetime.now(), datetime.now()],   \n    \"my_dimension\": [\"foo\", \"boo\"],\n    \"measure\": [1.0, 1.1],\n})\nrejected_records = wr.timestream.write(df,\n    database=\"sampleDB\",\n    table=\"sampleTable\",\n    time_col=\"time\",\n    measure_col=\"measure\",\n    dimensions_cols=[\"my_dimension\"],\n)\n\n# Amazon Timestream Query\nwr.timestream.query(\"\"\"\nSELECT time, measure_value::double, my_dimension\nFROM \"sampleDB\".\"sampleTable\" ORDER BY time DESC LIMIT 3\n\"\"\")\n\n```\n\n## At scale\nAWS SDK for pandas can also run your workflows at scale by leveraging [Modin](https://modin.readthedocs.io/en/stable/) and [Ray](https://www.ray.io/). Both projects aim to speed up data workloads by distributing processing over a cluster of workers.\n\nThe quickest way to get started is to use AWS Glue with Ray. Read our [docs](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/scale.html), our blogs ([1](https://aws.amazon.com/blogs/big-data/scale-aws-sdk-for-pandas-workloads-with-aws-glue-for-ray/)/[2](https://aws.amazon.com/blogs/big-data/advanced-patterns-with-aws-sdk-for-pandas-on-aws-glue-for-ray/)), or head to our latest [tutorials](https://github.com/aws/aws-sdk-pandas/tree/main/tutorials) to discover even more features.\n\n> \u26a0\ufe0f **Ray is currently not available for Python 3.12. While AWS SDK for pandas supports Python 3.12, it cannot be used at scale.**\n\n## [Read The Docs](https://aws-sdk-pandas.readthedocs.io/)\n\n- [**What is AWS SDK for pandas?**](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/about.html)\n- [**Install**](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/install.html)\n  - [PyPi (pip)](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/install.html#pypi-pip)\n  - [Conda](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/install.html#conda)\n  - [AWS Lambda Layer](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/install.html#aws-lambda-layer)\n  - [AWS Glue Python Shell Jobs](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/install.html#aws-glue-python-shell-jobs)\n  - [AWS Glue PySpark Jobs](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/install.html#aws-glue-pyspark-jobs)\n  - [Amazon SageMaker Notebook](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/install.html#amazon-sagemaker-notebook)\n  - [Amazon SageMaker Notebook Lifecycle](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/install.html#amazon-sagemaker-notebook-lifecycle)\n  - [EMR](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/install.html#emr)\n  - [From source](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/install.html#from-source)\n- [**At scale**](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/scale.html)\n  - [Getting Started](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/scale.html#getting-started)\n  - [Supported APIs](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/scale.html#supported-apis)\n  - [Resources](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/scale.html#resources)\n- [**Tutorials**](https://github.com/aws/aws-sdk-pandas/tree/main/tutorials)\n  - [001 - Introduction](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/001%20-%20Introduction.ipynb)\n  - [002 - Sessions](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/002%20-%20Sessions.ipynb)\n  - [003 - Amazon S3](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/003%20-%20Amazon%20S3.ipynb)\n  - [004 - Parquet Datasets](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/004%20-%20Parquet%20Datasets.ipynb)\n  - [005 - Glue Catalog](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/005%20-%20Glue%20Catalog.ipynb)\n  - [006 - Amazon Athena](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/006%20-%20Amazon%20Athena.ipynb)\n  - [007 - Databases (Redshift, MySQL, PostgreSQL, SQL Server and Oracle)](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/007%20-%20Redshift%2C%20MySQL%2C%20PostgreSQL%2C%20SQL%20Server%2C%20Oracle.ipynb)\n  - [008 - Redshift - Copy & Unload.ipynb](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/008%20-%20Redshift%20-%20Copy%20%26%20Unload.ipynb)\n  - [009 - Redshift - Append, Overwrite and Upsert](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/009%20-%20Redshift%20-%20Append%2C%20Overwrite%2C%20Upsert.ipynb)\n  - [010 - Parquet Crawler](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/010%20-%20Parquet%20Crawler.ipynb)\n  - [011 - CSV Datasets](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/011%20-%20CSV%20Datasets.ipynb)\n  - [012 - CSV Crawler](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/012%20-%20CSV%20Crawler.ipynb)\n  - [013 - Merging Datasets on S3](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/013%20-%20Merging%20Datasets%20on%20S3.ipynb)\n  - [014 - Schema Evolution](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/014%20-%20Schema%20Evolution.ipynb)\n  - [015 - EMR](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/015%20-%20EMR.ipynb)\n  - [016 - EMR & Docker](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/016%20-%20EMR%20%26%20Docker.ipynb)\n  - [017 - Partition Projection](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/017%20-%20Partition%20Projection.ipynb)\n  - [018 - QuickSight](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/018%20-%20QuickSight.ipynb)\n  - [019 - Athena Cache](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/019%20-%20Athena%20Cache.ipynb)\n  - [020 - Spark Table Interoperability](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/020%20-%20Spark%20Table%20Interoperability.ipynb)\n  - [021 - Global Configurations](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/021%20-%20Global%20Configurations.ipynb)\n  - [022 - Writing Partitions Concurrently](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/022%20-%20Writing%20Partitions%20Concurrently.ipynb)\n  - [023 - Flexible Partitions Filter](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/023%20-%20Flexible%20Partitions%20Filter.ipynb)\n  - [024 - Athena Query Metadata](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/024%20-%20Athena%20Query%20Metadata.ipynb)\n  - [025 - Redshift - Loading Parquet files with Spectrum](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/025%20-%20Redshift%20-%20Loading%20Parquet%20files%20with%20Spectrum.ipynb)\n  - [026 - Amazon Timestream](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/026%20-%20Amazon%20Timestream.ipynb)\n  - [027 - Amazon Timestream 2](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/027%20-%20Amazon%20Timestream%202.ipynb)\n  - [028 - Amazon DynamoDB](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/028%20-%20DynamoDB.ipynb)\n  - [029 - S3 Select](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/029%20-%20S3%20Select.ipynb)\n  - [030 - Data Api](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/030%20-%20Data%20Api.ipynb)\n  - [031 - OpenSearch](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/031%20-%20OpenSearch.ipynb)\n  - [033 - Amazon Neptune](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/033%20-%20Amazon%20Neptune.ipynb)\n  - [034 - Distributing Calls Using Ray](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/034%20-%20Distributing%20Calls%20using%20Ray.ipynb)\n  - [035 - Distributing Calls on Ray Remote Cluster](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/035%20-%20Distributing%20Calls%20on%20Ray%20Remote%20Cluster.ipynb)\n  - [036 - Distributing Calls with Glue Interactive Sessions on Ray](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/036%20-%20Distributing%20Calls%20with%20Glue%20Interactive%20Sessions%20on%20Ray.ipynb)\n  - [037 - Glue Data Quality](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/037%20-%20Glue%20Data%20Quality.ipynb)\n  - [038 - OpenSearch Serverless](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/038%20-%20OpenSearch%20Serverless.ipynb)\n  - [039 - Athena Iceberg](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/039%20-%20Athena%20Iceberg.ipynb)\n  - [040 - EMR Serverless](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/040%20-%20EMR%20Serverless.ipynb)\n  - [041 - Apache Spark on Amazon Athena](https://github.com/aws/aws-sdk-pandas/blob/main/tutorials/041%20-%20Apache%20Spark%20on%20Amazon%20Athena.ipynb)\n- [**API Reference**](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html)\n  - [Amazon S3](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#amazon-s3)\n  - [AWS Glue Catalog](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#aws-glue-catalog)\n  - [Amazon Athena](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#amazon-athena)\n  - [Amazon Redshift](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#amazon-redshift)\n  - [PostgreSQL](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#postgresql)\n  - [MySQL](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#mysql)\n  - [SQL Server](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#sqlserver)\n  - [Oracle](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#oracle)\n  - [Data API Redshift](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#data-api-redshift)\n  - [Data API RDS](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#data-api-rds)\n  - [OpenSearch](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#opensearch)\n  - [AWS Glue Data Quality](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#aws-glue-data-quality)\n  - [Amazon Neptune](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#amazon-neptune)\n  - [DynamoDB](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#dynamodb)\n  - [Amazon Timestream](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#amazon-timestream)\n  - [Amazon EMR](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#amazon-emr)\n  - [Amazon CloudWatch Logs](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#amazon-cloudwatch-logs)\n  - [Amazon Chime](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#amazon-chime)\n  - [Amazon QuickSight](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#amazon-quicksight)\n  - [AWS STS](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#aws-sts)\n  - [AWS Secrets Manager](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#aws-secrets-manager)\n  - [Global Configurations](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#global-configurations)\n  - [Distributed - Ray](https://aws-sdk-pandas.readthedocs.io/en/3.7.0b1/api.html#distributed-ray)\n- [**License**](https://github.com/aws/aws-sdk-pandas/blob/main/LICENSE.txt)\n- [**Contributing**](https://github.com/aws/aws-sdk-pandas/blob/main/CONTRIBUTING.md)\n\n## Getting Help\n\nThe best way to interact with our team is through GitHub. You can open an [issue](https://github.com/aws/aws-sdk-pandas/issues/new/choose) and choose from one of our templates for bug reports, feature requests...\nYou may also find help on these community resources:\n* The #aws-sdk-pandas Slack [channel](https://join.slack.com/t/aws-sdk-pandas/shared_invite/zt-sxdx38sl-E0coRfAds8WdpxXD2Nzfrg)\n* Ask a question on [Stack Overflow](https://stackoverflow.com/questions/tagged/awswrangler)\n  and tag it with `awswrangler`\n* [Runbook](https://github.com/aws/aws-sdk-pandas/discussions/1815) for AWS SDK for pandas with Ray\n\n## Community Resources\n\nPlease [send a Pull Request](https://github.com/aws/aws-sdk-pandas/edit/main/README.md) with your resource reference and @githubhandle.\n\n- [YouTube channel](https://www.youtube.com/playlist?list=PL7bE4nSzLSWdDdlfRgfKo2JBplB4p_v5O) [[@AdrianoNicolucci](https://github.com/AdrianoNicolucci)]\n- [Optimize Python ETL by extending Pandas with AWS SDK for pandas](https://aws.amazon.com/blogs/big-data/optimize-python-etl-by-extending-pandas-with-aws-data-wrangler/) [[@igorborgest](https://github.com/igorborgest)]\n- [Reading Parquet Files With AWS Lambda](https://aprakash.wordpress.com/2020/04/14/reading-parquet-files-with-aws-lambda/) [[@anand086](https://github.com/anand086)]\n- [Transform AWS CloudTrail data using AWS SDK for pandas](https://aprakash.wordpress.com/2020/09/17/transform-aws-cloudtrail-data-using-aws-data-wrangler/) [[@anand086](https://github.com/anand086)]\n- [Rename Glue Tables using AWS SDK for pandas](https://ananddatastories.com/rename-glue-tables-using-aws-sdk-pandas/) [[@anand086](https://github.com/anand086)]\n- [Getting started on AWS SDK for pandas and Athena](https://medium.com/@dheerajsharmainampudi/getting-started-on-aws-sdk-pandas-and-athena-7b446c834076) [[@dheerajsharma21](https://github.com/dheerajsharma21)]\n- [Simplifying Pandas integration with AWS data related services](https://medium.com/@bv_subhash/aws-sdk-pandas-simplifying-pandas-integration-with-aws-data-related-services-2b3325c12188) [[@bvsubhash](https://github.com/bvsubhash)]\n- [Build an ETL pipeline using AWS S3, Glue and Athena](https://www.linkedin.com/pulse/build-etl-pipeline-using-aws-s3-glue-athena-data-wrangler-tom-reid/) [[@taupirho](https://github.com/taupirho)]\n\n## Logging\n\nEnabling internal logging examples:\n\n```py3\nimport logging\nlogging.basicConfig(level=logging.INFO, format=\"[%(name)s][%(funcName)s] %(message)s\")\nlogging.getLogger(\"awswrangler\").setLevel(logging.DEBUG)\nlogging.getLogger(\"botocore.credentials\").setLevel(logging.CRITICAL)\n```\n\nInto AWS lambda:\n\n```py3\nimport logging\nlogging.getLogger(\"awswrangler\").setLevel(logging.DEBUG)\n```\n\n## Who uses AWS SDK for pandas?\n\nKnowing which companies are using this library is important to help prioritize the project internally.\nIf you would like us to include your company\u2019s name and/or logo in the README file to indicate that your company is using the AWS SDK for pandas, please raise a \"Support Us\" issue. If you would like us to display your company\u2019s logo, please raise a linked pull request to provide an image file for the logo. Note that by raising a Support Us issue (and related pull request), you are granting AWS permission to use your company\u2019s name (and logo) for the limited purpose described here and you are confirming that you have authority to grant such permission.\n\n- [Amazon](https://www.amazon.com/)\n- [AWS](https://aws.amazon.com/)\n- [Cepsa](https://cepsa.com) [[@alvaropc](https://github.com/alvaropc)]\n- [Cognitivo](https://www.cognitivo.ai/) [[@msantino](https://github.com/msantino)]\n- [Digio](https://www.digio.com.br/) [[@afonsomy](https://github.com/afonsomy)]\n- [DNX](https://www.dnx.solutions/) [[@DNXLabs](https://github.com/DNXLabs)]\n- [Fortescue Future Industries](https://ffi.com.au/) [[@spencervoorend](https://github.com/spencervoorend)]\n- [Funcional Health Tech](https://www.funcionalcorp.com.br/) [[@webysther](https://github.com/webysther)]\n- [Funding Circle](https://www.fundingcircle.com/) [[@pfig](https://github.com/pfig)]\n- [Infomach](https://www.infomach.com.br/)\n- [Informa Markets](https://www.informamarkets.com/en/home.html) [[@mateusmorato]](http://github.com/mateusmorato)\n- [LINE TV](https://www.linetv.tw/) [[@bryanyang0528](https://github.com/bryanyang0528)]\n- [LogicalCube](https://www.logicalcube.com) [[@zolabud](https://github.com/zolabud)]\n- [Magnataur](https://magnataur.com) [[@brianmingus2](https://github.com/brianmingus2)]\n- [M4U](https://www.m4u.com.br/) [[@Thiago-Dantas](https://github.com/Thiago-Dantas)]\n- [NBCUniversal](https://www.nbcuniversal.com/) [[@vibe](https://github.com/vibe)]\n- [nrd.io](https://nrd.io/) [[@mrtns](https://github.com/mrtns)]\n- [OKRA Technologies](https://okra.ai) [[@JPFrancoia](https://github.com/JPFrancoia), [@schot](https://github.com/schot)]\n- [Pier](https://www.pier.digital/) [[@flaviomax](https://github.com/flaviomax)]\n- [Pismo](https://www.pismo.io/) [[@msantino](https://github.com/msantino)]\n- [ringDNA](https://www.ringdna.com/) [[@msropp](https://github.com/msropp)]\n- [Serasa Experian](https://www.serasaexperian.com.br/) [[@andre-marcos-perez](https://github.com/andre-marcos-perez)]\n- [Shipwell](https://shipwell.com/) [[@zacharycarter](https://github.com/zacharycarter)]\n- [strongDM](https://www.strongdm.com/) [[@mrtns](https://github.com/mrtns)]\n- [Thinkbumblebee](https://www.thinkbumblebee.com/) [[@dheerajsharma21]](https://github.com/dheerajsharma21)\n- [VTEX](https://vtex.com/us-en/) [[@igorborgest]](https://github.com/igorborgest)\n- [Zillow](https://www.zillow.com/) [[@nicholas-miles]](https://github.com/nicholas-miles)\n", "release_dates": ["2024-02-14T18:22:38Z", "2024-01-25T01:21:38Z", "2024-01-12T20:54:52Z", "2024-01-11T12:32:35Z", "2023-11-13T19:02:13Z", "2023-10-24T13:19:55Z", "2023-09-11T18:35:05Z", "2023-08-01T19:53:16Z", "2023-06-14T21:59:06Z", "2023-06-13T00:07:22Z", "2023-05-16T00:12:01Z", "2023-05-15T17:54:34Z", "2023-04-13T16:54:05Z", "2023-03-21T23:13:41Z", "2023-03-09T13:38:19Z", "2023-03-01T15:20:04Z", "2023-01-09T20:59:02Z", "2022-12-02T16:30:52Z", "2022-11-23T12:54:13Z", "2022-10-27T18:25:15Z", "2022-10-12T15:28:18Z", "2022-09-30T09:42:57Z", "2022-09-22T16:38:30Z", "2022-09-20T23:11:07Z", "2022-08-17T10:35:06Z", "2022-08-17T10:06:12Z", "2022-06-28T16:39:07Z", "2022-06-22T18:21:32Z", "2022-04-11T15:35:37Z", "2022-03-28T14:36:53Z"]}, {"name": "aws-sdk-php", "description": "Official repository of the AWS SDK for PHP (@awsforphp)", "language": "PHP", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for PHP - Version 3\n\n[![Total Downloads](https://img.shields.io/packagist/dt/aws/aws-sdk-php.svg?style=flat)](https://packagist.org/packages/aws/aws-sdk-php)\n[![Apache 2 License](https://img.shields.io/packagist/l/aws/aws-sdk-php.svg?style=flat)](http://aws.amazon.com/apache-2-0/)\n[![Gitter](https://badges.gitter.im/aws/aws-sdk-php.svg)](https://gitter.im/aws/aws-sdk-php?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![codecov](https://codecov.io/gh/aws/aws-sdk-php/branch/master/graph/badge.svg)](https://codecov.io/gh/aws/aws-sdk-php)\n\nThe **AWS SDK for PHP** makes it easy for developers to access [Amazon Web\nServices][aws] in their PHP code, and build robust applications and software\nusing services like Amazon S3, Amazon DynamoDB, Amazon Glacier, etc. You can\nget started in minutes by [installing the SDK through Composer][docs-installation]\nor by downloading a single zip or phar file from our [latest release][latest-release].\n\nJump To:\n* [Getting Started](#Getting-Started)\n* [Quick Examples](#Quick-Examples)\n* [Getting Help](#Getting-Help)\n* [Features](#Features)\n* [Contributing](#Contributing)\n* [More Resources](#Resources)\n* [Related AWS Projects](#Related-AWS-Projects)\n\n## Getting Started\n\n1. **Sign up for AWS** \u2013 Before you begin, you need to\n   sign up for an AWS account and retrieve your [AWS credentials][docs-signup].\n2. **Minimum requirements** \u2013 To run the SDK, your system will need to meet the\n   [minimum requirements][docs-requirements], including having **PHP >= 7.2.5**.\n   We highly recommend having it compiled with the cURL extension and cURL\n   7.16.2+ compiled with a TLS backend (e.g., NSS or OpenSSL).\n3. **Install the SDK** \u2013 Using [Composer] is the recommended way to install the\n   AWS SDK for PHP. The SDK is available via [Packagist] under the\n   [`aws/aws-sdk-php`][install-packagist] package. If Composer is installed globally on your system, you can run the following in the base directory of your project to add the SDK as a dependency:\n   ```\n   composer require aws/aws-sdk-php\n   ```\n   Please see the\n   [Installation section of the User Guide][docs-installation] for more\n   detailed information about installing the SDK through Composer and other\n   means.\n4. **Using the SDK** \u2013 The best way to become familiar with how to use the SDK\n   is to read the [User Guide][docs-guide]. The\n   [Getting Started Guide][docs-quickstart] will help you become familiar with\n   the basic concepts.\n5. **Beta: Removing unused services** \u2014 To date, there are over 300 AWS services available for use with this SDK.\n   You will likely not need them all. If you use Composer and would like to learn more about this feature,\n    please read the [linked documentation][docs-script-composer].\n\n\n## Quick Examples\n\n### Create an Amazon S3 client\n\n```php\n<?php\n// Require the Composer autoloader.\nrequire 'vendor/autoload.php';\n\nuse Aws\\S3\\S3Client;\n\n// Instantiate an Amazon S3 client.\n$s3 = new S3Client([\n    'version' => 'latest',\n    'region'  => 'us-west-2'\n]);\n```\n\n### Upload a file to Amazon S3\n\n```php\n<?php\n// Upload a publicly accessible file. The file size and type are determined by the SDK.\ntry {\n    $s3->putObject([\n        'Bucket' => 'my-bucket',\n        'Key'    => 'my-object',\n        'Body'   => fopen('/path/to/file', 'r'),\n        'ACL'    => 'public-read',\n    ]);\n} catch (Aws\\S3\\Exception\\S3Exception $e) {\n    echo \"There was an error uploading the file.\\n\";\n}\n```\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues for tracking bugs and feature requests and have limited bandwidth to address them.\n\n* Ask a question on [StackOverflow](https://stackoverflow.com/) and tag it with [`aws-php-sdk`](http://stackoverflow.com/questions/tagged/aws-php-sdk)\n* Come join the AWS SDK for PHP [gitter](https://gitter.im/aws/aws-sdk-php)\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home/)\n* If it turns out that you may have found a bug, please [open an issue](https://github.com/aws/aws-sdk-php/issues/new/choose)\n\nThis SDK implements AWS service APIs. For general issues regarding the AWS services and their limitations, you may also take a look at the [Amazon Web Services Discussion Forums](https://forums.aws.amazon.com/).\n\n\n## Maintenance and support for SDK major versions\n\nFor information about maintenance and support for SDK major versions and their underlying dependencies, see the following in the [AWS SDKs and Tools Shared Configuration and Credentials Reference Guide](https://docs.aws.amazon.com/credref/latest/refdocs/overview.html):\n\n* [AWS SDKs and Tools Maintenance Policy](https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html)\n* [AWS SDKs and Tools Version Support Matrix](https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html)\n\n\n### Opening Issues\n\nIf you encounter a bug with `aws-sdk-php` we would like to hear about it. Search the existing issues and try to make sure your problem doesn\u2019t already exist before opening a new issue. It\u2019s helpful if you include the version of `aws-sdk-php`, PHP version and OS you\u2019re using. Please include a stack trace and a simple workflow to reproduce the case when appropriate, too.\n\nThe GitHub issues are intended for bug reports and feature requests. For help and questions with using `aws-sdk-php` please make use of the resources listed in the Getting Help section. There are limited resources available for handling issues and by keeping the list of open issues lean we can respond in a timely manner.\n\n## Features\n\n* Provides easy-to-use HTTP clients for all supported AWS\n  [services][docs-services], [regions][docs-rande], and authentication\n  protocols.\n* Is built on [Guzzle][guzzle-docs], and utilizes many of its features,\n  including persistent connections, asynchronous requests, middlewares, etc.\n* Provides convenience features including easy result pagination via\n  [Paginators][docs-paginators], [Waiters][docs-waiters], and simple\n  [Result objects][docs-results].\n* Provides a [multipart uploader tool][docs-s3-multipart] for Amazon S3 and\n  Amazon Glacier that can be paused and resumed.\n* Provides an [Amazon S3 Stream Wrapper][docs-streamwrapper], so that you can\n  use PHP's native file handling functions to interact with your S3 buckets and\n  objects like a local filesystem.\n* Provides an [Amazon S3 Encryption Client][docs-s3-encryption] for creating and interacting with encrypted objects in your S3 buckets.\n* Provides the [Amazon DynamoDB Session Handler][docs-ddbsh] for easily scaling\n  sessions on a fast, NoSQL database.\n* Automatically uses [IAM Instance Profile Credentials][aws-iam-credentials] on\n  configured Amazon EC2 instances.\n\n## Contributing\n\nWe work hard to provide a high-quality and useful SDK for our AWS services, and we greatly value feedback and contributions from our community. Please review our [contributing guidelines](./CONTRIBUTING.md) before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.\n\n\n## Resources\n\n* [User Guide][docs-guide] \u2013 For both getting started and in-depth SDK usage information\n* [API Docs][docs-api] \u2013 For details about operations, parameters, and responses\n* [Blog][sdk-blog] \u2013 Tips & tricks, articles, and announcements\n* [Sample Project][sdk-sample] - A quick, sample project to help get you started\n* [Forum][sdk-forum] \u2013 Ask questions, get help, and give feedback\n* [Issues][sdk-issues] \u2013 Report issues, submit pull requests, and get involved\n  (see [Apache 2.0 License][sdk-license])\n\n## Related AWS Projects\n\n* [AWS Service Provider for Laravel][mod-laravel]\n* [AWS SDK ZF2 Module][mod-zf2]\n* [AWS Service Provider for Silex][mod-silex]\n* [AWS SDK Bundle for Symfony][mod-symfony]\n* [Amazon SNS Message Validator for PHP][sns-validator] - SNS validator without requiring SDK\n* [Guzzle Version 7][guzzle-docs] \u2013 PHP HTTP client and framework\n* For Version 2 of the SDK (deprecated):\n  * [User Guide][docs-guide-v2]\n  * [API Docs][docs-api-v2]\n* [Serverless LAMP stack guide][serverless-LAMP-stack-guide] - A guide to building and deploying a serverless PHP application\n* Other [AWS SDKs & Tools][aws-tools] (e.g., js, cli, ruby, python, java, etc.)\n\n[sdk-website]: http://aws.amazon.com/sdkforphp\n[sdk-forum]: https://forums.aws.amazon.com/forum.jspa?forumID=80\n[sdk-issues]: https://github.com/aws/aws-sdk-php/issues\n[sdk-license]: http://aws.amazon.com/apache2.0/\n[sdk-blog]: https://aws.amazon.com/blogs/developer/category/php/\n[sdk-sample]: http://aws.amazon.com/developers/getting-started/php\n\n[install-packagist]: https://packagist.org/packages/aws/aws-sdk-php\n[latest-release]: https://github.com/aws/aws-sdk-php/releases\n\n[docs-api]: http://docs.aws.amazon.com/aws-sdk-php/v3/api/index.html\n[docs-guide]: http://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/welcome.html\n[docs-api-v2]: http://docs.aws.amazon.com/aws-sdk-php/v2/api/index.html\n[docs-guide-v2]: http://docs.aws.amazon.com/aws-sdk-php/v2/guide/index.html\n[docs-contribution]: https://github.com/aws/aws-sdk-php/blob/master/CONTRIBUTING.md\n[docs-migration]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_migration.html\n[docs-signup]: https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key/\n[docs-requirements]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_requirements.html\n[docs-installation]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_installation.html\n[docs-quickstart]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/welcome.html#getting-started\n[docs-paginators]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_paginators.html\n[docs-waiters]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_waiters.html\n[docs-results]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_basic-usage.html#result-objects\n[docs-exceptions]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_basic-usage.html#handling-errors\n[docs-wire-logging]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/faq.html#how-can-i-see-what-data-is-sent-over-the-wire\n[docs-ddbsh]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/service_dynamodb-session-handler.html\n[docs-services]: https://aws.amazon.com/products/\n[docs-rande]: http://docs.aws.amazon.com/general/latest/gr/rande.html\n[docs-streamwrapper]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/s3-stream-wrapper.html\n[docs-s3-transfer]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/s3-transfer.html\n[docs-s3-multipart]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/s3-multipart-upload.html\n[docs-s3-encryption]: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/s3-encryption-client.html\n[docs-script-composer]: https://github.com/aws/aws-sdk-php/tree/master/src/Script/Composer\n\n[aws]: http://aws.amazon.com\n[aws-iam-credentials]: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/UsingIAM.html#UsingIAMrolesWithAmazonEC2Instances\n[aws-tools]: http://aws.amazon.com/tools\n[guzzle-docs]: http://guzzlephp.org\n[composer]: http://getcomposer.org\n[packagist]: http://packagist.org\n[psr-7]: https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-7-http-message.md\n[psr-4]: https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-4-autoloader.md\n[psr-1]: https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-1-basic-coding-standard.md\n[psr-2]: https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-2-coding-style-guide.md\n\n[mod-laravel]: https://github.com/aws/aws-sdk-php-laravel\n[mod-zf2]: https://github.com/aws/aws-sdk-php-zf2\n[mod-silex]: https://github.com/aws/aws-sdk-php-silex\n[mod-symfony]: https://github.com/aws/aws-sdk-php-symfony\n[sns-validator]: https://github.com/aws/aws-php-sns-message-validator\n[serverless-LAMP-stack-guide]: https://github.com/aws-samples/php-examples-for-aws-lambda\n", "release_dates": ["2024-03-01T19:11:27Z", "2024-02-29T19:14:13Z", "2024-02-28T19:12:38Z", "2024-02-27T19:12:09Z", "2024-02-26T19:17:09Z", "2024-02-23T19:17:42Z", "2024-02-22T19:26:57Z", "2024-02-21T19:14:36Z", "2024-02-20T19:11:48Z", "2024-02-19T19:14:57Z", "2024-02-16T19:15:11Z", "2024-02-15T19:15:22Z", "2024-02-14T19:14:33Z", "2024-02-13T19:15:08Z", "2024-02-12T19:14:13Z", "2024-02-09T19:13:42Z", "2024-02-08T19:11:17Z", "2024-02-07T19:11:30Z", "2024-02-06T19:16:26Z", "2024-02-05T19:12:06Z", "2024-02-02T19:12:29Z", "2024-02-01T19:14:50Z", "2024-01-31T19:12:29Z", "2024-01-30T19:12:38Z", "2024-01-29T19:19:42Z", "2024-01-26T19:16:02Z", "2024-01-25T19:30:05Z", "2024-01-24T19:16:25Z", "2024-01-23T20:46:21Z", "2024-01-22T19:19:59Z"]}, {"name": "aws-sdk-php-laravel", "description": "A Laravel 5+ (and 4) service provider for the AWS SDK for PHP", "language": "PHP", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Service Provider for Laravel 5/6/7/8/9/10\n\n[![Latest Stable Version](https://img.shields.io/packagist/v/aws/aws-sdk-php-laravel.svg)](https://packagist.org/packages/aws/aws-sdk-php-laravel)\n[![Total Downloads](https://img.shields.io/packagist/dt/aws/aws-sdk-php-laravel.svg)](https://packagist.org/packages/aws/aws-sdk-php-laravel)\n[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/aws/aws-sdk-php?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\nThis is a simple [Laravel](http://laravel.com/) service provider for making it easy to include the official\n[AWS SDK for PHP](https://github.com/aws/aws-sdk-php) in your Laravel and Lumen applications.\n\nThis README is for version 3.x of the service provider, which is implemented to work with Version 3 of the AWS SDK for\nPHP and Laravel 5.1.\n\n**Major Versions:**\n\n* **3.x** (YOU ARE HERE) - For `laravel/framework:~5.1|~6.0|~7.0|~8.0|9.0|10.0` and `aws/aws-sdk-php:~3.0`\n* **2.x** ([2.0 branch](https://github.com/aws/aws-sdk-php-laravel/tree/2.0)) - For `laravel/framework:5.0.*` and `aws/aws-sdk-php:~2.4`\n* **1.x** ([1.0 branch](https://github.com/aws/aws-sdk-php-laravel/tree/1.0)) - For `laravel/framework:4.*` and `aws/aws-sdk-php:~2.4`\n\nJump To:\n* [Getting Started](#Getting-Started)\n* [Getting Help](#Getting-Help)\n* [Contributing](#Contributing)\n* [More Resources](#Resources)\n\n## Getting Started\n\n### Installation\nThe AWS Service Provider can be installed via [Composer](http://getcomposer.org) by requiring the\n`aws/aws-sdk-php-laravel` package in your project's `composer.json`.\n\n```json\n{\n    \"require\": {\n        \"aws/aws-sdk-php-laravel\": \"~3.0\"\n    }\n}\n```\n\nThen run a composer update\n```sh\nphp composer.phar update\n```\n\nTo use the AWS Service Provider, you must register the provider when bootstrapping your application.\n\n\n#### Lumen\nIn Lumen find the `Register Service Providers` in your `bootstrap/app.php` and register the AWS Service Provider.\n\n```php\n    $app->register(Aws\\Laravel\\AwsServiceProvider::class);\n```\n\n#### Laravel\nIn Laravel find the `providers` key in your `config/app.php` and register the AWS Service Provider.\n\n```php\n    'providers' => array(\n        // ...\n        Aws\\Laravel\\AwsServiceProvider::class,\n    )\n```\n\nFind the `aliases` key in your `config/app.php` and add the AWS facade alias.\n\n```php\n    'aliases' => array(\n        // ...\n        'AWS' => Aws\\Laravel\\AwsFacade::class,\n    )\n```\n\n### Configuration\n\nBy default, the package uses the following environment variables to auto-configure the plugin without modification:\n```\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\nAWS_REGION (default = us-east-1)\n```\n\nTo customize the configuration file, publish the package configuration using Artisan.\n\n```sh\nphp artisan vendor:publish  --provider=\"Aws\\Laravel\\AwsServiceProvider\"\n```\n\nThe settings can be found in the generated `config/aws.php` configuration file. By default, the credentials and region settings will pull from your `.env` file.\n\n```php\nreturn [\n    'credentials' => [\n        'key'    => env('AWS_ACCESS_KEY_ID', ''),\n        'secret' => env('AWS_SECRET_ACCESS_KEY', ''),\n    ],\n    'region' => env('AWS_REGION', 'us-east-1'),\n    'version' => 'latest',\n    // You can override settings for specific services\n    'Ses' => [\n        'region' => 'us-east-1',\n    ],\n];\n```\n\nNote that you can always delete the `credentials` line from this file if you'd like to use the [default SDK Configuration Provider chain](https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_credentials.html#default-credential-chain) instead.\n\nReferring Laravel 5.2.0 [Upgrade guide](https://laravel.com/docs/5.2/upgrade#upgrade-5.2.0), you must using config\nfile instead of environment variable option if using php artisan `config:cache`.\n\nLearn more about [configuring the SDK](http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/configuration.html) on\nthe SDK's User Guide.\n\n### Usage\n\nIn order to use the AWS SDK for PHP within your app, you need to retrieve it from the [Laravel Service\nContainer](https://laravel.com/docs/container#binding). The following example uses the Amazon S3 client to upload a file.\n\n```php\n$s3 = App::make('aws')->createClient('s3');\n$s3->putObject(array(\n    'Bucket'     => 'YOUR_BUCKET',\n    'Key'        => 'YOUR_OBJECT_KEY',\n    'SourceFile' => '/the/path/to/the/file/you/are/uploading.ext',\n));\n```\n\nIf the AWS facade is registered within the `aliases` section of the application configuration, you can also use the\nfollowing technique.\n\n```php\n$s3 = AWS::createClient('s3');\n$s3->putObject(array(\n    'Bucket'     => 'YOUR_BUCKET',\n    'Key'        => 'YOUR_OBJECT_KEY',\n    'SourceFile' => '/the/path/to/the/file/you/are/uploading.ext',\n));\n```\n\nTo use in Lumen, you need to retrieve it from the service container a bit differently:\n\n```php\n$s3 = app('aws')->createClient('s3');\n$s3->putObject(array(\n    'Bucket'     => 'YOUR_BUCKET',\n    'Key'        => 'YOUR_OBJECT_KEY',\n    'SourceFile' => '/the/path/to/the/file/you/are/uploading.ext',\n));\n```\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues for tracking bugs and feature requests and have limited bandwidth to address them.\n\n* Ask a question on [StackOverflow](https://stackoverflow.com/) and tag it with [`aws-php-sdk`](http://stackoverflow.com/questions/tagged/aws-php-sdk)\n* Come join the AWS SDK for PHP [gitter](https://gitter.im/aws/aws-sdk-php)\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home/)\n* If it turns out that you may have found a bug, please [open an issue](https://github.com/aws/aws-sdk-php-laravel/issues/new/choose)\n\nThis SDK implements AWS service APIs. For general issues regarding the AWS services and their limitations, you may also take a look at the [Amazon Web Services Discussion Forums](https://forums.aws.amazon.com/).\n\n### Opening Issues\n\nIf you encounter a bug with `aws-sdk-php-laravel` we would like to hear about it. Search the existing issues and try to make sure your problem doesn\u2019t already exist before opening a new issue. It\u2019s helpful if you include the version of `aws-sdk-php-laravel`, PHP version and OS you\u2019re using. Please include a stack trace and reduced repro case when appropriate, too.\n\nThe GitHub issues are intended for bug reports and feature requests. For help and questions with using `aws-sdk-php` please make use of the resources listed in the Getting Help section. There are limited resources available for handling issues and by keeping the list of open issues lean we can respond in a timely manner.\n\n## Contributing\n\nWe work hard to provide a high-quality and useful SDK for our AWS services, and we greatly value feedback and contributions from our community. Please review our [contributing guidelines](./CONTRIBUTING.md) before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.\n\n## Resources\n\n* [AWS SDK for PHP on Github](http://github.com/aws/aws-sdk-php/)\n* [AWS SDK for PHP website](http://aws.amazon.com/sdkforphp/)\n* [AWS on Packagist](https://packagist.org/packages/aws/)\n* [License](http://aws.amazon.com/apache2.0/)\n* [Laravel website](http://laravel.com/)\n", "release_dates": ["2023-03-24T20:28:08Z", "2023-02-16T17:01:23Z", "2022-03-08T22:22:45Z", "2020-09-14T17:35:22Z", "2020-03-11T19:36:45Z", "2019-09-09T21:02:17Z", "2019-03-15T18:00:25Z", "2018-12-03T22:38:39Z", "2018-11-12T21:57:42Z", "2018-10-19T23:49:42Z", "2018-09-28T18:43:39Z", "2016-01-18T06:58:08Z", "2015-09-30T17:59:11Z", "2015-06-11T00:15:49Z", "2015-06-10T19:48:06Z", "2015-03-12T17:21:05Z", "2015-02-19T01:12:23Z", "2015-02-18T23:04:06Z", "2014-05-13T00:38:59Z", "2013-09-04T18:28:13Z", "2013-09-04T18:23:26Z", "2013-09-04T18:20:04Z", "2013-09-04T18:18:06Z", "2013-09-04T18:17:19Z", "2013-09-04T18:12:22Z"]}, {"name": "aws-sdk-php-silex", "description": "Simple Silex service provider for including the AWS SDK for PHP", "language": "PHP", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Service Provider for Silex\n\n[![@awsforphp on Twitter](http://img.shields.io/badge/twitter-%40awsforphp-blue.svg?style=flat)](https://twitter.com/awsforphp)\n[![Build Status](https://travis-ci.org/aws/aws-sdk-php-silex.svg)](https://travis-ci.org/aws/aws-sdk-php-silex)\n[![Latest Stable Version](https://poser.pugx.org/aws/aws-sdk-php-silex/v/stable.png)](https://packagist.org/packages/aws/aws-sdk-php-silex)\n[![Total Downloads](https://poser.pugx.org/aws/aws-sdk-php-silex/downloads.png)](https://packagist.org/packages/aws/aws-sdk-php-silex)\n\nA simple Silex 2 / Pimple 3 service provider for including the [AWS SDK for PHP](https://github.com/aws/aws-sdk-php).\n\nnote:\n    If you are using the 1.x Silex version, Use [version 2.x]\n    (https://github.com/aws/aws-sdk-php-silex/tree/2.0) of this provider.\n\nJump To:\n* [Getting Started](_#Getting-Started_)\n* [Getting Help](_#Getting-Help_)\n* [Contributing](_#Contributing_)\n* [More Resources](_#Resources_) \n\n## Getting Started \n\n### Installation\n\nThe AWS Service Provider can be installed via [Composer](http://getcomposer.org) by requiring the\n`aws/aws-sdk-php-silex` package in your project's `composer.json`.\n\n```json\n{\n    \"require\": {\n        \"aws/aws-sdk-php-silex\": \"~3.0\"\n    }\n}\n```\n\n### Usage\n\nRegister the AWS Service Provider in your Silex application and provide your AWS SDK for PHP configuration to the app\nin the `aws.config` key. `$app['aws.config']` should contain an array of configuration options or the path to a\nconfiguration file. This value is passed directly into `new Aws\\Sdk`.\n\n```php\n<?php\n\nrequire __DIR__ . '/vendor/autoload.php';\n\nuse Aws\\Silex\\AwsServiceProvider;\nuse Silex\\Application;\n\n$app = new Application();\n\n$app->register(new AwsServiceProvider(), array(\n    'aws.config' => array(\n        'version' => 'latest',\n        'region' => 'us-east-1',\n    )\n));\n// Note: You can also specify a path to a config file\n// (e.g., 'aws.config' => '/path/to/aws/config/file.php')\n\n$app->match('/', function () use ($app) {\n    // Get the Amazon S3 client\n    $s3 = $app['aws']->createS3();\n\n    // Create a list of the buckets in your account\n    $output = \"<ul>\\n\";\n    foreach ($s3->getListBucketsIterator() as $bucket) {\n        $output .= \"<li>{$bucket['Name']}</li>\\n\";\n    }\n    $output .= \"</ul>\\n\";\n\n    return $output;\n});\n\n$app->run();\n```\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues for tracking bugs and feature requests and have limited bandwidth to address them.\n\n* Ask a question on [StackOverflow](https://stackoverflow.com/) and tag it with [`aws-php-sdk`](http://stackoverflow.com/questions/tagged/aws-php-sdk)\n* Come join the AWS SDK for PHP [gitter](https://gitter.im/aws/aws-sdk-php)\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home/)\n* If it turns out that you may have found a bug, please [open an issue](https://github.com/aws/aws-sdk-php-silex/issues/new/choose)\n\nThis SDK implements AWS service APIs. For general issues regarding the AWS services and their limitations, you may also take a look at the [Amazon Web Services Discussion Forums](https://forums.aws.amazon.com/).\n\n### Opening Issues\n\nIf you encounter a bug with `aws-sdk-php-silex` we would like to hear about it. Search the existing issues and try to make sure your problem doesn\u2019t already exist before opening a new issue. It\u2019s helpful if you include the version of `aws-sdk-php-silex`, PHP version and OS you\u2019re using. Please include a stack trace and reduced repro case when appropriate, too.\n\nThe GitHub issues are intended for bug reports and feature requests. For help and questions with using `aws-sdk-php` please make use of the resources listed in the Getting Help section. There are limited resources available for handling issues and by keeping the list of open issues lean we can respond in a timely manner.\n\n## Contributing\n\nWe work hard to provide a high-quality and useful SDK for our AWS services, and we greatly value feedback and contributions from our community. Please review our [contributing guidelines](./CONTRIBUTING.md) before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.\n\n## Resources\n\n* [AWS SDK for PHP on Github](http://github.com/aws/aws-sdk-php)\n* [AWS SDK for PHP website](http://aws.amazon.com/sdkforphp/)\n* [AWS on Packagist](https://packagist.org/packages/aws)\n* [License](http://aws.amazon.com/apache2.0/)\n* [Silex website](http://silex.sensiolabs.org)\n", "release_dates": ["2016-08-19T18:01:08Z", "2016-05-26T16:44:48Z", "2015-06-22T19:15:40Z", "2013-11-18T07:22:28Z"]}, {"name": "aws-sdk-php-symfony", "description": null, "language": "PHP", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Service Provider for Symfony\n\n[![Latest Stable Version](https://img.shields.io/packagist/v/aws/aws-sdk-php-symfony.svg)](https://packagist.org/packages/aws/aws-sdk-php-symfony)\n[![Total Downloads](https://img.shields.io/packagist/dt/aws/aws-sdk-php-symfony.svg)](https://packagist.org/packages/aws/aws-sdk-php-symfony)\n\nA Symfony bundle for including the [AWS SDK for PHP](https://github.com/aws/aws-sdk-php).\n\nJump To:\n* [Getting Started](#getting-started)\n* [Getting Help](#getting-help)\n* [Contributing](#contributing)\n* [More Resources](#resources)\n\n## Getting Started\n\n### Installation\n\nThe AWS bundle can be installed via [Composer](http://getcomposer.org) by\nrequiring the`aws/aws-sdk-php-symfony` package in your project's `composer.json`:\n\n```json\n{\n    \"require\": {\n        \"aws/aws-sdk-php-symfony\": \"~2.0\"\n    }\n}\n```\n\nand adding an instance of `Aws\\Symfony\\AwsBundle` to your application's kernel:\n\n```php\nclass AppKernel extends Kernel\n{\n    public function registerBundles()\n    {\n        return [\n            ...\n            new \\Aws\\Symfony\\AwsBundle(),\n        ];\n    }\n    ...\n}\n```\n\n### Configuration\n\nBy default, configuration is handled by the SDK rather than by the bundle, and\nno validation is performed at compile time. Full documentation of the\nconfiguration options available can be read in the [SDK Guide](http://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/configuration.html).\n\nIf AWS_MERGE_CONFIG environment variable is set to `true`, configuration\nvalidation and merging are enabled. The bundle validates and merges known\nconfiguration options, including for each service.  Additional configuration\noptions can be included in a single configuration file, but merging will fail\nif non-standard options are specified in more than once.\n\nTo use a service for any configuration value, use `@` followed by the service\nname, such as `@a_service`. This syntax will be converted to a service during\ncontainer compilation. If you want to use a string literal that begins with `@`,\nyou will need to escape it by adding another `@` sign.\n\nWhen using the SDK from an EC2 instance, you can write `credentials: ~` to use\n[instance profile credentials](https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_credentials.html#instance-profile-credentials).\nThis syntax means that temporary credentials will be automatically retrieved\nfrom the EC2 instance's metadata server. It's also the preferred technique for\nproviding credentials to applications running on that specific context.\n\nSample configuration can be found in the `tests/fixtures` folder for [YAML](https://github.com/aws/aws-sdk-php-symfony/blob/master/tests/fixtures/config.yml), [PHP](https://github.com/aws/aws-sdk-php-symfony/blob/master/tests/fixtures/config.php), and [XML](https://github.com/aws/aws-sdk-php-symfony/blob/master/tests/fixtures/config.xml).\n\n#### Sample YML Configuration\n\nThe sample configuration which can be placed in `app/config/config.yml` file.\n\n```yaml\nframework:\n    secret: \"Rosebud was the name of his sled.\"\n\naws:\n    version: latest\n    region: us-east-1\n    credentials:\n        key: not-a-real-key\n        secret: \"@@not-a-real-secret\" # this will be escaped as '@not-a-real-secret'\n    DynamoDb:\n        region: us-west-2\n    S3:\n        version: '2006-03-01'\n    Sqs:\n        credentials: \"@a_service\"\n    CloudSearchDomain:\n        endpoint: https://search-with-some-subdomain.us-east-1.cloudsearch.amazonaws.com\n\nservices:\n    a_service:\n        class: Aws\\Credentials\\Credentials\n        arguments:\n            - a-different-fake-key\n            - a-different-fake-secret\n```\n\n### Usage\n\nThis bundle exposes an instance of the `Aws\\Sdk` object as well as instances of\neach AWS client object as services to your symfony application. They are name\n`aws.{$namespace}`, where `$namespace` is the namespace of the service client.\nFor instance:\n\nService | Instance Of\n--- | ---\naws.dynamodb | Aws\\DynamoDb\\DynamoDbClient\naws.ec2 | Aws\\Ec2\\Ec2Client\naws.s3 | Aws\\S3\\S3Client\naws_sdk | Aws\\Sdk\n\nThe services made available depends on which version of the SDK is installed. To\nview a full list, run the following command from your application's root\ndirectory:\n```\nphp bin/console debug:container aws\n```\n\nFull documentation on each of the services listed can be found in the [SDK API\ndocs](http://docs.aws.amazon.com/aws-sdk-php/v3/api/).\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues for tracking bugs and feature requests and have limited bandwidth to address them.\n\n* Ask a question on [StackOverflow](https://stackoverflow.com/) and tag it with [`aws-php-sdk`](http://stackoverflow.com/questions/tagged/aws-php-sdk)\n* Come join the AWS SDK for PHP [gitter](https://gitter.im/aws/aws-sdk-php)\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home/)\n* If it turns out that you may have found a bug, please [open an issue](https://github.com/aws/aws-sdk-php-symfony/issues/new/choose)\n\nThis SDK implements AWS service APIs. For general issues regarding the AWS services and their limitations, you may also take a look at the [Amazon Web Services Discussion Forums](https://forums.aws.amazon.com/).\n\n### Opening Issues\n\nIf you encounter a bug with `aws-sdk-php-symfony` we would like to hear about it. Search the existing issues and try to make sure your problem doesn\u2019t already exist before opening a new issue. It\u2019s helpful if you include the version of `aws-sdk-php-symfony`, PHP version and OS you\u2019re using. Please include a stack trace and reduced repro case when appropriate, too.\n\nThe GitHub issues are intended for bug reports and feature requests. For help and questions with using `aws-sdk-php` please make use of the resources listed in the Getting Help section. There are limited resources available for handling issues and by keeping the list of open issues lean we can respond in a timely manner.\n\n## Contributing\n\nWe work hard to provide a high-quality and useful SDK for our AWS services, and we greatly value feedback and contributions from our community. Please review our [contributing guidelines](./CONTRIBUTING.md) before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.\n\n## Resources\n\n* [AWS SDK for PHP on Github](http://github.com/aws/aws-sdk-php)\n* [AWS SDK for PHP website](http://aws.amazon.com/sdkforphp/)\n* [AWS on Packagist](https://packagist.org/packages/aws)\n* [License](http://aws.amazon.com/apache2.0/)\n* [Symfony website](http://symfony.com/)\n", "release_dates": ["2023-03-30T17:32:19Z", "2023-03-23T19:51:26Z", "2022-05-26T16:06:33Z", "2022-04-13T17:36:52Z", "2022-03-29T15:37:53Z", "2020-04-08T21:43:35Z", "2020-03-05T21:49:54Z", "2020-03-05T19:33:48Z", "2019-11-25T18:07:41Z", "2019-11-07T19:12:47Z", "2019-02-27T00:51:18Z", "2018-01-19T00:02:13Z", "2017-07-12T21:03:40Z", "2016-09-19T16:53:53Z", "2015-12-10T18:41:49Z", "2015-09-06T06:08:19Z", "2015-08-06T00:01:05Z", "2015-07-09T23:58:14Z"]}, {"name": "aws-sdk-php-v3-bridge", "description": "A compatibility pack for services no longer supported in V3 of the AWS SDK for PHP", "language": "PHP", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for PHP - Version 3 Upgrade Bridge\n\n[![@awsforphp on Twitter](http://img.shields.io/badge/twitter-%40awsforphp-blue.svg?style=flat)](https://twitter.com/awsforphp)\n[![Build Status](https://travis-ci.org/aws/aws-sdk-php-v3-bridge.svg)](https://travis-ci.org/aws/aws-sdk-php-v3-bridge)\n[![Apache 2 License](https://img.shields.io/packagist/l/aws/aws-sdk-php-v3-bridge.svg?style=flat)](http://aws.amazon.com/apache-2-0/)\n[![Gitter](https://badges.gitter.im/Join Chat.svg)](https://gitter.im/aws/aws-sdk-php?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\nThis package provides support for using Amazon SimpleDB with version 3 of the\nAWS SDK for PHP. It depends on the AWS SDK for PHP v3 and allows users to\nconstruct SimpleDb clients as they would any other v3 service:\n```php\n<?php\n\n$sdb = new \\Aws\\SimpleDb\\SimpleDbClient([\n    'region' => 'us-east-1',\n    'version' => 'latest',\n]);\n\n$domains = $sdb->listDomains();\n```\n\nJump To:\n* [Getting Started](_#Getting-Started_)\n* [Getting Help](_#Getting-Help_)\n* [Contributing](_#Contributing_)\n* [More Resources](_#Resources_)\n\n## Getting Started\n\nPlease note that you cannot use the `Aws\\Sdk` service locator with SimpleDb.\nYou must create clients using the `new` keyword.\n\n### Installation\n\nThis package can be installed via [Composer](http://getcomposer.org) by requiring the\n`aws/aws-sdk-php-v3-bridge` package in your project's `composer.json`.\n\n```json\n{\n    \"require\": {\n        \"aws/aws-sdk-php-v3-bridge\": \"^0.2.0\"\n    }\n}\n```\n\nThen run a composer update\n```sh\nphp composer.phar update\n```\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues for tracking bugs and feature requests and have limited bandwidth to address them.\n\n* Ask a question on [StackOverflow](https://stackoverflow.com/) and tag it with [`aws-php-sdk`](http://stackoverflow.com/questions/tagged/aws-php-sdk)\n* Come join the AWS SDK for PHP [gitter](https://gitter.im/aws/aws-sdk-php)\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home/)\n* If it turns out that you may have found a bug, please [open an issue](https://github.com/aws/aws-sdk-php-v3-bridge/issues/new/choose)\n\nThis SDK implements AWS service APIs. For general issues regarding the AWS services and their limitations, you may also take a look at the [Amazon Web Services Discussion Forums](https://forums.aws.amazon.com/).\n\n### Opening Issues\n\nIf you encounter a bug with `aws-sdk-php-v3-bridge` we would like to hear about it. Search the existing issues and try to make sure your problem doesn\u2019t already exist before opening a new issue. It\u2019s helpful if you include the version of `aws-sdk-php-v3-bridge`, PHP version and OS you\u2019re using. Please include a stack trace and reduced repro case when appropriate, too.\n\nThe GitHub issues are intended for bug reports and feature requests. For help and questions with using `aws-sdk-php` please make use of the resources listed in the Getting Help section. There are limited resources available for handling issues and by keeping the list of open issues lean we can respond in a timely manner.\n\n## Contributing\n\nWe work hard to provide a high-quality and useful SDK for our AWS services, and we greatly value feedback and contributions from our community. Please review our [contributing guidelines](./CONTRIBUTING.md) before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.\n\n## Resources\n\n* [AWS SDK for PHP on Github](http://github.com/aws/aws-sdk-php/)\n* [AWS SDK for PHP website](http://aws.amazon.com/sdkforphp/)\n* [AWS on Packagist](https://packagist.org/packages/aws/)\n* [License](http://aws.amazon.com/apache2.0/)\n", "release_dates": ["2023-04-03T19:29:09Z", "2019-09-13T20:01:22Z", "2016-05-20T19:45:44Z", "2015-10-05T20:16:00Z", "2015-10-02T22:21:56Z"]}, {"name": "aws-sdk-php-zf2", "description": "ZF2 module for using the AWS SDK for PHP to interact with AWS services like S3, DynamoDB, SQS, EC2, etc.", "language": "PHP", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK ZF2 Module\n\n[![Latest Stable Version](https://poser.pugx.org/aws/aws-sdk-php-zf2/v/stable.png)](https://packagist.org/packages/aws/aws-sdk-php-zf2)\n[![Total Downloads](https://poser.pugx.org/aws/aws-sdk-php-zf2/downloads.png)](https://packagist.org/packages/aws/aws-sdk-php-zf2)\n[![Build Status](https://travis-ci.org/aws/aws-sdk-php-zf2.png)](https://travis-ci.org/aws/aws-sdk-php-zf2)\n\nThis module provides a simple wrapper for the AWS SDK for PHP. It registers the AWS service builder as a service in the\nZF2 service manager, making it easily accessible anywhere in your application.\n\nJump To:\n* [Getting Started](_#Getting-Started_)\n* [Getting Help](_#Getting-Help_)\n* [Contributing](_#Contributing_)\n* [Related Modules](_#Related-Modules_)\n* [More Resources](_#Resources_)\n\n## Getting Started\n\n### Installation\n\nInstall the module using Composer into your application's vendor directory. Add the following line to your\n`composer.json`. This will also install the AWS SDK for PHP.\n\nIf you want to use `ZF3` and your PHP version >= 5.6, use\n\n```json\n{\n    \"require\": {\n        \"aws/aws-sdk-php-zf2\": \"4.*\"\n    }\n}\n```\n\nOtherwise,\n\n```json\n{\n    \"require\": {\n        \"aws/aws-sdk-php-zf2\": \"3.*\"\n    }\n}\n```\n> If you are using ZF2 service manager < 2.7, please use the 2.0.* version.\n\n> If you are using AWS SDK v2, please use the 1.2.* version of the ZF2 module.\n\n### Configuration\n\nAdd the module name to your project's `config/application.config.php` or `config/modules.config.php`:\n\n```php\nreturn array(\n    /* ... */\n    'modules' => array(\n        /* ... */\n        'AwsModule'\n    ),\n    /* ... */\n);\n```\n\n\nCopy and paste the `aws.local.php.dist` file to your `config/autoload` folder and customize it with your credentials and\nother configuration settings. Make sure to remove `.dist` from your file. Your `aws.local.php` might look something like\nthe following:\n\n```php\n<?php\n\nreturn [\n    'aws' => [\n        'credentials' => [\n            'key'    => '<your-aws-access-key-id>',\n            'secret' => '<your-aws-secret-access-key>',\n        ]\n        'region' => 'us-west-2'\n    ]\n];\n```\n\n> NOTE: If you are using [IAM Instance Profile\ncredentials](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/UsingIAM.html#UsingIAMrolesWithAmazonEC2Instances)\n(also referred to as IAM Roles for instances), you can omit your `key` and `secret` parameters since they will be\nfetched from the Amazon EC2 instance automatically.\n\n### Usage\n\nYou can get the AWS service builder object from anywhere that the ZF2 service locator is available (e.g. controller\nclasses). The following example instantiates an Amazon DynamoDB client and creates a table in DynamoDB.\n\n```php\nuse Aws\\Sdk;\n\npublic function indexAction()\n{\n    $aws    = $this->getServiceLocator()->get(Sdk::class);\n    $client = $aws->createDynamoDb();\n\n    $table = 'posts';\n\n    // Create a \"posts\" table\n    $result = $client->createTable(array(\n        'TableName' => $table,\n        'KeySchema' => array(\n            'HashKeyElement' => array(\n                'AttributeName' => 'slug',\n                'AttributeType' => 'S'\n            )\n        ),\n        'ProvisionedThroughput' => array(\n            'ReadCapacityUnits'  => 10,\n            'WriteCapacityUnits' => 5\n        )\n    ));\n\n    // Wait until the table is created and active\n    $client->waitUntilTableExists(array('TableName' => $table));\n\n    echo \"The {$table} table has been created.\\n\";\n}\n```\n\n### View Helpers\n\nThe AWS SDK ZF2 Module now provides two view helpers to generate links for Amazon S3 and Amazon CloudFront resources.\n\n> **Note:** Starting from v2 of the AWS module, all URLs for both S3 and CloudFront are using HTTPS and this cannot\nbe modified.\n\n#### S3Link View Helper\n\nTo create a S3 link in your view:\n\n```php\n<?php echo $this->s3Link('my-object', 'my-bucket');\n```\n\nThe default bucket can be set globally by using the `setDefaultBucket` method:\n\n```php\n<?php\n    $this->plugin('s3Link')->setDefaultBucket('my-bucket');\n    echo $this->s3Link('my-object');\n```\n\nYou can also create signed URLs for private content by passing a third argument which is the expiration date:\n\n```php\n<?php echo $this->s3Link('my-object', 'my-bucket', '+10 minutes');\n```\n\n#### CloudFrontLink View Helper\n\nTo create CloudFront link in your view:\n\n```php\n<?php echo $this->cloudFrontLink('my-object', 'my-domain');\n```\n\nThe default domain can be set globally by using the `setDefaultDomain` method:\n\n```php\n<?php\n    $this->plugin('cloudFrontLink')->setDefaultDomain('my-domain');\n    echo $this->cloudFrontLink('my-object');\n```\n\nYou can also create signed URLs for private content by passing a third argument which is the expiration date:\n\n```php\n<?php echo $this->cloudFrontLink('my-object', 'my-bucket', time() + 60);\n```\n\n### Filters\n\nThe AWS SDK ZF2 module provides a simple file filter that allow to directly upload to S3.\nThe `S3RenameUpload` extends `RenameUpload` class, so please refer to [its\ndocumentation](http://framework.zend.com/manual/2.2/en/modules/zend.filter.file.rename-upload.html#zend-filter-file-rename-upload)\nfor available options.\n\nThis filter only adds one option to set the bucket name (through the `setBucket` method, or by passing a `bucket` key\nto the filter's `setOptions` method).\n\n```php\n$request = new Request();\n$files   = $request->getFiles();\n// e.g., $files['my-upload']['tmp_name'] === '/tmp/php5Wx0aJ'\n// e.g., $files['my-upload']['name'] === 'profile-picture.jpg'\n\n// Fetch the filter from the Filter Plugin Manager to automatically handle dependencies\n$filter = $serviceLocator->get('FilterManager')->get('S3RenameUpload');\n\n$filter->setOptions(\u2019[\n    'bucket'    => 'my-bucket',\n    'target'    => 'users/5/profile-picture.jpg',\n    'overwrite' => true\n]);\n\n$filter->filter($files['my-upload']);\n\n// File has been renamed and moved to 'my-bucket' bucket, inside the 'users/5' path\n```\n\n### Session Save Handlers\n\nRead the [session save handler section]\n(http://zf2.readthedocs.org/en/latest/modules/zend.session.save-handler.html) in\nthe ZF2 documentation for more information.\n\n#### DynamoDB\n\nTo follow the [ZF2 examples]\n(http://zf2.readthedocs.org/en/latest/modules/zend.session.save-handler.html),\nthe DynamoDB session save handler might be used like this:\n\n```php\nuse AwsModule\\Session\\SaveHandler\\DynamoDb as DynamoDbSaveHandler;\nuse Laminas\\Session\\SessionManager;\n\n// Assume we are in a context where $serviceLocator is a ZF2 service locator.\n\n$saveHandler = $serviceLocator->get(DynamoDbSaveHandler::class);\n\n$manager = new SessionManager();\n$manager->setSaveHandler($saveHandler);\n```\n\nYou will probably want to further configure the save handler, which you can do in your application. You can copy the\n`config/aws_zf2.local.php.dist` file into your project's `config/autoload` directory (without the `.dist` of course).\n\nSee `config/aws_zf2.local.php.dist` and [the AWS session handler documentation]\n(http://docs.aws.amazon.com/aws-sdk-php-2/latest/class-Aws.DynamoDb.Session.SessionHandler.html#_factory) for more\ndetailed configuration information.\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues for tracking bugs and feature requests and have limited bandwidth to address them.\n\n* Ask a question on [StackOverflow](https://stackoverflow.com/) and tag it with [`aws-php-sdk`](http://stackoverflow.com/questions/tagged/aws-php-sdk)\n* Come join the AWS SDK for PHP [gitter](https://gitter.im/aws/aws-sdk-php)\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home/)\n* If it turns out that you may have found a bug, please [open an issue](https://github.com/aws/aws-sdk-php-zf2/issues/new/choose)\n\nThis SDK implements AWS service APIs. For general issues regarding the AWS services and their limitations, you may also take a look at the [Amazon Web Services Discussion Forums](https://forums.aws.amazon.com/).\n\n### Opening Issues\n\nIf you encounter a bug with `aws-sdk-php-zf2` we would like to hear about it. Search the existing issues and try to make sure your problem doesn\u2019t already exist before opening a new issue. It\u2019s helpful if you include the version of `aws-sdk-php-zf2`, PHP version and OS you\u2019re using. Please include a stack trace and reduced repro case when appropriate, too.\n\nThe GitHub issues are intended for bug reports and feature requests. For help and questions with using `aws-sdk-php` please make use of the resources listed in the Getting Help section. There are limited resources available for handling issues and by keeping the list of open issues lean we can respond in a timely manner.\n\n## Contributing\n\nWe work hard to provide a high-quality and useful SDK for our AWS services, and we greatly value feedback and contributions from our community. Please review our [contributing guidelines](./CONTRIBUTING.md) before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.\n\n## Related Modules\n\nThe following are some ZF2 modules that use the AWS SDK for PHP by including this module:\n\n* [SlmMail](https://github.com/juriansluiman/SlmMail) - Module that allow to send emails with various providers\n  (including Amazon SES)\n* [SlmQueueSqs](https://github.com/juriansluiman/SlmQueueSqs) \u2013 Module that simplifies the use of Amazon SQS\n\n## Resources\n\n* [AWS SDK for PHP on Github](http://github.com/aws/aws-sdk-php)\n* [AWS SDK for PHP website](http://aws.amazon.com/sdkforphp/)\n* [AWS on Packagist](https://packagist.org/packages/aws)\n* [License](http://aws.amazon.com/apache2.0/)\n* [Laminas (ZF2) website](https://getlaminas.org/)\n", "release_dates": ["2022-04-29T21:23:39Z", "2020-10-29T17:47:47Z", "2019-08-05T20:45:16Z", "2018-02-13T17:23:27Z", "2016-12-14T00:05:51Z", "2016-07-22T18:13:18Z", "2016-06-27T21:49:46Z", "2015-06-22T19:10:22Z", "2014-01-09T16:39:17Z", "2013-12-13T21:00:18Z", "2013-08-01T20:37:46Z", "2013-08-01T20:33:35Z", "2013-08-01T20:32:48Z", "2013-08-01T20:32:23Z", "2013-08-01T20:30:29Z"]}, {"name": "aws-sdk-rails", "description": "Official repository for the aws-sdk-rails gem, which integrates the AWS SDK for Ruby with Ruby on Rails.", "language": "Ruby", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# AWS SDK for Ruby Rails Plugin\n\n[![Gem Version](https://badge.fury.io/rb/aws-sdk-rails.svg)](https://badge.fury.io/rb/aws-sdk-rails)\n[![Build Status](https://github.com/aws/aws-sdk-rails/workflows/CI/badge.svg)](https://github.com/aws/aws-sdk-rails/actions)\n [![Github forks](https://img.shields.io/github/forks/aws/aws-sdk-rails.svg)](https://github.com/aws/aws-sdk-rails/network)\n[![Github stars](https://img.shields.io/github/stars/aws/aws-sdk-rails.svg)](https://github.com/aws/aws-sdk-rails/stargazers)\n\nA Ruby on Rails plugin that integrates AWS services with your application using\nthe latest version of [AWS SDK For Ruby](https://github.com/aws/aws-sdk-ruby).\n\n## Installation\n\nAdd this gem to your Rails project's Gemfile:\n\n```ruby\ngem 'aws-sdk-rails'\n```\n\nThis gem also brings in the following AWS gems:\n\n* `aws-sdk-ses`\n* `aws-sdk-sesv2`\n* `aws-sdk-sqs`\n* `aws-record`\n* `aws-sessionstore-dynamodb`\n\nIf you want to use other services (such as S3), you will still need to add them to your\nGemfile:\n\n```ruby\ngem 'aws-sdk-rails', '~> 3'\ngem 'aws-sdk-s3', '~> 1'\n```\n\nYou will have to ensure that you provide credentials for the SDK to use. See the\nlatest [AWS SDK for Ruby Docs](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/index.html#Configuration)\nfor details.\n\nIf you're running your Rails application on Amazon EC2, the AWS SDK will\ncheck Amazon EC2 instance metadata for credentials to load. Learn more:\n[IAM Roles for Amazon EC2](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html)\n\n# Features\n\n## AWS SDK uses the Rails logger\n\nThe AWS SDK is configured to use the built-in Rails logger for any\nSDK log output. The logger is configured to use the `:info` log level. You can\nchange the log level by setting `:log_level` in the\n[Aws.config](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws.html) hash.\n\n```ruby\nAws.config.update(log_level: :debug)\n```\n\n## Rails 5.2+ Encrypted Credentials\n\nIf you are using Rails 5.2+ [Encrypted Credentials](http://guides.rubyonrails.org/security.html#custom-credentials),\nthe credentials will be decrypted and loaded under the `:aws` top level key:\n\n```yml\n# config/credentials.yml.enc\n# viewable with: `rails credentials:edit`\naws:\n  access_key_id: YOUR_KEY_ID\n  secret_access_key: YOUR_ACCESS_KEY\n```\n\nEncrypted Credentials will take precedence over any other AWS Credentials that\nmay exist in your environment (eg: credentials from profiles set in\n `~/.aws/credentials`).\n\nIf you are using [ActiveStorage](https://edgeguides.rubyonrails.org/active_storage_overview.html)\nwith `S3` then you do not need to specify your credentials in your `storage.yml`\nconfiguration: they will be loaded automatically.\n\n## DynamoDB Session Store\n\nYou can configure session storage in Rails to use DynamoDB instead of cookies,\nallowing access to sessions from other applications and devices. You will need\nto have an existing Amazon DynamoDB session table to use this feature.\n\nYou can generate a migration file for the session table using the following\ncommand (<MigrationName> is optional):\n\n```bash\nrails generate dynamo_db:session_store_migration <MigrationName>\n```\n\nThe session store migration generator command will generate two\tfiles: a\nmigration file, `db/migration/#{VERSION}_#{MIGRATION_NAME}.rb`, and a\nconfiguration YAML file, `config/dynamo_db_session_store.yml`.\n\nThe migration file will create and delete a table with default options. These\noptions can be changed prior to running the migration and are documented in the\n[Table](https://docs.aws.amazon.com/sdk-for-ruby/aws-sessionstore-dynamodb/api/Aws/SessionStore/DynamoDB/Table.html) class.\n\nTo create the table, run migrations as normal with:\n\n```bash\nrails db:migrate\n```\n\nNext, configure the Rails session store to be `:dynamodb_store` by editing\n`config/initializers/session_store.rb` to contain the following:\n\n```ruby\n# config/initializers/session_store.rb\nRails.application.config.session_store :dynamodb_store, key: '_your_app_session'\n```\n\nYou can now start your Rails application with session support.\n\n### Configuration\n\nYou can configure the session store with code, YAML files, or ENV, in this order\nof precedence. To configure in code, you can directly pass options to your\ninitializer like so:\n\n```ruby\n# config/initializers/session_store.rb\nRails.application.config.session_store :dynamodb_store,\n  key: '_your_app_session',\n  table_name: 'foo',\n  dynamo_db_client: my_ddb_client\n```\n\nAlternatively, you can use the generated YAML configuration file\n`config/dynamo_db_session_store.yml`. YAML configuration may also be specified\nper environment, with environment configuration having precedence. To do this,\ncreate `config/dynamo_db_session_store/#{Rails.env}.yml` files as needed.\n\nFor configuration options, see the [Configuration](https://docs.aws.amazon.com/sdk-for-ruby/aws-sessionstore-dynamodb/api/Aws/SessionStore/DynamoDB/Configuration.html) class.\n\n#### Rack Configuration\n\nDynamoDB session storage is implemented in the [\\`aws-sessionstore-dynamodb\\`](https://github.com/aws/aws-sessionstore-dynamodb-ruby)\ngem. The Rack middleware inherits from the [\\`Rack::Session::Abstract::Persisted\\`](https://www.rubydoc.info/github/rack/rack/Rack/Session/Abstract/Persisted)\nclass, which also includes additional options (such as `:key`) that can be\npassed into the Rails initializer.\n\n### Cleaning old sessions\n\nBy default sessions do not expire. See `config/dynamo_db_session_store.yml` to\nconfigure the max age or stale period of a session.\n\nYou can use the DynamoDB [Time to Live (TTL) feature](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html)\non the `expire_at` attribute to automatically delete expired items.\n\nAlternatively, a Rake task for garbage collection is provided:\n\n```bash\nrake dynamo_db:collect_garbage\n```\n\n## Amazon Simple Email Service (SES) as an ActionMailer Delivery Method\n\nThis gem will automatically register SES and SESV2 as ActionMailer delivery methods.\nYou simply need to configure Rails to use it in your environment configuration:\n\n```ruby\n# for e.g.: config/environments/production.rb\nconfig.action_mailer.delivery_method = :ses # or :sesv2\n```\n\n### Override credentials or other client options\n\nClient options can be overridden by re-registering the mailer with any set of\nSES or SESV2 Client options. You can create a Rails initializer\n`config/initializers/aws.rb` with contents similar to the following:\n\n```ruby\nrequire 'json'\n\n# Assuming a file \"path/to/aws_secrets.json\" with contents like:\n#\n#     { \"AccessKeyId\": \"YOUR_KEY_ID\", \"SecretAccessKey\": \"YOUR_ACCESS_KEY\" }\n#\n# Remember to exclude \"path/to/aws_secrets.json\" from version control, e.g. by\n# adding it to .gitignore\nsecrets = JSON.load(File.read('path/to/aws_secrets.json'))\ncreds = Aws::Credentials.new(secrets['AccessKeyId'], secrets['SecretAccessKey'])\n\nAws::Rails.add_action_mailer_delivery_method(\n  :ses, # or :sesv2\n  credentials: creds,\n  region: 'us-east-1',\n  # some other config\n)\n```\n\n### Using ARNs with SES\n\nThis gem uses [\\`Aws::SES::Client#send_raw_email\\`](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/SES/Client.html#send_raw_email-instance_method)\nand [\\`Aws::SESV2::Client#send_email\\`](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/SESV2/Client.html#send_email-instance_method)\nto send emails. This operation allows you to specify a cross-account identity\nfor the email's Source, From, and Return-Path. To set these ARNs, use any of the\nfollowing headers on your `Mail::Message` object returned by your Mailer class:\n\n* X-SES-SOURCE-ARN\n\n* X-SES-FROM-ARN\n\n* X-SES-RETURN-PATH-ARN\n\nExample:\n\n```\n# in your Rails controller\nmessage = MyMailer.send_email(options)\nmessage['X-SES-FROM-ARN'] = 'arn:aws:ses:us-west-2:012345678910:identity/bigchungus@memes.com'\nmessage.deliver\n```\n\n## Active Support Notification Instrumentation for AWS SDK calls\nTo add `ActiveSupport::Notifications` Instrumentation to all AWS SDK client\noperations call `Aws::Rails.instrument_sdk_operations` before you construct any\nSDK clients.\n\nExample usage in `config/initializers/instrument_aws_sdk.rb`\n```ruby\nAws::Rails.instrument_sdk_operations\n```\n\nEvents are published for each client operation call with the following event\nname: <operation>.<serviceId>.aws.  For example, S3's put_object has an event\nname of: `put_object.S3.aws`.  The service name will always match the\nnamespace of the service client (eg Aws::S3::Client => 'S3').\nThe payload of the event is the\n[request context](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Seahorse/Client/RequestContext.html).\n\nYou can subscribe to these events as you would other\n `ActiveSupport::Notifications`:\n\n ```ruby\nActiveSupport::Notifications.subscribe('put_object.S3.aws') do |name, start, finish, id, payload|\n  # process event\nend\n\n# Or use a regex to subscribe to all service notifications\nActiveSupport::Notifications.subscribe(/S3[.]aws/) do |name, start, finish, id, payload|\n  # process event\nend\n```\n\n## AWS SQS Active Job\nThis package provides a lightweight, high performance SQS backend\nfor [ActiveJob](https://guides.rubyonrails.org/active_job_basics.html).  \n\nTo use AWS SQS ActiveJob as your queuing backend, simply set the `active_job.queue_adapter`\nto `:amazon` or `:amazon_sqs` (note, `:amazon` has been used for a number of\n other Amazon rails adapters such as ActiveStorage, so has been\n carried forward as convention here).  For details on setting the\n queuing backend see:\n[ActiveJob: Setting the Backend](https://guides.rubyonrails.org/active_job_basics.html#setting-the-backend).\nTo use the non-blocking (async) adapter set `active_job.queue_adapter` to `:amazon_sqs_async`.  If you have\na lot of jobs to queue or you need to avoid the extra latency from an SQS call in your request then consider\nusing the async adapter.  However, you may also want to configure a `async_queue_error_handler` to\nhandle errors that may occur when queuing jobs.  See the\n[Aws::Rails::SqsActiveJob::Configuration](https://docs.aws.amazon.com/sdk-for-ruby/aws-sdk-rails/api/Aws/Rails/SqsActiveJob/Configuration.html)\nfor documentation.\n\n\n```ruby\n# config/application.rb\nmodule YourApp\n  class Application < Rails::Application\n    config.active_job.queue_adapter = :amazon_sqs # note: can use either :amazon or :amazon_sqs\n    # To use the non-blocking async adapter:\n    # config.active_job.queue_adapter = :amazon_sqs_async\n  end\nend\n\n# Or to set the adapter for a single job:\nclass YourJob < ApplicationJob\n  self.queue_adapter = :amazon_sqs\n  #....\nend\n```\n\nYou also need to configure a mapping of ActiveJob queue name to SQS Queue URL. For more details, see the configuration section below.\n\n```ruby\n# config/aws_sqs_active_job.yml\nqueues:\n  default: 'https://my-queue-url.amazon.aws'\n```\n\nTo queue a job, you can just use standard ActiveJob methods:\n```ruby\n# To queue for immediate processing\nYourJob.perform_later(args)\n\n# or to schedule a job for a future time:\nYourJob.set(wait: 1.minute).perform_later(args)\n```\n\nNote: Due to limitations in SQS, you cannot schedule jobs for\nlater than 15 minutes in the future.\n\n### Retry Behavior and Handling Errors\nSee the Rails ActiveJob Guide on \n[Exceptions](https://guides.rubyonrails.org/active_job_basics.html#exceptions)\nfor background on how ActiveJob handles exceptions and retries.\n\nIn general - you should configure retries for your jobs using \n[retry_on](https://edgeapi.rubyonrails.org/classes/ActiveJob/Exceptions/ClassMethods.html#method-i-retry_on).\nWhen configured, ActiveJob will catch the exception and reschedule the job for\nre-execution after the configured delay. This will delete the original\nmessage from the SQS queue and requeue a new message.\n\nBy default SQS ActiveJob is configured with `retry_standard_error` set to `true`\nand will not delete messages for jobs that raise a `StandardError` and that do\nnot handle that error via `retry_on` or `discard_on`.  These job messages\nwill remain on the queue and will be re-read and retried following the \nSQS Queue's configured \n[retry and DLQ settings](https://docs.aws.amazon.com/lambda/latest/operatorguide/sqs-retries.html).\nIf you do not have a DLQ configured, the message will continue to be attempted\nuntil it reaches the queues retention period.  In general, it is a best practice\nto configure a DLQ to store unprocessable jobs for troubleshooting and redrive.\n\nIf you want failed jobs that do not have `retry_on` or `discard_on` configured\nto be immediately discarded and not left on the queue, set `retry_standard_error`\nto `false`.  See the configuration section below for details.\n\n\n### Running workers - polling for jobs\nTo start processing jobs, you need to start a separate process\n(in additional to your Rails app) with `bin/aws_sqs_active_job`\n(an executable script provided with this gem).  You need to specify the queue to\nprocess jobs from:\n```sh\nRAILS_ENV=development bundle exec aws_sqs_active_job --queue default\n```\n\nTo see a complete list of arguments use `--help`.  \n\nYou can kill the process at any time with `CTRL+C` - the processor will attempt\nto shutdown cleanly and will wait up to `:shutdown_timeout` seconds for all\nactively running jobs to finish before killing them.\n\n\nNote: When running in production, its recommended that use a process\nsupervisor such as [foreman](https://github.com/ddollar/foreman), systemd,\nupstart, daemontools, launchd, runit, ect.  \n\n### Performance\nAWS SQS ActiveJob is a lightweight and performant queueing backend.  Benchmark performed using: Ruby MRI 2.6.5,  \nshoryuken 5.0.5, aws-sdk-rails 3.3.1 and aws-sdk-sqs 1.34.0 on a 2015 Macbook Pro dual-core i7 with 16GB ram.\n\n*AWS SQS ActiveJob* (default settings): Throughput 119.1 jobs/sec\n*Shoryuken* (default settings): Throughput 76.8 jobs/sec\n\n### Serverless workers: processing activejobs using AWS Lambda\nRather than managing the worker processes yourself, you can use Lambda with an SQS Trigger.\nWith [Lambda Container Image Support](https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/)\nand the lambda handler provided with `aws-sdk-rails` its easy to use lambda to run ActiveJobs for your dockerized\nrails app (see below for some tips).  All you need to do is:\n1. include the [aws_lambda_ric gem](https://github.com/aws/aws-lambda-ruby-runtime-interface-client)\n2. Push your image to ecr\n3. Create a lambda function from your image (see the lambda docs for details).\n4. Add an SQS Trigger for the queue(s) you want to process jobs from.\n5. Set the ENTRYPOINT to `/usr/local/bundle/bin/aws_lambda_ric` and the CMD\nto `config/environment.Aws::Rails::SqsActiveJob.lambda_job_handler` - this will load Rails and\nthen use the lambda handler provided by `aws-sdk-rails.` You can do this either as function config\nor in your Dockerfile.\n\nThere are a few\n[limitations/requirements](https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#images-reqs)\nfor lambda container images: the default lambda user must be able\nto read all the files and the image must be able to run on a read only file system.\nYou may need to disable bootsnap, set a HOME env variable and\nset the logger to STDOUT (which lambda will record to cloudwatch for you).\n\nYou can use the RAILS_ENV to control environment.  If you need to execute\nspecific configuration in the lambda, you can create a ruby file and use it\nas your entrypoint:\n\n```ruby\n# app.rb\n# some custom config\n\nrequire_relative 'config/environment' # load rails\n\n# Rails.config.custom....\n# Aws::Rails::SqsActiveJob.config....\n\n# no need to write a handler yourself here, as long as\n# aws-sdk-rails is loaded, you can still use the\n# Aws::Rails::SqsActiveJob.lambda_job_handler\n\n# To use this file, set CMD:  app.Aws::Rails::SqsActiveJob.lambda_job_handler\n```\n\n### Elastic Beanstalk workers: processing activejobs using worker environments\n\nAnother option for processing jobs without managing the worker process is hosting the application in a scalable\n[Elastic Beanstalk worker environment](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html).\nThis SDK includes Rack middleware that can be added conditionally and which will process requests from the\nSQS Daemon provided with each worker instance. The middleware will forward each request and parameters to their appropriate jobs.\n\nTo add the middleware on application startup, set the ```AWS_PROCESS_BEANSTALK_WORKER_REQUESTS``` environment variable to true\nin the worker environment configuration.\n\nTo protect against forgeries, daemon requests will only be processed if they originate from localhost or the Docker host.\n\nPeriodic (scheduled) jobs are also supported with this approach without requiring any additional dependencies.\nElastic Beanstalk workers support the addition of a ```cron.yaml``` file in the application root to configure this.\n\nExample:\n```yml\nversion: 1\ncron:\n - name: \"MyApplicationJob\"\n   url: \"/\"\n   schedule: \"0 */12 * * *\"\n```\n\nWhere 'name' must be the case-sensitive class name of the job.\n\n### Configuration\n\nFor a complete list of configuration options see the\n[Aws::Rails::SqsActiveJob::Configuration](https://docs.aws.amazon.com/sdk-for-ruby/aws-sdk-rails/api/Aws/Rails/SqsActiveJob/Configuration.html)\ndocumentation.\n\nYou can configure AWS SQS Active Job either through the yml file or\nthrough code in your config/<env>.rb or initializers.  \n\nFor file based configuration, you can use either:\n1. config/aws_sqs_active_job/<RAILS_ENV>.yml\n2. config/aws_sqs_active_job.yml\n\nThe yml file supports ERB.  \n\nTo configure in code:\n```ruby\nAws::Rails::SqsActiveJob.configure do |config|\n  config.logger = ActiveSupport::Logger.new(STDOUT)\n  config.max_messages = 5\n  config.client = Aws::SQS::Client.new(region: 'us-east-1')\nend\n```\n\n### Using FIFO queues\n\nIf the order in which your jobs executes is important, consider using a\n[FIFO Queue](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html).\nA FIFO queue ensures that messages are processed in the order they were sent\n(First-In-First-Out) and exactly-once processing (ensuring duplicates are never\nintroduced into the queue).  To use a fifo queue, simply set the queue url (which will end in \".fifo\")\nin your config.\n\nWhen using FIFO queues, jobs will NOT be processed concurrently by the poller\nto ensure the correct ordering.  Additionally, all jobs on a FIFO queue will be queued\nsynchronously, even if you have configured the `amazon_sqs_async` adapter.\n\n#### Message Deduplication ID\n\nFIFO queues support [Message deduplication ID](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html), which is the token used for deduplication of sent messages. \nIf a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval.\n\n##### Customize Deduplication keys\n\nIf necessary, the deduplication key used to create the message deduplication ID can be customized:\n\n```ruby\nAws::Rails::SqsActiveJob.configure do |config|\n  config.excluded_deduplication_keys = [:job_class, :arguments]\nend\n\n# Or to set deduplication keys to exclude for a single job:\nclass YourJob < ApplicationJob\n  include Aws::Rails::SqsActiveJob\n  deduplicate_without :job_class, :arguments\n  #...\nend\n```\n\nBy default, the following keys are used for deduplication keys:\n\n```\njob_class, provider_job_id, queue_name, priority, arguments, executions, exception_executions, locale, timezone, enqueued_at\n```\n\nNote that `job_id` is NOT included in deduplication keys because it is unique for each initialization of the job, and the run-once behavior must be guaranteed for ActiveJob retries. \nEven without setting job_id, it is implicitly excluded from deduplication keys.\n\n#### Message Group IDs\n\nFIFO queues require a message group id to be provided for the job. It is determined by:\n1. Calling `message_group_id` on the job if it is defined\n2. If `message_group_id` is not defined or the result is `nil`, the default value will be used.\nYou can optionally specify a custom value in your config as the default that will be used by all jobs.\n\n## AWS Record Generators\n\nThis package also pulls in the [\\`aws-record\\` gem](https://github.com/aws/aws-sdk-ruby-record)\nand provides generators for creating models and a rake task for performing\ntable config migrations.\n\n### Setup\n\nYou can either invoke the generator by calling `rails g aws_record:model ...`\n\nIf DynamoDB will be the only datastore you plan on using you can also set `aws-record-generator` to be your project's default orm with\n\n```ruby\nconfig.generators do |g|\n  g.orm :aws_record\nend\n```\nWhich will cause `aws_record:model` to be invoked by the Rails model generator.\n\n\n### Generating a model\n\nGenerating a model can be as simple as: `rails g aws_record:model Forum --table-config primary:10-5`\n`aws-record-generator` will automatically create a `uuid:hash_key` field for you, and a table config with the provided r/w units\n\n```ruby\n# app/models/forum.rb\n\nrequire 'aws-record'\n\nclass Forum\n  include Aws::Record\n\n  string_attr :uuid, hash_key: true\nend\n\n# db/table_config/forum_config.rb\n\nrequire 'aws-record'\n\nmodule ModelTableConfig\n  def self.config\n    Aws::Record::TableConfig.define do |t|\n      t.model_class Forum\n\n      t.read_capacity_units 10\n      t.write_capacity_units 5\n    end\n  end\nend\n```\n\nMore complex models can be created by adding more fields to the model as well as other options:\n\n`rails g aws_record Forum post_id:rkey author_username post_title post_body tags:sset:default_value{Set.new}`\n\n```ruby\n# app/models/forum.rb\n\nrequire 'aws-record'\n\nclass Forum\n  include Aws::Record\n\n  string_attr :uuid, hash_key: true\n  string_attr :post_id, range_key: true\n  string_attr :author_username\n  string_attr :post_title\n  string_attr :post_body\n  string_set_attr :tags, default_value: Set.new\nend\n\n# db/table_config/forum_config.rb\n# ...\n```\n\nFinally you can attach a variety of options to your fields, and even `ActiveModel` validations to the models:\n\n`rails g aws_record:model Forum forum_uuid:hkey post_id:rkey author_username post_title post_body tags:sset:default_value{Set.new} created_at:datetime:db_attr_name{PostCreatedAtTime} moderation:boolean:default_value{false} --table-config=primary:5-2 AuthorIndex:12-14 --required=post_title --length-validations=post_body:50-1000 --gsi=AuthorIndex:hkey{author_username}`\n\nWhich results in the following files being generated:\n\n```ruby\n# app/models/forum.rb\n\nrequire 'aws-record'\nrequire 'active_model'\n\nclass Forum\n  include Aws::Record\n  include ActiveModel::Validations\n\n  string_attr :forum_uuid, hash_key: true\n  string_attr :post_id, range_key: true\n  string_attr :author_username\n  string_attr :post_title\n  string_attr :post_body\n  string_set_attr :tags, default_value: Set.new\n  datetime_attr :created_at, database_attribute_name: \"PostCreatedAtTime\"\n  boolean_attr :moderation, default_value: false\n\n  global_secondary_index(\n    :AuthorIndex,\n    hash_key: :author_username,\n    projection: {\n      projection_type: \"ALL\"\n    }\n  )\n  validates_presence_of :post_title\n  validates_length_of :post_body, within: 50..1000\nend\n\n# db/table_config/forum_config.rb\n# ...\n```\n\nTo migrate your new models and begin using them you can run the provided rake task: `rails aws_record:migrate`\n\n### Docs\n\nThe syntax for creating an aws-record model follows:\n\n`rails generate aws_record:model NAME [field[:type][:opts]...] [options]`\n\nThe possible field types are:\n\nField Name | aws-record attribute type\n---------------- | -------------\n`bool \\| boolean` | :boolean_attr\n`date` | :date_attr\n`datetime` | :datetime_attr\n`float` | :float_attr\n`int \\| integer` | :integer_attr\n`list` | :list_attr\n`map` | :map_attr\n`num_set \\| numeric_set \\| nset` | :numeric_set_attr\n`string_set \\| s_set \\| sset` | :string_set_attr\n`string` | :string_attr\n\n\nIf a type is not provided, it will assume the field is of type `:string_attr`.\n\nAdditionally a number of options may be attached as a comma separated list to the field:\n\nField Option Name | aws-record option\n---------------- | -------------\n`hkey` | marks an attribute as a hash_key\n`rkey` | marks an attribute as a range_key\n`persist_nil` | will persist nil values in a attribute\n`db_attr_name{NAME}` | sets a secondary name for an attribute, these must be unique across attribute names\n`ddb_type{S\\|N\\|B\\|BOOL\\|SS\\|NS\\|BS\\|M\\|L}` | sets the dynamo_db_type for an attribute\n`default_value{Object}` | sets the default value for an attribute\n\nThe standard rules apply for using options in a model. Additional reading can be found [here](#links-of-interest)\n\nCommand Option Names | Purpose\n-------------------- | -----------\n  [--skip-namespace], [--no-skip-namespace]                                             | Skip namespace (affects only isolated applications)\n  [--disable-mutation-tracking], [--no-disable-mutation-tracking]                       | Disables dirty tracking\n  [--timestamps], [--no-timestamps]                                                     | Adds created, updated timestamps to the model\n  --table-config=primary:R-W [SecondaryIndex1:R-W]...                                   | Declares the r/w units for the model as well as any secondary indexes\n  [--gsi=name:hkey{ field_name }[,rkey{ field_name },proj_type{ ALL\\|KEYS_ONLY\\|INCLUDE }]...]  | Allows for the declaration of secondary indexes\n  [--required=field1...]                                                                | A list of attributes that are required for an instance of the model\n  [--length-validations=field1:MIN-MAX...]                                              | Validations on the length of attributes in a model\n  [--table-name=name] | Sets the name of the table in DynamoDB, if different than the model name\n  [--skip-table-config] | Doesn't generate a table config for the model\n  [--password-digest] | Adds a password field (note that you must have bcrypt has a dependency) that automatically hashes and manages the model password\n\nThe included rake task `aws_record:migrate` will run all of the migrations in `app/db/table_config`\n", "release_dates": ["2024-03-01T20:23:47Z", "2024-01-19T18:47:35Z", "2023-12-19T18:08:34Z", "2023-09-28T21:21:22Z", "2023-06-02T18:11:57Z", "2023-02-15T17:47:04Z", "2023-01-24T16:21:32Z", "2022-10-13T20:44:21Z", "2022-09-06T14:38:56Z", "2022-06-16T16:06:04Z", "2021-06-08T21:28:06Z", "2021-01-20T18:53:22Z", "2021-01-06T19:51:13Z", "2020-12-07T22:08:32Z", "2020-12-02T00:22:38Z", "2020-11-13T21:29:29Z", "2020-11-13T21:05:33Z", "2020-04-06T19:31:51Z", "2019-10-18T02:08:09Z", "2019-02-14T20:41:26Z", "2017-10-03T17:58:26Z", "2016-02-01T21:59:29Z", "2015-03-17T21:41:51Z"]}, {"name": "aws-sdk-ruby", "description": "The official AWS SDK for Ruby.", "language": "Ruby", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for Ruby - Version 3\n\n[![Gem Version](https://badge.fury.io/rb/aws-sdk-core.svg)](https://badge.fury.io/rb/aws-sdk-core)\n[![Build Status](https://github.com/aws/aws-sdk-ruby/workflows/CI/badge.svg)](https://github.com/aws/aws-sdk-ruby/actions)\n[![Github forks](https://img.shields.io/github/forks/aws/aws-sdk-ruby.svg)](https://github.com/aws/aws-sdk-ruby/network)\n[![Github stars](https://img.shields.io/github/stars/aws/aws-sdk-ruby.svg)](https://github.com/aws/aws-sdk-ruby/stargazers)\n\n## Links of Interest\n\n* [API Documentation](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/index.html)\n* [Developer Guide](https://docs.aws.amazon.com/sdk-for-ruby/v3/developer-guide/welcome.html)\n* [V3 Upgrading Guide](https://github.com/aws/aws-sdk-ruby/blob/version-3/V3_UPGRADING_GUIDE.md)\n* [AWS Developer Blog](https://aws.amazon.com/blogs/developer/category/programing-language/ruby/)\n* [Github Discussions](https://github.com/aws/aws-sdk-ruby/discussions)\n\n## Installation\n\nThe AWS SDK for Ruby is available from RubyGems. With V3 modularization, you\nshould pick the specific AWS service gems to install.\n\n```ruby\ngem 'aws-sdk-s3', '~> 1'\ngem 'aws-sdk-ec2', '~> 1'\n```\n\nAlternatively, the `aws-sdk` gem contains every available AWS service gem. This\ngem is very large; it is recommended to use it only as a quick way to migrate\nfrom V2 or if you depend on many AWS services.\n\n```ruby\ngem 'aws-sdk', '~> 3'\n```\n\n**Please use a pessimistic version constraint on the major version when\ndepending on service gems.**\n\n## Configuration\n\nYou will need to configure credentials and a region, either in configuration\nfiles or environment variables, to make API calls. It is recommended that you\nprovide these via your environment. This makes it easier to rotate credentials\nand it keeps your secrets out of source control.\n\nThe SDK searches the following locations for credentials:\n\n* `ENV['AWS_ACCESS_KEY_ID']` and `ENV['AWS_SECRET_ACCESS_KEY']`\n* The shared credentials ini file at `~/.aws/credentials`\n  * Credential options supported in this file are:\n    * Static Credentials (`aws_access_key_id`, `aws_secret_access_key`, `aws_session_token`)\n    * Assume Role Web Identity Credentials (`web_identity_token_file`, `role_arn`, `source_profile`)\n    * Assume Role Credentials (`role_arn`, `source_profile`)\n    * Process Credentials (`credential_process`)\n    * SSO Credentials (`sso_session`, `sso_account_id`, `sso_role_name`, `sso_region`)\n  * Unless `ENV['AWS_SDK_CONFIG_OPT_OUT']` is set, the shared configuration ini file at `~/.aws/config` will also be parsed for credentials.\n* From an instance profile when running on EC2 or from the ECS credential provider when running in an ECS container with that feature enabled.\n\n**Shared configuration is loaded only a single time, and credentials are provided statically at client creation time. Shared credentials do not refresh.**\n\nThe SDK searches the following locations for a region:\n\n* `ENV['AWS_REGION']`\n* `ENV['AMAZON_REGION']`\n* `ENV['AWS_DEFAULT_REGION']`\n* Unless `ENV['AWS_SDK_CONFIG_OPT_OUT']` is set, the shared configuration files (`~/.aws/credentials` and `~/.aws/config`) will also be checked for a region selection.\n\n**The region is used to construct an SSL endpoint**. If you need to connect to a non-standard endpoint, you may specify the `:endpoint` option.\n\n### Configuration Options\n\nYou can also configure default credentials and the region via the `Aws.config`\nhash. The `Aws.config` hash takes precedence over environment variables.\n\n```ruby\nrequire 'aws-sdk-core'\n\nAws.config.update(\n  region: 'us-west-2',\n  credentials: Aws::Credentials.new('akid', 'secret')\n)\n```\n\nValid region and credentials options are:\n\n* `:region` - A string like `us-west-2`. See [this page](https://docs.aws.amazon.com/general/latest/gr/aws-service-information.html) for a list of supported regions by service.\n* `:credentials` - An instance of one of the following classes:\n  * [`Aws::Credentials`](http://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/Credentials.html)\n  * [`Aws::AssumeRoleWebIdentityCredentials`](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/AssumeRoleWebIdentityCredentials.html)\n  * [`Aws::AssumeRoleCredentials`](http://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/AssumeRoleCredentials.html)\n  * [`Aws::SharedCredentials`](http://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/SharedCredentials.html)\n  * [`Aws::ProcessCredentials`](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/ProcessCredentials.html)\n  * [`Aws::InstanceProfileCredentials`](http://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/InstanceProfileCredentials.html)\n  * [`Aws::ECSCredentials`](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/ECSCredentials.html)\n  * [`Aws::CognitoIdentityCredentials`](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/CognitoIdentity/CognitoIdentityCredentials.html)\n\nYou may also pass configuration options directly to Client and Resource\nconstructors. These options take precedence over the environment and\n`Aws.config` defaults. A `:profile` Client option can also be used to choose a\nspecific profile defined in your configuration file.\n\n```ruby\n# using a credentials object\nec2 = Aws::EC2::Client.new(region: 'us-west-2', credentials: credentials)\n\n# using a profile name\nec2 = Aws::EC2::Client.new(profile: 'my_profile')\n```\n\nPlease take care to **never commit credentials to source control**. We strongly\nrecommended loading credentials from an external source.\n\n```ruby\nrequire 'aws-sdk'\nrequire 'json'\n\ncreds = JSON.load(File.read('secrets.json'))\nAws.config[:credentials] = Aws::Credentials.new(\n  creds['AccessKeyId'],\n  creds['SecretAccessKey']\n)\n```\n\nFor more information on how to configure credentials, see the developer guide\nfor [configuring AWS SDK for Ruby](https://docs.aws.amazon.com/sdk-for-ruby/v3/developer-guide/setup-config.html).\n\n## API Clients\n\nConstruct a service client to make API calls. Each client provides a 1-to-1\nmapping of methods to API operations. Refer to the\n[API documentation](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/index.html)\nfor a complete list of available methods.\n\n```ruby\n# list buckets in Amazon S3\ns3 = Aws::S3::Client.new\nresp = s3.list_buckets\nresp.buckets.map(&:name)\n#=> [\"bucket-1\", \"bucket-2\", ...]\n```\n\nAPI methods accept a hash of additional request parameters and return\nstructured response data.\n\n```ruby\n# list the first two objects in a bucket\nresp = s3.list_objects(bucket: 'aws-sdk', max_keys: 2)\nresp.contents.each do |object|\n  puts \"#{object.key} => #{object.etag}\"\nend\n```\n\n### Paging Responses\n\nMany AWS operations limit the number of results returned with each response.\nTo make it easy to get the next page of results, every AWS response object\nis enumerable:\n\n```ruby\n# yields one response object per API call made, this will enumerate\n# EVERY object in the named bucket\ns3.list_objects(bucket:'aws-sdk').each do |response|\n  puts response.contents.map(&:key)\nend\n```\n\nIf you prefer to control paging yourself, response objects have helper methods\nthat control paging:\n\n```ruby\n# make a request that returns a truncated response\nresp = s3.list_objects(bucket: 'aws-sdk')\n\nresp.last_page? #=> false\nresp.next_page? #=> true\nresp = resp.next_page # send a request for the next response page\nresp = resp.next_page until resp.last_page?\n```\n\n### Waiters\n\nWaiters are utility methods that poll for a particular state. To invoke a\nwaiter, call `#wait_until` on a client:\n\n```ruby\nbegin\n  ec2.wait_until(:instance_running, instance_ids:['i-12345678'])\n  puts \"instance running\"\nrescue Aws::Waiters::Errors::WaiterFailed => error\n  puts \"failed waiting for instance running: #{error.message}\"\nend\n```\n\nWaiters have sensible default polling intervals and maximum attempts. You can\nconfigure these per call to `#wait_until`. You can also register callbacks\nthat are triggered before each polling attempt and before waiting.\nSee the API documentation for more examples and for a list of supported\nwaiters per service.\n\n## Resource Interfaces\n\nResource interfaces are object oriented classes that represent actual\nresources in AWS. Resource interfaces built on top of API clients and provide\nadditional functionality.\n\n**Only a few services implement a resource interface. They are defined by hand\nin JSON and have limitations. Please use the Client API instead.**\n\n```ruby\ns3 = Aws::S3::Resource.new\n\n# reference an existing bucket by name\nbucket = s3.bucket('aws-sdk')\n\n# enumerate every object in a bucket\nbucket.objects.each do |obj|\n  puts \"#{obj.key} => #{obj.etag}\"\nend\n\n# batch operations, delete objects in batches of 1k\nbucket.objects(prefix: '/tmp-files/').delete\n\n# single object operations\nobj = bucket.object('hello')\nobj.put(body:'Hello World!')\nobj.etag\nobj.delete\n```\n\n## REPL - AWS Interactive Console\n\nThe `aws-sdk` gem ships with a REPL that provides a simple way to test\nthe Ruby SDK. You can access the REPL by running `aws-v3.rb` from the command line.\n\n```ruby\n$ aws-v3.rb\n[1] pry(Aws)> ec2.describe_instances.reservations.first.instances.first\n[Aws::EC2::Client 200 0.216615 0 retries] describe_instances()\n<struct\n instance_id=\"i-1234567\",\n image_id=\"ami-7654321\",\n state=<struct  code=16, name=\"running\">,\n ...>\n```\n\nYou can enable HTTP wire logging by setting the verbose flag:\n\n```\n$ aws-v3.rb -v\n```\n\nIn the REPL, every service class has a helper that returns a new client object.\nSimply downcase the service module name for the helper:\n\n* `s3` => `#<Aws::S3::Client>`\n* `ec2` => `#<Aws::EC2::Client>`\n* etc\n\n## Functionality requiring AWS Common Runtime (CRT)\n\nThe AWS SDK for Ruby has optional functionality that requires the \n[AWS Common Runtime (CRT)](https://docs.aws.amazon.com/sdkref/latest/guide/common-runtime.html) \nbindings to be included as a dependency with your application. This functionality includes:\n* [Amazon S3 Multi-Region Access Points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPoints.html)\n* CRC-32c support for [S3 Additional Checksums](https://aws.amazon.com/blogs/aws/new-additional-checksum-algorithms-for-amazon-s3/)\n\nIf the required AWS Common Runtime dependency is not installed you will receive an error\nindicating that the required dependency is missing to use the associated functionality. To install this dependency follow\nthe provided [instructions](#installing-the-aws-common-runtime-crt-dependency).\n\n### Installing the AWS Common Runtime (CRT) Dependency\nAWS CRT bindings are in developer preview and are available in the\nthe [aws-crt](https://rubygems.org/gems/aws-crt/) gem.  You can install them\nby adding the `aws-crt` gem to your Gemfile. \n\n[Sigv4a](https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html)\nis an extension to Sigv4 that allows signatures that are valid in more than one region.\nSigv4a is required to use some services/operations such as\n[S3 Multi-Region Access Points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPoints.html)\nand Amazon EventBridge Global Endpoints.\nCurrently sigv4a requires the [aws-crt](https://rubygems.org/gems/aws-crt/) gem.  The `aws-sigv4` signers \nwill automatically use the CRT provided signers with support for `sigv4a` when the `aws-crt` gem is available.\n\n```ruby\ngem 'aws-sdk-s3', '~> 1'\ngem 'aws-sigv4', '1.4.1.crt'\n```\n\n## Getting Help\n\nPlease use any of these resources for getting help:\n\n* Ask a question on [Github Discussions](https://github.com/aws/aws-sdk-ruby/discussions).\n* Ask a question on StackOverflow and [tag it](http://stackoverflow.com/questions/tagged/aws-sdk-ruby) with `aws-sdk-ruby`.\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home).\n\n## Maintenance and support for SDK major versions\n\nFor information about maintenance and support for SDK major versions and their underlying dependencies, see the following in the [AWS SDKs and Tools Shared Configuration and Credentials Reference Guide](https://docs.aws.amazon.com/credref/latest/refdocs/overview.html):\n\n* [AWS SDKs and Tools Maintenance Policy](https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html)\n* [AWS SDKs and Tools Version Support Matrix](https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html)\n\n## Opening Issues\n\nIf you encounter a bug or have a feature request, we would like to hear about\nit. Search the existing issues and try to make sure your problem doesn\u2019t already\nexist before opening a new issue.\n\nThe GitHub issues are intended for bug reports and feature requests. For help\nand questions with using `aws-sdk-ruby` please make use of the resources listed\nin the Getting Help section.\n\n## Versioning\n\nThis project uses [semantic versioning](http://semver.org/). You can safely\nexpress a dependency on a major version and expect all minor and patch versions\nto be backwards compatible.\n\nA CHANGELOG can be found at each gem's root path (i.e. `aws-sdk-s3` can be found\nat `gems/aws-sdk-s3/CHANGELOG.md`). The CHANGELOG is also accessible via the\nRubyGems.org page under \"LINKS\" section.\n\n## Supported Services\n\n| Service Name                                          | Service Module                           | gem_name                                    | API Version |\n| ----------------------------------------------------- | ---------------------------------------- | ------------------------------------------- | ----------- |\n| AWS ARC - Zonal Shift                                 | Aws::ARCZonalShift                       | aws-sdk-arczonalshift                       | 2022-10-30  |\n| AWS Account                                           | Aws::Account                             | aws-sdk-account                             | 2021-02-01  |\n| AWS Amplify                                           | Aws::Amplify                             | aws-sdk-amplify                             | 2017-07-25  |\n| AWS Amplify UI Builder                                | Aws::AmplifyUIBuilder                    | aws-sdk-amplifyuibuilder                    | 2021-08-11  |\n| AWS App Mesh                                          | Aws::AppMesh                             | aws-sdk-appmesh                             | 2019-01-25  |\n| AWS App Runner                                        | Aws::AppRunner                           | aws-sdk-apprunner                           | 2020-05-15  |\n| AWS AppConfig Data                                    | Aws::AppConfigData                       | aws-sdk-appconfigdata                       | 2021-11-11  |\n| AWS AppSync                                           | Aws::AppSync                             | aws-sdk-appsync                             | 2017-07-25  |\n| AWS Application Cost Profiler                         | Aws::ApplicationCostProfiler             | aws-sdk-applicationcostprofiler             | 2020-09-10  |\n| AWS Application Discovery Service                     | Aws::ApplicationDiscoveryService         | aws-sdk-applicationdiscoveryservice         | 2015-11-01  |\n| AWS Artifact                                          | Aws::Artifact                            | aws-sdk-artifact                            | 2018-05-10  |\n| AWS Audit Manager                                     | Aws::AuditManager                        | aws-sdk-auditmanager                        | 2017-07-25  |\n| AWS Auto Scaling Plans                                | Aws::AutoScalingPlans                    | aws-sdk-autoscalingplans                    | 2018-01-06  |\n| AWS B2B Data Interchange                              | Aws::B2bi                                | aws-sdk-b2bi                                | 2022-06-23  |\n| AWS Backup                                            | Aws::Backup                              | aws-sdk-backup                              | 2018-11-15  |\n| AWS Backup Gateway                                    | Aws::BackupGateway                       | aws-sdk-backupgateway                       | 2021-01-01  |\n| AWS Backup Storage                                    | Aws::BackupStorage                       | aws-sdk-backupstorage                       | 2018-04-10  |\n| AWS Batch                                             | Aws::Batch                               | aws-sdk-batch                               | 2016-08-10  |\n| AWS Billing and Cost Management Data Exports          | Aws::BCMDataExports                      | aws-sdk-bcmdataexports                      | 2023-11-26  |\n| AWS Budgets                                           | Aws::Budgets                             | aws-sdk-budgets                             | 2016-10-20  |\n| AWS Certificate Manager                               | Aws::ACM                                 | aws-sdk-acm                                 | 2015-12-08  |\n| AWS Certificate Manager Private Certificate Authority | Aws::ACMPCA                              | aws-sdk-acmpca                              | 2017-08-22  |\n| AWS Clean Rooms ML                                    | Aws::CleanRoomsML                        | aws-sdk-cleanroomsml                        | 2023-09-06  |\n| AWS Clean Rooms Service                               | Aws::CleanRooms                          | aws-sdk-cleanrooms                          | 2022-02-17  |\n| AWS Cloud Control API                                 | Aws::CloudControlApi                     | aws-sdk-cloudcontrolapi                     | 2021-09-30  |\n| AWS Cloud Map                                         | Aws::ServiceDiscovery                    | aws-sdk-servicediscovery                    | 2017-03-14  |\n| AWS Cloud9                                            | Aws::Cloud9                              | aws-sdk-cloud9                              | 2017-09-23  |\n| AWS CloudFormation                                    | Aws::CloudFormation                      | aws-sdk-cloudformation                      | 2010-05-15  |\n| AWS CloudHSM V2                                       | Aws::CloudHSMV2                          | aws-sdk-cloudhsmv2                          | 2017-04-28  |\n| AWS CloudTrail                                        | Aws::CloudTrail                          | aws-sdk-cloudtrail                          | 2013-11-01  |\n| AWS CloudTrail Data Service                           | Aws::CloudTrailData                      | aws-sdk-cloudtraildata                      | 2021-08-11  |\n| AWS CodeBuild                                         | Aws::CodeBuild                           | aws-sdk-codebuild                           | 2016-10-06  |\n| AWS CodeCommit                                        | Aws::CodeCommit                          | aws-sdk-codecommit                          | 2015-04-13  |\n| AWS CodeDeploy                                        | Aws::CodeDeploy                          | aws-sdk-codedeploy                          | 2014-10-06  |\n| AWS CodePipeline                                      | Aws::CodePipeline                        | aws-sdk-codepipeline                        | 2015-07-09  |\n| AWS CodeStar                                          | Aws::CodeStar                            | aws-sdk-codestar                            | 2017-04-19  |\n| AWS CodeStar Notifications                            | Aws::CodeStarNotifications               | aws-sdk-codestarnotifications               | 2019-10-15  |\n| AWS CodeStar connections                              | Aws::CodeStarconnections                 | aws-sdk-codestarconnections                 | 2019-12-01  |\n| AWS Comprehend Medical                                | Aws::ComprehendMedical                   | aws-sdk-comprehendmedical                   | 2018-10-30  |\n| AWS Compute Optimizer                                 | Aws::ComputeOptimizer                    | aws-sdk-computeoptimizer                    | 2019-11-01  |\n| AWS Config                                            | Aws::ConfigService                       | aws-sdk-configservice                       | 2014-11-12  |\n| AWS Control Tower                                     | Aws::ControlTower                        | aws-sdk-controltower                        | 2018-05-10  |\n| AWS Cost Explorer Service                             | Aws::CostExplorer                        | aws-sdk-costexplorer                        | 2017-10-25  |\n| AWS Cost and Usage Report Service                     | Aws::CostandUsageReportService           | aws-sdk-costandusagereportservice           | 2017-01-06  |\n| AWS Data Exchange                                     | Aws::DataExchange                        | aws-sdk-dataexchange                        | 2017-07-25  |\n| AWS Data Pipeline                                     | Aws::DataPipeline                        | aws-sdk-datapipeline                        | 2012-10-29  |\n| AWS DataSync                                          | Aws::DataSync                            | aws-sdk-datasync                            | 2018-11-09  |\n| AWS Database Migration Service                        | Aws::DatabaseMigrationService            | aws-sdk-databasemigrationservice            | 2016-01-01  |\n| AWS Device Farm                                       | Aws::DeviceFarm                          | aws-sdk-devicefarm                          | 2015-06-23  |\n| AWS Direct Connect                                    | Aws::DirectConnect                       | aws-sdk-directconnect                       | 2012-10-25  |\n| AWS Directory Service                                 | Aws::DirectoryService                    | aws-sdk-directoryservice                    | 2015-04-16  |\n| AWS EC2 Instance Connect                              | Aws::EC2InstanceConnect                  | aws-sdk-ec2instanceconnect                  | 2018-04-02  |\n| AWS Elastic Beanstalk                                 | Aws::ElasticBeanstalk                    | aws-sdk-elasticbeanstalk                    | 2010-12-01  |\n| AWS Elemental MediaConvert                            | Aws::MediaConvert                        | aws-sdk-mediaconvert                        | 2017-08-29  |\n| AWS Elemental MediaLive                               | Aws::MediaLive                           | aws-sdk-medialive                           | 2017-10-14  |\n| AWS Elemental MediaPackage                            | Aws::MediaPackage                        | aws-sdk-mediapackage                        | 2017-10-12  |\n| AWS Elemental MediaPackage VOD                        | Aws::MediaPackageVod                     | aws-sdk-mediapackagevod                     | 2018-11-07  |\n| AWS Elemental MediaPackage v2                         | Aws::MediaPackageV2                      | aws-sdk-mediapackagev2                      | 2022-12-25  |\n| AWS Elemental MediaStore                              | Aws::MediaStore                          | aws-sdk-mediastore                          | 2017-09-01  |\n| AWS Elemental MediaStore Data Plane                   | Aws::MediaStoreData                      | aws-sdk-mediastoredata                      | 2017-09-01  |\n| AWS EntityResolution                                  | Aws::EntityResolution                    | aws-sdk-entityresolution                    | 2018-05-10  |\n| AWS Fault Injection Simulator                         | Aws::FIS                                 | aws-sdk-fis                                 | 2020-12-01  |\n| AWS Free Tier                                         | Aws::FreeTier                            | aws-sdk-freetier                            | 2023-09-07  |\n| AWS Global Accelerator                                | Aws::GlobalAccelerator                   | aws-sdk-globalaccelerator                   | 2018-08-08  |\n| AWS Glue                                              | Aws::Glue                                | aws-sdk-glue                                | 2017-03-31  |\n| AWS Glue DataBrew                                     | Aws::GlueDataBrew                        | aws-sdk-gluedatabrew                        | 2017-07-25  |\n| AWS Greengrass                                        | Aws::Greengrass                          | aws-sdk-greengrass                          | 2017-06-07  |\n| AWS Ground Station                                    | Aws::GroundStation                       | aws-sdk-groundstation                       | 2019-05-23  |\n| AWS Health APIs and Notifications                     | Aws::Health                              | aws-sdk-health                              | 2016-08-04  |\n| AWS Health Imaging                                    | Aws::MedicalImaging                      | aws-sdk-medicalimaging                      | 2023-07-19  |\n| AWS Identity and Access Management                    | Aws::IAM                                 | aws-sdk-iam                                 | 2010-05-08  |\n| AWS Import/Export                                     | Aws::ImportExport                        | aws-sdk-importexport                        | 2010-06-01  |\n| AWS IoT                                               | Aws::IoT                                 | aws-sdk-iot                                 | 2015-05-28  |\n| AWS IoT 1-Click Devices Service                       | Aws::IoT1ClickDevicesService             | aws-sdk-iot1clickdevicesservice             | 2018-05-14  |\n| AWS IoT 1-Click Projects Service                      | Aws::IoT1ClickProjects                   | aws-sdk-iot1clickprojects                   | 2018-05-14  |\n| AWS IoT Analytics                                     | Aws::IoTAnalytics                        | aws-sdk-iotanalytics                        | 2017-11-27  |\n| AWS IoT Core Device Advisor                           | Aws::IoTDeviceAdvisor                    | aws-sdk-iotdeviceadvisor                    | 2020-09-18  |\n| AWS IoT Data Plane                                    | Aws::IoTDataPlane                        | aws-sdk-iotdataplane                        | 2015-05-28  |\n| AWS IoT Events                                        | Aws::IoTEvents                           | aws-sdk-iotevents                           | 2018-07-27  |\n| AWS IoT Events Data                                   | Aws::IoTEventsData                       | aws-sdk-ioteventsdata                       | 2018-10-23  |\n| AWS IoT Fleet Hub                                     | Aws::IoTFleetHub                         | aws-sdk-iotfleethub                         | 2020-11-03  |\n| AWS IoT FleetWise                                     | Aws::IoTFleetWise                        | aws-sdk-iotfleetwise                        | 2021-06-17  |\n| AWS IoT Greengrass V2                                 | Aws::GreengrassV2                        | aws-sdk-greengrassv2                        | 2020-11-30  |\n| AWS IoT Jobs Data Plane                               | Aws::IoTJobsDataPlane                    | aws-sdk-iotjobsdataplane                    | 2017-09-29  |\n| AWS IoT RoboRunner                                    | Aws::IoTRoboRunner                       | aws-sdk-iotroborunner                       | 2018-05-10  |\n| AWS IoT Secure Tunneling                              | Aws::IoTSecureTunneling                  | aws-sdk-iotsecuretunneling                  | 2018-10-05  |\n| AWS IoT SiteWise                                      | Aws::IoTSiteWise                         | aws-sdk-iotsitewise                         | 2019-12-02  |\n| AWS IoT Things Graph                                  | Aws::IoTThingsGraph                      | aws-sdk-iotthingsgraph                      | 2018-09-06  |\n| AWS IoT TwinMaker                                     | Aws::IoTTwinMaker                        | aws-sdk-iottwinmaker                        | 2021-11-29  |\n| AWS IoT Wireless                                      | Aws::IoTWireless                         | aws-sdk-iotwireless                         | 2020-11-22  |\n| AWS Key Management Service                            | Aws::KMS                                 | aws-sdk-kms                                 | 2014-11-01  |\n| AWS Lake Formation                                    | Aws::LakeFormation                       | aws-sdk-lakeformation                       | 2017-03-31  |\n| AWS Lambda                                            | Aws::Lambda                              | aws-sdk-lambda                              | 2015-03-31  |\n| AWS Lambda                                            | Aws::LambdaPreview                       | aws-sdk-lambdapreview                       | 2014-11-11  |\n| AWS Launch Wizard                                     | Aws::LaunchWizard                        | aws-sdk-launchwizard                        | 2018-05-10  |\n| AWS License Manager                                   | Aws::LicenseManager                      | aws-sdk-licensemanager                      | 2018-08-01  |\n| AWS License Manager Linux Subscriptions               | Aws::LicenseManagerLinuxSubscriptions    | aws-sdk-licensemanagerlinuxsubscriptions    | 2018-05-10  |\n| AWS License Manager User Subscriptions                | Aws::LicenseManagerUserSubscriptions     | aws-sdk-licensemanagerusersubscriptions     | 2018-05-10  |\n| AWS Marketplace Agreement Service                     | Aws::MarketplaceAgreement                | aws-sdk-marketplaceagreement                | 2020-03-01  |\n| AWS Marketplace Catalog Service                       | Aws::MarketplaceCatalog                  | aws-sdk-marketplacecatalog                  | 2018-09-17  |\n| AWS Marketplace Commerce Analytics                    | Aws::MarketplaceCommerceAnalytics        | aws-sdk-marketplacecommerceanalytics        | 2015-07-01  |\n| AWS Marketplace Deployment Service                    | Aws::MarketplaceDeployment               | aws-sdk-marketplacedeployment               | 2023-01-25  |\n| AWS Marketplace Entitlement Service                   | Aws::MarketplaceEntitlementService       | aws-sdk-marketplaceentitlementservice       | 2017-01-11  |\n| AWS MediaConnect                                      | Aws::MediaConnect                        | aws-sdk-mediaconnect                        | 2018-11-14  |\n| AWS MediaTailor                                       | Aws::MediaTailor                         | aws-sdk-mediatailor                         | 2018-04-23  |\n| AWS Migration Hub                                     | Aws::MigrationHub                        | aws-sdk-migrationhub                        | 2017-05-31  |\n| AWS Migration Hub Config                              | Aws::MigrationHubConfig                  | aws-sdk-migrationhubconfig                  | 2019-06-30  |\n| AWS Migration Hub Orchestrator                        | Aws::MigrationHubOrchestrator            | aws-sdk-migrationhuborchestrator            | 2021-08-28  |\n| AWS Migration Hub Refactor Spaces                     | Aws::MigrationHubRefactorSpaces          | aws-sdk-migrationhubrefactorspaces          | 2021-10-26  |\n| AWS Mobile                                            | Aws::Mobile                              | aws-sdk-mobile                              | 2017-07-01  |\n| AWS Network Firewall                                  | Aws::NetworkFirewall                     | aws-sdk-networkfirewall                     | 2020-11-12  |\n| AWS Network Manager                                   | Aws::NetworkManager                      | aws-sdk-networkmanager                      | 2019-07-05  |\n| AWS OpsWorks                                          | Aws::OpsWorks                            | aws-sdk-opsworks                            | 2013-02-18  |\n| AWS OpsWorks CM                                       | Aws::OpsWorksCM                          | aws-sdk-opsworkscm                          | 2016-11-01  |\n| AWS Organizations                                     | Aws::Organizations                       | aws-sdk-organizations                       | 2016-11-28  |\n| AWS Outposts                                          | Aws::Outposts                            | aws-sdk-outposts                            | 2019-12-03  |\n| AWS Panorama                                          | Aws::Panorama                            | aws-sdk-panorama                            | 2019-07-24  |\n| AWS Performance Insights                              | Aws::PI                                  | aws-sdk-pi                                  | 2018-02-27  |\n| AWS Price List Service                                | Aws::Pricing                             | aws-sdk-pricing                             | 2017-10-15  |\n| AWS Private 5G                                        | Aws::PrivateNetworks                     | aws-sdk-privatenetworks                     | 2021-12-03  |\n| AWS Proton                                            | Aws::Proton                              | aws-sdk-proton                              | 2020-07-20  |\n| AWS RDS DataService                                   | Aws::RDSDataService                      | aws-sdk-rdsdataservice                      | 2018-08-01  |\n| AWS Resilience Hub                                    | Aws::ResilienceHub                       | aws-sdk-resiliencehub                       | 2020-04-30  |\n| AWS Resource Access Manager                           | Aws::RAM                                 | aws-sdk-ram                                 | 2018-01-04  |\n| AWS Resource Explorer                                 | Aws::ResourceExplorer2                   | aws-sdk-resourceexplorer2                   | 2022-07-28  |\n| AWS Resource Groups                                   | Aws::ResourceGroups                      | aws-sdk-resourcegroups                      | 2017-11-27  |\n| AWS Resource Groups Tagging API                       | Aws::ResourceGroupsTaggingAPI            | aws-sdk-resourcegroupstaggingapi            | 2017-01-26  |\n| AWS RoboMaker                                         | Aws::RoboMaker                           | aws-sdk-robomaker                           | 2018-06-29  |\n| AWS Route53 Recovery Control Config                   | Aws::Route53RecoveryControlConfig        | aws-sdk-route53recoverycontrolconfig        | 2020-11-02  |\n| AWS Route53 Recovery Readiness                        | Aws::Route53RecoveryReadiness            | aws-sdk-route53recoveryreadiness            | 2019-12-02  |\n| AWS S3 Control                                        | Aws::S3Control                           | aws-sdk-s3control                           | 2018-08-20  |\n| AWS SSO Identity Store                                | Aws::IdentityStore                       | aws-sdk-identitystore                       | 2020-06-15  |\n| AWS SSO OIDC                                          | Aws::SSOOIDC                             | aws-sdk-core                                | 2019-06-10  |\n| AWS Savings Plans                                     | Aws::SavingsPlans                        | aws-sdk-savingsplans                        | 2019-06-28  |\n| AWS Secrets Manager                                   | Aws::SecretsManager                      | aws-sdk-secretsmanager                      | 2017-10-17  |\n| AWS Security Token Service                            | Aws::STS                                 | aws-sdk-core                                | 2011-06-15  |\n| AWS SecurityHub                                       | Aws::SecurityHub                         | aws-sdk-securityhub                         | 2018-10-26  |\n| AWS Server Migration Service                          | Aws::SMS                                 | aws-sdk-sms                                 | 2016-10-24  |\n| AWS Service Catalog                                   | Aws::ServiceCatalog                      | aws-sdk-servicecatalog                      | 2015-12-10  |\n| AWS Service Catalog App Registry                      | Aws::AppRegistry                         | aws-sdk-appregistry                         | 2020-06-24  |\n| AWS Shield                                            | Aws::Shield                              | aws-sdk-shield                              | 2016-06-02  |\n| AWS Signer                                            | Aws::Signer                              | aws-sdk-signer                              | 2017-08-25  |\n| AWS SimSpace Weaver                                   | Aws::SimSpaceWeaver                      | aws-sdk-simspaceweaver                      | 2022-10-28  |\n| AWS Single Sign-On                                    | Aws::SSO                                 | aws-sdk-core                                | 2019-06-10  |\n| AWS Single Sign-On Admin                              | Aws::SSOAdmin                            | aws-sdk-ssoadmin                            | 2020-07-20  |\n| AWS Snow Device Management                            | Aws::SnowDeviceManagement                | aws-sdk-snowdevicemanagement                | 2021-08-04  |\n| AWS Step Functions                                    | Aws::States                              | aws-sdk-states                              | 2016-11-23  |\n| AWS Storage Gateway                                   | Aws::StorageGateway                      | aws-sdk-storagegateway                      | 2013-06-30  |\n| AWS Supply Chain                                      | Aws::SupplyChain                         | aws-sdk-supplychain                         | 2024-01-01  |\n| AWS Support                                           | Aws::Support                             | aws-sdk-support                             | 2013-04-15  |\n| AWS Support App                                       | Aws::SupportApp                          | aws-sdk-supportapp                          | 2021-08-20  |\n| AWS Systems Manager Incident Manager                  | Aws::SSMIncidents                        | aws-sdk-ssmincidents                        | 2018-05-10  |\n| AWS Systems Manager Incident Manager Contacts         | Aws::SSMContacts                         | aws-sdk-ssmcontacts                         | 2021-05-03  |\n| AWS Systems Manager for SAP                           | Aws::SsmSap                              | aws-sdk-ssmsap                              | 2018-05-10  |\n| AWS Telco Network Builder                             | Aws::Tnb                                 | aws-sdk-tnb                                 | 2008-10-21  |\n| AWS Transfer Family                                   | Aws::Transfer                            | aws-sdk-transfer                            | 2018-11-05  |\n| AWS WAF                                               | Aws::WAF                                 | aws-sdk-waf                                 | 2015-08-24  |\n| AWS WAF Regional                                      | Aws::WAFRegional                         | aws-sdk-wafregional                         | 2016-11-28  |\n| AWS WAFV2                                             | Aws::WAFV2                               | aws-sdk-wafv2                               | 2019-07-29  |\n| AWS Well-Architected Tool                             | Aws::WellArchitected                     | aws-sdk-wellarchitected                     | 2020-03-31  |\n| AWS X-Ray                                             | Aws::XRay                                | aws-sdk-xray                                | 2016-04-12  |\n| AWS re:Post Private                                   | Aws::Repostspace                         | aws-sdk-repostspace                         | 2022-05-13  |\n| AWSBillingConductor                                   | Aws::BillingConductor                    | aws-sdk-billingconductor                    | 2021-07-30  |\n| AWSKendraFrontendService                              | Aws::Kendra                              | aws-sdk-kendra                              | 2019-02-03  |\n| AWSMainframeModernization                             | Aws::MainframeModernization              | aws-sdk-mainframemodernization              | 2021-04-28  |\n| AWSMarketplace Metering                               | Aws::MarketplaceMetering                 | aws-sdk-marketplacemetering                 | 2016-01-14  |\n| AWSServerlessApplicationRepository                    | Aws::ServerlessApplicationRepository     | aws-sdk-serverlessapplicationrepository     | 2017-09-08  |\n| Access Analyzer                                       | Aws::AccessAnalyzer                      | aws-sdk-accessanalyzer                      | 2019-11-01  |\n| Agents for Amazon Bedrock                             | Aws::BedrockAgent                        | aws-sdk-bedrockagent                        | 2023-06-05  |\n| Agents for Amazon Bedrock Runtime                     | Aws::BedrockAgentRuntime                 | aws-sdk-bedrockagentruntime                 | 2023-07-26  |\n| Alexa For Business                                    | Aws::AlexaForBusiness                    | aws-sdk-alexaforbusiness                    | 2017-11-09  |\n| Amazon API Gateway                                    | Aws::APIGateway                          | aws-sdk-apigateway                          | 2015-07-09  |\n| Amazon AppConfig                                      | Aws::AppConfig                           | aws-sdk-appconfig                           | 2019-10-09  |\n| Amazon AppIntegrations Service                        | Aws::AppIntegrationsService              | aws-sdk-appintegrationsservice              | 2020-07-29  |\n| Amazon AppStream                                      | Aws::AppStream                           | aws-sdk-appstream                           | 2016-12-01  |\n| Amazon Appflow                                        | Aws::Appflow                             | aws-sdk-appflow                             | 2020-08-23  |\n| Amazon Athena                                         | Aws::Athena                              | aws-sdk-athena                              | 2017-05-18  |\n| Amazon Augmented AI Runtime                           | Aws::AugmentedAIRuntime                  | aws-sdk-augmentedairuntime                  | 2019-11-07  |\n| Amazon Bedrock                                        | Aws::Bedrock                             | aws-sdk-bedrock                             | 2023-04-20  |\n| Amazon Bedrock Runtime                                | Aws::BedrockRuntime                      | aws-sdk-bedrockruntime                      | 2023-09-30  |\n| Amazon Chime                                          | Aws::Chime                               | aws-sdk-chime                               | 2018-05-01  |\n| Amazon Chime SDK Identity                             | Aws::ChimeSDKIdentity                    | aws-sdk-chimesdkidentity                    | 2021-04-20  |\n| Amazon Chime SDK Media Pipelines                      | Aws::ChimeSDKMediaPipelines              | aws-sdk-chimesdkmediapipelines              | 2021-07-15  |\n| Amazon Chime SDK Meetings                             | Aws::ChimeSDKMeetings                    | aws-sdk-chimesdkmeetings                    | 2021-07-15  |\n| Amazon Chime SDK Messaging                            | Aws::ChimeSDKMessaging                   | aws-sdk-chimesdkmessaging                   | 2021-05-15  |\n| Amazon Chime SDK Voice                                | Aws::ChimeSDKVoice                       | aws-sdk-chimesdkvoice                       | 2022-08-03  |\n| Amazon CloudDirectory                                 | Aws::CloudDirectory                      | aws-sdk-clouddirectory                      | 2017-01-11  |\n| Amazon CloudFront                                     | Aws::CloudFront                          | aws-sdk-cloudfront                          | 2020-05-31  |\n| Amazon CloudFront KeyValueStore                       | Aws::CloudFrontKeyValueStore             | aws-sdk-cloudfrontkeyvaluestore             | 2022-07-26  |\n| Amazon CloudHSM                                       | Aws::CloudHSM                            | aws-sdk-cloudhsm                            | 2014-05-30  |\n| Amazon CloudSearch                                    | Aws::CloudSearch                         | aws-sdk-cloudsearch                         | 2013-01-01  |\n| Amazon CloudSearch Domain                             | Aws::CloudSearchDomain                   | aws-sdk-cloudsearchdomain                   | 2013-01-01  |\n| Amazon CloudWatch                                     | Aws::CloudWatch                          | aws-sdk-cloudwatch                          | 2010-08-01  |\n| Amazon CloudWatch Application Insights                | Aws::ApplicationInsights                 | aws-sdk-applicationinsights                 | 2018-11-25  |\n| Amazon CloudWatch Events                              | Aws::CloudWatchEvents                    | aws-sdk-cloudwatchevents                    | 2015-10-07  |\n| Amazon CloudWatch Evidently                           | Aws::CloudWatchEvidently                 | aws-sdk-cloudwatchevidently                 | 2021-02-01  |\n| Amazon CloudWatch Internet Monitor                    | Aws::InternetMonitor                     | aws-sdk-internetmonitor                     | 2021-06-03  |\n| Amazon CloudWatch Logs                                | Aws::CloudWatchLogs                      | aws-sdk-cloudwatchlogs                      | 2014-03-28  |\n| Amazon CloudWatch Network Monitor                     | Aws::NetworkMonitor                      | aws-sdk-networkmonitor                      | 2023-08-01  |\n| Amazon CodeCatalyst                                   | Aws::CodeCatalyst                        | aws-sdk-codecatalyst                        | 2022-09-28  |\n| Amazon CodeGuru Profiler                              | Aws::CodeGuruProfiler                    | aws-sdk-codeguruprofiler                    | 2019-07-18  |\n| Amazon CodeGuru Reviewer                              | Aws::CodeGuruReviewer                    | aws-sdk-codegurureviewer                    | 2019-09-19  |\n| Amazon CodeGuru Security                              | Aws::CodeGuruSecurity                    | aws-sdk-codegurusecurity                    | 2018-05-10  |\n| Amazon Cognito Identity                               | Aws::CognitoIdentity                     | aws-sdk-cognitoidentity                     | 2014-06-30  |\n| Amazon Cognito Identity Provider                      | Aws::CognitoIdentityProvider             | aws-sdk-cognitoidentityprovider             | 2016-04-18  |\n| Amazon Cognito Sync                                   | Aws::CognitoSync                         | aws-sdk-cognitosync                         | 2014-06-30  |\n| Amazon Comprehend                                     | Aws::Comprehend                          | aws-sdk-comprehend                          | 2017-11-27  |\n| Amazon Connect Cases                                  | Aws::ConnectCases                        | aws-sdk-connectcases                        | 2022-10-03  |\n| Amazon Connect Contact Lens                           | Aws::ConnectContactLens                  | aws-sdk-connectcontactlens                  | 2020-08-21  |\n| Amazon Connect Customer Profiles                      | Aws::CustomerProfiles                    | aws-sdk-customerprofiles                    | 2020-08-15  |\n| Amazon Connect Participant Service                    | Aws::ConnectParticipant                  | aws-sdk-connectparticipant                  | 2018-09-07  |\n| Amazon Connect Service                                | Aws::Connect                             | aws-sdk-connect                             | 2017-08-08  |\n| Amazon Connect Wisdom Service                         | Aws::ConnectWisdomService                | aws-sdk-connectwisdomservice                | 2020-10-19  |\n| Amazon Data Lifecycle Manager                         | Aws::DLM                                 | aws-sdk-dlm                                 | 2018-01-12  |\n| Amazon DataZone                                       | Aws::DataZone                            | aws-sdk-datazone                            | 2018-05-10  |\n| Amazon Detective                                      | Aws::Detective                           | aws-sdk-detective                           | 2018-10-26  |\n| Amazon DevOps Guru                                    | Aws::DevOpsGuru                          | aws-sdk-devopsguru                          | 2020-12-01  |\n| Amazon DocumentDB Elastic Clusters                    | Aws::DocDBElastic                        | aws-sdk-docdbelastic                        | 2022-11-28  |\n| Amazon DocumentDB with MongoDB compatibility          | Aws::DocDB                               | aws-sdk-docdb                               | 2014-10-31  |\n| Amazon DynamoDB                                       | Aws::DynamoDB                            | aws-sdk-dynamodb                            | 2012-08-10  |\n| Amazon DynamoDB Accelerator (DAX)                     | Aws::DAX                                 | aws-sdk-dax                                 | 2017-04-19  |\n| Amazon DynamoDB Streams                               | Aws::DynamoDBStreams                     | aws-sdk-dynamodbstreams                     | 2012-08-10  |\n| Amazon EC2 Container Registry                         | Aws::ECR                                 | aws-sdk-ecr                                 | 2015-09-21  |\n| Amazon EC2 Container Service                          | Aws::ECS                                 | aws-sdk-ecs                                 | 2014-11-13  |\n| Amazon EKS Auth                                       | Aws::EKSAuth                             | aws-sdk-eksauth                             | 2023-11-26  |\n| Amazon EMR                                            | Aws::EMR                                 | aws-sdk-emr                                 | 2009-03-31  |\n| Amazon EMR Containers                                 | Aws::EMRContainers                       | aws-sdk-emrcontainers                       | 2020-10-01  |\n| Amazon ElastiCache                                    | Aws::ElastiCache                         | aws-sdk-elasticache                         | 2015-02-02  |\n| Amazon Elastic  Inference                             | Aws::ElasticInference                    | aws-sdk-elasticinference                    | 2017-07-25  |\n| Amazon Elastic Block Store                            | Aws::EBS                                 | aws-sdk-ebs                                 | 2019-11-02  |\n| Amazon Elastic Compute Cloud                          | Aws::EC2                                 | aws-sdk-ec2                                 | 2016-11-15  |\n| Amazon Elastic Container Registry Public              | Aws::ECRPublic                           | aws-sdk-ecrpublic                           | 2020-10-30  |\n| Amazon Elastic File System                            | Aws::EFS                                 | aws-sdk-efs                                 | 2015-02-01  |\n| Amazon Elastic Kubernetes Service                     | Aws::EKS                                 | aws-sdk-eks                                 | 2017-11-01  |\n| Amazon Elastic Transcoder                             | Aws::ElasticTranscoder                   | aws-sdk-elastictranscoder                   | 2012-09-25  |\n| Amazon Elasticsearch Service                          | Aws::ElasticsearchService                | aws-sdk-elasticsearchservice                | 2015-01-01  |\n| Amazon EventBridge                                    | Aws::EventBridge                         | aws-sdk-eventbridge                         | 2015-10-07  |\n| Amazon EventBridge Pipes                              | Aws::Pipes                               | aws-sdk-pipes                               | 2015-10-07  |\n| Amazon EventBridge Scheduler                          | Aws::Scheduler                           | aws-sdk-scheduler                           | 2021-06-30  |\n| Amazon FSx                                            | Aws::FSx                                 | aws-sdk-fsx                                 | 2018-03-01  |\n| Amazon Forecast Query Service                         | Aws::ForecastQueryService                | aws-sdk-forecastqueryservice                | 2018-06-26  |\n| Amazon Forecast Service                               | Aws::ForecastService                     | aws-sdk-forecastservice                     | 2018-06-26  |\n| Amazon Fraud Detector                                 | Aws::FraudDetector                       | aws-sdk-frauddetector                       | 2019-11-15  |\n| Amazon GameLift                                       | Aws::GameLift                            | aws-sdk-gamelift                            | 2015-10-01  |\n| Amazon Glacier                                        | Aws::Glacier                             | aws-sdk-glacier                             | 2012-06-01  |\n| Amazon GuardDuty                                      | Aws::GuardDuty                           | aws-sdk-guardduty                           | 2017-11-28  |\n| Amazon HealthLake                                     | Aws::HealthLake                          | aws-sdk-healthlake                          | 2017-07-01  |\n| Amazon Honeycode                                      | Aws::Honeycode                           | aws-sdk-honeycode                           | 2020-03-01  |\n| Amazon Import/Export Snowball                         | Aws::Snowball                            | aws-sdk-snowball                            | 2016-06-30  |\n| Amazon Inspector                                      | Aws::Inspector                           | aws-sdk-inspector                           | 2016-02-16  |\n| Amazon Interactive Video Service                      | Aws::IVS                                 | aws-sdk-ivs                                 | 2020-07-14  |\n| Amazon Interactive Video Service Chat                 | Aws::Ivschat                             | aws-sdk-ivschat                             | 2020-07-14  |\n| Amazon Interactive Video Service RealTime             | Aws::IVSRealTime                         | aws-sdk-ivsrealtime                         | 2020-07-14  |\n| Amazon Kendra Intelligent Ranking                     | Aws::KendraRanking                       | aws-sdk-kendraranking                       | 2022-10-19  |\n| Amazon Keyspaces                                      | Aws::Keyspaces                           | aws-sdk-keyspaces                           | 2022-02-10  |\n| Amazon Kinesis                                        | Aws::Kinesis                             | aws-sdk-kinesis                             | 2013-12-02  |\n| Amazon Kinesis Analytics                              | Aws::KinesisAnalytics                    | aws-sdk-kinesisanalytics                    | 2015-08-14  |\n| Amazon Kinesis Analytics                              | Aws::KinesisAnalyticsV2                  | aws-sdk-kinesisanalyticsv2                  | 2018-05-23  |\n| Amazon Kinesis Firehose                               | Aws::Firehose                            | aws-sdk-firehose                            | 2015-08-04  |\n| Amazon Kinesis Video Signaling Channels               | Aws::KinesisVideoSignalingChannels       | aws-sdk-kinesisvideosignalingchannels       | 2019-12-04  |\n| Amazon Kinesis Video Streams                          | Aws::KinesisVideo                        | aws-sdk-kinesisvideo                        | 2017-09-30  |\n| Amazon Kinesis Video Streams Archived Media           | Aws::KinesisVideoArchivedMedia           | aws-sdk-kinesisvideoarchivedmedia           | 2017-09-30  |\n| Amazon Kinesis Video Streams Media                    | Aws::KinesisVideoMedia                   | aws-sdk-kinesisvideomedia                   | 2017-09-30  |\n| Amazon Kinesis Video WebRTC Storage                   | Aws::KinesisVideoWebRTCStorage           | aws-sdk-kinesisvideowebrtcstorage           | 2018-05-10  |\n| Amazon Lex Model Building Service                     | Aws::LexModelBuildingService             | aws-sdk-lexmodelbuildingservice             | 2017-04-19  |\n| Amazon Lex Model Building V2                          | Aws::LexModelsV2                         | aws-sdk-lexmodelsv2                         | 2020-08-07  |\n| Amazon Lex Runtime Service                            | Aws::Lex                                 | aws-sdk-lex                                 | 2016-11-28  |\n| Amazon Lex Runtime V2                                 | Aws::LexRuntimeV2                        | aws-sdk-lexruntimev2                        | 2020-08-07  |\n| Amazon Lightsail                                      | Aws::Lightsail                           | aws-sdk-lightsail                           | 2016-11-28  |\n| Amazon Location Service                               | Aws::LocationService                     | aws-sdk-locationservice                     | 2020-11-19  |\n| Amazon Lookout for Equipment                          | Aws::LookoutEquipment                    | aws-sdk-lookoutequipment                    | 2020-12-15  |\n| Amazon Lookout for Metrics                            | Aws::LookoutMetrics                      | aws-sdk-lookoutmetrics                      | 2017-07-25  |\n| Amazon Lookout for Vision                             | Aws::LookoutforVision                    | aws-sdk-lookoutforvision                    | 2020-11-20  |\n| Amazon Machine Learning                               | Aws::MachineLearning                     | aws-sdk-machinelearning                     | 2014-12-12  |\n| Amazon Macie 2                                        | Aws::Macie2                              | aws-sdk-macie2                              | 2020-01-01  |\n| Amazon Managed Blockchain                             | Aws::ManagedBlockchain                   | aws-sdk-managedblockchain                   | 2018-09-24  |\n| Amazon Managed Blockchain Query                       | Aws::ManagedBlockchainQuery              | aws-sdk-managedblockchainquery              | 2023-05-04  |\n| Amazon Managed Grafana                                | Aws::ManagedGrafana                      | aws-sdk-managedgrafana                      | 2020-08-18  |\n| Amazon Mechanical Turk                                | Aws::MTurk                               | aws-sdk-mturk                               | 2017-01-17  |\n| Amazon MemoryDB                                       | Aws::MemoryDB                            | aws-sdk-memorydb                            | 2021-01-01  |\n| Amazon Neptune                                        | Aws::Neptune                             | aws-sdk-neptune                             | 2014-10-31  |\n| Amazon Neptune Graph                                  | Aws::NeptuneGraph                        | aws-sdk-neptunegraph                        | 2023-11-29  |\n| Amazon NeptuneData                                    | Aws::Neptunedata                         | aws-sdk-neptunedata                         | 2023-08-01  |\n| Amazon Omics                                          | Aws::Omics                               | aws-sdk-omics                               | 2022-11-28  |\n| Amazon OpenSearch Ingestion                           | Aws::OSIS                                | aws-sdk-osis                                | 2022-01-01  |\n| Amazon OpenSearch Service                             | Aws::OpenSearchService                   | aws-sdk-opensearchservice                   | 2021-01-01  |\n| Amazon Personalize                                    | Aws::Personalize                         | aws-sdk-personalize                         | 2018-05-22  |\n| Amazon Personalize Events                             | Aws::PersonalizeEvents                   | aws-sdk-personalizeevents                   | 2018-03-22  |\n| Amazon Personalize Runtime                            | Aws::PersonalizeRuntime                  | aws-sdk-personalizeruntime                  | 2018-05-22  |\n| Amazon Pinpoint                                       | Aws::Pinpoint                            | aws-sdk-pinpoint                            | 2016-12-01  |\n| Amazon Pinpoint Email Service                         | Aws::PinpointEmail                       | aws-sdk-pinpointemail                       | 2018-07-26  |\n| Amazon Pinpoint SMS Voice V2                          | Aws::PinpointSMSVoiceV2                  | aws-sdk-pinpointsmsvoicev2                  | 2022-03-31  |\n| Amazon Pinpoint SMS and Voice Service                 | Aws::PinpointSMSVoice                    | aws-sdk-pinpointsmsvoice                    | 2018-09-05  |\n| Amazon Polly                                          | Aws::Polly                               | aws-sdk-polly                               | 2016-06-10  |\n| Amazon Prometheus Service                             | Aws::PrometheusService                   | aws-sdk-prometheusservice                   | 2020-08-01  |\n| Amazon Q Connect                                      | Aws::QConnect                            | aws-sdk-qconnect                            | 2020-10-19  |\n| Amazon QLDB                                           | Aws::QLDB                                | aws-sdk-qldb                                | 2019-01-02  |\n| Amazon QLDB Session                                   | Aws::QLDBSession                         | aws-sdk-qldbsession                         | 2019-07-11  |\n| Amazon QuickSight                                     | Aws::QuickSight                          | aws-sdk-quicksight                          | 2018-04-01  |\n| Amazon Recycle Bin                                    | Aws::RecycleBin                          | aws-sdk-recyclebin                          | 2021-06-15  |\n| Amazon Redshift                                       | Aws::Redshift                            | aws-sdk-redshift                            | 2012-12-01  |\n| Amazon Rekognition                                    | Aws::Rekognition                         | aws-sdk-rekognition                         | 2016-06-27  |\n| Amazon Relational Database Service                    | Aws::RDS                                 | aws-sdk-rds                                 | 2014-10-31  |\n| Amazon Route 53                                       | Aws::Route53                             | aws-sdk-route53                             | 2013-04-01  |\n| Amazon Route 53 Domains                               | Aws::Route53Domains                      | aws-sdk-route53domains                      | 2014-05-15  |\n| Amazon Route 53 Resolver                              | Aws::Route53Resolver                     | aws-sdk-route53resolver                     | 2018-04-01  |\n| Amazon S3 on Outposts                                 | Aws::S3Outposts                          | aws-sdk-s3outposts                          | 2017-07-25  |\n| Amazon SageMaker Feature Store Runtime                | Aws::SageMakerFeatureStoreRuntime        | aws-sdk-sagemakerfeaturestoreruntime        | 2020-07-01  |\n| Amazon SageMaker Metrics Service                      | Aws::SageMakerMetrics                    | aws-sdk-sagemakermetrics                    | 2022-09-30  |\n| Amazon SageMaker Runtime                              | Aws::SageMakerRuntime                    | aws-sdk-sagemakerruntime                    | 2017-05-13  |\n| Amazon SageMaker Service                              | Aws::SageMaker                           | aws-sdk-sagemaker                           | 2017-07-24  |\n| Amazon SageMaker geospatial capabilities              | Aws::SageMakerGeospatial                 | aws-sdk-sagemakergeospatial                 | 2020-05-27  |\n| Amazon Sagemaker Edge Manager                         | Aws::SagemakerEdgeManager                | aws-sdk-sagemakeredgemanager                | 2020-09-23  |\n| Amazon Security Lake                                  | Aws::SecurityLake                        | aws-sdk-securitylake                        | 2018-05-10  |\n| Amazon Simple Email Service                           | Aws::SES                                 | aws-sdk-ses                                 | 2010-12-01  |\n| Amazon Simple Email Service                           | Aws::SESV2                               | aws-sdk-sesv2                               | 2019-09-27  |\n| Amazon Simple Notification Service                    | Aws::SNS                                 | aws-sdk-sns                                 | 2010-03-31  |\n| Amazon Simple Queue Service                           | Aws::SQS                                 | aws-sdk-sqs                                 | 2012-11-05  |\n| Amazon Simple Storage Service                         | Aws::S3                                  | aws-sdk-s3                                  | 2006-03-01  |\n| Amazon Simple Systems Manager (SSM)                   | Aws::SSM                                 | aws-sdk-ssm                                 | 2014-11-06  |\n| Amazon Simple Workflow Service                        | Aws::SWF                                 | aws-sdk-swf                                 | 2012-01-25  |\n| Amazon SimpleDB                                       | Aws::SimpleDB                            | aws-sdk-simpledb                            | 2009-04-15  |\n| Amazon Textract                                       | Aws::Textract                            | aws-sdk-textract                            | 2018-06-27  |\n| Amazon Timestream Query                               | Aws::TimestreamQuery                     | aws-sdk-timestreamquery                     | 2018-11-01  |\n| Amazon Timestream Write                               | Aws::TimestreamWrite                     | aws-sdk-timestreamwrite                     | 2018-11-01  |\n| Amazon Transcribe Service                             | Aws::TranscribeService                   | aws-sdk-transcribeservice                   | 2017-10-26  |\n| Amazon Transcribe Streaming Service                   | Aws::TranscribeStreamingService          | aws-sdk-transcribestreamingservice          | 2017-10-26  |\n| Amazon Translate                                      | Aws::Translate                           | aws-sdk-translate                           | 2017-07-01  |\n| Amazon VPC Lattice                                    | Aws::VPCLattice                          | aws-sdk-vpclattice                          | 2022-11-30  |\n| Amazon Verified Permissions                           | Aws::VerifiedPermissions                 | aws-sdk-verifiedpermissions                 | 2021-12-01  |\n| Amazon Voice ID                                       | Aws::VoiceID                             | aws-sdk-voiceid                             | 2021-09-27  |\n| Amazon WorkDocs                                       | Aws::WorkDocs                            | aws-sdk-workdocs                            | 2016-05-01  |\n| Amazon WorkLink                                       | Aws::WorkLink                            | aws-sdk-worklink                            | 2018-09-25  |\n| Amazon WorkMail                                       | Aws::WorkMail                            | aws-sdk-workmail                            | 2017-10-01  |\n| Amazon WorkMail Message Flow                          | Aws::WorkMailMessageFlow                 | aws-sdk-workmailmessageflow                 | 2019-05-01  |\n| Amazon WorkSpaces                                     | Aws::WorkSpaces                          | aws-sdk-workspaces                          | 2015-04-08  |\n| Amazon WorkSpaces Thin Client                         | Aws::WorkSpacesThinClient                | aws-sdk-workspacesthinclient                | 2023-08-22  |\n| Amazon WorkSpaces Web                                 | Aws::WorkSpacesWeb                       | aws-sdk-workspacesweb                       | 2020-07-08  |\n| AmazonApiGatewayManagementApi                         | Aws::ApiGatewayManagementApi             | aws-sdk-apigatewaymanagementapi             | 2018-11-29  |\n| AmazonApiGatewayV2                                    | Aws::ApiGatewayV2                        | aws-sdk-apigatewayv2                        | 2018-11-29  |\n| AmazonConnectCampaignService                          | Aws::ConnectCampaignService              | aws-sdk-connectcampaignservice              | 2021-01-30  |\n| AmazonMQ                                              | Aws::MQ                                  | aws-sdk-mq                                  | 2017-11-27  |\n| AmazonMWAA                                            | Aws::MWAA                                | aws-sdk-mwaa                                | 2020-07-01  |\n| AmazonNimbleStudio                                    | Aws::NimbleStudio                        | aws-sdk-nimblestudio                        | 2020-08-01  |\n| AmplifyBackend                                        | Aws::AmplifyBackend                      | aws-sdk-amplifybackend                      | 2020-08-11  |\n| AppFabric                                             | Aws::AppFabric                           | aws-sdk-appfabric                           | 2023-05-19  |\n| Application Auto Scaling                              | Aws::ApplicationAutoScaling              | aws-sdk-applicationautoscaling              | 2016-02-06  |\n| Application Migration Service                         | Aws::Mgn                                 | aws-sdk-mgn                                 | 2020-02-26  |\n| Auto Scaling                                          | Aws::AutoScaling                         | aws-sdk-autoscaling                         | 2011-01-01  |\n| Braket                                                | Aws::Braket                              | aws-sdk-braket                              | 2019-09-01  |\n| CloudWatch Observability Access Manager               | Aws::OAM                                 | aws-sdk-oam                                 | 2022-06-10  |\n| CloudWatch RUM                                        | Aws::CloudWatchRUM                       | aws-sdk-cloudwatchrum                       | 2018-05-10  |\n| CodeArtifact                                          | Aws::CodeArtifact                        | aws-sdk-codeartifact                        | 2018-09-22  |\n| Cost Optimization Hub                                 | Aws::CostOptimizationHub                 | aws-sdk-costoptimizationhub                 | 2022-07-26  |\n| EC2 Image Builder                                     | Aws::Imagebuilder                        | aws-sdk-imagebuilder                        | 2019-12-02  |\n| EMR Serverless                                        | Aws::EMRServerless                       | aws-sdk-emrserverless                       | 2021-07-13  |\n| Elastic Disaster Recovery Service                     | Aws::Drs                                 | aws-sdk-drs                                 | 2020-02-26  |\n| Elastic Load Balancing                                | Aws::ElasticLoadBalancing                | aws-sdk-elasticloadbalancing                | 2012-06-01  |\n| Elastic Load Balancing                                | Aws::ElasticLoadBalancingV2              | aws-sdk-elasticloadbalancingv2              | 2015-12-01  |\n| FinSpace Public API                                   | Aws::FinSpaceData                        | aws-sdk-finspacedata                        | 2020-07-13  |\n| FinSpace User Environment Management service          | Aws::Finspace                            | aws-sdk-finspace                            | 2021-03-12  |\n| Firewall Management Service                           | Aws::FMS                                 | aws-sdk-fms                                 | 2018-01-01  |\n| IAM Roles Anywhere                                    | Aws::RolesAnywhere                       | aws-sdk-rolesanywhere                       | 2018-05-10  |\n| Inspector Scan                                        | Aws::InspectorScan                       | aws-sdk-inspectorscan                       | 2023-08-08  |\n| Inspector2                                            | Aws::Inspector2                          | aws-sdk-inspector2                          | 2020-06-08  |\n| Managed Streaming for Kafka                           | Aws::Kafka                               | aws-sdk-kafka                               | 2018-11-14  |\n| Managed Streaming for Kafka Connect                   | Aws::KafkaConnect                        | aws-sdk-kafkaconnect                        | 2021-09-14  |\n| Migration Hub Strategy Recommendations                | Aws::MigrationHubStrategyRecommendations | aws-sdk-migrationhubstrategyrecommendations | 2020-02-19  |\n| OpenSearch Service Serverless                         | Aws::OpenSearchServerless                | aws-sdk-opensearchserverless                | 2021-11-01  |\n| Payment Cryptography Control Plane                    | Aws::PaymentCryptography                 | aws-sdk-paymentcryptography                 | 2021-09-14  |\n| Payment Cryptography Data Plane                       | Aws::PaymentCryptographyData             | aws-sdk-paymentcryptographydata             | 2022-02-03  |\n| PcaConnectorAd                                        | Aws::PcaConnectorAd                      | aws-sdk-pcaconnectorad                      | 2018-05-10  |\n| QBusiness                                             | Aws::QBusiness                           | aws-sdk-qbusiness                           | 2023-11-27  |\n| Redshift Data API Service                             | Aws::RedshiftDataAPIService              | aws-sdk-redshiftdataapiservice              | 2019-12-20  |\n| Redshift Serverless                                   | Aws::RedshiftServerless                  | aws-sdk-redshiftserverless                  | 2021-04-21  |\n| Route53 Recovery Cluster                              | Aws::Route53RecoveryCluster              | aws-sdk-route53recoverycluster              | 2019-12-02  |\n| Schemas                                               | Aws::Schemas                             | aws-sdk-schemas                             | 2019-12-02  |\n| Service Quotas                                        | Aws::ServiceQuotas                       | aws-sdk-servicequotas                       | 2019-06-24  |\n| Synthetics                                            | Aws::Synthetics                          | aws-sdk-synthetics                          | 2017-10-11  |\n| TrustedAdvisor Public API                             | Aws::TrustedAdvisor                      | aws-sdk-trustedadvisor                      | 2022-09-15  |\n| chatbot                                               | Aws::Chatbot                             | aws-sdk-chatbot                             | 2017-10-11  |\n\n## License\n\nThis library is distributed under the\n[Apache License, version 2.0](http://www.apache.org/licenses/LICENSE-2.0.html)\n\n```no-highlight\ncopyright 2013. amazon web services, inc. all rights reserved.\n\nlicensed under the apache license, version 2.0 (the \"license\");\nyou may not use this file except in compliance with the license.\nyou may obtain a copy of the license at\n\n    http://www.apache.org/licenses/license-2.0\n\nunless required by applicable law or agreed to in writing, software\ndistributed under the license is distributed on an \"as is\" basis,\nwithout warranties or conditions of any kind, either express or implied.\nsee the license for the specific language governing permissions and\nlimitations under the license.\n```\n", "release_dates": ["2020-11-20T19:46:19Z", "2020-11-19T19:59:10Z", "2020-11-18T19:45:37Z", "2020-11-17T19:53:21Z", "2020-11-16T19:44:41Z", "2020-11-13T19:49:48Z", "2020-11-12T19:57:16Z", "2020-11-11T19:47:42Z", "2020-11-10T19:49:32Z", "2020-11-09T17:54:41Z", "2020-11-06T19:47:24Z", "2020-11-05T20:02:36Z", "2020-11-04T20:44:28Z", "2020-11-04T08:04:50Z", "2020-10-30T18:53:49Z", "2020-10-29T19:03:13Z", "2020-10-28T19:06:09Z", "2020-10-27T18:51:57Z", "2020-10-26T18:53:52Z", "2020-10-23T18:51:16Z", "2020-10-22T19:03:39Z", "2020-10-21T21:37:52Z", "2020-10-21T18:53:51Z", "2020-10-20T19:04:26Z", "2020-10-19T19:25:37Z", "2020-10-16T19:06:51Z", "2020-10-15T19:03:43Z", "2020-10-09T18:54:19Z", "2020-10-08T18:57:25Z", "2020-10-07T19:03:02Z"]}, {"name": "aws-sdk-ruby-record", "description": "Official repository for the aws-record gem, an abstraction for Amazon DynamoDB.", "language": "Ruby", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Aws::Record\n\n[![Build Status](https://travis-ci.org/aws/aws-sdk-ruby-record.svg?branch=main)](https://travis-ci.org/aws/aws-sdk-ruby-record) [![Code Climate](https://codeclimate.com/github/aws/aws-sdk-ruby-record.svg)](https://codeclimate.com/github/aws/aws-sdk-ruby-record) [![Coverage Status](https://coveralls.io/repos/github/aws/aws-sdk-ruby-record/badge.svg?branch=main)](https://coveralls.io/github/aws/aws-sdk-ruby-record?branch=main)\n\nA data mapping abstraction over the AWS SDK for Ruby's client for Amazon\nDynamoDB.\n\nThis library is currently under development. More features will be added as we\napproach general availability, and while our initial release has as small of an\nAPI surface area as possible, the interface may change before the GA release.\n\nWe would like to invite you to be a part of the ongoing development of this gem.\nWe welcome your contributions, and would also be happy to hear from you about\nhow you would like to use this gem. Feature requests are welcome.\n\n## Table of Contents\n\n* [Installation](https://github.com/aws/aws-sdk-ruby-record#installation)\n* [Usage](https://github.com/aws/aws-sdk-ruby-record#usage)\n  * [Item Operations](https://github.com/aws/aws-sdk-ruby-record#item-operations) \n  * [BatchGetItem / BatchWriteItem](https://github.com/aws/aws-sdk-ruby-record#batchgetitem-and-batchwriteitem)\n  * [Transactions](https://github.com/aws/aws-sdk-ruby-record#transactions)\n  * [Inheritance Support](https://github.com/aws/aws-sdk-ruby-record#inheritance-support)\n\n## Links of Interest\n\n* [Documentation](http://docs.aws.amazon.com/awssdkrubyrecord/api/)\n* [Change Log](https://github.com/aws/aws-sdk-ruby-record/blob/main/CHANGELOG.md)\n* [Issues](https://github.com/aws/aws-sdk-ruby-record/issues)\n* [License](http://aws.amazon.com/apache2.0/)\n\n---\n## Installation\n\n`Aws::Record` is available as the `aws-record` gem from RubyGems.\n\n```shell\ngem install 'aws-record'\n```\n\n```ruby\ngem 'aws-record', '~> 2.0'\n```\n\nThis automatically includes a dependency on the `aws-sdk-dynamodb` gem (part of the modular version-3 of \nthe [AWS SDK for Ruby](https://aws.amazon.com/sdk-for-ruby/). If you need to pin to a specific version, \nyou can add [aws-sdk-dynamodb](https://rubygems.org/gems/aws-sdk-dynamodb) \nor [aws-sdk-core](https://rubygems.org/gems/aws-sdk-core) gem in your\nGemfile.\n\n---\n## Usage\n\nTo create a model that uses `aws-record` features, simply include the provided\nmodule:\n\n```ruby\nclass MyModel\n  include Aws::Record\nend\n```\n\nYou can then specify attributes using the `aws-record` DSL:\n\n```ruby\nclass MyModel\n  include Aws::Record\n  integer_attr :id, hash_key: true\n  string_attr  :name, range_key: true\n  boolean_attr :active, database_attribute_name: 'is_active_flag'\nend\n```\n\nIf a matching table does not exist in DynamoDB, you can use the TableConfig DSL to create your table:\n\n```ruby\ncfg = Aws::Record::TableConfig.define do |t|\n  t.model_class(MyModel)\n  t.read_capacity_units(5)\n  t.write_capacity_units(2)\nend\ncfg.migrate!\n```\n\nWith a table in place, you can then use your model class to manipulate items in\nyour table:\n\n```ruby\nitem = MyModel.find(id: 1, name: 'Hello Record')\nitem.active = true\nitem.save\nitem.delete!\n\nMyModel.find(id: 1, name: 'Hello Record') # => nil\n\nitem = MyModel.new\nitem.id = 2\nitem.name = 'Item'\nitem.active = false\nitem.save\n```\n---\n### Item Operations\nYou can use item operations on your model class to read and manipulate item(s).\n\nMore info under following documentation:\n* [Item Operations](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/ItemOperations.html)\n* [Item Operations Class Methods](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/ItemOperations/ItemOperationsClassMethods.html)\n\n**Example usage**\n```ruby\nclass MyModel\n  include Aws::Record\n  integer_attr :uuid,   hash_key: true\n  string_attr  :name, range_key: true\n  integer_attr :age\nend\n\nitem = MyModel.find(id: 1, name: 'Foo')\nitem.update(id: 1, name: 'Foo', age: 1)\n```\n\n---\n\n### `BatchGetItem` and `BatchWriteItem`\nAws Record provides [BatchGetItem](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/DynamoDB/Client.html#batch_get_item-instance_method)\nand [BatchWriteItem](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/DynamoDB/Client.html#batch_write_item-instance_method)\nsupport for aws-record models.\n\nMore info under the following documentation:\n\n* [Batch](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/Batch.html)\n* [BatchWrite](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/BatchWrite.html)\n* [BatchRead](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/BatchRead.html)\n\nSee examples below to see the feature in action.\n\n**`BatchGetItem` Example**\n```ruby\nclass Lunch\n  include Aws::Record\n  integer_attr :id,   hash_key: true\n  string_attr  :name, range_key: true\nend\n\nclass Dessert\n  include Aws::Record\n  integer_attr :id,   hash_key: true\n  string_attr  :name, range_key: true\nend\n\n# batch operations\noperation = Aws::Record::Batch.read do |db|\n  db.find(Lunch, id: 1, name: 'Papaya Salad')\n  db.find(Lunch, id: 2, name: 'BLT Sandwich')\n  db.find(Dessert, id: 1, name: 'Apple Pie')\nend\n\n# BatchRead is enumerable and handles pagination\noperation.each { |item| item.id }\n\n# Alternatively, BatchRead provides a lower level interface \n# through: execute!, complete? and items.\n# Unprocessed items can be processed by calling:\noperation.execute! until operation.complete?\n```\n\n**`BatchWriteItem` Example**\n```ruby\nclass Breakfast\n  include Aws::Record\n  integer_attr :id,   hash_key: true\n  string_attr  :name, range_key: true\n  string_attr  :body\nend\n\n# setup\neggs = Breakfast.new(id: 1, name: 'eggs').save!\nwaffles = Breakfast.new(id: 2, name: 'waffles')\npancakes = Breakfast.new(id: 3, name: 'pancakes')\n\n# batch operations\noperation = Aws::Record::Batch.write(client: Breakfast.dynamodb_client) do |db|\n  db.put(waffles)\n  db.delete(eggs)\n  db.put(pancakes)\nend\n\n# unprocessed items can be retried by calling Aws::Record::BatchWrite#execute!\noperation.execute! until operation.complete?\n```\n---\n### Transactions\nAws Record provides [TransactGetItems](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/DynamoDB/Client.html#transact_get_items-instance_method)\nand [TransactWriteItems](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/DynamoDB/Client.html#transact_write_items-instance_method)\nsupport for aws-record models.\n\nMore info under the [Transactions](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/Transactions.html) \ndocumentation.\n\nSee examples below to see the feature in action.\n\n**`TransactGetItems` Example**\n```ruby\nclass TableOne\n  include Aws::Record\n  string_attr :uuid, hash_key: true\nend\n\nclass TableTwo\n  include Aws::Record\n  string_attr :hk, hash_key: true\n  string_attr :rk, range_key: true\nend\n\nresults = Aws::Record::Transactions.transact_find(\n  transact_items: [\n    TableOne.tfind_opts(key: { uuid: 'uuid1234' }),\n    TableTwo.tfind_opts(key: { hk: 'hk1', rk: 'rk1'}),\n    TableTwo.tfind_opts(key: { hk: 'hk2', rk: 'rk2'})\n  ]\n) # => results.responses contains nil or marshalled items\nresults.responses.map { |r| r.class } # [TableOne, TableTwo, TableTwo]\n```\n\n**`TransactWriteItems` Example**\n```ruby\n# same models as `TransactGetItems` Example\ncheck_exp = TableOne.transact_check_expression(\n  key: { uuid: 'foo' },\n  condition_expression: 'size(#T) <= :v',\n  expression_attribute_names: {\n    '#T' => 'body'\n  },\n  expression_attribute_values: {\n    ':v' => 1024\n  }\n)\nnew_item = TableTwo.new(hk: 'hk1', rk: 'rk1', body: 'Hello!')\nupdate_item_1 = TableOne.find(uuid: 'bar')\nupdate_item_1.body = 'Updated the body!'\nput_item = TableOne.new(uuid: 'foobar', body: 'Content!')\nupdate_item_2 = TableTwo.find(hk: 'hk2', rk: 'rk2')\nupdate_item_2.body = 'Update!'\ndelete_item = TableOne.find(uuid: 'to_be_deleted')\n\nAws::Record::Transactions.transact_write(\n  transact_items: [\n    { check: check_exp },\n    { save: new_item },\n    { save: update_item_1 },\n    {\n      put: put_item,\n      condition_expression: 'attribute_not_exists(#H)',\n      expression_attribute_names: { '#H' => 'uuid' },\n      return_values_on_condition_check_failure: 'ALL_OLD'\n    },\n      { update: update_item_2 },\n      { delete: delete_item }\n  ]\n)\n```\n---\n### Inheritance Support\nAws Record models can be extended using standard ruby inheritance. The child model must \ninclude `Aws::Record` in their model and the following will be inherited:\n\n* [set_table_name](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/RecordClassMethods.html#set_table_name-instance_method)\n* [Attributes and Keys](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/Attributes.html#initialize-instance_method)\n* Mutation Tracking:\n  * [enable_mutation_tracking](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/RecordClassMethods.html#enable_mutation_tracking-instance_method)\n  * [disable_mutation_tracking](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/RecordClassMethods.html#disable_mutation_tracking-instance_method)\n* [local_secondary_indexes](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/SecondaryIndexes/SecondaryIndexesClassMethods.html#local_secondary_indexes-instance_method)\n* [global_secondary_indexes](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/SecondaryIndexes/SecondaryIndexesClassMethods.html#global_secondary_indexes-instance_method)\n* [configure_client](https://docs.aws.amazon.com/sdk-for-ruby/aws-record/api/Aws/Record/ClientConfiguration.html#configure_client-instance_method)\n\nSee example below to see the feature in action.\n\n```ruby\nclass Animal\n  include Aws::Record\n  string_attr :name, hash_key: true\n  integer_attr :age\nend\n\nclass Dog < Animal\n  include Aws::Record\n  boolean_attr :family_friendly\nend\n\ndog = Dog.find(name: 'Sunflower')\ndog.age = 3\ndog.family_friendly = true\n```", "release_dates": ["2023-10-17T21:15:45Z", "2023-09-28T20:14:09Z", "2023-06-02T18:02:40Z", "2023-01-17T21:54:56Z", "2023-01-13T16:34:38Z", "2022-11-16T17:54:29Z", "2022-10-12T19:32:27Z", "2021-05-19T20:16:58Z", "2020-10-13T19:49:18Z", "2020-05-29T19:46:41Z", "2019-07-16T22:54:12Z", "2019-02-08T17:33:16Z", "2018-12-05T23:14:28Z", "2018-11-15T17:55:51Z", "2018-07-10T20:55:38Z", "2018-06-25T17:15:32Z", "2018-06-08T16:46:40Z", "2017-08-29T18:24:18Z", "2017-06-16T15:57:24Z", "2017-04-21T16:51:26Z", "2016-12-19T21:03:13Z", "2016-12-02T20:24:31Z", "2016-08-24T19:45:21Z", "2016-08-15T20:01:03Z", "2016-08-03T19:54:47Z", "2016-07-22T22:20:50Z", "2016-05-20T19:54:09Z", "2016-04-21T21:56:38Z", "2016-07-22T22:21:24Z", "2016-07-22T22:21:38Z"]}, {"name": "aws-sdk-ruby-release-tools", "description": null, "language": "Ruby", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Shared Release tools for AWS SDK for Ruby 1P gems\nCommon release scripts/tasks intended to be used as a submodule.\nSee the internal runbook for usage instructions.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "aws-sdk-unity-net", "description": "This is the archive for legacy Unity support. This repository is provided for customers who are using the legacy, Unity-specific binaries. ", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SDK for .NET - Unity Archive\n\nIf you are using Unity 2018.1 or later, please use the [AWS SDK for .NET](https://github.com/aws/aws-sdk-net) .NET Standard 2.0\nbinaries. Doing so will let you use all AWS offerings, use new features as soon as they are available,\nand offers the same support as all other .NET Standard 2.0 platforms.\n\nThis is the archive for legacy Unity support. This repository is provided for customers who are currently using the Unity\nspecific binaries. This repository will not be supported by the AWS SDK for .NET team; use at your own risk. Please\nconsider migrating your app to a newer version of Unity, and use the AWS SDK for .NET - .NET Standard 2.0 binaries.\n\n## Supported Services\n\nThis repository offers Unity support for the following packages:\n\n* [Amazon Cognito](http://aws.amazon.com/cognito/)\n* [Amazon DynamoDB](http://aws.amazon.com/dynamodb/)\n* [AWS Identity and Access Management ](http://aws.amazon.com/)\n* [Amazon Kinesis Streams](https://aws.amazon.com/kinesis/streams/)\n* [AWS Lambda](https://aws.amazon.com/lambda/)\n* [Amazon Mobile Analytics](http://aws.amazon.com/mobileanalytics/)\n* [Amazon Simple Email Service](https://aws.amazon.com/ses/)\n* [Amazon Simple Notification Service](http://aws.amazon.com/sns/)\n* [Amazon Simple Queue Service](https://aws.amazon.com/sqs/)\n* [Amazon Simple Storage Service](http://aws.amazon.com/s3/)\n\n## Supported Unity Version\n\nUnity versions > 4.6\n\nIf you are using Unity 2018.1 or later, please use the [AWS SDK for .NET](https://github.com/aws/aws-sdk-net) .NET Standard 2.0\nbinaries.\n\n## Supported Platforms\n\nThe AWS SDK for .NET (Unity) is currently only supported on Android, iOS and on Standalone platforms.\n\n## Unity SDK Fundamentals\n\nThere are only a few fundamentals that are helpful to know when developing against the AWS SDK for .NET on Unity\n\n* To enable logging you need to create a config file called awsconfig.xml in a `Resources` directory add add the following\n\n\t\t<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\t\t<aws \n\t\t\t<logging\n\t    \t\tlogTo=\"UnityLogger\"\n\t    \t\tlogResponses=\"Always\"\n\t    \t\tlogMetrics=\"true\"\n\t    \t\tlogMetricsFormat=\"JSON\" />\n\t\t\t/>\n\t\t/>\n\t\n  You can also do this configuration in a script\n\n\t\tvar loggingConfig = AWSConfigs.LoggingConfig;\n\t\tloggingConfig.LogTo = LoggingOptions.UnityLogger;\n\t\tloggingConfig.LogMetrics = true;\n\t\tloggingConfig.LogResponses = ResponseLoggingOption.Always;\n\t\tloggingConfig.LogResponsesSizeLimit = 4096;\n\t\tloggingConfig.LogMetricsFormat = LogMetricsFormatOption.JSON;\n\n\n* To Build the SDK from the `AWSSDK.Unity.sln` solution file you will need to either:\n  * Have a Unity install location of C:\\Program Files\\Unity\\\n  * Specify the UnityDataPath msbuild parameter, pointing to the Editor>Data location inside your Unity install.\n\n* The SDK uses reflection for platform specific components. In case of IL2CPP since `strip bytecode` is always enabled on iOS you need to have a `link.xml` in your assembly root with the following entries\n\n\t\t<linker>\n\t\t\t<!-- if you are using AWSConfigs.HttpClient.UnityWebRequest option-->\n\n\t\t<assembly fullname=\"UnityEngine\">\n\t\t\t<type fullname=\"UnityEngine.Networking.UnityWebRequest\" preserve=\"all\" />\n\t\t\t<type fullname=\"UnityEngine.Networking.UploadHandlerRaw\" preserve=\"all\" />\n\t\t\t<type fullname=\"UnityEngine.Networking.UploadHandler\" preserve=\"all\" />\n\t\t\t<type fullname=\"UnityEngine.Networking.DownloadHandler\" preserve=\"all\" />\n\t\t\t<type fullname=\"UnityEngine.Networking.DownloadHandlerBuffer\" preserve=\"all\" />\n\t\t</assembly>\n\t\t\n\t\t<assembly fullname=\"mscorlib\">\n\t\t\t<namespace fullname=\"System.Security.Cryptography\" preserve=\"all\"/>\n   \t\t</assembly>\n\n\t\t<assembly fullname=\"System\">\n\t\t\t<namespace fullname=\"System.Security.Cryptography\" preserve=\"all\"/>\n   \t\t</assembly>\n\n\t\t<assembly fullname=\"AWSSDK.Core\" preserve=\"all\">\n\t\t\t<namespace fullname=\"Amazon.Util.Internal.PlatformServices\" preserve=\"all\"/>\n\t\t</assembly>\n   \t\t<assembly fullname=\"AWSSDK.CognitoIdentity\" preserve=\"all\"/>\n   \t\t<assembly fullname=\"AWSSDK.SecurityToken\" preserve=\"all\"/>\n\t\tadd more services that you need here... \n\t\t</linker>\n", "release_dates": []}, {"name": "aws-secretsmanager-caching-go", "description": "The AWS Secrets Manager Go caching client enables in-process caching of secrets for Go applications.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS Secrets Manager Go Caching Client\n\n[![Go Reference](https://pkg.go.dev/badge/github.com/aws/aws-secretsmanager-caching-go/secretcache.svg)](https://pkg.go.dev/github.com/aws/aws-secretsmanager-caching-go/secretcache)\n[![Tests](https://github.com/aws/aws-secretsmanager-caching-go/actions/workflows/go.yml/badge.svg?event=push)](https://github.com/aws/aws-secretsmanager-caching-go/actions/workflows/go.yml)\n[![codecov](https://codecov.io/gh/aws/aws-secretsmanager-caching-go/branch/master/graph/badge.svg?token=JZxWjXaZOC)](https://codecov.io/gh/aws/aws-secretsmanager-caching-go)\n\nThe AWS Secrets Manager Go caching client enables in-process caching of secrets for Go applications.\n\n## Getting Started\n\n### Required Prerequisites\nTo use this client you must have:\n\n* **A Go development environment**\n\n  If you do not have one, go to [Golang Getting Started](https://golang.org/doc/install) on The Go Programming Language website, then download and install Go.\n\nAn Amazon Web Services (AWS) account to access secrets stored in AWS Secrets Manager and use AWS SDK for Go.\n\n* **To create an AWS account**, go to [Sign In or Create an AWS Account](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html) and then choose **I am a new user.** Follow the instructions to create an AWS account.\n\n* **To create a secret in AWS Secrets Manager**, go to [Creating Secrets](https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_create-basic-secret.html) and follow the instructions on that page.\n\n\n### Get Started\n\nThe following code sample demonstrates how to get started:\n\n1. Instantiate the caching client.\n2. Request secret.\n\n```go\n// This example shows how an AWS Lambda function can be written\n// to retrieve a cached secret from AWS Secrets Manager caching\n// client.\npackage main\n\nimport (\n\t\"github.com/aws/aws-lambda-go/lambda\"\n\t\"github.com/aws/aws-secretsmanager-caching-go/secretcache\"\n)\n\nvar(\n\tsecretCache, _ = secretcache.New()\n)\n\nfunc HandleRequest(secretId string) string {\n\tresult, _ := secretCache.GetSecretString(secretId)\n\t// Use secret to connect to secured resource.\n\treturn \"Success\"\n}\n\nfunc main() {\n\tlambda.Start(HandleRequest)\n}\n```\n\n### Cache Configuration\n* `MaxCacheSize int` The maximum number of cached secrets to maintain before evicting secrets that have not been accessed recently.\n* `CacheItemTTL int64` The number of nanoseconds that a cached item is considered valid before requiring a refresh of the secret state.  Items that have exceeded this TTL will be refreshed synchronously when requesting the secret value.  If the synchronous refresh failed, the stale secret will be returned.\n* `VersionStage string` The version stage that will be used when requesting the secret values for this cache.\n* `Hook CacheHook` Used to hook in-memory cache updates.\n\n#### Instantiating Cache with a custom Config and a custom Client\n```go\n\n\t//Create a custom secretsmanager client\n\tclient := getCustomClient()\n\n\t//Create a custom CacheConfig struct\n\tconfig := secretcache.CacheConfig{\n\t\tMaxCacheSize: secretcache.DefaultMaxCacheSize + 10,\n\t\tVersionStage: secretcache.DefaultVersionStage,\n\t\tCacheItemTTL: secretcache.DefaultCacheItemTTL,\n\t}\n\t\n\t//Instantiate the cache\n\tcache, _ := secretcache.New(\n\t\tfunc(c *secretcache.Cache) { c.CacheConfig = config },\n\t\tfunc(c *secretcache.Cache) { c.Client = client },\n\t)\n```\n\n### Getting Help\nWe use GitHub issues for tracking bugs and caching library feature requests and have limited bandwidth to address them. Please use these community resources for getting help:\n* Ask a question on [Stack Overflow](https://stackoverflow.com/) and tag it with [aws-secrets-manager](https://stackoverflow.com/questions/tagged/aws-secrets-manager).\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home#/)\n* if it turns out that you may have found a bug, please [open an issue](https://github.com/aws/aws-secretsmanager-caching-python/issues/new).\n\n## License\n\nThis library is licensed under the Apache 2.0 License. \n", "release_dates": ["2023-11-15T21:30:09Z", "2023-06-21T21:28:14Z", "2023-06-21T18:40:02Z", "2019-05-06T17:08:38Z"]}, {"name": "aws-secretsmanager-caching-java", "description": "The AWS Secrets Manager Java caching client enables in-process caching of secrets for Java applications.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS Secrets Manager Java caching client\n\n[![build](https://github.com/aws/aws-secretsmanager-caching-java/actions/workflows/maven.yml/badge.svg?branch=master&event=push)](https://github.com/aws/aws-secretsmanager-caching-java/actions/workflows/maven.yml)\n[![coverage](https://codecov.io/gh/aws/aws-secretsmanager-caching-java/branch/master/graph/badge.svg?token=Kk9RDSuKTE)](https://codecov.io/gh/aws/aws-secretsmanager-caching-java)\n\nThe AWS Secrets Manager Java caching client enables in-process caching of secrets for Java applications.\n\n## Getting Started\n\n### Required Prerequisites\nTo use this client you must have:\n\n* **A Java 8 development environment**\n\n  If you do not have one, go to [Java SE Downloads](https://www.oracle.com/technetwork/java/javase/downloads/index.html) on the Oracle website, then download and install the Java SE Development Kit (JDK). Java 8 or higher is recommended.\n\nAn Amazon Web Services (AWS) account to access secrets stored in AWS Secrets Manager and use AWS SDK for Java.\n\n* **To create an AWS account**, go to [Sign In or Create an AWS Account](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html) and then choose **I am a new user.** Follow the instructions to create an AWS account.\n\n* **To create a secret in AWS Secrets Manager**, go to [Creating Secrets](https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_create-basic-secret.html) and follow the instructions on that page.\n\n* **To download and install the AWS SDK for Java**, go to [Installing the AWS SDK for Java](https://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/java-dg-install-sdk.html) in the AWS SDK for Java documentation and then follow the instructions on that page.\n\n### Download\n\nYou can get the latest release from Maven:\n\n```xml\n<dependency>\n  <groupId>com.amazonaws.secretsmanager</groupId>\n  <artifactId>aws-secretsmanager-caching-java</artifactId>\n  <version>2.0.0</version>\n</dependency>\n```\n\nDon't forget to enable the download of snapshot jars from Maven:\n\n```xml\n<profiles>\n  <profile>\n    <id>allow-snapshots</id>\n    <activation><activeByDefault>true</activeByDefault></activation>\n    <repositories>\n      <repository>\n        <id>snapshots-repo</id>\n        <url>https://aws.oss.sonatype.org/content/repositories/snapshots</url>\n        <releases><enabled>false</enabled></releases>\n        <snapshots><enabled>true</enabled></snapshots>\n      </repository>\n    </repositories>\n  </profile>\n</profiles>\n```\n\n### Get Started\n\nThe following code sample demonstrates how to get started:\n\n1. Instantiate the caching client.\n2. Request secret.\n\n```java\n// This example shows how an AWS Lambda function can be written\n// to retrieve a cached secret from AWS Secrets Manager caching\n// client.\npackage com.amazonaws.secretsmanager.caching.examples;\n\nimport com.amazonaws.services.lambda.runtime.Context;\nimport com.amazonaws.services.lambda.runtime.RequestHandler;\nimport com.amazonaws.services.lambda.runtime.LambdaLogger;\n\nimport com.amazonaws.secretsmanager.caching.SecretCache;\n\n/**\n * SampleClass.\n */\npublic class SampleClass implements RequestHandler<String, String> {\n\n    private final SecretCache cache = new SecretCache();\n\n    @Override\n    public String handleRequest(String secretId, Context context) {\n        final String secret = cache.getSecretString(secretId);\n        // Use secret to connect to secured resource.\n        return \"Success!\";\n    }\n}\n```\n\n## License\n\nThis library is licensed under the Apache 2.0 License. \n", "release_dates": ["2023-08-17T20:52:40Z", "2022-02-03T23:10:47Z", "2019-10-30T21:55:58Z", "2018-11-16T21:07:46Z"]}, {"name": "aws-secretsmanager-caching-net", "description": "The AWS Secrets Manager .NET caching client enables in-process caching of secrets for .NET applications.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Secrets Manager Caching Client for .NET\n\n[![NuGet](https://www.fuget.org/packages/AWSSDK.SecretsManager.Caching/badge.svg)](https://www.nuget.org/packages/AWSSDK.SecretsManager.Caching)\n[![.NET](https://github.com/aws/aws-secretsmanager-caching-net/actions/workflows/dotnet.yml/badge.svg?event=push)](https://github.com/aws/aws-secretsmanager-caching-net/actions/workflows/dotnet.yml)\n[![codecov](https://codecov.io/gh/aws/aws-secretsmanager-caching-net/branch/master/graph/badge.svg?token=cugbEh31cw)](https://codecov.io/gh/aws/aws-secretsmanager-caching-net)\n\nThe AWS Secrets Manager caching client enables in-process caching of secrets for .NET applications.\n\n## Required Prerequisites\n\nTo use this client, you must have:\n\n* A .NET project with one of the following:\n    * .NET Framework 4.6.2 or higher\n    * .NET Standard 2.0 or higher\n\n* An Amazon Web Services (AWS) account to access secrets stored in AWS Secrets Manager and use AWS SDK for .NET.\n\n    * **To create an AWS account**, go to [Sign In or Create an AWS Account](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html) and then choose **I am a new user.** Follow the instructions to create an AWS account.\n\n    * **To create a secret in AWS Secrets Manager**, go to [Creating Secrets](https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_create-basic-secret.html) and follow the instructions on that page.\n\n    * **To download and install the AWS SDK for .NET**, go to [Installing the AWS SDK for .NET](https://aws.amazon.com/sdk-for-net/) in the AWS SDK for .NET documentation and then follow the instructions on that page.\n\n## Download\n\nYou can get the latest release from `Nuget`:\n\n```xml\n<ItemGroup>\n     <PackageReference Include=\"AWSSDK.SecretsManager.Caching\" Version=\"1.0.6\" />\n</ItemGroup>\n```\n\n## Getting Started\n\nThe following code sample demonstrates how to start using the caching client:\n\n```cs\nusing System;\nusing Amazon.SecretsManager.Extensions.Caching.SecretsManagerCache;\n\nnamespace LambdaExample {\n    public class CachingExample \n    {\n        private SecretsManagerCache cache = new SecretsManagerCache();\n        private const String MySecretName = \"MySecret\";\n\n        public async Task<Response> FunctionHandlerAsync(String input, ILambdaContext context)\n        {\n            String MySecret = await cache.GetSecretString(MySecretName);\n            ...\n        }\n    }\n}\n```\n\n* After instantiating the cache, retrieve your secret using `GetSecretString` or `GetSecretBinary`. \n* On successive retrievals, the cache will return the cached copy of the secret. \n* Learn more about [AWS Lambda Function Handlers in C#](https://docs.aws.amazon.com/lambda/latest/dg/dotnet-programming-model-handler-types.html).\n\n### Cache Configuration\n\nYou can configure the `SecretCacheConfiguration` object with the following parameters:\n* `CacheItemTTL` - The TTL of a Cache item in milliseconds. The default value is `3600000` ms, or 1 hour.\n* `MaxCacheSize` - The maximum number of items the Cache can contain before evicting using LRU. The default value is `1024`.\n* `VersionStage` - The Version Stage the Cache will request when retrieving secrets from Secrets Manager. The default value is `AWSCURRENT`.\n* `Client` - The Secrets Manager client to be used by the Cache. The default value is `null`, which causes the Cache to instantiate a new Secrets Manager client.\n* `CacheHook` - An implementation of the ISecretCacheHook interface. The default value is `null`.\n\n## Getting Help\nWe use GitHub issues for tracking bugs and caching library feature requests and have limited bandwidth to address them. Please use these community resources for getting help:\n* Ask a question on [Stack Overflow](https://stackoverflow.com/) and tag it with [aws-secrets-manager](https://stackoverflow.com/questions/tagged/aws-secrets-manager).\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home#/).\n* If it turns out that you may have found a bug, please [open an issue](https://github.com/aws/aws-secretsmanager-caching-csharp/issues/new). \n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": ["2023-06-08T17:26:12Z", "2022-10-26T21:30:40Z", "2022-02-22T01:26:44Z", "2020-05-13T22:56:58Z", "2019-05-06T22:46:38Z", "2019-05-06T19:59:39Z", "2019-05-06T16:19:29Z"]}, {"name": "aws-secretsmanager-caching-python", "description": "The AWS Secrets Manager Python caching client enables in-process caching of secrets for Python applications.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS Secrets Manager Python caching client\n\n[![Build](https://github.com/aws/aws-secretsmanager-caching-python/actions/workflows/python-package.yml/badge.svg?event=push)](https://github.com/aws/aws-secretsmanager-caching-python/actions/workflows/python-package.yml)\n[![codecov](https://codecov.io/github/aws/aws-secretsmanager-caching-python/branch/master/graph/badge.svg?token=DkTHUP8lv5)](https://codecov.io/github/aws/aws-secretsmanager-caching-python)\n\nThe AWS Secrets Manager Python caching client enables in-process caching of secrets for Python applications.\n\n## Getting Started\n\n### Required Prerequisites\n\nTo use this client you must have:\n\n* Python 3.7 or newer.  Use of Python versions 3.6 or older are not supported.\n* An Amazon Web Services (AWS) account to access secrets stored in AWS Secrets Manager.\n  * **To create an AWS account**, go to [Sign In or Create an AWS Account](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html) and then choose **I am a new user.** Follow the instructions to create an AWS account.\n\n  * **To create a secret in AWS Secrets Manager**, go to [Creating Secrets](https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_create-basic-secret.html) and follow the instructions on that page.\n\n  * This library makes use of botocore, the low-level core functionality of the boto3 SDK.  For more information on boto3 and botocore, please review the [AWS SDK for Python](https://aws.amazon.com/sdk-for-python/) and [Botocore](https://botocore.amazonaws.com/v1/documentation/api/latest/index.html) documentation. \n\n### Dependencies\nThis library requires the following standard dependencies:\n* botocore\n* setuptools_scm\n\nFor development and testing purposes, this library requires the following additional dependencies:\n* pytest\n* pytest-cov\n* pytest-sugar\n* codecov\n* pylint\n* sphinx\n\nPlease review the `requirements.txt` and `dev-requirements.txt` file for specific version requirements.\n\n### Installation\nInstalling the latest release via **pip**:\n```bash\n$ pip install aws-secretsmanager-caching\n```\n\nInstalling the latest development release:\n```bash\n$ git clone https://github.com/aws/aws-secretsmanager-caching-python.git\n$ cd aws-secretsmanager-caching-python\n$ python setup.py install\n```\n\n### Development\n#### Getting Started\nAssuming that you have Python and virtualenv installed, set up your environment and install the required dependencies like this instead of the `pip install aws_secretsmanager_caching` defined above:\n\n```bash\n$ git clone https://github.com/aws/aws-secretsmanager-caching-python.git\n$ cd aws-secretsmanager-caching-python\n$ virtualenv venv\n...\n$ . venv/bin/activate\n$ pip install -r requirements.txt -r dev-requirements.txt\n$ pip install -e .\n```\n\n#### Running Tests\nYou can run tests in all supported Python versions using tox. By default, it will run all of the unit and integration tests, but you can also specify your own arguments to past to `pytest`.\n```bash\n$ tox # runs integ/unit tests, flake8 tests and pylint tests\n$ tox -- test/unit/test_decorators.py # runs specific test file\n$ tox -e py37 -- test/integ/ # runs specific test directory\n```\n\n#### Documentation\nYou can locally-generate the Sphinx-based documentation via:\n```bash\n$ tox -e docs\n```\nWhich will subsequently be viewable at `file://${CLONE_DIR}/.tox/docs_out/index.html`\n\n### Usage\nUsing the client consists of the following steps:\n1.  Instantiate the client while optionally passing in a `SecretCacheConfig()` object to the `config` parameter.  You can also pass in an existing `botocore.client.BaseClient` client to the client parameter.\n2.  Request the secret from the client instance.\n```python\nimport botocore\nimport botocore.session\nfrom aws_secretsmanager_caching import SecretCache, SecretCacheConfig\n\nclient = botocore.session.get_session().create_client('secretsmanager')\ncache_config = SecretCacheConfig() # See below for defaults\ncache = SecretCache(config=cache_config, client=client)\n\nsecret = cache.get_secret_string('mysecret')\n```\n\n#### Cache Configuration\nYou can configure the cache config object with the following parameters:\n* `max_cache_size` - The maximum number of secrets to cache.  The default value is `1024`.\n* `exception_retry_delay_base` - The number of seconds to wait after an exception is encountered and before retrying the request.  The default value is `1`.\n* `exception_retry_growth_factor` - The growth factor to use for calculating the wait time between retries of failed requests.  The default value is `2`.\n* `exception_retry_delay_max` - The maximum amount of time in seconds to wait between failed requests.  The default value is `3600`.\n* `default_version_stage` - The default version stage to request.  The default value is `'AWSCURRENT'`\n* `secret_refresh_interval` - The number of seconds to wait between refreshing cached secret information.  The default value is `3600.0`.\n* `secret_cache_hook` - An implementation of the SecretCacheHook abstract class.  The default value is `None`.\n\n#### Decorators\nThe library also includes several decorator functions to wrap existing function calls with SecretString-based secrets:\n* `@InjectedKeywordedSecretString` - This decorator expects the secret id and cache as the first and second arguments, with subsequent arguments mapping a parameter key from the function that is being wrapped to a key in the secret.  The secret being retrieved from the cache must contain a SecretString and that string must be JSON-based.\n* `@InjectSecretString` - This decorator also expects the secret id and cache as the first and second arguments.  However, this decorator simply returns the result of the cache lookup directly to the first argument of the wrapped function.  The secret does not need to be JSON-based but it must contain a SecretString.\n```python\nfrom aws_secretsmanager_caching import SecretCache\nfrom aws_secretsmanager_caching import InjectKeywordedSecretString, InjectSecretString\n\ncache = SecretCache()\n\n@InjectKeywordedSecretString(secret_id='mysecret', cache=cache, func_username='username', func_password='password')\ndef function_to_be_decorated(func_username, func_password):\n    print('Something cool is being done with the func_username and func_password arguments here')\n    ...\n\n@InjectSecretString('mysimplesecret', cache)\ndef function_to_be_decorated(arg1, arg2, arg3):\n    # arg1 contains the cache lookup result of the 'mysimplesecret' secret.\n    # arg2 and arg3, in this example, must still be passed when calling function_to_be_decorated().\n```\n\n## Getting Help\nWe use GitHub issues for tracking bugs and caching library feature requests and have limited bandwidth to address them. Please use these community resources for getting help:\n* Ask a question on [Stack Overflow](https://stackoverflow.com/) and tag it with [aws-secrets-manager](https://stackoverflow.com/questions/tagged/aws-secrets-manager).\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home#/)\n* if it turns out that you may have found a bug, please [open an issue](https://github.com/aws/aws-secretsmanager-caching-python/issues/new). \n## License\n\nThis library is licensed under the Apache 2.0 License. \n", "release_dates": ["2021-03-11T02:25:45Z", "2019-05-14T01:42:41Z", "2019-05-06T16:48:53Z", "2019-04-30T19:51:10Z", "2019-04-26T22:07:27Z"]}, {"name": "aws-secretsmanager-jdbc", "description": "The AWS Secrets Manager JDBC Library enables Java developers to easily connect to SQL databases using secrets stored in AWS Secrets Manager.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Secrets Manager JDBC Library\n\n[![Java Build](https://github.com/aws/aws-secretsmanager-jdbc/actions/workflows/CI.yml/badge.svg?event=push)](https://github.com/aws/aws-secretsmanager-jdbc/actions/workflows/CI.yml)\n[![Coverage](https://codecov.io/gh/aws/aws-secretsmanager-jdbc/branch/v2/graph/badge.svg?token=hCl7eBaSwn)](https://codecov.io/gh/aws/aws-secretsmanager-jdbc)\n\nThe **AWS Secrets Manager JDBC Library** enables Java developers to easily connect to SQL databases using secrets stored in AWS Secrets Manager.\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n\n## Features\n\n* Provides wrappers to common JDBC drivers enabling simple database connectivity\n* Provides database connection pooling support through c3p0\n\n## Building from Source\n\nAfter you've downloaded the code from GitHub, you can build it using Maven. To disable GPG signing in the build, use this command: `mvn clean install -Dgpg.skip=true`\n\n## Usage\nThe recommended way to use the SQL Connection Library is to consume it from Maven.  The latest released version can be found at: https://mvnrepository.com/artifact/com.amazonaws.secretsmanager/aws-secretsmanager-jdbc\n\n``` xml\n<dependency>\n    <groupId>com.amazonaws.secretsmanager</groupId>\n    <artifactId>aws-secretsmanager-jdbc</artifactId>\n    <version>2.0.0</version>\n</dependency>\n```\n\nTo use the latest build (pre-release), don't forget to enable the download of snapshot jars from Maven.\n\n``` xml\n<profiles>\n  <profile>\n    <id>allow-snapshots</id>\n    <activation><activeByDefault>true</activeByDefault></activation>\n    <repositories>\n      <repository>\n        <id>snapshots-repo</id>\n        <url>https://aws.oss.sonatype.org/content/repositories/snapshots</url>\n        <releases><enabled>false</enabled></releases>\n        <snapshots><enabled>true</enabled></snapshots>\n      </repository>\n    </repositories>\n  </profile>\n</profiles>\n```\n\n### Usage Example\nWe provide database drivers that intercept calls to real database drivers and swap out secret IDs for actual login credentials.\nThis prevents hard-coding database credentials into your application code. This can be integrated into your app through a few\nconfiguration file changes. Here is an example for making this work with your c3p0 config:\n\n```properties\n# c3p0.properties\n\n# MySQL example\nc3p0.user=secretId\nc3p0.driverClass=com.amazonaws.secretsmanager.sql.AWSSecretsManagerMySQLDriver\nc3p0.jdbcUrl=jdbc-secretsmanager:mysql://example.com:3306\n\n# PostgreSQL example\n# c3p0.user=secretId\n# c3p0.driverClass=com.amazonaws.secretsmanager.sql.AWSSecretsManagerPostgreSQLDriver\n# c3p0.jdbcUrl=jdbc-secretsmanager:postgresql://example.com:5432/database\n\n# Oracle example\n# c3p0.user=secretId\n# c3p0.driverClass=com.amazonaws.secretsmanager.sql.AWSSecretsManagerOracleDriver\n# c3p0.jdbcUrl=jdbc-secretsmanager:oracle:thin:@example.com:1521/ORCL\n\n# MSSQLServer example\n# c3p0.user=secretId\n# c3p0.driverClass=com.amazonaws.secretsmanager.sql.AWSSecretsManagerMSSQLServerDriver\n# c3p0.jdbcUrl=jdbc-secretsmanager:sqlserver://example.com:1433\n```\n\nThe only changes that need to happen in the c3p0 config are to:\n\n* change the jdbc url to one that our driver will intercept (starting with jdbc-secretsmanager),\n* change the c3p0 user to be the secret ID of the secret in secrets manager that has the username and password,\n* and change the `driverClass` to be our driver wrapper.\n\nThe secret being used should be in the JSON format we use for our rotation lambdas for RDS databases. E.g:\n\n```json\n{\n\t\"username\": \"user\",\n\t\"password\": \"pass\",\n\t...\n}\n```\n\n## Credentials\n\nThis library uses the [Default Credential Provider Chain](https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/credentials.html). The following options exist to override some of the defaults:\n\n1) Set a PrivateLink DNS endpoint URL and a region in the secretsmanager.properties file:\n```text\ndrivers.vpcEndpointUrl= #The endpoint URL\ndrivers.vpcEndpointRegion= #The endpoint region\n```\n\n2) Override the primary region by setting the 'AWS_SECRET_JDBC_REGION' environment variable to the preferred region, or via the secretsmanager.properties file:\n```text\ndrivers.region= #The region to use.\n```\n\nIf this driver is running on EKS, the library could pick up the credentials of the node it is running on instead of the service account role ([issue](https://github.com/aws/aws-secretsmanager-jdbc/issues/55)). To address this, add version `2` of `software.amazon.awssdk:sts` to your Gradle/Maven project file as a dependency.\n", "release_dates": ["2024-02-07T22:03:01Z", "2024-02-07T22:02:30Z", "2023-12-06T17:55:10Z", "2023-12-06T17:38:19Z", "2023-08-30T22:16:47Z", "2023-08-30T22:27:36Z", "2023-04-13T17:40:31Z", "2023-01-25T19:37:13Z", "2022-11-17T23:41:59Z", "2022-10-27T16:02:09Z", "2022-05-11T20:26:28Z", "2022-02-03T23:16:39Z", "2020-06-18T00:22:38Z", "2020-03-23T23:32:31Z", "2020-03-19T23:21:02Z", "2019-10-30T21:57:37Z", "2019-05-28T21:58:51Z", "2018-12-05T21:12:42Z", "2018-11-16T21:26:29Z"]}, {"name": "aws-security-services-best-practices", "description": null, "language": "HTML", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# AWS-Security-Services-best-practices\n\n## Welcome\n\nThis is the source for the AWS Security Services Best Practices site. Everyone is welcome to contribute here, not just AWS employees!\n\n## How to run/develop this site\n\nThis site is developed with `mkdocs` which is similar to Hugo with an auto-reload feature. Just save your progress and watch it refresh your browser.\n\n1) To get started with development, make sure you have a current version of `python` with `pip` installed.\n\n2) Install the following packages:\n\n```\npip install mkdocs\npip install mkdocs-material\npip install pymdown-extensions\n```\n\nFor more details or assistance setting up, see:\n* **mkdocs** - https://www.mkdocs.org/user-guide/installation/#installing-mkdocs\n* **mkdocs-material** - https://squidfunk.github.io/mkdocs-material/getting-started/\n* **pymdown-extensions** - https://facelessuser.github.io/pymdown-extensions/installation/\n\n3) Build and run locally on http://127.0.0.1:8000/\n\n```\nmkdocs serve\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.", "release_dates": []}, {"name": "aws-sessionstore-dynamodb-ruby", "description": "Handles sessions for Ruby web applications using DynamoDB as a backend.", "language": "Ruby", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Amazon DynamoDB Session Store\n\nThe **Amazon DynamoDB Session Store** handles sessions for Ruby web applications\nusing a DynamoDB backend. The session store is compatible with all Rack based\nframeworks. For Rails applications, use the [`aws-sdk-rails`][1] gem.\n\n## Installation\n\nFor Rack applications, you can create the Amazon DynamoDB table in a\nRuby file using the following method:\n\n    require 'aws-sessionstore-dynamodb'\n\n    Aws::SessionStore::DynamoDB::Table.create_table\n\nRun the session store as a Rack middleware in the following way:\n\n    require 'aws-sessionstore-dynamodb'\n    require 'some_rack_app'\n\n    options = { :secret_key => 'SECRET_KEY' }\n\n    use Aws::SessionStore::DynamoDB::RackMiddleware.new(options)\n    run SomeRackApp\n\nNote that `:secret_key` is a mandatory configuration option that must be set.\n\n## Detailed Usage\n\nThe session store is a Rack Middleware, meaning that it will implement the Rack\ninterface for dealing with HTTP request/responses.\n\nThis session store uses a DynamoDB backend in order to provide scaling and\ncentralized data benefits for session storage with more ease than other\ncontainers, like local servers or cookies. Once an application scales beyond\na single web server, session data will need to be shared across the servers.\nDynamoDB takes care of this burden for you by scaling with your application.\nCookie storage places all session data on the client side,\ndiscouraging sensitive data storage. It also forces strict data size\nlimitations. DynamoDB takes care of these concerns by allowing for a safe and\nscalable storage container with a much larger data size limit for session data.\n\nFor more developer information, see the [Full API documentation][2].\n\n### Configuration Options\n\nA number of options are available to be set in\n`Aws::SessionStore::DynamoDB::Configuration`, which is used by the\n`RackMiddleware` class. These options can be set directly by Ruby code or\nthrough environment variables.\n\nThe full set of options along with defaults can be found in the\n[Configuration class documentation][3].\n\n#### Environment Options\n\nCertain configuration options can be loaded from the environment. These\noptions must be specified in the following format:\n\n    DYNAMO_DB_SESSION_NAME-OF-CONFIGURATION-OPTION\n\nThe example below would be a valid way to set the session table name:\n\n    export DYNAMO_DB_SESSION_TABLE_NAME='sessions'\n\n### Garbage Collection\n\nYou may want to delete old sessions from your session table. You can use the\nDynamoDB [Time to Live (TTL) feature][4] on the `expire_at` attribute to\nautomatically delete expired items.\n\nIf you want to take other attributes into consideration for deletion, you could\ninstead use the `GarbageCollection` class. You can create your own Rake task for\ngarbage collection similar to below:\n\n    require \"aws-sessionstore-dynamodb\"\n\n    desc 'Perform Garbage Collection'\n    task :garbage_collect do |t|\n     options = {:max_age => 3600*24, max_stale => 5*3600 }\n     Aws::SessionStore::DynamoDB::GarbageCollection.collect_garbage(options)\n    end\n\nThe above example will clear sessions older than one day or that have been\nstale for longer than an hour.\n\n### Locking Strategy\n\nYou may want the Session Store to implement the provided pessimistic locking\nstrategy if you are concerned about concurrency issues with session accesses.\nBy default, locking is not implemented for the session store. You must trigger\nthe locking strategy through the configuration of the session store. Pessimistic\nlocking, in this case, means that only one read can be made on a session at\nonce. While the session is being read by the process with the lock, other\nprocesses may try to obtain a lock on the same session but will be blocked.\n\nLocking is expensive and will drive up costs depending on how it is used.\nWithout locking, one read and one write are performed per request for session\ndata manipulation. If a locking strategy is implemented, as many as the total\nmaximum wait time divided by the lock retry delay writes to the database.\nKeep these considerations in mind if you plan to enable locking.\n\n#### Configuration for Locking\n\nThe following configuration options will allow you to configure the pessimistic\nlocking strategy according to your needs:\n\n    options = {\n      :enable_locking => true,\n      :lock_expiry_time => 500,\n      :lock_retry_delay => 500,\n      :lock_max_wait_time => 1\n    }\n\n### Error Handling\n\nYou can pass in your own error handler for raised exceptions or you can allow\nthe default error handler to them for you. See the API documentation\non the {Aws::SessionStore::DynamoDB::Errors::BaseHandler} class for more\ndetails.\n\n[1]: https://github.com/aws/aws-sdk-rails/\n[2]: https://docs.aws.amazon.com/sdk-for-ruby/aws-sessionstore-dynamodb/api/\n[3]: https://docs.aws.amazon.com/sdk-for-ruby/aws-sessionstore-dynamodb/api/Aws/SessionStore/DynamoDB/Configuration.html\n[4]: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\n", "release_dates": ["2024-01-25T18:39:51Z", "2023-06-02T17:51:25Z", "2020-11-16T21:27:40Z", "2020-11-11T20:12:01Z", "2017-08-14T20:46:05Z"]}, {"name": "aws-sigv4-auth-cassandra-gocql-driver-plugin", "description": "A SigV4 authentication plugin for the open-source Gocql Driver for Apache Cassandra. Allows use of IAM users and roles", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# IMPORTANT: Latest Version\n\nThe current version is 1.0.0. Please see the [changelog](./CHANGELOG.md) for details on version history.\n\n# What\n\nThis package implements an authentication plugin for the open-source [Gocql Driver](https://github.com/gocql/gocql) for Apache Cassandra. The driver enables you to add authentication information to your API requests using the AWS Signature Version 4 Process (SigV4). Using the plugin, you can provide users and applications short-term credentials to access Amazon Keyspaces (for Apache Cassandra) using AWS Identity and Access Management (IAM) users and roles.\n\nThe plugin depends on the AWS SDK for Go. It uses the default credential provider chain to obtain credentials.\n\nYou must specify the service endpoint to use for the connection. You can provide the region as a function argument programmatically or via the `AWS_DEFAULT_REGION` environment variable.\n\nThe full documentation for the plugin is available at AWS Docs:\n[Creating Credentials to Access Amazon Keyspaces](https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.credentials.html#programmatic.credentials.SigV4_KEYSPACES)\n\n# Using the Plugin\nThe following sections describe how to use the authentication plugin for the open-source gocql Driver for Cassandra to access Amazon Keyspaces.\n\nTo install the plugin:\n```bash\n$ go get github.com/aws/aws-sigv4-auth-cassandra-gocql-driver-plugin\n```\n\n## SSL Configuration\n\nAmazon Keyspaces requires the use of Transport Layer Security (TLS) to help secure connections with clients. To connect to Amazon Keyspaces using TLS, you need to download an Amazon digital certificate and configure the Go driver to use TLS.\n\nDownload the Starfield digital certificate using the following command and save sf-class2-root.crt locally or in your home directory.\n\n```\ncurl https://certs.secureserver.net/repository/sf-class2-root.crt -O\n```\n\n## Region Configuration\n\nBefore you can start using the plugin, you must configure the AWS Region that the plugin will use when authenticating.  This is required because SigV4 signatures are Region-specific.  For example, if you are connecting to the `cassandra.us-east-2.amazonaws.com` endpoint,  the Region must be `us-east-2`.  For a list of available AWS Regions and endpoints, see [Service Endpoints for Amazon Keyspaces](https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.endpoints.html).\n\nYou can specify the Region using one of the following four methods:\n\n* Environment Variable\n* Configuration\n* Function Argument\n\n### Environment Variable\nYou can use the `AWS_DEFAULT_REGION` environment variable to match the endpoint that you are communicating with by setting it as part of your application start-up.\nIf `AWS_DEFAULT_REGION` environment variable is not set, falls back to the `AWS_REGION` environment variable.\n```\n$ export AWS_DEFAULT_REGION=us-east-1\n```\n\n### Function Argument\n\nOne of the functions takes a String representing the Region as an argument, that will be used for that instance.\n\n```go\nfunc NewAwsAuthenticatorWithRegion(region string) AwsAuthenticator {\n\n}\n```\n\n## How to use the Authentication Plugin\n\nWhen using the open-source gocql driver, the connection to your Amazon Keyspaces endpoint is represented by the `Cluster` class.\nSimply use AwsAuthenticator for the authenticator property of the cluster.\n\nHere is a simple example of use:\n\n```go\npackage main\n\nimport (\n        \"fmt\"\n        \"github.com/aws/aws-sigv4-auth-cassandra-gocql-driver-plugin/sigv4\"\n        \"github.com/gocql/gocql\"\n        \"log\"\n)\n\nfunc main() {\n\t// configuring the cluster options\n\tcluster := gocql.NewCluster(\"cassandra.us-west-2.amazonaws.com:9142\")\n\tvar auth sigv4.AwsAuthenticator = sigv4.NewAwsAuthenticator()\n\tauth.Region = \"us-west-2\"\n\tauth.AccessKeyId = \"AKIAIOSFODNN7EXAMPLE\"\n\tauth.SecretAccessKey = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" \n\n\tcluster.Authenticator = auth\n\n\tcluster.SslOpts = &gocql.SslOptions{\n\t\tCaPath: \"/Users/user1/.cassandra/AmazonRootCA1.pem\",\n\t}\n\tcluster.Consistency = gocql.LocalQuorum\n\tcluster.DisableInitialHostLookup = true\n\n\tsession, err := cluster.CreateSession()\n\tif err != nil {\n\t\tfmt.Println(\"err>\", err)\n\t\treturn\n\t}\n\tdefer session.Close()\n\n\t// doing the query\n\tvar text string\n\titer := session.Query(\"SELECT keyspace_name FROM system_schema.tables;\").Iter()\n\tfor iter.Scan(&text) {\n\t\tfmt.Println(\"keyspace_name:\", text)\n\t}\n\tif err := iter.Close(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n```\n\nWhen using AwsAuthenticator from an AWS Lambda function, the Lambda runtimes will initialize all the needed environment variables.\nAll you need to do is assign the authenticator.\n\n```go\n\tcluster.Authenticator = sigv4.NewAwsAuthenticator()\n```\n", "release_dates": []}, {"name": "aws-sigv4-auth-cassandra-java-driver-plugin", "description": "A SigV4 authentication plugin for the open-source DataStax Java Driver for Apache Cassandra.  Allows use of AWS IAM users and roles for direct authentication.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# IMPORTANT: Latest Version\n\nThe current version is 4.0.9. Please see the [changelog](./CHANGELOG.md) for details on version history.\n\n# What\n\nThis package implements an authentication plugin for the open-source Datastax Java Driver for Apache Cassandra. The driver enables you to add authentication information to your API requests using the AWS Signature Version 4 Process (SigV4). Using the plugin, you can provide users and applications short-term credentials to access Amazon Keyspaces (for Apache Cassandra) using AWS Identity and Access Management (IAM) users and roles.\n\nThe plugin depends on the AWS SDK for Java. It uses `AWSCredentialsProvider` to obtain credentials. Because the IAuthenticator interface operates at the level of `InetSocketAddress`, you must specify the service endpoint to use for the connection.\nYou can provide the Region in the constructor programmatically, via the `AWS_REGION` environment variable, or via the `aws.region` system property.\nYou can also provide an IAM role to assume for access to KeySpaces, programmatically or via the configuration file.\n\nThe full documentation for the plugin is available at\nhttps://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.credentials.html#programmatic.credentials.SigV4_KEYSPACES.\n\n# Example Usage\n\nFor example code, see https://github.com/aws-samples/aws-sigv4-auth-cassandra-java-driver-examples.\n\n# Using the Plugin\n\nThe following sections describe how to use the authentication plugin for the open-source DataStax Java Driver for Cassandra to access Amazon Keyspaces.\n\n## SSL Configuration\n\nThe first step is to get an Amazon digital certificate to encrypt your connections using Transport Layer Security (TLS). The DataStax Java driver must use an SSL trust store so that the client SSL engine can validate the Amazon Keyspaces certificate on connection. To use the trust store and create a certificate, see [Using a Cassandra Java Client Driver to Access Amazon Keyspaces Programmatically](https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.drivers.html#using_java_driver).\n\n## Region Configuration\n\nBefore you can start using the plugin, you must configure the AWS Region that the plugin will use when authenticating. This is required because SigV4 signatures are Region-specific. For example, if you are connecting to the `cassandra.us-east-2.amazonaws.com` endpoint, the Region must be `us-east-2`. For a list of available AWS Regions and endpoints, see [Service Endpoints for Amazon Keyspaces](https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.endpoints.html).\n\nYou can specify the Region using one of the following four methods:\n\n* Environment Variable\n* System Property\n* Constructor\n* Configuration\n\n### Environment Variable\n\nYou can use the `AWS_REGION` environment variable to match the endpoint that you are communicating with by setting it as part of your application start-up, as follows.\n\n``` shell\n$ export AWS_Region=us-east-1\n```\n### System Property\n\nYou can use the `aws.region` Java system property by specifying it on the command line, as follows.\n\n``` shell\n$ java -Daws.region=us=east-1 ...\n```\n\n### Constructor\n\nOne of the constructors for `software.aws.mcs.auth.SigV4AuthProvider` takes a `String` representing the Region that will be used for that instance.\n\n### Configuration\n\nSet the Region explicitly in your `advanced.auth-provider` configuration (see example below), by specifying the `advanced.auth-provider.aws-region` property.\n\n## Assume IAM Role Configuration\n\nYou can specify an IAM role to assume for access to KeySpaces using either the constructor or the driver configuration file\n\n### Constructor\n\nOne of the constructors for `software.aws.mcs.auth.SigV4AuthProvider` takes two Strings , the first representing the region and the second representing the ARN of the IAM role to assume. \n\n### Configuration\n\nSet the IAM Role explicitly in your `advanced.auth-provider` configuration (see example below), by specifying the `advanced.auth-provider.aws-role-arn` property.\n\n## Add the Authentication Plugin to the Application\n\nThe authentication plugin supports version 4.x of the DataStax Java Driver for Cassandra.\n\n### With Maven/Ivy\n\nIf you\u2019re using Apache Maven, or a build system that can use Maven dependencies, add the following dependencies to your `pom.xml` file.\n\n``` xml\n<dependency>\n    <groupId>software.aws.mcs</groupId>\n    <artifactId>aws-sigv4-auth-cassandra-java-driver-plugin</artifactId>\n    <version>4.0.6</version>\n</dependency>\n```\n\n### Download the Shaded JAR\n\nIf you just need the JAR to use with a third party tool, please use the shaded JAR (includes the SDK and other\ndependencies) located in the [releases](https://github.com/aws/aws-sigv4-auth-cassandra-java-driver-plugin/releases)\nsection on GitHub.\n\n## How to use the Authentication Plugin\n\nWhen using the open-source DataStax Java driver, the connection to your Amazon Keyspaces endpoint is represented by the `CqlSession` class. To create the `CqlSession`, you can either configure it programmatically using the `CqlSessionBuilder` class (accessed via `CqlSession.builder()`) or with the configuration file.\n\n### Programmatically Configure the Driver\n\nWhen using the DataStax Java driver, you interact with Amazon Keyspaces primarily through the `CQLSession` class. You can create an instance of `CqlSession` using the `CqlSession.builder()` function. `CqlSession.builder()` enables you to specify another authentication provider for the session by using the with `withAuthProvider` function.\n\nTo use the authentication plugin, you set a Region-specific instance of SigV4AuthProvider as the authentication provider, as in the following example.\n\n1. Call `addContactPoints` on the builder with a collection of `java.net.InetSocketAddress` instances corresponding to the endpoints for your Region. Contact points are the endpoints that the driver will connect to. For a full list of endpoints and Regions in the documentation, see [Service Endpoints for Amazon Keyspaces](https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.endpoints.html).\n1. Add an SSL context by calling `withSslContext` on the builder. This uses the trust store defined previously to negotiate SSL on the connection to the endpoints. SSL is required for Amazon Keyspaces. Without this setting, connections will time out and fail.\n1. Set the local data center to the region name, in this example it is `us-east-2`. The local data center is used by the driver for routing of requests, and it is required when the builder is constructed with `addContactPoints`.\n1. Set the authentication provider to a new instance of `software.aws.mcs.auth.SigV4AuthProvider`. The `SigV4AuthProvider` is the authentication handler provided by the plugin for performing SigV4 authentication. You can specify the Region for the endpoints that you\u2019re using in the constructor for `SigV4AuthProvider`, as in the following example. Or, you can set the environment variable or system property as shown previously.\n\nThe following code example demonstrates the previous steps.\n\n``` java\n    List<InetSocketAddress> contactPoints =\n      Collections.singletonList(\n       InetSocketAddress.createUnresolved(\"cassandra.us-east-2.amazonaws.com\", 9142));\n\n    try (CqlSession session = CqlSession.builder()\n      .addContactPoints(contactPoints)\n      .withSslContext(SSLContext.getDefault())\n      .withLocalDatacenter(\"us-east-2\")\n      .withAuthProvider(new SigV4AuthProvider(\"us-east-2\"))\n      .build()) {\n      // App code here...\n    }\n```\n\n### Use a Configuration File\n\nTo use the configuration file, set the `advanced.auth-provider.class` to `software.aws.mcs.auth.SigV4AuthProvider`. You can also set the region, local data center and enable SSL in the configuration.\n\n1. Set the `advanced.auth-provider.class` to `software.aws.mcs.auth.SigV4AuthProvider`.\n1. Set `basic.load-balancing-policy.local-datacenter` to the region name. In this case, use `us-east-2`.\n\nThe following is an example of this config without explicit role to be assumed. \n\n``` text\n    datastax-java-driver {\n        basic.load-balancing-policy {\n            class = DefaultLoadBalancingPolicy\n            local-datacenter = us-east-2\n        }\n        advanced {\n            auth-provider = {\n                class = software.aws.mcs.auth.SigV4AuthProvider\n                aws-region = us-east-2\n            }\n            ssl-engine-factory {\n                class = DefaultSslEngineFactory\n            }\n        }\n    }\n```\n\nThe following is an example of this config with an explicit role to be assumed.\n\n``` text\n    datastax-java-driver {\n        basic.load-balancing-policy {\n            class = DefaultLoadBalancingPolicy\n            local-datacenter = us-east-2\n        }\n        advanced {\n            auth-provider = {\n                class = software.aws.mcs.auth.SigV4AuthProvider\n                aws-region = us-east-2\n                aws-role-arn = \"arn:aws:iam::ACCOUNT_ID:role/ROLE_NAME\"\n            }\n            ssl-engine-factory {\n                class = DefaultSslEngineFactory\n            }\n        }\n    }\n```", "release_dates": ["2022-09-16T21:19:11Z", "2021-01-20T20:35:53Z"]}, {"name": "aws-sigv4-auth-cassandra-nodejs-driver-plugin", "description": "A SigV4 authentication client side plugin for the open-source DataStax NodeJS Driver for Apache Cassandra. Allows use of IAM users and roles.", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# IMPORTANT: Latest Version\n\nThe current version is 1.0.5. Please see the [changelog](./CHANGELOG.md) for details on version history.\n\n# What\n\nThis package implements an authentication plugin for the open-source Datastax NodeJS Driver for Apache Cassandra. The driver enables you to add authentication information to your API requests using the AWS Signature Version 4 Process (SigV4). Using the plugin, you can provide users and applications short-term credentials to access Amazon Keyspaces (for Apache Cassandra) using AWS Identity and Access Management (IAM) users and roles.\n\nThe plugin depends on the AWS SDK for NodeJS. It uses `AWSCredentialsProvider` to obtain credentials. You must specify the service endpoint to use for the connection.\nYou can provide the Region in the constructor programmatically, via the `AWS_REGION` environment variable.\n\nThe full documentation for the plugin is available at\n[Amazon Keyspaces AWS Docs](https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.credentials.html#programmatic.credentials.SigV4_KEYSPACES).\n\n\n# Using the Plugin\n\nThe following sections describe how to use the authentication plugin for the open-source DataStax NodeJS Driver for Cassandra to access Amazon Keyspaces.\n\n## SSL Configuration\n\nThe first step is to get an Amazon digital certificate to encrypt your connections using Transport Layer Security (TLS). The DataStax NodeJS driver must use an SSL trust store so that the client SSL engine can validate the Amazon Keyspaces certificate on connection. To use the trust store and create a certificate, see [Using a Cassandra Java Client Driver to Access Amazon Keyspaces Programmatically](https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.drivers.html#using_java_driver).\n\n## Region Configuration\n\nBefore you can start using the plugin, you must configure the AWS Region that the plugin will use when authenticating. This is required because SigV4 signatures are Region-specific. For example, if you are connecting to the `cassandra.us-east-2.amazonaws.com` endpoint, the Region must be `us-east-2`. For a list of available AWS Regions and endpoints, see [Service Endpoints for Amazon Keyspaces](https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.endpoints.html).\n\nYou can specify the Region using one of the following four methods:\n\n* Environment Variable\n* System Property\n* Constructor\n* Configuration\n\n## Environment Variable\n\nYou can use the `AWS_REGION` environment variable to match the endpoint that you are communicating with by setting it as part of your application start-up, as follows.\n\n``` shell\n$ export AWS_Region=us-east-1\n```\n\n## Add the Authentication Plugin to the Application\n\nThe authentication plugin supports version 4.x of the DataStax NodeJS Driver for Cassandra. To add this application use \n\n```bash\n$ npm install aws-sigv4-auth-cassandra-plugin --save\n```\n\n## How to use the Authentication Plugin\n\nWhen using the open-source DataStax NodeJS driver, the connection to your Amazon Keyspaces endpoint is represented by the `Client` class. \n\n### Programmatically Configure the Driver\n\nWhen using the DataStax NodeJS driver, you interact with Amazon Keyspaces primarily through the `Client` class.\n\nTo use the authentication plugin, you set a Region-specific instance of SigV4AuthProvider as the authentication provider, as in the following example.\n\n1. Create a `SigV4AuthProvider` from plugin.\n1. Add an SSL context using `AmazonRootCA1.pem` ssl and Keyspaces endpoint. \n1. Set the local data center to the region name, in this example it is `us-west-2`. \nThe local data center is used by the driver for routing of requests, and it is required when the builder is constructed with `addContactPoints`.\n1. Set the authentication provider to a new instance of the `SigV4AuthProvider`.\nYou can specify the Region for the endpoints that you\u2019re using in the constructor for `SigV4AuthProvider`, as in the following example. \nOr, you can set the environment variable or system property as shown previously.\n\nThe following code example demonstrates the previous steps.\n\n``` js\nconst cassandra = require('cassandra-driver');\nconst fs = require('fs');\nconst sigV4 = require('aws-sigv4-auth-cassandra-plugin');\n\nconst auth = new sigV4.SigV4AuthProvider({\n    region: 'us-west-2', \n    accessKeyId:'AKIAIOSFODNN7EXAMPLE',\n    secretAccessKey: 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'});\n\nconst sslOptions1 = {\n  ca: [\n      fs.readFileSync('~/.cassandra/AmazonRootCA1.pem', 'utf-8')],\n  host: 'cassandra.us-west-2.amazonaws.com',\n  rejectUnauthorized: true\n};\n\n\nconst client = new cassandra.Client({\n  contactPoints: ['cassandra.us-west-2.amazonaws.com'],\n  localDataCenter: 'us-west-2',\n  authProvider: auth,\n  sslOptions: sslOptions1,\n  protocolOptions: { port: 9142 }\n});\n\n\nconst query = 'SELECT * FROM system_schema.keyspaces';\n\nclient.execute(query).then(\n    result => console.log('Row from Keyspaces %s', result.rows[0]))\n    .catch( e=> console.log(`${e}`));\n```\n", "release_dates": []}, {"name": "aws-sigv4-auth-cassandra-python-driver-plugin", "description": "A SigV4 authentication plugin for the open-source Python Driver for Apache Cassandra. Allows use of IAM users and roles.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# IMPORTANT: Latest Version\n\nThe current version is 4.0.2. Please see the [changelog](./CHANGELOG.md) for details on version history.\n\n# What\n\nThis package implements an authentication plugin for the open-source Datastax Python Driver for Apache Cassandra.\nThe driver enables you to add authentication information to your API requests using the AWS Signature Version 4 Process (SigV4).\nUsing the plugin, you can provide users and applications short-term credentials to access Amazon Keyspaces (for Apache Cassandra)\nusing AWS Identity and Access Management (IAM) users and roles.\n\nThe plugin depends on the AWS SDK for Python (Boto3). It uses `boto3.Session` to obtain credentials.\n\n\n# Example Usage\n\n``` python\nssl_context = SSLContext(PROTOCOL_TLSv1_2)\nssl_context.load_verify_locations('./AmazonRootCA1.pem')\nssl_context.verify_mode = CERT_REQUIRED\nboto_session = boto3.Session(aws_access_key_id=\"AKIAIOSFODNN7EXAMPLE\",\n                             aws_secret_access_key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n                             aws_session_token=\"AQoDYXdzEJr...<remainder of token>\",\n                             region_name=\"us-east-2\")\nauth_provider = SigV4AuthProvider(boto_session)\ncluster = Cluster(['cassandra.us-east-2.amazonaws.com'], ssl_context=ssl_context, auth_provider=auth_provider,\n                  port=9142)\nsession = cluster.connect()\nr = session.execute('select * from system_schema.keyspaces')\nprint(r.current_rows)\n```\n\n# Using the Plugin\n\nThe following sections describe how to use the authentication plugin for the open-source DataStax Python Driver for Cassandra to access Amazon Keyspaces.\n\n## SSL Configuration\n\nThe first step is to get an Amazon digital certificate to encrypt your connections using Transport Layer Security (TLS).\nThe DataStax Python driver must use an SSL CA certificate so that the client SSL engine can validate the Amazon Keyspaces\ncertificate on connection.\n\n``` python\nssl_context = SSLContext(PROTOCOL_TLSv1_2)\nssl_context.load_verify_locations('./AmazonRootCA1.pem')\nssl_context.verify_mode = CERT_REQUIRED\n```\n\n## Region Configuration\n\nBefore you can start using the plugin, you must configure the AWS Region that the plugin will use when authenticating.\nThis is required because SigV4 signatures are Region-specific. For example, if you are connecting to the `cassandra.us-east-2.amazonaws.com` endpoint,\nthe Region must be `us-east-2`. For a list of available AWS Regions and endpoints, see [Service Endpoints for Amazon Keyspaces](https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.endpoints.html).\n\nYou can specify the Region using one of the following four methods:\n\n* Environment Variable\n* Constructor\n* Boto3 Session Configuration\n\n## Environment Variable\n\nYou can use the `AWS_DEFAULT_REGION` environment variable to match the endpoint that you are\ncommunicating with by setting it as part of your application start-up, as follows.\n\n``` shell\n$ export AWS_DEFAULT_REGION=us-east-2\n```\n\n## Constructor\n\nYou can either provide the constructor for `SigV4AuthProvider` with a boto3 session, aws credentials and a region,\nor a parameterless constructor to follow the default boto3 credential discovery path.\n\n## Install the plugin in your environment\n\n``` shell\npip install cassandra-sigv4\n```\n\n## Programmatically Configure the Driver With a boto3 session\n\nNote that if a session is provided, all other arguments for the constructor are ignored.\n\n``` python\nboto_session = boto3.Session(aws_access_key_id=\"AKIAIOSFODNN7EXAMPLE\",\n                             aws_secret_access_key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n                             aws_session_token=\"AQoDYXdzEJr...<remainder of token>\",\n                             region_name=\"us-east-2\")\nauth_provider = SigV4AuthProvider(boto_session)\ncluster = Cluster(['cassandra.us-east-2.amazonaws.com'], ssl_context=ssl_context, auth_provider=auth_provider,\n                  port=9142)\n```\n\n## Programmatically Configure the Drive with raw AWS Credentials\n\n``` python\nauth_provider = SigV4AuthProvider(aws_access_key_id=\"AKIAIOSFODNN7EXAMPLE\",\n                                  aws_secret_access_key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n                                  aws_session_token=\"AQoDYXdzEJr...<remainder of token>\",\n                                  region_name=\"us-east-2\")\ncluster = Cluster(['cassandra.us-east-2.amazonaws.com'], ssl_context=ssl_context, auth_provider=auth_provider,\n                  port=9142)\n```\n", "release_dates": []}, {"name": "aws-ssm-data-protection-provider-for-aspnet", "description": "An extension library to assist with ASP.NET data protection in AWS Lambda.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![.NET on AWS Banner](./logo.png \".NET on AWS\")\n\n# AWS Systems Manager ASP.NET Core Data Protection Provider\n[![nuget](https://img.shields.io/nuget/v/Amazon.AspNetCore.DataProtection.SSM.svg)](https://www.nuget.org/packages/Amazon.AspNetCore.DataProtection.SSM/)\n\n[Amazon.AspNetCore.DataProtection.SSM](https://www.nuget.org/packages/Amazon.AspNetCore.DataProtection.SSM/) allows you to use [AWS Systems Manager](https://aws.amazon.com/systems-manager/)'s [Parameter Store](https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html) to store keys generated by ASP.NET's Data Protection API.\nThis enables you to scale by allowing multiple web servers to share the keys.\n\nThe library introduces the following dependencies:\n\n* [AWSSDK.SimpleSystemsManagement](https://www.nuget.org/packages/AWSSDK.SimpleSystemsManagement/)\n* [Microsoft.AspNetCore.DataProtection.Extensions](https://www.nuget.org/packages/Microsoft.AspNetCore.DataProtection.Extensions/)\n\n# Getting Started\n\nFollow the examples below to see how the library can be integrated into your application.\n\n## ASP.NET Core Example\n```csharp\npublic void ConfigureServices(IServiceCollection services)\n{\n    services.AddDataProtection()\n        .PersistKeysToAWSSystemsManager(\"/MyApplication/DataProtection\");\n\n    services.AddMvc();\n}\n```\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues\nfor tracking bugs and feature requests.\n\n* Open a support ticket with [AWS Support](https://console.aws.amazon.com/support/home#/)\n* If it turns out that you may have found a bug,\n  please open an [issue](https://github.com/aws/aws-ssm-data-protection-provider-for-aspnet/issues/new)\n\n## Permissions\n\nThe AWS credentials used must have access to the **ssm:PutParameter** and **ssm:GetParametersByPath** \nservice operations from AWS System Manager. Below is an example IAM policy\nfor those actions.\n\n```javascript\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"rule1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ssm:PutParameter\",\n                \"ssm:GetParametersByPath\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n```\n\nIf the `KMSKeyId` property is set during the `PersistKeysToAWSSystemsManager` method then the IAM Policy\nwill also need access to **kms:Encrypt** for the KMS key used.\n\n\n\n## Contributing\n\nWe welcome community contributions and pull requests. See\n[CONTRIBUTING](./CONTRIBUTING.md) for information on how to set up a development\nenvironment and submit code.\n\n# Additional Resources\n\n[AWS Developer Center - Explore .NET on AWS](https://aws.amazon.com/developer/language/net/)\nFind all the .NET code samples, step-by-step guides, videos, blog content, tools, and information about live events that you need in one place.\n\n[AWS Developer Blog - .NET](https://aws.amazon.com/blogs/developer/category/programing-language/dot-net/)\nCome see what .NET developers at AWS are up to!  Learn about new .NET software announcements, guides, and how-to's.\n\n[@dotnetonaws](https://twitter.com/dotnetonaws)\nFollow us on twitter!\n\n# License\n\nLibraries in this repository are licensed under the Apache 2.0 License.\n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.\n", "release_dates": []}, {"name": "aws-step-functions-data-science-sdk-python", "description": "Step Functions Data Science SDK for building machine learning (ML) workflows and pipelines on AWS", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2022-01-24T20:52:54Z", "2021-06-22T18:28:11Z", "2021-05-28T00:52:19Z", "2021-03-02T02:58:54Z", "2020-09-24T01:08:50Z", "2020-09-23T20:08:39Z", "2020-07-27T21:26:29Z", "2020-01-14T23:46:35Z", "2019-12-24T00:02:31Z"]}, {"name": "aws-swf-build-tools", "description": null, "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon SWF Build Tools\n\nThe AWS SWF build tools helps to generate several key classes based on annotated source code in customer's flow code base. The build tools jar can be downloaded from Maven by:\n```xml\n    <dependency>\n       <groupId>com.amazonaws</groupId>\n       <artifactId>aws-swf-build-tools</artifactId>\n       <version>1.0</version>\n    </dependency>\n```\n", "release_dates": []}, {"name": "aws-swf-flow-library", "description": "AWS Simple Workflow Flow framework library", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Releases of aws-swf-flow-library\n## 1.12.x\nThe `1.12.x` release added following changes based on the `1.11.x` release:\n\n* Supported scaling up the number of threads for task polling and task processing independently. This helps to avoid hitting the hard limit of concurrent pollers per task list, as you can increase the task processing threads without increasing the polling threads.\n* Allowed the number of task processing threads to dynamically change between 0 and a configured value. This feature is disabled by default, and you can turn it on by `setAllowCoreThreadTimeOut(true)` for the workflow/activity workers.\n* Removed the `setTaskExecutorThreadPoolSize()` and `getTaskExecutorThreadPoolSize()` methods from `GenericActivityWorker` and `ActivityWorker`. To configure the polling thread count and task execution thread count for your activity worker, please use `setExecuteThreadCount()` and `getExecuteThreadCount()` instead.\n* Removed the `SpringGracefulShutdownActivityWorker` and `SpringGracefulShutdownWorkflowWorker` classes. The `SpringActivityWorker` and `SpringWorkflowWorker` have the graceful shutdown logic in themselves.\n* Supported `SimpleWorkflowClientConfig` for tuning HTTP request timeouts of SWF APIs.\n* Improved the retry policy during worker startup to avoid startup failure due to `RegisterActivityType` and `RegisterWorkflowType` throttling.\n* Truncated stack trace to comply with the length limit of the `details` field in the `RespondActivityTaskFailed` API. Since the `details` field has a maximum length of 32768, SWF will return 400s if the original exception has a large stack trace. This change truncates the stack trace in case of detail length exceeded, by preserving the first stack trace element and logging the original stack trace.\n* Upgraded the Jackson dependencies to 2.13.x. We've received customer reports that Jackson upgrade can trigger deserialization error (for certain data types) in their workflows. To make it smooth, you'll need to bump up your workflow type accordingly and let old workflow executions drain out. Essentially you should use the same deployment strategy (e.g., have two fleets running two versions of workflows) as what you do for a workflow logic change.\n", "release_dates": []}, {"name": "aws-toolkit-azure-devops", "description": "AWS Toolkit for Azure DevOps", "language": "TypeScript", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Overview\n\nCoverage: [![codecov](https://codecov.io/gh/aws/aws-vsts-tools/branch/master/graph/badge.svg)](https://codecov.io/gh/aws/aws-vsts-tools)\n\nThe AWS Toolkit for Azure DevOps adds tasks to easily enable build and release pipelines in Azure DevOps (formerly VSTS) and Azure DevOps Server (previously known as Team Foundation Server (TFS)) to work with AWS services including Amazon S3, AWS Elastic Beanstalk, AWS CodeDeploy, AWS Lambda, AWS CloudFormation, Amazon Simple Queue Service and Amazon Simple Notification Service, and run commands using the AWS Tools for Windows PowerShell module and the AWS CLI.\n\nThe AWS Toolkit for Azure DevOps is available from the [Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.aws-vsts-tools).\n\nThis is an open source project because we want you to be involved. We love issues, feature requests, code reviews, pull\nrequests or any positive contribution. Please see the the [CONTRIBUTING](CONTRIBUTING.md) guide for how to help, including how to build your own extension.\n\n## Highlighted Features\n\n-   AWSCLI - Interact with the AWSCLI (Windows hosts only)\n-   AWS Powershell Module - Interact with AWS through powershell (Windows hosts only)\n-   Beanstalk - Deploy ElasticBeanstalk applications\n-   CodeDeploy - Deploy with CodeDeploy\n-   CloudFormation - Create/Delete/Update CloudFormation stacks\n-   ECR - Push an image to an ECR repository\n-   Lambda - Deploy from S3, .net core applications, or any other language that builds on Azure DevOps\n-   S3 - Upload/Download to/from S3 buckets\n-   Secrets Manager - Create and retrieve secrets\n-   SQS - Send SQS messages\n-   SNS - Send SNS messages\n-   Systems manager - Get/set parameters and run commands\n\n## User Guide\n\nThe [User Guide](https://docs.aws.amazon.com/vsts/latest/userguide/welcome.html) contains additional instructions for getting up and running with the extension.\n\n**NOTE:** The user-guide source content that used to live in this folder has been moved to its own [GitHub repository](https://github.com/awsdocs/aws-tools-ado-vsts-user-guide).\n\n## Credentials Handling for AWS Services\n\nTo enable tasks to call AWS services when run as part of your build or release pipelines AWS credentials need to have been configured for the tasks or be available in the host process for the build agent. Note that the credentials are used specifically by the tasks when run in a build agent process, they are not related to end-user logins to your Azure DevOps instance.\n\nThe AWS tasks support the following mechanisms for obtaining AWS credentials:\n\n-   One or more service endpoints, of type _AWS_, can be created and populated with AWS access and secret keys, and optionally data for _Assumed Role_ credentials.\n-   If only the _Assumed Role_ is defined but neither access key ID nor secret key, the role be assumed regardless. This is useful when using instance profices, and and profile only allows to assume a role.\n    -   Tasks reference the configured service endpoint instances by name as part of their configuration and pull the required credentials from the endpoint when run.\n-   Variables defined on the task or build.\n    -   If tasks are not configured with the name of a service endpoint they will attempt to obtain credentials, and optionally region, from variables defined in the build environment. The\n        variables are named _AWS.AccessKeyID_, _AWS.SecretAccessKey_ and optionally _AWS.SessionToken_. To supply the ID of the region to make the call in, e.g. us-west-2, you can also use the variable _AWS.Region_. Optionally a role to assume can be specified by using the variable _AWS.AssumeRoleArn_. When assuming roles _AWS.RoleSessionName_ (optional) and _AWS.ExternalId_ (optional) can be provided in order to specify an identifier for the assumed role session and an external id to show in customers' accounts when assuming roles.\n-   Environment variables in the build agent's environment.\n    -   If tasks are not configured with the name of a service endpoint, and credentials or region are not available from task variables, the tasks will attempt to obtain credentials, and optionally region, from standard environment variables in the build process environment. These variables are _AWS_ACCESS_KEY_ID_, _AWS_SECRET_ACCESS_KEY_ and optionally _AWS_SESSION_TOKEN_. To supply the ID of the region to make the call in, e.g. us-west-2, you can also use the environment variable _AWS_REGION_.\n-   EC2 instance metadata, for build hosts running on EC2 instances.\n    -   Both credential and region information can be automatically obtained from the instance metadata in this scenario.\n\n### Configuring an AWS Service Endpoint\n\nTo use _AWS_ service endpoints add the AWS subscription(s) to use by opening the Account Administration screen (gear icon on the top-right of the screen) and then click on the Services Tab. Note that each Azure DevOps project is associated with its own set of credentials. Service endpoints are not shared across projects. You can associate a single service endpoint to be used with all AWS tasks in a build or multiple endpoints if you require.\n\nSelect the _AWS_ endpoint type and provide the following parameters. Please refer to [About Access Keys](https://aws.amazon.com/developers/access-keys/):\n\n-   A name used to refer to the credentials when configuring the AWS tasks\n-   AWS Access Key ID\n-   AWS Secret Access Key\n\n**Note** We strongly suggest you use access and secret keys generated for an Identity and Access Management (IAM) user account. You can configure an IAM user account with permissions granting access to only the services and resources required to support the tasks you intend to use in your build and release definitions.\n\nTasks can also use assumed role credentials by adding the Amazon Resource name (ARN) of the role to be assumed and an optional identifier when configuring the endpoint. The access and secret keys specified will then be used to generate temporary credentials for the tasks when they are executed by the build agents. Temporary credentials are valid for up to 15 minutes by default. To enable a longer validity period you can set the 'aws.rolecredential.maxduration' variable on your build or release definition, specifying a validity period in seconds between 15 minutes (900 seconds) and 12 hours (43200 seconds).\n\n## Supported environments\n\n-   Azure DevOps\n-   Team Foundation Server 2017 Update 1 (or higher) (now called Azure DevOps Server)\n\n## License\n\nThe project is licensed under the MIT license\n\n## Contributors\n\nWe thank the following contributor(s) for this extension: Visual Studio ALM Rangers.\n", "release_dates": ["2024-01-31T22:48:57Z", "2022-03-30T17:59:52Z", "2021-11-17T00:02:32Z", "2021-06-10T20:13:57Z", "2021-05-04T19:21:36Z", "2021-01-06T18:23:56Z", "2020-09-17T17:17:49Z", "2020-04-08T20:33:48Z", "2020-03-16T21:44:40Z", "2019-08-21T19:24:57Z", "2019-06-18T17:22:43Z", "2019-06-12T21:58:14Z", "2019-06-12T16:50:19Z", "2019-05-29T16:19:16Z"]}, {"name": "aws-toolkit-common", "description": "Shared components for the AWS Toolkits", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Toolkit Common\n\nThis repo contains shared components for the AWS Toolkits for\n\n* [Jetbrains](https://github.com/aws/aws-toolkit-jetbrains)\n* [VSCode](https://github.com/aws/aws-toolkit-vscode/)\n* [Visual Studio](https://github.com/aws/aws-toolkit-visual-studio)\n\nContributers looking to contribute to the above projects should consult the contributing guide (CONTRIBUTING.md)\non the repos they are interested in: this repo contains internal components that most contributers would find boring.\n\n## Components\n\n### Telemetry\n\nTelemetry is the first shared component in this repository. [Read about telemetry here.](telemetry/README.md)\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": []}, {"name": "aws-toolkit-eclipse", "description": "(End of life: May 31, 2023) AWS Toolkit for Eclipse", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Toolkit for Eclipse\n## _End of life_\n\n**On May 31, 2023 the AWS Toolkit for Eclipse reached end of life and is no longer supported by AWS.**\n\nThe following AWS Integrated Development Environments (IDE) Toolkits and Software Development Kits (SDKs) are supported alternatives to the AWS Toolkit for Eclipse.\n\n#### AWS Toolkit for Visual Studio Code\n- [Download the AWS Toolkit for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.aws-toolkit-vscode) from the Visual Studio Marketplace.\n- [View AWS Toolkit for Visual Studio Code Documentation](https://docs.aws.amazon.com/toolkit-for-vscode/latest/userguide/welcome.html)\n\n#### AWS Toolkit for JetBrains\n- [Download the AWS Toolkit for JetBrains](https://plugins.jetbrains.com/plugin/11349-aws-toolkit) from the Jetbrains Marketplace.\n- [View AWS Toolkit for JetBrains Documentation](https://docs.aws.amazon.com/toolkit-for-jetbrains/latest/userguide/welcome.html)\n\n#### AWS Toolkit for Visual Studio\n- [Download the AWS Toolkit for Visual Studio](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.AWSToolkitforVisualStudio2022) from the Visual Studio Marketplace.\n- [View AWS Toolkit for Visual Studio Code Documentation](https://docs.aws.amazon.com/docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/welcome.html)\n\n#### SDKs and additional resources\n- For information about additional AWS SDKs and tools see the [SDKs and additional resources](https://aws.amazon.com/developer/tools/)\n\n", "release_dates": ["2018-05-31T17:46:14Z", "2018-03-06T22:18:52Z", "2018-01-05T00:50:05Z", "2017-12-18T19:52:32Z", "2017-10-26T18:26:15Z", "2017-10-23T23:25:00Z", "2017-10-12T06:26:15Z", "2017-09-26T23:21:31Z", "2017-09-08T20:02:19Z", "2017-08-16T21:48:07Z", "2017-08-04T20:02:26Z", "2017-07-12T22:17:16Z", "2017-06-15T16:40:34Z", "2015-10-20T08:47:30Z", "2015-08-07T07:25:45Z", "2015-06-30T20:54:53Z", "2015-06-19T21:18:03Z", "2015-06-18T01:12:55Z", "2015-06-15T20:54:44Z", "2015-05-04T17:53:40Z"]}, {"name": "aws-toolkit-jetbrains", "description": "AWS Toolkit for JetBrains - a plugin for interacting with AWS from JetBrains IDEs", "language": "Kotlin", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![Build Status](https://codebuild.eu-west-1.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiekhxeERIMmNLSkNYUktnUFJzUVJucmJqWnFLMGlpNXJiNE1LLzVWV3B1QUpSSkhCS04veHZmUGxZZ0ZmZlRzYjJ3T1VtVEs1b3JxbWNVOHFOeFJDOTAwPSIsIml2UGFyYW1ldGVyU3BlYyI6ImZXNW5KaytDRGNLdjZuZDgiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=master) \n[![Coverage](https://img.shields.io/codecov/c/github/aws/aws-toolkit-jetbrains/master.svg)](https://codecov.io/gh/aws/aws-toolkit-jetbrains/branch/master) \n[![Downloads](https://img.shields.io/jetbrains/plugin/d/11349-aws-toolkit.svg)](https://plugins.jetbrains.com/plugin/11349-aws-toolkit) \n[![Version](https://img.shields.io/jetbrains/plugin/v/11349.svg?label=version)](https://plugins.jetbrains.com/plugin/11349-aws-toolkit)\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=aws_aws-toolkit-jetbrains&metric=alert_status)](https://sonarcloud.io/dashboard?id=aws_aws-toolkit-jetbrains)\n \n# AWS Toolkit for JetBrains\n\nAWS Toolkit for JetBrains is a plugin for JetBrains IDEs that \nmake it easier to write applications built on [Amazon Web Services](https://aws.amazon.com/)\n\nThe AWS Toolkit for JetBrains is open source because we want you to be involved. We appreciate issues, feature requests, pull \nrequests, code reviews or any other contributions.\n\n## Feedback\n\nWe want your feedback!\n\n- Vote on [feature requests](https://github.com/aws/aws-toolkit-jetbrains/issues?q=is%3Aissue+is%3Aopen+label%3Afeature-request+sort%3Areactions-%2B1-desc). Votes help us drive prioritization of features \n- [Request a new feature](https://github.com/aws/aws-toolkit-jetbrains/issues/new?labels=feature-request&template=feature_request.md)\n- [Ask a question](https://github.com/aws/aws-toolkit-jetbrains/issues/new?labels=guidance&template=guidance_request.md)\n- [File an issue](https://github.com/aws/aws-toolkit-jetbrains/issues/new?labels=bug&template=bug_report.md)\n- Code contributions. See [our contributing guide](CONTRIBUTING.md) for how to get started.\n\n## Supported IDEs\nAll JetBrains IDEs 2023.1+\n\n## Installation\n\nSee [Installing the AWS Toolkit for JetBrains](https://docs.aws.amazon.com/console/toolkit-for-jetbrains/install) in the AWS Toolkit for JetBrains User Guide.\n\nTo use this AWS Toolkit, you will first need an AWS account, a user within that account, and an access key for that \nuser. To use the AWS Toolkit to do AWS serverless application development and to run/debug AWS Lambda functions locally,\nyou will also need to install the AWS CLI, Docker, and the AWS SAM CLI. The installation guide covers setting up all of \nthese prerequisites.\n\n### EAP Builds\nWe also offer opt-in Early Access Preview builds that are built automatically.\n\nIn order to opt-in:\n* Add the URL `https://plugins.jetbrains.com/plugins/eap/aws.toolkit` to your IDE's plugin repository preferences by \ngoing to **Plugins->Gear Icon->Manage Plugin Repositories** and adding the URL to the list\n* Check for updates.\n\n### Installing From Source\nPlease see [CONTRIBUTING](CONTRIBUTING.md#building-from-source) for instructions.\n\n## Features\n\n### General\n\n* **AWS Resource Explorer** - tree-view of AWS resources available in your \nselected account/region. This does not represent all resources available in your account, only a sub-set of those \nresource types supported by the plugin.\n[Learn More](https://docs.aws.amazon.com/console/toolkit-for-jetbrains/aws-explorer)\n* **Authentication** - Connect to AWS using static credentials, credential process, AWS Builder ID or AWS SSO. [Learn more about\nauthentication options](https://docs.aws.amazon.com/console/toolkit-for-jetbrains/credentials)\n\n### Services\n\n#### ![CloudFormation][cloudformation-icon] AWS CloudFormation\n* View events, resources, and outputs for your CloudFormation stacks\n#### ![CloudWatch Logs][cloudwatch-logs-icon] CloudWatch Logs \n* View and search your CloudWatch log streams\n#### ![AWS Lambda][lambda-icon] AWS Lambda\n\nMany of these features require the [AWS SAM CLI](https://github.com/awslabs/aws-sam-cli) to be installed, see the \nServerless Application Model ([SAM](https://aws.amazon.com/serverless/sam/)) website for more information on \ninstallation of the SAM CLI.\n\n**SAM features support Java, Python, Node.js, and .NET Core**\n\n* **Run/Debug Local Lambda Functions** - Locally test and step-through debug functions in a Lambda-like execution \nenvironment provided by the SAM CLI.\n[Learn More](https://docs.aws.amazon.com/console/toolkit-for-jetbrains/lambda-local)\n* **Invoke Remote Lambda Functions** - Invoke remote functions using a sharable run-configuration\n[Learn More](https://docs.aws.amazon.com/console/toolkit-for-jetbrains/lambda-remote)\n* **Package & Deploy Lambda Functions** - Ability to package a Lambda function zip and create a remote lambda\n[Learn More](https://docs.aws.amazon.com/console/toolkit-for-jetbrains/lambda-deploy)\n* **Sync SAM-based Applications** - Sync & track SAM-based applications\n[Learn More](https://docs.aws.amazon.com/console/toolkit-for-jetbrains/sam-deploy)\n\n*Note: Python features are available in both PyCharm and IntelliJ with the \n[Python Plugin](https://www.jetbrains.com/help/idea/plugin-overview.html) installed.*\n\n#### ![Amazon Redshift][redshift-icon] Amazon RDS/Redshift\n* Connect to RDS/Redshift databases using temporary credentials with IAM/SecretsManager, no copy paste required\n\n*Note: database features require using a paid JetBrains product*\n#### ![Amazon S3][s3-icon] Amazon S3\n* View and manage your S3 buckets\n* Upload/Download to from buckets\n* [Learn more](https://docs.aws.amazon.com/console/toolkit-for-jetbrains/s3-tasks)\n\n### Experimental Features\n\nSometimes we'll introduce experimental features that we're trying out. These may have bugs, usability problems or may not be fully functional, and because these\naren't ready for prime-time we'll hide them behind an experimental feature flag. \n\nExperimental features can be enabled in the settings/preferences\n(`Settings -> Tools -> AWS -> Experimental Features`) or via the Addtional Settings (![Gear Icon][gear-icon]) in the AWS Explorer Tool Window. \n\nPlease note that experimental features may be disabled / removed at any time.\n\n## Licensing\n\nThe plugin is distributed according to the terms outlined in our [LICENSE](LICENSE).\n\n[lambda-icon]: jetbrains-core/resources/icons/resources/LambdaFunction.svg\n[s3-icon]: jetbrains-core/resources/icons/resources/S3Bucket.svg\n[cloudwatch-logs-icon]: jetbrains-core/resources/icons/resources/cloudwatchlogs/CloudWatchLogs.svg\n[cloudformation-icon]: jetbrains-core/resources/icons/resources/CloudFormationStack.svg\n[redshift-icon]: jetbrains-core/resources/icons/resources/Redshift.svg\n[find-action]: https://www.jetbrains.com/help/idea/searching-everywhere.html#search_actions\n[gear-icon]: https://raw.githubusercontent.com/JetBrains/intellij-community/master/platform/icons/src/general/gear.svg\n", "release_dates": ["2024-02-29T23:13:39Z", "2024-02-22T22:42:47Z", "2024-02-15T23:20:07Z", "2024-02-08T20:03:54Z", "2024-02-02T18:52:26Z", "2024-01-25T20:22:28Z", "2024-01-11T20:47:22Z", "2024-01-05T19:44:43Z", "2023-12-15T17:31:22Z", "2023-12-05T03:19:33Z", "2023-11-28T23:54:52Z", "2023-11-28T17:02:43Z", "2023-11-27T00:35:35Z", "2023-11-17T18:47:09Z", "2023-11-10T23:16:25Z", "2023-11-08T04:49:23Z", "2023-10-27T18:35:10Z", "2023-10-17T15:43:19Z", "2023-10-13T22:27:19Z", "2023-10-06T21:00:14Z", "2023-09-29T23:40:44Z", "2023-09-15T20:58:06Z", "2023-09-08T21:41:24Z", "2023-08-31T20:17:58Z", "2023-08-17T20:36:27Z", "2023-08-03T21:01:08Z", "2023-07-25T06:55:03Z", "2023-07-20T21:18:38Z", "2023-07-13T20:29:07Z", "2023-07-06T23:48:50Z"]}, {"name": "aws-toolkit-visual-studio", "description": "AWS Toolkit for Visual Studio - a plugin to interact with AWS", "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Toolkit For Visual Studio\n\nThe AWS Toolkit for Visual Studio is an extension for Microsoft Visual Studio running on Microsoft Windows that makes it easier for developers to develop, debug, and deploy applications using Amazon Web Services, allowing you to get started faster and be more productive.\n\nThe extension can be found on the Visual Studio Marketplace:\n* [AWS Toolkit for Visual Studio 2019](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.AWSToolkitforVisualStudio2017)\n* [AWS Toolkit for Visual Studio 2022](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.AWSToolkitforVisualStudio2022)\n\nSee the [user guide](https://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/welcome.html) for information on how to get started.\n\n**At this time, this repo is an issues only repo. It is for bug reports, feature requests, and questions.**\n\n## Supported Visual Studio Versions\n\nThe AWS Toolkit for Visual Studio supports Visual Studio 2019, and 2022.  For legacy versions of the toolkit that are no longer supported, see https://aws.amazon.com/visualstudio.\n\n## Highlighted Features\n\n- The AWS Explorer presents a tree view of your AWS resources such as Amazon EC2, Amazon S3, Amazon DynamoDB, AWS Lambda, AWS CloudFormation and other services as well. With the AWS Explorer, you can view and edit resources within these services.\n- Web Applications and Web Sites can be deployed to the AWS cloud by right clicking on the project in the Solution Explorer and selecting \"Publish to AWS Elastic Beanstalk\".\n- Serverless applications can be deployed to the AWS cloud by right clicking on the project in the Solution Explorer and selecting \"Publish to AWS Lambda\".\n- Using the Amazon EC2 Instance view you can quickly create new Windows instances and Remote Desktop into them simply by right clicking the instance and selecting \"Open Remote Desktop\".\n- You can browse the files stored in your S3 bucket and upload and download files.  You can create pre-signed URLs to objects to pass around and change the permissions of files.\n- AWS IAM users and groups can be created and users can be assigned to groups.  Access keys can be generated for IAM users and access policies can be created using the access policy editor for both users and groups.\n- Through the AWS Explorer, you can view, create, and delete Amazon DynamoDB tables. You can also add new items to tables, add new attributes to items, and edit attribute values. The AWS Toolkit also enables you to search your tables using Scan operations.\n- Using the editor for Amazon SQS queues you can see and edit the properties, send messages to the queue and view a sampling of the messages in the queue.\n- Using the editor for Amazon SNS topics you can see properties, publish messages to the queue and create subscriptions to the topic.  You can also drag and drop queues onto the topic editor to create subscriptions.\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## License\n\nThis repo is licensed under the Apache 2.0 License.\n\nThe AWS Toolkit for Visual Studio is licensed under the [AWS Customer Agreement](https://marketplace.visualstudio.com/items/AmazonWebServices.AWSToolkitforVisualStudio2017/license)\n", "release_dates": ["2023-12-11T18:15:48Z", "2023-05-16T16:26:12Z", "2022-04-13T19:50:27Z"]}, {"name": "aws-toolkit-vscode", "description": "Amazon Q, CodeWhisperer, CodeCatalyst, Local Lambda debug, SAM/CFN syntax, ECS Terminal, AWS resources", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Toolkit for Visual Studio Code\n\n[![Coverage](https://img.shields.io/codecov/c/github/aws/aws-toolkit-vscode/master.svg)](https://codecov.io/gh/aws/aws-toolkit-vscode/branch/master) [![Marketplace Version](https://img.shields.io/vscode-marketplace/v/AmazonWebServices.aws-toolkit-vscode.svg) ![Marketplace Downloads](https://img.shields.io/vscode-marketplace/d/AmazonWebServices.aws-toolkit-vscode.svg)](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.aws-toolkit-vscode)\n\nAWS Toolkit is a [VS Code extension](https://marketplace.visualstudio.com/itemdetails?itemName=AmazonWebServices.aws-toolkit-vscode) for connecting your IDE to your AWS resources:\n\n-   Connect with [IAM credentials](https://docs.aws.amazon.com/sdkref/latest/guide/access-users.html),\n    [IAM Identity Center (SSO)](https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html),\n    or [AWS Builder ID](https://docs.aws.amazon.com/signin/latest/userguide/differences-aws_builder_id.html)\n-   Use [CodeWhisperer](https://aws.amazon.com/codewhisperer/) to write code\n-   Connect to your [CodeCatalyst](https://codecatalyst.aws/) Dev Environments\n-   Debug your Lambda functions using [SAM CLI](https://github.com/aws/aws-sam-cli)\n-   Check and autocomplete code in SAM/CFN (CloudFormation) `template.yaml` files\n-   `Open Terminal` on your ECS tasks\n-   `Search Log Group` on your CloudWatch logs\n-   Browse your AWS resources\n\nThis project is open source. We love issues, feature requests, code reviews, pull requests or any\npositive contribution. See [CONTRIBUTING.md](CONTRIBUTING.md) to get started.\n\n## Documentation\n\n-   [Quick Start Guide](https://marketplace.visualstudio.com/itemdetails?itemName=AmazonWebServices.aws-toolkit-vscode)\n-   [FAQ / Troubleshooting](./docs/faq-credentials.md)\n-   [User Guide](https://docs.aws.amazon.com/console/toolkit-for-vscode/welcome)\n-   General info about [AWS SDKs and Tools](https://docs.aws.amazon.com/sdkref/latest/guide/overview.html)\n\n## Feedback\n\nWe want your feedback!\n\n-   Upvote \ud83d\udc4d [feature requests](https://github.com/aws/aws-toolkit-vscode/issues?q=is%3Aissue+is%3Aopen+label%3Afeature-request+sort%3Areactions-%2B1-desc)\n-   [Ask a question](https://github.com/aws/aws-toolkit-vscode/issues/new?labels=guidance&template=guidance_request.md)\n-   [Request a new feature](https://github.com/aws/aws-toolkit-vscode/issues/new?labels=feature-request&template=feature_request.md)\n-   [File an issue](https://github.com/aws/aws-toolkit-vscode/issues/new?labels=bug&template=bug_report.md)\n-   Or [send a pull request](CONTRIBUTING.md)!\n\n## License\n\nThe **AWS Toolkit for Visual Studio Code** is distributed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n", "release_dates": ["2024-03-01T19:43:22Z", "2024-03-01T19:45:11Z", "2024-02-29T21:52:03Z", "2024-02-22T22:22:40Z", "2024-02-16T20:45:36Z", "2024-02-09T21:03:46Z", "2024-02-01T18:54:52Z", "2024-01-25T18:28:51Z", "2024-01-18T21:55:29Z", "2024-01-11T21:12:26Z", "2023-12-19T22:08:48Z", "2023-12-15T20:52:01Z", "2023-12-06T18:03:42Z", "2023-11-30T16:31:57Z", "2023-11-28T15:52:46Z", "2023-11-28T20:05:44Z", "2023-11-27T00:35:01Z", "2023-11-22T21:41:22Z", "2023-11-17T20:08:51Z", "2023-11-09T00:30:26Z", "2023-11-02T18:00:47Z", "2023-10-26T21:07:31Z", "2023-10-17T15:24:44Z", "2023-10-12T22:07:09Z", "2023-10-06T15:37:28Z", "2023-09-29T22:04:07Z", "2023-09-22T20:07:13Z", "2023-09-15T18:59:34Z", "2023-09-08T21:04:51Z", "2023-09-08T03:21:23Z"]}, {"name": "aws-tools-for-powershell", "description": "The AWS Tools for PowerShell lets developers and administrators manage their AWS services from the PowerShell scripting environment.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS Tools for PowerShell\n\nThe _AWS Tools for PowerShell_ lets developers and administrators manage their AWS services from the PowerShell scripting environment.\n\nThis repository is meant to be used as a way to collect feedback from users of the _AWS Tools for PowerShell_ modules. You are invited to create GitHub issues to report bugs or make feature requests.\n\n## AWS Tools for PowerShell Modules\n\nThe _AWS Tools for PowerShell_ modules [AWS.Tools](https://www.powershellgallery.com/packages/AWS.Tools.Common), [AWSPowerShell.NetCore](https://www.powershellgallery.com/packages/AWSPowerShell.NetCore) and [AWSPowerShell](https://www.powershellgallery.com/packages/AWSPowerShell) are available from [PowerShell Gallery](https://www.powershellgallery.com/).\n\n\n| Module | **Compatible PowerShell Versions** |  |\n|---|---|---|\n| **[AWS.Tools](https://www.powershellgallery.com/packages/AWS.Tools.Common)*** | 6+ (and 5.1 when .NET Framework 4.7.2 is installed) |\n| **[AWSPowerShell.NetCore](https://www.powershellgallery.com/packages/AWSPowerShell.NetCore)** | 6+ (and 3.0 - 5.1 when .NET Framework 4.7.2 is installed) |\n| **[AWSPowerShell](https://www.powershellgallery.com/packages/AWSPowerShell)** | 2.0 - 5.1 |\n\n*_AWS.Tools_ is the new modular variant of AWS Tools for PowerShell. In order to manage each AWS service, install from [PowerShell Gallery](https://www.powershellgallery.com/) the corresponding module (e.g. [AWS.Tools.EC2](https://www.powershellgallery.com/packages/AWS.Tools.EC2), [AWS.Tools.S3](https://www.powershellgallery.com/packages/AWS.Tools.S3)...).\n\nMore details about _AWS.Tools_ can be found [here](https://github.com/aws/aws-tools-for-powershell/issues/67).\n\nThe [AWS.Tools.Installer](https://www.powershellgallery.com/packages/AWS.Tools.Installer) module can be used to simplify installing, updating and removing the _AWS.Tools_ modules.\n\n## Change Log\n\nThe change log for AWS Tools for PowerShell can be found in the [CHANGELOG.md](https://github.com/aws/aws-tools-for-powershell/blob/master/CHANGELOG.md) file.\n\nThe AWS Tools for PowerShell depends on the [AWS SDK for .NET](https://github.com/aws/aws-sdk-net), you can find the AWS SDK's changelog [here](https://github.com/aws/aws-sdk-net/blob/main/changelogs/SDK.CHANGELOG.ALL.md).\n\n## Maintenance and support for major versions\n\nFor information about maintenance and support for major versions and their underlying dependencies, see the following in the AWS SDKs and Tools Shared Configuration and Credentials Reference Guide:\n\n* [AWS SDKs and Tools Maintenance Policy](https://docs.aws.amazon.com/credref/latest/refdocs/maint-policy.html)\n* [AWS SDKs and Tools Version Support Matrix](https://docs.aws.amazon.com/credref/latest/refdocs/version-support-matrix.html)\n\n## Additional Resources\n\n[AWS Tools for PowerShell User Guide](https://docs.aws.amazon.com/powershell/latest/userguide/pstools-welcome.html)\n\nCheck out this user guide to get started quickly.\n\n[AWS Tools for PowerShell Cmdlet Reference](https://docs.aws.amazon.com/powershell/latest/reference/Index.html)\n\nTake a look at this reference for more in-depth documentation for each PowerShell cmdlet.\n\n[AWS Developer Blog](https://aws.amazon.com/blogs/developer/category/programing-language/dot-net/)\n\nCome see what .NET and PowerShell developers at AWS are up to! Learn about new software announcements, guides, and how-to's.\n\n[@dotnetonaws](https://twitter.com/dotnetonaws)\n\nFollow us on Twitter!\n\n## License\n\nThe content of this repository is licensed under the Apache 2.0 License.\n", "release_dates": []}, {"name": "aws-xray-daemon", "description": "The AWS X-Ray daemon listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "[![Build Status](https://travis-ci.org/aws/aws-xray-daemon.svg?branch=master)](https://travis-ci.org/aws/aws-xray-daemon)\n[![Go Report Card](https://goreportcard.com/badge/github.com/aws/aws-xray-daemon)](https://goreportcard.com/report/github.com/aws/aws-xray-daemon)\n\n# AWS X-Ray Daemon  \n\nThe AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API.   \nThe daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service. For more information,\n see [AWS X-Ray Daemon](https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html).\n\n## Getting Help  \n\nUse the following community resources for getting help with the AWS X-Ray Daemon. We use the GitHub issues for tracking bugs and feature requests.  \n\n* Ask a question in the [AWS X-Ray Forum](https://forums.aws.amazon.com/forum.jspa?forumID=241&start=0).  \n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).  \n* If you think you may have found a bug, open an [issue](https://github.com/aws/aws-xray-daemon/issues/new).  \n* For contributing guidelines refer [CONTRIBUTING.md](https://github.com/aws/aws-xray-daemon/blob/master/CONTRIBUTING.md).\n\n## Sending Segment Documents\n\nThe X-Ray SDK sends segment documents to the daemon to avoid making calls to AWS directly. You can send the segment/subsegment in JSON over UDP port 2000\nto the X-Ray daemon, prepended by the daemon header : `{\"format\": \"json\", \"version\": 1}\\n`\n\n```\n{\"format\": \"json\", \"version\": 1}\\n{<serialized segment data>}\n```  \nFor more details refer : [Link](https://docs.aws.amazon.com/xray/latest/devguide/xray-api-sendingdata.html)  \n\n## Installing  \n\nThe AWS X-Ray Daemon is compatible with Go 1.8 and later.\n\nInstall the daemon using the following command:  \n\n```  \ngo get -u github.com/aws/aws-xray-daemon/...  \n```  \n\n## Credential Configuration\n\nThe AWS X-Ray Daemon follows default credential resolution for the [aws-sdk-go](https://docs.aws.amazon.com/sdk-for-go/api/index.html#hdr-Configuring_Credentials).\n\nFollow the [guidelines](https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html) for the credential configuration.\n\n## Daemon Usage (command line args)  \n\nUsage: xray [options]   \n\n| | | Description |\n| --- | --- | --- |\n| -a | --resource-arn | Amazon Resource Name (ARN) of the AWS resource running the daemon. |\n| -o | --local-mode | Don't check for EC2 instance metadata. |\n| -m | --buffer-memory | Change the amount of memory in MB that buffers can use (minimum 3). |\n| -n | --region | Send segments to X-Ray service in a specific region. |\n| -b | --bind | Overrides default UDP address (127.0.0.1:2000). |\n| -t | --bind-tcp | Overrides default TCP address (127.0.0.1:2000). |\n| -r | --role-arn | Assume the specified IAM role to upload segments to a different account. |\n| -c | --config | Load a configuration file from the specified path. |\n| -f | --log-file | Output logs to the specified file path. |\n| -l | --log-level | Log level, from most verbose to least: dev, debug, info, warn, error, prod (default). |\n| -p | --proxy-address | Proxy address through which to upload segments. |\n| -v | --version | Show AWS X-Ray daemon version. |\n| -h | --help | Show this screen |\n\n## Build  \n\n`make build` would build binaries and .zip files in `/build` folder for Linux, MacOS, and Windows platforms.    \n\n### Linux  \n\n`make build-linux` would build binaries and .zip files in `/build` folder for the Linux platform.  \n\n### MAC  \n\n`make build-mac` would build binaries and .zip files in `/build` folder for the MacOS platform.  \n\n### Windows  \n\n`make build-windows` would build binaries and .zip files in `/build` folder for the Windows platform. \n\n## Build for ARM achitecture\nCurrently, the `make build` script builds artifacts for AMD architecture. You can build the X-Ray Daemon for ARM by using the `go build` command and setting the `GOARCH` to `arm64`. To build the daemon binary on a linux ARM machine, you can use the following command:\n```\nGOOS=linux GOARCH=arm64 go build -ldflags \"-s -w\" -o xray cmd/tracing/daemon.go cmd/tracing/tracing.go\n```\nAs of Aug 31, 2020, windows and darwin builds for ARM64 are not supported by `go build`.\n\n## Pulling X-Ray Daemon image from ECR Public Gallery\nBefore pulling an image you should authenticate your docker client to the Amazon ECR public registry. For registry authentication options follow this [link](https://docs.aws.amazon.com/AmazonECR/latest/public/public-registries.html#public-registry-auth)\n\nRun below command to authenticate to public ECR registry using `get-login-password` (AWS CLI)\n\n``\naws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws\n``\n\n####  Pull alpha tag from Public ECR Gallery\n``\ndocker pull public.ecr.aws/xray/aws-xray-daemon:alpha\n``\n\n####  Pull released version tag from Public ECR Gallery\n``\ndocker pull public.ecr.aws/xray/aws-xray-daemon:3.2.0\n``\n\nNOTE: We are not recommending to use daemon image with alpha tag in production environment. For production environment customer should pull in an image with released tag. \n\n## X-Ray Daemon Performance Report\n\n**EC2 Instance Type:** T2.Micro [1 vCPU, 1 GB Memory]\n\n**Collection time:** 10 minutes per TPS (TPS = Number of segments sent to daemon in 1 second)\n\n**Daemon version tested:** 3.3.6\n\n| **TPS** | **Avg CPU Usage (%)** | **Avg Memory Usage (MB)** |\n|---------|-----------------------|---------------------------|\n| 0       | 0                     | 17.07                     |\n| 100     | 0.9                   | 28.5                      |\n| 200     | 1.87                  | 29.3                      |\n| 400     | 3.76                  | 29.1                      |\n| 1000    | 9.36                  | 29.5                      |\n| 2000    | 18.9                  | 29.7                      |\n| 4000    | 38.3                  | 29.5                      |\n\n\n## Testing  \n\n`make test` will run unit tests for the X-Ray daemon.  \n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": ["2024-01-02T19:32:02Z", "2023-11-02T16:30:33Z", "2023-09-13T00:54:49Z", "2023-04-27T16:59:46Z", "2023-02-08T18:03:53Z", "2022-09-27T22:29:19Z", "2022-09-21T22:34:52Z", "2021-07-27T23:18:33Z", "2021-04-20T21:04:31Z", "2021-04-13T15:50:11Z", "2021-04-13T15:50:01Z", "2019-12-11T19:46:25Z", "2019-07-01T21:23:15Z", "2019-06-19T23:00:10Z", "2019-04-16T21:31:26Z", "2018-08-29T00:49:43Z", "2018-06-13T19:31:08Z", "2018-05-14T22:09:36Z", "2018-04-26T03:03:27Z"]}, {"name": "aws-xray-dotnet-agent", "description": "The official AWS X-Ray Auto Instrumentation Agent for .Net.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "\ufeff## AWS X-Ray .NET Agent\n\nThe AWS X-Ray .NET Agent is a drop-in solution that enables the propagation of X-Ray traces within your web applications. This includes automatic tracing for AWS X-Ray SDK supported frameworks and libraries. The agent enables you to use the X-Ray SDK out of box, and requires no code changes to enable the basic propagation of traces. See the compatibility chart below for the current feature parity between the AWS X-Ray .NET SDK and the AWS X-Ray .NET Agent.\n\nSee the [Sample App](https://github.com/aws-samples/aws-xray-dotnet-webapp) for a demonstration on how to use the agent.\n\n## Compatibility Chart\n\n| **Feature**\t| **X-Ray SDK(.NET)** | **X-Ray SDK(.NET Core)** | **X-Ray Agent(.NET)** | **X-Ray Agent(.NET Core)**|\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| [AWS](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet-sdkclients.html) | \u2714 | \u2714 | \u2714 | \u2714 |\n| [Incoming Http](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet-messagehandler.html) | \u2714 | \u2714 | \u2714 | \u2714 |\n| [HttpClient](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet-httpclients.html) | \u2714 | \u2714 | \u2714 | \u2714 |\n| [HttpWebRequest](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet-httpclients.html) | \u2714 | \u2714 | \u2714 | \u2714 |\n| [System.Data.SqlClient](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet-sqlqueries.html) | \u2714 | \u2714 | \u2714 | \u2714 |\n| [Microsoft.Data.SqlClient](https://docs.microsoft.com/en-us/dotnet/api/microsoft.data.sqlclient) | \u274c | \u274c |    \u274c |  \u2714 |\n| [EntityFramework](https://docs.microsoft.com/en-us/ef/) | \u274c |\u2714 (EF Core)| \u2714 (EF 6)| \u2714 (EF Core)|\n| [Local Sampling](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet-configuration.html#xray-sdk-dotnet-configuration-sampling) | \u2714 | \u2714 | \u2714 | \u2714 |\n| [Dynamic Sampling](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet-configuration.html#xray-sdk-dotnet-configuration-sampling) | \u2714 | \u2714 | \u2714 | \u2714 |\n| [Multithreaded Execution](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#multithreaded-execution-net-and-net-core--nuget) | \u2714 | \u2714 | \u2714 | \u2714 |\n| [Plugins](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet-configuration.html#xray-sdk-dotnet-configuration-plugins) | \u2714 | \u2714 | \u2714 | \u2714 |\n| [Custom Subsegment](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet-subsegments.html) | \u2714 | \u2714 | \u2714 | \u2714 |\n\n## Prerequisites\n\nIf you're running an Asp.Net Core application, you need to install the latest version of [Visual C++ Redistributable ](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\n\n## Configuration\n\nAWS X-Ray .Net Agent will register the [configuration items](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#configuration) as AWS X-Ray .NET SDK.\n\nBesides, AWS X-Ray .Net Agent will register the following configuration items.\n```\n{\n    \"ServiceName\" : \"DefaultService\",\n    \"DaemonAddress\" : \"127.0.0.1:2000\",\n    \"TraceHttpRequests\" : \"true\",\n    \"TraceAWSRequests\" : \"true\",\n    \"TraceSqlRequests\" : \"true\",\n    \"TraceEFRequests\" : \"true\"\n}\n```\nYou can customize the service name of your application, the daemon address and specify which request to trace through `appsettings.json` file (Asp.Net Core) or `web.config` file (Asp.Net).\n\nIf you don't provide these configuration items, the default values shown above will be applied by AWS X-Ray .NET Agent.\n\nNote:\n\n* .Net Agent doesn't provide configuration item to disable tracing incoming Http request. If you want to disable tracing incoming request, you may set `DisableXRayTracing` as `true`.\n\n* AWS request will trigger Http outgoing handler, so if you want to disable tracing AWS request, you have to disable both AWS handler and Http outgoing handler.\n\n* Similiar situation happens to Entity Framework request, which triggers both Entity Framework handler and Sql handler, therefore, if you want to disable tracing Entity Framework request, remember to disable Sql handler as well.\n\n## Installation\n\n### Minimum Requirements\n\nFor building `AWSXRayRecorder.AutoInstrumentation` package, you need to install **Visual Studio 2019**.\n\nFor building profiler, you need to have workloads **.NET desktop development** and **Desktop development with C++** installed within **Visual Studio 2019**.\n\n### Development\n\nNote:\n\nDotNet Coreclr Lib is required to build the profiler project in this repo. You can find it at this [repo](https://github.com/dotnet/runtime/tree/master/src/coreclr). Put coreclr folder under `aws-xray-dotnet-agent\\src\\profiler`, then you are good to go.\n\n### Automatic Instrumentation\n\n#### Internet Information Services (IIS)\n\n##### Asp.Net Core & Asp.Net\n\n1. Git clone this repo and import `AWSXRayRecorder.AutoInstrumentation` package into your project and **rebuild**.\n2. Download and run AWS X-Ray .NET Agent Installer ([x64](https://github.com/aws/aws-xray-dotnet-agent/releases/download/v2.10.0-beta.1/aws-xray-dotnet-agent-installer-beta-X64.msi) and [x86](https://github.com/aws/aws-xray-dotnet-agent/releases/download/v2.10.0-beta.1/aws-xray-dotnet-agent-installer-beta-X86.msi)).\n3. Restart IIS and launch your application.\n```\niisreset\n```\n\n#### Others (Not IIS)\n\n##### Asp.Net Core\n\n1. Git clone this repo and import `AWSXRayRecorder.AutoInstrumentation` package into your project and **rebuild**.\n2. Download and run AWS X-Ray .NET Agent Installer ([x64](https://github.com/aws/aws-xray-dotnet-agent/releases/download/v2.10.0-beta.1/aws-xray-dotnet-agent-installer-beta-X64.msi) and [x86](https://github.com/aws/aws-xray-dotnet-agent/releases/download/v2.10.0-beta.1/aws-xray-dotnet-agent-installer-beta-X86.msi)).\n3. Launch your application as follows.\n```\nSET CORECLR_PROFILER={AE47A175-390A-4F13-84CB-7169CEBF064A}\nSET CORECLR_ENABLE_PROFILING=1\n\ndotnet YourApplication.dll\n```\nNote:\n\n* **Do not set environment variables globally into the system variables as profiler will try to instrument all .NET processes running on the instance with AWS X-Ray tracing SDK.**\n\n##### Asp.Net\n\n1. Import `AWSXRayRecorder.AutoInstrumentation` package into your project and **rebuild**.\n2. Add the following snippet into the `web.config` file.\n```\n<system.webServer>\n <modules>\n  <add name=\"AWSXRayTracingModule\" type=\"Amazon.XRay.Recorder.AutoInstrumentation.AspNetAutoInstrumentationModule,AWSXRayRecorder.AutoInstrumentation,Version=2.10.0.0,Culture=neutral,PublicKeyToken=d427001f96b0d0b6\" />\n </modules>\n</system.webServer>\n```\n3. Launch your application\n\n### Manual Instrumentation\n\n#### Asp.Net Core\n\nInstead of using profiler, you may choose to manually instrument AWS X-Ray SDK into your Asp.Net Core application.\n\n1. Import `AWSXRayRecorder.AutoInstrumentation` package into your project.\n\n2. Add the following method into any method in `startup.cs` or `program.cs` file\n```\nAmazon.XRay.Recorder.AutoInstrumentation.Initialize.AddXRay();\n```\n\n## Getting Help\n\nPlease use these community resources for getting help.\n\n* If you think you may have found a bug or need assistance, please open an [issue](https://github.com/aws/aws-xray-dotnet-agent/issues/new).\n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n* Ask a question in the [AWS X-Ray Forum](https://forums.aws.amazon.com/forum.jspa?forumID=241&start=0).\n* For contributing guidelines refer to [CONTRIBUTING.md](https://github.com/aws/aws-xray-dotnet-agent/blob/master/CONTRIBUTING.md).\n\n## Documentation\n\nThe [developer guide](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet.html) provides guidance on using the AWS X-Ray DotNet Agent. Please refer to the [Sample App](https://github.com/aws-samples/aws-xray-dotnet-webapp) for an example.\n\n## License\n\nThe AWS X-Ray SDK DotNet Agent is licensed under the Apache 2.0 License. See LICENSE for more information.\n", "release_dates": ["2021-05-10T18:51:04Z", "2020-08-03T21:05:14Z"]}, {"name": "aws-xray-java-agent", "description": "The official AWS X-Ray Auto Instrumentation Agent for Java.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "[![Build Status](https://github.com/aws/aws-xray-java-agent/workflows/Continuous%20Build/badge.svg)](https://github.com/aws/aws-xray-java-agent/actions)\n\n![Screenshot of the AWS X-Ray console](/images/xray-agent-sample.png?raw=true)\n\n# AWS X-Ray Java Agent\n\nThe AWS X-Ray Java Agent is a drop-in solution that enables the propagation of X-Ray traces within your Java web applications and servlet-based microservices. \nThis includes automatic tracing for supported frameworks and libraries, including Spring, the AWS SDK, Apache HTTP clients, and JDBC-based SQL queries. \nThe agent enables you to use the X-Ray SDK out of box, and requires no code changes to enable the basic propagation of traces. \nSee the chart below for the current feature parity between the AWS X-Ray SDK and the AWS X-Ray Java Agent.\n\nThe X-Ray Java Agent is implemented using the [DiSCo library](https://github.com/awslabs/disco), a toolkit for building Java Agents in distributed environments.\n\n## Compatibility Chart\n\n| *Feature*\t| *X-Ray SDK*\t| *X-Ray Agent* |\n| ----------- | ----------- | ----------- |\n| AWS SDK V1 Instrumentation (Confirmed on 1.11.x) | \u2714 | \u2714 | \n| AWS SDK V2 Instrumentation | \u2714 | \u2714 | \n| [Centralized Sampling](https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html) | \u2714 | \u2714 | \n| Automatic Multi-threaded Support | \u274c | \u2714 | \n| Custom manual instrumentation | \u2714 | \u2714 | \n| [SQL Queries](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-sqlclients.html) | \u2714 | \u2714 | \n| [Plugins](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-configuration.html#xray-sdk-java-configuration-plugins) | \u2714 | \u2714 | \n| [Apache HTTP Client](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-httpclients.html) | \u2714 | \u2714 | \n| [HttpServlet](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-filters.html) | \u2714 | \u2714 | \n| [Lambda Layers](https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html) | \u2714 | \u274c | \n| Log Injection | \u2714 | \u2714 | \n| Spring Framework | \u2714 | \u2714 | \n\n## Prerequisites\n\nThe AWS X-Ray Java Agent is compatible with Java 8 and 11. You must be able to modify the JVM arguments of your\napplication to use the agent.\n\n## Installation\n\nYou can download the latest version of the X-Ray Agent [here](https://github.com/aws/aws-xray-java-agent/releases/latest/download/xray-agent.zip),\nor you can browse the [releases](https://github.com/aws/aws-xray-java-agent/releases) to download earlier versions.\n\nAlternatively, you can download the agent from Maven Central by adding a dependency on it. This approach is\nnot recommended because you will be responsible for assembling all the JARs into the required file structure. \nTo depend on the agent from your project, just add these dependencies:\n\n```xml\n<dependencies>\n    <dependency>\n        <groupId>software.amazon.disco</groupId>\n        <artifactId>disco-toolkit-bom</artifactId>\n        <version>0.11.0</version>\n        <type>pom</type>\n        <scope>import</scope>\n    </dependency>\n    <dependency>\n        <groupId>software.amazon.disco</groupId>\n        <artifactId>disco-java-agent</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>software.amazon.disco</groupId>\n        <artifactId>disco-java-agent-aws-plugin</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>software.amazon.disco</groupId>\n        <artifactId>disco-java-agent-sql-plugin</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>software.amazon.disco</groupId>\n        <artifactId>disco-java-agent-web-plugin</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>com.amazonaws</groupId>\n        <artifactId>aws-xray-agent-plugin</artifactId>\n        <version>2.9.1</version>\n    </dependency>\n</dependencies>\n```\n\nTo get started with the agent in your application, see the [official documentation](https://docs.aws.amazon.com/xray/latest/devguide/aws-x-ray-auto-instrumentation-agent-for-java.html#XRayAutoInstrumentationAgent-GettingStarted).\n\n## Sample App\n\nTo see the Agent in action first, checkout the `xray-agent` branch of the [eb-java-scorekeep](https://github.com/aws-samples/eb-java-scorekeep/tree/xray-agent).\nThe README has steps to set up a fully-functioning Spring Boot application instrumented with the X-Ray\nAgent and complemented with manual instrumentation by the X-Ray SDK.\n\n## Performance impacts\n\nTo get an idea of how much impact the X-Ray Agent might have on your system, please see the [benchmarking package](https://github.com/aws/aws-xray-java-agent/tree/main/aws-xray-agent-benchmark).\n\n## Customizing the Agent\n\n### Configuration\n\nThe X-Ray Agent is configured by an external, user-provided JSON file. By default this file is expected to be located at \nthe root of the user's classpath and titled `xray-agent.json`. You can configure a custom location for the config\nfile by setting the `com.amazonaws.xray.configFile` system property to the absolute filesystem path OR\nabsolute location on the classpath of your configuration file.\n\nFor more details about configuring the agent, see the [official documentation](https://docs.aws.amazon.com/xray/latest/devguide/aws-x-ray-auto-instrumentation-agent-for-java.html#XRayAutoInstrumentationAgent-Configuration).\n\n### Toggling Tracing for Different Events\n\nThe benefit of the X-Ray Agent operating as a plugin rather than a monolith is that you can add or remove other\ncompatible DiSCo plugins as you please. All of the plugins that the X-Ray agent will use are in the directory pointed\nto by the `pluginPath` parameter in your JVM argument. The plugins are what instrument different events like AWS SDK requests\nand SQL queries so they can be traced by X-Ray. Normally, this is the `disco-plugins` directory in the ZIP distribution,\nbut it can be any directory.\n\nTo remove an undesirable plugin, simply remove it from your `pluginPath` directory and restart your app. For example, to\ndisable tracing of SQL queries, remove the `disco-java-agent-sql-plugin` from your `pluginPath`.\n\nTo add new DiSCo plugins, just add them as a JAR to your `pluginPath` directory and restart your app. For example, if a\nplugin to intercept HTTP requests made with OkHTTP was developed, you should be able to just build the plugin and add the\nJAR to your `pluginPath`. No additional configuration from the agent should be required, but feel free to open an issue if it\ndoesn't work out of the box.\n\n### Troubleshooting\n\nWhen troubleshooting the agent, one of the first steps is to enable logging for the agent. The agent uses Apache Commons Logging,\nso you may need to add a bridge like `log4j-jcl` or `jcl-over-slf4j` to your classpath to see logs. To configure the log level,\nset this system property:\n\n```\nlogging.level.com.amazonaws.xray = DEBUG\n```\n\nIf your problem cannot be diagnosed with X-Ray logs, then DiSCo logs could help too.\nTo enable DiSCo logs, append the classpath of the DiSCo logger to your JVM argument that's enabling the X-Ray agent, like so:\n\n```\n-javaagent:/<path-to-disco>/disco-java-agent.jar=pluginPath=/<path-to-disco>/disco-plugins:loggerfactory=software.amazon.disco.agent.reflect.logging.StandardOutputLoggerFactory:verbose\n```\n\nFor more troubleshooting steps, see the [official documentation](https://docs.aws.amazon.com/xray/latest/devguide/aws-x-ray-auto-instrumentation-agent-for-java.html#XRayAutoInstrumentationAgent-Troubleshooting).\n\n## Developing on the Agent\n\n### Structure of this repo\n\nThis repository contains the X-Ray Agent as a DiSCo plugin. Note that this is NOT a proper Java agent with \na premain and bytecode manipulation. Rather it is a *plugin* to extend the functionality of a proper Java agent \nlike the one described. To learn more about DiSCo plugins and how they\nwork with the DiSCo java agent, see the [DiSCo documentation](https://github.com/awslabs/disco/tree/main/disco-java-agent/disco-java-agent).\n\nThe layout of this project is:\n\n[`aws-xray-agent`](https://github.com/aws/aws-xray-java-agent/tree/main/aws-xray-agent) - The source code of the AWS\nX-Ray agent plugin. This contains the hooks that allow our plugin to communicate with the DiSCo Agent. It is also where\ninstrumentation using the X-Ray SDK happens.\n\n[`aws-xray-agent-plugin`](https://github.com/aws/aws-xray-java-agent/tree/main/aws-xray-agent-plugin) - This package\ncontains no source code. It only uses a series of build rules to bundle the above source code into a JAR that represents\na DiSCo plugin, run integration tests against that JAR, then finally bundle that JAR and all needed DiSCo dependencies into\nan archive for the end user.\n\n[`aws-xray-agent-benchmark`](https://github.com/aws/aws-xray-java-agent/tree/main/aws-xray-agent-benchmark) - This package\nalso contains no source code. It runs tests to compare the performance of the X-Ray Agent and the X-Ray SDK.\n\n### Building from Source\n\nIf there are unreleased changes on the `main` branch that you'd like to try out early, you can build the agent from its source code. The agent uses Gradle to manage its builds and produce the `xray-agent.zip` artifact that is ultimately distributed with [releases](https://github.com/aws/aws-xray-java-agent/releases). You can build the agent distribution locally by running the following commands:\n\n```bash\ngit clone https://github.com/aws/aws-xray-java-agent.git\ncd aws-xray-java-agent/\n./gradlew build\n```\n\nNow, the latest changes on `main` will be bundled into a ZIP file located at `aws-xray-agent-plugin/build/dist/xray-agent.zip`. This ZIP file is structured the same as the one described in the [installation documentation](https://docs.aws.amazon.com/xray/latest/devguide/aws-x-ray-auto-instrumentation-agent-for-java.html#XRayAutoInstrumentationAgent-GettingStarted), so you can follow those instructions using this artifact. For example, if you'd like to extract the X-Ray Agent JAR and its dependencies to use in your project, you could run the following commands:\n\n```bash\ncd aws-xray-agent-plugin/build/dist/\nunzip xray-agent.zip                 # Unpackages Agent JAR and disco dependencies into a disco directory\ncp -r disco /path/to/your/project    # Copies the disco directory for use in your project with the -javaagent argument\n```\n\n## Getting Help\n\nPlease use these community resources for getting help.\n\n* If you think you may have found a bug or need assistance, please open an [issue](https://github.com/aws/aws-xray-java-agent/issues/new).\n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n* Ask a question in the [AWS X-Ray Forum](https://forums.aws.amazon.com/forum.jspa?forumID=241&start=0).\n* For contributing guidelines refer to [CONTRIBUTING.md](https://github.com/aws/aws-xray-java-agent/blob/main/CONTRIBUTING.md).\n\n## License\n\nThe AWS X-Ray SDK Java Agent is licensed under the Apache 2.0 License. See LICENSE and NOTICE.txt for more information.\n", "release_dates": ["2021-06-03T18:21:55Z", "2021-04-27T21:36:14Z", "2020-12-04T23:19:54Z", "2020-09-03T17:56:50Z", "2019-12-02T23:19:51Z"]}, {"name": "aws-xray-sdk-dotnet", "description": "The official AWS X-Ray SDK for .NET. ", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![Build Status](https://github.com/aws/aws-xray-sdk-dotnet/actions/workflows/ci.yml/badge.svg)\n\n### :mega: OpenTelemetry .NET with AWS X-Ray\n\nAWS X-Ray supports using OpenTelemetry .NET and the AWS Distro for OpenTelemetry (ADOT) Collector to instrument your application and send trace data to X-Ray. The OpenTelemetry SDKs are an industry-wide standard for tracing instrumentation. They provide more instrumentations and have a larger community for support, but may not have complete feature parity with the X-Ray SDKs. See [choosing between the ADOT and X-Ray SDKs](https://docs.aws.amazon.com/xray/latest/devguide/xray-instrumenting-your-app.html#xray-instrumenting-choosing) for more help with choosing between the two.\n\nIf you want additional features when tracing your .NET applications, please [open an issue on the OpenTelemetry .NET Instrumentation repository](https://github.com/open-telemetry/opentelemetry-dotnet-contrib/issues/new?labels=enhancement&template=miscellaneous.md&title=X-Ray%20Compatible%20Feature%20Request).\n\n# AWS X-Ray SDK for .NET and .NET Core\n\n![Screenshot of the AWS X-Ray console](images/example_servicemap.png?raw=true)\n\n## Installing\n\nThe AWS X-Ray SDK for .NET and .NET Core (.netstandard 2.0 and above) is in the form of Nuget packages. You can install the packages from [Nuget](https://www.nuget.org/packages?q=AWSXRayRecorder) gallery or from Visual Studio editor. Search `AWSXRayRecorder*` to see various middlewares available.  \n\n## Getting Help\n\nUse the following community resources for getting help with the SDK. We use the GitHub issues for tracking bugs and feature requests.\n\n* Ask a question in the [AWS X-Ray Forum](https://forums.aws.amazon.com/forum.jspa?forumID=241&start=0).\n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n* If you think you may have found a bug, open an [issue](https://github.com/aws/aws-xray-sdk-dotnet/issues/new).\n\n## Opening Issues\n\nIf you encounter a bug with the AWS X-Ray SDK for .NET/.NET Core, we want to hear about\nit. Before opening a new issue, search the [existing issues](https://github.com/aws/aws-xray-sdk-dotnet/issues) to see if others are also experiencing the issue. Include platform (.NET/ .NET Core). \nIn addition, include the repro case when appropriate.\n\nThe GitHub issues are intended for bug reports and feature requests. For help and questions about using the AWS X-Ray SDK for .NET and .NET Core, use the resources listed\nin the [Getting Help](https://github.com/aws/aws-xray-sdk-dotnet#getting-help) section. Keeping the list of open issues lean helps us respond in a timely manner.\n\n## Documentation\n\nThe [developer guide](https://docs.aws.amazon.com/xray/latest/devguide) provides in-depth guidance about using the AWS X-Ray service.\nFollowing API reference documentation provides guidance for using the SDK and module-level documentation.\n* The [API Reference for .NET](http://docs.aws.amazon.com/xray-sdk-for-dotnet/latest/reference/index.html)\n* The [API Reference for .NET Core](http://docs.aws.amazon.com/xray-sdk-for-dotnetcore/latest/reference/index.html)\n* AWS X-Ray SDK Documentation for [.NET SDK](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet.html)\n* [Sample Apps](https://github.com/aws-samples/aws-xray-dotnet-webapp)\n\n## Quick Start\n\n1. [Configuration](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#configuration)\n2. [ASP.NET Core Framework](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#aspnet-core-framework-net-core--nuget)\n3. [ASP.NET Framework](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#aspnet-framework-net--nuget)\n4. [Trace AWS SDK request](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#trace-aws-sdk-request-net-and-net-core--nuget) \n5. [Trace out-going HTTP requests](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#trace-out-going-http-requests-net-and-net-core--nuget)\n6. [Trace Query to SQL Server](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#trace-query-to-sql-server-net-and-net-core--nuget)\n7. [Trace SQL Query through Entity Framework](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#trace-sql-query-through-entity-framework-net-and-net-core--nuget)\n8. [Multithreaded Execution](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#multithreaded-execution-net-and-net-core--nuget)\n9. [Trace custom methods ](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#trace-custom-methods-net-and-net-core)\n10. [Creating custom Segment/Subsegment](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#creating-custom-segmentsubsegment-net-and-net-core)\n11. [Adding metadata/annotations](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#adding-metadataannotations-net-and-net-core)\n12. [AWS Lambda support (.NET Core)](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#aws-lambda-support-net-core)\n13. [ASP.NET Core on AWS Lambda](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#aspnet-core-on-aws-lambda-net-core)\n14. [Logging](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#logging-net)\n15. [Enabling X-Ray on Elastic Beanstalk](https://docs.aws.amazon.com/xray/latest/devguide/xray-services-beanstalk.html)\n16. [Enabling X-Ray on AWS Lambda](https://docs.aws.amazon.com/xray/latest/devguide/xray-services-lambda.html)\n\n## Configuration\n\n### .NET\n\nYou can configure X-Ray in the `appsettings` of your `App.config` or `Web.config` file.\n\n```xml\n<configuration>\n  <appSettings>\n    <add key=\"DisableXRayTracing\" value=\"false\"/>\n    <add key=\"AWSXRayPlugins\" value=\"EC2Plugin, ECSPlugin, ElasticBeanstalkPlugin\"/>\n    <add key=\"SamplingRuleManifest\" value=\"JSONs\\DefaultSamplingRules.json\"/>\n    <add key=\"AwsServiceHandlerManifest\" value=\"JSONs\\AWSRequestInfo.json\"/>\n    <add key=\"UseRuntimeErrors\" value=\"false\"/>\n    <add key=\"CollectSqlQueries\" value=\"false\"/>\n  </appSettings>\n</configuration>\n```\n\n### .NET Core\n\nFollowing are the steps to configure your .NET Core project with X-Ray.\n\na) In `appsettings.json` file, configure items under `XRay` key\n\n```\n{\n  \"XRay\": {\n    \"DisableXRayTracing\": \"false\",\n    \"SamplingRuleManifest\": \"SamplingRules.json\",\n    \"AWSXRayPlugins\": \"EC2Plugin, ECSPlugin, ElasticBeanstalkPlugin\",\n    \"AwsServiceHandlerManifest\": \"JSONs\\AWSRequestInfo.json\",\n    \"UseRuntimeErrors\":\"false\",\n    \"CollectSqlQueries\":\"false\"\n  }\n}\n```\n\nb) Register `IConfiguration` instance with X-Ray:\n\n```csharp\nusing Amazon.XRay.Recorder.Core;\nAWSXRayRecorder.InitializeInstance(configuration); // pass IConfiguration object that reads appsettings.json file\n```\n\n*Note*:  \n1. You should configure this before initialization of `AWSXRayRecorder` instance and using any AWS X-Ray methods.  \n2. If you manually need to configure `IConfiguration` object refer: [Link](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration/?tabs=basicconfiguration)  \n3. For more information on configuration, please refer : [Link](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet-configuration.html)\n\n### Programmatic Configuration (.NET and .NET Core)\n\nAlternatively, you can also set up the `AWSXRayRecorder` instance programmatically by using the `AWSXRayRecorderBuilder` class instead of a configuration file. \nFor initializing an AWSXRayRecorder instance with default configurations, simply do the following.\n```csharp\nusing Amazon.XRay.Recorder.Core;\n\nAWSXRayRecorder recorder = new AWSXRayRecorderBuilder().Build();\nAWSXRayRecorder.InitializeInstance(recorder: recorder);\n```\n\nThe following code initializes an `AWSXRayRecorder` instance with a custom `IStreamingStrategy` and a custom `ISamplingStrategy`. \n```csharp\nusing Amazon.XRay.Recorder.Core;\n\nAWSXRayRecorder recorder = new AWSXRayRecorderBuilder().WithStreamingStrategy(new CustomStreamingStrategy()).WithSamplingStrategy(CustomSamplingStrategy()).Build();\nAWSXRayRecorder.InitializeInstance(recorder: recorder);\n```\n\n*Note*:\n1. `CustomStreamingStrategy` and `CustomSamplingStrategy` must implement `IStreamingStrategy` and `ISamplingStrategy` before being used to build the `recorder`.\n2. `recorder` must be instantiated using `AWSXRayRecorder.InitializeInstance(recorder: recorder)` before being used in the program. \n\n\n\n\n## How to Use\n\n### Incoming Requests\n\n### ASP.NET Core Framework (.NET Core) : [Nuget](https://www.nuget.org/packages/AWSXRayRecorder.Handlers.AspNetCore/)\n\nYou can instrument X-Ray for your `ASP.NET Core` App in the `Configure()` method of `Startup.cs` file of your project.  \n*Note* :  \n1. For .Net Core 2.1 and above, use `app.UseXRay()` middleware **before** any other middleware to trace incoming requests. For .Net Core 2.0 place the `app.UseXRay()` middleware **after** the `app.UseExceptionHandler(\"/Error\")` in order to catch exceptions. You would be able to see any runtime exception with its stack trace, however, the status code might show 200 due to a known limitation of the ExceptionHandler middleware in .Net Core 2.0. \n2. You need to install `AWSXRayRecorder.Handlers.AspNetCore` nuget package. This package adds extension methods to the `IApplicationBuilder` to make it easy to register AWS X-Ray to the ASP.NET Core HTTP pipeline.\n\nA) With default configuration:\n\n* For .Net Core 2.1 and above: \n\n```csharp\nusing Microsoft.AspNetCore.Builder;\n\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env)\n{\n    app.UseXRay(\"SampleApp\"); // name of the app\n    app.UseExceptionHandler(\"/Error\");\n    app.UseStaticFiles(); // rest of the middlewares\n    app.UseMVC();\n}\n```\n\n* For .Net Core 2.0: \n\n```csharp\nusing Microsoft.AspNetCore.Builder;\n\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env)\n{\n    app.UseExceptionHandler(\"/Error\");\n    app.UseXRay(\"SampleApp\"); // name of the app\n    app.UseStaticFiles(); // rest of the middlewares\n    app.UseMVC();\n}\n```\n\nB) With custom X-Ray configuration  \n\n```csharp\nusing Microsoft.AspNetCore.Builder;\n\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env)\n{\n    app.UseExceptionHandler(\"/Error\");\n    app.UseXRay(\"SampleApp\",configuration); // IConfiguration object is not required if you have used \"AWSXRayRecorder.InitializeInstance(configuration)\" method\n    app.UseStaticFiles(); // rest of the middlewares\n    app.UseMVC();\t\n}\n```\n\nInstead of name you can also pass `SegmentNamingStrategy` in the above two ways. Please refer: [Link](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-dotnet-messagehandler.html#xray-sdk-dotnet-messagehandler-naming)  \n\n### ASP.NET Framework (.NET) : [Nuget](https://www.nuget.org/packages/AWSXRayRecorder.Handlers.AspNet/) \n\n**HTTP Message handler for ASP.NET framework**  \nRegister your application with X-Ray in the `Init()` method of ***Global.asax*** file\n\n```csharp\nusing Amazon.XRay.Recorder.Handlers.AspNet;\n\npublic class MvcApplication : System.Web.HttpApplication\n{\n     public override void Init()\n     {\n        base.Init();\n        AWSXRayASPNET.RegisterXRay(this, \"ASPNETTest\"); // default name of the web app\n     }\n}\n```\n\nAt the start of each Http request, a `segment` is created and stored in the `context` (Key : AWSXRayASPNET.XRayEntity) of `HttpApplication` instance. If users write their custom error handler for ASP.NET framework, they can access `segment` for the current request by following way : \n\n```csharp\n<%@ Import Namespace=\"Amazon.XRay.Recorder.Handlers.AspNet\" %>\n<%@ Import Namespace=\"Amazon.XRay.Recorder.Core.Internal.Entities\" %>\n<script runat=\"server\">\n  protected void Page_Load(object sender, EventArgs e)\n  {\n     var context = System.Web.HttpContext.Current.ApplicationInstance.Context;\n     var segment = (Segment) context.Items[AWSXRayASPNET.XRayEntity]; // get segment from the context\n     segment.AddMetadata(\"Error\",\"404\");\n  }\n</script>\n```\n\n### Trace AWS SDK request (.NET and .NET Core) : [Nuget](https://www.nuget.org/packages/AWSXRayRecorder.Handlers.AwsSdk/)\n\n```csharp\nusing Amazon.XRay.Recorder.Handlers.AwsSdk;\n\nAWSSDKHandler.RegisterXRayForAllServices(); //place this before any instantiation of AmazonServiceClient\nAmazonDynamoDBClient client = new AmazonDynamoDBClient(RegionEndpoint.USWest2); // AmazonDynamoDBClient is automatically registered with X-Ray\n```\n\nMethods of `AWSSDKHandler` class:\n\n```csharp\nAWSSDKHandler.RegisterXRayForAllServices(); // all instances of AmazonServiceClient created after this line are registered\n\nAWSSDKHandler.RegisterXRay<IAmazonDynamoDB>(); // Registers specific type of AmazonServiceClient : All instances of IAmazonDynamoDB created after this line are registered\n\nAWSSDKHandler.RegisterXRayManifest(String path); // To configure custom AWS Service Manifest file. This is optional, if you have followed \"Configuration\" section\n```\n\n### Trace out-going HTTP requests (.NET and .NET Core) : [Nuget](https://www.nuget.org/packages/AWSXRayRecorder.Handlers.System.Net/)\n\n#### Using `System.Net.HttpWebRequest`\n\n#### Synchronous request\n\nAn extension method `GetResponseTraced()` is provided to trace `GetResponse()` in `System.Net.HttpWebRequest` class. If you want to trace the out-going HTTP request, call the `GetResponseTraced()` instead of `GetResponse()`. The extension method will generate a trace subsegment, inject the trace header to the out-going HTTP request header and collect trace information. \n\n```csharp\nusing Amazon.XRay.Recorder.Handlers.System.Net;\n\nHttpWebRequest request = (HttpWebRequest) WebRequest.Create(URL); // enter desired url\n\n// Any other configuration to the request\n\nrequest.GetResponseTraced();\n```\n\nfor query parameter stripped http requests in trace \n\n```csharp\nusing Amazon.XRay.Recorder.Handlers.System.Net;\n\nHttpWebRequest request = (HttpWebRequest) WebRequest.Create(URL); // enter desired url\n\n// Any other configuration to the request\n\nrequest.GetResponseTraced(true);\n```\n\n#### Asynchronous request\n\nAn extension method `GetAsyncResponseTraced()` is provided to trace `GetResponseAsync()` in `System.Net.HttpWebRequest` class. If you want to trace the out-going HTTP request, call the `GetAsyncResponseTraced()` instead of `GetResponseAsync()`. The extension method will generate a trace subsegment, inject the trace header to the out-going HTTP request header and collect trace information. \n\n```csharp\nusing Amazon.XRay.Recorder.Handlers.System.Net;\n\nHttpWebRequest request = (HttpWebRequest) WebRequest.Create(URL); // enter desired url\n\n// Any other configuration to the request\n\nrequest.GetAsyncResponseTraced();\n```\n\nfor query parameter stripped http requests in trace \n\n```csharp\nusing Amazon.XRay.Recorder.Handlers.System.Net;\n\nHttpWebRequest request = (HttpWebRequest) WebRequest.Create(URL); // enter desired url\n\n// Any other configuration to the request\n\nrequest.GetAsyncResponseTraced(true);\n```\n\n#### Using `System.Net.HttpClient`\n\nA handler derived from `DelegatingHandler` is provided to trace the `HttpMessageHandler.SendAsync` method\n\n```csharp\nusing Amazon.XRay.Recorder.Handlers.System.Net;\n\nvar httpClient = new HttpClient(new HttpClientXRayTracingHandler(new HttpClientHandler()));\n\n// Any other configuration to the client\n\nhttpClient.GetAsync(URL);\n```\n\nIf you want to santize the Http request tracing then define the Tracing Handler as - \n\n```CSharp\n\nusing Amazon.XRay.Recorder.Handlers.System.Net;\n\nvar httpClient = new HttpClient(new HttpClientXRaySanitizedTracingHandler(new HttpClientHandler()));\n\n// Any other configuration to the client\n\nhttpClient.GetAsync(URL);\n\n```\n\n#### Using `System.Net.Http.HttpClientFactory` (.Net Core 2.1 and above)\n\nThe `Amazon.XRay.Recorder.Handlers.System.Net` package includes a delegate that can be used to trace outbound requests without the need to specifically wrap outbound requests from that class.\n\nRegister the `HttpClientXRayTracingHandler` as a middleware for your http client.\n\n```csharp\nservices.AddHttpClient<IFooClient, FooClient>()\n        .AddHttpMessageHandler<HttpClientXRayTracingHandler>();\n```\nor\n\n```csharp\nservices.AddHttpClient(\"foo\")\n        .ConfigurePrimaryHttpMessageHandler(() =>\n        {\n            return new HttpClientXRayTracingHandler(new HttpClientHandler());\n        });\n```\n\nAnd to get sanitized http requests in tracing \n\n```csharp\nservices.AddHttpClient<IFooClient, FooClient>()\n        .AddHttpMessageHandler<HttpClientXRaySanitizedTracingHandler>();\n```\nor\n\n```csharp\nservices.AddHttpClient(\"foo\")\n        .ConfigurePrimaryHttpMessageHandler(() =>\n        {\n            return new HttpClientXRaySanitizedTracingHandler(new HttpClientHandler());\n        });\n```\n\nUse the above client factory to create clients with outgoing requests traced.\n\n```csharp\nvar client = _clientFactory.CreateClient(\"foo\");\nvar request = new HttpRequestMessage(HttpMethod.Get, \"https://www.foobar.com\");\nvar response = await client.SendAsync(request);\n```\n\n### Trace Query to SQL Server (.NET and .NET Core) : [Nuget](https://www.nuget.org/packages/AWSXRayRecorder.Handlers.SqlServer/)\n\nThe SDK provides a wrapper class for `System.Data.SqlClient.SqlCommand`. The wrapper class can be used interchangable with `SqlCommand` class. By replacing instance of `SqlCommand` to `TraceableSqlCommand`, synchronized/asynchronized method will automatically generate subsegment for the SQL query.\n\nFollowing examples illustrate the use of `TraceableSqlCommand` to automatically trace SQL Server queries using Synchronous/Asynchronous methods:\n\n#### Synchronous query\n\n```csharp\nusing Amazon.XRay.Recorder.Handlers.SqlServer;\n\nusing (var connection = new SqlConnection(\"fake connection string\"))\nusing (var command = new TraceableSqlCommand(\"SELECT * FROM products\", connection))\n{\n    command.ExecuteNonQuery();\n}\n```\n\n#### Asynchronous query\n\n```csharp\nusing Amazon.XRay.Recorder.Handlers.SqlServer;\n\nusing (var connection = new SqlConnection(ConnectionString))\n{\n    var command = new TraceableSqlCommand(\"SELECT * FROM Products FOR XML AUTO, ELEMENTS\", connection);\n    command.Connection.Open();\n    await command.ExecuteXmlReaderAsync();\n}\n```\n\n#### Capture SQL Query text in the traced SQL calls to SQL Server\n\n You can also opt in to capture the `SqlCommand.CommandText` as part of the subsegment created for your SQL query. The collected `SqlCommand.CommandText` will appear as `sanitized_query` in the subsegment JSON. By default, this feature is disabled due to security reasons. If you want to enable this feature, it can be done in two ways. First, by setting the `CollectSqlQueries` to `true` in the global configuration for your application as follows:\n\n##### For .Net (In `appsettings` of your `App.config` or `Web.config` file)\n\n```xml\n<configuration>\n  <appSettings>\n    <add key=\"CollectSqlQueries\" value=\"true\">\n  </appSettings>\n</configuration>\n```\n\n##### For .Net Core (In `appsettings.json` file, configure items under XRay key)\n\n```json\n{\n  \"XRay\": {\n    \"CollectSqlQueries\":\"true\"\n  }\n}\n```\n\nThis will enable X-Ray to collect all the sql queries made to SQL Server by your application.\n\nSecondly, you can set the `collectSqlQueries` parameter in the `TraceableSqlCommand` instance as `true` to collect the SQL query text for SQL Server query calls made using this instance. If you set this parameter as `false`, it will disable the CollectSqlQuery feature for this `TraceableSqlCommand` instance.\n\n```csharp\nusing Amazon.XRay.Recorder.Handlers.SqlServer;\n\nusing (var connection = new SqlConnection(\"fake connection string\"))\nusing (var command = new TraceableSqlCommand(\"SELECT * FROM products\", connection, collectSqlQueries: true))\n{\n    command.ExecuteNonQuery();\n}\n```\n\n*NOTE:* \n1. You should **not** enable either of these properties if you are including sensitive information as clear text in your SQL queries.\n2. Parameterized values will appear in their tokenized form and will not be expanded.\n3. The value of `collectSqlQueries` in the `TraceableSqlCommand` instance overrides the value set in the global configuration using the `CollectSqlQueries` property.\n\n### Trace SQL Query through Entity Framework (.NET and .NET Core) : [Nuget](https://www.nuget.org/packages/AWSXRayRecorder.Handlers.EntityFramework/)\n\n#### Setup\n\n##### .NET Core\n\nAWS XRay SDK for .NET Core provides interceptor for tracing SQL query through Entity Framework Core (>=3.0).\n\nFor how to start with Entity Framework Core in an ASP.NET Core web app, please take reference to [Link](https://docs.microsoft.com/en-us/aspnet/core/data/ef-mvc/?view=aspnetcore-3.1)\n\n*NOTE:*\n\n* You need to install `AWSXRayRecorder.Handlers.EntityFramework` nuget package. This package adds extension methods to the `DbContextOptionsBuilder` to make it easy to register AWS X-Ray interceptor.\n* Not all database provider support Entity Framework Core 3.0 and above, please make sure that you are using the [Nuget package](https://docs.microsoft.com/en-us/ef/core/providers/?tabs=dotnet-core-cli) with a compatible version (EF Core >= 3.0).\n\n*Known Limitation (as of 12-03-2020):* If you're using another `DbCommandInterceptor` implementation along with the `AddXRayInterceptor` in the `DbContext`, it may not work as expected and you may see a \"EntityNotAvailableException\" from the XRay EFCore interceptor. This is due to [`AsyncLocal`](https://docs.microsoft.com/en-us/dotnet/api/system.threading.asynclocal-1?view=netcore-2.0) not being able to maintain context across the `ReaderExecutingAsync` and `ReaderExecutedAsync` methods. Ref [here](https://github.com/dotnet/efcore/issues/22766) for more details on the issue.\n\nIn order to trace SQL query, you can register your `DbContext` with `AddXRayInterceptor()` accordingly in the `ConfigureServices` method in `startup.cs` file. \n\nFor instance, when dealing with MySql server using Nuget: [Pomelo.EntityFrameworkCore.MySql](https://www.nuget.org/packages/Pomelo.EntityFrameworkCore.MySql) (V 3.1.1). \n\n```csharp\nusing Microsoft.EntityFrameworkCore;\n\npublic void ConfigureServices(IServiceCollection services)\n{ \n    services.AddDbContext<your_DbContext>(options => options.UseMySql(your_connectionString).AddXRayInterceptor());\n}\n```\n\nAlternatively, you can register `AddXRayInterceptor()` in the `Onconfiguring` method in your `DbContext` class. Below we are using Nuget: [Microsoft.EntityFrameworkCore.Sqlite](https://www.nuget.org/packages/Microsoft.EntityFrameworkCore.Sqlite) (V 3.1.2)\n\n```csharp\nusing Microsoft.EntityFrameworkCore;\n\npublic class your_DbContext : DbContext \n{\n\tprotected override void OnConfiguring(DbContextOptionsBuilder options)\n    => options.UseSqlite(your_connectionString).AddXRayInterceptor();\n}\n```\n\nThe connection string can be either hard coded or configured from `appsettings.json` file.\n\n##### .NET \n\nAWS XRay SDK for .NET provides interceptor for tracing SQL query through Entity Framework 6 (>= 6.2.0). \n\nFor how to start with Entity Framework 6 in an ASP.NET web app, please take reference to [link](https://docs.microsoft.com/en-us/aspnet/mvc/overview/getting-started/getting-started-with-ef-using-mvc/creating-an-entity-framework-data-model-for-an-asp-net-mvc-application).\n\nFor instrumentation, you will need to install `AWSXRayRecorder.Handlers.EntityFramework` nuget package and call `AWSXRayEntityFramework6.AddXRayInterceptor()` in your code. Make sure to call it **only once** to avoid duplicate tracing.\n\nFor instance, you can call `AddXRayInterceptor()` in the `Application_Start` method of **Global.asax** file.\n\n```\nusing Amazon.XRay.Recorder.Handlers.EntityFramework;\n\nprotected void Application_Start()\n{\n    AWSXRayEntityFramework6.AddXRayInterceptor();\n}\n```\n\nOr you can call it in the `DbConfiguration` class if there is one in your application to configure execution policy.\n\n```\nusing Amazon.XRay.Recorder.Handlers.EntityFramework;\n\npublic class YourDbConfiguration : DbConfiguration\n{\n    public YourDbConfiguration()\n    {\n        AWSXRayEntityFramework6.AddXRayInterceptor();\n    }\n}\n```\n\n#### Capture SQL Query text in the traced SQL calls to SQL Server\n\nYou can also opt in to capture the `DbCommand.CommandText` as part of the subsegment created for your SQL query. The collected `DbCommand.CommandText` will appear as `sanitized_query` in the subsegment JSON. By default, this feature is disabled due to security reasons. \n\n##### .NET Core\n\nIf you want to enable this feature, it can be done in two ways. First, by setting the `CollectSqlQueries` to **true** in the `appsettings.json` file as follows:\n\n```json\n{\n  \"XRay\": {\n    \"CollectSqlQueries\":\"true\"\n  }\n}\n```\n\nSecondly, you can set the `collectSqlQueries` parameter in the `AddXRayInterceptor()` as **true** to collect the SQL query text. If you set this parameter as **false**, it will disable the `collectSqlQueries` feature for this `AddXRayInterceptor()`. Opting in `AddXRayInterceptor()` has the highest execution priority, which will override the configuration item in `appsettings.json` mentioned above.\n\n```csharp\nusing Microsoft.EntityFrameworkCore;\n\npublic void ConfigureServices(IServiceCollection services)\n{\n    services.AddDbContext<your_DbContext>(options => options.UseMySql(your_connectionString).AddXRayInterceptor(true));\n}\n```\n\nOr\n\n```csharp\nusing Microsoft.EntityFrameworkCore;\n\npublic class your_DbContext : DbContext \n{\n\tprotected override void OnConfiguring(DbContextOptionsBuilder options)\n    => options.UseSqlite(your_connectionString).AddXRayInterceptor(true);\n}\n```\n\n##### .NET \n\nYou can enable tracing SQL query text for EF 6 interceptor in the `Web.config` file.\n\n```xml\n<configuration>\n  <appSettings>\n    <add key=\"CollectSqlQueries\" value=\"true\"/>\n  </appSettings>\n</configuration>\n```\n\nYou can also pass **true** to `AddXRayInterceptor()` to collect SQL query text, otherwise pass **false** to disable. Opting in `AddXRayInterceptor()` has the highest execution priority, which will override the configuration item in `Web.config` mentioned above.\n\n```\nusing Amazon.XRay.Recorder.Handlers.EntityFramework;\n\nAWSXRayEntityFramework6.AddXRayInterceptor(true);\n```\n\n### Multithreaded Execution (.NET and .NET Core) : [Nuget](https://www.nuget.org/packages/AWSXRayRecorder.Core/)\n\nIn multithreaded execution, X-Ray context from current to its child thread is automatically set.  \n\n```csharp\nusing Amazon.XRay.Recorder.Core;\n\nprivate static void TestMultiThreaded()\n{\n    int numThreads = 3;\n    AWSXRayRecorder.Instance.BeginSegment(\"MainThread\");\n    Thread[] t= new Thread[numThreads];\n \n    for(int i = 0; i < numThreads; i++)\n    {\n    \tt[i] = new Thread(()=>MakeHttpRequest(i)); \n        t[i].Start();\n    }\n    for (int i = 0; i < numThreads; i++)\n    {\n        t[i].Join();\n    }\n\n    AWSXRayRecorder.Instance.EndSegment();\n}\n\nprivate static void MakeHttpRequest(int i)\n{\n    AWSXRayRecorder.Instance.TraceMethodAsync(\"Thread \"+i, CreateRequestAsync<HttpResponseMessage>).Wait();\n}\n\nprivate static async Task<HttpResponseMessage> CreateRequestAsync <TResult>()\n{\n    var request = new HttpClient();\n    var result = await request.GetAsync(URL); // Enter desired url\n    return result;\n}\n```\n\n*Note*:\n1. Context used to save traces in .NET : [CallContext](https://docs.microsoft.com/en-us/dotnet/api/system.runtime.remoting.messaging.callcontext?view=netframework-4.5)\n2. Context used to save traces in .NET Core : [AsyncLocal](https://docs.microsoft.com/en-us/dotnet/api/system.threading.asynclocal-1?view=netcore-2.0)\n\n### Trace custom methods (.NET and .NET Core)\n\nIt may be useful to further decorate portions of an application for which performance is critical. Generating subsegments around these hot spots will help in understanding their impact on application performance.\n\n#### Synchronous method\n\n```csharp\nusing Amazon.XRay.Recorder.Core;\n\nAWSXRayRecorder.Instance.TraceMethod(\"custom method\", () => DoSomething(arg1, arg2, arg3));\n```\n\n#### Asynchronous method\n\n```csharp\nusing Amazon.XRay.Recorder.Core;\n\nvar response = await AWSXRayRecorder.Instance.TraceMethodAsync(\"AddProduct\", () => AddProduct<Document>(product));\n\nprivate async Task<Document> AddProduct <TResult>(Product product)\n{\n    var document = new Document();\n    document[\"Id\"] = product.Id;\n    document[\"Name\"] = product.Name;\n    document[\"Price\"] = product.Price;\n    return await LazyTable.Value.PutItemAsync(document);\n}\n```\n\n### Creating custom Segment/Subsegment (.NET and .NET Core)\n\n#### Segment\n```csharp\nusing Amazon.XRay.Recorder.Core;\n\nAWSXRayRecorder.Instance.BeginSegment(\"segment name\"); // generates `TraceId` for you\ntry\n{\n    DoSometing();\n    // can create custom subsegments\n}\ncatch (Exception e)\n{\n    AWSXRayRecorder.Instance.AddException(e);\n}\nfinally\n{\n    AWSXRayRecorder.Instance.EndSegment();\n}\n```\n\nIf you want to pass custom `TraceId`:\n\n```csharp\nusing Amazon.XRay.Recorder.Core;\n\nString traceId = TraceId.NewId(); // This function is present in : Amazon.XRay.Recorder.Core.Internal.Entities\nAWSXRayRecorder.Instance.BeginSegment(\"segment name\",traceId); // custom traceId used while creating segment\ntry\n{\n    DoSometing();\n    // can create custom subsegments\n}\ncatch (Exception e)\n{\n    AWSXRayRecorder.Instance.AddException(e);\n}\nfinally\n{\n    AWSXRayRecorder.Instance.EndSegment();\n}\n```\n\n#### Subsegment\n\n*Note*: This should only be used after `BeginSegment()` method.  \n```csharp\nusing Amazon.XRay.Recorder.Core;\n\nAWSXRayRecorder.Instance.BeginSubsegment(\"subsegment name\");\ntry\n{\n    DoSometing();\n}\ncatch (Exception e)\n{\n    AWSXRayRecorder.Instance.AddException(e);\n}\nfinally\n{\n    AWSXRayRecorder.Instance.EndSubsegment();\n}\n```\n\n\n\n### Adding metadata/annotations (.NET and .NET Core)\n\n```csharp\nusing Amazon.XRay.Recorder.Core;\nAWSXRayRecorder.Instance.AddAnnotation(\"mykey\", \"my value\");\nAWSXRayRecorder.Instance.AddMetadata(\"my key\", \"my value\");\n```\n\n### AWS Lambda support (.NET Core) \nThe AWS Lambda execution environment by default creates a `Segment` before each Lambda function invocation and sends it to the X-Ray service. AWS X-Ray .NET/Core SDK will make sure there will be a `FacadeSegment` inside the lambda context so that you can instrument your application successfully through subsegments only. `Subsegments` generated inside a Lambda function are attached to this `FacadeSegment` and only subsegments are streamed by the SDK. In addition to the custom subsegments, the middlewares would generate subsegments for outgoing HTTP calls, SQL queries, and AWS SDK calls within the lambda function the same way they do in a normal application.\n\n*Note*: You can only create and close `Subsegment` inside a lambda function. `Segment` cannot be created inside the lambda function. All operations on `Segment` will throw an `UnsupportedOperationException` exception.\n\n```csharp\npublic string FunctionHandler(string input, ILambdaContext context)\n{\n    AWSXRayRecorder recorder = new AWSXRayRecorder();\n    recorder.BeginSubsegment(\"UpperCase\");\n    recorder.BeginSubsegment(\"Inner 1\");\n    String result = input?.ToUpper();\n    recorder.EndSubsegment();\n    recorder.BeginSubsegment(\"Inner 2\");\n    recorder.EndSubsegment();\n    recorder.EndSubsegment();\n    return result;\n}\n```\n\n### Oversampling Mitigation\nOversampling mitigation allows you to ignore a parent segment/subsegment's sampled flag and instead sets the subsegment's sampled flag to false.\nThis ensures that downstream calls are not sampled and this subsegment is not emitted.\n\n```csharp\nusing Amazon.Lambda.Core;\nusing Amazon.Lambda.SQSEvents;\nusing Amazon.XRay.Recorder.Core;\nusing Amazon.SQS;\nusing Amazon.SQS.Model;\n\n[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.Json.JsonSerializer))]\n\nnamespace MyFunction;\n\npublic class Function\n{\n    public string HandleSQSEvent(SQSEvent sqsEvent, ILambdaContext context)\n    {\n        AWSXRayRecorder.Instance.BeginSubsegmentWithoutSampling(\"Processing Event\");\n\n        var client = new AmazonSQSClient();\n\n        var request = new ListQueuesRequest();\n\n        var response = client.ListQueuesAsync(request);\n\n        foreach (var url in response.Result.QueueUrls)\n        {\n            Console.WriteLine(url);\n        }\n\n        AWSXRayRecorder.Instance.EndSubsegment();\n\n        return \"Success\";\n    }\n}\n```\n\nThe code below demonstrates overriding the sampled flag based on the SQS messages sent to Lambda.\n\n```csharp\nusing Amazon.Lambda.Core;\nusing Amazon.Lambda.SQSEvents;\nusing Amazon.XRay.Recorder.Core;\nusing Amazon.XRay.Recorder.Core.Lambda;\n\n[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.Json.JsonSerializer))]\n\nnamespace MyFunction;\n\npublic class Function\n{\n    public string HandleSQSEvent(SQSEvent sqsEvent, ILambdaContext context)\n    {\n\n        foreach (SQSEvent.SQSMessage sqsMessage in sqsEvent.Records)\n        {\n            if (SQSMessageHelper.IsSampled(sqsMessage))\n            {\n                AWSXRayRecorder.Instance.BeginSubsegment(\"Processing Message\");\n            }\n            else\n            {\n                AWSXRayRecorder.Instance.BeginSubsegmentWithoutSampling(\"Processing Message\");\n            }\n\n\n            // Do my processing work here\n            Console.WriteLine(\"Doing processing work\");\n\n            // End my subsegment\n            AWSXRayRecorder.Instance.EndSubsegment();\n        }\n\n        return \"Success\";\n    }\n}\n```\n\n### ASP.NET Core on AWS Lambda (.NET Core)\n\nWe support instrumenting ASP.NET Core web app on Lambda. Please follow the steps of [ASP.NET Core](https://github.com/aws/aws-xray-sdk-dotnet/tree/master#aspnet-core-framework-net-core--nuget) instrumentation.\n\n### Logging (.NET)\n\nThe AWS X-Ray .NET SDK share the same logging mechanism as [AWS .NET SDK](https://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/net-dg-config-other.html#config-setting-awslogging). If the application had already configured logging for AWS .NET SDK, it should just work for AWS X-Ray .NET SDK.\nThe recommended way to configure an application is to use the <aws> element in the project\u2019s `App.config` or `Web.config` file.\n\n```xml\n<configuration>\n  <configSections>\n    <section name=\"aws\" type=\"Amazon.AWSSection, AWSSDK.Core\"/>\n  </configSections>\n  <aws>\n    <logging logTo=\"Log4Net\"/>\n  </aws>\n</configuration>\n```\n\nOther ways to configure logging is to edit the <appsetting> element in the `App.config` or `Web.config` file, and set property values in the `AWSConfig` class. Refer to the following page for more details and example : [Link](http://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/net-dg-config.html)\n\n\n### Logging (.NET Core)\n\nThe AWS X-Ray .NET SDK share the same logging mechanism as [AWS .NET SDK](https://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/net-dg-config-other.html#config-setting-awslogging). To configure logging for .NET Core application, pass one of these options to the `AWSXRayRecorder.RegisterLogger` method.\n\t\nFollowing is the way to configure `log4net` with X-Ray SDK:\n\n```csharp\nusing Amazon;\nusing Amazon.XRay.Recorder.Core;\n\nclass Program\n{\n    static Program()\n    {\n         AWSXRayRecorder.RegisterLogger(LoggingOptions.Log4Net); // Log4Net instance should already be configured before this point\n    }\n}\n``` \n\nlog4net.config example:\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n<log4net>\n  <appender name=\"FileAppender\" type=\"log4net.Appender.FileAppender,log4net\">\n    <file value=\"c:\\logs\\sdk-log.txt\" />\n    <layout type=\"log4net.Layout.PatternLayout\">\n      <conversionPattern value=\"%date [%thread] %level %logger - %message%newline\" />\n    </layout>\n  </appender>\n  <logger name=\"Amazon\">\n    <level value=\"DEBUG\" />\n    <appender-ref ref=\"FileAppender\" />\n  </logger>\n</log4net>\n```\n\n*Note*: For `log4net` configuration, refer : [Link](https://logging.apache.org/log4net/release/manual/configuration.html)\n\n## License\n\nThe AWS X-Ray SDK for .NET and .NET Core is licensed under the Apache 2.0 License. See LICENSE and NOTICE.txt for more information.\n", "release_dates": ["2023-03-01T17:26:28Z", "2022-11-09T23:06:48Z", "2022-11-02T16:56:54Z", "2022-07-21T16:49:29Z", "2022-07-14T20:30:47Z", "2021-04-21T22:17:51Z", "2021-03-02T23:16:42Z", "2020-06-10T17:09:34Z", "2020-05-22T07:51:20Z", "2019-10-10T03:18:23Z", "2019-09-05T23:19:49Z", "2019-07-18T22:27:13Z", "2019-05-22T22:41:03Z", "2019-04-24T17:41:12Z", "2019-04-17T17:23:18Z", "2019-02-05T19:44:43Z", "2018-11-02T02:43:13Z", "2018-08-29T00:42:26Z", "2018-06-11T19:16:01Z", "2018-03-20T23:52:49Z"]}, {"name": "aws-xray-sdk-go", "description": "AWS X-Ray SDK for the Go programming language.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![Test](https://github.com/aws/aws-xray-sdk-go/workflows/Test/badge.svg)\n[![Go Report Card](https://goreportcard.com/badge/github.com/aws/aws-xray-sdk-go)](https://goreportcard.com/report/github.com/aws/aws-xray-sdk-go)\n\n### :mega: OpenTelemetry Go with AWS X-Ray\n\nAWS X-Ray supports using OpenTelemetry Go and the AWS Distro for OpenTelemetry (ADOT) Collector to instrument your application and send trace data to X-Ray. The OpenTelemetry SDKs are an industry-wide standard for tracing instrumentation. They provide more instrumentations and have a larger community for support, but may not have complete feature parity with the X-Ray SDKs. See [choosing between the ADOT and X-Ray SDKs](https://docs.aws.amazon.com/xray/latest/devguide/xray-instrumenting-your-app.html#xray-instrumenting-choosing) for more help with choosing between the two.\n\nIf you want additional features when tracing your Go applications, please [open an issue on the OpenTelemetry Go Instrumentation repository](https://github.com/open-telemetry/opentelemetry-go-contrib/issues/new?title=X-Ray%20Compatible%20Feature%20Request).\n\n# AWS X-Ray SDK for Go\n\n![Screenshot of the AWS X-Ray console](/images/example.png?raw=true)\n\n## Installing into GOPATH\n\nThe AWS X-Ray SDK for Go is compatible with Go 1.19 and above.\n\nInstall the SDK using the following command (The SDK's non-testing dependencies will be installed):\nUse `go get` to retrieve the SDK to add it to your `GOPATH` workspace:\n\n```\ngo get github.com/aws/aws-xray-sdk-go\n```\n\nTo update the SDK, use `go get -u` to retrieve the latest version of the SDK.\n\n```\ngo get -u github.com/aws/aws-xray-sdk-go\n```\n\nIf you also want to install SDK's testing dependencies. They can be installed using:\n\n```\ngo get -u -t github.com/aws/aws-xray-sdk-go/...\n```\n\n## Installing using Go Modules\n\nThe latest version of the SDK is the recommended version.\n\nIf you are using Go 1.11 and above, you can install the SDK using Go Modules (in project's go.mod), like so: \n\n```\ngo get github.com/aws/aws-xray-sdk-go\n```\n\nTo get a different specific release version of the SDK use `@<tag>` in your `go get` command. Also, to get the rc version use this command with the specific version.\n\n```\ngo get github.com/aws/aws-xray-sdk-go@v1.0.0\n```\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues for tracking bugs and feature requests.\n\n* Ask a question in the [AWS X-Ray Forum](https://forums.aws.amazon.com/forum.jspa?forumID=241&start=0).\n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n* If you think you may have found a bug, please open an [issue](https://github.com/aws/aws-xray-sdk-go/issues/new).\n\n## Opening Issues\n\nIf you encounter a bug with the AWS X-Ray SDK for Go we would like to hear about it. Search the [existing issues](https://github.com/aws/aws-xray-sdk-go/issues) and see if others are also experiencing the issue before opening a new issue. Please include the version of AWS X-Ray SDK for Go, AWS SDK for Go, Go language, and OS you\u2019re using. Please also include repro case when appropriate.\n\nThe GitHub issues are intended for bug reports and feature requests. For help and questions regarding the use of the AWS X-Ray SDK for Go please make use of the resources listed in the [Getting Help](https://github.com/aws/aws-xray-sdk-go#getting-help) section. Keeping the list of open issues lean will help us respond in a timely manner.\n\n## Documentation\n\nThe [developer guide](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-go.html) provides in-depth guidance on using the AWS X-Ray service and the AWS X-Ray SDK for Go.\n\nSee [aws-xray-sdk-go-sample](https://github.com/aws-samples/aws-xray-sdk-go-sample) for a sample application that provides example of tracing SQL queries, incoming and outgoing request. Follow [README instructions](https://github.com/aws-samples/aws-xray-sdk-go-sample/blob/master/README.md) in that repository to get started with sample application.\n\n## Quick Start\n\n**Configuration**\n\n```go\nimport \"github.com/aws/aws-xray-sdk-go/xray\"\n\nfunc init() {\n  xray.Configure(xray.Config{\n    DaemonAddr:       \"127.0.0.1:2000\", // default\n    ServiceVersion:   \"1.2.3\",\n  })\n}\n```\n***Logger***\n\nxray uses an interface for its logger:\n\n```go\ntype Logger interface {\n  Log(level LogLevel, msg fmt.Stringer)\n}\n\nconst (\n  LogLevelDebug LogLevel = iota + 1\n  LogLevelInfo\n  LogLevelWarn\n  LogLevelError\n)\n```\n\nThe default logger logs to [stdout](https://golang.org/pkg/syscall/#Stdout) at \"info\" and above. To change the logger, call `xray.SetLogger(myLogger)`. There is a default logger implementation that writes to an `io.Writer` from a specified minimum log level. For example, to log to stderr at \"error\" and above:\n\n```go\nxray.SetLogger(xraylog.NewDefaultLogger(os.Stderr, xraylog.LogLevelError))\n```\n\nNote that the `xray.Config{}` fields `LogLevel` and `LogFormat` are deprecated starting from version `1.0.0-rc.10` and no longer have any effect.\n\n***Plugins***\n\nPlugins can be loaded conditionally at runtime. For this purpose, plugins under \"github.com/aws/aws-xray-sdk-go/awsplugins/\" have an explicit `Init()` function. Customer must call this method to load the plugin:\n\n```go\nimport (\n  \"os\"\n\n  \"github.com/aws/aws-xray-sdk-go/awsplugins/ec2\"\n  \"github.com/aws/aws-xray-sdk-go/xray\"\n)\n\nfunc init() {\n  // conditionally load plugin\n  if os.Getenv(\"ENVIRONMENT\") == \"production\" {\n    ec2.Init()\n  }\n\n  xray.Configure(xray.Config{\n    ServiceVersion:   \"1.2.3\",\n  })\n}\n```\n\n**Start a custom segment/subsegment**\nNote that customers using xray.BeginSegment API directly will only be able to evaluate sampling rules based on service name.\n\n```go\n  // Start a segment\n  ctx, seg := xray.BeginSegment(context.Background(), \"service-name\")\n  // Start a subsegment\n  subCtx, subSeg := xray.BeginSubsegment(ctx, \"subsegment-name\")\n  // ...\n  // Add metadata or annotation here if necessary\n  // ...\n  subSeg.Close(nil)\n  // Close the segment\n  seg.Close(nil)\n```\n\n**Generate no-op trace and segment id**\n\nX-Ray Go SDK will by default generate no-op trace and segment id for unsampled requests and secure random trace and entity id for sampled requests. If customer wants to enable generating secure random trace and entity id for all the (sampled/unsampled) requests (this is applicable for trace id injection into logs use case) then they achieve that by setting AWS_XRAY_NOOP_ID environment variable as False.\n\n**Disabling XRay Tracing**\n\nXRay tracing can be disabled by setting up environment variable `AWS_XRAY_SDK_DISABLED` . Disabling XRay can be useful for specific use case like if customer wants to stop tracing in their test environment they can do so just by setting up the environment variable.\n\n\n\n```go\n  // Set environment variable TRUE to disable XRay\n  os.Setenv(\"AWS_XRAY_SDK_DISABLED\", \"TRUE\")\n```\n\n**Capture**\n\n```go\nfunc criticalSection(ctx context.Context) {\n  // This example traces a critical code path using a custom subsegment\n  xray.Capture(ctx, \"MyService.criticalSection\", func(ctx1 context.Context) error {\n    var err error\n\n    section.Lock()\n    result := someLockedResource.Go()\n    section.Unlock()\n\n    xray.AddMetadata(ctx1, \"ResourceResult\", result)\n  })\n}\n```\n\n**HTTP Handler**\n\n```go\nfunc main() {\n  http.Handle(\"/\", xray.Handler(xray.NewFixedSegmentNamer(\"myApp\"), http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n    w.Write([]byte(\"Hello!\"))\n  })))\n  http.ListenAndServe(\":8000\", nil)\n}\n```\n\n**HTTP Client**\n\n```go\nfunc getExample(ctx context.Context) ([]byte, error) {\n  resp, err := ctxhttp.Get(ctx, xray.Client(nil), \"https://aws.amazon.com/\")\n  if err != nil {\n    return nil, err\n  }\n  return ioutil.ReadAll(resp.Body)\n}\n```\n\n**AWS SDK Instrumentation**\n\n```go\nfunc main() {\n  // Create a segment\n  ctx, root := xray.BeginSegment(context.TODO(), \"AWSSDKV1_Dynamodb\")\n  defer root.Close(nil)\n\n  sess := session.Must(session.NewSession())\n  dynamo := dynamodb.New(sess)\n  // Instrumenting with AWS SDK v1:\n  // Wrap the client object with `xray.AWS()`\n  xray.AWS(dynamo.Client)\n  // Use the `-WithContext` version of the `ListTables` method\n  output := dynamo.ListTablesWithContext(ctx, &dynamodb.ListTablesInput{})\n  doSomething(output)\n}\n```\n*Segment creation is not necessary in an AWS Lambda function, where the segment is created automatically*\n\n**AWS SDK V2 Instrumentation**\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"log\"\n\n\t\"github.com/aws/aws-sdk-go-v2/aws\"\n\t\"github.com/aws/aws-sdk-go-v2/config\"\n\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\n\t\"github.com/aws/aws-xray-sdk-go/instrumentation/awsv2\"\n\t\"github.com/aws/aws-xray-sdk-go/xray\"\n)\n\nfunc main() {\n\t// Create a segment\n\tctx, root := xray.BeginSegment(context.TODO(), \"AWSSDKV2_Dynamodb\")\n\tdefer root.Close(nil)\n  \n\tcfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(\"us-west-2\"))\n\tif err != nil {\n\t\tlog.Fatalf(\"unable to load SDK config, %v\", err)\n\t}\n\t// Instrumenting AWS SDK v2\n\tawsv2.AWSV2Instrumentor(&cfg.APIOptions)\n\t// Using the Config value, create the DynamoDB client\n\tsvc := dynamodb.NewFromConfig(cfg)\n\t// Build the request with its input parameters\n\t_, err = svc.ListTables(ctx, &dynamodb.ListTablesInput{\n\t\tLimit: aws.Int32(5),\n\t})\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to list tables, %v\", err)\n\t}\n}\n```\n*Segment creation is not necessary in an AWS Lambda function, where the segment is created automatically*\n\n**S3**\n\n`aws-xray-sdk-go` does not currently support [`*Request.Presign()`](https://docs.aws.amazon.com/sdk-for-go/api/aws/request/#Request.Presign) operations and will panic if one is encountered.  This results in an error similar to: \n\n`panic: failed to begin subsegment named 's3': segment cannot be found.`\n\nIf you encounter this, you can set `AWS_XRAY_CONTEXT_MISSING` environment variable to `LOG_ERROR`.  This will instruct the SDK to log the error and continue processing your requests.\n\n```go\nos.Setenv(\"AWS_XRAY_CONTEXT_MISSING\", \"LOG_ERROR\")\n```\n\n**SQL**\n\nAny `database/sql` calls can be traced with X-Ray by replacing the `sql.Open` call with `xray.SQLContext`. It is recommended to use URLs instead of configuration strings if possible.\n\n```go\nfunc main() {\n  db, err := xray.SQLContext(\"postgres\", \"postgres://user:password@host:port/db\")\n  row, err := db.QueryRowContext(ctx, \"SELECT 1\") // Use as normal\n}\n```\n\n**Lambda**\n\n```\nFor Lambda support use version v1.0.0-rc.1 and higher\n```\n\nIf you are using the AWS X-Ray Go SDK inside a Lambda function, there will be a FacadeSegment inside the Lambda context.  This allows you to instrument your Lambda function using `Configure`, `Capture`, `HTTP Client`, `AWS`, `SQL` and `Custom Subsegments` usage.  `Segment` operations are not supported.\n\n```go\nfunc HandleRequest(ctx context.Context, name string) (string, error) {\n    sess := session.Must(session.NewSession())\n    dynamo := dynamodb.New(sess)\n    xray.AWS(dynamo.Client)\n    input := &dynamodb.PutItemInput{\n        Item: map[string]*dynamodb.AttributeValue{\n            \"12\": {\n                S: aws.String(\"example\"),\n            },\n        },\n        TableName: aws.String(\"xray\"),\n    }\n    _, err := dynamo.PutItemWithContext(ctx, input)\n    if err != nil {\n        return name, err\n    }\n    \n    _, err = ctxhttp.Get(ctx, xray.Client(nil), \"https://www.twitch.tv/\")\n    if err != nil {\n        return name, err\n    }\n    \n    _, subseg := xray.BeginSubsegment(ctx, \"subsegment-name\")\n    subseg.Close(nil)\n    \n    db := xray.SQLContext(\"postgres\", \"postgres://user:password@host:port/db\")\n    row, _ := db.QueryRow(ctx, \"SELECT 1\")\n    \n    return fmt.Sprintf(\"Hello %s!\", name), nil\n}\n```\n\n**gRPC**\n\nNote: `aws-xray-sdk-go` doesn't currently support streaming gRPC call.\n\nApply xray gRPC interceptors (`xray.UnaryServerInterceptor` or `xray.UnaryClientInterceptor`) to instrument gRPC unary requests/responses, and the handling code.\n\n**gRPC Client**\n\n```go\nconn, err := grpc.Dial(\n    serverAddr,\n    // use grpc.WithChainUnaryInterceptor instead to apply multiple interceptors\n    grpc.WithUnaryInterceptor(\n        xray.UnaryClientInterceptor(),\n        // or xray.UnaryClientInterceptor(xray.WithSegmentNamer(xray.NewFixedSegmentNamer(\"myApp\"))) to use a custom segment namer\n    ),\n)\n```\n\n**gRPC Server**\n\n```go\ngrpcServer := grpc.NewServer(\n    // use grpc.ChainUnaryInterceptor instead to apply multiple interceptors\n    grpc.UnaryInterceptor(\n        xray.UnaryServerInterceptor(),\n        // or xray.UnaryServerInterceptor(xray.WithSegmentNamer(xray.NewFixedSegmentNamer(\"myApp\"))) to use a custom segment namer\n    ),\n)\n```\n\n## fasthttp instrumentation \n\nSupport for incoming requests with [valyala/fasthttp](https://github.com/valyala/fasthttp):\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/aws/aws-xray-sdk-go/xray\"\n\t\"github.com/aws/aws-xray-sdk-go/xraylog\"\n\t\"github.com/fasthttp/router\"\n\t\"github.com/valyala/fasthttp\"\n)\n\nfunc index(ctx *fasthttp.RequestCtx) {\n\tctx.WriteString(\"Welcome!\")\n}\n\nfunc hello(ctx *fasthttp.RequestCtx) {\n\tfmt.Fprintf(ctx, \"Hello, %s!\\n\", ctx.UserValue(\"name\"))\n}\n\nfunc middleware(name string, h fasthttp.RequestHandler, fh xray.FastHTTPHandler) fasthttp.RequestHandler {\n\tf := func(ctx *fasthttp.RequestCtx) {\n\t\th(ctx)\n\t}\n\n\treturn fh.Handler(xray.NewFixedSegmentNamer(name), f)\n}\n\nfunc init() {\n\tif err := xray.Configure(xray.Config{\n\t\tDaemonAddr:     \"xray:2000\",\n\t\tServiceVersion: \"0.1\",\n\t}); err != nil {\n\t\tpanic(err)\n\t}\n\n\txray.SetLogger(xraylog.NewDefaultLogger(os.Stdout, xraylog.LogLevelDebug))\n}\n\nfunc main() {\n\tfh := xray.NewFastHTTPInstrumentor(nil)\n\tr := router.New()\n\tr.GET(\"/\", middleware(\"index\", index, fh))\n\tr.GET(\"/hello/{name}\", middleware(\"hello\", hello, fh))\n\n\tlog.Fatal(fasthttp.ListenAndServe(\":8080\", r.Handler))\n}\n```\n\n## Oversampling Mitigation\nOversampling mitigation allows you to ignore a parent segment/subsegment's sampled flag and instead sets the subsegment's sampled flag to false.\nThis ensures that downstream calls are not sampled and this subsegment is not emitted.\n\n```go\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/aws/aws-lambda-go/events\"\n    \"github.com/aws/aws-lambda-go/lambda\"\n    \"github.com/aws/aws-sdk-go/aws/session\"\n    \"github.com/aws/aws-sdk-go/service/sqs\"\n    \"github.com/aws/aws-xray-sdk-go/xray\"\n)\n\nfunc HandleRequest(ctx context.Context, event events.SQSEvent) (string, error) {\n    _, subseg := xray.BeginSubsegmentWithoutSampling(ctx, \"Processing Event\")\n\n    sess := session.Must(session.NewSessionWithOptions(session.Options{\n        SharedConfigState: session.SharedConfigEnable,\n    }))\n\n    svc := sqs.New(sess)\n\n    result, _ := svc.ListQueues(nil)\n\n    for _, url := range result.QueueUrls {\n        fmt.Printf(\"%s\\n\", *url)\n    }\n\n    subseg.Close(nil)\n\n    return \"Success\", nil\n}\n\nfunc main() {\n    lambda.Start(HandleRequest)\n}\n```\n\nThe code below demonstrates overriding the sampled flag based on the SQS messages sent to Lambda.\n\n```go\nimport (\n    \"context\"\n    \"fmt\"\n    \"strconv\"\n    \"github.com/aws/aws-lambda-go/events\"\n    \"github.com/aws/aws-lambda-go/lambda\"\n    xrayLambda \"github.com/aws/aws-xray-sdk-go/lambda\"\n    \"github.com/aws/aws-xray-sdk-go/xray\"\n)\n\nfunc HandleRequest(ctx context.Context, event events.SQSEvent) (string, error) {\n\n    var i = 1\n\n    for _, message := range event.Records {\n        var subseg *xray.Segment\n\n        if xrayLambda.IsSampled(message) {\n            _, subseg = xray.BeginSubsegment(ctx, \"Processing Message - \" + strconv.Itoa(i))\n        } else {\n            _, subseg = xray.BeginSubsegmentWithoutSampling(ctx, \"Processing Message - \" + strconv.Itoa(i))\n        }\n\n        i++;\n\n        // Do your procesing work here\n        fmt.Println(\"Doing processing work\")\n\n        // End your subsegment\n        subseg.Close(nil)\n    }\n\n    return \"Success\", nil\n}\n\nfunc main() {\n    lambda.Start(HandleRequest)\n}\n```\n\n## License\n\nThe AWS X-Ray SDK for Go is licensed under the Apache 2.0 License. See LICENSE and NOTICE.txt for more information.\n", "release_dates": ["2023-11-13T18:55:55Z", "2023-10-02T17:57:14Z", "2023-02-28T17:43:37Z", "2022-11-15T01:33:49Z", "2022-09-15T20:53:00Z", "2022-04-11T19:07:25Z", "2021-07-07T17:48:33Z", "2021-06-10T18:27:41Z", "2021-05-03T18:48:02Z", "2021-02-02T18:52:22Z", "2021-01-05T19:35:26Z", "2020-06-08T21:13:35Z", "2020-04-28T22:08:18Z", "2020-04-16T17:22:17Z", "2020-03-11T19:15:04Z", "2019-09-03T21:44:55Z", "2019-07-18T20:46:18Z", "2019-06-11T20:31:43Z", "2019-03-15T21:40:11Z", "2019-03-15T21:07:24Z", "2019-02-19T21:33:34Z", "2018-10-04T21:21:31Z", "2018-09-28T01:25:52Z", "2018-09-25T22:15:39Z"]}, {"name": "aws-xray-sdk-java", "description": "The official AWS X-Ray Recorder SDK for Java.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![Build Status](https://github.com/aws/aws-xray-sdk-java/actions/workflows/master-build.yml/badge.svg)\n\n### :mega: OpenTelemetry Java with AWS X-Ray\n\nAWS X-Ray supports using OpenTelemetry Java and the AWS Distro for OpenTelemetry (ADOT) Collector to instrument your application and send trace data to X-Ray. The OpenTelemetry SDKs are an industry-wide standard for tracing instrumentation. They provide more instrumentations and have a larger community for support, but may not have complete feature parity with the X-Ray SDKs. See [choosing between the ADOT and X-Ray SDKs](https://docs.aws.amazon.com/xray/latest/devguide/xray-instrumenting-your-app.html#xray-instrumenting-choosing) for more help with choosing between the two.\n\nIf you want additional features when tracing your Java applications, please [open an issue on the OpenTelemetry Java Instrumentation repository](https://github.com/open-telemetry/opentelemetry-java-instrumentation/issues/new?labels=enhancement&template=feature_request.md&title=X-Ray%20Compatible%20Feature%20Request).\n\n# AWS X-Ray SDK for Java\n\n![Screenshot of the AWS X-Ray console](/images/example_servicemap.png?raw=true)\n\n## Installing\n\nThe AWS X-Ray SDK for Java is compatible with Java 8 and 11.\n\nAdd the AWS X-Ray SDK dependencies to your pom.xml:\n\n```\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-core</artifactId>\n  <version>2.13.0</version>\n</dependency>\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-apache-http</artifactId>\n  <version>2.13.0</version>\n</dependency>\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-aws-sdk</artifactId>\n  <version>2.13.0</version>\n</dependency>\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-aws-sdk-v2</artifactId>\n  <version>2.13.0</version>\n</dependency>\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-aws-sdk-instrumentor</artifactId>\n  <version>2.13.0</version>\n</dependency>\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-aws-sdk-v2-instrumentor</artifactId>\n  <version>2.13.0</version>\n</dependency>\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-sql</artifactId>\n  <version>2.13.0</version>\n</dependency>\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-sql-mysql</artifactId>\n  <version>2.13.0</version>\n</dependency>\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-sql-postgres</artifactId>\n  <version>2.13.0</version>\n</dependency>\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-spring</artifactId>\n  <version>2.13.0</version>\n</dependency>\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-log4j</artifactId>\n  <version>2.13.0</version>\n</dependency>\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-slf4j</artifactId>\n  <version>2.13.0</version>\n</dependency>\n<dependency>\n  <groupId>com.amazonaws</groupId>\n  <artifactId>aws-xray-recorder-sdk-metrics</artifactId>\n  <version>2.13.0</version>\n</dependency>\n```\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues for tracking bugs and feature requests.\n\n* Ask a question in the [AWS X-Ray Forum](https://forums.aws.amazon.com/forum.jspa?forumID=241&start=0).\n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n* If you think you may have found a bug, please open an [issue](https://github.com/aws/aws-xray-sdk-java/issues/new).\n\n## Opening Issues\n\nIf you encounter a bug with the AWS X-Ray SDK for Java we would like to hear about it. Search the [existing issues](https://github.com/aws/aws-xray-sdk-java/issues) and see if others are also experiencing the issue before opening a new issue. Please include the version of AWS X-Ray SDK for Java, AWS SDK for Java, JDK, and OS you\u2019re using. Please also include repro case when appropriate.\n\nThe GitHub issues are intended for bug reports and feature requests. For help and questions with using AWS X-Ray SDK for Java please make use of the resources listed in the [Getting Help](https://github.com/aws/aws-xray-sdk-java#getting-help) section. Keeping the list of open issues lean will help us respond in a timely manner.\n\n## Documentation\n\nThe [developer guide](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java.html) provides in-depth guidance on using the AWS X-Ray service and the AWS X-Ray SDK for Java.\n\nSee [awslabs/eb-java-scorekeep](https://github.com/awslabs/eb-java-scorekeep/tree/xray) for a sample application that uses this SDK.\n\n## Quick Start\n\n### Intercept incoming HTTP requests\n\nFor many applications, work on a task begins with an incoming HTTP request.\n\nThere are a few different options for intercepting this incoming HTTP request.\n\n##### Applications using `javax.servlet` may utilize the `AWSXRayServletFilter`\nAdd the filter in Tomcat's `web.xml`:\n```\n  <filter>\n     <filter-name>AWSXRayServletFilter</filter-name>\n     <filter-class>com.amazonaws.xray.javax.servlet.AWSXRayServletFilter</filter-class>\n     <init-param>\n        <param-name>fixedName</param-name>\n        <param-value>defaultSegmentName</param-value>\n     </init-param>\n  </filter>\n  <filter-mapping>\n     <filter-name>AWSXRayServletFilter</filter-name>\n     <url-pattern>*</url-pattern>\n  </filter-mapping>\n```\nAlternatively, Spring users may add the `AWSXRayServletFilter` to their `WebConfig`:\n```\n@Configuration\npublic class WebConfig {\n\n    ...\n\n    @Bean\n    public Filter TracingFilter() {\n        return new AWSXRayServletFilter(new FixedSegmentNamingStrategy(\"defaultSegmentName\"));\n    }\n}\n```\nThe servlet filter will fail to serve incoming requests if a `SegmentNamingStrategy` is not supplied, either through web.xml init-params or through the constructor.\n\n##### Applications using `jakarta.servlet` or Spring 6\nInclude from the Jakarta namespace. Example: `com.amazonaws.xray.javax.servlet.AWSXRayServletFilter`\n\n##### Applications not using `javax.servlet` may include custom interceptors to begin and end trace segments\n\nDirectly call `beginSegment` and `endSegment` as necessary. *Note:* this method requires additional work to ensure that the `X-Amzn-Trace-Id` header is properly propogated and sufficient information about the request and response is captured with the segment.\n\n### Intercept AWS requests\n\nApplications may make calls to Amazon Web Services. Included in the X-Ray SDK is an extension of the AWS SDK's `RequestHandler2`.\n\nTo instrument an example instance of `AmazonWebServiceClient`:\n\n```\nAmazonDynamoDBClient tracedDynamoClient = \n    new AmazonDynamoDBClient().standard().withRequestHandlers(new TracingHandler()).withRegion(Regions.US_EAST_1).build();\n```\n\n### Intercept outgoing HTTP requests\n\nApplications may make downstream HTTP calls to communicate with other applications. If these downstream applications are also traced, trace context information will need to be passed so that the trace segments may be properly grouped into a single trace. \n\nThe following options are available for ensuring these downstream calls include trace context information as well as locally generate the appropriate trace subsegments.\n\n##### Applications using Apache's `HttpClient` library may utilize proxy classes included in `com.amazonaws.xray.proxies.apache.http`\n\nChange the import line for your `DefaultHttpClient` or `HttpClientBuilder` to the appropriate proxy import. Continue to use the class as normal; method signatures do not change.\n\n```\n// Change the import\nimport com.amazonaws.xray.proxies.apache.http.DefaultHttpClient;\n...\n// Keep the invocation\nHttpClient httpClient = new DefaultHttpClient();\nhttpClient.execute(request);\n```\n\n### Intercept JDBC-Based SQL Queries\n\nIn addition to our Postgres and MySQL patchers documented in the [official docs](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-sqlclients.html), this SDK also includes the `aws-xray-recorder-sdk-sql` library. This library can instrument any JDBC data source, connection, or statement so that its queries are recorded by AWS X-Ray.\n\n```java\nimport com.amazonaws.xray.sql.TracingConnection;\nimport com.amazonaws.xray.sql.TracingDataSource;\nimport com.amazonaws.xray.sql.TracingStatement;\nimport java.sql.*;\n\n// Choose the one that you'd like to trace\nString sql = \"SELECT * FROM MYTABLE\";\nDataSource dataSource = TracingDataSource.decorate(dataSource);\nConnection connection = TracingConnection.decorate(connection);\nStatement statement = TracingStatement.decorateStatement(statement);\nPreparedStatement preparedStatement = TracingStatement.decoratePreparedStatement(preparedStatement, sql);\nCallableStatement callableStatement = TracingStatement.decorateCallableStatement(callableStatement, sql);\n```\nFor security reasons, the SQL query is not recorded by default. However, you can opt-in to SQL query recording by setting the `AWS_XRAY_COLLECT_SQL_QUERIES` environment variable or the `com.amazonaws.xray.collectSqlQueries` system property to `true`.\n\n### Intercept custom methods\n\nIt may be useful to further decorate portions of an application for which performance is critical. Generating subsegments around these hot spots will help in understanding their impact on application performance. There are a few different styles available for tracing custom methods.\n\n##### Using traced closures\n\n```\nimport com.amazonaws.xray.AWSXRayRecorder;\nimport com.amazonaws.xray.AWSXRayRecorderBuilder;\n...\nAWSXRayRecorder xrayRecorder = AWSXRayRecorderBuilder.defaultRecorder();\n...\nxrayRecorder.createSubsegment(\"getMovies\" (subsegment) -> {\n    doSomething();\n});\n```\n##### Using explicit calls to begin and end subsegments.\n```\nSubsegment subsegment = xrayRecorder.beginSubsegment(\"providedMovie\");\ntry {\n    doSomething();\n    throw new RuntimeException(\"user error\");\n} catch (RuntimeException e) {\n    subsegment.addException(e);\n    subsegment.setError(true);\n} finally {\n    xrayRecorder.endSubsegment();\n}\n```\nNote that in the closure-based example above, exceptions are intercepted automatically.\n\n### Oversampling Mitigation\nOversampling mitigation allows you to ignore a parent segment/subsegment's sampled flag and instead sets the subsegment's sampled flag to false.\nThis ensures that downstream calls are not sampled and this subsegment is not emitted.\n\n```Java\npublic class Handler implements RequestHandler<SQSEvent, String> {\n    public Handler() {\n    }\n\n    @Override\n    public String handleRequest(SQSEvent event, Context context) {\n        AWSXRay.beginSubsegmentWithoutSampling(\"Processing Event\");\n\n        AmazonSQS sqs = AmazonSQSClientBuilder.defaultClient();\n\n        ListQueuesResult lq_result = sqs.listQueues();\n      \n        System.out.println(\"Your SQS Queue URLs:\");\n\n        for (String url : lq_result.getQueueUrls()) {\n            System.out.println(url);\n        }\n\n        AWSXRay.endSubsegment();\n\n        return \"Success\";\n    }\n}\n```\n\nThe code below demonstrates overriding the sampled flag based on the SQS message.\n\n```java\npublic class Handler implements RequestHandler<SQSEvent, String> {\n    public Handler() {\n    }\n\n    @Override\n    public String handleRequest(SQSEvent event, Context context) {\n\n        int i = 1;\n\n        for (SQSMessage message: event.getRecords()) {\n\n            // Check if the message is sampled\n            if (SQSMessageHelper.isSampled(message)) {\n                AWSXRay.beginSubsegment(\"Processing Message - \" + i);\n            } else {\n                AWSXRay.beginSubsegmentWithoutSampling(\"Processing Message - \" + i);\n            }\n\n            i++;\n\n            // Do your procesing work here\n            System.out.println(\"Doing processing work\");\n\n            // End your subsegment\n            AWSXRay.endSubsegment();\n        }\n        \n        return \"Success\";\n    }\n}\n```\n\n## Integration with ServiceLens\n\nAs of version 2.4.0, the X-Ray SDK for Java is integrated with [CloudWatch ServiceLens](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ServiceLens.html). This allows you to use a wide range of new observability features which connect your traces, logs, and metrics in one place.\n\n### Trace ID Injection into Logs\n\nYou can automatically inject your current Trace ID into logging statements if you use the Log4J or SLF4J logging frontends. To learn more and enable this feature on your instrumented project, see the [developer guide](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-configuration.html#xray-sdk-java-configuration-logging). \n\n### Vended Segment-Level Metrics\n\nThe X-Ray SDK can now automatically vend metrics that aggregate information about the segments your application sends as a custom CloudWatch metric. To learn more and enable this feature on your instrumented project, see the [developer guide](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-monitoring.html#xray-sdk-java-monitoring-enable).\n\n### Log Group Correlation\n\nIf you are working in an environment with a supported plugin enabled and you use CloudWatch logs, the X-Ray SDK will automatically record the log group(s) you are using in that environment in the segment document. To learn more and see which plugins are supported, see the [developer guide](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-java-configuration.html#xray-sdk-java-configuration-plugins).\n\nAlternatively, you can manually configure a log group to be associated with your application's segment data by setting the `AWS_LOG_GROUP` environment variable to the name of your application's log group. Note this setting makes the assumption the log group is in the same AWS account and region as the application.\n\n## Snapshots\n\nSnapshots are published for each commit to AWS Sonatype snapshots repository at https://aws.oss.sonatype.org/content/repositories/snapshots\n\n## Building From Source\n\nOnce you check out the code from GitHub, you can build it using Maven. To disable the GPG-signing in the build, use:\n\n```\n./gradlew build\n```\n\n## License\n\nThe AWS X-Ray SDK for Java is licensed under the Apache 2.0 License. See LICENSE and NOTICE.txt for more information.\n", "release_dates": ["2024-02-15T21:17:38Z", "2023-12-07T21:13:02Z", "2023-04-13T19:20:47Z", "2022-11-10T17:53:02Z", "2022-10-10T20:20:37Z", "2022-06-06T16:30:37Z", "2022-04-07T17:13:22Z", "2022-02-02T22:35:19Z", "2021-10-19T20:45:58Z", "2021-06-02T17:27:51Z", "2021-04-08T17:58:46Z", "2020-11-25T16:46:21Z", "2020-08-28T21:46:07Z", "2020-08-27T22:22:59Z", "2020-06-21T07:06:30Z", "2020-06-15T18:30:18Z", "2020-05-11T19:37:35Z", "2019-11-22T22:27:50Z", "2019-07-19T17:09:23Z", "2019-05-08T00:17:02Z", "2018-11-21T00:37:07Z", "2018-09-06T21:50:01Z", "2018-01-13T00:22:26Z", "2018-01-04T00:48:51Z"]}, {"name": "aws-xray-sdk-node", "description": "The official AWS X-Ray SDK for Node.js.", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![Continuous Build](https://github.com/aws/aws-xray-sdk-node/workflows/Node.js%20SDK%20Continuous%20Build/badge.svg)\n\n### :mega: OpenTelemetry JavaScript with AWS X-Ray\n\nAWS X-Ray supports using OpenTelemetry JavaScript and the AWS Distro for OpenTelemetry (ADOT) Collector to instrument your application and send trace data to X-Ray. The OpenTelemetry SDKs are an industry-wide standard for tracing instrumentation. They provide more instrumentations and have a larger community for support, but may not have complete feature parity with the X-Ray SDKs. See [choosing between the ADOT and X-Ray SDKs](https://docs.aws.amazon.com/xray/latest/devguide/xray-instrumenting-your-app.html#xray-instrumenting-choosing) for more help with choosing between the two.\n\nIf you want additional features when tracing your Node.js applications, please [open an issue on the OpenTelemetry JS Instrumentation repository](https://github.com/open-telemetry/opentelemetry-js-contrib/issues/new?labels=enhancement&template=feature_request.md&title=X-Ray%20Compatible%20Feature%20Request).\n\n# AWS X-Ray SDK for Node.js\n\n![Screenshot of the AWS X-Ray console](/images/example_servicemap.png?raw=true)\n\n## Installing\n\nThe AWS X-Ray SDK for Node.js is compatible with Node.js version 14.x and later.\nThere may be issues when running on the latest odd-numbered release of Node.js.\n\nThe latest stable version of the SDK is available from NPM. For local development, install the SDK in your project directory with npm.\n\n```\nnpm install aws-xray-sdk\n```\n\nUse the --save option to save the SDK as a dependency in your application's `package.json`.\n\n```\nnpm install aws-xray-sdk --save\n```\n\n## Documentation\n\nThis repository hosts all the packages we publish, which each have their own README. The [Core package README](https://github.com/aws/aws-xray-sdk-node/tree/master/packages/core) covers all basic use cases of the main X-Ray SDK, including its use in Lambda.\nThe [developer guide](https://docs.aws.amazon.com/xray/latest/devguide) provides in-depth\nguidance about using the AWS X-Ray service and SDKs.\nThe [API Reference](http://docs.aws.amazon.com/xray-sdk-for-nodejs/latest/reference/)\nprovides guidance for using this SDK and module-level documentation.\n\n[CHANGELOG.md](https://github.com/aws/aws-xray-sdk-node/blob/master/packages/full_sdk/CHANGELOG.md)\n\n## Sample App\n\nTo get started with a functional web application instrumented with the X-Ray SDK, check out our [sample app](https://github.com/aws-samples/aws-xray-sdk-node-sample).\n\n## Getting Help\n\nUse the following community resources for getting help with the SDK. We use the GitHub\nissues for tracking bugs and feature requests.\n\n* Ask a question in the [AWS X-Ray Forum](https://forums.aws.amazon.com/forum.jspa?forumID=241&start=0).\n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n* If you think you may have found a bug, open an [issue](https://github.com/aws/aws-xray-sdk-node/issues/new).\n\n## Opening Issues\n\nIf you encounter a bug with the AWS X-Ray SDK for Node.js, we want to hear about\nit. Before opening a new issue, search the [existing issues](https://github.com/aws/aws-xray-sdk-node/issues)\nto see if others are also experiencing the issue. Include the version of the AWS X-Ray\nSDK for Node.js, Node.js runtime, and other dependencies if applicable. In addition, \ninclude the repro case when appropriate.\n\nThe GitHub issues are intended for bug reports and feature requests. For help and\nquestions about using the AWS X-Ray SDK for Node.js, use the resources listed\nin the [Getting Help](https://github.com/aws/aws-xray-sdk-node#getting-help) section. Keeping the list of open issues lean helps us respond in a timely manner.\n\n## Contributing\n\nWe support and accept PRs from the community.\n\nThis monorepo hosts the following npm packages for the SDK:\n- [aws-xray-sdk](https://www.npmjs.com/package/aws-xray-sdk)\n- [aws-xray-sdk-core](https://www.npmjs.com/package/aws-xray-sdk-core)\n- [aws-xray-sdk-express](https://www.npmjs.com/package/aws-xray-sdk-express)\n- [aws-xray-sdk-mysql](https://www.npmjs.com/package/aws-xray-sdk-mysql)\n- [aws-xray-sdk-postgres](https://www.npmjs.com/package/aws-xray-sdk-postgres)\n- [aws-xray-sdk-restify](https://www.npmjs.com/package/aws-xray-sdk-restify)\n- [Community contributed packages](https://github.com/aws/aws-xray-sdk-node/tree/master/sdk_contrib)\n\n## Community contributions for new Middleware\nIf you'd like to add support for a new web framework by writing middleware for X-Ray, \nplease do so by creating a new package within the `sdk_contrib` \n[directory](https://github.com/aws/aws-xray-sdk-node/tree/master/sdk_contrib).\nWe are not accepting pull requests for first-party packages at this time, \nbut will be more than happy to host them as community contributions. This means that AWS will:\n\n- Host them in this repository\n- Publish them to NPM\n- Consider them the officially recommended way of using X-Ray with that framework\n- Review & merge pull requests made against them by the community\n- Allow issues related to them on this repo for visibility to the community\n\nAWS will not:\n\n- Provide first party support through AWS Forums, AWS customer support, etc for things like questions & debugging help\n- Actively develop on them (e.g. if we add a feature to the Express middleware, it will not necessarily be added to middleware in `sdk_contrib`)\n\n## Testing from Source\n\nThis repo uses [Lerna](https://lerna.js.org) (use v6 or lower) to manage multiple packages. To install Lerna as a CLI:\n```\nnpm install -g lerna\n```\nTo install devDependencies and peerDependencies for all packages:\n```\nlerna bootstrap --hoist\n```\nThis repo has a combination of TypeScript and JavaScript source files. To transpile the TypeScript files for testing, run:\n```\nlerna run compile\n```\nTo run tests for all packages:\n```\nlerna run test\n```\nor go to each package and run `npm test` as usual.\n\n## License\n\nThe AWS X-Ray SDK for Node.js is licensed under the Apache 2.0 License. See LICENSE and NOTICE.txt for more information.\n", "release_dates": ["2024-02-22T19:41:43Z", "2023-10-26T18:17:16Z", "2023-09-25T15:57:29Z", "2023-07-19T17:08:00Z", "2023-04-04T18:04:42Z", "2023-01-06T00:02:53Z", "2022-11-10T19:32:37Z", "2022-09-27T21:06:09Z", "2022-08-22T17:20:41Z", "2022-05-31T18:25:06Z", "2022-04-14T19:21:15Z", "2021-11-11T19:50:02Z", "2021-05-13T18:20:36Z", "2021-05-13T00:18:10Z", "2021-04-12T22:23:53Z", "2021-04-12T16:39:03Z", "2020-09-11T00:00:10Z"]}, {"name": "aws-xray-sdk-python", "description": "AWS X-Ray SDK for the Python programming language", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![Build Status](https://github.com/aws/aws-xray-sdk-python/actions/workflows/IntegrationTesting.yaml/badge.svg)\n[![codecov](https://codecov.io/gh/aws/aws-xray-sdk-python/branch/master/graph/badge.svg)](https://codecov.io/gh/aws/aws-xray-sdk-python)\n\n### :mega: OpenTelemetry Python with AWS X-Ray\n\nAWS X-Ray supports using OpenTelemetry Python and the AWS Distro for OpenTelemetry (ADOT) Collector to instrument your application and send trace data to X-Ray. The OpenTelemetry SDKs are an industry-wide standard for tracing instrumentation. They provide more instrumentations and have a larger community for support, but may not have complete feature parity with the X-Ray SDKs. See [choosing between the ADOT and X-Ray SDKs](https://docs.aws.amazon.com/xray/latest/devguide/xray-instrumenting-your-app.html#xray-instrumenting-choosing) for more help with choosing between the two.\n\nIf you want additional features when tracing your Python applications, please [open an issue on the OpenTelemetry Python Instrumentation repository](https://github.com/open-telemetry/opentelemetry-python-contrib/issues/new?labels=feature-request&template=feature_request.md&title=X-Ray%20Compatible%20Feature%20Request).\n\n### :mega: Python Versions End-of-Support Notice\n\nAWS X-Ray SDK for Python versions `>2.11.0` has dropped support for Python 2.7, 3.4, 3.5, and 3.6.\n\n# AWS X-Ray SDK for Python\n\n![Screenshot of the AWS X-Ray console](/images/example_servicemap.png?raw=true)\n\n## Installing\n\nThe AWS X-Ray SDK for Python is compatible with Python 3.7, 3.8, 3.9, 3.10, and 3.11.\n\nInstall the SDK using the following command (the SDK's non-testing dependencies will be installed).\n\n```\npip install aws-xray-sdk\n```\n\nTo install the SDK's testing dependencies, use the following command.\n\n```\npip install tox\n```\n\n## Getting Help\n\nUse the following community resources for getting help with the SDK. We use the GitHub\nissues for tracking bugs and feature requests.\n\n* Ask a question in the [AWS X-Ray Forum](https://forums.aws.amazon.com/forum.jspa?forumID=241&start=0).\n* Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n* If you think you may have found a bug, open an [issue](https://github.com/aws/aws-xray-sdk-python/issues/new).\n\n## Opening Issues\n\nIf you encounter a bug with the AWS X-Ray SDK for Python, we want to hear about\nit. Before opening a new issue, search the [existing issues](https://github.com/aws/aws-xray-sdk-python/issues)\nto see if others are also experiencing the issue. Include the version of the AWS X-Ray\nSDK for Python, Python language, and botocore/boto3 if applicable. In addition, \ninclude the repro case when appropriate.\n\nThe GitHub issues are intended for bug reports and feature requests. For help and\nquestions about using the AWS SDK for Python, use the resources listed\nin the [Getting Help](https://github.com/aws/aws-xray-sdk-python#getting-help) section. Keeping the list of open issues lean helps us respond in a timely manner.\n\n## Documentation\n\nThe [developer guide](https://docs.aws.amazon.com/xray/latest/devguide) provides in-depth\nguidance about using the AWS X-Ray service.\nThe [API Reference](http://docs.aws.amazon.com/xray-sdk-for-python/latest/reference/)\nprovides guidance for using the SDK and module-level documentation.\n\n## Quick Start\n\n### Configuration\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\n\nxray_recorder.configure(\n    sampling=False,\n    context_missing='LOG_ERROR',\n    plugins=('EC2Plugin', 'ECSPlugin', 'ElasticBeanstalkPlugin'),\n    daemon_address='127.0.0.1:3000',\n    dynamic_naming='*mysite.com*'\n)\n```\n\n### Start a custom segment/subsegment\n\nUsing context managers for implicit exceptions recording:\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\n\nwith xray_recorder.in_segment('segment_name') as segment:\n    # Add metadata or annotation here if necessary\n    segment.put_metadata('key', dict, 'namespace')\n    with xray_recorder.in_subsegment('subsegment_name') as subsegment:\n        subsegment.put_annotation('key', 'value')\n        # Do something here\n    with xray_recorder.in_subsegment('subsegment2') as subsegment:\n        subsegment.put_annotation('key2', 'value2')\n        # Do something else \n```\n\nasync versions of context managers:\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\n\nasync with xray_recorder.in_segment_async('segment_name') as segment:\n    # Add metadata or annotation here if necessary\n    segment.put_metadata('key', dict, 'namespace')\n    async with xray_recorder.in_subsegment_async('subsegment_name') as subsegment:\n        subsegment.put_annotation('key', 'value')\n        # Do something here\n    async with xray_recorder.in_subsegment_async('subsegment2') as subsegment:\n        subsegment.put_annotation('key2', 'value2')\n        # Do something else \n```\n\nDefault begin/end functions:\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\n\n# Start a segment\nsegment = xray_recorder.begin_segment('segment_name')\n# Start a subsegment\nsubsegment = xray_recorder.begin_subsegment('subsegment_name')\n\n# Add metadata or annotation here if necessary\nsegment.put_metadata('key', dict, 'namespace')\nsubsegment.put_annotation('key', 'value')\nxray_recorder.end_subsegment()\n\n# Close the segment\nxray_recorder.end_segment()\n```\n\n### Oversampling Mitigation\nTo modify the sampling decision at the subsegment level, subsegments that inherit the decision of their direct parent (segment or subsegment) can be created using `xray_recorder.begin_subsegment()` and unsampled subsegments can be created using\n`xray_recorder.begin_subsegment_without_sampling()`.\n\nThe code snippet below demonstrates creating a sampled or unsampled subsegment based on the sampling decision of each SQS message processed by Lambda.\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\nfrom aws_xray_sdk.core.models.subsegment import Subsegment\nfrom aws_xray_sdk.core.utils.sqs_message_helper import SqsMessageHelper\n\ndef lambda_handler(event, context):\n\n    for message in event['Records']:\n        if SqsMessageHelper.isSampled(message):\n            subsegment = xray_recorder.begin_subsegment('sampled_subsegment')\n            print('sampled - processing SQS message')\n\n        else:\n            subsegment = xray_recorder.begin_subsegment_without_sampling('unsampled_subsegment')\n            print('unsampled - processing SQS message')\n    \n    xray_recorder.end_subsegment()   \n```\n\nThe code snippet below demonstrates wrapping a downstream AWS SDK request with an unsampled subsegment.\n```python\nfrom aws_xray_sdk.core import xray_recorder, patch_all\nimport boto3\n\npatch_all()\n\ndef lambda_handler(event, context):\n    subsegment = xray_recorder.begin_subsegment_without_sampling('unsampled_subsegment')\n    client = boto3.client('sqs')\n    print(client.list_queues())\n    \n    xray_recorder.end_subsegment()\n```\n\n### Capture\n\nAs a decorator:\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\n\n@xray_recorder.capture('subsegment_name')\ndef myfunc():\n    # Do something here\n\nmyfunc()\n```\n\nor as a context manager:\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\n\nwith xray_recorder.capture('subsegment_name') as subsegment:\n    # Do something here\n    subsegment.put_annotation('mykey', val)\n    # Do something more\n```\n\nAsync capture as decorator:\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\n\n@xray_recorder.capture_async('subsegment_name')\nasync def myfunc():\n    # Do something here\n\nasync def main():\n    await myfunc()\n```\n\nor as context manager:\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\n\nasync with xray_recorder.capture_async('subsegment_name') as subsegment:\n    # Do something here\n    subsegment.put_annotation('mykey', val)\n    # Do something more\n```\n\n### Adding annotations/metadata using recorder\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\n\n# Start a segment if no segment exist\nsegment1 = xray_recorder.begin_segment('segment_name')\n\n# This will add the key value pair to segment1 as it is active\nxray_recorder.put_annotation('key', 'value')\n\n# Start a subsegment so it becomes the active trace entity\nsubsegment1 = xray_recorder.begin_subsegment('subsegment_name')\n\n# This will add the key value pair to subsegment1 as it is active\nxray_recorder.put_metadata('key', 'value')\n\nif xray_recorder.is_sampled():\n    # some expensitve annotations/metadata generation code here\n    val = compute_annotation_val()\n    metadata = compute_metadata_body()\n    xray_recorder.put_annotation('mykey', val)\n    xray_recorder.put_metadata('mykey', metadata)\n```\n\n### Generate NoOp Trace and Entity Id\nX-Ray Python SDK will by default generate no-op trace and entity id for unsampled requests and secure random trace and entity id for sampled requests. If customer wants to enable generating secure random trace and entity id for all the (sampled/unsampled) requests (this is applicable for trace id injection into logs use case) then they should set the `AWS_XRAY_NOOP_ID` environment variable as False.\n\n### Disabling X-Ray\nOften times, it may be useful to be able to disable X-Ray for specific use cases, whether to stop X-Ray from sending traces at any moment, or to test code functionality that originally depended on X-Ray instrumented packages to begin segments prior to the code call. For example, if your application relied on an XRayMiddleware to instrument incoming web requests, and you have a method which begins subsegments based on the segment generated by that middleware, it would be useful to be able to disable X-Ray for your unit tests so that `SegmentNotFound` exceptions are not thrown when you need to test your method.\n\nThere are two ways to disable X-Ray, one is through environment variables, and the other is through the SDKConfig module.\n\n**Disabling through the environment variable:**\n\nPrior to running your application, make sure to have the environment variable `AWS_XRAY_SDK_ENABLED` set to `false`. \n\n**Disabling through the SDKConfig module:**\n```\nfrom aws_xray_sdk import global_sdk_config\n\nglobal_sdk_config.set_sdk_enabled(False)\n```\n\n**Important Notes:**\n* Environment Variables always take precedence over the SDKConfig module when disabling/enabling. If your environment variable is set to `false` while your code calls `global_sdk_config.set_sdk_enabled(True)`, X-Ray will still be disabled.\n\n* If you need to re-enable X-Ray again during runtime and acknowledge disabling/enabling through the SDKConfig module, you may run the following in your application:\n```\nimport os\nfrom aws_xray_sdk import global_sdk_config\n\ndel os.environ['AWS_XRAY_SDK_ENABLED']\nglobal_sdk_config.set_sdk_enabled(True)\n```\n\n### Trace AWS Lambda functions\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\n\ndef lambda_handler(event, context):\n    # ... some code\n\n    subsegment = xray_recorder.begin_subsegment('subsegment_name')\n    # Code to record\n    # Add metadata or annotation here, if necessary\n    subsegment.put_metadata('key', dict, 'namespace')\n    subsegment.put_annotation('key', 'value')\n\n    xray_recorder.end_subsegment()\n\n    # ... some other code\n```\n\n### Trace ThreadPoolExecutor\n\n```python\nimport concurrent.futures\n\nimport requests\n\nfrom aws_xray_sdk.core import xray_recorder\nfrom aws_xray_sdk.core import patch\n\npatch(('requests',))\n\nURLS = ['http://www.amazon.com/',\n        'http://aws.amazon.com/',\n        'http://example.com/',\n        'http://www.bilibili.com/',\n        'http://invalid-domain.com/']\n\ndef load_url(url, trace_entity):\n    # Set the parent X-Ray entity for the worker thread.\n    xray_recorder.set_trace_entity(trace_entity)\n    # Subsegment captured from the following HTTP GET will be\n    # a child of parent entity passed from the main thread.\n    resp = requests.get(url)\n    # prevent thread pollution\n    xray_recorder.clear_trace_entities()\n    return resp\n\n# Get the current active segment or subsegment from the main thread.\ncurrent_entity = xray_recorder.get_trace_entity()\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Pass the active entity from main thread to worker threads.\n    future_to_url = {executor.submit(load_url, url, current_entity): url for url in URLS}\n    for future in concurrent.futures.as_completed(future_to_url):\n        url = future_to_url[future]\n        try:\n            data = future.result()\n        except Exception:\n            pass\n```\n\n### Trace SQL queries\nBy default, if no other value is provided to `.configure()`, SQL trace streaming is enabled\nfor all the supported DB engines. Those currently are:\n- Any engine attached to the Django ORM.\n- Any engine attached to SQLAlchemy.\n\nThe behaviour can be toggled by sending the appropriate `stream_sql` value, for example:\n```python\nfrom aws_xray_sdk.core import xray_recorder\n\nxray_recorder.configure(service='fallback_name', stream_sql=True)\n```\n\n### Patch third-party libraries\n\n```python\nfrom aws_xray_sdk.core import patch\n\nlibs_to_patch = ('boto3', 'mysql', 'requests')\npatch(libs_to_patch)\n```\n\n#### Automatic module patching\n\nFull modules in the local codebase can be recursively patched by providing the module references\nto the patch function.\n```python\nfrom aws_xray_sdk.core import patch\n\nlibs_to_patch = ('boto3', 'requests', 'local.module.ref', 'other_module')\npatch(libs_to_patch)\n```\nAn `xray_recorder.capture()` decorator will be applied to all functions and class methods in the\ngiven module and all the modules inside them recursively. Some files/modules can be excluded by\nproviding to the `patch` function a regex that matches them.\n```python\nfrom aws_xray_sdk.core import patch\n\nlibs_to_patch = ('boto3', 'requests', 'local.module.ref', 'other_module')\nignore = ('local.module.ref.some_file', 'other_module.some_module\\.*')\npatch(libs_to_patch, ignore_module_patterns=ignore)\n```\n\n### Django\n#### Add Django middleware\n\nIn django settings.py, use the following.\n\n```python\nINSTALLED_APPS = [\n    # ... other apps\n    'aws_xray_sdk.ext.django',\n]\n\nMIDDLEWARE = [\n    'aws_xray_sdk.ext.django.middleware.XRayMiddleware',\n    # ... other middlewares\n]\n```\n\nYou can configure the X-Ray recorder in a Django app under the \u2018XRAY_RECORDER\u2019 namespace. For a minimal configuration, the 'AWS_XRAY_TRACING_NAME' is required unless it is specified in an environment variable.\n```\nXRAY_RECORDER = {\n    'AWS_XRAY_TRACING_NAME': 'My application', # Required - the segment name for segments generated from incoming requests\n}\n```\nFor more information about configuring Django with X-Ray read more about it in the [API reference](https://docs.aws.amazon.com/xray-sdk-for-python/latest/reference/frameworks.html)\n\n#### SQL tracing\nIf Django's ORM is patched - either using the `AUTO_INSTRUMENT = True` in your settings file\nor explicitly calling `patch_db()` - the SQL query trace streaming can then be enabled or \ndisabled updating the `STREAM_SQL` variable in your settings file. It is enabled by default.\n\n#### Automatic patching\nThe automatic module patching can also be configured through Django settings.\n```python\nXRAY_RECORDER = {\n    'PATCH_MODULES': [\n        'boto3',\n        'requests',\n        'local.module.ref',\n        'other_module',\n    ],\n    'IGNORE_MODULE_PATTERNS': [\n        'local.module.ref.some_file',\n        'other_module.some_module\\.*',\n    ],\n    ...\n}\n```\nIf `AUTO_PATCH_PARENT_SEGMENT_NAME` is also specified, then a segment parent will be created \nwith the supplied name, wrapping the automatic patching so that it captures any dangling\nsubsegments created on the import patching.\n\n### Django in Lambda\nX-Ray can't search on http annotations in subsegments.   To enable searching the middleware adds the http values as annotations\nThis allows searching in the X-Ray console like so\n\nThis is configurable in settings with `URLS_AS_ANNOTATION` that has 3 valid values\n`LAMBDA` - the default, which uses URLs as annotations by default if running in a lambda context\n`ALL` - do this for every request (useful if running in a mixed lambda/other deployment)\n`NONE` - don't do this for any (avoiding hitting the 50 annotation limit)\n\n```\nannotation.url BEGINSWITH \"https://your.url.com/here\"\n```\n\n### Add Flask middleware\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\nfrom aws_xray_sdk.ext.flask.middleware import XRayMiddleware\n\napp = Flask(__name__)\n\nxray_recorder.configure(service='fallback_name', dynamic_naming='*mysite.com*')\nXRayMiddleware(app, xray_recorder)\n```\n\n### Add Bottle middleware(plugin)\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\nfrom aws_xray_sdk.ext.bottle.middleware import XRayMiddleware\n\napp = Bottle()\n\nxray_recorder.configure(service='fallback_name', dynamic_naming='*mysite.com*')\napp.install(XRayMiddleware(xray_recorder))\n```\n\n### Serverless Support for Flask & Django & Bottle Using X-Ray\nServerless is an application model that enables you to shift more of your operational responsibilities to AWS. As a result, you can focus only on your applications and services, instead of the infrastructure management tasks such as server provisioning, patching, operating system maintenance, and capacity provisioning. With serverless, you can deploy your web application to [AWS Lambda](https://aws.amazon.com/lambda/) and have customers interact with it through a Lambda-invoking endpoint, such as [Amazon API Gateway](https://aws.amazon.com/api-gateway/). \n\nX-Ray supports the Serverless model out of the box and requires no extra configuration. The middlewares in Lambda generate `Subsegments` instead of `Segments` when an endpoint is reached. This is because `Segments` cannot be generated inside the Lambda function, but it is generated automatically by the Lambda container. Therefore, when using the middlewares with this model, it is important to make sure that your methods only generate `Subsegments`.\n\nThe following guide shows an example of setting up a Serverless application that utilizes API Gateway and Lambda:\n\n[Instrumenting Web Frameworks in a Serverless Environment](https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-python-serverless.html)\n\n### Working with aiohttp\n\nAdding aiohttp middleware. Support aiohttp >= 2.3.\n\n```python\nfrom aiohttp import web\n\nfrom aws_xray_sdk.ext.aiohttp.middleware import middleware\nfrom aws_xray_sdk.core import xray_recorder\nfrom aws_xray_sdk.core.async_context import AsyncContext\n\nxray_recorder.configure(service='fallback_name', context=AsyncContext())\n\napp = web.Application(middlewares=[middleware])\napp.router.add_get(\"/\", handler)\n\nweb.run_app(app)\n```\n\nTracing aiohttp client. Support aiohttp >=3.\n\n```python\nfrom aws_xray_sdk.ext.aiohttp.client import aws_xray_trace_config\n\nasync def foo():\n    trace_config = aws_xray_trace_config()\n    async with ClientSession(loop=loop, trace_configs=[trace_config]) as session:\n        async with session.get(url) as resp\n            await resp.read()\n```\n\n### Use SQLAlchemy ORM\nThe SQLAlchemy integration requires you to override the Session and Query Classes for SQL Alchemy\n\nSQLAlchemy integration uses subsegments so you need to have a segment started before you make a query.\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\nfrom aws_xray_sdk.ext.sqlalchemy.query import XRaySessionMaker\n\nxray_recorder.begin_segment('SQLAlchemyTest')\n\nSession = XRaySessionMaker(bind=engine)\nsession = Session()\n\nxray_recorder.end_segment()\napp = Flask(__name__)\n\nxray_recorder.configure(service='fallback_name', dynamic_naming='*mysite.com*')\nXRayMiddleware(app, xray_recorder)\n```\n\n### Add Flask-SQLAlchemy\n\n```python\nfrom aws_xray_sdk.core import xray_recorder\nfrom aws_xray_sdk.ext.flask.middleware import XRayMiddleware\nfrom aws_xray_sdk.ext.flask_sqlalchemy.query import XRayFlaskSqlAlchemy\n\napp = Flask(__name__)\napp.config[\"SQLALCHEMY_DATABASE_URI\"] = \"sqlite:///:memory:\"\n\nXRayMiddleware(app, xray_recorder)\ndb = XRayFlaskSqlAlchemy(app)\n\n```\n\n### Ignoring httplib requests\n\nIf you want to ignore certain httplib requests you can do so based on the hostname or URL that is being requsted. The hostname is matched using the Python [fnmatch library](https://docs.python.org/3/library/fnmatch.html) which does Unix glob style matching.\n\n```python\nfrom aws_xray_sdk.ext.httplib import add_ignored as xray_add_ignored\n\n# ignore requests to test.myapp.com\nxray_add_ignored(hostname='test.myapp.com')\n\n# ignore requests to a subdomain of myapp.com with a glob pattern\nxray_add_ignored(hostname='*.myapp.com')\n\n# ignore requests to /test-url and /other-test-url\nxray_add_ignored(urls=['/test-path', '/other-test-path'])\n\n# ignore requests to myapp.com for /test-url\nxray_add_ignored(hostname='myapp.com', urls=['/test-url'])\n```\n\nIf you use a subclass of httplib to make your requests, you can also filter on the class name that initiates the request. This must use the complete package name to do the match.\n\n```python\nfrom aws_xray_sdk.ext.httplib import add_ignored as xray_add_ignored\n\n# ignore all requests made by botocore\nxray_add_ignored(subclass='botocore.awsrequest.AWSHTTPConnection')\n```\n\n## License\n\nThe AWS X-Ray SDK for Python is licensed under the Apache 2.0 License. See LICENSE and NOTICE.txt for more information.\n", "release_dates": ["2023-10-12T17:10:43Z", "2023-04-03T17:06:11Z", "2022-11-10T23:33:06Z", "2022-06-30T18:49:46Z", "2021-12-06T20:59:35Z", "2021-04-29T00:09:26Z", "2021-03-24T17:28:00Z", "2020-06-08T22:07:54Z", "2020-04-14T13:57:20Z", "2019-12-06T00:42:21Z", "2019-03-05T18:37:29Z", "2019-02-28T00:37:20Z", "2019-02-26T21:04:22Z", "2019-01-11T00:40:43Z", "2018-10-05T23:30:22Z", "2018-09-05T23:23:16Z", "2018-08-29T01:09:04Z", "2018-05-15T19:14:42Z", "2018-04-19T20:15:30Z", "2018-03-28T18:55:36Z", "2018-03-01T19:30:21Z", "2017-12-12T20:02:13Z", "2017-11-15T23:21:58Z", "2017-10-10T18:38:17Z"]}, {"name": "aws-xray-sdk-ruby", "description": "The official AWS X-Ray Recorder SDK for Ruby", "language": "Ruby", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![Build Status](https://github.com/aws/aws-xray-sdk-ruby/actions/workflows/continuous-build.yml/badge.svg)\n\n### :mega: OpenTelemetry Ruby with AWS X-Ray\n\nAWS X-Ray supports using OpenTelemetry Ruby and the AWS Distro for OpenTelemetry (ADOT) Collector to instrument your application and send trace data to X-Ray. The OpenTelemetry SDKs are an industry-wide standard for tracing instrumentation. They provide more instrumentations and have a larger community for support, but may not have complete feature parity with the X-Ray SDKs. See [choosing between the ADOT and X-Ray SDKs](https://docs.aws.amazon.com/xray/latest/devguide/xray-instrumenting-your-app.html#xray-instrumenting-choosing) for more help with choosing between the two. Note that using OpenTelemetry Ruby and ADOT is in public preview, and like this SDK in beta, is not recommended for production applications.\n\nIf you want additional features when tracing your Ruby applications, please [open an issue on the OpenTelemetry Ruby Instrumentation repository](https://github.com/open-telemetry/opentelemetry-ruby-contrib/issues/new?labels=enhancement&title=X-Ray%20Compatible%20Feature%20Request).\n\n# AWS X-Ray SDK for Ruby (beta)\n\n![Screenshot of the AWS X-Ray console](https://raw.githubusercontent.com/aws/aws-xray-sdk-ruby/master/images/example_servicemap.png)\n\n## Installing\n\nThe AWS X-Ray SDK for Ruby is compatible with Ruby 2.3.6 and newer Ruby versions. It has experimental support for JRuby 9.2.0.0 (still forthcoming).\n\nTo install the Ruby gem for your project, add it to your project Gemfile. You must also add either the [Oj](https://github.com/ohler55/oj) or [JrJackson](https://github.com/guyboertje/jrjackson) gems, for MRI and JRuby respectively, for JSON parsing. The default JSON parser will not work properly, currently.\n\n```\n# Gemfile\ngem 'aws-xray-sdk'\ngem 'oj', platform: :mri\ngem 'jrjackson', platform: :jruby\n```\n\nThen run `bundle install`.\n\n## Getting Help\n\nUse the following community resources for getting help with the SDK. We use the GitHub\nissues for tracking bugs and feature requests.\n\n- Ask a question in the [AWS X-Ray Forum](https://forums.aws.amazon.com/forum.jspa?forumID=241&start=0).\n- Open a support ticket with [AWS Support](http://docs.aws.amazon.com/awssupport/latest/user/getting-started.html).\n- If you think you may have found a bug, open an [issue](https://github.com/aws/aws-xray-sdk-ruby/issues/new).\n\n## Opening Issues\n\nIf you encounter a bug with the AWS X-Ray SDK for Ruby, we want to hear about\nit. Before opening a new issue, search the [existing issues](https://github.com/aws/aws-xray-sdk-ruby/issues)\nto see if others are also experiencing the issue. Include the version of the AWS X-Ray\nSDK for Ruby, Ruby language, and other gems if applicable. In addition,\ninclude the repro case when appropriate.\n\nThe GitHub issues are intended for bug reports and feature requests. For help and\nquestions about using the AWS SDK for Ruby, use the resources listed\nin the [Getting Help](https://github.com/aws/aws-xray-sdk-ruby#getting-help) section. Keeping the list of open issues lean helps us respond in a timely manner.\n\n## Documentation\n\nThe [developer guide](https://docs.aws.amazon.com/xray/latest/devguide) provides in-depth guidance about using the AWS X-Ray service.\nThe [API Reference](http://docs.aws.amazon.com/xray-sdk-for-ruby/latest/reference/) provides documentation for public APIs of all classes in the SDK.\n\n## Quick Start\n\n**Configuration**\n\n```ruby\nrequire 'aws-xray-sdk'\n\n# For configuring sampling rules through X-Ray service\n# please see https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html.\n# The doc below defines local fallback sampling rules which has lower priority.\nmy_sampling_rules = {\n  version: 2,\n  rules: [\n    {\n      description: 'healthcheck',\n      host: '*',\n      http_method: 'GET',\n      url_path: '/ping',\n      fixed_target: 0,\n      rate: 0\n    }\n  ],\n  default: {\n    fixed_target: 1,\n    rate: 0.2\n  }\n}\n\nuser_config = {\n  sampling: true,\n  sampling_rules: my_sampling_rules,\n  name: 'default_segment_name',\n  daemon_address: '127.0.0.1:3000',\n  context_missing: 'LOG_ERROR',\n  patch: %I[net_http aws_sdk]\n}\n\nXRay.recorder.configure(user_config)\n```\n\n**Working with Lambda**\n\n```ruby\n# Bundle aws-xray-sdk with your code.\n# Require any aws-sdks and http libraries to be used, then...\nrequire 'aws-xray-sdk/lambda'\n# aws-sdk and http calls from here on will be instrumented\n```\n\nSee also [lib/aws-xray-sdk/lambda.rb](lib/aws-xray-sdk/lambda.rb)\n\n**Working with Rails**\n\n```ruby\n# Edit Gemfile to add XRay middleware\ngem 'aws-xray-sdk', require: ['aws-xray-sdk/facets/rails/railtie']\n\n# Configure the recorder in app_root/config/initializers/aws_xray.rb\nRails.application.config.xray = {\n  # default segment name generated by XRay middleware\n  name: 'myrails',\n  patch: %I[net_http aws_sdk],\n  # record db transactions as subsegments\n  active_record: true\n}\n```\n\n**Adding metadata/annotations using recorder**\n\n```ruby\nrequire 'aws-xray-sdk'\n\n# Add annotations to the current active entity\nXRay.recorder.annotations[:k1] = 'v1'\nXRay.recorder.annotations.update k2: 'v2', k3: 3\n\n# Add metadata to the current active entity\nXRay.recorder.metadata[:k1] = 'v1' # add to default namespace\nXRay.recorder.metadata(namespace: :my_ns).update k2: 'v2'\n\nXRay.recorder.sampled? do\n  XRay.recorder.metadata.update my_meta # expensive metadata generation here\nend\n```\n\n**Capture**\n\n```ruby\nrequire 'aws-xray-sdk'\n\nXRay.recorder.capture('name_for_subsegment') do |subsegment|\n  resp = myfunc()\n  subsegment.annotations.update k1: 'v1'\n  resp\nend\n\n# Manually apply the parent segment for the captured subsegment\nXRay.recorder.capture('name_for_subsegment', segment: my_segment) do |subsegment|\n  myfunc()\nend\n```\n\n**Thread Injection**\n\n```ruby\nrequire 'aws-xray-sdk'\n\nXRay.recorder.configure({patch: %I[net_http]})\n\nuri = URI.parse('http://aws.amazon.com/')\n# Get the active entity from current call context\nentity = XRay.recorder.current_entity\n\nworkers = (0...3).map do\n  Thread.new do\n    begin\n      # set X-Ray context for this thread\n      XRay.recorder.inject_context entity do\n        http = Net::HTTP.new(uri.host, uri.port)\n        http.request(Net::HTTP::Get.new(uri.request_uri))\n      end\n    rescue ThreadError\n    end\n  end\nend\n\nworkers.map(&:join)\n```\n\n**Start a custom segment/subsegment**\n\n```ruby\nrequire 'aws-xray-sdk'\n\n# Start a segment\nsegment = XRay.recorder.begin_segment 'my_service'\n# Start a subsegment\nsubsegment = XRay.recorder.begin_subsegment 'outbound_call', namespace: 'remote'\n\n# Add metadata or annotation here if necessary\nmy_annotations = {\n  k1: 'v1',\n  k2: 1024\n}\nsegment.annotations.update my_annotations\n\n# Add metadata to default namespace\nsubsegment.metadata[:k1] = 'v1'\n\n# Set user for the segment (subsegment is not supported)\nsegment.user = 'my_name'\n\n# End segment/subsegment\nXRay.recorder.end_subsegment\nXRay.recorder.end_segment\n```\n\n## License\n\nThe AWS X-Ray SDK for Ruby is licensed under the Apache 2.0 License. See LICENSE and NOTICE for more information.\n", "release_dates": ["2023-10-18T17:03:31Z", "2023-04-05T16:12:42Z", "2022-01-04T22:21:29Z", "2021-04-09T00:32:05Z", "2020-06-10T17:40:42Z", "2020-06-10T17:39:28Z", "2019-10-31T23:00:46Z", "2019-07-18T21:22:15Z", "2018-09-25T21:53:49Z", "2018-01-22T23:06:20Z"]}, {"name": "awsui-documentation", "description": "Information on how to get started using AWS UI components", "language": "TypeScript", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# AWS-UI is now Cloudscape Design System\n \nOn July 19th, 2022, we launched [Cloudscape Design System](https://cloudscape.design). Cloudscape is an evolution of AWS-UI. It offers user interface guidelines, front-end components, design resources, and development tools for building intuitive, engaging, and inclusive user experiences at scale. As part of the release, we have begun to publish npm packages to the new `@cloudscape-design` namespace.\n \n**We recommend migrating your existing AWS-UI applications to *[Cloudscape](https://cloudscape.design)*.**\n \n## Difference between AWS-UI and Cloudscape\nCloudscape uses a different visual theme than AWS-UI. The underlying code base remains the same.\n \n## Migration from AWS-UI to Cloudscape\nTo migrate, replace the AWS-UI related NPM packages you\u2019ve installed with the associated Cloudscape packages:\n \n|AWS-UI package name\t|Cloudscape package name\t|\n|---\t|---\t|\n|[@awsui/components-react](https://www.npmjs.com/package/@awsui/components-react)\t|[@cloudscape-design/components](https://www.npmjs.com/package/@cloudscape-design/components)\t|\n|[@awsui/global-styles](https://www.npmjs.com/package/@awsui/global-styles)\t|[@cloudscape-design/global-styles](https://www.npmjs.com/package/@cloudscape-design/global-styles)\t|\n|[@awsui/collection-hooks](https://www.npmjs.com/package/@awsui/collection-hooks)\t|[@cloudscape-design/collection-hooks](https://www.npmjs.com/package/@cloudscape-design/collection-hooks)\t|\n|[@awsui/design-tokens](https://www.npmjs.com/package/@awsui/design-tokens)\t|[@cloudscape-design/design-tokens](https://www.npmjs.com/package/@cloudscape-design/design-tokens)\t|\n \nFor more information, see [Get started with Cloudscape](https://cloudscape.design/get-started/guides/get-started-dev/).\n \n---\n \n## Getting started with AWS UI Components\n \n## Introduction\n \nIn 2017, AWS launched the AWS Design System to unify the customer experience across a vast portfolio of AWS services. The AWS Design System consists of human interface guidelines and user interface components that ensure a consistent, predictable experience for all AWS customers. It includes [AWS UI](https://www.npmjs.com/package/@awsui/components-react)\u2014a collection of more than 50 React components that help create intuitive, responsive, and accessible interfaces for web applications.\n \nAWS released the AWS UI components to NPM under the [Apache 2.0 open source license](https://www.apache.org/licenses/LICENSE-2.0.txt) in December 2020. In continuing with this release, we want to share the AWS Design System with the wider community. Anyone inside or outside of AWS looking to build custom experiences or projects can use the design system to meet their needs. This release is being staggered\u2014so the rest of the system, including the documentation, will not be fully open sourced until early 2022.\n \nThis guide provides a short introduction on how to use the AWS UI components.\n \n## Prerequisites\n \n- Familiarity with building [React](https://reactjs.org/) applications and using [NPM](https://www.npmjs.com) modules.\n- An existing React application (for example, created using [Create React App](https://reactjs.org/docs/create-a-new-react-app.html)).\n \n## What's in the packages?\n \n### [@awsui/components-react](https://www.npmjs.com/package/@awsui/components-react)\n \nThis is the main AWS UI package that contains the actual components. They're React components, with TypeScript definitions included, so this is the best place to start exploring.\n \n### [@awsui/global-styles](https://www.npmjs.com/package/@awsui/global-styles)\n \nIn this package, you'll find global typography-related styles, including the [Noto Sans](https://www.google.com/get/noto/) font and base font sizes. Make sure you import it once into every AWS UI application to ensure consistent styling.\n \n### [@awsui/collection-hooks](https://www.npmjs.com/package/@awsui/collection-hooks)\n \nA set of React hooks that you can use to control the state of the Table and Cards components (as well as related components, such as TextFilter and Pagination).\n \n### [@awsui/design-tokens](https://www.npmjs.com/package/@awsui/design-tokens)\n \nWith this package, you'll get a set of design tokens in a variety of formats, which you can use to build custom components that are visually consistent with the AWS Design System.\n \n### [@awsui/test-utils-core](https://www.npmjs.com/package/@awsui/test-utils-core)\n \nThis internal package is used to create utilities for writing unit and integration tests.\n \n## Using the components\n \n### Step 1: Include the [AWS UI Global Styles](https://www.npmjs.com/package/@awsui/global-styles) package\n \nThe AWS UI Global Styles package contains global styles for AWS UI components, including the [Noto Sans](https://www.google.com/get/noto/) font and base font sizes.\n \n#### 1. Install the package by running the following command:\n \n```\nnpm install @awsui/global-styles\n```\n \n#### 2. Include the styles in your application by adding the following import to the main component/page of your application:\n \n```\nimport \"@awsui/global-styles/index.css\"\n```\n \n### Step 2: Install the the [AWS UI React components](https://www.npmjs.com/package/@awsui/components-react)\n \n#### 1. Install the package by running the following command:\n \n```\nnpm install @awsui/components-react\n```\n \n#### 2. Import a component using the following syntax:\n \n```\nimport ComponentName from \"@awsui/components-react/{component-name}\"\n```\n \nFor example, to import the Button component:\n \n```\nimport Button from \"@awsui/components-react/button\"\n```\n \nAfter you import it, you can use the component as you would any other React component:\n \n```\n<Button>Hello!</Button>\n```\n \nNote: You can import components using the following syntax, but this might result in a larger overall bundle size:\n \n```\nimport { Button } from \"@awsui/components-react\"\n```\n \n### Step 3: View component properties\n \nYou can find a full list of components and documentation for those components' properties in the [components folder](./components/).\n \nWe also use TypeScript definitions to document component properties. If you're using a TypeScript-aware editor (such as [VSCode](https://code.visualstudio.com/)) you should see the full list of components, their available properties, and property types/accepted values via autocomplete/IntelliSense.\n \n### Step 4: Build an example form\n \nNow that you've downloaded and installed the component packages, you can get started building user interfaces. This walkthrough demonstrates how to use these components together by showing you how to build a simple form.\n \n![An example form interface](./example.png)\n \nTo build a form to capture user input, we recommend that you combine the following components:\n \n- [_Form_](./components/form.md) \u2013 Provides the basic form structure, including header and actions slots for action buttons.\n- [_Container_](./components/container.md) \u2013 Holds a group of related pieces of content, showing users that the items are related.\n- [_Header_](./components/header.md) \u2013 Provides styling for header elements.\n- [_SpaceBetween_](./components/space-between.md) \u2013 Adds consistent spacing between elements on your page.\n- [_FormField_](./components/form-field.md) \u2013 Enables you to associate labels and descriptions with controls such as text inputs and selects.\n- [_Input_](./components/input.md) \u2013 Provides a text input control, similar to the HTML `<input>` tag.\n- [_Select_](./components/select.md) \u2013 Provides a select control, similar to the HTML `<select>` tag.\n \nYou can find a full working example using these components in the [example folder](./example/).\n \n## Where can I find more documentation?\n \nYou can find a full list of components and documentation for those components' properties in the [components folder](./components/). Component properties are also documented via TypeScript definitions. If you\u2019re using a TypeScript-aware editor (such as [VSCode](https://code.visualstudio.com/)) you should see the full list of components, their available properties, and property types/accepted values via autocomplete/IntelliSense.\n \n## Having problems?\n \nContact us by [opening an issue](http://github.com/aws/awsui-documentation/issues).\n \n## Security\n \nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n \n## License Summary\n \nThe documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.\n \nThe sample code within this documentation is made available under the MIT-0 license. See the LICENSE-SAMPLECODE file.\n", "release_dates": []}, {"name": "base64io-python", "description": "A stream implementation for Python that provides transparent base64 encoding and decoding of an underlying stream. ", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2018-12-10T20:44:14Z"]}, {"name": "c3r", "description": "Cryptographic Computing for Clean Rooms (C3R) encryption client and SDK", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Cryptographic Computing for Clean Rooms (C3R)\n\nThe Cryptographic Computing for Clean Rooms (C3R) encryption client and software development kit (SDK) provide client-side tooling which allows users to participate in AWS Clean Rooms collaborations leveraging cryptographic computing by pre- and post-processing data.\n\nThe [AWS Clean Rooms User Guide](https://docs.aws.amazon.com/clean-rooms/latest/userguide/index.html) contains detailed information regarding how to use the C3R encryption client in conjunction with an AWS Clean Rooms collaboration.\n\nNOTICE: This project is released as open source under the Apache 2.0 license but is only intended for use with AWS Clean Rooms. Any other use cases may result in errors or inconsistent results.\n\n## Table of Contents\n- [Cryptographic Computing for Clean Rooms (C3R)](#cryptographic-computing-for-clean-rooms-c3r)\n  - [Table of Contents](#table-of-contents)\n  - [Getting Started](#getting-started)\n    - [Downloading Releases](#downloading-releases)\n    - [System Requirements](#system-requirements)\n    - [Supported Data Formats](#supported-data-formats)\n    - [AWS CLI Options in C3R](#aws-cli-options-in-c3r)\n    - [C3R CLI Modes](#c3r-cli-modes)\n      - [`schema` mode](#schema-mode)\n      - [`encrypt` mode](#encrypt-mode)\n      - [`decrypt` mode](#decrypt-mode)\n    - [SDK Usage Examples](#sdk-usage-examples)\n  - [Running C3R on Apache Spark](#running-c3r-on-apache-spark)\n    - [Security Notes for running C3R on Apache Spark](#security-notes-for-running-c3r-on-apache-spark)\n  - [General Security Notes](#general-security-notes)\n    - [Trusted Computing Environment](#trusted-computing-environment)\n    - [Temporary Files](#temporary-files)\n  - [Frequently Asked Questions](#frequently-asked-questions)\n    - [What data types can be encrypted?](#what-data-types-can-be-encrypted)\n    - [What Parquet data types are supported?](#what-parquet-data-types-are-supported)\n    - [What is an equivalence class?](#what-is-an-equivalence-class)\n    - [Does the C3R encryption client implement any non-standard cryptography?](#does-the-c3r-encryption-client-implement-any-non-standard-cryptography)\n  - [License](#license)\n\n\n## Getting Started\n\n### Downloading Releases\n\nThe C3R encryption client command line interface and related JARs can be downloaded from the [Releases](https://github.com/aws/c3r/releases) section of this repository. The SDK artifacts are also available on Maven's central repository.\n\n### System Requirements\n\n1. Java Runtime Environment version 11 or newer.\n\n2. Enough disk storage to hold cleartext data, temporary files, and the encrypted output. See the \"[Guidelines for the C3R encryption client](https://docs.aws.amazon.com/clean-rooms/latest/userguide/crypto-computing-guidelines.html)\" section of the user guide for details on how settings affect storage needs.\n\n### Supported Data Formats\n\nCSV and Parquet file formats are supported. For CSV files, the C3R encryption client treats all values as strings. For Parquet files, the data types are listed in [What Parquet data types are supported?](#what-parquet-data-types-are-supported). See [What data types can be encrypted?](#what-data-types-can-be-encrypted) for information on encryption of particular data types. Further details and  limitations are found in the \"[Supported file and data types](https://docs.aws.amazon.com/clean-rooms/latest/userguide/crypto-computing-file-types.html)\" section of the user guide.\n\nThe core functionality of the C3R encryption client is format agnostic; the SDK can be used for any format by implementing an appropriate [RowReader](https://github.com/aws/c3r/blob/main/c3r-sdk-core/src/main/java/com/amazonaws/c3r/io/RowReader.java) and [RowWriter](https://github.com/aws/c3r/blob/main/c3r-sdk-core/src/main/java/com/amazonaws/c3r/io/RowWriter.java).\n\n### AWS CLI Options in C3R\n\nModes which make API calls to AWS services feature optional `--profile` and `--region` flags, allowing for convenient selection of an AWS CLI named profile and AWS region respectively.\n\n### C3R CLI Modes\n\nThe C3R encryption client is an executable JAR with a command line interface (CLI). It has several modes of operation which are described in the usage help message, e.g.:\n\n```\n  schema   Generate an encryption schema for a tabular file.\n  encrypt  Encrypt tabular file content for use in a secure collaboration.\n  decrypt  Decrypt tabular file content derived from a secure collaboration.\n```\n\nThese modes are briefly described in the subsequent portions of this README.\n\n#### `schema` mode\n\nFor the C3R encryption client to encrypt a tabular file for a collaboration, it must have a corresponding schema file specifying how the encrypted output should be derived from the input.\n\nThe C3R encryption client can help generate schema files for an `INPUT` file using the `schema` command. E.g.,\n\n```\n$ java -jar c3r-cli.jar schema --interactive INPUT\n```\n\nSee the \"[Generate an encryption schema for a tabular file](https://docs.aws.amazon.com/clean-rooms/latest/userguide/prepare-encrypted-data.html#gen-encryption-schema-csv)\" section of the user guide for more information.\n\n#### `encrypt` mode\n\nGiven the following:\n\n1. a tabular `INPUT` file,\n\n2. a corresponding `SCHEMA` file,\n\n3. a collaboration `COLLABORATION_ID` in the form of a UUID, and\n\n4. an environment variable `C3R_SHARED_SECRET` containing a Base64-encoded 256-bit secret. See the \"[Preparing encrypted data tables](https://docs.aws.amazon.com/clean-rooms/latest/userguide/prepare-encrypted-data.html#create-SSK)\" section of the user guide for details on how to generate a shared secret key.\n\n\nAn encrypted `OUTPUT` file can be generated by running the C3R encryption client at the command line as follows:\n\n```\n$ java -jar c3r-cli.jar encrypt INPUT \\\n  --schema=SCHEMA \\\n  --id=COLLABORATION_ID \\\n  --output=OUTPUT\n```\n\nSee the \"[Encrypt data](https://docs.aws.amazon.com/clean-rooms/latest/userguide/prepare-encrypted-data.html#encrypt-data)\" section of the user guide for more information.\n\n#### `decrypt` mode\n\nOnce queries have been executed on encrypted data in an AWS Clean Rooms collaboration, that encrypted query results `INPUT` file can be decrypted generating a cleartext `OUTPUT` file using the same Base64-encoded 256-bit secret stored in the `C3R_SHARED_SECRET` environment variable, and `COLLABORATION_ID` as follows:\n\n```\n$ java -jar c3r-cli.jar decrypt INPUT \\\n  --id=COLLABORATION_ID \\\n  --output=OUTPUT\n```\n\nSee the \"[Decrypting data tables with the C3R encryption client](https://docs.aws.amazon.com/clean-rooms/latest/userguide/decrypt-data.html)\" section of the user guide.\n\n### SDK Usage Examples\n\nSDK usage examples are available in the SDK packages' `src/examples` directories.\n\n## Running C3R on [Apache Spark](http://spark.apache.org/)\n\nThe `c3r-cli-spark` package is a version of `c3r-cli` which must be submitted as a job to a running [Apache Spark](http://spark.apache.org/) server.\n\nThe JAR's `com.amazonaws.c3r.spark.cli.Main` class is submitted via the Apache Spark `spark-submit` script and the JAR is then run using passed command line arguments. E.g., here is how to view the top-level usage information:\n\n```\n./spark-3.4.0-bin-hadoop3-scala2.13/bin/spark-submit \\\n  --master SPARK_SERVER_URL \\\n  ... spark-specific options omitted ... \\\n  --class com.amazonaws.c3r.spark.cli.Main \\\n  c3r-cli-spark.jar \\\n  --help\n```\n\nAnd here is how to submit a job for encryption:\n\n```\nAWS_REGION=... \\\nC3R_SHARED_SECRET=... \\\n./spark-3.4.0-bin-hadoop3-scala2.13/bin/spark-submit \\\n  --master SPARK_SERVER_URL \\\n  ... spark-specific options omitted ... \\\n  --class com.amazonaws.c3r.spark.cli.Main \\\n  c3r-cli-spark.jar \\\n  encrypt INPUT.parquet \\\n  --schema=... \\\n  --output=... \\\n  --id=...\n```\n\n### Security Notes for running C3R on Apache Spark\n\nIt is important to note that `c3r-cli-spark` makes no effort to add _additional_ encryption to data transmitted or stored in temporary files by Apache Spark. This means, for example, that on an Apache Spark server with no encryption enabled, sensitive info such as the `C3R_SHARED_SECRET` will appear in plaintext RPC calls between the server and workers. It is up to users to ensure their Apache Spark server has been configured according to their specific security needs. See the Apache Spark [security documentation](https://spark.apache.org/docs/latest/security.html) for guidance on how to configure Apache Spark server security settings.\n\n## General Security Notes\n\nThe following is a high level description of some security concerns to keep in mind when using the C3R encryption client to encrypt data.\n\n### Trusted Computing Environment\n\nThe shared secret key and data-to-be-encrypted is by default consumed directly from disk by the C3R encryption client on a user\u2019s machine. It is, therefore, left to users to take any and all necessary precautions to ensure those security concerns beyond what the C3R is capable of enforcing are met. For example:\n\n1. the machine running the C3R encryption client meets the user\u2019s needs as a trusted computing platform,\n\n2. the C3R encryption client is run in a minimally privileged manner and not exposed to untrusted data/networks/etc., and\n\n3. any post-encryption cleanup/wiping of keys and/or data is performed as needed on the system post encryption.\n\n### Temporary Files\n\nWhen encrypting a source file, the C3R encryption client will create temporary files on disk. These files will be deleted when the C3R encryption client finishes generating the encrypted output. Unexpected termination of the C3R encryption client execution may prevent the C3R encryption client or JVM from deleting these files, allowing them to persist on disk. These temporary files will have all columns of type `fingerprint` or `sealed` encrypted, but some additional privacy-enhancing post-processing may not have been completed. By default, the C3R encryption client will utilize the host operating system\u2019s temporary directory for these temporary files. If a user prefers an explicit location for such files, the optional `--tempDir=DIR` flag can specify a different location to create such files.\n\n\n## Frequently Asked Questions\n\n### What data types can be encrypted?\nCurrently, only string values are supported by sealed columns.\n\nFor fingerprint columns, types are grouped into [equivalence classes](#what-is-an-equivalence-class). Equivalence classes allow identical fingerprints to be assigned to the same semantic value regardless of the original representation. For example, the _integral value_ `42` will be assigned the same fingerprint regardless of whether it was originally an `SmallInt`, `Int`, or `BigInt`. No non-integral values, however, will ever be assigned the same fingerprint as the integral value `42`. \n\nThe following equivalence classes are supported by fingerprint columns:\n- `BOOLEAN`\n- `DATE`\n- `INTEGRAL`\n- `STRING`\n\nFor CSV files, the C3R encryption client treats all values simply as UTF-8 encoded text and makes no attempt to interpret them differently prior to encryption.\n\nFor Parquet files, an error will be raised if a non-supported type for a particular column type is used.\n\n### What Parquet data types are supported?\nThe C3R encryption client can process any non-complex (i.e., primitive) data in a Parquet file that represents a data type supported by Clean Rooms. The following Parquet data types are supported:\n- `Binary` with the following logical annotations:\n  - None if the `--parquetBinaryAsString` is set (`STRING` data type)\n  - `Decimal(scale, precision)` (`DECIMAL` data type)\n  - `String` (`STRING` data type)\n- `Boolean` with no logical annotation (`BOOLEAN` data type)\n- `Double` with no logical annotation (`DOUBLE` data type)\n- `Fixed_Len_Binary_Array` with the `Decimal(scale, precision)` logical annotation (`DECIMAL` data type)\n- `Float` with no logical annotation (`FLOAT` data type)\n- `Int32` with the following logical annotations:\n  - None (`INT` data type)\n  - `Date` (`DATE` data type)\n  - `Decimal(scale, precision)` (`DECIMAL` data type)\n  - `Int(16, true)` (`SMALLINT` data type)\n  - `Int(32, true)` (`INT` data type)\n- `Int64` with the following logical annotations:\n  - None (`BIGINT` data type)\n  - `Decimal(scale, precision)` (`DECIMAL` data type)\n  - `Int(64, true)` (`BIGINT` data type)\n  - `Timestamp(isUTCAdjusted, TimeUnit.MILLIS)` (`TIMESTAMP` data type)\n  - `Timestamp(isUTCAdjusted, TimeUnit.MICROS)` (`TIMESTAMP` data type)\n  - `Timestamp(isUTCAdjusted, TimeUnit.NANOS)` (`TIMESTAMP` data type)\n\n### What is an equivalence class?\nAn equivalence class is a set of data types that can be unambiguously compared for equality via a representative data type.\n\nThe equivalence classes are:\n- `BOOLEAN` containing data types: `BOOLEAN`\n- `DATE` containing data types: `DATE`\n- `INTEGRAL` containing data types: `BIGINT`, `INT`, `SMALLINT`\n- `STRING` containing data types: `CHAR`, `STRING`, `VARCHAR`\n\n### Does the C3R encryption client implement any non-standard cryptography?\n\nThe C3R encryption client uses only NIST-standardized algorithms and-- with one exception-- only by calling their implementation in the Java standard cryptographic library. The sole exception is that the client has its own implementation of HKDF (from RFC5869), but using MAC algorithms from the Java standard cryptographic library.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2024-01-16T19:26:25Z", "2023-12-07T01:00:47Z", "2023-11-06T19:25:42Z", "2023-10-09T19:56:33Z", "2023-09-06T16:56:41Z", "2023-08-14T19:07:50Z", "2023-07-10T18:26:51Z", "2023-06-08T23:53:27Z", "2023-05-01T18:35:06Z", "2023-04-04T21:49:50Z", "2023-03-21T21:14:04Z", "2023-03-06T20:14:34Z", "2023-01-12T22:55:44Z"]}, {"name": "chalice", "description": "Python Serverless Microframework for AWS", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": []}, {"name": "chrony-candm", "description": "Rust API for Chrony's control & monitoring interface", "language": "Rust", "license": {"key": "gpl-2.0", "name": "GNU General Public License v2.0", "spdx_id": "GPL-2.0", "url": "https://api.github.com/licenses/gpl-2.0", "node_id": "MDc6TGljZW5zZTg="}, "readme": "## chrony-candm\n\n`chrony-candm` is a Rust crate for communicating with\n[Chrony](https://chrony.tuxfamily.org/)'s control & monitoring interface.\nIt provides programmatic access to information about Chrony's status that\none would otherwise have to scrape from the output of `chronyc`. For API\ndocumentation, see [docs.rs](https://docs.rs/chrony-candm).\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the GNU General Public License, version 2.0.\nFor details, see [LICENSE](LICENSE). See [NOTICE](NOTICE) and [THIRD-PARTY](THIRD-PARTY) respectively for first- and third-party copyright notices and acknowledgements.\n\n`chrony-candm` is a community contribution from Amazon and is not affiliated\nwith the Chrony project.\n", "release_dates": []}, {"name": "clock-bound", "description": "Used to generate and compare bounded timestamps.", "language": "Rust", "license": null, "readme": "# ClockBound\n\n## Summary:\nClockBound allows you to generate and compare bounded timestamps that include accumulated error as reported from the local chronyd process. On every request, ClockBound uses two pieces of information: the current time and the associated absolute error range, which is also known as the clock error bound. This means that the \u201ctrue\u201d time of a ClockBound timestamp is within a set range.  \n\nUsing ClockBound with a consistent, trusted time service will allow you to compare timestamps to determine order and consistency for events and transactions, independent from the instances\u2019 respective geographic locations. We recommend you use the Amazon Time Sync Service,  a highly accurate and reliable time reference that is natively accessible from Amazon EC2 instances, to get the most out of ClockBound on your AWS infrastructure. For more information on the Amazon Time Sync Service, see the [EC2 User Guide](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-time.html).  \n\n## Calculations:\nClock accuracy is a measure of clock error, typically defined as the offset to UTC. This clock error is the difference between the observed time on the computer and reference time (also known as true time). In an NTP architecture, this error can be bounded using three measurements that are defined by the protocol:\n-\tLocal offset (the system time): The residual adjustment to be applied to the operating system clock.\n-\tRoot dispersion: The accumulation of clock drift at each NTP server on the path to the reference clock.\n-\tRoot delay: The accumulation of network delays on the path to the reference clock.\n\nThe clock error bound is calculated using the formula below:  \n\nClock Error Bound = |Local Offset| + Root Dispersion + (Root Delay / 2)  \n\n![Clock Error Bound Image](ClockErrorBound.png)  \nFigure 1: The clock error bound provides a bound on the worst case offset of a clock with regard to \u201ctrue time\u201d.\n\nThe combination of local offset, root dispersion, and root delay provides us with a clock error bound. For a given reading of the clock C(t) at true time t, this bound makes sure that true time exists within the clock error bound. The clock error bound is used as a proxy for clock accuracy and measures the worst case scenario (see Figure 1). Therefore, clock error bound is the main metric used to determine the accuracy of an NTP service.  \n\nClockBound uses this clock error bound to return a bounded range of timestamps. This is calculated by adding and subtracting the clock error bound from the timestamp provided by a system's clock. It also contains functionality to check if a given timestamp is in the past or future. This allows users to have consistency when dealing with time sensitive transactions.\n\n## Usage\n\nClockBound is composed of two parts:  \n[ClockBoundD](clock-bound-d/README.md) - A daemon to provide clients with an error bounded timestamp interval.  \n[ClockBoundC](clock-bound-c/README.md) - A client library to communicate with ClockBoundD.\n\nTo use ClockBound you first need to install and set up ClockBoundD. See the [ClockBoundD README](clock-bound-d/README.md) for more info.\n\nOnce ClockBoundD is set up you will need a client to communicate with it. The ClockBoundC client library is the recommended method of usage from a rust codebase.\n\n### Custom Client\n\nThe [ClockBound Protocol](PROTOCOL.md) is provided if there is interest in creating a custom client.\n\nClients can be created in any programming language that can communicate with Unix Datagram Sockets:\n1. Bind the client to its own Unix Datagram Socket.\n2. Connect the client's Unix Datagram Socket to ClockBoundD's own socket.\n3. Send and receive messages as defined in the [ClockBound Protocol](PROTOCOL.md).\n\nSee the implementation in [ClockBoundC](clock-bound-c/README.md) as an example.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nClockBoundC is licensed under the [Apache 2.0 LICENSE](clock-bound-c/LICENSE).\n\nClockBoundD is licensed under the [GPL v2 LICENSE](clock-bound-d/LICENSE)\n\n", "release_dates": []}, {"name": "codeartifact-origin-control-toolkit", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS CodeArtifact Package Origin Control toolkit\n\n## Overview\n\n[AWS CodeArtifact](https://docs.aws.amazon.com/codeartifact/index.html) is a fully managed artifact repository service\nthat makes it easy for organizations to securely store and share software packages used for application development. \nOn [launch date] we introduced a feature called \u201cPackage Origin Control\u201d which allows customers to protect themselves \nagainst \u201cdependency substitution\u201c or \n\u201c[dependency confusion](https://medium.com/@alex.birsan/dependency-confusion-4a5d60fec610)\" attacks.\n\nWhile this feature protects new packages by default, packages which lived in CodeArtifact repositories prior to the\nfeature release are not protected without explicit configuration.\n\nThe purpose of this toolkit is to provide repository administrators with an easy way to set Origin Control policies\nin bulk on packages that have not received the default protection because they pre-date feature release. \nThis can be achieved by blocking upstream versions for internal packages. The toolkit also supports blocking publishing\npackage versions to avoid the creation of a potentially vulnerable mixed state for external packages as well.  \n\nMore information can be found on the [origin control feature documentation](https://docs.aws.amazon.com/codeartifact/latest/ug/package-origin-controls.html) as well as in the \n[blog post](https://aws.amazon.com/blogs/devops/tighten-your-package-security-with-codeartifact-package-origin-control-toolkit/) announcing the availability of this toolkit. \n\n## Structure\n\nThe toolkit is comprised of two scripts: a first one called `generate_package_configurations.py` for creating a manifest \nfile listing the packages in a domain alongside their proposed origin configuration to apply, and a second one named \n`apply_package_configurations.py` that reads the manifest file and applies the configuration within. \n\n\n`generate_package_configurations.py` can operate on a whole repository, or on a subset of packages \n(specified either via filters, or though a list) and supports two origin control resolution modes: \n\n- **Manual**: Supply the origin configuration yourself via a manifest file. This is an appropriate option if you \nalready maintain a list of internal packages, or if they are published in a consistent internal namespace which \nallows for them to be easily selected.\n- **Automated**: Identifies which packages should have their upstreams blocked by analyzing the upstream repository \ngraph and external connections, looking for evidence that package versions are only available from the repository at \nhand- in which case it determines it can disable sourcing of upstream versions can be done without risk of breaking \nbuilds. This is a good option if you want a quick way to tighten your security posture without having to manually \nanalyze your whole repository.\n\n`apply_package_configurations.py` takes the manifest file generated by `generate_package_configurations.py` as in input, \nand applies the origin control changes by calling the new [`PutPackageOriginConfiguration`](https://docs.aws.amazon.com/id_id/cli/latest/reference/codeartifact/put-package-origin-configuration.html) API.\n\nPrecisely because it is meant to set these values in bulk, \nthis script supports backup and revert operations by default, as well as dry-run and step-by-step confirmation options.\nIf you identify an issue after applying origin control changes, you will be able to safely revert to the original, \nworking configuration before trying again. See the [Backup and restore](#backup-and-restore) section for details.\n\n\n## Installing\n\nThe toolkit only depends on the `boto3` and `tqdm` packages. In order to install, simply run:\n\n```bash\npip install -r requirements.txt\n```\n\n## Configuring\n\nThe toolkit uses the same configuration as the AWS CLI to run. This means that you can either set one the following \ntwo environment variable sets:\n\n```\nAWS_PROFILE\n``` \nor \n\n```\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\nAWS_SESSION_TOKEN\n```\n\nAlternatively, you can use the `--profile` flag to indicate what specific AWS CLI profile you want to use. Please note \nthat this flag is only used for authentication purposes and thus even if you have specified a `region` parameter in your\nAWS CLI profile, you will still be required to pass in a `--profile` flag to the script.\n\nAdditionally, the account you are using to authenticate must have at least repository-level read permissions to run \nthe first stage in manual mode, and read permissions on all repositories upstream of the target repository in auto mode.\nStage 2 requires repository read permissions if the backup feature is enabled (default) as well as write permissions \nto execute unless you want to use dry-run mode(see the \"More Options\" section below).\n\n\n## Using\nThe toolkit works on a per-repository basis. It is structured in two stages: in the first one a manifest is produced \nconsisting of a CSV listing all the packages in the target repository, alongside their desired origin\ncontrol configuration. The second stage is responsible for taking the generated CSV and setting the desired origin \ncontrol configuration on every package listed within.\n\n### Stage 1: Generating the changes manifest\n\nThe first stage is invoked through `generate_package_configurations.py`. It requires values for\n `domain`, `repository` as well as `region` to be supplied. \n\n#### Specifying origin configuration\nOrigin configuration is always supplied as as string like \n```\npublish=[value],upstream=[value]\n```\nwhere `[value]` can be either `ALLOW` or `BLOCK`. So by default all existing packages will have \n\n```\npublish=ALLOW,upstream=ALLOW\n```\n\nIn order to tighten security for an internally-published package, you would want to disable upstream versions like\n\n```\npublish=ALLOW,upstream=BLOCK\n```\n\nConversely, if you wanted to prevent users from publishing new versions to a package, you would set: \n\n```\npublish=BLOCK,upstream=ALLOW\n```\n\nThese settings are always supplied as a tuple and should be thought of as working in concert. \n\n#### Generating from list vs generating from query\nOnce the repository, domain, and region values have been supplied, you must select which packages to generate origin \ncontrol configurations for. It is possible to select either all packages within the repository, or a subset.\n\nIn order to select a subset of packages available in the repository, two options are available: either by supplying\na list of package names or through a query. Please note that multiple namespaces and package formats aren't supported at once and \nyou will have to repeat this operation explicitly for each one.\n\nWorking with a supplied packages list is as easy as specifying the input file name, which should have one name per line.\nFor example, if you wanted to `BLOCK` upstreams for some internal `npm` packages as listed in an `inputfile.log` \nfile:\n\n```\ninternal-package-1\ninternal-package-2\ninternal-package-3\n```\n\nYou would call the first stage script:\n\n```\npython generate_package_configurations.py \n--region us-west-2  \n--domain test-domain \n--repository test-repository \n--from-list inputfile.csv\n--format npm\n```\n\nAlternatively, you can select the packages in question:\n\n```\npython generate_package_configurations.py \n--region us-west-2  \n--domain test-domain \n--repository test-repository \n--format maven\n--namespace example-namespace\n```\n\n#### Automatic vs. manual origin control setting\nOnce you selected a package set, you have two ways of bulk-setting the origin configuration for each package in it: \nthe simplest is by explicitly setting the policy via the `--set-restrictions` flag, which we refer to as \"manual\" mode.\n\nFor example\n\n```\npython generate_package_configurations.py\n--region us-west-2 \n--domain test-domain \n--repository test-repository \n--format pypi \n--prefix some-prefix \n--set-restrictions upstream=ALLOW,publish=BLOCK\n```\n\nOtherwise, you can use \"automatic\" mode simply by omitting the above flag. This mode is meant for administrators who\nwant the most hassle-free experience: the toolkit will try to identify packages which can have their upstreams blocked \nsafely, and will otherwise fall back on allowing upstreams.\n\nThe heuristic will block acquisition of new versions from upstreams if and only if the target repository doesn't have \ndirect access to an external connection, and no versions of the package are available via any of the upstreams, \neither because the target repository doesn't have any upstreams or because none of the upstreams have the package. \nTherefore, we assume there isn\u2019t an immediate external connection attached to the repository for the package format(s) \nyou are trying to run this script against. \n\nIn order to generate the list of new origin control configuration for the same subset of packages as in the \nprevious example, simply omit the `--set-restrictions` flag and run:\n\n```\npython generate_package_configurations.py\n--region us-west-2 \n--domain test-domain \n--repository test-repository \n--format pypi \n--prefix some-prefix \n```\n\n#### Saving to a file\n\nBy default the script will save its results to a file called `origin_configuration.csv`. You can use the `--output` flag \nto change this to a path of your liking.\n\n### Stage 2: Applying changes\nOnce a well-formed CSV has been produced, it can be fed to the second stage, `apply_package_configurations.py`.\n\nThe same parameters as before (`region`, `domain`, `repository`) must be provided even though they are also present\nin the CSV columns. This is to ensure there is no ambiguity and to confirm you are operating on the right repository.\n\nInvoking the second stage on `origin_configuration.csv` therefore looks like this:\n\n```\npython apply_package_configurations.py \n--region us-west-2  \n--domain test-domain \n--repository test-repository \n--input origin_configuration.csv \n```\n\n#### More options \n\n- `--validate-only`: Verifies that the CSV is well-formed\n- `--dry-run`: Doesn't actually call the API, but shows what the script would do.\n- `--trace`: Enables a more verbose mode.\n- `--list-failed `:  In case of failure, lists packages that have failed to update the origin control configuration.\n- `--retry-failed `:  Tries again to set the origin control configuration for packages that have failed to do so.\n- `--ask-confirmation`: Requires step-by-step confirmation for all write actions.\n- `--num-workers`: Controls the number of parallel workers making calls to CodeArtifact (default: 4)\n\n#### Backup and restore\nBy default, before changing any origin control configurations the script will back up the existing configuration for \nevery package it touches (this behavior can be disabled with the `--no-backup` flag). Should you want to revert to the\nprevious configuration, you can simply use the `--revert` flag on the same input file.\n\n```\npython apply_package_configurations.py \n--region us-west-2  \n--domain test-domain \n--repository test-repository \n--input origin_configuration.csv \n--restore\n```\n\n## Links\n\n- [Toolkit release blog post](https://aws.amazon.com/blogs/devops/tighten-your-package-security-with-codeartifact-package-origin-control-toolkit/)\n- [Package Origin Configuration documentation](https://docs.aws.amazon.com/codeartifact/latest/ug/package-origin-controls.html) \n\n## License\nThis software is released under the Apache 2.0 license.\n", "release_dates": []}, {"name": "codecatalyst-blueprints", "description": null, "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![Build](https://github.com/aws/codecatalyst-blueprints/actions/workflows/build-action.yml/badge.svg) \n\n[AWS official Blueprints documentation](https://docs.aws.amazon.com/codecatalyst/latest/userguide/custom-blueprints.html) and\n[Wiki](https://github.com/aws/codecatalyst-blueprints/wiki).\n\nThis repository contains common blueprint components, the base blueprint contructs and several public blueprints. Codecatalyst blueprints are\navailable for anyone to develop today. Blueprints are built by a number of teams internally, and this repository only contains the base constructs and\na small number of blueprints maintained by the core blueprints team.\n\n## Blueprints\n\nBlueprints are code generators used to create and maintain projects in [Amazon CodeCatalyst](https://codecatalyst.aws/). You can build your own\nblueprint today by upgrading to the CodeCatalyst Enterprise tier.\n\n### Building your own blueprint\n\nCustom blueprints and lifecycle management are generally available to everyone. To build your own blueprint, go to\n[codecatalyst.aws](https://codecatalyst.aws/) and make sure your space is upgraded to the Enterprise tier. For more information, see\n[Changing your CodeCatalyst billing tier](https://docs.aws.amazon.com/codecatalyst/latest/adminguide/managing-billing-change-plan.html) and\n[Getting started with custom blueprints](https://docs.aws.amazon.com/codecatalyst/latest/userguide/getting-started-bp.html).\n\n### Contributions\n\nSee our [contribution guidelines](./CONTRIBUTING.md) and our community [Code of Conduct](./CODE_OF_CONDUCT.md) before opening a pull request.\nCodeCatalyst blueprints want your feedback and bug reports too! Please add them to our github issues for triage by the service team.\n\n### Learning resources\n\nTo learn more about blueprints, see the [wiki](https://github.com/aws/codecatalyst-blueprints/wiki). To learn about using blueprints for your\nCodeCatalyst projects or steps to create a custom blueprint, see the\n[AWS documentation](https://docs.aws.amazon.com/codecatalyst/latest/userguide/blueprints.html).\n\nIn this repository you can find our blueprints SDK, tooling, and several sample blueprints\n\n- Blueprint [base and examples](https://github.com/aws/codecatalyst-blueprints/tree/main/packages/blueprints): `/packages/blueprints/`\n  - These are some of the blueprints available to you on CodeCatalyst.\n  - [Base blueprint](https://github.com/aws/codecatalyst-blueprints/tree/main/packages/blueprints/blueprint): All blueprints extend this base\n    blueprint.\n  - [Blueprint builder](https://github.com/aws/codecatalyst-blueprints/tree/main/packages/blueprints/blueprint-builder): A blueprint that generates\n    additional blueprints.\n  - ...\n- Blueprint [component constructs](https://github.com/aws/codecatalyst-blueprints/tree/main/packages/components): `/packages/components/`\n  - These are components used to make working with and generating CodeCatalyst resources easier. See the\n    [AWS documentation](https://docs.aws.amazon.com/codecatalyst/latest/userguide/develop-publish-bp.html) for detailed API docs on how to use these\n    components in your project.\n- Blueprint [utility tooling](https://github.com/aws/codecatalyst-blueprints/tree/main/packages/utils): `packages/utils/`\n  - This tooling contains the blueprints CLI (used to publish blueprints), as well as basic Projen constructs that define the blueprint and component\n    construct codebases.\n\n# Development\n\nThis section details how to develop in this repository. We recommend you use [VSCode](https://code.visualstudio.com/). While plugins also exist for\nvim, many gitignored files might be invisible in vim and can cause disruptive issues. To get an overview of blueprints and what they are, see the\n[Wiki](https://github.com/aws/codecatalyst-blueprints/wiki).\n\n## Install prerequisite node tooling\n\nBlueprints are TypeScript node modules by default (they don't have to be). Install the node tooling globally. These are requirements for various\ntooling to work properly and are available from public npm.\n\n```\nbrew install nvm            # blueprints work with Node 18.x\nbrew install jq\nnvm use\nnpm install npm@6.14.13 -g  # we suggest using npm 6.14.13, v9.7.2 has performance issues\nnpm install yarn ts-node webpack webpack-cli -g\n```\n\n## Developing codecatalyst-blueprints\n\nPull down this codebase. We recommend making your own fork. For more information, see the [contribution guidelines](./CONTRIBUTING.md).\n\n```\ngit clone <my-fork-codecatalyst-blueprints>\n```\n\nRun these commands to get started building blueprints. The first time set-up may take a few minutes.\n\n```\ncd codecatalyst-blueprints\nnvm use\nyarn && yarn build\n```\n\nAfter completing the setup, you can make changes to the repository and test them out yourself.\n\n### Test a blueprint in CodeCatalyst\n\nThe easiest way to test a blueprint directly in CodeCatalyst is publishing that blueprint into a space you own. Publishing a blueprint allows you to\ntest a CodeCatalyst blueprint directly in a space. You must be an adminstrator of the target space in order to successfully publish. Your target space\nmust be part of the Enterprise tier as well.\n\n```\ncd packages/blueprints/<blueprint>\nyarn blueprint:preview --space my-awesome-space # publishes under a \"preview\" version tag to 'my-awesome-space'\nyarn blueprint:release --space my-awesome-space # publishes normal version to 'my-awesome-space'\n\nyarn blueprint:preview --space my-awesome-space --project my-project # previews blueprint application to an existing project\n```\n\nThis will publish a private verision of your blueprint into `my-awesome-space`. It will only be available for that space. You may run the command\nmultiple times to publish to multiple spaces.\n\n### Test a blueprint locally\n\nThe fastest way to test a blueprint is to build it locally. You can do this by invoking the following command:\n\n```\ncd packages/blueprints/<blueprint>\nyarn blueprint:synth (--cache) # cache will emulate how the wizard processes the blueprint\nyarn blueprint:resynth (--cache)\n```\n\nThe `yarn blueprint:synth` command will mock generate a **new** project with a set of options, while `yarn blueprint:resynth` command will mock\ngenerate into an existing project or changing options. Each of these commands result in an output bundle being added into the `./synth/` folder for\neach mocked wizard configuration under `wizard-configuration/*.json`. Each of these JSONs represent a partial set of options to be merged on top of\nthe `defaults.json` when synthesizing.\n\nFor a deep dive on blueprint generation, the bundle format, and how to think about lifecycle updates, see the\n[Wiki](https://github.com/aws/codecatalyst-blueprints/wiki/Resynthesis).\n\n## Testing Changes to a blueprint component\n\nBlueprints are made up of components found under `./packages/components`. These component constructs represent project components (such as a source\nrepository).\n\nModify a component:\n\n```\ncd packages/components/<component>\n```\n\nRebuild the component:\n\n```\nyarn build\n```\n\nTo see the changes applied in a blueprint run synth:\n\n```\ncd packages/blueprints/<blueprint>\nyarn blueprint:synth\n```\n\nThis generates the blueprint in the `synth` folder:\n\n```\npackages/blueprints/<blueprint>/synth/\n```\n\n## Snapshot testing\n\nBlueprints support [snapshot testing](https://jestjs.io/docs/snapshot-testing) on configurations provided by blueprint authors. Once snapshot testing\nis enabled and configured, the build/test process will synthesize the given configurations and verify that the synthesized outputs haven't changed\nfrom the reference snapshot.\n\nTo enable:\n\n1. In `.projenrc.ts`, update the input object to `ProjenBlueprint` with the file(s) you want snapshoted.\n\n```\n{\n  ....\n  blueprintSnapshotConfiguration: {\n    snapshotGlobs: ['**', '!environments/**', '!aws-account-to-environment/**'],\n  },\n}\n```\n\n2. Resynthesize the blueprint with `yarn projen`. This will create several TypeScript files in your blueprint project. Don't edit these source files,\n   as they're maintained and regenerated by Projen.\n3. Find the directory `src/wizard-configurations`, where you'll find the file `default-config.json` with an empty object. Customize or replace this\n   file with one or more of your own test configurations. Each test configurations will be merged with the project's `defaults.json`, synthesized, and\n   compared to snapshots during `yarn test`.\n\nTo run: `yarn test` or `yarn test:update` or any task that includes _test_. The first time you run it, expect to see the following lines:\n\n> Snapshot Summary \u203a NN snapshots written from 1 test suite.\n\nSubsequent test runs will verify that synthesized output hasn't changed from these snapshots and display a line like:\n\n> Snapshots: NN passed, NN total\n\nIf you intentionally change your blueprint to emit different output, then run `yarn test:update` to update the reference snapshots.\n\nSnapshots expect synthesized output to be constant from run to run. If your blueprint generates files that vary, you must exclude it from snapshot\ntesting. Update the `blueprintSnapshotConfiguration` object of your `ProjenBlueprint` input object to add the `snapshotGlobs` property. This property\nis an array of [globs](https://github.com/isaacs/node-glob#glob-primer) that determine which files to include and exclude from snapshotting. Note that\nthere is a _default_ list of globs; if you specify your own list, you may need to explicitly bring back the default entries.\n", "release_dates": []}, {"name": "codelyzer", "description": "Codelyzer is a framework that provides interfaces to build and analyze source code in various languages and generates a platform-independent representation as a universal abstract syntax tree (UAST) model or a JSON file.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Codelyzer\n![Build Test](https://github.com/aws/codelyzer/workflows/Build%20Test/badge.svg)\n\nCodelyzer is a framework that provides interfaces to build and analyze source code in various languages and generates a platform-independent representation as a universal abstract syntax tree (UAST) model or a JSON file. It offers fine-grained controls to specify the kind of metadata (properties of classes, methods, etc.) to gather and how deep in the hierarchy of the code to search while generating these artifacts. Currently, the framework only supports the C# language.\n\nBy generating the output as a JSON file, this framework allows you to develop analysis tools in any language.\n\n## Codelyzer - Net\n\nCodelyzer-Net is an analyzer engine for languages based on the Roslyn compiler platform, like C# and VB. The CSharpRoslynProcessor walks an AST to collect metadata of source file components (e.g. solution, projects, namespaces, classes, methods, method invocations, literal expressions, etc). It uses semantic information from a design-time build to collect properties with fully qualified names.\n\n1. Add the Codelyzer NuGet package source into your Nuget configuration. \n   * [https://s3-us-west-2.amazonaws.com/aws.portingassistant.dotnet.download/nuget/index.json](https://s3-us-west-2.amazonaws.com/aws.portingassistant.dotnet.download/nuget/index.json)\n2. Add Codelyzer.Analysis to your project as a Nuget Package.\n\n## Getting Started\n\nFollow the example below to see how the library can be integrated into your application for analyzing .NET application.\n\n```csharp\n/* 1. Create logger object */\nvar loggerFactory = LoggerFactory.Create(builder => \n        builder.SetMinimumLevel(LogLevel.Debug).AddConsole());\nvar logger = loggerFactory.CreateLogger(\"Analyzer\");\nvar outputPath = @\"/home/users/steve/porting-analysis\";\n\n/* 2. Create Configuration settings */\nvar configuration = new AnalyzerConfiguration(LanguageOptions.CSharp);\nconfiguration.ExportSettings.OutputPath = outputPath;\n\n/* 3. Get Analyzer instance based on language */\nvar analyzer = CodeAnalyzerFactory.GetAnalyzer(configuration, logger);\n\n/* 4. Analyze the project or solution */\nvar projectFilePath = @\"/home/users/steve/projects/TestProject.csproj\";\nvar analyzerResult = await analyzer.AnalyzeProject(projectFilePath);\n\nConsole.WriteLine(\"The results are exported to file : \" + analyzerResult.OutputJsonFilePath);\n\n/* 5. Consume the results as model objects */\nvar sourcefile = analyzerResult.ProjectResult.SourceFileResults.First();\nforeach (var invocation in sourcefile.AllInvocationExpressions())\n{\n    Console.WriteLine(invocation.MethodName + \":\" + invocation.SemanticMethodSignature);\n}\n\nvar objectCreations = sourcefile.AllObjectCreationExpressions();\nvar allClasses = sourcefile.AllClasses();\nvar allMethods = sourcefile.AllMethods();\nvar allLiterals = sourcefile.AllLiterals();\n```\n\n## How to use this code?\n* Clone the Git repository.\n* Load the solution `Codelyzer.sln` using Visual Studio or Rider. \n* Create a \"Run/Debug\" Configuration for the \"Codelyzer.Analysis\" project.\n* Provide command line arguments for a solution and output path, then run the application.\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues\nfor tracking bugs and feature requests.\n\n* Send us an email to: aws-porting-assistant-support@amazon.com\n* If it turns out that you may have found a bug,\n  please open an [issue](https://github.com/aws/codelyzer/issues/new)\n  \n## Contributing\n\nWe welcome community contributions and pull requests. See\n[CONTRIBUTING](./CONTRIBUTING.md) for information on how to set up a development\nenvironment and submit code.\n\n## Thank you\n* [The .NET Compiler Platform (\"Roslyn\")](https://github.com/dotnet/roslyn) - Roslyn provides open-source C# and Visual Basic compilers with rich code analysis APIs.   \n* [Buildalyzer](https://github.com/daveaglick/Buildalyzer) - Buildalyzer lets you run MSBuild from your own code and returns information about the project.  \n* [THIRD-PARTY](./THIRD-PARTY) - This project would not be possible without additional dependencies listed in [THIRD-PARTY](./THIRD-PARTY).\n  \n# License\n\nLibraries in this repository are licensed under the Apache 2.0 License.\n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.    \n\n\n\n", "release_dates": []}, {"name": "codewhisperer-command-line-discussions", "description": null, "language": null, "license": null, "readme": "<div align=\"center\">\n    <img width=\"150\" src=\"assets/icon.png\" />\n    <h2 style=\"margin-top: 0px\">CodeWhisperer for command line - Discussions repo</h1>\n</div>\n\n\n[CodeWhisperer for command line](https://aws.amazon.com/codewhisperer/resources/) makes the command line easier for beginners and more productive for advanced engineers. \n\nThis repo hosts all **[discussions](https://github.com/aws/codewhisperer-command-line-discussions/discussions)** for bug reports, feature requests, announcements, and Q&A.\n\n<div align=\"center\">\n    <img width=\"450\" src=\"assets/cli-completions.gif\" />\n</div>\n\n\n## \u26a1\ufe0f Installation\n\nDownload for macOS [here](https://desktop-release.codewhisperer.us-east-1.amazonaws.com/latest/CodeWhisperer.dmg)\n\n\n## Environment support\n* Operating systems: macOS\n* Shells: bash, zsh, fish\n* Terminal emulators: iTerm2, macOS terminal, Hyper, Alacritty, Kitty, wezTerm\n* IDEs: VS Code terminal, Jetbrains terminals (except Fleet)\n* CLIs: 500+ of the most popular CLIs such as git, aws, docker, npm, yarn\n\n\n## Bugs?\n* Run `cw doctor` to self-heal common issues\n* Run `cw restart` to see if restarting the app helps\n* Run `cw issue` to create a new Discussion in this repo with pre-populated diagnostic information\n\n\n## Feature request or questions?\nCreate a new Discussion [here](https://github.com/aws/codewhisperer-command-line-discussions/discussions/new/choose) \n\n\n## Docs\n* [User guide](https://docs.aws.amazon.com/codewhisperer/latest/userguide/command-line.html)\n* [Completion spec contribution](https://fig.io/docs)\n\n## Other\nWant AI coding suggestions in your IDE? Checkout https://aws.amazon.com/codewhisperer\n", "release_dates": []}, {"name": "common-io-basic", "description": "A basic set of I/O libraries for pins and serial communications on embedded devices.", "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "## Common IO - basic\n\nThis repository provides a basic set of I/O libraries (**HAL**, **H**ardware **A**bstraction **L**ayer) for pins and serial communications on embedded devices.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the MIT License. See [LICENSE](LICENSE.md) for the license body.\n", "release_dates": ["2023-04-24T17:38:59Z"]}, {"name": "common-io-ble", "description": null, "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "## Common IO - BLE\n\nCommon IO - BLE provides an abstraction layer which offer a common set of APIs to control the device, perform GAP and GATT operations.\n\n**bt_hal_manager.h** : Contains the interface to control the Bluetooth device, perform device discovery operations and other connectivity related tasks.\n\n**bt_hal_manager_adapter_ble.h** : Contains the interface for the GAP API functions that are specific to BLE.\n\n**bt_hal_manager_adapter_classic.h** : Contains the interface to control BT classic functionalities of a device.\n\n**bt_hal_gatt_server.h** : Contains the interface to use Bluetooth GATT server feature.\n\n**bt_hal_gatt_client.h** : Contains the interface to use Bluetooth GATT client feature.\n\n**bt_hal_avsrc_profile.h** : Contains the interface for A2DP Source profile for the local device.\n\n## Contributing\n\nSee [CONTRIBUTING.md](./.github/CONTRIBUTING.md) for information on contributing.\n\n## Security\n\nSee [SECURITY](SECURITY.md) for more information.\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.\n\n", "release_dates": ["2023-04-25T22:14:34Z", "2023-11-16T20:34:55Z"]}, {"name": "connect-rtc-js", "description": "Provide softphone support to AmazonConnect customers when they choose to directly integrate with our API and not using our web app.", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "[![Build Status](https://travis-ci.org/aws/connect-rtc-js.svg)](https://travis-ci.org/aws/connect-rtc-js)\n\n# Amazon Connect connect-rtc-js #\n**connect-rtc.js** provides softphone support to AmazonConnect customers when they choose to directly integrate with AmazonConnect API and not using AmazonConnect web application.\nIt implements Amazon Connect WebRTC signaling protocol and integrates with browser WebRTC APIs to provide a simple contact session interface which can be integrated with [Amazon Connect StreamJS](https://github.com/aws/amazon-connect-streams) seemlessly.\n\n## Usage ##\n### Prebuilt releases\nIn the [gh-pages branch](https://github.com/aws/connect-rtc-js/tree/gh-pages) prebuilt ready to use files can be downloaded/linked directly.\n### Build your own\n1. Install latest LTS version of [NodeJS](https://nodejs.org)\n2. Install [Grunt](https://gruntjs.com/)\n3. `git clone https://github.com/aws/connect-rtc-js.git`\n4. `cd connect-rtc-js`\n5. `npm install`\n5. To build:\n    1. `grunt`\n    2. Find build artifacts in **out** directory\n6. To run unit tests:\n    1. `npm test`\n7. To run demo page:\n    1. `grunt demo`\n    2. Open the URL printed out by connect task, it looks like \"Started connect web server on <https://localhost:9943>\"\n    3. Click **demo** folder\n\n## Amazon Connect StreamJS integration ##\nIn a typical [amazon-connect-streams](https://github.com/aws/amazon-connect-streams) integration, connect-rtc-js is not required on parent page. Softphone call handling is done by embedded CCP.\n\nHowever the following steps could further customize softphone experience.\n1. Load connect-rtc-js along with amazon-connect-streams on parent page\n2. Following [amazon-connect-streams instructions](https://github.com/aws/amazon-connect-streams/blob/master/README.md) to initialize CCP\n3. Replace the softphone parameter (within the second parameter of ***connect.core.initCCP()***) with\n    `allowFramedSoftphone: false`\n    This would stop embedded CCP from handling softphone call\n4. Add this line after initCCP\n    `connect.core.initSoftphoneManager({allowFramedSoftphone: true});`\n    This would allow your page to handle softphone call with the connect-rtc-js loaded by your page. ***allowFramedSoftphone*** is necessary if your page also lives in a frame, otherwise you can remove that parameter.\n5. Add this HTML element to your web page\n    `<audio id=\"remote-audio\" autoplay></audio>`\n    amazon-connect-streams library will look for this element and inject it into connect-rtc-js so that connect-rtc-js can play downstream audio through this element.\n6. Customize it (some ideas below)\n    * Customize audio device for `remote-audio` element\n    * Look at all the documented APIs in [RtcSession](https://github.com/aws/connect-rtc-js/blob/master/src/js/rtc_session.js) class, modify [softphone.js](https://github.com/aws/amazon-connect-streams/blob/master/src/softphone.js) as you need\n    * Revert step 4, add your own glue layer between amazon-connect-streams and connect-rtc-js (use [softphone.js](https://github.com/aws/amazon-connect-streams/blob/master/src/softphone.js) as a template)\n", "release_dates": ["2024-01-19T17:52:30Z", "2024-01-08T22:23:32Z", "2023-07-10T21:54:35Z", "2023-04-04T14:49:15Z", "2023-02-09T20:48:44Z", "2022-03-24T00:07:08Z", "2021-09-28T19:48:55Z", "2021-04-22T22:48:20Z", "2021-03-16T00:50:39Z", "2021-03-12T23:22:22Z", "2021-01-29T18:31:26Z", "2020-07-16T23:18:44Z", "2020-03-09T19:51:35Z", "2019-10-10T19:12:33Z", "2019-08-28T22:30:28Z", "2019-01-15T23:37:59Z", "2018-07-02T19:16:32Z", "2018-05-08T00:44:47Z", "2017-08-23T22:16:35Z", "2017-07-17T20:55:34Z", "2017-05-12T22:19:08Z", "2017-05-12T21:36:15Z"]}, {"name": "constructs", "description": "Define composable configuration models through code", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Constructs\n\n> Software-defined persistent state\n\n[![Release](https://github.com/aws/constructs/actions/workflows/release.yml/badge.svg)](https://github.com/aws/constructs/actions/workflows/release.yml)\n[![npm version](https://badge.fury.io/js/constructs.svg)](https://badge.fury.io/js/constructs)\n[![PyPI version](https://badge.fury.io/py/constructs.svg)](https://badge.fury.io/py/constructs)\n[![NuGet version](https://badge.fury.io/nu/Constructs.svg)](https://badge.fury.io/nu/Constructs)\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/software.constructs/constructs/badge.svg?style=plastic)](https://maven-badges.herokuapp.com/maven-central/software.constructs/constructs)\n\n## What are constructs?\n\nConstructs are classes which define a \"piece of system state\". Constructs can be composed together to form higher-level building blocks which represent more complex state.\n\nConstructs are often used to represent the _desired state_ of cloud applications. For example, in the AWS CDK, which is used to define the desired state for AWS infrastructure using CloudFormation, the lowest-level construct represents a _resource definition_ in a CloudFormation template. These resources are composed to represent higher-level logical units of a cloud application, etc.\n\n## Contributing\n\nThis project has adopted the [Amazon Open Source Code of\nConduct](https://aws.github.io/code-of-conduct).\n\nWe welcome community contributions and pull requests. See our [contribution\nguide](./CONTRIBUTING.md) for more information on how to report issues, set up a\ndevelopment environment and submit code.\n\n## License\n\nThis project is distributed under the [Apache License, Version 2.0](./LICENSE).\n", "release_dates": ["2023-10-07T12:43:13Z", "2023-08-29T15:09:20Z", "2023-07-05T00:10:27Z", "2023-07-05T00:19:50Z", "2023-07-04T00:10:39Z", "2023-07-04T00:20:17Z", "2023-07-03T00:10:13Z", "2023-07-03T00:20:16Z", "2023-07-02T00:10:16Z", "2023-07-02T00:21:54Z", "2023-07-01T00:10:12Z", "2023-07-01T00:21:04Z", "2023-06-30T00:10:39Z", "2023-06-30T00:19:26Z", "2023-06-29T00:10:50Z", "2023-06-29T00:20:27Z", "2023-06-28T00:10:15Z", "2023-06-28T00:18:41Z", "2023-06-27T00:10:05Z", "2023-06-27T00:19:53Z", "2023-06-26T00:10:27Z", "2023-06-26T00:20:32Z", "2023-06-25T00:11:01Z", "2023-06-25T00:21:43Z", "2023-06-24T00:10:09Z", "2023-06-24T00:19:24Z", "2023-06-23T00:10:12Z", "2023-06-23T00:20:44Z", "2023-06-22T00:09:08Z", "2023-06-22T00:17:16Z"]}, {"name": "constructs-go", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## constructs-go\n\nGolang bindings for [constructs](https://github.com/aws/constructs)\n\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2021-02-02T12:08:05Z"]}, {"name": "containers-roadmap", "description": "This is the public roadmap for AWS container services (ECS, ECR, Fargate, and EKS). ", "language": "Shell", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## Containers Roadmap\n\nThis is the public roadmap for AWS container services (ECS, ECR, Fargate, and EKS).\n\n## Introduction\nThis is the public roadmap for AWS Container services.\nKnowing about our upcoming products and priorities helps our customers plan. This repository contains information about what we are working on and allows all AWS customers to give direct feedback.\n\n[See the roadmap \u00bb](https://github.com/aws/containers-roadmap/projects/1)\n\n**Other AWS Public Roadmaps**\n* [AWS App Mesh](https://github.com/aws/aws-app-mesh-roadmap)\n* [AWS Proton](https://github.com/aws/aws-proton-public-roadmap)\n* [CloudFormation coverage](https://github.com/aws-cloudformation/aws-cloudformation-coverage-roadmap)\n* [AWS Elastic Beanstalk](https://github.com/aws/elastic-beanstalk-roadmap)\n* [Amazon EC2 Spot Instances integrations](https://github.com/aws/ec2-spot-instances-integrations-roadmap)\n* [AWS Controllers for Kubernetes (ACK)](https://github.com/aws-controllers-k8s/community/projects/1)\n* [AWS CDK](https://github.com/orgs/aws/projects/7)\n* [AWS App Runner](https://github.com/aws/apprunner-roadmap/projects/1)\n\n## Developer Preview Programs\nWe now have information for developer preview programs within this repository. Issues tagged [Developer Preview](https://github.com/aws/containers-roadmap/labels/Developer%20Preview) on the public roadmap are active preview programs.\n\n**Current Programs**\n* There's more to come! Stay tuned!\n\n**Past Programs**\n* EKS on ARM - *[EKS on AWS Graviton2 is generally available](https://aws.amazon.com/blogs/containers/eks-on-graviton-generally-available/)*\n* ECS Multiple Target Groups - *[Support for multiple target groups on ECS is generally available](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/register-multiple-targetgroups.html)*\n* Firelens - *[Support for custom log routing is generally available](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_firelens.html)*\n* EKS Windows - *[Support for Windows on EKS is generally available](https://docs.aws.amazon.com/eks/latest/userguide/windows-support.html)*\n\n## Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n\n## FAQs\n**Q: Why did you build this?**\n\nA: We know that our customers are making decisions and plans based on what we are developing, and we want to provide our customers the insights they need to plan.\n\n**Q: Why are there no dates on your roadmap?**\n\nA: Because job zero is security and operational stability, we can't provide specific target dates for features. The roadmap is subject to change at any time, and roadmap issues in this repository do not guarantee a feature will be launched as proposed.\n\n**Q: What do the roadmap categories mean?**\n* *Just shipped* - obvious, right?\n* *Coming soon* - coming up.  Think a couple of months out, give or take.\n* *We're working on it* - in progress, but further out.  We might still be working through the implementation details, or scoping stuff out.\n* *Researching* - We're thinking about it. This might mean we're still designing, or thinking through how this might work. This is a great phase to send how you want to see something implemented!  We'd love to see your usecase or design ideas here.\n\n**Q: Is everything on the roadmap?**\n\nA: The majority of our development work for Amazon ECS, Fargate, ECR, EKS and other AWS-sponsored OSS projects are included on this roadmap. Of course, there will be technologies we are very excited about that we are going to launch without notice to surprise and delight our customers.\n\n**Q: How can I provide feedback or ask for more information?**\n\nA: Please open an issue!\n\n**Q: How can I request a feature be added to the roadmap?**\n\nA: Please open an issue!  You can read about how to contribute [here](/CONTRIBUTING.md). Community submitted issues will be tagged \"Proposed\" and will be reviewed by the team.\n\n**Q: Will you accept a pull request?**\n\nA: We haven't worked out how pull requests should work for a public roadmap page, but we will take all PRs very seriously and review for inclusion. Read about [contributing](/CONTRIBUTING.md).\n\n## License\n\nThis library is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License.\n\nTo learn more about the services, head here: http://aws.amazon.com/containers\n", "release_dates": []}, {"name": "copilot-cli", "description": "The AWS Copilot CLI is a tool for developers to build, release and operate production ready containerized applications on AWS App Runner or Amazon ECS on AWS Fargate. ", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "##  <img align=\"left\" alt=\"AWS Copilot CLI\" src=\"./site/content/assets/images/copilot-logo-48-light.svg\" width=\"85\" /> AWS Copilot CLI\n###### _Build, Release and Operate Containerized Applications on AWS._ \n\n![latest version](https://img.shields.io/github/v/release/aws/copilot-cli)\n[![Join the chat at https://gitter.im/aws/copilot-cli](https://badges.gitter.im/aws/copilot-cli.svg)](https://gitter.im/aws/copilot-cli?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n* **Documentation**: [https://aws.github.io/copilot-cli/](https://aws.github.io/copilot-cli/)\n\nThe AWS Copilot CLI is a tool for developers to build, release and operate production-ready containerized applications\non AWS App Runner or Amazon ECS on AWS Fargate.\n\nUse Copilot to:\n* Deploy production-ready, scalable services on AWS from a Dockerfile in one command.\n* Add databases or inject secrets to your services.  \n* Grow from one microservice to a collection of related microservices in an application.\n* Set up test and production environments, across regions and accounts.\n* Set up CI/CD pipelines to release your services to your environments.\n* Monitor and debug your services from your terminal.\n\n<p align=\"center\">\n    <img alt=\"init\" src=\"./site/content/assets/images/init-cropped.gif\" width=\"600\"/>\n</p>\n\n## Installation\n\nTo install with homebrew:\n```sh\n$ brew install aws/tap/copilot-cli\n```\nTo install manually, we're distributing binaries from our GitHub releases:\n\n<details>\n  <summary>Instructions for installing Copilot for your platform</summary>\n\n\n| Platform | Command to install |\n|---------|---------\n| macOS | `curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-darwin && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help` |\n| Linux x86 (64-bit) | `curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help` |\n| Linux (ARM) | `curl -Lo copilot https://github.com/aws/copilot-cli/releases/latest/download/copilot-linux-arm64 && chmod +x copilot && sudo mv copilot /usr/local/bin/copilot && copilot --help` |\n| Windows | `Invoke-WebRequest -OutFile 'C:\\Program Files\\copilot.exe' https://github.com/aws/copilot-cli/releases/latest/download/copilot-windows.exe` |\n\n</details>\n\n\n## Getting started\n\nMake sure you have the AWS command line tool installed and have already run `aws configure` before you start.\n\nTo get a sample app up and running in one command, run the following:\n\n```sh\n$ git clone git@github.com:aws-samples/aws-copilot-sample-service.git demo-app\n$ cd demo-app\n$ copilot init --app demo                \\\n  --name api                             \\\n  --type 'Load Balanced Web Service'     \\\n  --dockerfile './Dockerfile'            \\\n  --deploy\n```\n\nThis will create a VPC, Application Load Balancer, an Amazon ECS Service with the sample app running on AWS Fargate.\nThis process will take around 8 minutes to complete - at which point you'll get a URL for your sample app running! \ud83d\ude80\n\n## Learning more \n\nWant to learn more about what's happening? Check out our documentation [https://aws.github.io/copilot-cli/](https://aws.github.io/copilot-cli/) for a getting started guide, learning about Copilot concepts, and a breakdown of our commands. \n\n## Feedback\n\nHave any feedback at all? \ud83d\ude4f Drop us an [issue](https://github.com/aws/copilot-cli/issues/new) or join us on [gitter](https://gitter.im/aws/copilot-cli).\n\nWe're happy to hear feedback or answer questions, so reach out, anytime!\n\n## Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues. Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or email AWS security directly at [aws-security@amazon.com](mailto:aws-security@amazon.com).\n\n## License\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": ["2024-01-30T22:53:47Z", "2024-01-17T19:46:04Z", "2023-12-13T17:46:24Z", "2023-11-09T17:44:09Z", "2023-10-06T22:08:31Z", "2023-09-01T14:02:46Z", "2023-08-30T20:56:00Z", "2023-08-02T18:13:58Z", "2023-07-19T19:31:23Z", "2023-05-25T16:53:12Z", "2023-03-28T16:34:35Z", "2023-02-21T22:50:34Z", "2023-01-17T20:24:27Z", "2022-11-29T21:17:46Z", "2022-11-01T17:29:20Z", "2022-10-11T19:39:02Z", "2022-09-27T16:57:10Z", "2022-08-31T23:25:05Z", "2022-08-17T15:29:15Z", "2022-07-19T15:55:18Z", "2022-06-13T19:59:58Z", "2022-05-13T00:47:23Z", "2022-05-09T15:41:07Z", "2022-04-12T16:02:24Z", "2022-04-06T16:41:50Z", "2022-02-08T18:48:04Z", "2022-01-26T21:36:11Z", "2021-11-23T18:39:04Z", "2021-10-28T20:02:40Z", "2021-10-04T18:00:15Z"]}, {"name": "credentials-fetcher", "description": "Credentials-fetcher is a Linux daemon that retrieves gMSA credentials from Active Directory over LDAP. It creates and refreshes kerberos tickets from gMSA credentials. Kerberos tickets can be used by containers to run apps/services that authenticate using Active Directory.", "language": "C", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Credentials Fetcher\n\n`credentials-fetcher` is a Linux daemon that retrieves gMSA credentials from Active Directory over LDAP. It creates and refreshes kerberos tickets from gMSA credentials. Kerberos tickets can be used by containers to run apps/services that authenticate using Active Directory.\n\nThis daemon works in a similar way as ccg.exe and the gMSA plugin in Windows as described in - https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/manage-serviceaccounts#gmsa-architecture-and-improvements\n\n### How to install and run\n\nOn [Fedora 36](_https://alt.fedoraproject.org/cloud/_) and similar distributions, the binary RPM can be installed as\n`sudo dnf install credentials-fetcher`.\nYou can also use yum if dnf is not present.\nThe daemon can be started using `sudo systemctl start credentials-fetcher`.\n\nOn Enterprise Linux 9 ( RHEL | CentOS | AlmaLinux ), the binary can be installed from EPEL. To add EPEL, see the [EPEL Quickstart](_https://docs.fedoraproject.org/en-US/epel/#_quickstart_).\nOnce EPEL is enabled, install credentials-fetcher with \n`sudo dnf install credentials-fetcher`.\n\nFor other linux distributions, the daemon binary needs to be built from source code.\n\n## Development\n\n### Prerequisites\n\n- Active Directory server ( Windows Server )\n- Linux instances or hosts that are domain-joined to Active Directory\n- gMSA account(s) in Active Directory - Follow instructions provided to create service accounts - https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/manage-serviceaccounts\n- Required packages as mentioned in RPM spec file.\n- Create username ec2-user or modify the systemd unit file.\n\n#### Create credentialspec associated with gMSA account:\n\n- Create a domain joined windows instance\n- Install powershell module - \"Install-Module CredentialSpec\"\n- New-CredentialSpec -AccountName WebApp01 // Replace 'WebApp01' with your own gMSA\n- You will find the credentialspec in the directory\n  'C:\\\\Program Data\\\\Docker\\\\Credentialspecs\\\\WebApp01_CredSpec.json'\n\n#### Standalone mode\n\nTo start a local dev environment from scratch:\n\n```\n* Clone the Git repository.\n* cd credentials-fetcher && mkdir build\n* cd build && cmake ../ && make -j\n* ./credentials-fetcher to start the program in non-daemon mode.\n```\n\n#### Testing\n\nTo communicate with the daemon over gRPC, install grpc-cli. For example\n`sudo yum install grpc-cli`\n\n##### AddKerberosLease API:\nNote: APIs use unix domain socket\n```\nInvoke the AddkerberosLease API with the credentialsspec input as shown:\ngrpc_cli call {unix_domain_socket} AddKerberosLease \"credspec_contents: '{credentialspec}'\"\n\nSample:\ngrpc_cli call unix:/var/credentials-fetcher/socket/credentials_fetcher.sock\nAddKerberosLease \"credspec_contents: '{\\\"CmsPlugins\\\":[\\\"ActiveDirectory\\\"],\\\"DomainJoinConfig\\\":{\\\"Sid\\\":\\\"S-1-5-21-4217655605-3681839426-3493040985\\\",\n\\\"MachineAccountName\\\":\\\"WebApp01\\\",\\\"Guid\\\":\\\"af602f85-d754-4eea-9fa8-fd76810485f1\\\",\\\"DnsTreeName\\\":\\\"contoso.com\\\",\n\\\"DnsName\\\":\\\"contoso.com\\\",\\\"NetBiosName\\\":\\\"contoso\\\"},\\\"ActiveDirectoryConfig\\\":{\\\"GroupManagedServiceAccounts\\\":[{\\\"Name\\\":\\\"WebApp01\\\",\\\"Scope\\\":\\\"contoso.com\\\"}\n,{\\\"Name\\\":\\\"WebApp01\\\",\\\"Scope\\\":\\\"contoso\\\"}]}}'\"\n\n* Response:\n  lease_id - unique identifier associated to the request\n  created_kerberos_file_paths - Paths associated to the Kerberos tickets created corresponding to the gMSA accounts\n```\n\n##### DeleteKerberosLease API:\n\n```\nInvoke the Delete kerberosLease API with lease id input as shown:\ngrpc_cli call {unix_domain_socket} DeleteKerberosLease \"lease_id: '{lease_id}'\"\n\nSample:\ngrpc_cli call unix:/var/credentials-fetcher/socket/credentials_fetcher.sock DeleteKerberosLease \"lease_id: '${response_lease_id_from_add_kerberos_lease}'\"\n\n* Response:\n    lease_id - unique identifier associated to the request\n    deleted_kerberos_file_paths - Paths associated to the Kerberos tickets deleted corresponding to the gMSA accounts\n\n```\n\n### Logging\n\nLogs about request/response to the daemon and any failures.\n\n```\njournalctl -u credentials-fetcher\n```\n\n#### Default environment variables\n\n| Environment Key             | Examples values                    | Description                                                                                  |\n| :-------------------------- | ---------------------------------- | :------------------------------------------------------------------------------------------- |\n| `CF_KRB_DIR`                | '/var/credentials-fetcher/krbdir'  | *(Default)* Dir path for storing the kerberos tickets                                        |\n| `CF_UNIX_DOMAIN_SOCKET_DIR` | '/var/credentials-fetcher/socket'  | *(Default)* Dir path for the domain socker for gRPC communication 'credentials_fetcher.sock' |\n| `CF_LOGGING_DIR`            | '/var/credentials-fetcher/logging' | *(Default)* Dir Path for log                                                                 |\n| `CF_TEST_DOMAIN_NAME`       | 'contoso.com'                      | Test domain name                                                                             |\n| `CF_TEST_GMSA_ACCOUNT`      | 'webapp01'                         | Test gMSA account name                                                                       |\n\n\n#### Runtime environment variables\n\n| Environment Variable        | Examples values                          | Description                                                                                  |\n| :-------------------------- | ---------------------------------------- | :------------------------------------------------------------------------------------------- |\n| `CF_CRED_SPEC_FILE`         | '/var/credentials-fetcher/my-credspec.json' | Path to a credential spec file used as input. (Lease id default: credspec) |\n|                             | '/var/credentials-fetcher/my-credspec.json:myLeaseId' | An optional lease id specified after a colon\n| `CF_GMSA_OU`                | 'CN=Managed Service Accounts' | Component of GMSA distinguished name (see docs/cf_gmsa_ou.md) |\n\n## Compatibility\n\nRunning the Credentials-fetcher outside of Linux distributions is not\nsupported.\n\n## Contributing\n\nContributions and feedback are welcome! Proposals and pull requests will be considered and responded to. For more\ninformation, see the [CONTRIBUTING.md](https://github.com/aws/credentials-fetcher/blob/master/CONTRIBUTING.md) file.\nIf you have a bug/and issue around the behavior of the credentials-fetcher,\nplease open it here.\n\nAmazon Web Services does not currently provide support for modified copies of this software.\n\n## Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n## License\n\nThe Credentials Fetcher is licensed under the Apache 2.0 License.\nSee [LICENSE](LICENSE) and [NOTICE](NOTICE) for more information.\n", "release_dates": ["2024-02-02T22:07:16Z", "2024-01-24T19:59:03Z", "2024-01-04T19:51:30Z", "2023-12-13T02:16:26Z", "2023-12-05T21:46:22Z", "2023-11-30T06:04:52Z", "2023-05-17T05:30:55Z", "2022-10-27T17:45:22Z", "2022-10-17T02:35:36Z", "2022-08-29T20:26:14Z", "2022-08-29T19:37:45Z"]}, {"name": "crypto-tools", "description": "AWS Crypto Tools Team", "language": "Python", "license": {"key": "cc-by-sa-4.0", "name": "Creative Commons Attribution Share Alike 4.0 International", "spdx_id": "CC-BY-SA-4.0", "url": "https://api.github.com/licenses/cc-by-sa-4.0", "node_id": "MDc6TGljZW5zZTI2"}, "readme": "[//]: # \"Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\"\n[//]: # \"SPDX-License-Identifier: CC-BY-SA-4.0\"\n\n# AWS Crypto Tools\n\nWe build open source cryptographic libraries that are easy to use, and hard to misuse.\nCryptography is hard to do safely and correctly.\nOur libraries are designed to help you do cryptography right, even without special expertise.\nOur client-side encryption libraries help you to protect your sensitive data\nat its source using secure cryptographic algorithms, envelope encryption, and signing.\n\n[Security issue notifications](./CONTRIBUTING.md#security-issue-notifications)\n\n## AWS Encryption SDK\n\nThe AWS Encryption SDK is a client-side encryption library\ndesigned to make it easy for you to encrypt and decrypt data using industry standards and best practices.\nIt enables you to focus on the core functionality of your application,\nrather than on how to best encrypt and decrypt your data.\n\n- Developer Guide\n  [[source]](https://github.com/awsdocs/aws-encryption-sdk-docs)\n  [[hosted]](https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html)\n- Specification\n  [[source]](https://github.com/awslabs/aws-encryption-sdk-specification)\n- C\n  [[source]](https://github.com/aws/aws-encryption-sdk-c)\n  [[API docs]](https://aws.github.io/aws-encryption-sdk-c/html)\n- Java\n  [[source]](https://github.com/aws/aws-encryption-sdk-java)\n  [[API docs]](https://aws.github.io/aws-encryption-sdk-java/javadoc)\n- Javascript\n  [[source]](https://github.com/aws/aws-encryption-sdk-javascript)\n- Python\n  [[source]](https://github.com/aws/aws-encryption-sdk-python)\n  [[API docs]](https://aws-encryption-sdk-python.readthedocs.io)\n\n### AWS Encryption CLI\n\nYou can use the AWS Encryption CLI to encrypt and decrypt files and directories with the AWS Encryption SDK.\n\n- AWS Encryption CLI\n  [[source]](https://github.com/aws/aws-encryption-sdk-cli)\n  [[API docs]](https://aws-encryption-sdk-cli.readthedocs.io)\n\n## AWS DynamoDB Encryption Client\n\nThe Amazon DynamoDB Encryption Client is a software library\nthat helps you to protect your table data before you send it to Amazon DynamoDB.\n\n- Developer Guide\n  [[source]](https://github.com/awsdocs/aws-dynamodb-encryption-docs)\n  [[hosted]](https://docs.aws.amazon.com/dynamodb-encryption-client/latest/devguide/what-is-ddb-encrypt.html)\n- Java\n  [[source]](https://github.com/aws/aws-dynamodb-encryption-java)\n  [[API docs]](https://aws.github.io/aws-dynamodb-encryption-java/javadoc)\n- Python\n  [[source]](https://github.com/aws/aws-dynamodb-encryption-python)\n  [[API docs]](https://aws-dynamodb-encryption-python.readthedocs.io)\n\n## Amazon Corretto Crypto Provider (ACCP)\n\nACCP implements the standard Java Cryptography Architecture (JCA) interfaces\nand replaces the default Java cryptographic implementations\nwith those provided by `libcrypto` from the OpenSSL project.\nACCP allows you to take full advantage of\nassembly-level and CPU-level performance tuning,\nto gain significant cost reduction, latency reduction, and higher throughput.\n\n- ACCP\n  [[source]](https://github.com/corretto/amazon-corretto-crypto-provider)\n\n## Other products\n\n- Base64IO\n  [[source]](https://github.com/aws/base64io-python)\n  [[API docs]](https://base64io-python.readthedocs.io):\n  A stream implementation for Python that provides transparent base64 encoding and decoding of an underlying stream.\n- AWS SDK for JS v3 Crypto Helpers\n  [[source]](https://github.com/aws/aws-sdk-js-crypto-helpers):\n  AWS Cryptographic helpers for Javascript and Node.js\n\n## Experimental\n\n- PKCS11 runners for Project Wycheproof\n  [[source]](https://github.com/awslabs/pkcs11-runners-for-project-wycheproof)\n- Pipeformer\n  [[source]](https://github.com/awslabs/pipeformer)\n\n# Documentation\n\n- [AWS Crypto Tools Documentation](https://docs.aws.amazon.com/aws-crypto-tools)\n\n# Need Help?\n\nTo get in touch with us, open an issue in the relevant repository and we'll help out.\n\n# Security Issue Notifications\n\nIf you identify a potential security issue in one of these projects please immediately notify AWS/Amazon Security\nvia our [vulnerability reporting page](https://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public Github issue.\n\n# Contributing\n\nWe welcome issues and PRs on any of our repositories.\n\n# Jobs\n\nInterested in making the world more secure?\n[We're hiring](https://www.amazon.jobs/en/search?cities[]=Seattle%2C%20Washington%2C%20USA&business_category[]=amazon-web-services&base_query=crypto%20tools)!\n", "release_dates": []}, {"name": "cta", "description": "Code translation assistance, a feature of Porting Assistant for .NET, helps users automate some aspects of their porting experience using a set of predefined rules and actions.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Code translation in Porting Assistant for .NET\n![Build Test](https://github.com/aws/cta/workflows/Build%20Test/badge.svg)\n\nPorting Assistant for .NET is an analysis tool that scans .NET Framework applications and generates a .NET Core compatibility assessment, helping customers port their applications to Linux faster.\n\nPorting Assistant for .NET scans .NET Framework applications to identify incompatibilities with .NET Core, finds known replacements, generates detailed compatibility assessment reports and makes changes to the source code to fix common issues encountered while porting an application from .NET Framework to .NET Core. This reduces the manual effort involved in modernizing applications to Linux.\n\nThe code translation package contains the source of the engine that allow developers to define recommendations to fix incompatible patterns in the source code when porting from .NET Framework to .NET Core or .NET 5.\n\nFor more information about Porting Assistant and to try the tool, please refer to the documentation: https://aws.amazon.com/porting-assistant-dotnet/\n\n# Introduction\n\nCode translation feature helps users automate some of their porting experience when converting their applications from .NET Framework to .NET Core. It identifies common issues such as usage of Entity Framework or ASP.NET MVC and makes changes to the source code to reduce the amount of work needed to port applications from .NET Framework to .NET Core or .NET 5. \n\nIn order to perform code translations, code translation relies on a set of predefined rules and actions. The rules files define patterns that the code translation package searches for in the user code, the tool then performs actions specified in the rules files to fix the issues that were identified. Below we have highlighted some examples of rules that are already available. \n\n#### ASP .NET MVC:\nIf you\u2019re porting your MVC project to .NET Core or .NET 5, below are some of the steps that Porting Assistant will do:\n\n* Creates a new project file that uses the SDK for Web projects\n* Update the namespaces to use the new Microsoft.AspNetCore.Mvc\n* Update the project structure and move your static files (CSS, JavaScript, images, etc.) into the static files folder\n* Add the required template files to start your application (Program.cs and Startup.cs)\n* Archive framework files that are no longer needed (global.asax, BundleConfig.cs, etc.)\n\n#### Database Connectivity:\nWhether you\u2019re using ADO .NET or Entity Framework, Porting Assistant for .NET can help you with the porting process.\n\nADO .NET:\nPorting Assistant for .NET will add the needed packages to the csproj files. In addition, it will scan your code and automatically update any references to the framework ADO .NET namespace, System.Data.SqlClient, to the new .NET core or .NET 5 compatible package, Microsoft.Data.SqlClient\n\nEntity Framework:\nUsers who have dependencies on Entity Framework in their projects will also see their porting experience enhanced. During porting, Porting assistant will automatically:\n\n* Add the NuGet packages for EF Core\n* Update the namespaces in your code files\n* Add an OnConfiguring method to your DbContext classes to allow you to easily connect to your database\n\nIn some cases, and depending where you store your connection strings, the porting process will also migrate your connection strings to the new appsettings file in .NET Core.\n\nIn addition to the above rules, Porting Assistant will apply other rules depending on your code. We have a list of around 20 rules and 40 actions that are grouped by namespace, and are [open source](https://github.com/aws/porting-assistant-dotnet-datastore). We will continue enhancing and adding to this list, and welcome any contributions from the community.\n\n\n\n## Getting Started\n\n* Clone the Git repository.\n* Load the solution CTA.Rules.sln using Visual Studio or Rider.\n* Create a \"Run/Debug\" Configuration for the \"CTA.Rules.PortCore\" project.\n    * The project will pre-load rules for porting your framework solution to .net core if you use the argument -d true to load these rules\n* Provide command line arguments for a solution and output path, then run the application. List of arguments below:\n  * -p (project-path): Path to a project (or solution file)\n  * -s (solution-path): Path to a solution file\n  * -r (rules-input-file): Path to directory containing custom rule files to be applied\n  * -d (use-builtin-rules): Download and apply set of default rules from S3\n  * -a (assemblies-dir): Path to assembly containing custom actions specified in your rules\n  * -v (version): Version of .net core to port to (currently supports netcoreapp3.1 and net5.0)\n  * -m (mock-run): Run the rules without applying changes. Used to generate a run report before applying any changes\n  * -c (create-new): Create a new folder to output the ported solution into (folder will be created at the parent folder of the given solution or project file)\n  \n## Contributing\n\nWe welcome community contributions and pull requests. See\n[CONTRIBUTING](./CONTRIBUTING.md) for information on how to set up a development\nenvironment and submit code.\n\n## Recommendations\n\nRecommendations files are used to create mappings that can be used when porting a .net framework project to .net core. Each namespace will have its own file, and the data from the namespace file will be used when analyzing the compatibility and possible replacements for artifacts inside that namespace.\nThe recommendation file will have the following properties:\n\nProperty Name|Description\n---|---\nName|The namespace this file applies to\nPackages|The list of packages that this namespace belongs to\nVersion|The version of number of this recommendation\nRecommendations|The list of recommendations that are applicable under this namespace\n\nThe Recommendations section is where we specify the changes needed to convert to .net core. Each recommendation will have the below structure:\n\nProperty Name|Description\n---|---\nType|The node to look for in the code for this recommendation. This can be one of the following values:<br/>-\tNamespace: Looks for a namespace to match the token<br/>-\tClass: Looks for a class declaration to match the token<br/>-\tInterface: Looks for an interface to match<br/>-\tMethod: Looks for a method declaration<br/>-\tAttribute: Looks for an attribute<br/>-\tObjectCreation: Looks for an object creation expression<br/>-\tProject: Used for project level modifications (file changes, csproj modifications, etc\u2026)<br/>Name|The name of the node to look for. For example, if we\u2019re looking for a class, name can be the name of the class. For example, SelectList would be a name of class we look for<br/>\nValue|The value of the node to look for. This is usually the name, in addition to the namespace and type. For example, the value of SelectList would be System.Web.Mvc.SelectList\nKeyType|This is the type of key we\u2019re looking for. By default, this looks for the name of the node. However, there are other options to find nodes by. Below is a list of these options:<br/>-\tNamespace: Name<br/>- Class: BaseClass, ClassName, Identifier<br/>- Attribute: Name<br/>- Method: Name<br/>\nContainingType|This will be the type in which a node resides. This is an optional field that is used to find nodes only within certain objects\nRecommendedActions|This section describes the actions needed to migrate the node found using the properties above\n\n\nThe RecommendedAction section has the below properties:\nProperty Name|Description\n---|---\nSource|The source of the recommendation. This can be from the open source community (OpenSource) or Amazon (Amazon)\nPreferred|If multiple versions of an action apply to node, the first preferred version will be picked\nTargetFrameworks|This section describes what version of .net core this recommendation applies to. The supported versions are 3.1 and 5.0\nDescription|This is the description of the action.\nActions|This node defines what actions are to be taken when a node is matched\n\n## Actions\nProperty Name |\tDescription\n---|---\nName\t| The name of the action\nType\t|The type of the action (what node it runs on)\nValue\t| The parameter(s) passed to the action. It can be a string or a json object\nDescription\t| The description of the action\nActionValidation\t| An optional action validation that runs on the file after its completion. The system checks the resulting file for the Contains/NotContains attributes to confirm the success of the action\n\n\n\n### Attribute\n```javascript\n{\n   \"Name\": \"ChangeAttribute\",\n   \"Type\": \"Attribute\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"Test\"}\n}\n```\n### AttributeList\n```javascript\n{\n   \"Name\": \"AddComment\",\n   \"Type\": \"AttributeList\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n```\n### Class\n```javascript\n{\n   \"Name\": \"RemoveBaseClass\",\n   \"Type\": \"Class\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"AddBaseClass\",\n   \"Type\": \"Class\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"ChangeName\",\n   \"Type\": \"Class\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"RemoveAttribute\",\n   \"Type\": \"Class\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"AddAttribute\",\n   \"Type\": \"Class\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"AddComment\",\n   \"Type\": \"Class\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"AddMethod\",\n   \"Type\": \"Class\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"RemoveMethod\",\n   \"Type\": \"Class\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"RenameClass\",\n   \"Type\": \"Class\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n```\n### CompilationUnit\n```javascript\n{\n   \"Name\": \"AddDirective\",\n   \"Type\": \"Using\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"RemoveDirective\",\n   \"Type\": \"Using\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n```\n### IdentifierName\n```javascript\n{\n   \"Name\": \"ReplaceIdentifier\",\n   \"Type\": \"Identifier\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"ReplaceIdentifierInsideClass\",\n   \"Type\": \"Identifier\",\n   \"Value\": {\"identifier\": \"\",\"ClassFullKey\": \"\"}, //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n```\n### Interface\n```javascript\n{\n   \"Name\": \"ChangeName\",\n   \"Type\": \"Interface\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"RemoveAttribute\",\n   \"Type\": \"Interface\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"AddAttribute\",\n   \"Type\": \"Interface\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"AddComment\",\n   \"Type\": \"Interface\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"AddMethod\",\n   \"Type\": \"Interface\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"RemoveMethod\",\n   \"Type\": \"Interface\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n```\n### InvocationExpression\n```javascript\n{\n   \"Name\": \"ReplaceMethod\",\n   \"Type\": \"Method\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"AppendMethod\",\n   \"Type\": \"Method\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"AddComment\",\n   \"Type\": \"Method\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n```\n### Namespace\n```javascript\n{\n   \"Name\": \"RenameNamespace\",\n   \"Type\": \"Namespace\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n```\n### ObjectCreation\n```javascript\n{\n   \"Name\": \"ReplaceObjectInitialization\",\n   \"Type\": \"ObjectCreation\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n```\n### ProjectFile\n```javascript\n{\n   \"Name\": \"MigrateProjectFile\",\n   \"Type\": \"ProjectFile\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n```\n### ProjectLevel\n```javascript\n{\n   \"Name\": \"ArchiveFiles\",\n   \"Type\": \"Project\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"CreateNet3FolderHierarchy\",\n   \"Type\": \"Project\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"CreateNet5FolderHierarchy\",\n   \"Type\": \"Project\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n{\n   \"Name\": \"MigrateConfig\",\n   \"Type\": \"Project\",\n   \"Value\": \"\", //Parameter(s) passed to the action\n   \"Description\": \"Sample Description for the actions\",\n   \"ActionValidation\": {\"Contains\": \"\",\"NotContains\": \"\"}\n}\n```\n\n# Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n# License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "dcv-color-primitives", "description": "DCV Color Primitives Library", "language": "Rust", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# DCV Color Primitives - dcp\n\n[![Build Status](https://github.com/aws/dcv-color-primitives/actions/workflows/ci.yml/badge.svg)](https://github.com/aws/dcv-color-primitives/actions/workflows/ci.yml)\n[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE)\n[![crates.io](https://img.shields.io/crates/v/dcv-color-primitives.svg)](https://crates.io/crates/dcv-color-primitives)\n[![documentation](https://docs.rs/dcv-color-primitives/badge.svg)](https://docs.rs/dcv-color-primitives)\n[![Coverage Status](https://codecov.io/gh/aws/dcv-color-primitives/branch/master/graph/badge.svg)](https://codecov.io/gh/aws/dcv-color-primitives)\n\nDCV Color Primitives is a library to perform image color model conversion.\n\n## Design guidelines\n\n* Aware of the underlying hardware and supplemental cpu extension sets (up to avx2)\n* Support data coming from a single buffer or coming from multiple image planes\n* Support non-tightly packed data\n* Support images greater than 4GB (64 bit)\n* Support ARM (aarch64)[*]\n* Support WebAssembly[*]\n\n[*]: Supplemental cpu extension sets not yet supported.\n\n## Image format conversion\n\nThe library is currenty able to convert the following pixel formats:\n\n| Source pixel format  | Destination pixel formats  |\n| -------------------- | -------------------------- |\n| ARGB                 | I420, I444, NV12           |\n| BGR                  | I420, I444, NV12, RGB      |\n| BGRA                 | I420, I444, NV12, RGB      |\n| I420                 | BGRA, RGBA                 |\n| I444                 | BGRA, RGBA                 |\n| NV12                 | BGRA, RGB, RGBA            |\n| RGB                  | BGRA                       |\n\n### Color models\n\nThe supported color models are:\n\n* YCbCr, ITU-R Recommendation BT.601 (standard video system)\n* YCbCr, ITU-R Recommendation BT.709 (CSC systems)\n\nBoth standard range (0-235) and full range (0-255) are supported.\n\n## Requirements\n\n* Rust 1.70 and newer\n\n### Windows\n\n* Install rustup: https://www.rust-lang.org/tools/install\n\n### Linux\n\n* Install rustup (see https://forge.rust-lang.org/infra/other-installation-methods.html)\n    ```\n    curl https://sh.rustup.rs -sSf | sh\n    ```\n\nYou may require administrative privileges.\n\n## Building\n\nOpen a terminal inside the library root directory.\n\nTo build for debug experience:\n```\ncargo build\n```\n\nTo build an optimized library:\n```\ncargo build --release\n```\n\nRun unit tests:\n```\ncargo test\n```\n\nRun benchmark:\n```\ncargo bench\n```\n\nAdvanced benchmark mode.\nThere are two benchmark scripts:\n* `run-bench.ps1` for Windows\n* `run-bench.sh` for Linux and MacOS\n\nThey allow to obtain more stable results than `cargo bench`, by reducing variance due to:\n* CPU migration\n* File system caching\n* Process priority\n\nMoreover, the Linux script support hardware performance counters, e.g. it is possible to output\nconsumed CPU cycles instead of elapsed time.\n\nLinux examples:\n```\n./run-bench -c 1 # runs cargo bench and outputs CPU cycles\n./run.bench -c 1 -p \"/i420\" # runs cargo bench, output CPU cycles, filtering tests that contains '/i420'\n```\n\n## WebAssembly\n\nInstall the needed dependencies:\n```\nrustup target add wasm32-unknown-unknown\n```\n\nTo build for debug experience:\n```\ncargo build --target wasm32-unknown-unknown\n```\n\nTo test, ensure you have installed [wasm-pack](https://rustwasm.github.io/wasm-pack/installer/). Then:\n```\nwasm-pack test --node\n```\n\n## Usage\n\n### Image conversion\n\nConvert an image from bgra to nv12 (single plane) format containing yuv in BT601:\n\n```rust\nuse dcv_color_primitives as dcp;\nuse dcp::{convert_image, ColorSpace, ImageFormat, PixelFormat};\n\nfn main() {\n    const WIDTH: u32 = 640;\n    const HEIGHT: u32 = 480;\n\n    let src_data = Box::new([0u8; 4 * (WIDTH as usize) * (HEIGHT as usize)]);\n    let mut dst_data = Box::new([0u8; 3 * (WIDTH as usize) * (HEIGHT as usize) / 2]);\n\n    let src_format = ImageFormat {\n        pixel_format: PixelFormat::Bgra,\n        color_space: ColorSpace::Rgb,\n        num_planes: 1,\n    };\n\n    let dst_format = ImageFormat {\n        pixel_format: PixelFormat::Nv12,\n        color_space: ColorSpace::Bt601,\n        num_planes: 1,\n    };\n\n    convert_image(\n        WIDTH,\n        HEIGHT,\n        &src_format,\n        None,\n        &[&*src_data],\n        &dst_format,\n        None,\n        &mut [&mut *dst_data],\n    );\n}\n```\n\n### Error Handling\n\nThe library functions return a `Result` describing the operation outcome:\n\n| Result                             | Description                                                           |\n| ---------------------------------- | --------------------------------------------------------------------- |\n| `Ok(())`                           | The operation succeeded                                               |\n| `Err(ErrorKind::InvalidValue)`     | One or more parameters have invalid values for the called function    |\n| `Err(ErrorKind::InvalidOperation)` | The combination of parameters is unsupported for the called function  |\n| `Err(ErrorKind::NotEnoughData)`    | One or more buffers are not correctly sized                           |\n\nIn the following example, `result` will match `Err(ErrorKind::InvalidValue)`, because `ColorSpace::Bt709`\ncolor space is not compatible with `PixelFormat::Bgra`:\n\n```rust\nuse dcv_color_primitives as dcp;\nuse dcp::{convert_image, ColorSpace, ErrorKind, ImageFormat, PixelFormat};\n\nfn main() {\n    const WIDTH: u32 = 640;\n    const HEIGHT: u32 = 480;\n\n    let src_data = Box::new([0u8; 4 * (WIDTH as usize) * (HEIGHT as usize)]);\n    let mut dst_data = Box::new([0u8; 3 * (WIDTH as usize) * (HEIGHT as usize) / 2]);\n\n    let src_format = ImageFormat {\n        pixel_format: PixelFormat::Bgra,\n        color_space: ColorSpace::Bt709,\n        num_planes: 1,\n    };\n\n    let dst_format = ImageFormat {\n        pixel_format: PixelFormat::Nv12,\n        color_space: ColorSpace::Bt601,\n        num_planes: 1,\n    };\n\n    let status = convert_image(\n        WIDTH,\n        HEIGHT,\n        &src_format,\n        None,\n        &[&*src_data],\n        &dst_format,\n        None,\n        &mut [&mut *dst_data],\n    );\n\n    match status {\n        Err(ErrorKind::InvalidValue) => (),\n        _ => panic!(\"Expected ErrorKind::InvalidValue\"),\n    }\n}\n```\n\nEven better, you might want to propagate errors to the caller function or mix with some other error types:\n```rust\nuse dcv_color_primitives as dcp;\nuse dcp::{convert_image, ColorSpace, ImageFormat, PixelFormat};\nuse std::error;\n\nfn main() -> Result<(), Box<dyn error::Error>> {\n    const WIDTH: u32 = 640;\n    const HEIGHT: u32 = 480;\n\n    let src_data = Box::new([0u8; 4 * (WIDTH as usize) * (HEIGHT as usize)]);\n    let mut dst_data = Box::new([0u8; 3 * (WIDTH as usize) * (HEIGHT as usize) / 2]);\n\n    let src_format = ImageFormat {\n        pixel_format: PixelFormat::Bgra,\n        color_space: ColorSpace::Bt709,\n        num_planes: 1,\n    };\n\n    let dst_format = ImageFormat {\n        pixel_format: PixelFormat::Nv12,\n        color_space: ColorSpace::Bt601,\n        num_planes: 1,\n    };\n\n    convert_image(\n        WIDTH,\n        HEIGHT,\n        &src_format,\n        None,\n        &[&*src_data],\n        &dst_format,\n        None,\n        &mut [&mut *dst_data],\n    )?;\n\n    Ok(())\n}\n```\n\n### Buffer size computation\n\nSo far, buffers were sized taking into account the image pixel format and dimensions; However,\nyou can use a function to compute how many bytes are needed to store an image of a given format\nand size:\n\n```rust\nuse dcv_color_primitives as dcp;\nuse dcp::{get_buffers_size, ColorSpace, ImageFormat, PixelFormat};\nuse std::error;\n\nfn main() -> Result<(), Box<dyn error::Error>> {\n    const WIDTH: u32 = 640;\n    const HEIGHT: u32 = 480;\n    const NUM_PLANES: u32 = 1;\n\n    let format = ImageFormat {\n        pixel_format: PixelFormat::Bgra,\n        color_space: ColorSpace::Rgb,\n        num_planes: NUM_PLANES,\n    };\n\n    let sizes: &mut [usize] = &mut [0usize; NUM_PLANES as usize];\n    get_buffers_size(WIDTH, HEIGHT, &format, None, sizes)?;\n\n    let buffer: Vec<_> = vec![0u8; sizes[0]];\n\n    // Do something with buffer\n    // --snip--\n\n    Ok(())\n}\n```\n\n### Image planes\n\nIf your data is scattered in multiple buffers that are not necessarily contiguous, you can provide image planes:\n\n```rust\nuse dcv_color_primitives as dcp;\nuse dcp::{convert_image, get_buffers_size, ColorSpace, ImageFormat, PixelFormat};\nuse std::error;\n\nfn main() -> Result<(), Box<dyn error::Error>> {\n    const WIDTH: u32 = 640;\n    const HEIGHT: u32 = 480;\n    const NUM_SRC_PLANES: u32 = 2;\n    const NUM_DST_PLANES: u32 = 1;\n\n    let src_format = ImageFormat {\n        pixel_format: PixelFormat::Nv12,\n        color_space: ColorSpace::Bt709,\n        num_planes: NUM_SRC_PLANES,\n    };\n\n    let src_sizes: &mut [usize] = &mut [0usize; NUM_SRC_PLANES as usize];\n    get_buffers_size(WIDTH, HEIGHT, &src_format, None, src_sizes)?;\n\n    let src_y: Vec<_> = vec![0u8; src_sizes[0]];\n    let src_uv: Vec<_> = vec![0u8; src_sizes[1]];\n\n    let dst_format = ImageFormat {\n        pixel_format: PixelFormat::Bgra,\n        color_space: ColorSpace::Rgb,\n        num_planes: NUM_DST_PLANES,\n    };\n\n    let dst_sizes: &mut [usize] = &mut [0usize; NUM_DST_PLANES as usize];\n    get_buffers_size(WIDTH, HEIGHT, &dst_format, None, dst_sizes)?;\n\n    let mut dst_rgba: Vec<_> = vec![0u8; dst_sizes[0]];\n\n    convert_image(\n        WIDTH,\n        HEIGHT,\n        &src_format,\n        None,\n        &[&src_y[..], &src_uv[..]],\n        &dst_format,\n        None,\n        &mut [&mut dst_rgba[..]],\n    )?;\n\n    Ok(())\n}\n```\n\n### Stride support\n\nTo take into account data which is not tightly packed, you can provide image strides:\n\n```rust\nuse dcv_color_primitives as dcp;\nuse dcp::{convert_image, get_buffers_size, ColorSpace, ImageFormat, PixelFormat};\nuse std::error;\n\nfn main() -> Result<(), Box<dyn error::Error>> {\n    const WIDTH: u32 = 640;\n    const HEIGHT: u32 = 480;\n    const NUM_SRC_PLANES: u32 = 1;\n    const NUM_DST_PLANES: u32 = 2;\n    const RGB_STRIDE: usize = 4 * (((3 * (WIDTH as usize)) + 3) / 4);\n\n    let src_format = ImageFormat {\n        pixel_format: PixelFormat::Bgr,\n        color_space: ColorSpace::Rgb,\n        num_planes: NUM_SRC_PLANES,\n    };\n\n    let src_strides: &[usize] = &[RGB_STRIDE];\n\n    let src_sizes: &mut [usize] = &mut [0usize; NUM_SRC_PLANES as usize];\n    get_buffers_size(WIDTH, HEIGHT, &src_format, Some(src_strides), src_sizes)?;\n\n    let src_rgba: Vec<_> = vec![0u8; src_sizes[0]];\n\n    let dst_format = ImageFormat {\n        pixel_format: PixelFormat::Nv12,\n        color_space: ColorSpace::Bt709,\n        num_planes: NUM_DST_PLANES,\n    };\n\n    let dst_sizes: &mut [usize] = &mut [0usize; NUM_DST_PLANES as usize];\n    get_buffers_size(WIDTH, HEIGHT, &dst_format, None, dst_sizes)?;\n\n    let mut dst_y: Vec<_> = vec![0u8; dst_sizes[0]];\n    let mut dst_uv: Vec<_> = vec![0u8; dst_sizes[1]];\n\n    convert_image(\n        WIDTH,\n        HEIGHT,\n        &src_format,\n        Some(src_strides),\n        &[&src_rgba[..]],\n        &dst_format,\n        None,\n        &mut [&mut dst_y[..], &mut dst_uv[..]],\n    )?;\n\n    Ok(())\n}\n```\n\nSee documentation for further information.\n\n## C bindings\n\nDCV Color Primitives provides C bindings. A static library will be automatically generated for the\ndefault build.\n\nIn order to include DCV Color Primitives inside your application library, you need to:\n* Statically link to dcv_color_primitives\n* Link to ws2_32.lib, userenv.lib, bcrypt.lib and ntdll.lib, for Windows\n* Link to libdl and libm, for Linux\n\nThe API is slightly different than the rust one. Check dcv_color_primitives.h for examples and further information.\n\nA meson build system is provided in order to build the static library and install it together\nwith include file and a pkgconfig file. There are also some unit tests written in C,\nto add some coverage also for the bindings. Minimal instructions are provided below, refer to meson's\nhelp for further instructions:\n\n* **Windows**\n  Visual Studio is required. At least the following packages are required:\n  * MSBuild\n  * MSVC - C++ build tools\n  * Windows 10 SDK\n\n  Install meson, you can choose one of the following methods:\n  1. Using meson msi installer\n   * Download from https://github.com/mesonbuild/meson/releases\n   * Install both Meson and Ninja\n  2. Install meson through pip\n   * Download and install python3: https://www.python.org/downloads/\n   * Install meson and ninja:\n     ```\n     pip install meson ninja\n     ```\n\n  Note: Minimum required meson version is 1.0.0.\n\n  All build commands have to be issued from Native Tools Command Prompt for VS (x86 or x64 depending on what platform you want to build)\n\n* **Linux**\n  The following example is for Ubuntu:\n\n  ```\n  #install python3\n  apt install python3\n\n  #install meson. See https://mesonbuild.com/Getting-meson.html for details or if you want to install through pip.\n  apt install meson\n\n  #install ninja\n  apt install ninja-build\n  ```\n\n  You may require administrative privileges.\n\n* **Build**\n  Move inside the library root directory:\n  ```\n  cd `dcv_color_primitives_root_dir`\n  ```\n\n  Then:\n  ```\n  meson setup --buildtype release builddir\n  ninja -C builddir\n  ```\n\n* **Run the tests**\n  ```\n  cd builddir\n  meson test -t 10\n  ```\n\n  A timeout scale factor of 10 is required because some tests take longer than default\n  30 seconds to complete.\n\n* **Install**\n  ```\n  ninja -C builddir install\n  ```\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.\n\n", "release_dates": ["2023-11-03T16:30:55Z", "2023-06-06T12:51:45Z", "2023-03-07T20:04:32Z", "2023-02-27T16:34:53Z", "2022-06-28T08:39:24Z", "2022-05-17T07:36:04Z", "2022-04-22T14:40:53Z", "2022-01-18T13:56:33Z", "2021-12-17T08:58:29Z", "2021-10-20T09:30:36Z", "2021-02-24T16:46:08Z", "2020-10-29T08:42:09Z", "2020-10-22T15:24:34Z", "2020-08-13T10:35:26Z", "2020-07-31T09:01:32Z", "2020-07-22T13:14:11Z", "2020-07-22T07:56:48Z", "2020-06-11T17:26:42Z", "2020-05-20T10:21:12Z", "2020-04-01T07:05:13Z", "2020-03-25T09:20:36Z", "2020-03-24T08:37:40Z", "2020-03-17T16:05:57Z", "2020-02-28T11:31:10Z", "2020-02-19T14:24:42Z"]}, {"name": "dcv-gnome-shell-extension", "description": "A GNOME Shell extension to provide functionalities required by NICE DCV", "language": "JavaScript", "license": {"key": "gpl-2.0", "name": "GNU General Public License v2.0", "spdx_id": "GPL-2.0", "url": "https://api.github.com/licenses/gpl-2.0", "node_id": "MDc6TGljZW5zZTg="}, "readme": "## dcv-gnome-shell-extension\n\nA GNOME Shell extension to provide functionalities required by NICE DCV.\n\nNICE DCV is a high-performance remote display protocol that provides customers with\na secure way to deliver remote desktops and application streaming from any cloud or\ndata center to any device, over varying network conditions.\n\nThis extension provides the following features:\n\n - Single Sign On\n\n### Disclaimer\n\nThis is still work in progress and should not be used with the current version of NICE DCV.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the GPLv2 License.\n\n", "release_dates": ["2022-08-04T15:59:51Z", "2022-07-26T13:29:24Z", "2022-06-28T15:32:45Z"]}, {"name": "deep-learning-containers", "description": "AWS Deep Learning Containers (DLCs) are a set of Docker images for training and serving models in TensorFlow, TensorFlow 2, PyTorch, and MXNet.", "language": "Python", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## AWS Deep Learning Containers\n\nAWS [Deep Learning Containers (DLCs)](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/what-is-dlc.html) \nare a set of Docker images for training and serving models in TensorFlow, TensorFlow 2, PyTorch, and MXNet. \nDeep Learning Containers provide optimized environments with TensorFlow and MXNet, Nvidia CUDA (for GPU instances), and \nIntel MKL (for CPU instances) libraries and \nare available in the Amazon Elastic Container Registry (Amazon ECR). \n\nThe AWS DLCs are used in Amazon SageMaker as the default vehicles for your SageMaker jobs such as training, inference, \ntransforms etc. They've been tested for machine\nlearning workloads on Amazon EC2, Amazon ECS and Amazon EKS services as well.\n\nFor the list of available DLC images, see [Available Deep Learning Containers Images](available_images.md). \nYou can find more information on the images available in Sagemaker [here](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html)\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n`smdistributed.dataparallel` and `smdistributed.modelparallel` are released under the [AWS Customer Agreement](https://aws.amazon.com/agreement/).\n\n## Table of Contents\n\n [Getting Started](#getting-started)\n\n [Building your Image](#building-your-image)\n\n [Running Tests Locally](#running-tests-locally)\n\n### Getting started\n\nWe describe here the setup to build and test the DLCs on the platforms Amazon SageMaker, EC2, ECS and EKS.\n\nWe take an example of building a ***MXNet GPU python3 training*** container.\n\n* Ensure you have access to an AWS account i.e. [setup](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) \nyour environment such that awscli can access your account via either an IAM user or an IAM role. We recommend an IAM role for use with AWS. \nFor the purposes of testing in your personal account, the following managed permissions should suffice: <br>\n-- [AmazonEC2ContainerRegistryFullAccess](https://console.aws.amazon.com/iam/home#policies/arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess) <br>\n-- [AmazonEC2FullAccess](https://console.aws.amazon.com/iam/home#policies/arn:aws:iam::aws:policy/AmazonEC2FullAccess) <br>\n-- [AmazonEKSClusterPolicy](https://console.aws.amazon.com/iam/home#policies/arn:aws:iam::aws:policy/AmazonEKSClusterPolicy) <br>\n-- [AmazonEKSServicePolicy](https://console.aws.amazon.com/iam/home#policies/arn:aws:iam::aws:policy/AmazonEKSServicePolicy) <br>\n-- [AmazonEKSServiceRolePolicy](https://console.aws.amazon.com/iam/home#policies/arn:aws:iam::aws:policy/AmazonEKSServiceRolePolicy) <br>\n-- [AWSServiceRoleForAmazonEKSNodegroup](https://console.aws.amazon.com/iam/home#policies/arn:aws:iam::aws:policy/AWSServiceRoleForAmazonEKSNodegroup) <br>\n-- [AmazonSageMakerFullAccess](https://console.aws.amazon.com/iam/home#policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess) <br>\n-- [AmazonS3FullAccess](https://console.aws.amazon.com/iam/home#policies/arn:aws:iam::aws:policy/AmazonS3FullAccess) <br>\n* [Create](https://docs.aws.amazon.com/cli/latest/reference/ecr/create-repository.html) an ECR repository with the name \u201cbeta-mxnet-training\u201d in the us-west-2 region\n* Ensure you have [docker](https://docs.docker.com/get-docker/) client set-up on your system - osx/ec2\n\n1. Clone the repo and set the following environment variables: \n    ```shell script\n    export ACCOUNT_ID=<YOUR_ACCOUNT_ID>\n    export REGION=us-west-2\n    export REPOSITORY_NAME=beta-mxnet-training\n    ``` \n2. Login to ECR\n    ```shell script\n    aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com\n    ``` \n3. Assuming your working directory is the cloned repo, create a virtual environment to use the repo and install requirements\n    ```shell script\n    python3 -m venv dlc\n    source dlc/bin/activate\n    pip install -r src/requirements.txt\n    ``` \n4. Perform the initial setup\n    ```shell script\n    bash src/setup.sh mxnet\n    ```\n### Building your image\n\nThe paths to the dockerfiles follow a specific pattern e.g., mxnet/training/docker/\\<version>/\\<python_version>/Dockerfile.<processor>\n\nThese paths are specified by the buildspec.yml residing in mxnet/training/buildspec.yml i.e. \\<framework>/<training|inference>/buildspec.yml. \nIf you want to build the dockerfile for a particular version, or introduce a new version of the framework, re-create the \nfolder structure as per above and modify the buildspec.yml file to specify the version of the dockerfile you want to build.\n\n1. To build all the dockerfiles specified in the buildspec.yml locally, use the command\n    ```shell script\n    python src/main.py --buildspec mxnet/training/buildspec.yml --framework mxnet\n    ``` \n    The above step should take a while to complete the first time you run it since it will have to download all base layers \n    and create intermediate layers for the first time. \n    Subsequent runs should be much faster.\n2. If you would instead like to build only a single image\n    ```shell script\n    python src/main.py --buildspec mxnet/training/buildspec.yml \\\n                       --framework mxnet \\\n                       --image_types training \\\n                       --device_types cpu \\\n                       --py_versions py3\n    ```\n3. The arguments \u2014image_types, \u2014device_types and \u2014py_versions are all comma separated list who\u2019s possible values are as follows:\n    ```shell script\n    --image_types <training/inference>\n    --device_types <cpu/gpu>\n    --py_versions <py2/py3>\n    ```\n4. For example, to build all gpu, training containers, you could use the following command\n    ```shell script\n    python src/main.py --buildspec mxnet/training/buildspec.yml \\\n                       --framework mxnet \\\n                       --image_types training \\\n                       --device_types gpu \\\n                       --py_versions py3\n    ```\n\n### Upgrading the framework version\n1. Suppose, if there is a new framework version for MXNet (version 1.7.0) then this would need to be changed in the \nbuildspec.yml file for MXNet training.\n    ```yaml\n    # mxnet/training/buildspec.yml\n      1   account_id: &ACCOUNT_ID <set-$ACCOUNT_ID-in-environment>\n      2   region: &REGION <set-$REGION-in-environment>\n      3   framework: &FRAMEWORK mxnet\n      4   version: &VERSION 1.6.0 *<--- Change this to 1.7.0*\n          ................\n    ```\n2. The dockerfile for this should exist at mxnet/docker/1.7.0/py3/Dockerfile.gpu. This path is dictated by the \ndocker_file key for each repository. \n    ```yaml\n    # mxnet/training/buildspec.yml\n     41   images:\n     42     BuildMXNetCPUTrainPy3DockerImage:\n     43       <<: *TRAINING_REPOSITORY\n              ...................\n     49       docker_file: !join [ docker/, *VERSION, /, *DOCKER_PYTHON_VERSION, /Dockerfile., *DEVICE_TYPE ]\n     \n    ```\n3. Build the container as described above.\n### Adding artifacts to your build context\n1. If you are copying an artifact from your build context like this:\n    ```dockerfile\n    # deep-learning-containers/mxnet/training/docker/1.6.0/py3\n    COPY README-context.rst README.rst\n    ```\n    then README-context.rst needs to first be copied into the build context. You can do this by adding the artifact in \n    the framework buildspec file under the context key:\n    ```yaml\n    # mxnet/training/buildspec.yml\n     19 context:\n     20   README.xyz: *<---- Object name (Can be anything)*\n     21     source: README-context.rst *<--- Path for the file to be copied*\n     22     target: README.rst *<--- Name for the object in** the build context*\n    ```\n2. Adding it under context makes it available to all images. If you need to make it available only for training or \ninference images, add it under training_context or inference_context.\n    ```yaml\n     19   context:\n        .................\n     23       training_context: &TRAINING_CONTEXT\n     24         README.xyz:\n     25           source: README-context.rst\n     26           target: README.rst\n        ...............\n    ```\n3. If you need it for a single container add it under the context key for that particular image:\n    ```yaml\n     41   images:\n     42     BuildMXNetCPUTrainPy3DockerImage:\n     43       <<: *TRAINING_REPOSITORY\n              .......................\n     50       context:\n     51         <<: *TRAINING_CONTEXT\n     52         README.xyz:\n     53           source: README-context.rst\n     54           target: README.rst\n    ```\n4. Build the container as described above.\n### Adding a package\nThe following steps outline how to add a package to your image. For more information on customizing your container, see [Building AWS Deep Learning Containers Custom Images](custom_images.md).\n1. Suppose you want to add a package to the MXNet 1.6.0 py3 GPU docker image, then change the dockerfile from:\n    ```dockerfile\n    # mxnet/training/docker/1.6.0/py3/Dockerfile.gpu\n    139 RUN ${PIP} install --no-cache --upgrade \\\n    140     keras-mxnet==2.2.4.2 \\\n    ...........................\n    159     ${MX_URL} \\\n    160     awscli\n    ```\n    to\n    ```dockerfile\n    139 RUN ${PIP} install --no-cache --upgrade \\\n    140     keras-mxnet==2.2.4.2 \\\n    ...........................\n    160     awscli \\\n    161     octopush\n    ```\n2. Build the container as described above.\n\n### Running tests locally\nAs part of your iteration with your PR, sometimes it is helpful to run your tests locally to avoid using too many\nextraneous resources or waiting for a build to complete. The testing is supported using pytest. \n\nSimilar to building locally, to test locally, you\u2019ll need access to a personal/team AWS account. To test out:\n\n1. Either on an EC2 instance with the deep-learning-containers repo cloned, or on your local machine, make sure you have\nthe images you want to test locally (likely need to pull them from ECR). Then change directory into the cloned folder.\nInstall the requirements for tests.\n    ```shell script\n    cd deep-learning-containers/\n    pip install -r src/requirements.txt\n    pip install -r test/requirements.txt\n    ```\n2. In a shell, export environment variable DLC_IMAGES to be a space separated list of ECR uris to be tested. Set \nCODEBUILD_RESOLVED_SOURCE_VERSION to some unique identifier that you can use to identify the resources your test spins up. \nSet PYTHONPATH as the absolute path to the src/ folder.\nExample:\n[Note: change the repository name to the one setup in your account]\n    ```shell script\n    export DLC_IMAGES=\"$ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/pr-pytorch-training:training-gpu-py3 $ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/pr-mxnet-training:training-gpu-py3\"\n    export PYTHONPATH=$(pwd)/src\n    export CODEBUILD_RESOLVED_SOURCE_VERSION=\"my-unique-test\"\n    ```\n3. Our pytest framework expects the root dir to be test/dlc_tests, so change directories in your shell to be here\n    ```shell script\n    cd test/dlc_tests\n    ```\n4. To run all tests (in series) associated with your image for a given platform, use the following command\n    ```shell script\n    # EC2\n    pytest -s -rA ec2/ -n=auto\n    # ECS\n    pytest -s -rA ecs/ -n=auto\n    \n    #EKS\n    cd ../\n    export TEST_TYPE=eks\n    python test/testrunner.py\n    ```\n    Remove `-n=auto` to run the tests sequentially.\n5. To run a specific test file, provide the full path to the test file\n    ```shell script\n    pytest -s ecs/mxnet/training/test_ecs_mxnet_training.py\n    ```\n6. To run a specific test function (in this example we use the cpu dgl ecs test), modify the command to look like so:\n    ```shell script\n    pytest -s ecs/mxnet/training/test_ecs_mxnet_training.py::test_ecs_mxnet_training_dgl_cpu\n    ```\n\n7. To run SageMaker local mode tests, launch a cpu or gpu EC2 instance with latest Deep Learning AMI.\n   * Clone your github branch with changes and run the following commands\n       ```shell script\n       git clone https://github.com/{github_account_id}/deep-learning-containers/\n       cd deep-learning-containers && git checkout {branch_name}\n       ```\n   * Login into the ECR repo where the new docker images built exist\n       ```shell script\n       $(aws ecr get-login --no-include-email --registry-ids ${aws_id} --region ${aws_region})\n       ```\n   * Change to the appropriate directory (sagemaker_tests/{framework}/{job_type}) based on framework and job type of the image being tested.\n       The example below refers to testing mxnet_training images\n       ```shell script\n       cd test/sagemaker_tests/mxnet/training/\n       pip3 install -r requirements.txt\n       ```\n   * To run the SageMaker local integration tests (aside from tensorflow_inference), use the pytest command below:\n       ```shell script\n       python3 -m pytest -v integration/local --region us-west-2 \\\n       --docker-base-name ${aws_account_id}.dkr.ecr.us-west-2.amazonaws.com/mxnet-inference \\\n        --tag 1.6.0-cpu-py36-ubuntu18.04 --framework-version 1.6.0 --processor cpu \\\n        --py-version 3\n       ```\n   * To test tensorflow_inference py3 images, run the command below:\n     ```shell script\n     python3 -m  pytest -v integration/local \\\n     --docker-base-name ${aws_account_id}.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference \\\n     --tag 1.15.2-cpu-py36-ubuntu16.04 --framework-version 1.15.2 --processor cpu\n     ```\n8. To run SageMaker remote tests on your account please setup following pre-requisites\n\n    * Create an IAM role with name \u201cSageMakerRole\u201d in the above account and add the below AWS Manged policies\n       ```\n       AmazonSageMakerFullAccess\n       ```\n   *  Change to the appropriate directory (sagemaker_tests/{framework}/{job_type}) based on framework and job type of the image being tested.\"\n       The example below refers to testing mxnet_training images\n       ```shell script\n       cd test/sagemaker_tests/mxnet/training/\n       pip3 install -r requirements.txt\n       ```\n   *  To run the SageMaker remote integration tests (aside from tensorflow_inference), use the pytest command below:\n       ```shell script\n       pytest integration/sagemaker/test_mnist.py \\\n       --region us-west-2 --docker-base-name mxnet-training \\\n       --tag training-gpu-py3-1.6.0 --framework-version 1.6.0 --aws-id {aws_id} \\\n       --instance-type ml.p3.8xlarge\n       ```\n   * For tensorflow_inference py3 images run the below command\n      ```shell script\n      python3 -m pytest test/integration/sagemaker/test_tfs. --registry {aws_account_id} \\\n      --region us-west-2  --repo tensorflow-inference --instance-types ml.c5.18xlarge \\\n      --tag 1.15.2-py3-cpu-build --versions 1.15.2\n      ```\n9. To run SageMaker benchmark tests on your account please perform the following steps:\n    * Create a file named `sm_benchmark_env_settings.config` in the deep-learning-containers/ folder\n    * Add the following to the file (commented lines are optional):\n        ```shell script\n        export DLC_IMAGES=\"<image_uri_1-you-want-to-benchmark-test>\"\n        # export DLC_IMAGES=\"$DLC_IMAGES <image_uri_2-you-want-to-benchmark-test>\"\n        # export DLC_IMAGES=\"$DLC_IMAGES <image_uri_3-you-want-to-benchmark-test>\"\n        export BUILD_CONTEXT=PR\n        export TEST_TYPE=benchmark-sagemaker\n        export CODEBUILD_RESOLVED_SOURCE_VERSION=$USER\n        export REGION=us-west-2\n        ```\n    * Run:\n        ```shell script\n        source sm_benchmark_env_settings.config\n        ```\n    * To test all images for multiple frameworks, run:\n        ```shell script\n        pip install -r requirements.txt\n        python test/testrunner.py\n        ```\n    * To test one individual framework image type, run:\n        ```shell script\n        # Assuming that the cwd is deep-learning-containers/\n        cd test/dlc_tests\n        pytest benchmark/sagemaker/<framework-name>/<image-type>/test_*.py\n        ```\n    * The scripts and model-resources used in these tests will be located at:\n        ```\n        deep-learning-containers/test/dlc_tests/benchmark/sagemaker/<framework-name>/<image-type>/resources/\n        ```\n\nNote: SageMaker does not support tensorflow_inference py2 images.\n\n", "release_dates": ["2024-03-01T22:04:23Z", "2024-02-26T20:43:42Z", "2024-02-27T01:32:10Z", "2024-02-21T22:50:13Z", "2024-02-21T22:16:51Z", "2024-02-21T17:39:30Z", "2024-02-21T17:35:02Z", "2024-02-20T00:45:53Z", "2024-02-19T23:56:26Z", "2024-02-19T22:28:48Z", "2024-02-20T02:03:30Z", "2024-02-16T21:16:41Z", "2024-02-15T23:16:24Z", "2024-02-13T11:43:18Z", "2024-02-13T09:12:57Z", "2024-02-09T00:48:14Z", "2024-02-09T20:00:32Z", "2024-02-06T01:28:12Z", "2024-02-06T01:00:11Z", "2024-02-06T01:28:11Z", "2024-02-06T01:00:10Z", "2024-02-03T00:32:54Z", "2024-02-02T21:52:56Z", "2024-02-02T02:59:14Z", "2024-02-01T21:42:28Z", "2024-01-31T04:19:09Z", "2024-01-31T04:53:36Z", "2024-01-31T04:39:14Z", "2024-01-31T04:21:44Z", "2024-01-30T19:31:27Z"]}, {"name": "Device-Defender-for-AWS-IoT-embedded-sdk", "description": "Client library for using AWS IoT Defender service on embedded devices", "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# AWS IoT Device Defender Library\n\n**[API Documentation Pages for current and previous releases of this library can be found here](https://aws.github.io/Device-Defender-for-AWS-IoT-embedded-sdk/)**\n\nThe Device Defender library enables you to send device metrics to the\n[AWS IoT Device Defender Service](https://aws.amazon.com/iot-device-defender/).\nThis library also supports custom metrics, a feature that helps you monitor\noperational health metrics that are unique to your fleet or use case. For\nexample, you can define a new metric to monitor the memory usage or CPU usage on\nyour devices. This library has no dependencies on any additional libraries other\nthan the standard C library, and therefore, can be used with any MQTT client\nlibrary. This library is distributed under the\n[MIT Open Source License](LICENSE).\n\nThis library has gone through code quality checks including verification that no\nfunction has a\n[GNU Complexity](https://www.gnu.org/software/complexity/manual/complexity.html)\nscore over 8, and checks against deviations from mandatory rules in the\n[MISRA coding standard](https://www.misra.org.uk). Deviations from the MISRA\nC:2012 guidelines are documented under [MISRA Deviations](MISRA.md). This\nlibrary has also undergone static code analysis using\n[Coverity static analysis](https://scan.coverity.com/), and validation of memory\nsafety through the\n[CBMC automated reasoning tool](https://www.cprover.org/cbmc/).\n\nSee memory requirements for this library\n[here](./docs/doxygen/include/size_table.md).\n\n**AWS IoT Device Defender v1.3.0\n[source code](https://github.com/aws/Device-Defender-for-AWS-IoT-embedded-sdk/tree/v1.3.0/source)\nis part of the\n[FreeRTOS 202210.00 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202210.00-LTS)\nrelease.**\n\n**AWS IoT Device Defender v1.1.0\n[source code](https://github.com/aws/Device-Defender-for-AWS-IoT-embedded-sdk/tree/v1.1.0/source)\nis part of the\n[FreeRTOS 202012.01 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202012.01-LTS)\nrelease.**\n\n## AWS IoT Device Defender Client Config File\n\nThe AWS IoT Device Defender Client Library exposes build configuration macros\nthat are required for building the library. A list of all the configurations and\ntheir default values are defined in\n[defender_config_defaults.h](source/include/defender_config_defaults.h). To\nprovide custom values for the configuration macros, a config file named\n`defender_config.h` can be provided by the application to the library.\n\nBy default, a `defender_config.h` config file is required to build the library.\nTo disable this requirement and build the library with default configuration\nvalues, provide `DEFENDER_DO_NOT_USE_CUSTOM_CONFIG` as a compile time\npreprocessor macro.\n\n**Thus, the Device Defender client library can be built by either**:\n\n- Defining a `defender_config.h` file in the application, and adding it to the\n  include directories list of the library.\n\n**OR**\n\n- Defining the `DEFENDER_DO_NOT_USE_CUSTOM_CONFIG` preprocessor macro for the\n  library build.\n\n## Building the Library\n\nThe [defenderFilePaths.cmake](defenderFilePaths.cmake) file contains the\ninformation of all source files and the header include paths required to build\nthe Device Defender client library.\n\nAs mentioned in the previous section, either a custom config file (i.e.\n`defender_config.h`) or `DEFENDER_DO_NOT_USE_CUSTOM_CONFIG` macro needs to be\nprovided to build the Device Defender client library.\n\nFor a CMake example of building the Device Defender client library with the\n`defenderFilePaths.cmake` file, refer to the `coverity_analysis` library target\nin [test/CMakeLists.txt](test/CMakeLists.txt) file.\n\n## Building Unit Tests\n\n### Platform Prerequisites\n\n- For running unit tests:\n  - **C90 compiler** like gcc.\n  - **CMake 3.13.0 or later**.\n  - **Ruby 2.0.0 or later** is additionally required for the CMock test\n    framework (that we use).\n- For running the coverage target, **gcov** and **lcov** are additionally\n  required.\n\n### Steps to build **Unit Tests**\n\n1. Go to the root directory of this repository.\n\n1. Run the _cmake_ command:\n   `cmake -S test -B build -DBUILD_CLONE_SUBMODULES=ON`.\n\n1. Run this command to build the library and unit tests: `make -C build all`.\n\n1. The generated test executables will be present in `build/bin/tests` folder.\n\n1. Run `cd build && ctest` to execute all tests and view the test run summary.\n\n## CBMC\n\nTo learn more about CBMC and proofs specifically, review the training material\n[here](https://model-checking.github.io/cbmc-training).\n\nThe `test/cbmc/proofs` directory contains CBMC proofs.\n\nIn order to run these proofs you will need to install CBMC and other tools by\nfollowing the instructions\n[here](https://model-checking.github.io/cbmc-training/installation.html).\n\n## Reference examples\n\nThe AWS IoT Embedded C-SDK repository contains a demo showing the use of AWS IoT\nDevice Defender Client Library\n[here](https://github.com/aws/aws-iot-device-sdk-embedded-C/tree/main/demos/defender/defender_demo_json)\non a POSIX platform.\n\n## Documentation\n\n### Existing documentation\n\nFor pre-generated documentation, please see the documentation linked in the\nlocations below:\n\n|                                                                Location                                                                 |\n| :-------------------------------------------------------------------------------------------------------------------------------------: |\n|          [AWS IoT Device SDK for Embedded C](https://github.com/aws/aws-iot-device-sdk-embedded-C#releases-and-documentation)           |\n| [FreeRTOS.org](https://freertos.org/Documentation/api-ref/device-defender-for-aws-iot-embedded-sdk/docs/doxygen/output/html/index.html) |\n\nNote that the latest included version of the AWS IoT Device Defender library may\ndiffer across repositories.\n\n### Generating documentation\n\nThe Doxygen references were created using Doxygen version 1.9.2. To generate the\nDoxygen pages, please run the following command from the root of this\nrepository:\n\n```shell\ndoxygen docs/doxygen/config.doxyfile\n```\n\n## Contributing\n\nSee [CONTRIBUTING.md](./.github/CONTRIBUTING.md) for information on\ncontributing.\n", "release_dates": ["2022-10-14T16:05:58Z", "2021-11-11T23:47:44Z", "2021-07-23T01:51:47Z", "2021-02-27T01:32:15Z", "2020-12-14T17:46:48Z", "2020-11-04T22:33:43Z"]}, {"name": "Device-Shadow-for-AWS-IoT-embedded-sdk", "description": "Client library for using AWS IoT Shadow service on embedded devices", "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# AWS IoT Device Shadow library\n\n**[API Documentation Pages for current and previous releases of this library can be found here](https://aws.github.io/Device-Shadow-for-AWS-IoT-embedded-sdk/)**\n\nThe AWS IoT Device Shadow library enables you to store and retrieve the current\nstate (the \u201cshadow\u201d) of every registered device. The device\u2019s shadow is a\npersistent, virtual representation of your device that you can interact with\nfrom AWS IoT Core even if the device is offline. The device state is captured as\nits \u201cshadow\u201d within a [JSON](https://www.json.org/) document. The device can\nsend commands over MQTT to get, update and delete its latest state as well as\nreceive notifications over MQTT about changes in its state. Each device\u2019s shadow\nis uniquely identified by the name of the corresponding \u201cthing\u201d, a\nrepresentation of a specific device or logical entity on the AWS Cloud. See\n[Managing Devices with AWS IoT](https://docs.aws.amazon.com/iot/latest/developerguide/iot-thing-management.html)\nfor more information on IoT \"thing\". More details about AWS IoT Device Shadow\ncan be found in\n[AWS IoT documentation](https://docs.aws.amazon.com/iot/latest/developerguide/iot-device-shadows.html).\nThis library is distributed under the [MIT Open Source License](LICENSE).\n\n**Note**: From\n[v1.1.0](https://github.com/aws/Device-Shadow-for-AWS-IoT-embedded-sdk/tree/v1.1.0)\nrelease onwards, you can used named shadow, a feature of the AWS IoT Device\nShadow service that allows you to create multiple shadows for a single IoT\ndevice.\n\nThis library has gone through code quality checks including verification that no\nfunction has a\n[GNU Complexity](https://www.gnu.org/software/complexity/manual/complexity.html)\nscore over 8, and checks against deviations from mandatory rules in the\n[MISRA coding standard](https://www.misra.org.uk). Deviations from the MISRA\nC:2012 guidelines are documented under [MISRA Deviations](MISRA.md). This\nlibrary has also undergone both static code analysis from\n[Coverity static analysis](https://scan.coverity.com/), and validation of memory\nsafety through the\n[CBMC automated reasoning tool](https://www.cprover.org/cbmc/).\n\nSee memory requirements for this library\n[here](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/aws/device-shadow-for-aws-iot-embedded-sdk/docs/doxygen/output/html/index.html#shadow_memory_requirements).\n\n**AWS IoT Device Shadow v1.3.0\n[source code](https://github.com/aws/Device-Shadow-for-AWS-IoT-embedded-sdk/tree/v1.3.0)\nis part of the\n[FreeRTOS 202210.00 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202210.00-LTS)\nrelease.**\n\n**AWS IoT Device Shadow v1.0.2\n[source code](https://github.com/aws/Device-Shadow-for-AWS-IoT-embedded-sdk/tree/v1.0.2)\nis part of the\n[FreeRTOS 202012.00 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202012.00-LTS)\nrelease.**\n\n### AWS IoT Device Shadow Config File\n\nThe AWS IoT Device Shadow library exposes configuration macros that are required\nfor building the library. A list of all the configurations and their default\nvalues are defined in\n[shadow_config_defaults.h](source/include/shadow_config_defaults.h). To provide\ncustom values for the configuration macros, a custom config file named\n`shadow_config.h` can be provided by the user application to the library.\n\nBy default, a `shadow_config.h` custom config is required to build the library.\nTo disable this requirement and build the library with default configuration\nvalues, provide `SHADOW_DO_NOT_USE_CUSTOM_CONFIG` as a compile time preprocessor\nmacro.\n\n## Building the Library\n\nThe [shadowFilePaths.cmake](shadowFilePaths.cmake) file contains the information\nof all source files and the header include path required to build the AWS IoT\nDevice Shadow library.\n\nAs mentioned in the [previous section](#aws-iot-device-shadow-config-file),\neither a custom config file (i.e. `shadow_config.h`) OR the\n`SHADOW_DO_NOT_USE_CUSTOM_CONFIG` macro needs to be provided to build the AWS\nIoT Device Shadow library.\n\nFor a CMake example of building the AWS IoT Device Shadow library with the\n`shadowFilePaths.cmake` file, refer to the `coverity_analysis` library target in\n[test/CMakeLists.txt](test/CMakeLists.txt) file.\n\n## Building Unit Tests\n\n### Checkout CMock Submodule\n\nBy default, the submodules in this repository are configured with `update=none`\nin [.gitmodules](.gitmodules) to avoid increasing clone time and disk space\nusage of other repositories (like\n[amazon-freertos](https://github.com/aws/amazon-freertos) that submodules this\nrepository).\n\nTo build unit tests, the submodule dependency of CMock is required. Use the\nfollowing command to clone the submodule:\n\n```\ngit submodule update --checkout --init --recursive --test/unit-test/CMock\n```\n\n### Platform Prerequisites\n\n- For building the library, **CMake 3.13.0** or later and a **C90 compiler**.\n- For running unit tests, **Ruby 2.0.0** or later is additionally required for\n  the CMock test framework (that we use).\n- For running the coverage target, **gcov** and **lcov** are additionally\n  required.\n\n### Steps to build unit tests\n\n1. Go to the root directory of this repository. (Make sure that the **CMock**\n   submodule is cloned as described [above](#checkout-cmock-submodule).)\n\n1. Run the _cmake_ command: `cmake -S test -B build`\n\n1. Run this command to build the library and unit tests: `make -C build all`\n\n1. The generated test executables will be present in `build/bin/tests` folder.\n\n1. Run `cd build && ctest` to execute all tests and view the test run summary.\n\n## CBMC\n\nTo learn more about CBMC and proofs specifically, review the training material\n[here](https://model-checking.github.io/cbmc-training).\n\nThe `test/cbmc/proofs` directory contains CBMC proofs.\n\nIn order to run these proofs you will need to install CBMC and other tools by\nfollowing the instructions\n[here](https://model-checking.github.io/cbmc-training/installation.html).\n\n## Reference examples\n\nPlease refer to the demos of the AWS IoT Device Shadow library in the following\nlocations for reference examples on POSIX and FreeRTOS platforms:\n\n| Platform |                                                             Location                                                              | Transport Interface Implementation <br> (for [coreMQTT](https://github.com/FreeRTOS/coreMQTT) stack) </br> |\n| :------: | :-------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------: |\n|  POSIX   | [AWS IoT Device SDK for Embedded C](https://github.com/aws/aws-iot-device-sdk-embedded-C/tree/main/demos/shadow/shadow_demo_main) |                             POSIX sockets for TCP/IP and OpenSSL for TLS stack                             |\n| FreeRTOS |    [FreeRTOS/FreeRTOS](https://github.com/FreeRTOS/FreeRTOS/tree/main/FreeRTOS-Plus/Demo/AWS/Device_Shadow_Windows_Simulator)     |                             FreeRTOS+TCP for TCP/IP and mbedTLS for TLS stack                              |\n| FreeRTOS |        [FreeRTOS AWS Reference Integrations](https://github.com/aws/amazon-freertos/tree/main/demos/device_shadow_for_aws)        |                                    Based on Secure Sockets Abstraction                                     |\n\n## Documentation\n\n### Existing Documentation\n\nFor pre-generated documentation, please see the documentation linked in the\nlocations below:\n\n|                                                               Location                                                                |\n| :-----------------------------------------------------------------------------------------------------------------------------------: |\n|         [AWS IoT Device SDK for Embedded C](https://github.com/aws/aws-iot-device-sdk-embedded-C#releases-and-documentation)          |\n| [FreeRTOS.org](https://freertos.org/Documentation/api-ref/device-shadow-for-aws-iot-embedded-sdk/docs/doxygen/output/html/index.html) |\n\nNote that the latest included version of IoT Device Shadow library may differ\nacross repositories.\n\n## Generating documentation\n\nThe Doxygen references were created using Doxygen version 1.9.2. To generate the\nDoxygen pages, please run the following command from the root of this\nrepository:\n\n```shell\ndoxygen docs/doxygen/config.doxyfile\n```\n\n## Contributing\n\nSee [CONTRIBUTING.md](./.github/CONTRIBUTING.md) for information on\ncontributing.\n", "release_dates": ["2022-10-14T16:04:14Z", "2021-11-11T23:48:12Z", "2021-07-23T18:42:40Z", "2021-03-02T02:48:27Z", "2020-12-15T15:09:50Z", "2020-11-04T22:31:23Z", "2020-09-16T23:23:52Z"]}, {"name": "dotnet", "description": "GitHub home for .NET development on AWS", "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "[![alt text](./logo.png \".NET on AWS\")](https://aws.amazon.com/developer/language/net/)\n\n<strong>aws/dotnet</strong> is the GitHub home for .NET development on AWS.  You'll find libraries, tools, and resources to help you build .NET applications and services on AWS.\n\nClick on the links below to jump to a section:\n* [Online Resources](#Online-Resources)\n* [Software and Libraries](#Software-and-Libraries)\n* [.NET Development Tools](#NET-Development-Tools)\n* [AWS Cloud Resources for .NET](#AWS-Cloud-Resources-for-NET)\n* [Documentation](#Documentation)\n\n## Online Resources\n\n[AWS Developer Center - Explore .NET on AWS](https://aws.amazon.com/developer/language/net/)  \nFind all the .NET code samples, step-by-step guides, videos, blog content, tools, and information about live events that you need in one place. \n\n[AWS Developer Blog - .NET](https://aws.amazon.com/blogs/developer/category/programing-language/dot-net/)  \nCome see what .NET developers at AWS are up to!  Learn about new .NET software announcements, guides, and how-to's.\n\n[@dotnetonaws](https://twitter.com/dotnetonaws)  \nFollow us on Twitter!\n\n## Software and Libraries\n\n[AWS SDK for .NET](https://github.com/aws/aws-sdk-net)  \nThe AWS SDK for .NET enables .NET developers to easily work with Amazon Web Services and build scalable solutions with Amazon S3, Amazon DynamoDB, Amazon Glacier, and more.\n\n[AWS Lambda for .NET Core](https://github.com/aws/aws-lambda-dotnet)  \nThis repository contains tools and blueprints used to create C# and Powershell AWS Lambda functions.\n\n[AWS Extensions .NET Core Setup](https://github.com/aws/aws-sdk-net/tree/master/extensions/src/AWSSDK.Extensions.NETCore.Setup)  \n[![nuget](https://img.shields.io/nuget/v/AWSSDK.Extensions.NETCore.Setup.svg) ![downloads](https://img.shields.io/nuget/dt/AWSSDK.Extensions.NETCore.Setup.svg)](https://www.nuget.org/packages/AWSSDK.Extensions.NETCore.Setup/)  \nThis library is an extension for the AWS SDK for .NET to integrate with .NET Core configuration and dependency injection frameworks.\n\n[AWS Logging .NET](https://github.com/aws/aws-logging-dotnet)  \n[![nuget](https://img.shields.io/nuget/v/AWS.Logger.AspNetCore.svg) ![downloads](https://img.shields.io/nuget/dt/AWS.Logger.AspNetCore.svg)](https://www.nuget.org/packages/AWS.Logger.AspNetCore/)  \nThese libraries integrate Amazon CloudWatch Logs with popular .NET logging libraries.\n\n[Amazon Cognito Authentication Extension Library](https://github.com/aws/aws-sdk-net-extensions-cognito)  \n[![nuget](https://img.shields.io/nuget/v/Amazon.Extensions.CognitoAuthentication.svg) ![downloads](https://img.shields.io/nuget/dt/Amazon.Extensions.CognitoAuthentication.svg)](https://www.nuget.org/packages/Amazon.Extensions.CognitoAuthentication/)  \nThe Amazon Cognito Extension Library simplifies the authentication process of Amazon Cognito User Pools for .NET developers.  It allows you to use various authentication methods for Amazon Cognito User Pools with only a few short method calls, along with making the process intuitive.\n\n[AWS Secrets Manager Caching Library for .NET](https://github.com/aws/aws-secretsmanager-caching-net)  \n[![nuget](https://img.shields.io/nuget/v/AWSSDK.SecretsManager.Caching.svg) ![downloads](https://img.shields.io/nuget/dt/AWSSDK.SecretsManager.Caching.svg)](https://www.nuget.org/packages/AWSSDK.SecretsManager.Caching/)  \nThe AWS Secrets Manager caching client enables in-process caching of secrets for .NET applications.\n\n[ASP.NET Core Identity Provider for Amazon Cognito](https://github.com/aws/aws-aspnet-cognito-identity-provider)  \n[![nuget](https://img.shields.io/nuget/v/Amazon.AspNetCore.Identity.Cognito.svg) ![downloads](https://img.shields.io/nuget/dt/Amazon.AspNetCore.Identity.Cognito.svg)](https://www.nuget.org/packages/Amazon.AspNetCore.Identity.Cognito/)  \nASP.NET Core Identity Provider for Amazon Cognito simplifies using Amazon Cognito as a membership storage solution for building ASP.NET Core web applications using ASP.NET Core Identity.\n\n[AWS .NET Configuration Extension for Systems Manager](https://github.com/aws/aws-dotnet-extensions-configuration)  \n[![nuget](https://img.shields.io/nuget/v/Amazon.Extensions.Configuration.SystemsManager.svg) ![downloads](https://img.shields.io/nuget/dt/Amazon.Extensions.Configuration.SystemsManager.svg)](https://www.nuget.org/packages/Amazon.Extensions.Configuration.SystemsManager/)  \nConfiguration Extension for Systems Manager library simplifies using AWS SSM's Parameter Store as a source for configuration information for .NET Core applications. This project was contributed by [@KenHundley](https://github.com/kenhundley).\n\n[Amazon ElastiCache Cluster Configuration for .NET](https://github.com/awslabs/elasticache-cluster-config-net)  \n[![nuget](https://img.shields.io/nuget/v/Amazon.ElastiCacheCluster.svg) ![downloads](https://img.shields.io/nuget/dt/Amazon.ElastiCacheCluster.svg)](https://www.nuget.org/packages/Amazon.ElastiCacheCluster/)  \nAmazon ElastiCache Cluster Configuration is an enhanced .NET library that supports connecting to an Amazon ElastiCache cluster for Auto Discovery. This client library is an extension built upon Enyim and is released under the Apache 2.0 License.\n\n[AWS Systems Manager ASP.NET Core Data Protection Provider](https://github.com/aws/aws-ssm-data-protection-provider-for-aspnet)  \n[![nuget](https://img.shields.io/nuget/v/Amazon.AspNetCore.DataProtection.SSM.svg) ![downloads](https://img.shields.io/nuget/dt/Amazon.AspNetCore.DataProtection.SSM.svg)](https://www.nuget.org/packages/Amazon.AspNetCore.DataProtection.SSM/)  \nAWS Systems Manager ASP.NET Core Data Protection Provider library allows you to use AWS SSM's Parameter Store to store keys generated by ASP.NET's Data Protection API. This enables you to scale by allowing multiple web servers to share the keys.\n\n[Amazon.Lambda.RuntimeSupport](https://github.com/aws/aws-lambda-dotnet/tree/master/Libraries/src/Amazon.Lambda.RuntimeSupport)  \n[![nuget](https://img.shields.io/nuget/v/Amazon.Lambda.RuntimeSupport.svg) ![downloads](https://img.shields.io/nuget/dt/Amazon.Lambda.RuntimeSupport.svg)](https://www.nuget.org/packages/Amazon.Lambda.RuntimeSupport/)  \n Amazon.Lambda.RuntimeSupport library makes it easy to create Lambda functions using .NET standard 2.0-compatible runtimes.\n\n[DynamoDB Accelerator (DAX) for Microsoft .NET](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.client.run-application-dotnet.html)  \n[![nuget](https://img.shields.io/nuget/v/AWSSDK.DAX.Client.svg) ![downloads](https://img.shields.io/nuget/dt/AWSSDK.DAX.Client.svg)](https://www.nuget.org/packages/AWSSDK.DAX.Client/)  \nDynamoDB Accelerator (DAX) for Microsoft .NET -- DAX is a fully managed, in-memory cache for DynamoDB.\n\n[AWS X-Ray SDK for .NET](https://github.com/aws/aws-xray-sdk-dotnet)  \nAWS X-Ray helps developers analyze and debug distributed applications. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors.\n\n[Amazon S3 Encryption Client for .NET](https://github.com/aws/amazon-s3-encryption-client-dotnet)  \n[![nuget](https://img.shields.io/nuget/v/Amazon.Extensions.S3.Encryption.svg) ![downloads](https://img.shields.io/nuget/dt/Amazon.Extensions.S3.Encryption.svg)](https://www.nuget.org/packages/Amazon.Extensions.S3.Encryption/)  \nAmazon S3 Encryption Client for .NET is a client-side encryption library designed to make it easy for everyone to encrypt and decrypt data using industry standards and best practices. It enables you to focus on the core functionality of your application, rather than on how to best encrypt and decrypt your data.\n\n[AWS .NET Distributed Cache Provider](https://github.com/awslabs/aws-dotnet-distributed-cache-provider)  \n[![nuget](https://img.shields.io/nuget/v/AWS.AspNetCore.DistributedCacheProvider.svg) ![downloads](https://img.shields.io/nuget/dt/AWS.AspNetCore.DistributedCacheProvider.svg)](https://www.nuget.org/packages/AWS.AspNetCore.DistributedCacheProvider/)  \nThe AWS .NET Distributed Cache Provider provides an implementation of the ASP.NET Core interface [IDistributedCache](https://docs.microsoft.com/en-us/aspnet/core/performance/caching/distributed) backed by Amazon DynamoDB. A common use of an `IDistributedCache` implementation is to store ephemeral, non-critical [session state](https://learn.microsoft.com/en-us/aspnet/core/fundamentals/app-state?#session-state) data in ASP.NET Core applications.\n\n\n## .NET Development Tools\n[AWS Toolkit for Visual Studio 2022](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.AWSToolkitforVisualStudio2022)  \n[![VS Marketplace](https://img.shields.io/vscode-marketplace/v/AmazonWebServices.AWSToolkitforVisualStudio2022.svg) ![downloads](https://img.shields.io/vscode-marketplace/d/AmazonWebServices.AWSToolkitforVisualStudio2022.svg)](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.AWSToolkitforVisualStudio2022)    \nThe AWS Toolkit for Visual Studio 2022 is an extension for Microsoft Visual Studio 2022 on Microsoft Windows. The toolkit makes it easier for developers to develop, debug, and deploy .NET and .NET Core applications using Amazon Web Services.\n\n[AWS Toolkit for Visual Studio 2017 and 2019](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.AWSToolkitforVisualStudio2017)  \n[![VS Marketplace](https://img.shields.io/vscode-marketplace/v/AmazonWebServices.AWSToolkitforVisualStudio2017.svg) ![downloads](https://img.shields.io/vscode-marketplace/d/AmazonWebServices.AWSToolkitforVisualStudio2017.svg)](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.AWSToolkitforVisualStudio2017)    \nThe AWS Toolkit for Visual Studio 2017 and 2019 is an extension for Microsoft Visual Studio 2017 and 2019 on Microsoft Windows. The toolkit makes it easier for developers to develop, debug, and deploy .NET and .NET Core applications using Amazon Web Services.\n\n[AWS Toolkit for JetBrains](https://github.com/aws/aws-toolkit-jetbrains)  \n[![JetBrains Marketplace](https://img.shields.io/jetbrains/plugin/v/11349-aws-toolkit.svg?label=version) ![downloads](https://img.shields.io/jetbrains/plugin/d/11349-aws-toolkit)](https://plugins.jetbrains.com/plugin/11349-aws-toolkit)  \nThe AWS Toolkit for JetBrains works with Rider and adds support for working with AWS services such as AWS Lambda and S3.\n\n[AWS Toolkit for Visual Studio Code](https://github.com/aws/aws-toolkit-vscode)  \n[![VS Marketplace](https://img.shields.io/vscode-marketplace/v/AmazonWebServices.aws-toolkit-vscode.svg) ![downloads](https://img.shields.io/vscode-marketplace/d/AmazonWebServices.aws-toolkit-vscode.svg)](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.aws-toolkit-vscode)  \nThe AWS Toolkit for Visual Studio Code is a VS Code extension that lets you work with AWS services such as AWS Lambda.\n\n[AWS Toolkit for Azure DevOps](https://github.com/aws/aws-vsts-tools)  \n[![VS Marketplace](https://img.shields.io/vscode-marketplace/v/AmazonWebServices.aws-vsts-tools.svg) ![downloads](https://img.shields.io/vscode-marketplace/d/AmazonWebServices.aws-vsts-tools.svg)](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.aws-vsts-tools)    \nTasks for Amazon S3, AWS Elastic Beanstalk, AWS CodeDeploy, AWS Lambda and AWS CloudFormation and more, and running commands in the AWS Tools for Windows PowerShell module and the AWS CLI.\n\n[AWS Tools for Windows PowerShell and PowerShell Core](https://github.com/aws/aws-tools-for-powershell)  \n[![](https://img.shields.io/powershellgallery/v/AWSPowerShell.svg?label=AWSPowerShell)](https://www.powershellgallery.com/packages/AWSPowerShell)  \n[![](https://img.shields.io/powershellgallery/v/AWSPowerShell.NetCore.svg?label=AWSPowerShell.NetCore)](https://www.powershellgallery.com/packages/AWSPowerShell.NetCore)  \nThe AWS Tools for Windows PowerShell and PowerShell Core let developers and administrators manage their AWS services from the PowerShell scripting environment.\n\n[AWS .NET Mock Lambda Test Tool - Preview](https://github.com/aws/aws-lambda-dotnet/tree/master/Tools/LambdaTestTool)  \n[![nuget](https://img.shields.io/nuget/v/Amazon.Lambda.TestTool-2.1.svg) ![downloads](https://img.shields.io/nuget/dt/Amazon.Lambda.TestTool-2.1.svg)](https://www.nuget.org/packages/Amazon.Lambda.TestTool-2.1/)  \nThe AWS .NET Mock Lambda Test Tool is a tool that can be used to load a .NET Core Lambda project and execute the selected code inside an emulated Lambda environment. An IDE that is attached to the process hosting this tool can then debug and step through the .NET Core Lambda code. The tool is optimized for quick local debugging with minimal dependencies.\n\n[AWS Lambda Tools for Powershell](https://www.powershellgallery.com/packages/AWSLambdaPSCore)  \nThe AWS Lambda Tools for Powershell can be used to create and deploy AWS Lambda functions written in PowerShell.\n\n[AWS Extensions for dotnet CLI](https://github.com/aws/aws-extensions-for-dotnet-cli)  \n[![nuget](https://img.shields.io/nuget/v/Amazon.Lambda.Tools.svg) ![downloads](https://img.shields.io/nuget/dt/Amazon.Lambda.Tools.svg)](https://www.nuget.org/packages/Amazon.Lambda.Tools/)  \nThis repository contains AWS tool extensions to the .NET CLI. These tool extensions are focused on building .NET Core and ASP.NET Core applications and deploying them to AWS services. Many of these deployment commands are the same commands the AWS Toolkit for Visual Studio uses to perform its deployment features. \n\n[AWS .NET deployment tool](https://github.com/aws/aws-dotnet-deploy)  \n[![nuget](https://img.shields.io/nuget/v/AWS.Deploy.Tools.svg) ![downloads](https://img.shields.io/nuget/dt/AWS.Deploy.Tools.svg)](https://www.nuget.org/packages/AWS.Deploy.Tools/)  \nThis repository contains the AWS .NET deployment tool for .NET CLI - the opinionated tooling that simplifies deployment of .NET applications with minimum AWS knowledge. The tool suggests the right AWS compute service to deploy your application to. It then builds and packages your application as required by the chosen compute service, generates the deployment infrastructure, deploys your application by using the Cloud Development Kit (CDK), and displays the endpoint. \n\n## AWS Cloud Resources for .NET\n\n[.NET Core EC2 AMIs](https://aws.amazon.com/about-aws/whats-new/2018/03/announcing--net-core-ami-for-amazon-ec2/)  \nAmazon Web Services offers Amazon Machine Images (AMI) with .NET Core on Amazon Linux 2 and Ubuntu.\n\n[.NET Core CodeBuild Images](https://github.com/aws/aws-codebuild-docker-images/tree/master/ubuntu/dot-net)  \nThis repository holds Dockerfiles of official AWS CodeBuild curated Docker images. Please refer to the [AWS CodeBuild User Guide](https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref.html) for list of environments supported by AWS CodeBuild.\n\n## Documentation\n[AWS .NET Developer Guide](https://docs.aws.amazon.com/sdk-for-net/latest/developer-guide/)  \nThe AWS SDK for .NET Developer Guide describes how to implement applications for AWS using the AWS SDK for .NET\n\n[AWS SDK for .NET V3 API Reference](https://docs.aws.amazon.com/sdkfornet/v3/apidocs/Index.html)  \nOfficial AWS SDK for .NET API reference.\n\n[AWS Tools for Powershell Cmdlet Reference](https://docs.aws.amazon.com/powershell/latest/reference/Index.html)  \nOfficial AWS Tools for Powershell Cmdlet reference.\n\n[AWS Elastic Beanstalk - Working with .NET](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_NET.html)  \nLearn about creating and deploying Elastic Beanstalk Applications in .NET Using AWS Toolkit for Visual Studio\n\n[AWS Lambda - Lambda Functions (C#)](https://docs.aws.amazon.com/lambda/latest/dg/dotnet-programming-model.html)  \nLambda Developer guide describes in detail on how to write C# Lambda functions and how to deploy them.\n\n[AWS Lambda - Lambda Functions (PowerShell)](https://docs.aws.amazon.com/lambda/latest/dg/powershell-programming-model.html)  \nLambda Developer guide describes in detail on how to write PowerShell Lambda functions and how to deploy them.\n", "release_dates": []}, {"name": "dotnet-foss", "description": null, "language": null, "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# .NET on AWS Open Source Software Fund ([FOSS](https://github.com/indeedeng/FOSS-Contributor-Fund) fund)\r\n\r\nThe .NET on AWS team is committed to supporting a healthy and thriving .NET OSS ecosystem. We understand that .NET Open Source is a labor of love for a lot of project maintainers and we want to show our appreciation to projects that are vital to the ecosystem.\r\n\r\n### What is the fund?\r\n\r\nFor 2023, we are growing the size of the program, continuing on the work done in 2022 when 10 projects were selected and given a one-time donation of $5,000. In 2023, we will have an expanded process for selecting projects, as well as increasing the number of projects sponsored and the funding amount, moving to 12 total projects, one per month, with a per project funding of $6,000.\r\n\r\nEvery month a new fund will be initiated, with a selection process that is unique every month. Instead of a one-time donation, the selected project will receive 12 monthly donations, via GitHub sponsors, in the amount of $500, totalling $6,000.\r\n\r\n### How projects get selected?\r\n\r\nEvery month, the selection committee nominates 2-3 projects to put up for selection. There are some criteria for projects to be eligible.\r\n\r\n- The project has an [OSI-approved open source license](https://opensource.org/)\r\n- Must have [GitHub Sponsors](https://github.com/sponsors) enabled for the organization/primary user of the project\r\n- Project is making a decision to actively prioritize diversity, equity, inclusion and accessibility.\r\n\r\nOnce all nominations are complete, the selection committee will vote, via [ranked-choice voting](https://en.wikipedia.org/wiki/Ranked_voting), to select a project to award the grant to. The news of this award will be made to the maintainer and if the recipient chooses to receive the award, the donation will be initiated via GitHub sponsors. If the recipient declines the award, the 2nd place vote getter will be notified and so on. Once initial funding has been completed, the selected project will be announced here as well as amplified on the [@dotnetOnAWS Twitter Handle](https://twitter.com/dotnetonaws)\r\n\r\n## 2024 Selection Committee\r\n<table>\r\n    <tr>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/abhi-gujjewar.jpeg\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://www.linkedin.com/in/abhiramgujjewar/\" target=\"_blank\">Abhi Gujjewar</a>\r\n        </td>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/alexey-zimarev.png\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://twitter.com/Zimareff\" target=\"_blank\">Alexey Zimarev</a>\r\n        </td>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/biroj-nayak.jpeg\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://www.linkedin.com/in/biroj/\" target=\"_blank\">Biroj Nayak</a>\r\n        </td style=\"text-align: center; vertical-align: middle;\">\r\n        <td>\r\n            <img src=\"./committee/bryan-hogan.png\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://twitter.com/bryanjhogan\" target=\"_blank\">Bryan Hogan</a>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/dave-glick.jpg\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://www.linkedin.com/in/daveaglick/\" target=\"_blank\">Dave Glick</a>\r\n        </td>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/dennis-doomen.jpeg\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://twitter.com/ddoomen\" target=\"_blank\">Dennis Doomen</a>\r\n        </td>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/egil-hansen.jpg\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://twitter.com/egilhansen\" target=\"_blank\">Egil Hansen</a>\r\n        </td>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/isaac-levin.jpeg\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://twitter.com/isaacrlevin\" target=\"_blank\">Isaac Levin</a>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/javier-lozano.jpg\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://twitter.com/jglozano\" target=\"_blank\">Javier Lozano</a>\r\n        </td>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/jeremy-sinclair.jpeg\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://twitter.com/sinclairinat0r\" target=\"_blank\">Jeremy Sinclair</a>\r\n        </td>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/joe-guadagno.png\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://twitter.com/jguadagno\" target=\"_blank\">Joe Guadagno</a>\r\n        </td>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/jonas-nyrup.png\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://twitter.com/jnyrup\" target=\"_blank\">Jonas Nyrup</a>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/norm-johanson.png\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://twitter.com/socketnorm\" target=\"_blank\">Norm Johanson</a>\r\n        </td>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/pj-pittle.jpeg\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://www.linkedin.com/in/philip-p-9796183/\" target=\"_blank\">PJ Pittle</a>\r\n        </td>\r\n        <td style=\"text-align: center; vertical-align: middle;\">\r\n            <img src=\"./committee/samiullah-mohammed.jpeg\" height=\"100\" />\r\n            <br />\r\n            <a href=\"https://www.linkedin.com/in/samiullah-mohammed-6948842/\" target=\"_blank\">Samiullah Mohammed</a>\r\n        </td>\r\n    </tr>\r\n</table>\r\n\r\n\r\n## Funding Recipients\r\n\r\n### 2024\r\n\r\n| Month | Project  |\r\n| --------- | --------------- |\r\n| January   | [EF Core Power Tools](https://github.com/ErikEJ/EFCorePowerTools) |\r\n| February  | [CvsHelper](https://github.com/JoshClose/CsvHelper) |\r\n| March     |  |\r\n| April     |  |\r\n| May       |  |\r\n| June      |  |\r\n| July      |  |\r\n| August    |  |\r\n| September |  | \r\n| October   |  |\r\n| November  |  |\r\n| December  |  |\r\n\r\n### 2023\r\n\r\n| Month | Project  |\r\n| -------- | --------------- |\r\n| January  | [AngleSharp](https://github.com/AngleSharp/AngleSharp) |\r\n| February | [MassTransit](https://github.com/MassTransit/MassTransit) |\r\n| March    | [Nuke](https://github.com/nuke-build/nuke)|\r\n| April    | [Marten](https://github.com/JasperFx/marten)|\r\n| May      | [AvaloniaUI](https://github.com/AvaloniaUI/Avalonia)|\r\n| June     | [ApiEndpoints](https://github.com/ardalis/ApiEndpoints)|\r\n| July     | [Blazored LocalStorage](https://github.com/Blazored/LocalStorage)|\r\n| August   | [YamlDotNet](https://github.com/aaubry/YamlDotNet) |\r\n| September | [Verify](https://github.com/VerifyTests/Verify) | \r\n| October | [Dapper](https://github.com/DapperLib/Dapper) |\r\n| November  | [PuppeteerSharp](https://github.com/hardkoded/puppeteer-sharp) |\r\n| December  | [BenchmarkDotNet](https://github.com/dotnet/BenchmarkDotNet) |\r\n\r\n\r\n\r\n### 2022\r\n\r\nIn 2022, the .NET on AWS team sponsored 10 projects with one-time donations of $5,000 for their contributions to the .NET Open Source Ecosystem.\r\n\r\n| Project |\r\n| --------------- |\r\n| [AutoMapper](https://github.com/AutoMapper/AutoMapper) |\r\n| [FluentValidation](https://github.com/FluentValidation/FluentValidation) |\r\n| [FluentAssertions](https://github.com/fluentassertions/fluentassertions) |\r\n| [Autofac](https://github.com/autofac/Autofac) |\r\n| [Buildalyzer](https://github.com/daveaglick/Buildalyzer) |\r\n| [SixLabors.ImageSharp](https://github.com/SixLabors/ImageSharp) |\r\n| [Swashbuckle](https://github.com/domaindrivendev/Swashbuckle.AspNetCore) |\r\n| [RestSharp](https://github.com/restsharp/RestSharp) |\r\n| [bUnit](https://github.com/bUnit-dev/bUnit) |\r\n\r\n## Other FOSS Funds\r\n\r\nThe .NET on AWS team is not alone in funding to Open Source via a FOSS Fund model. The model was initially created by the open source engineering group at Indeed, and has been iterated by countless other companies. If you are interested in learning more about their approach, you can read the blog post [The FOSS Contributor Fund: Six Months In](https://engineering.indeedblog.com/blog/2019/07/foss-fund-six-months-in/).\r\n\r\n## Resources\r\n\r\nTo learn more about the work the .NET on AWS team, and AWS in general is doing, here are some links to learn more about our efforts\r\n\r\n- [.NET on AWS](http://aws.amazon.com/dotnet)\r\n- [AWS Open Source](https://aws.amazon.com/opensource)\r\n- [.NET on AWS GitHub](https://github.com/aws/dotnet)\r\n", "release_dates": []}, {"name": "ec2-hibernate-linux-agent", "description": "A Hibernating Agent for Linux on Amazon EC2", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# The EC2 Spot hibernation agent (Legacy)\n\n> With the release of a new generation of Spot Hibernation, this repo has entered legacy status\n> \n> Please refer to the [hibinit-agent](https://github.com/aws/amazon-ec2-hibinit-agent) repo now used for Spot Hibernation\n> \n> Related Documentation: \n> * Instructions to enable Spot Hibernation | [Link](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/hibernate-spot-instances.html)\n\n## License\nThe code is released under Apache License Vesion 2.0. See LICENSE.txt for details.\n\n## Description\n\nThis agent does several things:\n\n1. Upon startup it checks for sufficient swap space to allow hibernate and fails\n    if it's present but there's not enough of it.\n2. If there's no swap space, it creates it and launches a background thread to\n    touch all of its blocks to make sure that EBS volumes are pre-warmed.\n3. It updates the offset of the swap file in the kernel using SNAPSHOT_SET_SWAP_AREA ioctl.\n4. It daemonizes and starts a polling thread to listen for instance termination notifications.\n\n## Building\nThe code can be build using the usual Python setuptools:\n\n```\npython setup.py install\n```\n\nAdditionally, you can build an sRPM package for CentOS/RedHat by running \"make sources\".\n", "release_dates": []}, {"name": "ec2-hibernate-windows-agent", "description": "A Hibernating Agent for Windows on Amazon EC2", "language": "PowerShell", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# EC2 Hibernate Windows Agent (Deprecated)\n\n> As of December 2023, the ec2-hibernate-windows-agent is officially deprecated. It is superseded by the new version of EC2 Spot Hibernation.\n> The new version of EC2 Spot Hibernation provides significant improvements.\n> For more information about the new version and to access its features, please visit | [Link](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/hibernate-spot-instances.html)\n\nA Hibernating Agent for Windows on Amazon EC2\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n\n## Build\n\nRunning EC2HibernateAgentCompiler.ps1 will generate a C# executable called EC2HibernateAgent.exe.  This is a wrapper for EC2HibernateAgent.ps1.\n\n## Install and Run\n\nFollow the instructions [here](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-hibernation.html \"EC2 Spot hibernation user guide\") to install and run the agent \u2013 i.e. place EC2HibernateAgent.exe and EC2HibernateAgent.ps1 in the directory \"C:\\Program Files\\Amazon\\Hibernate\" of your Windows EC2 Spot instance, then run EC2HibernateAgent.exe from your instance user data.\n\n", "release_dates": []}, {"name": "ec2-image-builder-roadmap", "description": "Public Roadmap for EC2 Image Builder. ", "language": null, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## EC2 Image Builder Roadmap\n\n## Introduction\nThis is the public roadmap for EC2 Image Builder, directly managed by the service team.\nKnowing about our upcoming products and priorities helps our customers plan. This repository contains information about what we are working on and allows all AWS customers to give direct feedback.\n\n[See the roadmap \u00bb](https://github.com/aws/ec2-image-builder-roadmap/projects/3)\n\nPlease also refer to [Image Builder samples](https://github.com/aws-samples/amazon-ec2-image-builder-samples) for quick demo of the service capabilities.\n\n**Other AWS Public Roadmaps**\n* [AWS Container Services](https://github.com/aws/containers-roadmap)\n* [AWS App Mesh](https://github.com/aws/aws-app-mesh-roadmap)\n* [CloudFormation coverage](https://github.com/aws-cloudformation/aws-cloudformation-coverage-roadmap)\n* [AWS Elastic Beanstalk](https://github.com/aws/elastic-beanstalk-roadmap)\n* [Amazon EC2 Spot Instances integrations](https://github.com/aws/ec2-spot-instances-integrations-roadmap)\n* [AWS Controllers for Kubernetes (ACK)](https://github.com/aws/aws-controllers-k8s/projects/1)\n\n## Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n\n## FAQs\n**Q: Why did you build this?**\n\nA: We know that our customers are making decisions and plans based on what we are developing, and we want to provide our customers the insights they need to plan.\n\n**Q: Why are there no dates on your roadmap?**\n\nA: Because job zero is security and operational stability, we can't provide specific target dates for features.\n\n**Q: What do the roadmap categories mean?**\n* *In Production* - obvious, right?\n* *In Development* - in progress, but further out.  We might still be working through the implementation details, or scoping stuff out.\n* *Researching* - We're thinking about it. This might mean we're still designing, or thinking through how this might work. This is a great phase to send how you want to see something implemented!  We'd love to see your usecase or design ideas here.\n* *In Backlog* - These features are on the consideration list and might be picked up based on several prioritization signals.\n\n**Q: Is everything on the roadmap?**\n\nA: The majority of our development work for EC2 Image Builder is included on this roadmap. Of course, there will be technologies we are very excited about that we are going to launch without notice to surprise and delight our customers.\n\n**Q: How can I provide feedback or ask for more information?**\n\nA: Please open an issue!\n\n**Q: How can I request a feature be added to the roadmap?**\n\nA: Please open an issue!  You can read about how to contribute [here](/CONTRIBUTING.md). Community submitted issues will be tagged *Proposed* and will be reviewed by the service team.\n\n**Q: Will you accept a pull request?**\n\nA: We haven't worked out how pull requests should work for a public roadmap page, but we will take all PRs very seriously and review for inclusion. Read about [contributing](/CONTRIBUTING.md).\n\n## License\n\nThis library is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License.\n\nTo learn more about EC2 Image Builder, head here: https://aws.amazon.com/image-builder/\n", "release_dates": []}, {"name": "ec2-macos-init", "description": "EC2 macOS Init is the launch daemon used to initialize Mac instances within EC2.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# EC2 macOS Init\n\n## Overview\n**EC2 macOS Init** is the launch daemon used to initialize Mac instances within EC2. It runs many tasks quickly and \nin parallel through the use of Priority Groups. Priority Groups are logical groupings of tasks which can be run \nat the same time without impacting each other. EC2 macOS Init will wait for all modules in a priority group to \ncomplete before moving on to the next group.\n\nImportant files for EC2 macOS Init are located in the following locations:\n\n* `/usr/local/aws/ec2-macos-init/init.toml` - The configuration file used when EC2 macOS Init is run.\n* `/usr/local/aws/ec2-macos-init/instances/<instance-id>/` - The location of all instance history (previous runs).\n* `/usr/local/bin/ec2-macos-init` - The EC2 macOS Init binary file.\n* `/Library/LaunchDaemons/com.amazon.ec2.macos-init.plist` - The `launchd` plist file used to trigger EC2 macOS Init to \nrun on boot.\n\n## Usage\nMost of the time, no interaction with EC2 macOS Init will be needed. It is automatically run on every boot by `launchd` \nusing the included `com.amazon.ec2.macos-init.plist` file. However, it can also be used interactively with the \nfollowing options:\n\n### Run\n```\nsudo ec2-macos-init run\n```\n\nThe `run` flag runs EC2 macOS Init using the current configuration located at `/usr/local/aws/ec2-macos-init/init.toml`. \nIf EC2 macOS Init has been previously run on the current instance, the instance history will be read and the current \nrun will be treated as a second boot (things may be skipped depending on their run type).\n\n### Clean\n```\nsudo ec2-macos-init clean (-all)\n```\n\nThe `clean` flag removes instance history located in the `/usr/local/aws/ec2-macos-init/instances/` directory. With no \narguments, it will only remove any history matching the current instance ID. If provided `-all`, it will remove all \ninstance history. This easily allows EC2 macOS Init to be re-run as though it were the first boot, something which is \nrecommended as a part of the process to generate a custom AMI from a currently running instance resulting in a \nclean history for the new AMI.\n\n### Version\n```\nsudo ec2-macos-init version\n```\n\nThe `version` flag returns the current version of EC2 macOS init as well as the date of the commit used to build the \nexecutable.\n\n## Init.toml Configuration Options\nEC2 macOS Init uses a single [TOML](https://toml.io/) file to configure boot options. These are divided into modules \nwhich can be added to any launch group and run in any order. Current modules and options include:\n\n### Common Options\nThe following options are available for all modules:\n\n* `Name` (`string`) - Required; This is a unique string used to identify the module both in logging and instance history.\n* `PriorityGroup` (`int`) - Required; An integer defining the priority group. Modules with the same Priority Group \nnumber will run in parallel. \n* `FatalOnError` (`bool`) - Optional; Fatal on error will halt the run at the current group and not continue to later \nPriority Groups. Defaults to `false`.\n\nAdditionally, all module configurations must contain exactly one of the following, set to `true`:\n\n* `RunOnce` (`bool`) - Required; Run this module only once, ever. Any history of a module with this set will prevent it \nfrom running again. Defaults to `false`.\n* `RunPerBoot` (`bool`) - Required; Run this module on every boot. Defaults to `false`.\n* `RunPerInstance` (`bool`) - Required; Run this module once per instance ID. Defaults to `false`.\n\n### Command\nThe `Command` module runs a single command. This can be used for a wide variety of tasks on launch. It should be noted \nthat any shell redirection will not work as anticipated as this is intended only for simple commands. In more complex \ncases, it's suggested to use this module to execute a shell script containing the required commands.\n\n* `Cmd` (`string array`) - Required; This is the command to be run. The first element should be the name of the \nexecutable and all following elements are arguments.\n* `RunAsUser` (`string`) - Optional; The user the command should be run as. Default is `root`.\n* `EnvironmentVars` (`[]string`) - Optional; A slice of environment variables in the form `key=value`. Default is \nempty.\n\t\n#### Example\n```toml\n[[Module]]\n  Name = \"Important-Init-Command\"\n  PriorityGroup = 4 # Fourth group\n  RunOnce = true # Run once, ever\n  FatalOnError = true # Stop running Init if there is an error \n  [Module.Command]\n    Cmd = [\"touch\", \"/tmp/file.txt\"] # A simple command\n    RunAsUser = \"ec2-user\" # Run as ec2-user\n    EnvironmentVars = [\"MY_KEY=myValue\"] # One environment variable named MY_KEY\n```\n\n### Network Check\nThe `NetworkCheck` module gets the default gateway and pings it to check if the network is up. This is useful as a \nway to gate subsequent modules which require network access (internet or IMDS).\n\n* `PingCount` (`int`) - Optional; The number of ping attempts to try against the default gateway. Default is `3`.\n\n#### Example\n```toml\n[[Module]]\n  Name = \"Network-Check\"\n  PriorityGroup = 1 # First group\n  RunPerBoot = true # Run every boot\n  FatalOnError = true # Fatal if there's an error - this must succeed\n  [Module.NetworkCheck]\n    PingCount = 6 # Six attempts\n```\n\n### SSH Keys\nThe `SSHKeys` module manages the `.ssh/authorized_keys` file on boot.  There are many options here, but it is primarily \nused to pull OpenSSH keys from IMDS on first launch.\n\n* `DedupKeys` (`bool`) - Optional; Enable deduplication of keys. This option will cause the entire `authorized_keys` \nfile for the user (default is `ec2-user`) to be read and all keys will be deduplicated. This is useful in preventing \nthe user's keys file from having many of the same key after multiple launches. Default is `false`.\n* `GetIMDSOpenSSHKey` (`bool`) - Optional; Get the OpenSSH key from IMDS, if provided. On launch of an EC2 instance, \nusers are offered the option to provide an EC2 Key Pair. This option will add that OpenSSH key to `authorized_keys`. \nDefault is `false`.\n* `StaticOpenSSHKeys` (`[]string`) - Optional; This option takes a string array of keys in SSH RSA public key \nformat (`ssh-rsa <material> <comment>`) and adds them to `authorized_keys`. Default is empty.\n* `OverwriteAuthorizedKeys` (`bool`) - Optional; Overwrite the `authorized_keys` file each time this module runs. \nThis can be useful in ensuring that old keys are removed every launch and replaced by new ones through either of the \nIMDS or static key options. Default is `false`.\n* `User` (`string`) - Optional; The owner of the `authorized_keys` file. Default is `ec2-user`.\n\n#### Example\n```toml\n[[Module]]\n  Name = \"Get-SSH-Keys\"\n  PriorityGroup = 3 # Third group\n  FatalOnError = true # Exit on failure - this is required to log in\n  RunPerInstance = true # Run only once per instance\n  [Module.SSHKeys]\n    GetIMDSOpenSSHKey = true # Get the key from IMDS\n    User = \"ec2-user\" # Apply the key to ec2-user\n    DedupKeys = true # Remove duplicate keys\n    OverwriteAuthorizedKeys = false # Append to authorized_keys to avoid erasing any additional keys on future instances\n```\n\n### Userdata\nThe `UserData` module pulls User Data from IMDS and provides the option to execute it. This is stored in a file at \n`/usr/local/aws/ec2-macos-init/instances/<instance-id>/userdata`. This can be useful for non-executables (like JSON) \n as well, by pulling the data from IMDS and making it immediately available without having to retrieve it directly.\n\n* `ExecuteUserData` (`bool`) - Optional; If set to `true`, Init will treat the userdata file as an executable and \nattempt to run it. Default is `false`.\n\n#### Example\n```toml\n[[Module]]\n  Name = \"Execute-User-Data\"\n  PriorityGroup = 4 # Fourth group\n  RunPerInstance = true # Run once per instance\n  FatalOnError = false # Best effort, don't fatal on error\n  [Module.UserData]\n    ExecuteUserData = true # Execute the userdata\n```\n\n### System Configuration\nThe `SystemConfig` module provides a few interfaces for setting system configuration parameters, primarily through \nthe use of `sysctl` and `defaults`.\n\n* `[Module.SystemConfig.Sysctl]` - Optional; Contains the value to be set by `sysctl`.\n    * `value` (`string`) - Required; The value in the form: `\"parameter=value\"`.\n* `[Module.SystemConfig.Defaults]` - Optional; Contains a parameter and value to be set by `defaults`.\n    * `plist` (`string`) - Required; The plist to containing the parameter to be set.\n    * `parameter` (`string`) - Required; The parameter to be updated.\n    * `type` (`string`) - Required; The type of parameter to be set. Currently, this can only be `\"bool\"`.\n    * `value` (`string`) - Required; The value to assign to the plist parameter.\n* `secureSSHDConfig` (`bool`) - Optional; Reapply the default SSHD config security settings after an OS update.\n\n#### Example\n```toml\n[[Module]]\n  Name = \"System-Configuration\"\n  PriorityGroup = 2 # Second group\n  RunPerBoot = true # Run every boot to enforce these parameters\n  FatalOnError = false # Best effort, don't fatal on error\n  [Module.SystemConfig]\n    secureSSHDConfig = true # secure sshd_config on OS update\n    [[Module.SystemConfig.Sysctl]]\n      value = \"my.favorite.parameter=42\" # use sysctl to set my.favorite.parameter\n    [[Module.SystemConfig.Defaults]]\n      plist = \"/Library/Preferences/com.amazon.ec2.plist\" # use defaults to set a parameter in this plist\n      parameter = \"PlistParameter\"\n      type = \"bool\"\n      value = \"false\"\n```\n\n\n### User Management\nThe `UserManagement` module provides the ability to safely randomize an existing user's password. \n\n* `User` (`string`) - Optional; The user (which must already exist) to manage. Default is `ec2-user`.\n* `RandomizePassword` (`bool`) - Optional; Configures whether the user's password should be randomized \n  on first boot. Default is `true`.\n  \n#### Example\n```toml\n[[Module]]\n  Name = \"ManageEC2User\"\n  PriorityGroup = 2 # Second group\n  RunOnce = true # Run only on the first boot\n  FatalOnError = true # Must succeed\n  [Module.UserManagement]\n    User = \"ec2-user\" # This user must exist locally in /Users/\n    RandomizePassword = true # default is true\n```\n\n## Building\n\nThe `build.sh` script has been provided for easy builds.  This script sets build-time variables, gets dependencies, \nand then builds the binary for `darwin/amd64`.  Once complete, the binary, launchd plist, and `init.toml` configuration \nfile need to be copied to the locations described in the Overview section of this README before testing.\n\n## Contributing\n\nPlease feel free to submit issues, fork the repository and send pull requests! \nSee [CONTRIBUTING](CONTRIBUTING.md) for more information.\n\n## Security\n\nSee the Security section of [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.", "release_dates": []}, {"name": "ec2-macos-system-monitor", "description": "Amazon EC2 System Monitor for macOS is the on instance agent that provides basic CloudWatch metrics like CPU Utilization. ", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon EC2 System Monitor for macOS\n\n\n## Overview\nAmazon EC2 System Monitor for macOS is a small agent that runs on every [mac1.metal](https://aws.amazon.com/ec2/instance-types/mac/)\ninstance to provide on-instance metrics in CloudWatch. Currently the primary use case for this agent is to send CPU utilization\nmetrics. This uses a serial connection attached via the [AWS Nitro System](https://aws.amazon.com/ec2/nitro/) \nand is forwarded to CloudWatch for the instance automatically. \n\n## Usage\nThe agent is installed and enabled by default for all AMIs vended by AWS. It logs to \n`/var/log/amazon/ec2/system-monitoring.log` and can be updated via [Homebrew](https://github.com/aws/homebrew-aws). \n\n### Managing the monitor with `setup-ec2monitoring`\nThe package includes a shell script for enabling, disabling, and listing the current status of the agent \naccording to `launchd`. \n\n### Viewing the agent status\nTo view the status of the agent:\n```bash\nsudo setup-ec2monitoring list\n```\n\n### Enabling the agent\nTo enable/install ec2-macos-system-monitor:\n```bash\nsudo setup-ec2monitoring enable\n```\nThis must be run if updating to a new version to ensure it is scheduled to run again. \n\n### Disabling the agent\nTo disable ec2-macos-system-monitor:\n```bash\nsudo setup-ec2monitoring disable\n```\n\n## Design\nThe Amazon EC2 System Monitor for macOS uses multiple goroutines to manage two primary mechanisms:\n1. The serial relay takes data from a UNIX domain socket and writes the data in a payload via a basic wire protocol.\n2. Runs a ticker that reads CPU utilization and sends the CPU usage percentage to the UNIX domain socket.\n\nThis design allows for multiple different processes to write to the serial device while allowing one process to\nalways have the device open for writing. \n\n### Wire protocol\nThe wire protocol's primary purpose is to ensure the payload is complete by wrapping the payload in a checksum.\nThere is a tag which is used as a namespace to ensure the reader knows what type of data is being written. The data\nitself which, in this case is typically the CPU utilization as a percentage of total usage the second field. Finally, a \nboolean is set specifying if the data should be compressed before sending. A checksum is then computed on this payload \nand included along with the payload. This payload with checksum allows the receiver to ensure that all the data was \ncorrectly received as well as inform if the data should be decompressed before parsing.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the [Apache License, version 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n\n", "release_dates": ["2021-02-27T01:10:18Z"]}, {"name": "ec2-macos-utils", "description": "Amazon EC2 macOS Utils provides common commands for Mac instances.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# EC2 macOS Utils\n\n## Overview\n\n**EC2 macOS Utils** is a CLI-based utility that provides commands for customizing AWS EC2 [Mac instances](https://aws.amazon.com/ec2/instance-types/mac/).\nCurrently, there exists one command (`grow`) for resizing volumes to their maximum size.\nThis is done by wrapping `diskutil(8)`, gathering disk information, and resizing the disk.\n\n## Usage\n\nThe utility will be installed by default for all AMIs vended by AWS after December 8, 2021.\n`ec2-macos-utils` is also available as a cask for install and updates via  [AWS' Homebrew tap](https://github.com/aws/homebrew-aws).\n\nSee the [ec2-macos-utils docs](docs/ec2-macos-utils.md) for more information.\n\n### Global Flags\n\nEC2 macOS Utils supports global flags that can be set with any command.\nThe supported global flags are as follows:\n* `--verbose` or `-v` this flag enables more detailed information to be outputted.\n\n### Growing APFS Containers\n\n```\nec2-macos-utils grow [flags]\n```\n\nThe `grow` command resizes an APFS container to its maximum size.\nThis is done by fetching all disk and system partition information, repairing the physical device to update partition information, calculating the amount of free space available, and resizing the container to its max size.\nRepairing the physical device is necessary in order to properly allocate the amount of available free space.\n\nThe `grow` command should be run with `sudo` as it requires root access in order to repair the physical disk.\n\nSee the [grow docs](docs/ec2-macos-utils_grow.md) for more information.\n\n## Building\n\n`ec2-macos-utils` can be built using the provided [Makefile](Makefile).\nThe default target compiles the executable with other targets provided for development & release activities.\nEach target has preset values that it uses but these can be overridden and tailored as needed.\n\n### Dependencies\n\nThis project requires the following build time dependencies:\n\n- Go\n- GNU Make\n\n### Build\n\n```shell\nmake build\n# or, alternatively use the default target to build:\nmake\n```\n\nThis builds the `ec2-macos-utils` binary.\n\n### Generate Docs\n\n```shell\nmake docs\n```\n\nThis builds the `ec2-macos-utils-docs` binary which can be used to generate the latest [command docs](docs).\n\n### Tests\n\n```shell\nmake test\n```\n\nThis runs a cover of all Go tests in the package.\n\n### Imports\n\n```shell\nmake imports\n```\n\nRun [`goimports`](https://pkg.go.dev/golang.org/x/tools/cmd/goimports) to reformat source files according to gofmt.\n\n## Contributing\n\nPlease feel free to submit issues, fork the repository and send pull requests!\nSee [CONTRIBUTING](CONTRIBUTING.md) for more information.\n\n## Security\n\nSee the Security section of [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.", "release_dates": ["2023-09-28T22:44:21Z", "2023-08-30T20:22:52Z", "2023-05-23T22:42:52Z", "2021-12-13T22:33:15Z"]}, {"name": "ec2-spot-instances-integrations-roadmap", "description": null, "language": null, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## EC2 Spot Instances integrations roadmap\n\nThis is the public roadmap for Amazon EC2 Spot Instances integrations (https://aws.amazon.com/ec2/spot/).\n\n## Introduction\nThis is the experimental public roadmap for Amazon EC2 Spot Instances integrations with open source software and frameworks.\nKnowing about our upcoming projects and priorities helps our customers plan. This repository contains information about what we are working on and allows all AWS customers to give direct feedback.\n\n[See the roadmap \u00bb](https://github.com/aws/ec2-spot-instances-integrations-roadmap/projects/1)\n\n**Other AWS Public Roadmaps**\n* [AWS Containers](https://github.com/aws/containers-roadmap)\n* [AWS App Mesh](https://github.com/aws/aws-app-mesh-roadmap)\n* [AWS CloudFormation coverage](https://github.com/aws-cloudformation/aws-cloudformation-coverage-roadmap)\n* [AWS Elastic Beanstalk](https://github.com/aws/elastic-beanstalk-roadmap)\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues. Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n## FAQs\n\n**Q: Why did you build this?**\n\nA: We know that our customers are making decisions and plans based on what we are developing, and we want to provide our customers the insights they need to plan.\n\n**Q: Why are there no dates on your roadmap?**\n\nA: Because job zero is security and operational stability, we can't provide specific target dates for features.\n\n**Q: What do the roadmap categories mean?**\n* *Just shipped* - ready to use!\n* *We're working on it* - in progress, but further out. We might still be working through the implementation details, or scoping stuff out.\n* *Researching* - We're thinking about it. This might mean we're still designing, or thinking through how this might work. This is a great phase to send how you want to see something implemented! We'd love to see your use case or design ideas here.\n\n**Q: Is everything on the roadmap?**\n\nA: The majority of our development work on AWS-sponsored OSS projects and third-party OSS projects are included on this roadmap. Of course, there will be technologies we are very excited about that we are going to launch without notice to surprise and delight our customers.\n\n**Q: How can I provide feedback or ask for more information?**\n\nA: Please open an issue!\n\n**Q: How can I request a feature be added to the roadmap?**\n\nA: Please open an issue! You can read about how to contribute here. Community submitted issues will be tagged \"Proposed\" and will be reviewed by the team.\n\n**Q: Will you accept a pull request?**\n\nA: We haven't worked out how pull requests should work for a public roadmap page, but we will take all PRs very seriously and review for inclusion. Read about contributing.\n\n\n## License Summary\n\nThe documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.\n\nThe sample code within this documentation is made available under the MIT-0 license. See the LICENSE-SAMPLECODE file.\n", "release_dates": []}, {"name": "efawin", "description": "This library provides the necessary interface for applications and libraries to interact with AWS Elastic Fabric Adapter on Windows.", "language": "C", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Efawin - Efa library for Windows\n\nEfawin library helps applications on Windows to interact with Elastic Fabric Adapter hardware on AWS EC2.\nIt provides the necessary functionality through an interface that is very close to \ninfiniband verbs interface. At this time, this library is scoped to support the libfabric\nefa provider that is part of the Open Fabrics Interfaces (OFI). See \n[the OFI web site](http://libfabric.org) for more information on libfabric.  Any other use of the library is not tested or supported.\n\n## Library functionality\n\nThis library provides the necessary functionality to create and manipulate the queues associated with AWS Elastic Fabric Adapter. Majority of the library's functionality comes from rdmacore's [efa provider](https://github.com/linux-rdma/rdma-core/tree/master/providers/efa). \nTo allow compilation of the library on Windows, some helper/compat files have been copied from the OFI's [libfabric](https://github.com/ofiwg/libfabric).\nThe Efa driver interaction files have been copied from a AWS fork of [libfabric](https://github.com/aws/libfabric/tree/v1.9.x-cdi).\nIn addition to these, the Infiniband header files from OpenIB are used to provide an interface to the library.\n\n## Building and installing Efawin from source\n\nEfawin can be built from a git clone or a zip file downloaded from github.\nEfawin requires Microsoft Visual Studio 2019 or higher to be compiled. This can be installed from \n[Microsoft's website](https://visualstudio.microsoft.com/downloads/).\n\nThe solution file `efawin.sln` can be opened in the IDE and built using the `Build` menu option.  \nYou can also build it using the command line tool `msbuild` from a Visual Studio developer \npowershell/command prompt.\n\nOnce built, the generated efawin.dll must be placed in the same folder as libfabric.dll for the\nefa provider in libfabric to load it during execution.\n\n## Using Efawin in your project\n\nTo use Efawin in your project, copy the contents of interface folder into your project and use `efa_load_efawin_lib` function to load the dll and `efa_free_efawin_lib` function to free the dll.  The supported list of `ibv_*` functions can be found in `interface\\efawinver.h`. `interface\\infiniband\\verbs.h` will provide the function declarations for your application.\n\n## Runtime considerations\n\nThe version of efawin dll used must be compatible with the installed efa driver version.  Efawin dll will return a failure when it cannot interact with a compatible version of efa driver.\nEfawin dll uses `EFA_API_INTERFACE_VERSION` from `efaioctl.h` when interacting with the driver.\nIf you have an incompatible driver, please update your efa driver to match the EFA_API_INTERFACE_VERSION as listed below.\n\nDriver version compatibility is as listed below:\n- EFA_API_INTERFACE_VERSION 1 is compatible with efa.sys version 1.0.0.4\n- EFA_API_INTERFACE_VERSION 2 is compatible with efa.sys version 1.1.0.9\n\nEFA driver for Windows can be installed using the AWS public release S3 bucket: https://ec2-windows-drivers-efa.s3-us-west-2.amazonaws.com/Latest/EFADriver.zip\n\n## Validate installation\n\nThere is no separate test for efawin installation at this time.\nThe fi_info utility from libfabric can be used to validate efawin installation.  It will return the efa fabric attributes on successful installation.\nSee [libfabric validation](https://github.com/ofiwg/libfabric#validate-installation) for details.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nUnless otherwise stated in individual source, this software is available under one of two licenses you may choose from: [GPL-2.0](LICENSE-GPL-2.0) OR\n[OpenIB.org BSD license](LICENSE-Linux-OpenIB)\n", "release_dates": ["2021-11-19T23:22:49Z"]}, {"name": "efs-utils", "description": "Utilities for Amazon Elastic File System (EFS)", "language": "Python", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "[![CircleCI](https://circleci.com/gh/aws/efs-utils.svg?style=svg)](https://circleci.com/gh/aws/efs-utils)\n\n# efs-utils\n\nUtilities for Amazon Elastic File System (EFS)\n\nThe `efs-utils` package has been verified against the following Linux distributions:\n\n| Distribution         | Package Type | `init` System |\n|----------------------| ----- | --------- |\n| Amazon Linux 2017.09 | `rpm` | `upstart` |\n| Amazon Linux 2       | `rpm` | `systemd` |\n| Amazon Linux 2023    | `rpm` | `systemd` |\n| CentOS 7             | `rpm` | `systemd` |\n| CentOS 8             | `rpm` | `systemd` |\n| RHEL 7               | `rpm`| `systemd` |\n| RHEL 8               | `rpm`| `systemd` |\n| Fedora 28            | `rpm` | `systemd` |\n| Fedora 29            | `rpm` | `systemd` |\n| Fedora 30            | `rpm` | `systemd` |\n| Fedora 31            | `rpm` | `systemd` |\n| Fedora 32            | `rpm` | `systemd` |\n| Debian 9             | `deb` | `systemd` |\n| Debian 10            | `deb` | `systemd` |\n| Ubuntu 16.04         | `deb` | `systemd` |\n| Ubuntu 18.04         | `deb` | `systemd` |\n| Ubuntu 20.04         | `deb` | `systemd` |\n| OpenSUSE Leap        | `rpm` | `systemd` |\n| OpenSUSE Tumbleweed  | `rpm` | `systemd` |\n| Oracle8              | `rpm` | `systemd` |\n| SLES 12              | `rpm` | `systemd` |\n| SLES 15              | `rpm` | `systemd` |\n\nThe `efs-utils` package has been verified against the following MacOS distributions:\n\n| Distribution   | `init` System |\n|----------------|---------------|\n| MacOS Big Sur  | `launchd`     |\n| MacOS Monterey | `launchd`     |\n| MacOS Ventura  | `launchd`     |\n| MacOS Sonoma   | `launchd`     |\n\n## README contents\n  - [Prerequisites](#prerequisites)\n  - [Optional](#optional)\n  - [Installation](#installation)\n    - [On Amazon Linux distributions](#on-amazon-linux-distributions)\n    - [Install via AWS Systems Manager Distributor](#install-via-aws-systems-manager-distributor)\n    - [On other Linux distributions](#on-other-linux-distributions)\n    - [On MacOS Big Sur, macOS Monterey and macOS Ventura distribution](#on-macos-big-sur-macos-monterey-and-macos-ventura-distribution)\n      - [Run tests](#run-tests)\n  - [Usage](#usage)\n    - [mount.efs](#mountefs)\n    - [MacOS](#macos)\n    - [amazon-efs-mount-watchdog](#amazon-efs-mount-watchdog)\n  - [Troubleshooting](#troubleshooting)\n  - [Upgrading stunnel for RHEL/CentOS](#upgrading-stunnel-for-rhelcentos)\n  - [Upgrading stunnel for SLES12](#upgrading-stunnel-for-sles12)\n  - [Upgrading stunnel for MacOS](#upgrading-stunnel-for-macos)\n  - [Install botocore](#install-botocore)\n      - [RPM](#rpm)\n      - [DEB](#deb)\n      - [On Debian10 and Ubuntu20, the botocore needs to be installed in specific target folder](#on-debian10-and-ubuntu20-the-botocore-needs-to-be-installed-in-specific-target-folder)\n      - [To install botocore on MacOS](#to-install-botocore-on-macos)\n  - [Upgrade botocore](#upgrade-botocore)\n  - [Enable mount success/failure notification via CloudWatch log](#enable-mount-successfailure-notification-via-cloudwatch-log)\n    - [Step 1. Install botocore](#step-1-install-botocore)\n    - [Step 2. Enable CloudWatch log feature in efs-utils config file `/etc/amazon/efs/efs-utils.conf`](#step-2-enable-cloudwatch-log-feature-in-efs-utils-config-file-etcamazonefsefs-utilsconf)\n    - [Step 3. Attach the CloudWatch logs policy to the IAM role attached to instance.](#step-3-attach-the-cloudwatch-logs-policy-to-the-iam-role-attached-to-instance)\n  - [Optimize readahead max window size on Linux 5.4+](#optimize-readahead-max-window-size-on-linux-54)\n  - [Using botocore to retrieve mount target ip address when dns name cannot be resolved](#using-botocore-to-retrieve-mount-target-ip-address-when-dns-name-cannot-be-resolved)\n    - [Step 1. Install botocore](#step-1-install-botocore-1)\n    - [Step 2. Allow DescribeMountTargets and DescribeAvailabilityZones action in the IAM policy](#step-2-allow-describemounttargets-and-describeavailabilityzones-action-in-the-iam-policy)\n  - [The way to access instance metadata](#the-way-to-access-instance-metadata)\n  - [Use the assumed profile credentials for IAM](#use-the-assumed-profile-credentials-for-iam)\n  - [Enabling FIPS Mode](#enabling-fips-mode)\n  - [License Summary](#license-summary)\n\n\n## Prerequisites\n\n* `nfs-utils` (RHEL/CentOS/Amazon Linux/Fedora) or `nfs-common` (Debian/Ubuntu)\n* OpenSSL 1.0.2+\n* Python 3.4+\n* `stunnel` 4.56+\n\n## Optional\n\n* `botocore` 1.12.0+\n\n## Installation\n\n### On Amazon Linux distributions\n\nFor those using Amazon Linux or Amazon Linux 2, the easiest way to install `efs-utils` is from Amazon's repositories:\n\n```bash\n$ sudo yum -y install amazon-efs-utils\n```\n\n### Install via AWS Systems Manager Distributor\nYou can now use AWS Systems Manage Distributor to automatically install or update `amazon-efs-utils`. \nPlease refer to [Using AWS Systems Manager to automatically install or update Amazon EFS clients](https://docs.aws.amazon.com/efs/latest/ug/manage-efs-utils-with-aws-sys-manager.html) for more guidance.\n\nThe following are prerequisites for using AWS Systems Manager Distributor to install or update `amazon-efs-utils`:\n\n1. AWS Systems Manager agent is installed on the distribution (For `Amazon Linux` and `Ubuntu`, AWS Systems Manager agent\nis pre-installed, for other distributions, please refer to [install AWS Systems Manager agent on Linux EC2 instance](https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-install-ssm-agent.html)\nfor more guidance.)\n\n2. Instance is attached with IAM role with AWS managed policy `AmazonElasticFileSystemsUtils`, this policy will enable your instance to be managed by\n AWS Systems Manager agent, also it contains permissions to support specific features.\n\n### On other Linux distributions\n\nOther distributions require building the package from source and installing it.\n\n- To build and install an RPM:\n\nIf the distribution is not OpenSUSE or SLES\n\n```bash\n$ sudo yum -y install git rpm-build make\n$ git clone https://github.com/aws/efs-utils\n$ cd efs-utils\n$ make rpm\n$ sudo yum -y install build/amazon-efs-utils*rpm\n```\n\nOtherwise\n\n```bash\n$ sudo zypper refresh\n$ sudo zypper install -y git rpm-build make\n$ git clone https://github.com/aws/efs-utils\n$ cd efs-utils\n$ make rpm\n$ sudo zypper --no-gpg-checks install -y build/amazon-efs-utils*rpm\n```\n\nOn OpenSUSE, if you see error like `File './suse/noarch/bash-completion-2.11-2.1.noarch.rpm' not found on medium 'http://download.opensuse.org/tumbleweed/repo/oss/'`\nduring installation of `git`, run the following commands to re-add repo OSS and NON-OSS, then run the install script above again.\n\n```bash\nsudo zypper ar -f -n OSS http://download.opensuse.org/tumbleweed/repo/oss/ OSS\nsudo zypper ar -f -n NON-OSS http://download.opensuse.org/tumbleweed/repo/non-oss/ NON-OSS\nsudo zypper refresh\n```\n\n- To build and install a Debian package:\n\n```bash\n$ sudo apt-get update\n$ sudo apt-get -y install git binutils\n$ git clone https://github.com/aws/efs-utils\n$ cd efs-utils\n$ ./build-deb.sh\n$ sudo apt-get -y install ./build/amazon-efs-utils*deb\n```\n\n### On MacOS Big Sur, macOS Monterey, macOS Sonoma and macOS Ventura distribution\n\nFor EC2 Mac instances running macOS Big Sur, macOS Monterey, macOS Sonoma and macOS Ventura, you can install amazon-efs-utils from the \n[homebrew-aws](https://github.com/aws/homebrew-aws) respository. **Note that this will ONLY work on EC2 instances\nrunning macOS Big Sur, macOS Monterey, macOS Sonoma and macOS Ventura, not local Mac computers.**\n```bash\nbrew install amazon-efs-utils\n```\n\nThis will install amazon-efs-utils on your EC2 Mac Instance running macOS Big Sur, macOS Monterey and macOS Ventura in the directory `/usr/local/Cellar/amazon-efs-utils`. \n  \t\t  \n***Follow the instructions in caveats when using efs-utils on EC2 Mac instance for the first time.*** To check the package caveats run below command\n```bash\nbrew info amazon-efs-utils\n```\n\n#### Run tests\n\n- [Set up a virtualenv](http://libzx.so/main/learning/2016/03/13/best-practice-for-virtualenv-and-git-repos.html) for efs-utils\n\n```bash\n$ virtualenv ~/.envs/efs-utils\n$ source ~/.envs/efs-utils/bin/activate\n$ pip install -r requirements.txt\n```\n\n- Run tests\n\n```bash\n$ make test\n```\n\n## Usage\n\n### mount.efs\n\n`efs-utils` includes a mount helper utility to simplify mounting and using EFS file systems.\n\nTo mount with the recommended default options, simply run:\n\n```bash\n$ sudo mount -t efs file-system-id efs-mount-point/\n```\n\nTo mount file system to a specific mount target of the file system, run:\n\n```bash\n$ sudo mount -t efs -o mounttargetip=mount-target-ip-address file-system-id efs-mount-point/\n```\n\nTo mount file system within a given network namespace, run:\n\n```bash\n$ sudo mount -t efs -o netns=netns-path file-system-id efs-mount-point/\n```\n\nTo mount file system to the mount target in specific availability zone (e.g. us-east-1a), run:\n\n```bash\n$ sudo mount -t efs -o az=az-name file-system-id efs-mount-point/\n```\n\nTo mount over TLS, simply add the `tls` option:\n\n```bash\n$ sudo mount -t efs -o tls file-system-id efs-mount-point/\n```\n\nTo authenticate with EFS using the system\u2019s IAM identity, add the `iam` option. This option requires the `tls` option.\n\n```bash\n$ sudo mount -t efs -o tls,iam file-system-id efs-mount-point/\n```\n\nTo mount using an access point, use the `accesspoint=` option. This option requires the `tls` option.\nThe access point must be in the \"available\" state before it can be used to mount EFS.\n\n```bash\n$ sudo mount -t efs -o tls,accesspoint=access-point-id file-system-id efs-mount-point/\n```\n\nTo mount your file system automatically with any of the options above, you can add entries to `/efs/fstab` like:\n\n```bash\nfile-system-id efs-mount-point efs _netdev,tls,iam,accesspoint=access-point-id 0 0\n```\n\nFor more information on mounting with the mount helper, see the manual page:\n\n```bash\nman mount.efs\n```\n\nor refer to the [documentation](https://docs.aws.amazon.com/efs/latest/ug/using-amazon-efs-utils.html).\n\n### MacOS \n\nFor EC2 instances using Mac distribution, the recommended default options will perform a tls mount:\n\n```bash\n$ sudo mount -t efs file-system-id efs-mount-point/\n```\n or\n```bash\n$ sudo mount -t efs -o tls file-system-id efs-mount-point/\n```\n\nTo mount without TLS, simply add the `notls` option:\n\n```bash\n$ sudo mount -t efs -o notls file-system-id efs-mount-point/\n```\n\n\n### amazon-efs-mount-watchdog\n\n`efs-utils` contains a watchdog process to monitor the health of TLS mounts. This process is managed by either `upstart` or `systemd` depending on your Linux distribution and `launchd` on Mac distribution, and is started automatically the first time an EFS file system is mounted over TLS.\n\n## Troubleshooting\nIf you run into a problem with efs-utils, please open an issue in this repository.  We can more easily\nassist you if relevant logs are provided.  You can find the log file at `/var/log/amazon/efs/mount.log`.  \n\nOften times, enabling debug level logging can help us find problems more easily.  To do this, run  \n`sed -i '/logging_level = INFO/s//logging_level = DEBUG/g' /etc/amazon/efs/efs-utils.conf`.  \n\nYou can also enable stunnel debug logs with  \n`sed -i '/stunnel_debug_enabled = false/s//stunnel_debug_enabled = true/g' /etc/amazon/efs/efs-utils.conf`.   \n\nMake sure to perform the failed mount again after running the prior commands before pulling the logs.\n\n## Upgrading stunnel for RHEL/CentOS\n\nBy default, when using the EFS mount helper with TLS, it enforces certificate hostname checking. The EFS mount helper uses the `stunnel` program for its TLS functionality. Please note that some versions of Linux do not include a version of `stunnel` that supports TLS features by default. When using such a Linux version, mounting an EFS file system using TLS will fail. \n\nOnce you\u2019ve installed the `amazon-efs-utils` package, to upgrade your system\u2019s version of `stunnel`, see [Upgrading Stunnel](https://docs.aws.amazon.com/efs/latest/ug/using-amazon-efs-utils.html#upgrading-stunnel).\n\n## Upgrading stunnel for SLES12\n\nRun the following commands and follow the output hint of zypper package manager to upgrade the stunnel on your SLES12 instance\n\n```bash\nsudo zypper addrepo https://download.opensuse.org/repositories/security:Stunnel/SLE_12_SP5/security:Stunnel.repo\nsudo zypper refresh\nsudo zypper install -y stunnel\n```\n\n## Upgrading stunnel for MacOS\n\nThe installation installs latest stunnel available in brew repository. You can also upgrade the version of stunnel on your instance using the command below:\n```bash\nbrew upgrade stunnel\n```\n\n## Install botocore\n\n`efs-utils` uses botocore to interact with other AWS services. Please note the package type from the above table and install\nbotocore based on that info. If botocore is already installed and does not meet the minimum required version, \nyou can upgrade the botocore by following the [upgrade botocore section](#Upgrade-botocore).\n \n- Download the `get-pip.py` script\n#### RPM\n```bash\nsudo yum -y install wget\n```\n```bash\nif [[ \"$(python3 -V 2>&1)\" =~ ^(Python 3.6.*) ]]; then\n    sudo wget https://bootstrap.pypa.io/pip/3.6/get-pip.py -O /tmp/get-pip.py\nelif [[ \"$(python3 -V 2>&1)\" =~ ^(Python 3.5.*) ]]; then\n    sudo wget https://bootstrap.pypa.io/pip/3.5/get-pip.py -O /tmp/get-pip.py\nelif [[ \"$(python3 -V 2>&1)\" =~ ^(Python 3.4.*) ]]; then\n    sudo wget https://bootstrap.pypa.io/pip/3.4/get-pip.py -O /tmp/get-pip.py\nelse\n    sudo wget https://bootstrap.pypa.io/get-pip.py -O /tmp/get-pip.py\nfi\n```\n#### DEB\n```bash\nsudo apt-get update\nsudo apt-get -y install wget\n```\n```bash\nif echo $(python3 -V 2>&1) | grep -e \"Python 3.6\"; then\n    sudo wget https://bootstrap.pypa.io/pip/3.6/get-pip.py -O /tmp/get-pip.py\nelif echo $(python3 -V 2>&1) | grep -e \"Python 3.5\"; then\n    sudo wget https://bootstrap.pypa.io/pip/3.5/get-pip.py -O /tmp/get-pip.py\nelif echo $(python3 -V 2>&1) | grep -e \"Python 3.4\"; then\n    sudo wget https://bootstrap.pypa.io/pip/3.4/get-pip.py -O /tmp/get-pip.py\nelse\n    sudo apt-get -y install python3-distutils\n    sudo wget https://bootstrap.pypa.io/get-pip.py -O /tmp/get-pip.py\nfi\n```\n\n- To install botocore on RPM\n```bash\nsudo python3 /tmp/get-pip.py\nsudo pip3 install botocore || sudo /usr/local/bin/pip3 install botocore\n```\n\n- To install botocore on DEB\n```bash\nsudo python3 /tmp/get-pip.py\nsudo pip3 install botocore || sudo /usr/local/bin/pip3 install botocore\n```\n\n#### On Debian10 and Ubuntu20, the botocore needs to be installed in specific target folder\n```bash\nsudo python3 /tmp/get-pip.py\nsudo pip3 install --target /usr/lib/python3/dist-packages botocore || sudo /usr/local/bin/pip3 install --target /usr/lib/python3/dist-packages botocore\n```\n\n#### To install botocore on MacOS\n```bash\nsudo pip3 install botocore\n```\n\n## Upgrade botocore\nPass `--upgrade` to the corresponding installation scripts above based on system platform and distribution\n\n```bash\nsudo pip3 install botocore --upgrade\n```\n\n## Enable mount success/failure notification via CloudWatch log\n`efs-utils` now support publishing mount success/failure logs to CloudWatch log. By default, this feature is disabled. There are three\nsteps you must follow to enable and use this feature:\n\n### Step 1. Install botocore\nFollow [install botocore section](#Install-botocore)\n\n### Step 2. Enable CloudWatch log feature in efs-utils config file `/etc/amazon/efs/efs-utils.conf`\n```bash\nsudo sed -i -e '/\\[cloudwatch-log\\]/{N;s/# enabled = true/enabled = true/}' /etc/amazon/efs/efs-utils.conf\n```\n\n- For MacOS:\n```bash\n    EFS_UTILS_VERSION=<e.g. 1.34.5>\n    sudo sed -i -e '/\\[cloudwatch-log\\]/{N;s/# enabled = true/enabled = true/;}' /usr/local/Cellar/amazon-efs-utils/${EFS_UTILS_VERSION}/libexec/etc/amazon/efs/efs-utils.conf\n```\n- For Mac2 instance:\n```bash\n    EFS_UTILS_VERSION=<e.g. 1.34.5>\n    sudo sed -i -e '/\\[cloudwatch-log\\]/{N;s/# enabled = true/enabled = true/;}' /opt/homebrew/Cellar/amazon-efs-utils/${EFS_UTILS_VERSION}/libexec/etc/amazon/efs/efs-utils.conf\n```\nYou can also configure CloudWatch log group name and log retention days in the config file.\nIf you want to have separate log groups in Cloudwatch for every mounted file system, add `/{fs_id}` to the end of the `log_group_name` field in `efs-utils.conf` file. For example, the `log_group_name` in `efs-utils.conf` file would look something like:\n\n```bash\n[cloudwatch-log]\nlog_group_name = /aws/efs/utils/{fs_id}\n```\n### Step 3. Attach the CloudWatch logs policy to the IAM role attached to instance.\nAttach AWS managed policy `AmazonElasticFileSystemsUtils` to the iam role you attached to the instance, or the aws credentials\nconfigured on your instance.\n\nAfter completing the three prerequisite steps, you will be able to see mount status notifications in CloudWatch Logs.\n\n## Optimize readahead max window size on Linux 5.4+\n\nA change in the Linux kernel 5.4+ results a throughput regression on NFS client. With [patch](https://www.spinics.net/lists/linux-nfs/msg75018.html), starting from 5.4.\\*, Kernels containing this patch now set the default read_ahead_kb size to 128 KB instead of the previous 15 MB. This read_ahead_kb is used by the Linux kernel to optimize performance on NFS read requests by defining the maximum amount of data an NFS client can pre-fetch in a read call. With the reduced value, an NFS client has to make more read calls to the file system, resulting in reduced performance.\n\nTo avoid above throughput regression, efs-utils will modify read_ahead_kb to 15 \\* rsize (could be configured via mount option, 1MB by default) after mount success on Linux 5.4+. (not support on MacOS)\n\nThis optimization will be enabled by default. To disable this optimization:\n\n```bash\nsed -i \"s/optimize_readahead = false/optimize_readahead = true/\" /etc/amazon/efs/efs-utils.conf\n```\n\nTo re-enable this optimization\n\n```bash\nsed -i \"s/optimize_readahead = true/optimize_readahead = false/\" /etc/amazon/efs/efs-utils.conf\n```\n\nYou can mount file system with a given rsize, run:\n\n```bash\n$ sudo mount -t efs -o rsize=rsize-value-in-bytes file-system-id efs-mount-point/\n```\n\nYou can also manually chose a value of read_ahead_kb to optimize read throughput on Linux 5.4+ after mount.\n\n```bash\n$ sudo bash -c \"echo read-ahead-value-in-kb > /sys/class/bdi/0:$(stat -c '%d' efs-mount-point)/read_ahead_kb\"\n```\n\n## Using botocore to retrieve mount target ip address when dns name cannot be resolved\n\n`efs-utils` now supports using botocore to retrieve mount target ip address when dns name cannot be resolved, e.g. \nwhen user is mounting a file system in another VPC. There are two prerequisites to use this feature:\n\n### Step 1. Install botocore\nFollow [install botocore section](#Install-botocore)\n\n### Step 2. Allow DescribeMountTargets and DescribeAvailabilityZones action in the IAM policy\nAllow the `elasticfilesystem:DescribeMountTargets` and `ec2:DescribeAvailabilityZones` action in your policy attached to \nthe iam role you attached to the instance, or the aws credentials configured on your instance. We recommend you attach \nAWS managed policy `AmazonElasticFileSystemsUtils`.\n\nThis feature will be enabled by default. To disable this feature:\n\n```bash\nsed -i \"s/fall_back_to_mount_target_ip_address_enabled = true/fall_back_to_mount_target_ip_address_enabled = false/\" /etc/amazon/efs/efs-utils.conf\n```\n\nIf you decide that you do not want to use this feature, but need to mount a cross-VPC file system, you can use the mounttargetip \noption to do so, using the desired mount target ip address in the mount command.\n\n## The way to access instance metadata\n`efs-utils` by default uses IMDSv2, which is a session-oriented method used to access instance metadata. If you don't want to use \nIMDSv2, you can disable the token fetching feature by running the following command:\n\n```bash\nsed -i \"s/disable_fetch_ec2_metadata_token = false/disable_fetch_ec2_metadata_token = true/\" /etc/amazon/efs/efs-utils.conf\n```\n\n## Use the assumed profile credentials for IAM\nTo authenticate with EFS using the system\u2019s IAM identity of an awsprofile, add the `iam` option and pass the profile name to \n`awsprofile` option. These options require the `tls` option.\n\n```bash\n$ sudo mount -t efs -o tls,iam,awsprofile=test-profile file-system-id efs-mount-point/\n```\n\nTo configure the named profile, see the [Named Profiles doc](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html)\nand [Support Config File Settings doc](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html#cli-configure-files-settings)\nfor more details. If the credentials (e.g. aws_access_key_id) are not configured in `/root/.aws/credentials` or `/root/.aws/config` \n(note that the path prefix may vary based on the root path of sudo), efs-utils will use botocore to assume the named profile. \nThis will require botocore is pre-installed, please follow [install botocore section](#Install-botocore) to install botocore first.\n\nNormally you will need to configure your profile IAM policy to make the assume works. For example, if you want to perform a\ncross-account mounting, suppose you have established \n[vpc-peering-connections](https://docs.aws.amazon.com/vpc/latest/peering/create-vpc-peering-connection.html) between your vpcs,\nnext step you need to do is giving permission to account B so that it can assume a role in account A and then mount the file system \nthat belongs to account A. You can see \n[IAM doc](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html) for more details.\n\nAfter the IAM identity is setup, you can configure your awsprofile credentials or config. You can refer to \n[sdk settings](https://docs.aws.amazon.com/sdkref/latest/guide/settings-global.html). For example you can define\nthe profile to use the credentials of profile `default` to assume role in account A by defining the `source_profile`.\n\n```bash\n# /root/.aws/credentials\n[default]\naws_access_key_id = AKIAIOSFODNN7EXAMPLE\naws_secret_access_key_id =wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n# /root/.aws/config\n[default]\n...\n\n[profile test-profile]\nrole_arn = <role-arn-in-account-A>\nsource_profile = default\n```\n\nOr you can use the credentials from IAM role attached to instance to assume the named profile, e.g.\n\n```bash\n# /root/.aws/config\n[profile test-profile]\nrole_arn = <role-arn-in-account-A>\ncredential_source = Ec2InstanceMetadata\n```\n\n## Use AssumeRoleWithWebIdentity\n\nYou can use [web identity to assume a role](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html) which has the permission to attach to the EFS filesystem. You need to have a valid JWT token and a role arn to assume. There are two ways you can leverage them:\n\n1) By setting environment variable the path to the file containing the JWT token in `AWS_WEB_IDENTITY_TOKEN_FILE` and by setting `ROLE_ARN` environment variable. The command below shows an example of to leverage it.\n\n```bash\n$ sudo mount -t efs -o tls,iam file-system-id efs-mount-point/\n```\n\n2) By passing the JWT token file path and the role arn as parameters to the mount command. The command below shows an example of to leverage it.\n\n```bash\n$ sudo mount -t efs -o tls,iam,rolearn=\"ROLE_ARN\",jwtpath=\"PATH/JWT_TOKEN_FILE\" file-system-id efs-mount-point/\n```\n\n## Enabling FIPS Mode\nEfs-Utils is able to enter FIPS mode when mounting your file system. To enable FIPS you need to modify the EFS-Utils config file:\n```bash\nsed -i \"s/fips_mode_enabled = false/fips_mode_enabled = true/\" /etc/amazon/efs/efs-utils.conf\n```\nThis will enable any potential API call from EFS-Utils to use FIPS endpoints and cause stunnel to enter FIPS mode \n\nNote: FIPS mode requires that the installed version of OpenSSL is compiled with FIPS.\n\nTo verify that the installed version is compiled with FIPS, look for `OpenSSL X.X.Xx-fips` in the `stunnel -version` command output e.g.\n```bash\n$ stunnel -version\nstunnel 4.56 on x86_64-koji-linux-gnu platform\nCompiled/running with OpenSSL 1.0.2k-fips  26 Jan 2017\nThreading:PTHREAD Sockets:POLL,IPv6 SSL:ENGINE,OCSP,FIPS Auth:LIBWRAP\n```\n\nFor more information on how to configure OpenSSL with FIPS see the [OpenSSL FIPS README](https://github.com/openssl/openssl/blob/master/README-FIPS.md).\n\n## License Summary\n\nThis code is made available under the MIT license.\n", "release_dates": []}, {"name": "eks-anywhere", "description": "Run Amazon EKS on your own infrastructure \ud83d\ude80", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon EKS Anywhere\n[![Release](https://img.shields.io/github/v/release/aws/eks-anywhere.svg?logo=github&color=green)](https://github.com/aws/eks-anywhere/releases)\n[![Downloads](https://img.shields.io/github/downloads/aws/eks-anywhere/total.svg?color=brown)](https://github.com/aws/eks-anywhere/releases)\n[![Go Report Card](https://goreportcard.com/badge/github.com/aws/eks-anywhere)](https://goreportcard.com/report/github.com/aws/eks-anywhere)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/6109/badge)](https://bestpractices.coreinfrastructure.org/projects/6109)\n[![Contributors](https://img.shields.io/github/contributors/aws/eks-anywhere?color=purple)](CONTRIBUTING.md)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)\n\n**Build status:** ![Build status](https://codebuild.us-west-2.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiVW5aOStkOUtKbW1zZGE2eVJxdzBzTTdRMmh2M1JEMFZtdXJxSnRoSEltcEV1N3B0elR6MWNZbVU3RkJYdWlZeFZ0NWpNQWdxZi9CUFUyQ1plbGEyS3BzPSIsIml2UGFyYW1ldGVyU3BlYyI6IjFFNytHMTNiVGI1elkxYUMiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=main)\n\n**Conformance test status:** ![Conformance test status](https://codebuild.us-west-2.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiV2dmeFNqWEJORzUreTdwQzZMc2JaYVZQUDMvaElkNnFwbHdsVkEwV3VlVTJRUDhyRU1DVWtXTTNqMCtSMWU5ZFhJRk03aTR5ZGgxYXBMS0JVcllwMlpRPSIsIml2UGFyYW1ldGVyU3BlYyI6IlBuSWFpMGhBZ2lDbUVGMTYiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=main)\n\nAmazon EKS Anywhere is a new deployment option for Amazon EKS that enables you to easily create and operate Kubernetes clusters on-premises with your own virtual machines or bare metal hosts.\nIt brings a consistent AWS management experience to your data center, building on the strengths of [Amazon EKS Distro](https://github.com/aws/eks-distro), the same distribution of Kubernetes that powers EKS on AWS.\nIts goal is to include full lifecycle management of multiple Kubernetes clusters that are capable of operating completely independently of any AWS services.\n\nHere are the steps for [getting started](https://anywhere.eks.amazonaws.com/docs/getting-started/) with EKS Anywhere.\nFull documentation for releases can be found on [https://anywhere.eks.amazonaws.com](https://anywhere.eks.amazonaws.com/).\n\n<!-- \nSource: https://github.com/cncf/artwork/tree/master/projects/kubernetes/certified-kubernetes\n-->\n[<img src=\"docs/static/images/certified-kubernetes-1.25-pantone.svg\" height=150>](https://github.com/cncf/k8s-conformance/pull/2454)\n[<img src=\"docs/static/images/certified-kubernetes-1.26-pantone.svg\" height=150>](https://github.com/cncf/k8s-conformance/pull/2531)\n[<img src=\"docs/static/images/certified-kubernetes-1.27-pantone.svg\" height=150>](https://github.com/cncf/k8s-conformance/pull/2636)\n\n## Development\n\nThe EKS Anywhere is tested using\n[Prow](https://github.com/kubernetes/test-infra/tree/master/prow), the Kubernetes CI system.\nEKS operates an installation of Prow, which is visible at [https://prow.eks.amazonaws.com/](https://prow.eks.amazonaws.com/).\nPlease read our [CONTRIBUTING](CONTRIBUTING.md) guide before making a pull request.\nRefer to our [end to end guide](https://github.com/aws/eks-anywhere/tree/main/test/e2e) to run E2E tests locally.\n\nThe dependencies which make up EKS Anywhere are defined and built via the [build-tooling](https://github.com/aws/eks-anywhere-build-tooling) repo.\nTo update dependencies please review the Readme for the specific dependency before opening a PR.\n\nSee [Cherry picking](./docs/developer/cherry-picks.md) for backporting to release branches\n\n## Security\n\nIf you discover a potential security issue in this project, or think you may\nhave discovered a security issue, we ask that you notify AWS Security via our\n[vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public GitHub issue.\n\n## License\n\nThis project is licensed under the [Apache-2.0 License](LICENSE).\n", "release_dates": ["2024-02-29T23:55:09Z", "2024-02-29T23:45:35Z", "2024-02-22T23:43:27Z", "2024-02-17T00:32:48Z", "2024-02-02T22:21:09Z", "2024-01-26T18:58:47Z", "2024-01-16T18:12:41Z", "2024-01-04T23:44:17Z", "2023-12-28T23:45:12Z", "2023-12-20T03:56:36Z", "2023-12-11T21:58:37Z", "2023-11-24T07:24:10Z", "2023-11-13T21:00:52Z", "2023-11-09T18:08:53Z", "2023-10-30T23:00:55Z", "2023-10-26T23:59:31Z", "2023-10-16T17:18:27Z", "2023-09-28T22:58:55Z", "2023-09-12T18:50:40Z", "2023-09-02T00:11:28Z", "2023-08-24T23:52:45Z", "2023-08-17T23:51:17Z", "2023-08-16T18:16:36Z", "2023-08-14T23:41:42Z", "2023-08-10T22:08:18Z", "2023-08-03T21:51:37Z", "2023-07-10T20:57:00Z", "2023-07-06T23:47:13Z", "2023-06-22T21:48:52Z", "2023-06-15T23:49:02Z"]}, {"name": "eks-anywhere-build-tooling", "description": "Build artifacts for upstream dependencies of Amazon EKS Anywhere - https://github.com/aws/eks-anywhere", "language": "Makefile", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon EKS Anywhere Build Tooling Repository\n\nThe EKS Anywhere Build Tooling repository contains the code to build artifacts corresponding to the various upstream dependency projects of [Amazon EKS Anywhere](https://github.com/aws/eks-anywhere). The build artifacts include container images, binary archives, and OVA image archives that will be consumed by the EKS Anywhere CLI during the cluster creation/deletion/upgrade workflow.\n\n## Base Image Tracker\n\nThis table tracks the base images used to build the container images for the upstream dependencies of EKS Anywhere.\n\n<details>\n<summary>Click to view/hide table</summary>\n\n\n| Dockerfile | Image Repo | Base image |\n| --- | --- | --- |\n| [EKS-A tools](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/aws/eks-anywhere-build-tooling/docker/linux/Dockerfile) | [EKS-A tools image](https://gallery.ecr.aws/eks-anywhere/cli-tools) | [EKS Distro Minimal Base Docker Client Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base-docker-client) |\n| [Bottlerocket bootstrap](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/aws/bottlerocket-bootstrap/docker/linux/Dockerfile) | [Bottlerocket bootstrap image](https://gallery.ecr.aws/eks-anywhere/bottlerocket-bootstrap) | [EKS Distro Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-base) |\n| [EKS Anywhere cluster controller](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/aws/eks-anywhere/docker/linux/eks-anywhere-cluster-controller/Dockerfile) | [EKS Anywhere cluster controller image](https://gallery.ecr.aws/eks-anywhere/cluster-controller) | [EKS Distro Minimal Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base) |\n| [Kube RBAC Proxy](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/brancz/kube-rbac-proxy/docker/linux/Dockerfile) | [Kube RBAC Proxy image](https://gallery.ecr.aws/eks-anywhere/brancz/kube-rbac-proxy) | [EKS Distro Minimal Base Nonroot Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base-nonroot) |\n| [Helm controller](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/fluxcd/helm-controller/docker/linux/Dockerfile) | [Helm controller image](https://gallery.ecr.aws/eks-anywhere/fluxcd/helm-controller) | [EKS Distro Minimal Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base) |\n| [Kustomize controller](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/fluxcd/kustomize-controller/docker/linux/Dockerfile) | [Kustomize controller image](https://gallery.ecr.aws/eks-anywhere/fluxcd/kustomize-controller) | [EKS Distro Minimal Base Git Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base-git) |\n| [Notification controller](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/fluxcd/notification-controller/docker/linux/Dockerfile) | [Notification controller image](https://gallery.ecr.aws/eks-anywhere/fluxcd/notification-controller) | [EKS Distro Minimal Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base) |\n| [Source controller](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/fluxcd/source-controller/docker/linux/Dockerfile) | [Source controller image](https://gallery.ecr.aws/eks-anywhere/fluxcd/source-controller) | [EKS Distro Minimal Base Git Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base-git) |\n| [Certmanager Acmesolver](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/cert-manager/cert-manager/docker/linux/cert-manager-acmesolver/Dockerfile) | [Certmanager Acmesolver image](https://gallery.ecr.aws/eks-anywhere/cert-manager/cert-manager-acmesolver) | [EKS Distro Minimal Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base) |\n| [Certmanager CA injector](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/cert-manager/cert-manager/docker/linux/cert-manager-cainjector/Dockerfile) | [Certmanager CA Injector image](https://gallery.ecr.aws/eks-anywhere/cert-manager/cert-manager-cainjector) | [EKS Distro Minimal Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base) |\n| [Certmanager Controller](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/cert-manager/cert-manager/docker/linux/cert-manager-controller/Dockerfile) | [Certmanager Controller image](https://gallery.ecr.aws/eks-anywhere/cert-manager/cert-manager-controller) | [EKS Distro Minimal Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base) |\n| [Certmanager Webhook](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/cert-manager/cert-manager/docker/linux/cert-manager-webhook/Dockerfile) | [Certmanager Webhook image](https://gallery.ecr.aws/eks-anywhere/cert-manager/cert-manager-webhook) | [EKS Distro Minimal Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base) |\n| [vSphere Cloud Provider](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/kubernetes/cloud-provider-vsphere/docker/linux/Dockerfile) | [vSphere Cloud Provider image](https://gallery.ecr.aws/eks-anywhere/kubernetes/cloud-provider-vsphere/cpi/manager) | [EKS Distro Minimal Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base) |\n| [Cluster API controller](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/kubernetes-sigs/cluster-api/docker/linux/cluster-api-controller/Dockerfile) | [Cluster API controller image](https://gallery.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api/cluster-api-controller) | [EKS Distro Minimal Base Nonroot Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base-nonroot) |\n| [Kubeadm bootstrap controller](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/kubernetes-sigs/cluster-api/docker/linux/kubeadm-bootstrap-controller/Dockerfile) | [Kubeadm bootstrap controller image](https://gallery.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api/kubeadm-bootstrap-controller) | [EKS Distro Minimal Base Nonroot Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base-nonroot) |\n| [Kubeadm controlplane controller](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/kubernetes-sigs/cluster-api/docker/linux/kubeadm-control-plane-controller/Dockerfile) | [Kubeadm controlplane controller image](https://gallery.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api/kubeadm-control-plane-controller) | [EKS Distro Minimal Base Nonroot Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base-nonroot) |\n| [Cluster API Docker controller](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/kubernetes-sigs/cluster-api/docker/linux/cluster-api-docker-controller/Dockerfile) | [Cluster API Docker controller image](https://gallery.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api/cluster-api-docker-controller) | [EKS Distro Minimal Base Docker Client Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base-docker-client) |\n| [Cluster API vSphere controller](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/kubernetes-sigs/cluster-api-provider-vsphere/docker/linux/cluster-api-vsphere-controller/Dockerfile) | [Cluster API vSphere controller image](https://gallery.ecr.aws/eks-anywhere/kubernetes-sigs/cluster-api-provider-vsphere/release/manager) | [EKS Distro Minimal Base Nonroot Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base-nonroot) |\n| [Kind node](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/kubernetes-sigs/kind/images/node/Dockerfile.squash) | [Kind node image](https://gallery.ecr.aws/eks-anywhere/kubernetes-sigs/kind/node) | [EKS Distro Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-base) |\n| [Kindnetd](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/kubernetes-sigs/kind/images/kindnetd/Dockerfile) | [Kindnetd image](https://gallery.ecr.aws/eks-anywhere/kubernetes-sigs/kind/kindnetd) | [EKS Distro Minimal Base Iptables Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base-iptables) |\n| [Etcdadm bootstrap provider](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/aws/etcdadm-bootstrap-provider/docker/linux/Dockerfile) | [Etcdadm bootstrap provider image](https://gallery.ecr.aws/eks-anywhere/aws/etcdadm-bootstrap-provider) | [EKS Distro Minimal Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base) |\n| [Etcdadm controller](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/aws/etcdadm-controller/docker/linux/Dockerfile) | [Etcdadm controller image](https://gallery.ecr.aws/eks-anywhere/aws/etcdadm-controller) | [EKS Distro Minimal Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base) |\n| [Kube VIP](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/plunder-app/kube-vip/docker/linux/Dockerfile) | [Kube VIP image](https://gallery.ecr.aws/eks-anywhere/plunder-app/kube-vip) | [EKS Distro Minimal Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base) |\n| [Local path provisioner](https://github.com/aws/eks-anywhere-build-tooling/blob/main/projects/rancher/local-path-provisioner/docker/linux/Dockerfile) | [Local path provisioner image](https://gallery.ecr.aws/eks-anywhere/rancher/local-path-provisioner) | [EKS Distro Minimal Base Image](https://gallery.ecr.aws/eks-distro-build-tooling/eks-distro-minimal-base) |\n\n</details>\n\n## Contribution\n\nWe appreciate your interest in contributing. Please refer to the [Amazon EKS Anywhere Contribution Guide](https://github.com/aws/eks-anywhere/blob/main/CONTRIBUTING.md) before submitting any issues or pull requests.\n\n- [Building locally](./docs/development/building-locally.md)\n- Dealing with [attribution](./docs/development/attribution-files.md) files\n- [Cherry picking](./docs/development/cherry-picks.md) to release branches\n\n## Security\n\nIf you discover a potential security issue in this project, or think you may\nhave discovered a security issue, we ask that you notify AWS Security via our\n[vulnerability reporting\npage](http://aws.amazon.com/security/vulnerability-reporting/). Please do\n**not** create a public GitHub issue.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": []}, {"name": "eks-anywhere-packages", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon EKS Anywhere Curated Packages\n\n[![Release](https://img.shields.io/github/v/release/aws/eks-anywhere-packages.svg?logo=github&color=green)](https://github.com/aws/eks-anywhere-packages/releases)\n[![Go Report Card](https://goreportcard.com/badge/github.com/aws/eks-anywhere-packages)](https://goreportcard.com/report/github.com/aws/eks-anywhere-packages)\n[![Contributors](https://img.shields.io/github/contributors/aws/eks-anywhere-packages?color=purple)](CONTRIBUTING.md)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)\n\n**Build status:** ![Build status](https://codebuild.us-west-2.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiRmp0cVVpck53WjVxYUVibGxFdSsxM05sby9zenRkN1YwRTVLTjhBUUFORXpGQkVkR2Y3aThhdDhEN3pHZzRpRHl0K2xRcFd0U2VIcWpUaW9kb1hOV3FFPSIsIml2UGFyYW1ldGVyU3BlYyI6InNKTm5MNWZPNVA3T0tOV0EiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=main)\n\n---\nThe Amazon EKS Anywhere Curated Packages are only available to customers with the Amazon EKS Anywhere Enterprise Subscription. To request a free trial, talk to your Amazon representative or connect with one [here](https://aws.amazon.com/contact-us/sales-support-eks/).\n\n---\n\nEKS Anywhere Curated Packages is a management system for installation, configuration and maintenance of additional components for your Kubernetes cluster. Examples of these components may include Container Registry, Ingress, and LoadBalancer, etc.\n\nHere are the steps for [getting started](docs/README.md) with EKS Anywhere Curated Packages.\n\n## Development\n\nEKS Anywhere Curated Packages is tested using\n[Prow](https://github.com/kubernetes/test-infra/tree/master/prow), the Kubernetes CI system.\nEKS operates an installation of Prow, which is visible at [https://prow.eks.amazonaws.com/](https://prow.eks.amazonaws.com/).\nPlease read our [CONTRIBUTING](CONTRIBUTING.md) guide before making a pull request.\n\nThe dependencies which make up EKS Anywhere Curated Packages are defined and built via the [build-tooling](https://github.com/aws/eks-anywhere-build-tooling) repo.\n\n### Local Development\n\nLocal development can be done using [tilt](https://tilt.dev/).\n\n#### Setup\n- install tilt binary on your machine following [instructions](https://docs.tilt.dev/)\n- install and configure [amazon-ecr-credential-helper](https://github.com/awslabs/amazon-ecr-credential-helper)\n- set REGISTRY and KUBERNETES_CONTEXTS env var:\n```\nexport REGISTRY='public.ecr.aws/<your-public-ecr-registry-id>'\nexport KUBERNETES_CONTEXTS=$(kubectl config current-context)\n```\n\nIf running tilt on a remote host, you can port-forward tilt's web UI by forwarding over ssh:\n```\nssh -v -L 10350:localhost:10350 <remote-host>\n```\n\nAfter running `tilt up`, tilt's UI should now be available at `localhost:10350` on your local machine.\n\n## Security\n\nIf you discover a potential security issue in this project, or think you may\nhave discovered a security issue, we ask that you notify AWS Security via our\n[vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public GitHub issue.\n\n## License\n\nThis project is licensed under the [Apache-2.0 License](LICENSE).\n", "release_dates": ["2023-01-20T00:56:26Z", "2022-12-16T20:57:57Z", "2022-10-21T18:58:00Z", "2022-08-19T00:07:31Z", "2022-06-30T17:04:11Z", "2022-05-11T13:52:09Z"]}, {"name": "eks-anywhere-prow-jobs", "description": "This repository contains Prowjob configurations for Amazon EKS Anywhere. You can view the jobs at https://prow.eks.amazonaws.com.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon EKS Anywhere Prow Jobs\n\nThis repository contains Prowjob configuration for the Amazon EKS Anywhere project, which includes the [eks-anywhere](https://github.com/aws/eks-anywhere) and [eks-anywhere-build-tooling](https://github.com/aws/eks-anywhere-build-tooling) repositories. Jobs can be viewed on the EKS installation of\nProw, which is available at https://prow.eks.amazonaws.com/.\n\nFor more info on how to write a Prowjob, read the [test-infra\nintroduction](https://github.com/kubernetes/test-infra/blob/master/prow/jobs.md)\nto prow jobs.\n\n## Contributing\n\nPlease read our [CONTRIBUTING](CONTRIBUTING.md) guide before making a pull\nrequest.\n\n## Security\n\nIf you discover a potential security issue in this project, or think you may\nhave discovered a security issue, we ask that you notify AWS Security via our\n[vulnerability reporting\npage](http://aws.amazon.com/security/vulnerability-reporting/). Please do\n**not** create a public GitHub issue.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n## Adding/Removing Kubernetes Versions\n\nKubernetes/Release branch versions are managed [here](https://github.com/aws/eks-distro-prow-jobs/blob/c23ec5a128f01b11672886da3c9a6da3c2bb846b/templater/jobs/utils/utils.go#L16).\n\nTo update the templates, update the eks-distro-prow-jobs module:\n```\ngo get github.com/aws/eks-distro-prow-jobs\nmake prowjobs -C templater\n```\n", "release_dates": []}, {"name": "eks-charts", "description": "Amazon EKS Helm chart repository", "language": "Mustache", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![EKS Charts](https://github.com/aws/eks-charts/actions/workflows/ci.yaml/badge.svg)\n\n## EKS Charts\n\nAdd the EKS repository to Helm:\n\n```sh\nhelm repo add eks https://aws.github.io/eks-charts\n```\n\n### App Mesh\n\n* [appmesh-controller](stable/appmesh-controller): App Mesh controller Helm chart for Kubernetes\n* [appmesh-prometheus](stable/appmesh-prometheus): App Mesh Prometheus Helm chart for Kubernetes\n* [appmesh-grafana](stable/appmesh-grafana): App Mesh Grafana Helm chart for Kubernetes\n* [appmesh-jaeger](stable/appmesh-jaeger): App Mesh Jaeger Helm chart for Kubernetes\n* [appmesh-spire-server](stable/appmesh-spire-server): App Mesh SPIRE Server Helm chart for Kubernetes\n* [appmesh-spire-agent](stable/appmesh-spire-agent): App Mesh SPIRE Agent Helm chart for Kubernetes\n* [appmesh-gateway](stable/appmesh-gateway): App Mesh Gateway Helm chart for Kubernetes\n* [appmesh-inject](stable/appmesh-inject)(**deprecated**): App Mesh inject Helm chart for Kubernetes\n\n### AWS Node Termination Handler\n\n* [aws-node-termination-handler](stable/aws-node-termination-handler): Gracefully handle EC2 instance shutdown within Kubernetes. <https://github.com/aws/aws-node-termination-handler>\n\n### AWS Calico\n\n**This Helm chart is deprecated**. To install Calico network policy enforcement on AWS, follow the EKS [user guide](https://docs.aws.amazon.com/eks/latest/userguide/calico.html).\n\n### AWS CloudWatch Metrics\n\n* [aws-cloudwatch-metrics](stable/aws-cloudwatch-metrics): A helm chart for CloudWatch Agent to Collect Cluster Metrics\n\n### AWS for Fluent Bit\n\n* [aws-for-fluent-bit](stable/aws-for-fluent-bit): A helm chart for [AWS-for-fluent-bit](https://github.com/aws/aws-for-fluent-bit)\n\n### AWS Load Balancer Controller\n\n* [aws-load-balancer-controller](stable/aws-load-balancer-controller): A helm chart for [AWS Load Balancer Controller](https://github.com/kubernetes-sigs/aws-load-balancer-controller)\n\n### AWS VPC CNI\n\n* [aws-vpc-cni](stable/aws-vpc-cni): Networking plugin for pod networking in Kubernetes using Elastic Network Interfaces on AWS. <https://github.com/aws/amazon-vpc-cni-k8s>\n\n### AWS SIGv4 Proxy Admission Controller\n\n* [aws-sigv4-proxy-admission-controller](stable/aws-sigv4-proxy-admission-controller): A helm chart for [AWS SIGv4 Proxy Admission Controller](https://github.com/aws-observability/aws-sigv4-proxy-admission-controller)\n\n### AWS Secrets Manager and Config Provider for Secret Store CSI Driver\n\n**This Helm chart is deprecated, please switch to <https://aws.github.io/secrets-store-csi-driver-provider-aws/> which is reviewed, owned and maintained by AWS.**\n\n* [csi-secrets-store-provider-aws](stable/csi-secrets-store-provider-aws): A helm chart for [AWS Secrets Manager and Config Provider](https://github.com/aws/secrets-store-csi-driver-provider-aws)\n\n### Amazon EC2 Metadata Mock\n\n* [amazon-ec2-metadata-mock](stable/amazon-ec2-metadata-mock): A tool to simulate Amazon EC2 instance metadata service for local testing\n\n### CNI Metrics Helper\n\n* [cni-metrics-helper](stable/cni-metrics-helper): A helm chart for [CNI Metrics Helper](https://github.com/aws/amazon-vpc-cni-k8s/blob/master/cmd/cni-metrics-helper/README.md)\n\n### EKS EFA Plugin\n* [aws-efa-k8s-device-plugin](stable/aws-efa-k8s-device-plugin): A helm chart for the [Elastic Fabric Adapter](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html) plugin, which automatically discovers and mounts EFA devices into pods that request them\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2024-02-20T17:38:29Z", "2024-02-09T19:14:44Z", "2024-02-01T21:37:25Z", "2024-01-26T16:23:06Z", "2023-12-22T22:06:07Z", "2023-12-19T22:36:34Z", "2023-12-14T22:39:46Z", "2023-11-22T16:46:51Z", "2023-11-20T17:18:47Z", "2023-11-14T23:46:10Z", "2023-11-03T15:12:42Z", "2023-10-28T00:20:42Z", "2023-10-18T21:13:49Z", "2023-10-13T15:45:56Z", "2023-09-12T20:36:17Z", "2023-09-08T17:57:16Z", "2023-09-06T19:29:51Z", "2023-08-29T20:58:21Z", "2023-08-25T21:50:13Z", "2023-08-10T20:59:08Z", "2023-07-27T13:27:03Z", "2023-07-26T18:26:14Z", "2023-07-13T00:00:51Z", "2023-07-12T16:52:36Z", "2023-06-23T19:28:28Z", "2023-06-23T01:04:46Z", "2023-06-19T00:44:31Z", "2023-06-15T21:16:54Z", "2023-06-05T20:35:57Z", "2023-06-02T20:23:32Z"]}, {"name": "eks-distro", "description": "Amazon EKS Distro (EKS-D) is a Kubernetes distribution based on and used by Amazon Elastic Kubernetes Service (EKS) to create reliable and secure Kubernetes clusters.", "language": "Shell", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## EKS Distro Repository\n---\n\n| Release | Development Build Status                                                                                                                  |\n|---------|-------------------------------------------------------------------------------------------------------------------------------------------|\n| 1-25    | [![1-25](https://prow.eks.amazonaws.com/badge.svg?jobs=build-1-25-postsubmit)](https://prow.eks.amazonaws.com/?job=build-1-25-postsubmit) |\n| 1-26    | [![1-26](https://prow.eks.amazonaws.com/badge.svg?jobs=build-1-26-postsubmit)](https://prow.eks.amazonaws.com/?job=build-1-26-postsubmit) |\n| 1-27    | [![1-27](https://prow.eks.amazonaws.com/badge.svg?jobs=build-1-27-postsubmit)](https://prow.eks.amazonaws.com/?job=build-1-27-postsubmit) |\n| 1-28    | [![1-28](https://prow.eks.amazonaws.com/badge.svg?jobs=build-1-28-postsubmit)](https://prow.eks.amazonaws.com/?job=build-1-28-postsubmit) |\n| 1-29    | [![1-29](https://prow.eks.amazonaws.com/badge.svg?jobs=build-1-29-postsubmit)](https://prow.eks.amazonaws.com/?job=build-1-29-postsubmit) |\n\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/6111/badge)](https://bestpractices.coreinfrastructure.org/projects/6111)\n\nAmazon **EKS Distro** (EKS-D) is a Kubernetes distribution based on and used by\nAmazon Elastic Kubernetes Service (EKS) to create reliable and secure Kubernetes\nclusters. With EKS-D, you can rely on the same versions of Kubernetes and its\ndependencies deployed by Amazon EKS. This includes the latest upstream updates,\nas well as extended security patching support. EKS-D follows the same Kubernetes\nversion release cycle as Amazon EKS, and we provide the bits here. EKS-D offers\nthe same software that has enabled tens of thousands of Kubernetes clusters on\nAmazon EKS.\n\nThis GitHub repository has everything required to build the components that make\nup the EKS Distro from source.\n\n## Releases\n\nFull documentation for releases can be found on [https://distro.eks.amazonaws.com](https://distro.eks.amazonaws.com).\n\nTo receive notifications about new EKS-D releases, subscribe to the EKS-D updates SNS topic:\n`arn:aws:sns:us-east-1:379412251201:eks-distro-updates`\n\n[<img src=\"docs/contents/certified-kubernetes-1.26-color.svg\" height=150>](https://github.com/cncf/k8s-conformance/pull/2507)\n<!--\nSource: https://github.com/cncf/artwork/tree/master/projects/kubernetes/certified-kubernetes\n-->\n### Kubernetes 1-29\n\n| Release | Manifest | Kubernetes Version |\n| -- | --- | --- |\n| 5 | [v1-29-eks-5](https://distro.eks.amazonaws.com/kubernetes-1-29/kubernetes-1-29-eks-5.yaml) | [v1.29.1](https://github.com/kubernetes/kubernetes/release/tag/v1.29.1) |\n\n### Kubernetes 1-28\n\n| Release | Manifest | Kubernetes Version |\n| -- | --- | --- |\n| 16 | [v1-28-eks-16](https://distro.eks.amazonaws.com/kubernetes-1-28/kubernetes-1-28-eks-16.yaml) | [v1.28.6](https://github.com/kubernetes/kubernetes/release/tag/v1.28.6) |\n\n\n### Kubernetes 1-27\n\n| Release | Manifest | Kubernetes Version |\n| -- | --- | --- |\n| 23 | [v1-27-eks-23](https://distro.eks.amazonaws.com/kubernetes-1-27/kubernetes-1-27-eks-23.yaml) | [v1.27.10](https://github.com/kubernetes/kubernetes/release/tag/v1.27.10) |\n\n\n### Kubernetes 1-26\n\n| Release | Manifest | Kubernetes Version |\n| -- | --- | --- |\n| 29 | [v1-26-eks-29](https://distro.eks.amazonaws.com/kubernetes-1-26/kubernetes-1-26-eks-29.yaml) | [v1.26.13](https://github.com/kubernetes/kubernetes/release/tag/v1.26.13) |\n\n\n### Kubernetes 1-25\n\n| Release | Manifest | Kubernetes Version |\n| -- | --- | --- |\n| 33 | [v1-25-eks-33](https://distro.eks.amazonaws.com/kubernetes-1-25/kubernetes-1-25-eks-33.yaml) | [v1.25.16](https://github.com/kubernetes/kubernetes/release/tag/v1.25.16) |\n\n\n### Kubernetes 1.18 - 1.24: DEPRECATED\n\nIn alignment with the [Amazon EKS release calendar](https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar),\nEKS Distro has discontinued support of Kubernetes v1.18 - v1.24. While there are\nno plans to remove these versions' images from EKS Distro ECR, there will be no\nmore updates, including security fixes, for them.\n\n**Due to the increased security risk this poses, it is HIGHLY recommended that\nusers of v1.18 - v1.24 update to a supported version (v1.24+) as soon as\npossible.**\n\n## Development\n\nThe EKS Distro is built using\n[Prow](https://github.com/kubernetes/test-infra/tree/master/prow), the\nKubernetes CI/CD system. EKS operates an installation of Prow, which is visible\nat https://prow.eks.amazonaws.com/. Please read our\n[CONTRIBUTING](CONTRIBUTING.md) guide before making a Pull Request.\n\nFor building EKS Distro locally, refer to the\n[building-locally](docs/development/building-locally.md) guide.\n\nFor updating project dependencies, refer to the\n[update-project-dependency](docs/development/update-project-dependency.md) guide.\n\n## Security\n\nIf you discover a potential security issue in this project, or think you may\nhave discovered a security issue, we ask that you notify AWS Security via our\n[vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public GitHub issue.\n\n## License\n\nThis project is licensed under the [Apache-2.0 License](LICENSE).\n", "release_dates": ["2024-02-19T21:47:10Z", "2024-02-19T21:47:07Z", "2024-02-19T21:47:01Z", "2024-02-19T21:46:57Z", "2024-02-19T21:46:51Z", "2024-01-26T18:57:08Z", "2024-01-23T20:11:32Z", "2024-01-23T20:11:28Z", "2024-01-23T20:11:21Z", "2024-01-23T20:11:16Z", "2024-01-23T20:10:51Z", "2024-01-09T20:18:10Z", "2024-01-09T20:18:03Z", "2024-01-09T20:17:56Z", "2024-01-09T20:17:49Z", "2024-01-09T20:17:42Z", "2023-12-26T18:53:40Z", "2023-12-26T18:52:26Z", "2023-12-26T18:53:53Z", "2023-12-26T18:53:46Z", "2023-12-26T18:53:33Z", "2023-12-09T05:31:31Z", "2023-12-09T05:31:10Z", "2023-12-09T05:30:06Z", "2023-12-09T05:28:13Z", "2023-12-09T05:22:37Z", "2023-11-10T23:40:42Z", "2023-11-11T00:20:19Z", "2023-11-11T00:19:29Z", "2023-11-10T23:43:48Z"]}, {"name": "eks-distro-build-tooling", "description": "This repository contains tooling used to build the EKS Distro, and all the projects contained in https://github.com/aws/eks-distro.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## EKS Distro Build Tooling Repository\n\n[![Build status](https://prow.eks.amazonaws.com/badge.svg?jobs=*-tooling-postsubmit)](https://prow.eks.amazonaws.com/?repo=aws%2Feks-distro-build-tooling&type=postsubmit)\n\nThis repository contains tooling used to build the [EKS\nDistro](https://distro.eks.amazonaws.com), and all the projects contained in\nhttps://github.com/aws/eks-distro.\n\n### builder-base\n\nbuilder-base contains a Dockerfile and install scripting for building a\ncontainer image used to run [prow\njobs](https://github.com/aws/eks-distro-prow-jobs) in our [prow\ncluster](https://prow.eks.amazonaws.com).\n\n### eks-distro-base\n\neks-distro-base contains a Dockerfile used to build an up-to-date Amazon Linux 2\nbase image. This base will be updated whenever there are any security updates to\nRPMs contained in the base image.\n\n### helm-charts\n\nThe helm-charts directory contains [Helm](https://helm.sh) charts used to\noperate Prow and supporting tooling on the Prow EKS clusters. These charts are\nnot considered stable for external use.\n\n### release\n\nThe release directory contains release tooling and build code for generating EKS\nDistro release CRDs.\n\n## Security\n\nIf you discover a potential security issue in this project, or think you may\nhave discovered a security issue, we ask that you notify AWS Security via our\n[vulnerability reporting\npage](http://aws.amazon.com/security/vulnerability-reporting/). Please do\n**not** create a public GitHub issue.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": []}, {"name": "eks-distro-prow-jobs", "description": "This repository contains Prow Job configuration for the EKS Distro installation of Prow, which is available at https://prow.eks.amazonaws.com/.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# EKS Distro Prow Jobs\n\nThis repository contains Prow Job configuration for the EKS installation of\nProw, which is available at https://prow.eks.amazonaws.com/.\n\nFor more info on how to write a prow job, read the [test-infra\nintroduction](https://github.com/kubernetes/test-infra/blob/master/prow/jobs.md)\nto prow jobs.\n\n## Creating a New EKS Distro Prow Job\nCheck out [the development documentation](https://github.com/aws/eks-distro-prow-jobs/blob/main/docs/prowjobs.md) for instructions on how to create a new EKS Distro Prow Job in this repository. \n\n## Contributing\n\nPlease read our [CONTRIBUTING](CONTRIBUTING.md) guide before making a pull\nrequest.\n\nRefer to the [documentation](docs/prowjobs.md) for information on how to add or update Prowjobs.\n\n## Security\n\nIf you discover a potential security issue in this project, or think you may\nhave discovered a security issue, we ask that you notify AWS Security via our\n[vulnerability reporting\npage](http://aws.amazon.com/security/vulnerability-reporting/). Please do\n**not** create a public GitHub issue.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": []}, {"name": "eks-news", "description": "The latest and greatest on all things AWS Elastic Kubernetes Service, Kubernetes, and Cloud Native. Keeping up with what\u2019s happening so you can stay on top of this evolving space.", "language": "HTML", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# EKS News\n\n[EKS News](https://eks.news) is curated by the [Amazon Elastic Kubernetes Service](https://aws.amazon.com/eks/) (Amazon EKS) team.\n\nThis newsletter intends to provide the latest and greatest on all things AWS Elastic Kubernetes Service, Kubernetes, and Cloud Native\n\n\"Keeping up with what\u2019s happening so you can stay on top of this evolving space\"\n\n## Subscribe\n\nWant the newsletter delivered straight to your inbox?\n\nSubscribe here: <https://eks.news/#subscribe>\n\n### Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n### License Summary\n\nThe documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the [LICENSE](/LICENSE) file.\n\nThe sample code within this documentation is made available under the MIT-0 license. See the [LICENSE-SAMPLECODE](/LICENSE-SAMPLECODE) file.\n", "release_dates": []}, {"name": "elastic-beanstalk-roadmap", "description": "AWS Elastic Beanstalk roadmap", "language": null, "license": {"key": "cc-by-sa-4.0", "name": "Creative Commons Attribution Share Alike 4.0 International", "spdx_id": "CC-BY-SA-4.0", "url": "https://api.github.com/licenses/cc-by-sa-4.0", "node_id": "MDc6TGljZW5zZTI2"}, "readme": "## Elastic Beanstalk Roadmap\n\nThis is the public roadmap for AWS Elastic Beanstalk.\n\n## Introduction\nThis is the experimental public roadmap for AWS Elastic Beanstalk.\nKnowing about our upcoming products and priorities helps our customers plan. This repository contains information about what we are working on and allows all AWS customers to give direct feedback.\n\n[See the roadmap \u00bb](https://github.com/aws/elastic-beanstalk-roadmap/projects/1)\n\n**Other AWS Public Roadmaps**\n* [AWS App Mesh](https://github.com/aws/aws-app-mesh-roadmap)\n* [AWS Container services](https://github.com/aws/containers-roadmap)\n* [CloudFormation coverage](https://github.com/aws-cloudformation/aws-cloudformation-coverage-roadmap)\n* [Amazon EC2 Spot Instances integrations](https://github.com/aws/ec2-spot-instances-integrations-roadmap)\n\n\n## Security disclosures\n\nIf you think you\u2019ve found a potential security issue, please do not post it in the Issues.  Instead, please follow the instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).\n\n\n## FAQs\n**Q: Why did you build this?**\n\nA: We know that our customers are making decisions and plans based on what we are developing, and we want to provide our customers the insights they need to plan.\n\n**Q: Why are there no dates on your roadmap?**\n\nA: Because job zero is security and operational stability, we can't provide specific target dates for features.\n\n**Q: What do the roadmap categories mean?**\n* *Just shipped* - Launched and available in roughly the last six months.\n* *Coming soon* - coming up.  Think a couple of months out, give or take.\n* *We're working on it* - In progress, but further out.  We might still be working through the implementation details, or scoping stuff out.\n* *Researching* - We're thinking about it. This might mean we're still designing, or thinking through how this might work. This is a great phase to send how you want to see something implemented!  We'd love to see your use case or design ideas here.\n\n**Q: Is everything on the roadmap?**\n\nA: Much of our development work for AWS Elastic Beanstalk is included on this roadmap, but not everything. Of course, there will be features we are very excited about that we are going to launch without notice to surprise and delight our customers. We also don\u2019t include every platform security update, but will include significant runtime updates (for example new major versions of Node.js).\n\n**Q: How can I request a feature be added to the roadmap?**\n\nA: Please open an issue!  You can read about how to contribute [here](/CONTRIBUTING.md). Community submitted issues will be tagged \"Proposed\" and will be reviewed by the team.\n\n\n**Q: How can I provide feedback or ask for more information?**\n\nA: Please open an issue!\n\n\n## License\n\nThis library is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License.\n\nTo learn more about the Elastic Beanstalk service, head here: https://aws.amazon.com/elasticbeanstalk/\n", "release_dates": []}, {"name": "elastic-load-balancing-tools", "description": "AWS Elastic Load Balancing Tools", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Elastic Load Balancing Tools\n \n![ELB](images/ELB.png)\n\n[Elastic Load Balancing (ELB)](https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html) automatically distributes incoming application traffic across Amazon EC2 instances,containers, or resources identified by IP addresses. \n\nThe types of ELB are:\n\n- ALB - Application Load Balancer\n- NLB - Network Load Balancer\n- GWLB - Gateway Load Balancer\n- CLB - Classic Load Balancer\n\nElastic Load Balancing Tools is a collection of utilities, tutorials and code samples to help AWS customers with their use of Elastic Load Balancing. \n\n\n[Classic Load Balancer to Application Load Balancer copy utility](application-load-balancer-copy-utility/) <br />\n[Classic Load Balancer to Network Load Balancer copy utility](network-load-balancer-copy-utility/) <br /> \n[Classic Load Balancer Console Link utility](classic-load-balancer-consolelink-utility/) <br /> \n[Proxy Protocol v2 implementation Java library](proprot/) <br /> \n[Step by step for Log Analysis with Amazon Athena](amazon-athena-for-elb/) <br /> \n[CDK & CloudFormation samples for Log Analysis with Amazon Athena](log-analysis-elb-cdk-cf-template/) <br /> ", "release_dates": []}, {"name": "elb-doctor", "description": "This is a user friendly console style command line tool for troubleshooting ELB issues with many additional features to come.", "language": "Python", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# ELB Doctor\n\nSometimes troubleshooting AWS ELB can be very difficult and intimidating due to the complexity of the virtual cloud network and the various ELB types/features. Navigating in the multi-layered AWS console to look for clues is not an easy task either and could often be time-consuming. This tool provides a no-brainer CLI wizard to help you check ELB basics, display target group health status, and potentially identify the root cause.\n\n<p align=\"center\"><img src=\"elbdoc-demo.gif\" alt=\"animated\" /></p>\n\n## Installation\nELB Doctor has not been published to PyPI yet, though we do have a plan in the near future.\nWe recommend using AWS CloudShell to install and run ELB Doctor, as it automatically configures your credentials for use with AWS services. To use AWS CloudShell, simply open the AWS Management Console, click on the CloudShell icon, and enter the commands below into the CloudShell command line interface.\n\n1. Ensure you can run Python from the command line. You should get some output like Python 3.X.X. \n     ```bash \n     python3 --version\n     ```\n\n2. Clone this repository and change to its directory\n     ```bash\n     git clone https://github.com/aws/elb-doctor\n     cd elb-doctor/\n     ```\n3. Install ELB Doctor locally using `pip`: \n     ```bash\n     python3 -m pip install .\n     ```\n\n4. Start ELB Doctor using `elbdoc` command: \n     ```bash\n     elbdoc\n     ? What is the AWS region of your ELB?  (Use arrow keys)\n    \u276f us-east-1(N. Virginia)\n      us-east-2(Ohio)\n      us-west-1(N. California)\n      us-west-2(Oregon)\n     ```\n\nIf you want to use ELB Doctor in your local terminal, see the instructions below:\n\n  * Ensure a valid CLI credential is configured as described [here](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html). See the below IAM Permission section for the required permissions.\n  * Create a virtual Python environment with the following commands prior to running `python3 -m pip install .`:\n     ```bash\n     # Example commands to create a virtual environment \n\n     $ python3 -m venv env \n     $ source env/bin/activate  # On Windows, use `.\\env\\Scripts\\activate`\n     ```\n\n\nThis procedure will help in setting up ELB Doctor for local use while ensuring a separated environment for its dependencies.\n\n## IAM Permission\n\nThe least IAM previledge required for using ELB Doctor is listed below. \nThe IAM policy that's attached to your IAM role must include at least the following permissions. \nIt is recommended to run the tool in [AWS CloudShell](https://docs.aws.amazon.com/cloudshell/latest/userguide/getting-started.html) which requires some additional permission as listed [here](https://docs.aws.amazon.com/cloudshell/latest/userguide/sec-auth-with-identities.html).\n\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"VisualEditor0\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"elasticloadbalancing:DescribeLoadBalancers\",\n        \"elasticloadbalancing:DescribeTargetHealth\",\n        \"elasticloadbalancing:DescribeTargetGroups\",\n        \"elasticloadbalancing:DescribeInstanceHealth\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n```\n\n## Contributing\nWe welcome community contributions and pull requests. See [CONTRIBUTING.md](CONTRIBUTING.md) for our guidelines on how to submit code.\n\n## Security\n\nSee [SECURITY](.github/SECURITY.md) and [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\nThis library is licensed under the MIT-0 License. See the [LICENSE](LICENSE) file.", "release_dates": []}, {"name": "etcdadm-bootstrap-provider", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Bootstrap provider for creating an etcd cluster using etcdadm\n\n### Using this with CAPI+CAPD:\n1. There are some changes required within CAPI and CAPD for the provisioning of external etcd clusters to work. Checkout this fork+branch locally:\n```\nhttps://github.com/mrajashree/cluster-api/tree/etcdadm_bootstrap\n```\n2. Modify cluster-api/tilt-settings.json to add this provider:\n```json\n{\n  \"default_registry\": \"\",\n  \"provider_repos\": [\"../../mrajashree/etcdadm-bootstrap-provider\"],\n  \"enable_providers\": [\"core\",\"docker\", \"kubeadm-bootstrap\", \"kubeadm-control-plane\", \"etcdadm-bootstrap\"],\n  \"kustomize_substitutions\": {\n    \"ETCDADM_BOOTSTRAP_IMAGE\": \"mrajashree/etcdadm-bootstrap-provider:latest\"\n  }\n}\n\n```\n3. This provider has a tilt-provider.json that will be used by CAPI\n4. Create a Kind cluster, and run tilt up\n\n### Using this with CAPI+CAPA\n1. There are some security group changes required for CAPA, I made them based off of the PR that adds v1alpha4 to CAPA. Checkout the changes locally from this fork+branch:\n```\nhttps://github.com/mrajashree/cluster-api-provider-aws/tree/etcadm\n```\n2. Modify cluster-api/tilt-settings.json to add this provider:\n```json\n{\n  \"default_registry\": \"\",\n  \"provider_repos\": [\"../cluster-api-provider-aws\"],\n  \"enable_providers\": [\"core\",\"docker\", \"aws\", \"kubeadm-bootstrap\", \"kubeadm-control-plane\", \"etcdadm-bootstrap\"],\n  \"kustomize_substitutions\": {\n    \"AWS_B64ENCODED_CREDENTIALS\": \"...\",\n    \"ETCDADM_BOOTSTRAP_IMAGE\": \"mrajashree/etcdadm-bootstrap-provider:latest\"\n  }\n}\n```\n3. Create a Kind cluster, and run tilt up\n", "release_dates": []}, {"name": "etcdadm-controller", "description": "ETCD Admin Controller for EKS Anywhere", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## ETCD Admin Controller for Amazon EKS Anywhere\n\nThe etcdadm-controller is responsible for managing external etcd clusters created by Amazon EKS Anywhere (EKS-A)\n\nAmazon EKS Anywhere is a new deployment option for Amazon EKS that enables you to easily create and operate Kubernetes clusters on-premises with your own virtual machines.\nIt brings a consistent AWS management experience to your data center, building on the strengths of [Amazon EKS Distro](https://github.com/aws/eks-distro), the same distribution of Kubernetes that powers EKS on AWS.\nIts goal is to include full lifecycle management of multiple Kubernetes clusters that are capable of operating completely independently of any AWS services.\n\nHere are the steps for [getting started](https://anywhere.eks.amazonaws.com/docs/getting-started/) with EKS Anywhere.\nFull documentation for releases can be found on [https://anywhere.eks.amazonaws.com](https://anywhere.eks.amazonaws.com/).\n\n## Security\n\nIf you discover a potential security issue in this project, or think you may\nhave discovered a security issue, we ask that you notify AWS Security via our\n[vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/).\nPlease do **not** create a public GitHub issue.\n\n## License\n\nThis project is licensed under the [Apache-2.0 License](LICENSE).", "release_dates": []}, {"name": "event-ruler", "description": "Event Ruler is a Java library that allows matching many thousands of Events per second to any number of expressive and sophisticated rules.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Event Ruler\n\n[![License](https://img.shields.io/github/license/aws/event-ruler.svg?color=blue&logo=apache)]([https://www.apache.org/licenses/LICENSE-2.0.html](https://github.com/aws/event-ruler/blob/main/LICENSE))\n[![Build](https://github.com/aws/event-ruler/actions/workflows/CI.yml/badge.svg)](https://github.com/aws/event-ruler/actions/workflows/CI.yml)\n[![Latest Release](https://img.shields.io/github/release/aws/event-ruler.svg?logo=github&style=flat-square)](https://github.com/aws/event-ruler/releases/latest)\n![Maven Central](https://img.shields.io/maven-central/v/software.amazon.event.ruler/event-ruler?logo=apachemaven)\n\n\nEvent Ruler (called Ruler in rest of the doc for brevity) is a Java library \nthat allows matching **Rules** to **Events**. An event is a list of fields, which \nmay be given as name/value pairs or as a JSON object.  A rule associates event \nfield names with lists of possible values.  There are two reasons to use Ruler:\n\n1. It's fast; the time it takes to match Events doesn't depend on the number of Rules.\n2. Customers like the JSON \"query language\" for expressing rules.\n\nContents:\n\n1. [Ruler by Example](#ruler-by-example)\n2. [And and Or With Ruler](#and-and-or-relationship-among-fields-with-ruler)\n3. [How to Use Ruler](#how-to-use-ruler)\n4. [JSON Text Matching](#json-text-matching)\n5. [JSON Array Matching](#json-array-matching)\n6. [Compiling and Checking Rules](#compiling-and-checking-rules)\n7. [Performance](#performance)\n\nIt's easiest to explain by example.\n\n##  Ruler by Example\n\nAn Event is a JSON object.  Here's an example:\n\n```javascript\n{\n  \"version\": \"0\",\n  \"id\": \"ddddd4-aaaa-7777-4444-345dd43cc333\",\n  \"detail-type\": \"EC2 Instance State-change Notification\",\n  \"source\": \"aws.ec2\",\n  \"account\": \"012345679012\",\n  \"time\": \"2017-10-02T16:24:49Z\",\n  \"region\": \"us-east-1\",\n  \"resources\": [\n    \"arn:aws:ec2:us-east-1:123456789012:instance/i-000000aaaaaa00000\"\n  ],\n  \"detail\": {\n    \"c-count\": 5,\n    \"d-count\": 3,\n    \"x-limit\": 301.8,\n    \"source-ip\": \"10.0.0.33\",\n    \"instance-id\": \"i-000000aaaaaa00000\",\n    \"state\": \"running\"\n  }\n}\n```\n\nYou can also see this as a set of name/value pairs. For brevity, we present\nonly a sampling.  Ruler has APIs for providing events both in JSON form and\nas name/value pairs:\n\n```\n    +--------------+------------------------------------------+\n    | name         | value                                    |\n    |--------------|------------------------------------------|\n    | source       | \"aws.ec2\"                                |\n    | detail-type  | \"EC2 Instance State-change Notification\" |\n    | detail.state | \"running\"                                |\n    +--------------+------------------------------------------+\n```\n\nEvents in the JSON form may be provided in the form of a raw JSON String,\nor a parsed [Jackson JsonNode](https://fasterxml.github.io/jackson-databind/javadoc/2.12/com/fasterxml/jackson/databind/JsonNode.html).\n\n### Simple matching\n\nThe rules in this section all match the sample event above:\n\n```javascript\n{\n  \"detail-type\": [ \"EC2 Instance State-change Notification\" ],\n  \"resources\": [ \"arn:aws:ec2:us-east-1:123456789012:instance/i-000000aaaaaa00000\" ],\n  \"detail\": {\n    \"state\": [ \"initializing\", \"running\" ]\n  }\n}\n```\nThis will match any event with the provided values for the `resource`,\n`detail-type`, and `detail.state` values, ignoring any other fields in the\nevent. It would also match if the value of `detail.state` had been\n`\"initializing\"`.\n\nValues in rules are always provided as arrays, and match if the value in the\nevent is one of the values provided in the array.  The reference to `resources`\nshows that if the value in the event is also an array, the rule matches if the\nintersection between the event array and rule-array is non-empty.\n\n### Prefix matching\n\n```javascript\n{\n  \"time\": [ { \"prefix\": \"2017-10-02\" } ]\n}\n```\nPrefix matches only work on string-valued fields.\n\n### Prefix equals-ignore-case matching\n\n```javascript\n{\n  \"source\": [ { \"prefix\": { \"equals-ignore-case\": \"EC2\" } } ]\n}\n```\nPrefix equals-ignore-case matches only work on string-valued fields.\n\n### Suffix matching\n\n```javascript\n{\n  \"source\": [ { \"suffix\": \"ec2\" } ]\n}\n```\nSuffix matches only work on string-valued fields.\n\n### Suffix equals-ignore-case matching\n\n ```javascript\n {\n   \"source\": [ { \"suffix\": { \"equals-ignore-case\": \"EC2\" } } ]\n }\n ```\nSuffix equals-ignore-case matches only work on string-valued fields.\n\n### Equals-ignore-case matching\n\n```javascript\n{\n  \"source\": [ { \"equals-ignore-case\": \"EC2\" } ]\n}\n```\nEquals-ignore-case matches only work on string-valued fields.\n\n### Wildcard matching\n\n```javascript\n{\n  \"source\": [ { \"wildcard\": \"Simple*Service\" } ]\n}\n```\nWildcard matches only work on string-valued fields. A single value can contain zero to many wildcard characters, but\nconsecutive wildcard characters are not allowed. To match the asterisk character specifically, a wildcard character can\nbe escaped with a backslash. Two consecutive backslashes (i.e. a backslash escaped with a backslash) represents the\nactual backslash character. A backslash escaping any character other than asterisk or backslash is not allowed.\n\n### Anything-but matching\n\nAnything-but matching does what the name says: matches anything *except* what's provided in the rule.\n\nAnything-but works with single string and numeric values or lists, which have to contain entirely strings or entirely\nnumerics. It also may be applied to a prefix, suffix, or equals-ignore-case match of a string or a list of strings.\n\nSingle anything-but (string, then numeric):\n```javascript\n{\n  \"detail\": {\n    \"state\": [ { \"anything-but\": \"initializing\" } ]\n  }\n}\n\n{\n  \"detail\": {\n    \"x-limit\": [ { \"anything-but\": 123 } ]\n  }\n}\n```\nAnything-but list (strings):\n```javascript\n{\n  \"detail\": {\n    \"state\": [ { \"anything-but\": [ \"stopped\", \"overloaded\" ] } ]\n  }\n}\n```\n\nAnything-but list (numbers):\n```javascript\n{\n  \"detail\": {\n    \"x-limit\": [ { \"anything-but\": [ 100, 200, 300 ] } ]\n  }\n}\n```\n\nAnything-but prefix:\n```javascript\n{\n  \"detail\": {\n    \"state\": [ { \"anything-but\": { \"prefix\": \"init\" } } ]\n  }\n}\n```\n\nAnything-but prefix list (strings):\n```javascript\n{\n  \"detail\": {\n    \"state\": [ { \"anything-but\": { \"prefix\": [ \"init\", \"error\" ] } } ]\n  }\n}\n```\n\nAnything-but suffix:\n```javascript\n{\n  \"detail\": {\n    \"instance-id\": [ { \"anything-but\": { \"suffix\": \"1234\" } } ]\n  }\n}\n```\n\nAnything-but suffix list (strings):\n```javascript\n{\n  \"detail\": {\n    \"instance-id\": [ { \"anything-but\": { \"suffix\": [ \"1234\", \"6789\" ] } } ]\n  }\n}\n```\n\nAnything-but-ignore-case:\n```javascript\n{\n  \"detail\": {\n    \"state\": [ { \"anything-but\": {\"equals-ignore-case\": \"Stopped\" } } ]\n  }\n}\n\n```\n\nAnything-but-ignore-case list (strings):\n```javascript\n{\n  \"detail\": {\n    \"state\": [ { \"anything-but\": {\"equals-ignore-case\": [ \"Stopped\", \"OverLoaded\" ] } } ]\n  }\n}\n\n```\n\n### Numeric matching\n```javascript\n{\n  \"detail\": {\n    \"c-count\": [ { \"numeric\": [ \">\", 0, \"<=\", 5 ] } ],\n    \"d-count\": [ { \"numeric\": [ \"<\", 10 ] } ],\n    \"x-limit\": [ { \"numeric\": [ \"=\", 3.018e2 ] } ]\n  }\n}  \n```\n\nAbove, the references to `c-count`, `d-count`, and `x-limit` illustrate numeric matching,\nand only\nwork with values that are JSON numbers.  Numeric matching is limited to value between\n-5.0e9 and +5.0e9 inclusive, with 15 digits of precision, that is to say 6 digits\nto the right of the decimal point.\n\n### IP Address Matching\n```javascript\n{\n  \"detail\": {\n    \"source-ip\": [ { \"cidr\": \"10.0.0.0/24\" } ]\n  }\n}\n```\n\nThis also works with IPv6 addresses.\n\n### Exists matching\n\nExists matching works on the presence or absence of a field in the JSON event.\n\nThe rule below will match any event which has a detail.c-count field present.\n\n```javascript\n{\n  \"detail\": {\n    \"c-count\": [ { \"exists\": true  } ]\n  }\n}  \n```\n\nThe rule below will match any event which has no detail.c-count field.\n\n```javascript\n{\n  \"detail\": {\n    \"c-count\": [ { \"exists\": false  } ]\n  }\n}  \n```\n\n**Note** ```Exists``` match **only works on the leaf nodes.** It does not work on intermediate nodes.\n\nAs an example, the above example for ```exists : false ``` would match the event below:\n\n```javascript\n{\n  \"detail-type\": [ \"EC2 Instance State-change Notification\" ],\n  \"resources\": [ \"arn:aws:ec2:us-east-1:123456789012:instance/i-000000aaaaaa00000\" ],\n  \"detail\": {\n    \"state\": [ \"initializing\", \"running\" ]\n  }\n}\n```\n\nbut would also match the event below because ```c-count``` is not a leaf node:\n\n```javascript\n{\n  \"detail-type\": [ \"EC2 Instance State-change Notification\" ],\n  \"resources\": [ \"arn:aws:ec2:us-east-1:123456789012:instance/i-000000aaaaaa00000\" ],\n  \"detail\": {\n    \"state\": [ \"initializing\", \"running\" ]\n    \"c-count\" : {\n       \"c1\" : 100\n    }\n  }\n}\n```\n\n\n### Complex example\n\n```javascript\n{\n  \"time\": [ { \"prefix\": \"2017-10-02\" } ],\n  \"detail\": {\n    \"state\": [ { \"anything-but\": \"initializing\" } ],\n    \"c-count\": [ { \"numeric\": [ \">\", 0, \"<=\", 5 ] } ],\n    \"d-count\": [ { \"numeric\": [ \"<\", 10 ] } ],\n    \"x-limit\": [ { \"anything-but\": [ 100, 200, 300 ] } ],\n    \"source-ip\": [ { \"cidr\": \"10.0.0.0/8\" } ]\n  }\n}\n```\n\nAnd and Or Relationship among fields with Ruler\n=====================\n### Default \"And\" relationship\nAs the examples above show, Ruler considers a rule to match if **all** of the fields\nnamed in the rule match, and it considers a field to match if **any** of the provided\nfield values match, __that is to say Ruler has applied \"And\" logic to all fields by\ndefault without \"And\" primitive is required__.\n\n### \"Or\" relationship\nThere are two ways to reach the \"Or\" effects:\n* Add multiple rules with the same rule name and each individual rule will be treated as one of \"Or\" condition by Ruler.\n  Refer to below under **addRule()** section on how to achieve an \"Or\" effect in that way.\n* Use the \"$or\" primitive to express the \"Or\" relationship within the rule.\n\n#### The \"$or\" Primitive\nThe \"$or\" primitive to allow the customer directly describe the \"Or\" relationship among fields in the rule.\n\nRuler recognizes \"Or\" relationship **only** when the rule has met **all** below conditions:\n* There is \"$or\" on field attribute in the rule followed with an array \u2013 e.g. \"$or\": []\n* There are 2+ objects in the \"$or\" array at least : \"$or\": [{}, {}]\n* There is no filed name using Ruler keywords in Object of \"$or\" array, refer to RESERVED_FIELD_NAMES_IN_OR_RELATIONSHIP in `/src/main/software/amazon/event/ruler/Constants.java#L38`\n  for example, below rule will be not parsed as \"Or\" relationship because \"numeric\" and \"prefix\" are Ruler reserved keywords.\n  ```\n  { \n     \"$or\": [ {\"numeric\" : 123}, {\"prefix\": \"abc\"} ] \n  } \n  ```\nOtherwise, Ruler just treats the \"$or\" as normal filed name the same as other string in the rule.\n\n#### Rule examples with \"$or\" Primitive\nNormal \"Or\":\n```javascript\n// Effect of \"source\" && (\"metricName\" || \"namespace\")\n{\n  \"source\": [ \"aws.cloudwatch\" ], \n  \"$or\": [\n    { \"metricName\": [ \"CPUUtilization\", \"ReadLatency\" ] },\n    { \"namespace\": [ \"AWS/EC2\", \"AWS/ES\" ] }\n  ] \n}\n```\nParallel \"Or\":\n```javascript\n// Effect of (\"metricName\" || \"namespace\") && (\"detail.source\" || \"detail.detail-type\")\n{\n  \"$or\": [\n    { \"metricName\": [ \"CPUUtilization\", \"ReadLatency\" ] },\n    { \"namespace\": [ \"AWS/EC2\", \"AWS/ES\" ] }\n  ], \n  \"detail\" : {\n    \"$or\": [\n      { \"source\": [ \"aws.cloudwatch\" ] },\n      { \"detail-type\": [ \"CloudWatch Alarm State Change\"] }\n    ]\n  }\n}\n```\n\"Or\" has an \"And\" inside\n```javascript\n// Effect of (\"source\" && (\"metricName\" || (\"metricType && \"namespace\") || \"scope\"))\n{\n  \"source\": [ \"aws.cloudwatch\" ],\n  \"$or\": [\n    { \"metricName\": [ \"CPUUtilization\", \"ReadLatency\" ] },\n    {\n      \"metricType\": [ \"MetricType\" ] ,\n      \"namespace\": [ \"AWS/EC2\", \"AWS/ES\" ]\n    },\n    { \"scope\": [ \"Service\" ] }\n  ]\n}\n```\nNested \"Or\" and \"And\"\n```javascript\n// Effect of (\"source\" && (\"metricName\" || (\"metricType && \"namespace\" && (\"metricId\" || \"spaceId\")) || \"scope\"))\n{\n  \"source\": [ \"aws.cloudwatch\" ],\n  \"$or\": [\n    { \"metricName\": [ \"CPUUtilization\", \"ReadLatency\" ] },\n    {\n      \"metricType\": [ \"MetricType\" ] ,\n      \"namespace\": [ \"AWS/EC2\", \"AWS/ES\" ],\n      \"$or\" : [\n        { \"metricId\": [ 1234 ] },\n        { \"spaceId\": [ 1000 ] }\n      ]\n    },\n    { \"scope\": [ \"Service\" ] }\n  ]\n}\n```\n\n#### The backward compatibility of using \"$or\" as field name in the rule\n\"$or\" is possibly already used as a normal key in some applications (though its likely rare). For these cases, \nRuler tries its best to maintain the backward compatibility. Only when the 3 conditions mentioned above, will \nruler change behaviour because it assumes your rule really wanted an OR and was mis-configured until today. For example, \nthe rule below will keep working as normal rule with treating \"$or\" as normal field name in the rule and event:\n```javascript\n{\n    \"source\": [ \"aws.cloudwatch\" ],\n    \"$or\": {\n        \"metricType\": [ \"MetricType\" ] , \n        \"namespace\": [ \"AWS/EC2\", \"AWS/ES\" ]\n    }\n}\n```\nRefer to `/src/test/data/normalRulesWithOrWording.json` for more examples that \"$or\" is parsed as normal field name by Ruler.\n\n#### Caveat\nThe keyword \"$or\" as \"Or\" relationship primitive should not be designed as normal field in both Events and Rules.\nRuler supports the legacy rules where \"$or\" is parsed as normal field name to keep backward\ncompatibility and give time for team to migrate their legacy \"$or\" usage away from their events and rules as normal filed name. \nMix usage of \"$or\" as \"Or\" primitive, and \"$or\" as normal field name is not supported\nintentionally by Ruler to avoid the super awkward ambiguities on \"$or\" from occurring.\n\nHow to use Ruler\n================\n\nThere are two ways to use Ruler.  You can compile multiple rules\ninto a \"Machine\", and then use either of its `rulesForEvent()` method\nor `rulesForJSONEvent()` methods to check  which of the rules match any Event.\nThe difference between these two methods is discussed below.  This discussion\nwill use `rulesForEvent()` generically except where the difference matters.\n\nAlternatively, you can use a single static boolean method to determine\nwhether an individual event matches a particular rule.\n\n## Static Rule Matching\n\nThere is a single static boolean method `Ruler.matchesRule(event, rule)` -\nboth arguments are provided as JSON strings. \n\nNOTE: There is another deprecated method called `Ruler.matches(event, rule)`which \nshould not be used as its results are inconsistent with `rulesForJSONEvent()` and \n`rulesForEvent()`. See the documentation on `Ruler.matches(event, rule)` for details.\n\n## Matching with a Machine\n\nThe matching time does not depend on the number of rules.  This is the best choice\nif you have multiple possible rules you want to select from, and especially\nif you have a way to store the compiled Machine.\n\nThe matching time is impacted by the degree of non-determinism introduced by wildcard rules. Performance deteriorates as\nan increasing number of the wildcard rule prefixes match a theoretical worst-case event. To avoid this, wildcard rules\npertaining to the same event field should avoid common prefixes leading up to their first wildcard character. If a\ncommon prefix is required, then use the minimum number of wildcard characters and limit repeating character sequences\nthat occur following a wildcard character. MachineComplexityEvaluator can be used to evaluate a machine and determine\nthe degree of non-determinism, or \"complexity\" (i.e. how many wildcard rule prefixes match a theoretical worst-case\nevent). Here are some data points showing a typical decrease in performance for increasing complexity scores.\n\n- Complexity = 1, Events per Second = 140,000\n- Complexity = 17, Events per Second = 12,500\n- Complexity = 34, Events per Second = 3500\n- Complexity = 50, Events per Second = 2500\n- Complexity = 100, Events per Second = 1250\n- Complexity = 275, Events per Second = 100 (extrapolated data point)\n- Complexity = 650, Events per Second = 10 (extrapolated data point)\n\nIt is important to limit machine complexity to protect your application. There are at least two different strategies for\nlimiting machine complexity. Which one makes more sense may depend on your application.\n\n1. Aggregate Complexity. Create a machine using all rules, evaluate the complexity, and reject the rule set (or the\n   current rule being added) if it exceeds the threshold.\n2. Individual Complexity. Create a machine using just an individual rule (not all rules), evaluate the complexity, and\n   reject the rule if it exceeds the threshold. In addition, limit the number of rules that can be created that contain\n   a wildcard pattern.\n\nStrategy #1 is more ideal in that it measures the actual complexity of the machine containing all the rules. When\npossible, this strategy should be used. The downside is, let's say you have a control plane that allows the creation of\none rule at a time, up to a very large number. Then for each of these control plane operations, you must load all the\nexisting rules to perform the validation. This could be very expensive. It is also prone to race conditions.\nStrategy #2 is a compromise. The threshold used by strategy #2 will be lower than strategy #1 since it is a per-rule\nthreshold. Let's say you want a machine's complexity, with all rules added, to be no more than 300. Then with\nstrategy #2, for example, you could limit each single-rule machine to complexity of 10, and allow for 30 rules containing\nwildcard patterns. In an absolute worst case where complexity is perfectly additive (unlikely), this would lead to a\nmachine with complexity of 300. The downside is that it is unlikely that the complexity will be perfectly additive, and\nso the number of wildcard-containing rules will likely be limited unnecessarily.\n\nFor strategy #2, depending on how rules are stored, an additional attribute may need to be added to rules to indicate\nwhich ones are nondeterministic (i.e. contain wildcard patterns) in order to limit the number of wildcard-containing\nrules.\n\nThe following is a code snippet illustrating how to limit complexity for a given pattern, like for strategy #2.\n\n```java\npublic class Validate {\n    private void validate(String pattern, MachineComplexityEvaluator machineComplexityEvaluator) {\n        // If we cannot compile, then return exception.\n        List<Map<String, List<Patterns>>> compilationResult = Lists.newArrayList();\n        try {\n            compilationResult.addAll(JsonRuleCompiler.compile(pattern));\n        } catch (Exception e) {\n            InvalidPatternException internalException =\n                    EXCEPTION_FACTORY.invalidPatternException(e.getLocalizedMessage());\n            throw ExceptionMapper.mapToModeledException(internalException);\n        }\n\n        // Validate wildcard patterns. Look for wildcard patterns out of all patterns that have been used.\n        Machine machine = new Machine();\n        int i = 0;\n        for (Map<String, List<Patterns>> rule : compilationResult) {\n            if (containsWildcard(rule)) {\n                // Add rule to machine for complexity evaluation.\n                machine.addPatternRule(Integer.toString(++i), rule);\n            }\n        }\n\n        // Machine has all rules containing wildcard match types. See if the complexity is under the limit.\n        int complexity = machine.evaluateComplexity(machineComplexityEvaluator);\n        if (complexity > MAX_MACHINE_COMPLEXITY) {\n            InvalidPatternException internalException = EXCEPTION_FACTORY.invalidPatternException(\"Rule is too complex\");\n            throw ExceptionMapper.mapToModeledException(internalException);\n        }\n    }\n    \n    private boolean containsWildcard(Map<String, List<Patterns>> rule) {\n        for (List<Patterns> fieldPatterns : rule.values()) {\n            for (Patterns fieldPattern : fieldPatterns) {\n                if (fieldPattern.type() == WILDCARD) {\n                    return true;\n                }\n            }\n        }\n        return false;\n    }\n}\n```\n\nThe main class you'll interact with implements state-machine based rule\nmatching.  The interesting methods are:\n\n* `addRule()` - adds a new rule to the machine\n* `deleteRule()` - deletes a rule from the machine\n* `rulesForEvent()`/`rulesForJSONEvent()` - finds the rules in the machine that match an event\n\nThere are two flavors: `Machine` and `GenericMachine<T>`.  Machine is simply `GenericMachine<String>`.  The\nAPI refers to the generic type as \"name\", which reflects history: The String version was built first and\nthe strings it stored and returned were thought of as rule names.\n\nFor safety, the type used to \"name\" rules should be immutable. If you change the content of an object while\nit's being used as a rule name, this may break the operation of Ruler.\n\n### Configuration\n\nThe GenericMachine and Machine constructors optionally accept a GenericMachineConfiguration object, which exposes the\nfollowing configuration options.\n\n#### additionalNameStateReuse\nDefault: false\nNormally, NameStates are re-used for a given key subsequence and pattern if this key subsequence and pattern have been\npreviously added, or if a pattern has already been added for the given key subsequence. Hence, by default, NameState\nre-use is opportunistic. But by setting this flag to true, NameState re-use will be forced for a key subsequence. This\nmeans that the first pattern being added for a key subsequence will re-use a NameState if that key subsequence has been\nadded before. Meaning each key subsequence has a single NameState. This improves memory utilization exponentially in\nsome cases but does lead to more sub-rules being stored in individual NameStates, which Ruler sometimes iterates over,\nwhich can cause a modest runtime performance regression. This defaults to false for backwards compatibility, but likely,\nall but the most latency sensitive of applications would benefit from setting this to true.\n\nHere's a simple example. Consider:\n\n```javascript\nmachine.addRule(\"0\", \"{\\\"key1\\\": [\\\"a\\\", \\\"b\\\", \\\"c\\\"]}\");\n```\n\nThe pattern \"a\" creates a NameState, and then, even with additionalNameStateReuse=false, the second pattern (\"b\") and\nthird pattern (\"c\") re-use that same NameState. But consider the following instead:\n\n```javascript\nmachine.addRule(\"0\", \"{\\\"key1\\\": [\\\"a\\\"]}\");\nmachine.addRule(\"1\", \"{\\\"key1\\\": [\\\"b\\\"]}\");\nmachine.addRule(\"2\", \"{\\\"key1\\\": [\\\"c\\\"]}\");\n```\n\nNow, with additionalNameStateReuse=false, we end up with three NameStates, because the first pattern encountered for a\nkey subsequence on each rule addition will create a new NameState. So, \"a\", \"b\", and \"c\" all get their own NameStates.\nHowever, with additionalNameStateReuse=true, \"a\" will create a new NameState, then \"b\" and \"c\" will reuse this same\nNameState. This is accomplished by storing that we already have a NameState for the key subsequence \"key1\".\n\nNote that it doesn't matter if each addRule uses a different rule name or the same rule name.\n\n### addRule()\n\nAll forms of this method have the same first argument, a String which provides\nthe name of the Rule and is returned by `rulesForEvent()`.  The rest of the\narguments provide the name/value pairs.  They may be provided in JSON as in\nthe examples above (via a String, a Reader, an InputStream, or `byte[]`), or as\na `Map<String, List<String>>`, where the keys are the field names and the\nvalues are the list of possible matches; using the example above, there would\nbe a key named `detail.state` whose value would be the list containing\n`\"initializing\"` and `\"running\"`.\n\nNote: This method (and also `deleteRule()`) is synchronized, so only one thread\nmay be updating the machine at any point in time.\n\n#### Rules and rule names\n\nYou can call `addRule()` multiple times with the same name but multiple different\nname/value patterns, thus  achieving an \"or\" relationship;\n`rulesForEvent()` will return that name if any of the patterns match.\n\nFor example, suppose you call `addRule()` with rule name as \"R1\" and add\nthe following pattern:\n```javascript\n{\n  \"detail\": {\n    \"c-count\": [ { \"numeric\": [ \">\", 0, \"<=\", 5 ] } ]\n  }\n}\n```\nThen you call it again with the same name but a different pattern:\n\n```javascript\n{\n  \"detail\": {\n    \"x-limit\": [ { \"numeric\": [ \"=\", 3.018e2 ] } ]\n  }\n}\n```\nAfter this, `rulesForEvent()` will return \"R1\" for **either** a `c-count` value of 2\n**or** an `x-limit` value of 301.8.\n\n### deleteRule()\n\nThis is a mirror-image of `addRule()`; in each case the first argument is the rule\nname, given as a String.  Subsequent arguments provide the names and values,\nand may be given in any of the same ways as with `addRule()`.\n\nNote: This method (and also `addRule()`) is synchronized, so only one thread may\nbe updating the machine at any point in time.\n\nThe operation of this API can be subtle.  The Machine compiles the mapping\nof name/value patterns to Rule names into a finite automaton, but does not\nremember what patterns are mapped to a given Rule name. Thus, there is no\nrequirement that the pattern in a `deleteRule()` exactly match that in the\ncorresponding `addRule()`.  Ruler will look for matches to the name/value patterns\nand see if they give a match to a rule with the provided name, and if so\nremove them. Bear in mind that while performing `deleteRule()` calls that do not exactly\nmatch the corresponding `addRule()` calls will not fail and will not leave the\nmachine in an inconsistent state, they may cause \"garbage\" to build up in the\nMachine.\n\nA specific consequence is that if you have called `addRule()` multiple times with\nthe same name but different patterns, as illustrated above in the *Rules and rule\nnames* section, you would have to call `deleteRule()` the same number of times,\nwith the same associated patterns, to remove all references to that rule name\nfrom the machine.\n\n### rulesForEvent() / rulesForJSONEvent()\n\nThis method returns a `List<String>` for Machine (and `List<T>` for GenericMachine) which contains\nthe names of the rules that match the provided event.  The event may be provided to either method\nas a single `String` representing its JSON form.\n\nThe event may also be provided to `rulesForEvent()` as a collection of strings which alternate field\nnames and values, and must be sorted lexically by field-name.  This may be a `List<String>` or `String[]`.\n\nProviding the event in JSON is the recommended approach and has several advantages. First of all,\npopulating the String list or array with alternating name/value quantities, in an order sorted by name,\nis tricky, and Ruler doesn't help, just fails to work correctly if the list is improperly structured.  Adding\nto the difficulty, the representation of field values, provided as strings, must follow JSON-syntax\nrules - see below under *JSON text matching*.\n\nFinally, the list/array version of an event makes it impossible for Ruler to recognize array\nstructures and provide array-consistent matching, described below in this document. The\n`rulesForEvent(String eventJSON)` API is deprecated in favor of `rulesForJSONEvent()`\nspecifically because it does not support array-consistent matching.\n\n`rulesForJSONEvent()` also has the advantage that the code which turns the JSON form\nof the event into a sorted list has been extensively profiled and optimized.\n\nThe performance of `rulesForEvent()` and `rulesForJSONEvent()` do not depend on the number of rules added\nwith `addRule()`.  `rulesForJSONEvent()` is generally faster because of the optimized\nevent processing. If you do your own event processing and call `rulesForEvent()`\nwith a pre-sorted list of name and values, that is faster still; but you may not\nbe able to do the field-list preparation as fast as `rulesForJSONEvent()` does.\n\n### approximateObjectCount()\n\nThis method roughly the number of objects within the machine. It's value only varies as rule are added or\nremoved. This is useful to identify large machines that potentially require loads of memory.\nAs this method is dependent on number of internal objects, this counts may change when ruler library internals\nare changed. The method performs all of its calculation at runtime to avoid taking up memory and making the\nimpact of large rule-machines worse. Its computation is intentionally NOT thread-safe to avoid blocking rule\nevaluations and machine changes. It means that if a parallel process is adding or removing from the machine,\nyou may get a different results compared to when such parallel processes are complete. Also, as the library\nmakes optimizations to its internals for some patterns (see `ShortcutTransition.java` for more details), you\nmay also get different results depending on the order in which rules were added or removed.\n\n### The Patterns API\n\nIf you think of your events as name/value pairs rather than nested JSON-style\ndocuments, the `Patterns` class (and its `Range` subclass) may be useful in constructing rules.  The following\nstatic methods are useful.\n\n```java\npublic static ValuePatterns exactMatch(final String value);\npublic static ValuePatterns prefixMatch(final String prefix);\npublic static ValuePatterns prefixEqualsIgnoreCaseMatch(final String prefix);\npublic static ValuePatterns suffixMatch(final String suffix);\npublic static ValuePatterns suffixEqualsIgnoreCaseMatch(final String suffix);\npublic static ValuePatterns equalsIgnoreCaseMatch(final String value);\npublic static ValuePatterns wildcardMatch(final String value);\npublic static AnythingBut anythingButMatch(final String anythingBut);\npublic static AnythingBut anythingButMatch(final Set<String> anythingButs);\npublic static AnythingBut anythingButMatch(final double anythingBut);\npublic static AnythingBut anythingButNumberMatch(final Set<Double> anythingButs);\npublic static AnythingButValuesSet anythingButPrefix(final String prefix);\npublic static AnythingButValuesSet anythingButPrefix(final Set<String> anythingButs);\npublic static AnythingButValuesSet anythingButSuffix(final String suffix);\npublic static AnythingButValuesSet anythingButSuffix(final Set<String> anythingButs);\npublic static AnythingButValuesSet anythingButIgnoreCaseMatch(final String anythingBut);\npublic static AnythingButValuesSet anythingButIgnoreCaseMatch(final Set<String> anythingButs);\npublic static ValuePatterns numericEquals(final double val);\npublic static Range lessThan(final double val);\npublic static Range lessThanOrEqualTo(final double val);\npublic static Range greaterThan(final double val);\npublic static Range greaterThanOrEqualTo(final double val);\npublic static Range between(final double bottom, final boolean openBottom, final double top, final boolean openTop);\n```\n\nOnce you have constructed appropriate `Patterns` matchers with these methods, you can use the\nfollowing methods to add to or delete from your machine:\n\n```java\npublic void addPatternRule(final String name, final Map<String, List<Patterns>> namevals);\npublic void deletePatternRule(final String name, final Map<String, List<Patterns>> namevals);\n```\n\nNOTE: The cautions listed in [`deleteRule()`](#deleterule) apply\nto `deletePatternRule()` as well.\n\n## JSON text matching\n\nThe field values in rules must be provided in their JSON representations.\nThat is to say, string values must be enclosed in \"quotes\". Unquoted values\nare allowed, such as numbers (`-3.0e5`) and certain JSON-specific literals (`true`,\n`false`, and `null`).\n\nThis can be entirely ignored if rules are provided to `addRule()`() in JSON form,\nor if you are working with Patterns as opposed to literal strings.\nBut if you are providing rules as name/value pairs, and you want to specify\nthat the field \"xyz\" matches the string \"true\", that has to be expressed as\n`\"xyz\", \"\\\"true\\\"\"`.  On the other hand, `\"xyz\", \"true\"` would match only the\nJSON literal `true`.\n\n## JSON Array Matching\n\nRuler supports rule-matching for events containing arrays, but only when the event\nis provided in JSON form - when it's a list of pre-sorted fields, the array structure\nin the event is lost.  The behavior also depends on whether you use `rulesForEvent()`\nor `rulesForJSONEvent`.\n\nConsider the following Event.\n\n\n```javascript\n{\n  \"employees\":[\n    { \"firstName\":\"John\", \"lastName\":\"Doe\" },\n    { \"firstName\":\"Anna\", \"lastName\":\"Smith\" },\n    { \"firstName\":\"Peter\", \"lastName\":\"Jones\" }\n  ]\n}\n```\n\nThen this rule will match:\n\n```javascript\n{ \"employees\": { \"firstName\": [ \"Anna\" ] } }\n```\n\nThat is to say, the array structure is \"crushed out\" of the rule pattern,\nand any contained objects are treated as if they are the value of the\nparent field.  This works for multi-level arrays too:\n\n```javascript\n{\n  \"employees\":[\n    [\n      { \"firstName\":\"John\", \"lastName\":\"Doe\" },\n      { \"firstName\":\"Anna\", \"lastName\":\"Smith\" }\n    ],\n    [\n      { \"firstName\":\"Peter\", \"lastName\":\"Jones\" }\n    ]\n  ]\n}\n```\n\nIn earlier versions of Ruler, the only Machine-based matching method\nwas `rulesForEvent()` which unfortunately will also match the following rule:\n\n```javascript\n{ \"employees\": { \"firstName\": [ \"Anna\" ], \"lastName\": [ \"Jones\" ] } }\n```\n\nAs a fix, Ruler introduced `rulesForJSONEvent()` which, as the name suggests, only\nmatches events provided in JSON form. `rulesForJsonEvent()` will *not* match the\n\"Anna\"/\"Jones\" rule above.\n\nFormally: `rulesForJSONEvent()` will refuse to recognize any match in which\nany two fields are within JSON objects that are in different elements of the same array.\nIn practice, this means that it does about what you would expect.\n\n## Compiling and checking rules\n\nThere is a supporting class `com.amazon.fsm.ruler.RuleCompiler`.  It contains a\nmethod named `check()` which accepts a JSON rule definition and returns a\nString value which, if null, means that the rule was syntactically valid.  If\nthe return value is non-Null it contains a human-readable error message\ndescribing the problem.\n\nFor convenience, it also contains a method named `compile()` which works just\nlike `check()` but signals an error by throwing an IOException and, on\nsuccess, returns a `Map<String>, List<String>>` in the form that Machine's\n`addRule()` method expects. Since the Machine class uses this internally,\nthis method may be a time-saver.\n\n#### Caveat: Compiled rules and JSON keys with dots\n\nWhen Ruler compiles keys, it uses dot (`.`) as the joining character. This means \nit will compile the following two rules to the same internal representation \n\n```javascript\n## has no dots in keys\n{ \"detail\" : { \"state\": { \"status\": [ \"running\" ] } } }\n\n## has dots in keys\n{ \"detail\" : { \"state.status\": [ \"running\" ] } }\n```\n\nIt also means that these rules will match against following two events : \n\n```javascript\n## has no dots in keys\n{ \"detail\" : { \"state\": { \"status\": \"running\" } } }\n\n## has dots in keys\n{ \"detail\" : { \"state.status\": \"running\"  } }\n```\n\nThis behaviour may change in future version (to avoid any confusions) and should not be relied upon.\n\n## Performance\n\nWe measure Ruler's performance by compiling multiple rules into a Machine and matching events provided as JSON strings.\n\nA benchmark which processes 213,068 JSON events with average size about 900 bytes against 5 each exact-match,\nprefix-match, suffix-match, equals-ignore-case-match, wildcard-match, numeric-match, and anything-but-match rules and\ncounts the matches, yields the following on a 2019 MacBook:\n\nEvents are processed at over 220K/second except for:\n - equals-ignore-case matches, which are processed at over 200K/second.\n - prefix/equals-ignore-case matches, which are processed at over 200K/second.\n - suffix/equals-ignore-case matches, which are processed at over 200K/second.\n - wildcard matches, which are processed at over 170K/second.\n - anything-but matches, which are processed at over 150K/second.\n - numeric matches, which are processed at over 120K/second.\n - complex array matches, which are processed at over 2.5K/second.\n\n### Suggestions for better performance\n\nHere are some suggestions on processing rules and events:\n1. If your team is still using old API -- rulesForEvent, switch to rulesForJSONEvent API. Due to limited resource, old API will not be maintained well thought contributions are always welcomed.\n2. If your team does event flattening by yourself,  you are recommended to use Ruler to flatten the event, just pass Json string or Json node. We have many optimizations within Ruler parsing code.\n3. if your team does Rule Json parsing by yourself, you are recommended to just pass the Json described rule string directly to Ruler, in which will do some pre-processing, e.g. add \u201c\u201d.\n4. In order to well protect the system and prevent ruler from hitting worse condition, limit number of fields in event and rule, e.g. for big event, consider to split to multiple small event and call ruler multiple times. while number of rule is purely depending on your memory budget which is up to you to decide that, but number of fields described in the rule is most important and sensitive on performance, if possible, try to design it as small as possible.\n\nFrom performance consideration, Ruler is sensitive on below items, so, when you design the schema of your event and rule, here are some suggestions:\n1. Try to make Key be diverse both in event and rules, the more heterogeneous fields in event and rule, the higher performance.\n2. Shorten number of fields inside rules, the less key in the rules, the short path to find them out.\n3. Shorten number of fields inside event,  the less key inside event, the less attempts will be required to find out rules.\n4. Shorten number of possible value in [\u2026](e.g. \u201ca\u201d:[1,2,3 \u2026100] ) both inside event and rules, the more value, the more branches produced in FSM to iterator, then the more time takes for matching.\n\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License. See [LICENSE](LICENSE) for more information.\n\n\n", "release_dates": ["2024-02-06T18:53:26Z", "2024-01-22T20:22:02Z", "2023-11-23T16:36:33Z", "2023-11-22T00:21:01Z", "2023-10-25T03:05:05Z", "2023-10-11T02:43:29Z", "2023-05-04T17:50:41Z", "2023-04-01T01:47:33Z", "2023-02-16T18:45:54Z", "2023-02-03T04:58:46Z", "2022-11-07T21:38:36Z", "2022-10-06T23:14:42Z", "2022-10-02T07:04:01Z", "2022-09-06T15:36:08Z"]}, {"name": "firelens-datajet", "description": "Route test data to Fluent Bit flexibly", "language": "EJS", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "\n# Firelens Datajet\nRoute test data to Fluent Bit flexibly.\nThis system can be run locally with a local Fluent Bit process or compiled to a docker image and run with a sidecar aws-for-fluent-bit container.\n\nThe purpose of Firelens-Datajet is to abstract test configuration from implementation code\nIt does so by running tests based on a single JSON file and outputing the results.\n\nFuture work on this system involves making a REST interface for tests to be invoked by sending a POST request with the test configuration and responding with a test result JSON as the response.\n\nThe current driver.ts does not support multiple stages, however the core does support async and sync stages as well as validators.\nThe goal of this project is to eventually encapsulate test stages, validation wrappers, and data transmission in a portable JSON which can be sent to this program and executed at runtime.\n\n# Prerequisite Installation Instructions for Amazon Linux\nTo install NodeJS on Amazon Linux, use the following commands\n```\n# Source: https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-up-node-on-ec2-instance.html\n\n# Install Node Version Manager\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | bash\n. ~/.nvm/nvm.sh\nnvm install node\n\n# Verify Node is installed\nnode -e \"console.log('Running Node.js ' + process.version)\"\n\n# Add helpful aliases for VSCode\nNODE_PATH=$(whereis node | awk '{print $NF}')\nNPM_PATH=$(whereis npm | awk '{print $NF}')\nsudo ln -s $NODE_PATH /usr/bin/node\nsudo ln -s $NPM_PATH /usr/bin/npm\n\n# Verify added aliases (should see two locations)\nwhereis npm\nwhereis node\n```\nCMake will also need to be installed if Execution wrappers are used, as the testing\nframework can manage Fluent Bit compilation via CMake.\n# Setup instructions\nTo run Firelens Datajet locally please install NPM: https://nodejs.org/en/ and run the following commands\n```\ncd firelens-datajet\nnpm install\nnpm start\n```\nA .env file can be added to the project to configure environment variables while testing locally. The file might look something like:\n```\n# ---------- Environment Variables ----------\nCLIENTS='[\"environment\", \"request\", \"file\"]'\nCLIENT_REQUEST_PORT=3334\nCLIENT_REQUEST_ACCESS_TOKEN=123OPENSESAME\nCLIENT_ENVIRONMENT_CONFIG='{\"generator\": {\"name\": \"increment\", \"config\": {\"batchSize\": 100, \"waitTime\": 0.050}}, \"datajet\": {\"name\": \"firelens\", \"config\": {\"logStream\": \"stderr\"}}, \"stage\": {\"batchRate\": 1, \"batchLimit\": 5}}'\nCLIENT_FILE_NAME='firelens-datajet.json'\n\n```\n# Examples\nSee the `/examples` folder for a set of testing configuration files.\nThe simplest way to configure FireLens Datajet test framework is with a firelens-datajet.json file.\n1. Find an example JSON from the `/examples` folder and copy it to `./firelens-datajet/firelens-datajet.json`.\n2. If you haven't installed the FireLens Datajet dependencies, then run `npm install` from the root of the repository.\n3. Then build and run the testing system with `npm start`.\n\nA good first example to use is `/examples/tcp-forward-file-input/firelens-datajet.json`. This example shows Fluent Bit build and\nexecuted, then logs sent in parallel to tcp, forward, and file inputs, and logs, instrumentation data, and byproducts from tail\noutput plugin captured. You can see this in the `./firelens-datajet/output` folder. A new `output/fluent-lock-hash` folder will\nbe created in the output folder to help organize testing results. The hash is a hash of the Fluent Bit source commits, and the\nFluent Bit configuration template. Commits and config template are automatically documented in the `output/fluent-lock-hash folder`.\nIndividual tests will be added to this folder in a subfolder and timestamped.\nAfter running the test, take a look at the auto-generated output folder's contents\n```\n[firelens-datajet]$ tree ./output\n./output\n\u2514\u2500\u2500 fluent-lock-41dce83e498df910ba21a2b4c02bbc15\n    \u251c\u2500\u2500 fluent-bit-template.conf\n    \u251c\u2500\u2500 fluent-lock.json\n    \u251c\u2500\u2500 source-lock-info.json\n    \u251c\u2500\u2500 test-2022-03-14T18:11:28.588Z\n    \u2502   \u251c\u2500\u2500 byproduct\n    \u2502   \u251c\u2500\u2500 fluent-bit.conf\n    \u2502   \u251c\u2500\u2500 instrumentation\n    \u2502   \u251c\u2500\u2500 logs\n    \u2502   \u2502   \u251c\u2500\u2500 cmake.log\n    \u2502   \u2502   \u251c\u2500\u2500 fluent-bit-2022-03-14T18:11:21.886Z.log\n    \u2502   \u2502   \u2514\u2500\u2500 make.log\n    \u2502   \u251c\u2500\u2500 source.json\n    \u2502   \u2514\u2500\u2500 test-pipeline-schema.json\n    \u2514\u2500\u2500 test-2022-03-14T18:15:18.374Z\n        \u251c\u2500\u2500 byproduct\n        \u2502   \u251c\u2500\u2500 forwardLogs\n        \u2502   \u251c\u2500\u2500 tailLogs\n        \u2502   \u2514\u2500\u2500 tcpLogs\n        \u251c\u2500\u2500 fluent-bit.conf\n        \u251c\u2500\u2500 instrumentation\n        \u2502   \u2514\u2500\u2500 647281770073\n        \u2502       \u2514\u2500\u2500 flb_engine_ready_coroutines.csv\n        \u251c\u2500\u2500 logs\n        \u2502   \u251c\u2500\u2500 cmake.log\n        \u2502   \u251c\u2500\u2500 fluent-bit-2022-03-14T18:15:17.267Z.log\n        \u2502   \u2514\u2500\u2500 make.log\n        \u251c\u2500\u2500 source.json\n        \u2514\u2500\u2500 test-pipeline-schema.json\n```\n# Containerization\nFirelens datajet can be contained in a Docker image.\n## Create docker image\n```\nmake\n```\n## ECR quick publish procedure\n```\nmake publish tag=\"0.1.0\"\n```\n\n\n# Test Definition\nFirelens Datajet currently supports configuration only with file, environment variable, and request.\nDocumentation on the format of a test definition is still in progress, but in summary, the test definition\n- chooses from a data generator which generates data\n- sends data via a datajet which outputs data in a way that is accessable to Fluent Bit\n- sets some values such as batch rate and batch limits\n\nIn the future, test definitions should support multiple\nsynchronous and asynchronous stages, as well as validation wrappers.\n\n# Clients\nClients are methods for obtaining test configuration and can be selected and configured via environment variables.\nThe following clients are supported:\n- Request: tests are posted to the url/execute endpoint\n- File: tests description found in file\n- Environment: tests description is found as environment variable\n\nA single or multiple clients can be selected with the environment variable CLIENT\n```\n# A variable number of clients can be selected\nCLIENTS='[\"environment\", \"request\", \"environment\"]'\n```\n\n## Client Configuration\nOther environment variables are used to configure specific clients\n### Request\n```\nCLIENTS='[\"request\"]'\n\n# Provide an access token to secure the /execute endpoint\nCLIENT_REQUEST_ACCESS_TOKEN=<access_token>\n# Change the port FirelensDatajet client listens on\nCLIENT_REQUEST_PORT=3334\n```\n\n### Environment\n```\nCLIENTS='[\"environment\"]'\n\n# Test definition\nCLIENT_ENVIRONMENT_CONFIG='{\"generator\": {\"name\": \"increment\", \"config\": {\"batchSize\": 100, \"waitTime\": 0.050}}, \"datajet\": {\"name\": \"firelens\", \"config\": {\"logStream\": \"stderr\"}}, \"stage\": {\"batchRate\": 1, \"batchLimit\": 5}}'\n```\n\n### File\n(this client may need to be updated to allow for root access)\n```\nCLIENTS='[\"file\"]'\n\nCLIENT_FILE_NAME='firelens-datajet.json'\n```\n\n# Request Client\n\n## Environment variables\n- CLIENT_REQUEST_ACCESS_TOKEN: A secret token used to secure request client endpoints via bearer_token (optional), defaults to no security.\n- CLIENT_REQUEST_PORT: The port Firelens Datajet will listen on for test configurations (optional), defaults to 3333\n\n## Generating the ACCESS_TOKEN\nSet environment variable ACCESS_TOKEN to the following and save the value.\n```\nnpm run generate-access-token\n```\n```\nCLIENT_REQUEST_ACCESS_TOKEN=<myaccesstoken>\n```\n\n## Example Request\nThe following request uses the increment data generator and forwards logs to Fire Lens via stdout datajet\n> POST http://localhost:3333/execute \\\n> Bearer Token: `<myaccesstoken>`\n```\n    {\n        \"generator\": {\n            \"name\": \"increment\",\n            \"config\": {\n                \"batchSize\": 1,\n                \"waitTime\": 0.050\n            }\n        },\n        \"datajet\": {\n            \"name\": \"stdout\",\n            \"config\": {\n                \"logStream\": \"stderr\"\n            }\n        },\n        \"stage\": {\n            \"batchRate\": 1000,\n            \"batchLimit\": 10\n        }\n    }\n```\n## Example Response\n- Response: 200 OK\n- Body:\n```\n{\n    \"testId\": 2,\n    \"status\": \"Execution success. Validation success\",\n    \"metrics\": [],\n    \"testConfiguration\": {\n        \"generator\": {\n            \"name\": \"increment\",\n            \"config\": {\n                \"batchSize\": 1,\n                \"waitTime\": 0.05\n            }\n        },\n        \"datajet\": {\n            \"name\": \"stdout\",\n            \"config\": {\n                \"logStream\": \"stderr\"\n            }\n        },\n        \"stage\": {\n            \"batchRate\": 1000,\n            \"batchLimit\": 10\n        }\n    },\n    \"executionResult\": {\n        \"builtStage\": {\n            \"children\": [],\n            \"stageLeaf\": {\n                \"generator\": {\n                    \"generatorTemplate\": {\n                        \"name\": \"increment\",\n                        \"defaultConfig\": {\n                            \"batchSize\": 10\n                        }\n                    }\n                },\n                \"datajet\": {\n                    \"datajetTemplate\": {\n                        \"name\": \"stdout\",\n                        \"defaultConfig\": {\n                            \"logStream\": \"auto\",\n                            \"defaultStream\": \"stdout\"\n                        }\n                    }\n                },\n                \"config\": {\n                    \"batchRate\": 1000,\n                    \"batchLimit\": 10\n                }\n            },\n            \"type\": \"stage\"\n        },\n        \"isValidationSuccess\": true,\n        \"isExecutionSuccess\": true,\n        \"pendingValidators\": [],\n        \"children\": []\n    }\n}\n```\n\n\n# Example ECS Task Configuration\n```\n{\n    \"family\": \"FirelensDatajetTaskDefinition\",\n    \"taskRoleArn\": null,\n    \"executionRoleArn\": \"arn:aws:iam::826489191740:role/ecsTaskExecutionRole\",\n    \"containerDefinitions\": [\n        {\n            \"essential\": true,\n            \"image\": \"<aws_for_fluent_bit_image>\",\n            \"name\": \"log_router\",\n            \"firelensConfiguration\": {\n                \"type\": \"fluentbit\"\n            },\n            \"logConfiguration\": {\n                \"logDriver\": \"awslogs\",\n                \"options\": {\n                    \"awslogs-group\": \"firelens-datajet-client-test\",\n                    \"awslogs-region\": \"us-west-2\",\n                    \"awslogs-create-group\": \"true\",\n                    \"awslogs-stream-prefix\": \"fluent-bit-\"\n                }\n            },\n            \"memoryReservation\": 50\n        },\n        {\n            \"essential\": true,\n            \"image\": \"<firelens_datajet_image>\",\n            \"name\": \"firelensDatajet\",\n\t\t        \"environment\": [\n                {\n                    \"name\": \"CLIENTS\",\n                    \"value\": \"[\\\"environment\\\", \\\"request\\\"]\"\n                },\n                {\n                    \"name\": \"CLIENT_REQUEST_PORT\",\n                    \"value\": \"80\"\n                },\n                {\n                    \"name\": \"CLIENT_REQUEST_ACCESS_TOKEN\",\n                    \"value\": \"<access_token>\"\n                },\n                {\n                    \"name\": \"CLIENT_ENVIRONMENT_CONFIG\",\n                    \"value\": \"{\\\"generator\\\": {\\\"name\\\": \\\"increment\\\", \\\"config\\\": {\\\"batchSize\\\": 100, \\\"waitTime\\\": 0.050}}, \\\"datajet\\\": {\\\"name\\\": \\\"stdout\\\", \\\"config\\\": {\\\"logStream\\\": \\\"stderr\\\"}}, \\\"stage\\\": {\\\"batchRate\\\": 1, \\\"batchLimit\\\": 5}}\"\n                },\n                {\n                    \"name\": \"CLIENT_FILE_NAME\",\n                    \"value\": \"firelens-datajet.json\"\n                }\n            ],\n            \"portMappings\": [\n                {\n                    \"hostPort\": 80,\n                    \"protocol\": \"tcp\",\n                    \"containerPort\": 80\n\t              }\n            ],\n            \"logConfiguration\": {\n                \"logDriver\": \"awsfirelens\",\n                \"options\": {\n                    \"Name\": \"cloudwatch_logs\",\n                    \"region\": \"us-west-2\",\n                    \"log_group_name\": \"firelens-datajet-client-test\",\n                    \"auto_create_group\": \"true\",\n                    \"log_stream_name\": \"client-\"\n                }\n            },\n            \"memoryReservation\": 100\n        }\n    ],\n    \"inferenceAccelerators\": [],\n    \"volumes\": [],\n    \"placementConstraints\": [],\n    \"memory\": null,\n    \"cpu\": null,\n    \"tags\": []\n}\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2022-03-14T19:17:19Z", "2022-03-07T23:41:08Z"]}, {"name": "Fleet-Provisioning-for-AWS-IoT-embedded-sdk", "description": "Client library for using AWS IoT Fleet Provisioning service on embedded devices", "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# AWS IoT Fleet Provisioning Library\n\n**[API Documentation Pages for current and previous releases of this library can be found here](https://aws.github.io/Fleet-Provisioning-for-AWS-IoT-embedded-sdk/)**\n\nThe Fleet Provisioning library enables you to provision IoT devices without\ndevice certificates using the [Fleet Provisioning feature of AWS IoT Core][a1].\nFor an overview of provisioning options available, see [Device\nprovisioning][a2]. This library has no dependencies on any additional libraries\nother than the standard C library, and therefore, can be used with any MQTT\nlibrary. This library is distributed under the [MIT Open Source License][a3].\n\n[a1]:\n  https://docs.aws.amazon.com/iot/latest/developerguide/provision-wo-cert.html\n[a2]: https://docs.aws.amazon.com/iot/latest/developerguide/iot-provision.html\n[a3]: LICENSE\n\nThis library has gone through code quality checks including verification that no\nfunction has a [GNU Complexity][a4] score over 8, and checks against deviations\nfrom mandatory rules in the [MISRA coding standard][a5]. Deviations from the\nMISRA C:2012 guidelines are documented under [MISRA Deviations][a6]. This\nlibrary has also undergone static code analysis using [Coverity static\nanalysis][a7], and validation of memory safety through the [CBMC automated\nreasoning tool][a8].\n\n[a4]: https://www.gnu.org/software/complexity/manual/complexity.html\n[a5]: https://www.misra.org.uk\n[a6]: MISRA.md\n[a7]: https://scan.coverity.com/\n[a8]: https://www.cprover.org/cbmc/\n\nSee memory requirements for this library [here][a9].\n\n[a9]: ./docs/doxygen/include/size_table.md\n\n**AWS IoT Fleet Provisioning Library v1.1.0\n[source code](https://github.com/aws/Fleet-Provisioning-for-AWS-IoT-embedded-sdk/tree/v1.1.0/source)\nis part of the\n[FreeRTOS 202210.00 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202210.00-LTS)\nrelease.**\n\n## AWS IoT Fleet Provisioning Library Config File\n\nThe AWS IoT Fleet Provisioning Library exposes build configuration macros that\nare required for building the library. A list of all the configurations and\ntheir default values are defined in [fleet_provisioning_config_defaults.h][b1].\nTo provide custom values for the configuration macros, a config file named\n`fleet_provisioning_config.h` can be provided by the application to the library.\n\n[b1]: source/include/fleet_provisioning_config_defaults.h\n\nBy default, a `fleet_provisioning_config.h` config file is required to build the\nlibrary. To disable this requirement and build the library with default\nconfiguration values, provide `FLEET_PROVISIONING_DO_NOT_USE_CUSTOM_CONFIG` as a\ncompile time preprocessor macro.\n\n**Thus, the Fleet Provisioning library can be built by either**:\n\n- Defining a `fleet_provisioning_config.h` file in the application, and adding\n  it to the include directories list of the library.\n\n**OR**\n\n- Defining the `FLEET_PROVISIONING_DO_NOT_USE_CUSTOM_CONFIG` preprocessor macro\n  for the library build.\n\n## Building the Library\n\nThe [fleetprovisioningFilePaths.cmake][c1] file contains the information of all\nsource files and the header include paths required to build the Fleet\nProvisioning library.\n\n[c1]: fleetprovisioningFilePaths.cmake\n\nAs mentioned in the previous section, either a custom config file (i.e.\n`fleet_provisioning_config.h`) or `FLEET_PROVISIONING_DO_NOT_USE_CUSTOM_CONFIG`\nmacro needs to be provided to build the Fleet Provisioning library.\n\nFor a CMake example of building the Fleet Provisioning library with the\n`fleetprovisioningFilePaths.cmake` file, refer to the `coverity_analysis`\nlibrary target in [test/CMakeLists.txt][c2] file.\n\n[c2]: test/CMakeLists.txt\n\n## Building Unit Tests\n\n### Platform Prerequisites\n\n- For running unit tests:\n  - **C90 compiler** like gcc.\n  - **CMake 3.13.0 or later**.\n- For running the coverage target, **gcov** and **lcov** are additionally\n  required.\n\n### Steps to build **Unit Tests**\n\n1. Go to the root directory of this repository.\n\n1. Run the _cmake_ command:\n   `cmake -S test -B build -DBUILD_CLONE_SUBMODULES=ON`.\n\n1. Run this command to build the library and unit tests: `make -C build all`.\n\n1. The generated test executables will be present in `build/bin/tests` folder.\n\n1. Run `cd build && ctest` to execute all tests and view the test run summary.\n\n## CBMC\n\nTo learn more about CBMC and proofs specifically, review the training material\n[here](https://model-checking.github.io/cbmc-training).\n\nThe `test/cbmc/proofs` directory contains CBMC proofs.\n\nIn order to run these proofs you will need to install CBMC and other tools by\nfollowing the instructions\n[here](https://model-checking.github.io/cbmc-training/installation.html).\n\n## Reference examples\n\nThe [AWS IoT Embedded C-SDK repository][e1] contains a demo showing the use of\nthe AWS IoT Fleet Provisioning Library on a POSIX platform [here][e2].\n\n[e1]: https://github.com/aws/aws-iot-device-sdk-embedded-C\n[e2]:\n  https://github.com/aws/aws-iot-device-sdk-embedded-C/tree/main/demos/fleet_provisioning/fleet_provisioning_with_csr\n\n## Generating documentation\n\nThe Doxygen references were created using Doxygen version 1.9.2. To generate the\nDoxygen pages, please run the following command from the root of this\nrepository:\n\n```sh\ndoxygen docs/doxygen/config.doxyfile\n```\n\n## Contributing\n\nSee [CONTRIBUTING.md][g1] for information on contributing.\n\n[g1]: .github/CONTRIBUTING.md\n", "release_dates": ["2022-10-14T20:39:30Z", "2021-12-15T23:38:17Z", "2021-08-16T23:25:46Z"]}, {"name": "fmeval", "description": "Foundation Model Evaluations Library", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Foundation Model Evaluations Library\n`fmeval` is a library to evaluate Large Language Models (LLMs) in order to help select the best LLM\nfor your use case. The library evaluates LLMs for the following tasks:\n* Open-ended generation - The production of natural human responses to text that does not have a pre-defined structure.\n* Text summarization - The generation of a condensed summary retaining the key information contained in a longer text.\n* Question Answering - The generation of a relevant and accurate response to an answer.\n* Classification - Assigning a category, such as a label or score to text, based on its content.\n\nThe library contains\n* Algorithms to evaluate LLMs for Accuracy, Toxicity, Semantic Robustness and\n  Prompt Stereotyping across different tasks.\n* Implementations of the `ModelRunner` interface. `ModelRunner` encapsulates the logic for invoking different types of LLMs, exposing a `predict`\n  method to simplify interactions with LLMs within the eval algorithm code. We have built-in support for AWS SageMaker Jumpstart Endpoints, AWS SageMaker Endpoints and Bedrock Models. The user can extend the interface for their own model classes by implementing the `predict` method.\n\n\n## Installation\n`fmeval` is developed under python3.10. To install the package from PIP you can simply do:\n\n```\npip install fmeval\n```\n\n## Usage\nYou can see examples of running evaluations on your LLMs with built-in or custom datasets in\nthe [examples folder](https://github.com/aws/fmeval/tree/main/examples).\n\nThe main steps for using `fmeval` are:\n1. Create a [ModelRunner](https://github.com/aws/fmeval/blob/main/src/fmeval/model_runners/model_runner.py)\n   which can perform invocations on your LLM. We have built-in support for\n   [AWS SageMaker Jumpstart Endpoints](https://github.com/aws/fmeval/blob/main/src/fmeval/model_runners/sm_jumpstart_model_runner.py),\n   [AWS SageMaker Endpoints](https://github.com/aws/fmeval/blob/main/src/fmeval/model_runners/sm_model_runner.py)\n   and [AWS Bedrock Models](https://github.com/aws/fmeval/blob/main/src/fmeval/model_runners/bedrock_model_runner.py).\n   You can also extend the ModelRunner interface for any LLMs hosted anywhere.\n2. Use any of the supported [eval_algorithms](https://github.com/aws/fmeval/tree/main/src/fmeval/eval_algorithms).\n\nFor example,\n```\nfrom fmeval.eval_algorithms.toxicity import Toxicity, ToxicityConfig\n\neval_algo = Toxicity(ToxicityConfig())\neval_output = eval_algo.evaluate(model=model_runner)\n```\n*Note: You can update the default eval config parameters for your specific use case.*\n\n### Using a custom dataset for an evaluation\nWe have our built-in datasets configured, which are consumed for computing the scores in eval algorithms.\nYou can choose to use a custom dataset in the following manner.\n1. Create a [DataConfig](https://github.com/aws/fmeval/blob/main/src/fmeval/data_loaders/data_config.py)\n   for your custom dataset\n```\nconfig = DataConfig(\n    dataset_name=\"custom_dataset\",\n    dataset_uri=\"./custom_dataset.jsonl\",\n    dataset_mime_type=\"application/jsonlines\",\n    model_input_location=\"question\",\n    target_output_location=\"answer\",\n)\n```\n\n2. Use an eval algorithm with a custom dataset\n```\neval_algo = Toxicity(ToxicityConfig())\neval_output = eval_algo.evaluate(model=model_runner, dataset_config=config)\n```\n\n*Please refer to the [developer guide](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-foundation-model-evaluate-auto.html) and\n[examples](https://github.com/aws/fmeval/tree/main/examples) for more details around the usage of\neval algorithms.*\n\n## Troubleshooting\n\n1. If you you run into the error `error: can't find Rust compiler` while installing on a Mac, please try running the steps below.\n\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nrustup install 1.72.1\nrustup default 1.72.1-aarch64-apple-darwin\nrustup toolchain remove stable-aarch64-apple-darwin\nrm -rf $HOME/.rustup/toolchains/stable-aarch64-apple-darwin\nmv $HOME/.rustup/toolchains/1.72.1-aarch64-apple-darwin $HOME/.rustup/toolchains/stable-aarch64-apple-darwin\n```\n\n2. If you run into OOM errors, especially while running evaluations that use LLMs as evaluators like toxicity and\nsummarization accuracy, it might be happening because your machine does not have enough memory to load the evaluator\nmodels on to all the cores available to it by default to maximize parallelization. To reduce parallelization, users can\nset the environment variable `PARALLELIZATION_FACTOR` to a value that works on their machine. This reduces the number of\ncores on to which these models are loaded on to, and avoids running into OOM errors.\n\n## Development\n\n### Setup and the use of `devtool`\nOnce you have created a virtual environment with python3.10, run the following command to setup the development environment:\n```\n./devtool install_deps_dev\n./devtool install_deps\n./devtool all\n```\n\nBefore submitting a PR, rerun `./devtool all` for testing and linting. It should run without errors.\n\n### Adding python dependencies\nWe use [poetry](https://python-poetry.org/docs/) to manage python dependencies in this project. If you want to add a new\ndependency, please update the [pyproject.toml](./pyproject.toml) file, and run the `poetry update` command to update the\n`poetry.lock` file (which is checked in).\n\nOther than this step to add dependencies, use devtool commands for installing dependencies, linting and testing. Execute the command `./devtool` without any arguments to see a list of available options.\n\n### Adding your own Eval Algorithm\n\n*Details TBA*\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2024-02-21T00:02:23Z", "2023-12-13T17:41:20Z", "2023-12-07T00:38:39Z", "2023-11-29T02:20:15Z"]}, {"name": "git-remote-codecommit", "description": "An implementation of Git Remote Helper that makes it easier to interact with AWS CodeCommit", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": []}, {"name": "glide-for-redis", "description": null, "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# GLIDE for Redis\nGeneral Language Independent Driver for the Enterprise (GLIDE) for Redis, is an AWS-sponsored, open-source Redis client. GLIDE for Redis works with any Redis distribution that adheres to the Redis Serialization Protocol (RESP) specification, including open-source Redis, Amazon ElastiCache for Redis, and Amazon MemoryDB for Redis.\nStrategic, mission-critical Redis-based applications have requirements for security, optimized performance, minimal downtime, and observability.  GLIDE for Redis is designed to provide a client experience that helps meet these objectives. It is sponsored and supported by AWS, and comes pre-configured with best practices learned from over a decade of operating Redis-compatible services used by hundreds of thousands of customers. To help ensure consistency in development and operations, GLIDE for Redis is implemented using a core driver framework, written in Rust, with extensions made available for each supported programming language. This design ensures that updates easily propagate to each language and reduces overall complexity. In this Preview release, GLIDE for Redis is available for Python and Javascript (Node.js), with support for Java actively under development.\n\n## Supported Redis Versions\nGLIDE for Redis is API-compatible with open source Redis version 6 and 7.\n\n## Current Status\nWe've made GLIDE for Redis an open-source project, and are releasing it in Preview to the community to gather feedback, and actively collaborate on the project roadmap. We welcome questions and contributions from all Redis stakeholders. \nThis preview release is recommended for testing purposes only. It is available in Python and Javascript (Node.js), with Java to follow. We're tracking its production readiness and future features on the [roadmap](https://github.com/orgs/aws/projects/187/).\n\n\n## Getting Started\n\n-   [Node](./node/README.md)\n-   [Python](./python/README.md)\n-   [Documentation](https://github.com/aws/glide-for-redis/wiki)\n\n## Getting Help\nIf you have any questions, feature requests, encounter issues, or need assistance with this project, please don't hesitate to open a GitHub issue. Our community and contributors are here to help you. Before creating an issue, we recommend checking the [existing issues](https://github.com/aws/glide-for-redis/issues) to see if your question or problem has already been addressed. If not, feel free to create a new issue, and we'll do our best to assist you. Please provide as much detail as possible in your issue description, including: \n\n1. A clear and concise title\n2. Detailed description of the problem or question\n3. A reproducible test case or series of steps\n4. The GLIDE for Redis version in use\n5. Operating system\n6. Redis version\n7. Redis cluster information, cluster topology, number of shards, number of replicas, used data types\n8. Any modifications you've made that are relevant to the issue\n9. Anything unusual about your environment or deployment\n10. Log files\n\n\n## Contributing\n\nGitHub is a platform for collaborative coding. If you're interested in writing code, we encourage you to contribute by submitting pull requests from forked copies of this repository. Additionally, please consider creating GitHub issues for reporting bugs and suggesting new features. Feel free to comment on issues that interest. For more info see [Contributing](./CONTRIBUTING.md).\n\n## License\n* [Apache License 2.0](./LICENSE)\n", "release_dates": ["2024-02-13T08:15:32Z", "2024-01-17T21:51:38Z", "2024-01-16T20:26:53Z"]}, {"name": "go-kafka-event-source", "description": "Go/Kafka client library for developing event sourcing applications", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Go Kafka Event Source\n\nGKES (Go Kafka Event Source) attempts to fill the gaps ub the Go/Kafka library ecosystem. It supplies EOS (Exactly Once Semantics),\nlocal state stores and incremental consumer rebalancing to Go Kafka consumer applications, making it a viable alternative to\na traditional Kafka Streams application written in Java.\n\n## Getting Started\n\nThis repository is organized into several modules. The primary module being \"streams\".\nTo use GKES in your go program, simply add it as you would any other Go module.\n\n```\ngo get github.com/aws/go-kafka-event-source/streams\n```\n\nIf you are using AWS MSK, you may find the provided \"msk\" module useful for cluster connectivity, though this is not required.\n\n```\ngo get github.com/aws/go-kafka-event-source/msk\n```\n\nFor API documentation on these modules, see https://pkg.go.dev/github.com/aws/go-kafka-event-source/streams and https://pkg.go.dev/github.com/aws/go-kafka-event-source/msk. The [docs](./docs) folder also contains detailed examples.\n\n## Project Status\n\nGKES is usable in it's current form but there are a few things slated for the very near future. This library is being used by AWS for internal workloads.\n\n- [X] More comprehensive documentation for and async processing - [Complete](./docs/asyncprocessing.md) \n- [ ] More comprehensive documentation for StateStores - In Progress\n- [X] Per topic partitioner support for EventContext.Forward() and Producer/BatchProducer APIs, Currently, only the Java default murmur2 partitioner is supported - Complete\n- [ ] Instructions for testing locally - In progress\n- [ ] More robust functional tests for async processing. Though this is tested extenisively using a local test harness, this process needs to be more repeatable and available to contributors - In progress\n\n## Compatibility Notes\n\nIt is recommnded tp use Go v1.19.2 or greater for GKES. There was a known compiler issue in previous versions of Go 1.19 which prevented modules using GKES from compiling. There was a back-port fix made to previous versions of Go, but it probably simpler and safer to update your Go environment to the latest available if you run into this issue.\n\nGKES has been extensively test with Kafka 3.2 but should be fine to use with any Kafka version > 2.5.1. Kafka versions < 2.5.1 are not likely to be compatible with GKES due to transaction semantics, and they have not been tested.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "graph-explorer", "description": "React-based web application that enables users to visualize both property graph and RDF data and explore connections between data without having to write graph queries. ", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Graph Explorer\n\nThe Graph Explorer project provides a React-based web application that enables users to visualize both property graph and RDF data and explore connections between data without having to write graph queries. You can connect to a graph database that supports either the [W3C RDF/SPARQL](https://www.w3.org/TR/sparql11-overview/) open standard or the open source [Apache TinkerPop Gremlin Server](https://tinkerpop.apache.org/).\n\nTo get started, you can deploy Graph Explorer on a local machine using [Docker Desktop](https://www.docker.com/products/docker-desktop/), or in the cloud using [Amazon EC2](https://aws.amazon.com/ec2/) or a container service like [Amazon ECS](https://aws.amazon.com/ecs/).\n\n![A sample image of property graph created by Graph Explorer](./images/LPGIMDb.png)\n![A sample image of RDF graph created by Graph Explorer](./images/RDFEPL.png)\n\n## Getting Started\n\nThis project contains the code needed to create a Docker image of the Graph Explorer. The image will create the Graph Explorer application and proxy server that will be served over the standard HTTP or HTTPS ports (HTTPS by default). The proxy server will be created automatically, but will only be necessary if you are connecting to Neptune. Gremlin-Server and BlazeGraph can be connected to directly. Additionally, the image will create a self-signed certificate that can be optionally used.\n\nThere are many ways to deploy the Graph Explorer application. The following instructions detail how to deploy graph-explorer onto an Amazon EC2 instance and use it as a proxy server with SSH tunneling to connect to Amazon Neptune. Note that this README is not an official recommendation on network setups as there are many ways to connect to Amazon Neptune from outside of the VPC, such as setting up a load balancer or VPC peering.\n\n### Prerequisites\n\n- Provision an Amazon EC2 instance that will be used to host the application and connect to Neptune as a proxy server. For more details, see instructions [here](https://github.com/aws/graph-notebook/tree/main/additional-databases/neptune).\n- Ensure the Amazon EC2 instance can send and receive on ports `22` (SSH), `8182` (Neptune), and `443` or `80` depending on protocol used (graph-explorer).\n- Open an SSH client and connect to the EC2 instance.\n- Download and install the necessary command line tools such as `git` and `docker`.\n\n### Steps to install Graph Explorer\n\n1. To download the source project, run `git clone https://github.com/aws/graph-explorer/`. Navigate to the newly created `graph-explorer` directory.\n2. To build the image, run `docker build -t graph-explorer .` from the root directory. If you receive an error relating to the docker service not running, run `service docker start`.\n3. Run `docker run -p 80:80 -p 443:443 --env HOST={hostname-or-ip-address} graph-explorer` to run the docker container.\n4. Now, open a browser and type in the public URL of your EC2 instance accessing the explorer endpoint (e.g., `https://ec2-1-2-3-4.us-east-1.compute.amazonaws.com/explorer`). You will receive a warning as the SSL certificate used is self-signed.\n5. Since the application is set to use HTTPS by default and contains a self-signed certificate, you will need to add the Graph Explorer certificates to the trusted certificates directory and manually trust them. See [HTTPS Connections](#https-connections) section.\n6. After completing the trusted certification step and refreshing the browser, you should now see the Connections UI. See below description on Connections UI to configure your first connection to Amazon Neptune.\n\n### Troubleshooting\n\n1. If the container does not start, or immediately stops, use `docker logs graph-explorer` to check the container console logs for any related error messages that might provide guidance on why graph-explorer did not start.\n2. If you are having issues connecting graph-explorer to your graph database, use your browser's Developer Tools feature to monitor both the browser console and network calls to determine if here are any errors related to connectivity.\n\n## Features\n\n### _Connections UI:_\n\nYou can create and manage connections to graph databases using this feature. Connections is accessible as the first screen after deploying the application, when you click `Open Connections` on the top-right. Click `+` on the top-right to add a new connection. You can also edit and delete connections.\n\n- **Add a new connection:**\n\n  - **Name:** Enter a name for your connection (e.g., `MyNeptuneCluster`).\n  - **Graph Type:** Choose a graph data model that corresponds to your graph database.\n  - **Public or proxy endpoint:** Provide the publicly accessible endpoint URL for a graph database, e.g., Gremlin Server. If connecting to Amazon Neptune, then provide a proxy endpoint URL that is accessible from outside the VPC, e.g., EC2.\n    - **Note:** For connecting to Amazon Neptune, ensure that the graph connection URL is in the format `https://[NEPTUNE_ENDPOINT]:8182`, and that the proxy endpoint URL is either `https://[EC2_PUBLIC_HOSTNAME]:443` or `http://[EC2_PUBLIC_HOSTNAME]:80`, depending on the protocol used. Ensure that you don't end either of the URLs with `/`.\n  - **Using proxy server:** Check this box if using a proxy endpoint.\n  - **Graph connection URL:** Provide the endpoint for the graph database\n  - **AWS IAM Auth Enabled:** Check this box if connecting to Amazon Neptune using IAM Auth and SigV4 signed requests\n  - **Service Type:** Choose the service type\n  - **AWS Region:** Specify the AWS region where the Neptune cluster is hosted (e.g., us-east-1)\n  - **Fetch Timeout:** Specify the timeout for the fetch request\n\n- **Available Connections:** Once a connection is created, this section will appear as a left-hand pane. When you create more than one connection to a graph database, you can only connect to and visualize from one graph database endpoint at a time. To select the active database, toggle the \u201cActive\u201d switch.\n\n- **Connection Details:** Once a connection is created, this section will appear as a right-hand information pane for a selected connection. It shows details such as the connection name, graph data model type, endpoint and a summary of the graph data, such as the count of nodes, edges, and a list of node types.\n- **Last Synchronization:** When a connection is created, Graph Explorer will perform a scan of the graph to provide summary data. To re-synchronize after data has changed on your graph, select a connection, and then click the \u201crefresh\u201d button next to \u201cLast Synchronization\u201d text.\n- **Data Explorer UI:** Under a listed node type, you can click on the \u2018>\u2019 arrow to get to the \u201cData Explorer\u201d view. This allows you to see a sample list of nodes under this type and choose one or more nodes to \u201cSend to Explorer\u201d for getting started quickly if you are new to the data.\n\n#### _Graph Explorer UI:_\n\nYou can search, browse, expand, customize views of your graph data using Graph Explorer, which is the main UI of this application. Once you create a connection, you can click \u201cOpen Graph Explorer\u201d on the top-right to navigate here. There are several key features on this UI:\n\n- **Top Bar UI:**\n  - **Search bar:** If a user wants to start without using the Data Explorer, they can go directly to the search bar and use the search to visualize a starting node in the graph.\n  - **Toggles:** You can toggle to show/hide the Graph View and/or Table View for screen real-estate management.\n  - **Open Connections:** This takes the user back to Connections UI.\n- **Graph View UI:** The graph visualization canvas that you can interact with. Double-click to expand the first-order neighbors of a node.\n  - **Layout drop-down & reset:** You can display graph data using standard graph layouts in the Graph View. You can use the circular arrow to reset the physics of a layout.\n  - **Screenshot:** Download a picture of the current window in Graph View.\n  - **Zoom In/Out & Clear:** To help users quickly zoom in/out or clear the whole canvas in the Graph View.\n  - **Legend (i):** This displays an informational list of icons, colors, and display names available.\n- **Right-hand Pane UI:** There are 5-6 functions in the collapsible right-hand pane of Graph Explorer:\n  - **Details View** shows details about a selected node/edge such as properties etc.\n  - **Entities Filter** is used to control the display of nodes and edges that are already expanded in the Graph View; click to hide or show nodes/edges.\n  - **Expand** is used when expanding will result in 10+ neighbors and control the meaningful expansion. You will need to select a number as the limit to expand to. You can also add text filters for expansion.\n  - **Node Styling** of node display options (e.g., color, icon, the property to use for the displayed name).\n  - **Edge Styling** of edge display options (e.g., color, icon, the property to use for the displayed name).\n  - **Namespaces (RDF only):** This RDF-specific configuration feature allows you to shorten the display of Resource URIs within the app based on auto-generated prefixes, commonly-used prefix libraries, or custom prefixes set by the user. Order of priority is set to Custom > Common > Auto-generated.\n- **Table View UI:** This collapsible view shows a row-column display of the data in the Graph View. You can use filters in the Table to show/hide elements in the Graph View, and you can export the table view into a CSV or JSON file. The following columns are available for filtering on property graphs (RDF graphs in parentheses):\n\n  - Node ID (Resource URI)\n  - Node Type (Class)\n  - Edge Type (Predicate)\n  - Source ID (Source URI)\n  - Source Type (Source Class)\n  - Target ID (Target URI)\n  - Target Type (Target Class)\n  - Display Name - Set in the Node/Edge Styling panes\n  - Display Description - Set in the Node/Edge Styling panes\n  - Total Neighbors - Enter an integer to be used as the >= limit\n\n- **Additional Table View UI Features**\n  - Visibility - manually show or hide nodes or edges\n  - All Nodes / All Edges (or All Resources / All Predicates) dropdown - allows you to display a list of either nodes or edges and control display/filter on them\n  - Download - You can download the current Table View as a CSV or JSON file with additional customization options\n  - Default columns - You can set which columns you want to display\n  - Paging of rows\n\n## Connections\n\nThe Graph Explorer supports visualizing both **property graphs** and **RDF graphs**. You can connect to Amazon Neptune or you can also connect to open graph databases that implement an Apache TinkerPop Gremlin Server endpoint or the SPARQL 1.1 protocol, such as Blazegraph. For additional details on connecting to different graph databases, see [Connections](./additionaldocs/connections.md).\n\n### Providing a Default Connection\n\nTo provide a default connection such that initial loads of the graph explorer always result with the same starting connection, modify the `docker run ...` command to either take in a JSON configuration or runtime environment variables. If you provide both a JSON configuration and environmental variables, the JSON will be prioritized.\n\n#### Valid ENV connection variables, their defaults, and their descriptions\n\n- Required:\n  - `PUBLIC_OR_PROXY_ENDPOINT` - `None` - See [Add a New Connection](#connections-ui)\n- Optional\n  - `GRAPH_TYPE` - `None` - If not specified, multiple connections will be created for every available graph type / query language. See [Add a New Connection](#connections-ui)\n  - `USING_PROXY_SERVER` - `False` - See [Add a New Connection](#connections-ui)\n  - `IAM` - `False` - See [Add a New Connection](#connections-ui)\n  - `GRAPH_EXP_HTTPS_CONNECTION` - `True` - Controls whether the Graph Explorer uses SSL or not\n  - `PROXY_SERVER_HTTPS_CONNECTION` - `True` - Controls whether the server uses SSL or not\n  - `GRAPH_EXP_FETCH_REQUEST_TIMEOUT` - `240000` - Controls the timeout for the fetch request\n- Conditionally Required:\n  - Required if `USING_PROXY_SERVER=True`\n    - `GRAPH_CONNECTION_URL` - `None` - See [Add a New Connection](#connections-ui)\n  - Required if `USING_PROXY_SERVER=True` and `IAM=True`\n    - `AWS_REGION` - `None` - See [Add a New Connection](#connections-ui)\n    - `SERVICE_TYPE` - `neptune-db`, Set this as `neptune-db` for Neptune database or `neptune-graph` for Neptune Analytics.\n\n#### JSON Configuration Approach\n\nFirst, create a `config.json` file containing values for the connection attributes:\n\n```bash\n{\n     \"PUBLIC_OR_PROXY_ENDPOINT\": \"https://public-endpoint\",\n     \"GRAPH_CONNECTION_URL\": \"https://cluster-cqmizgqgrsbf.us-west-2.neptune.amazonaws.com:8182\",\n     \"USING_PROXY_SERVER\": true, (Can be string or boolean)\n     \"IAM\": true, (Can be string or boolean)\n     \"SERVICE_TYPE\": \"neptune-db\",\n     \"AWS_REGION\": \"us-west-2\",\n     \"GRAPH_TYPE\": \"gremlin\" (Possible Values: \"gremlin\", \"sparql\", \"opencypher\"),\n     \"GRAPH_EXP_HTTPS_CONNECTION\": true (Can be string or boolean),\n     \"PROXY_SERVER_HTTPS_CONNECTION\": true, (Can be string or boolean),\n     \"GRAPH_EXP_FETCH_REQUEST_TIMEOUT\": 240000 (Can be number)\n}\n```\n\nPass the `config.json` file path to the `docker run` command.\n\n```bash\ndocker run -p 80:80 -p 443:443 --env HOST={hostname-or-ip-address} -v /path/to/config.json:/graph-explorer/config.json graph-explorer` \n```\n\n#### Environment Variable Approach\n\nProvide the desired connection variables directly to the `docker run` command, as follows:\n\n```bash\ndocker run -p 80:80 -p 443:443 \\\n --env HOST={hostname-or-ip-address} \\\n --env PUBLIC_OR_PROXY_ENDPOINT=https://public-endpoint \\\n --env GRAPH_TYPE=gremlin \\\n --env USING_PROXY_SERVER=true \\\n --env IAM=false \\\n --env GRAPH_CONNECTION_URL=https://cluster-cqmizgqgrsbf.us-west-2.neptune.amazonaws.com:8182 \\\n --env AWS_REGION=us-west-2 \\\n --env SERVICE_TYPE=neptune-db \\\n --env PROXY_SERVER_HTTPS_CONNECTION=true \\\n --env GRAPH_EXP_FETCH_REQUEST_TIMEOUT=240000 \\\n graph-explorer\n```\n\n## Development\n\nFor development guidance, see [Development](./additionaldocs/development.md).\n\n## Security\n\nYou can use the Graph Explorer to connect to a publicly accessible graph database endpoint, or connect to a proxy endpoint that redirects to a private graph database endpoint.\n\nGraph Explorer supports the HTTPS protocol by default and provides a self-signed certificate as part of the Docker image. You can choose to use HTTP instead by changing the [environment variable default settings](./additionaldocs/development.md#environment-variables).\n\n### HTTPS Connections\n\nIf either of the Graph Explorer or the proxy-server are served over an HTTPS connection (which it is by default), you will have to bypass the warning message from the browser due to the included certificate being a self-signed certificate. You can bypass by manually ignoring them from the browser or downloading the correct certificate and configuring them to be trusted. Alternatively, you can provide your own certificate. The following instructions can be used as an example to bypass the warnings for Chrome, but note that different browsers and operating systems will have slightly different steps.\n\n1. Download the certificate directly from the browser. For example, if using Google Chrome, click the \u201cNot Secure\u201d section on the left of the URL bar and select \u201cCertificate is not valid\u201d to show the certificate. Then click Details tab and click Export at the bottom.\n2. Once you have the certificate, you will need to trust it on your machine. For MacOS, you can open the Keychain Access app. Select System under System Keychains. Then go to File > Import Items... and import the certificate you downloaded in the previous step.\n3. Once imported, select the certificate and right-click to select \"Get Info\". Expand the Trust section, and change the value of \"When using this certificate\" to \"Always Trust\".\n4. You should now refresh the browser and see that you can proceed to open the application. For Chrome, the application will remain \u201cNot Secure\u201d due to the fact that this is a self-signed certificate. If you have trouble accessing Graph Explorer after completing the previous step and reloading the browser, consider running a docker restart command and refreshing the browser again.\n\nNote: To get rid of the \u201cNot Secure\u201d warning, see [Using self-signed certificates on Chrome](./additionaldocs/development.md#using-self-signed-certificates-on-chrome).\n\n### Connection Cache\n\nSetting up a new connection (or editing an existing connection) allows you to enable a cache for the connector requests. The cache store is configured to use the browser IndexedDB that allows you to make use of data stored between sessions. The time that the data stored in the cache is also configurable, by default it has a lifetime of 10 minutes.\n\nThe purpose of the cache is to avoid making multiple requests to the database with the same criteria. Therefore, a request with particular parameters will be cached at most the time set just with the response obtained. After that time, if the exact same request is made again, the response will be updated and stored again.\n\n## Authentication\n\nAuthentication for Amazon Neptune connections is enabled using the [SigV4 signing protocol](https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html).\n\nTo use AWS IAM authentication, you must run requests through a proxy endpoint, such as an EC2 instance, where credentials are resolved and where requests are signed.\n\nTo set up a connection in Graph Explorer UI with AWS IAM auth enabled on Neptune, check Using Proxy-Server, then check AWS IAM Auth Enabled and type in the AWS Region where the Neptune cluster is hosted (e.g., us-east-1).\n\nFor further information on how AWS credentials are resolved in Graph Explorer, refer to this [documentation](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CredentialProviderChain.html).\n\n## Logging\n\nLogs are, by default, sent to the console and will be visible as output to the docker logs. If you want to access the full set of logs, you can run `docker logs {container name or id}`. The log level will be set via the `LOG_LEVEL` env variable at `/packages/graph-explorer/.env` where the possible options, from highest to lowest, are `error`, `warn`, `info`, `debug`, and `trace` such that `error` is the highest level and will only include logs labeled as errors and `trace` the lowest and will include any type of log. By default, the log level is set to `info` and the only type of logs generated are those of `error`, `info`, or `debug`. If you need more detailed logs, you can change the log level from `info` in the default .env file to `debug` and the logs will begin printing the error's stack trace.\n\nWithin node-server.js, you'll notice three things.\n\n1. A proxyLogger object - This is responsible for actually recording the logs.\n2. An errorHandler - This automatically sends errors to the proxyLogger and can log extra information by adding wanted text to the error object at a key called extraInfo.\n3. An endpoint called `/logger` - This is how you would log things from the browser. It needs a log level and message header passed and you can then expect to see the message logged at the provided log level.\n\n## Contributing Guidelines\n\nSee [CONTRIBUTING](./CONTRIBUTING.md) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2024-01-20T03:02:41Z", "2024-01-09T03:32:15Z", "2023-09-15T00:01:50Z", "2023-08-15T21:18:06Z", "2023-08-09T21:41:52Z", "2023-05-25T22:26:19Z", "2023-03-02T05:25:53Z", "2023-01-03T22:07:40Z"]}, {"name": "graph-notebook", "description": "Library extending Jupyter notebooks to integrate with Apache TinkerPop, openCypher, and RDF SPARQL.", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Graph Notebook: easily query and visualize graphs\n\nThe graph notebook provides an easy way to interact with graph databases using Jupyter notebooks. Using this open-source Python package, you can connect to any graph database that supports the [Apache TinkerPop](https://tinkerpop.apache.org/), [openCypher](https://github.com/opencypher/openCypher) or the [RDF SPARQL](https://www.w3.org/TR/rdf-sparql-query/) graph models. These databases could be running locally on your desktop or in the cloud. Graph databases can be used to explore a variety of use cases including [knowledge graphs](https://aws.amazon.com/neptune/knowledge-graphs-on-aws/) and [identity graphs](https://aws.amazon.com/neptune/identity-graphs-on-aws/).\n\n![A colorful graph picture](./images/ColorfulGraph.png)\n\n## Visualizing Gremlin queries\n\n![Gremlin query and graph](./images/GremlinQueryGraph.png)\n\n## Visualizing openCypher queries\n\n![openCypher query and graph](./images/OCQueryGraph.png)\n\n## Visualizing SPARQL queries\n\n![SPARL query and graph](./images/SPARQLQueryGraph.png)\n\nInstructions for connecting to the following graph databases:\n\n|             Endpoint            |       Graph model       |   Query language    |\n| :-----------------------------: | :---------------------: | :-----------------: |\n|[Gremlin Server](#gremlin-server)|     property graph      |       Gremlin       |\n|    [Blazegraph](#blazegraph)    |            RDF          |       SPARQL        |\n|[Amazon Neptune](#amazon-neptune)|  property graph or RDF  |  Gremlin, openCypher, or SPARQL  |\n|         [Neo4J](#neo4j)         |     property graph      |       Cypher        |\n\nWe encourage others to contribute configurations they find useful. There is an [`additional-databases`](https://github.com/aws/graph-notebook/blob/main/additional-databases) folder where more information can be found.\n\n## Features\n\n### Notebook cell 'magic' extensions in the IPython 3 kernel\n\n`%%sparql` - Executes a SPARQL query against your configured database endpoint. [Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/notebooks-magics.html#notebooks-cell-magics-sparql)\n\n`%%gremlin` - Executes a Gremlin query against your database using web sockets. The results are similar to those a Gremlin console would return. [Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/notebooks-magics.html#notebooks-cell-magics-gremlin)\n\n`%%opencypher` or `%%oc` Executes an openCypher query against your database. [Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/notebooks-magics.html#notebooks-cell-magics-opencypher)\n\n`%%graph_notebook_config` - Sets the executing notebook's database configuration to the JSON payload provided in the cell body.\n\n`%%graph_notebook_vis_options` - Sets the executing notebook's [vis.js options](https://visjs.github.io/vis-network/docs/network/physics.html) to the JSON payload provided in the cell body.\n\n`%%neptune_ml` - Set of commands to integrate with NeptuneML functionality, as described [here](https://docs.aws.amazon.com/neptune/latest/userguide/notebooks-magics.html#notebooks-line-magics-neptune_ml). [Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/machine-learning.html)\n\n**TIP** :point_right: `%%sparql`, `%%gremlin`, and `%%oc` share a [suite of common arguments](https://docs.aws.amazon.com/neptune/latest/userguide/notebooks-magics.html#notebook-magics-query-args) that be used to customize the appearance of rendered graphs. Example usage of these arguments can also be found in the sample notebooks under [02-Visualization](https://github.com/aws/graph-notebook/tree/main/src/graph_notebook/notebooks/02-Visualization).\n\n**TIP** :point_right: There is syntax highlighting for language query magic cells to help you structure your queries more easily.\n\n#### Notebook line 'magic' extensions in the IPython 3 kernel\n\n`%gremlin_status` - Obtain the status of Gremlin queries. [Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/gremlin-api-status.html)\n\n`%sparql_status` - Obtain the status of SPARQL queries. [Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/sparql-api-status.html)\n\n`%opencypher_status` or `%oc_status` - Obtain the status of openCypher queries. [Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-opencypher-status.html)\n\n`%load` - Generate a form to submit a bulk loader job. [Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html)\n\n`%load_ids` - Get ids of bulk load jobs. [Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/load-api-reference-status-examples.html)\n\n`%load_status` - Get the status of a provided `load_id`. [Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/load-api-reference-status-examples.html)\n\n`%cancel_load` - Cancels a bulk load job. You can either provide a single `load_id`, or specify `--all-in-queue` to cancel all queued (and not actively running) jobs. [Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/load-api-reference-cancel.html)\n\n`%neptune_ml` - Set of commands to integrate with NeptuneML functionality, as described [here](https://docs.aws.amazon.com/neptune/latest/userguide/notebooks-magics.html#notebooks-cell-magics-neptune_ml). You can find a set of tutorial notebooks [here](https://github.com/aws/graph-notebook/tree/main/src/graph_notebook/notebooks/04-Machine-Learning).\n[Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/machine-learning.html)\n\n`%status` - Check the Health Status of the configured host endpoint. [Documentation](https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-status.html)\n\n`%seed` - Provides a form to add data to your graph, using sets of insert queries instead of a bulk loader. Sample RDF and Property Graph data models are provided with this command. Alternatively, you can select a language type and provide a file path(or a directory path containing one or more of these files) to load the queries from.\n\n`%stream_viewer` - Interactively explore the Neptune CDC stream (if enabled)\n\n`%graph_notebook_config` - Returns a JSON payload that contains connection information for your host.\n\n`%graph_notebook_host` - Set the host endpoint to send queries to.\n\n`%graph_notebook_version` - Print the version of the `graph-notebook` package\n\n`%graph_notebook_vis_options` - Print the Vis.js options being used for rendered graphs\n\n**TIP** :point_right: You can list all the magics installed in the Python 3 kernel using the `%lsmagic` command.\n\n**TIP** :point_right: Many of the magic commands support a `--help` option in order to provide additional information.\n\n## Example notebooks\n\nThis project includes many example Jupyter notebooks. It is recommended to explore them. All of the commands and features supported by `graph-notebook` are explained in detail with examples within the sample notebooks. You can find them  [here](./src/graph_notebook/notebooks/). As this project has evolved, many new features have been added. If you are already familiar with graph-notebook but want a quick summary of new features added, a good place to start is the Air-Routes notebooks in the [02-Visualization](./src/graph_notebook/notebooks/02-Visualization) folder.\n\n## Keeping track of new features\n\nIt is recommended to check the [ChangeLog.md](ChangeLog.md) file periodically to keep up to date as new features are added.\n\n## Prerequisites\n\nYou will need:\n\n* [Python](https://www.python.org/downloads/) 3.8.x-3.10.13\n* A graph database that provides one or more of:\n  * A SPARQL 1.1 endpoint\n  * An Apache TinkerPop Gremlin Server compatible endpoint\n  * An endpoint compatible with openCypher\n  \n## Installation\n\nBegin by installing `graph-notebook` and its prerequisites, then follow the remaining instructions for either Jupyter Classic Notebook or JupyterLab.\n\n``` bash\n# install the package\npip install graph-notebook\n```\n\n### Jupyter Classic Notebook\n\n``` bash\n# Enable the visualization widget\njupyter nbextension enable  --py --sys-prefix graph_notebook.widgets\n\n# copy static html resources\npython -m graph_notebook.static_resources.install\npython -m graph_notebook.nbextensions.install\n\n# copy premade starter notebooks\npython -m graph_notebook.notebooks.install --destination ~/notebook/destination/dir\n\n# create nbconfig file and directory tree, if they do not already exist\nmkdir ~/.jupyter/nbconfig\ntouch ~/.jupyter/nbconfig/notebook.json\n\n# start jupyter notebook\npython -m graph_notebook.start_notebook --notebooks-dir ~/notebook/destination/dir\n```\n\n### JupyterLab 3.x\n\n``` bash\n# install jupyterlab\npip install \"jupyterlab>=3,<4\"\n\n# copy premade starter notebooks\npython -m graph_notebook.notebooks.install --destination ~/notebook/destination/dir\n\n# start jupyterlab\npython -m graph_notebook.start_jupyterlab --jupyter-dir ~/notebook/destination/dir\n```\n\n#### Loading magic extensions in JupyterLab\n\nWhen attempting to run a line/cell magic on a new notebook in JupyterLab, you may encounter the error:\n\n``` bash\nUsageError: Cell magic `%%graph_notebook_config` not found.\n```\n\nTo fix this, run the following command, then restart JupyterLab.\n\n``` bash\npython -m graph_notebook.ipython_profile.configure_ipython_profile\n```\n\nAlternatively, the magic extensions can be manually reloaded for a single notebook by running the following command in any empty cell.\n\n``` bash\n%load_ext graph_notebook.magics\n```\n\n## Upgrading an existing installation\n\n``` bash\n# upgrade graph-notebook\npip install graph-notebook --upgrade\n```\n\nAfter the above command completes, rerun the commands given at [Jupyter Classic Notebook](#jupyter-classic-notebook) or [JupyterLab 3.x](#jupyterlab-3x) based on which flavour is installed.\n\n## Connecting to a graph database\n\nConfiguration options can be set using the `%graph_notebook_config` magic command. The command accepts a JSON object as an argument. The JSON object can contain any of the configuration options listed below. The command can be run multiple times to change the configuration. The configuration is stored in the notebook's metadata and will be used for all subsequent queries.\n\n| Configuration Option | Description | Default Value | Type |\n| --- | --- | --- | --- |\n| auth_mode | The authentication mode to use for Amazon Neptune connections | DEFAULT | string |\n| aws_region | The AWS region to use for Amazon Neptune connections | your-region-1 | string |\n| host | The host url to form a connection with | localhost | string |\n| load_from_s3_arn | The ARN of the S3 bucket to load data from [Amazon Neptune only] | | string |\n| neptune_service | The name of the Neptune service for the host url [Amazon Neptune only] | neptune-db | string |\n| port | The port to use when creating a connection | 8182 | number |\n| proxy_host | The proxy host url to route a connection through [Amazon Neptune only]| | string |\n| proxy_port | The proxy port to use when creating proxy connection [Amazon Neptune only] | 8182 | number |\n| ssl | Whether to make connections to the created endpoint with ssl or not [True/False] | False | boolean |\n| ssl_verify | Whether to verify the server's TLS certificate or not [True/False] | True | boolean |\n| sparql | SPARQL connection object | ``` { \"path\": \"sparql\" } ``` | string |\n| gremlin | Gremlin connection object | ``` { \"username\": \"\", \"password\": \"\", \"traversal_source\": \"g\",  \"message_serializer\": \"graphsonv3\" } ```| string |\n| neo4j | Neo4J connection object |``` { \"username\": \"neo4j\", \"password\": \"password\", \"auth\": true, \"database\": null } ``` | string |\n\n### Gremlin Server\n\nIn a new cell in the Jupyter notebook, change the configuration using `%%graph_notebook_config` and modify the fields for `host`, `port`, and `ssl`. Optionally, modify `traversal_source` if your graph traversal source name differs from the default value, `username` and `password` if required by the graph store, or `message_serializer` for a specific data transfer format. For a local Gremlin server (HTTP or WebSockets), you can use the following command:\n\n``` python\n%%graph_notebook_config\n{\n  \"host\": \"localhost\",\n  \"port\": 8182,\n  \"ssl\": false,\n  \"gremlin\": {\n    \"traversal_source\": \"g\",\n    \"username\": \"\",\n    \"password\": \"\",\n    \"message_serializer\": \"graphsonv3\"\n  }\n}\n```\n\nTo setup a new local Gremlin Server for use with the graph notebook, check out [`additional-databases/gremlin server`](additional-databases/gremlin-server)\n\n### Blazegraph\n\nChange the configuration using `%%graph_notebook_config` and modify the fields for `host`, `port`, and `ssl`. For a local Blazegraph database, you can use the following command:\n\n``` python\n%%graph_notebook_config\n{\n  \"host\": \"localhost\",\n  \"port\": 9999,\n  \"ssl\": false,\n  \"sparql\": {\n    \"path\": \"sparql\"\n  }\n}\n```\n\nYou can also make use of namespaces for Blazegraph by specifying the path `graph-notebook` should use when querying your SPARQL like below:\n\n``` python\n%%graph_notebook_config\n\n{\n  \"host\": \"localhost\",\n  \"port\": 9999,\n  \"ssl\": false,\n  \"sparql\": {\n    \"path\": \"blazegraph/namespace/foo/sparql\"\n  }\n}\n```\n\nThis will result in the url `localhost:9999/blazegraph/namespace/foo/sparql` being used when executing any `%%sparql` magic commands.\n\nTo setup a new local Blazegraph database for use with the graph notebook, check out the [Quick Start](https://github.com/blazegraph/database/wiki/Quick_Start) from Blazegraph.\n\n### Amazon Neptune\n\nChange the configuration using `%%graph_notebook_config` and modify the defaults as they apply to your Neptune instance.\n\n#### Neptune DB\n\n``` python\n%%graph_notebook_config\n{\n  \"host\": \"your-neptune-endpoint\",\n  \"neptune_service\": \"neptune-db\",\n  \"port\": 8182,\n  \"auth_mode\": \"DEFAULT\",\n  \"load_from_s3_arn\": \"\",\n  \"ssl\": true,\n  \"ssl_verify\": true,\n  \"aws_region\": \"your-neptune-region\"\n}\n```\n\n#### Neptune Analytics\n\n``` python\n%%graph_notebook_config\n{\n  \"host\": \"your-neptune-endpoint\",\n  \"neptune_service\": \"neptune-graph\",\n  \"port\": 443,\n  \"auth_mode\": \"IAM\",\n  \"ssl\": true,\n  \"ssl_verify\": true,\n  \"aws_region\": \"your-neptune-region\"\n}\n```\n\nTo setup a new Amazon Neptune cluster, check out the [Amazon Web Services documentation](https://docs.aws.amazon.com/neptune/latest/userguide/manage-console-launch.html).\n\nWhen connecting the graph notebook to Neptune via a private endpoint, make sure you have a network setup to communicate to the VPC that Neptune runs on. If not, you can follow [this guide](https://github.com/aws/graph-notebook/tree/main/additional-databases/neptune).\n\nIn addition to the above configuration options, you can also specify the following options:\n\n### Amazon Neptune Proxy Connection\n\n``` python\n%%graph_notebook_config\n{\n  \"host\": \"clustername.cluster-ididididid.us-east-1.neptune.amazonaws.com\",\n  \"neptune_service\": \"neptune-db\",\n  \"port\": 8182,\n  \"ssl\": true,\n  \"proxy_port\": 8182,\n  \"proxy_host\": \"host.proxy.com\",\n  \"auth_mode\": \"IAM\",\n  \"aws_region\": \"us-east-1\",\n  \"load_from_s3_arn\": \"\"\n}\n```\n\nSee also: Connecting to Amazon Neptune from clients outside the Neptune VPC using AWS Network [Load Balancer](https://aws-samples.github.io/aws-dbs-refarch-graph/src/connecting-using-a-load-balancer/#connecting-to-amazon-neptune-from-clients-outside-the-neptune-vpc-using-aws-network-load-balancer)\n\n## Authentication (Amazon Neptune)\n\nIf you are running a SigV4 authenticated endpoint, ensure that your configuration has `auth_mode` set to `IAM`:\n\n``` python\n%%graph_notebook_config\n{\n  \"host\": \"your-neptune-endpoint\",\n  \"neptune_service\": \"neptune-db\",\n  \"port\": 8182,\n  \"auth_mode\": \"IAM\",\n  \"load_from_s3_arn\": \"\",\n  \"ssl\": true,\n  \"ssl_verify\": true,\n  \"aws_region\": \"your-neptune-region\"\n}\n```\n\nAdditionally, you should have the following Amazon Web Services credentials available in a location accessible to Boto3:\n\n* Access Key ID\n* Secret Access Key\n* Default Region\n* Session Token (OPTIONAL. Use if you are using temporary credentials)\n\nThese variables must follow a specific naming convention, as listed in the [Boto3 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-environment-variables)\n\nA list of all locations checked for Amazon Web Services credentials can also be found [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#configuring-credentials).\n\n### Neo4J\n\nChange the configuration using `%%graph_notebook_config` and modify the fields for `host`, `port`, `ssl`, and `neo4j` authentication.\n\nIf your Neo4J instance supports [multiple databases](https://neo4j.com/developer/manage-multiple-databases/), you can specify a database name via the `database` field. Otherwise, leave the `database` field blank to query the default database.\n\nFor a local Neo4j Desktop database, you can use the following command:\n\n``` python\n%%graph_notebook_config\n{\n  \"host\": \"localhost\",\n  \"port\": 7687,\n  \"ssl\": false,\n  \"neo4j\": {\n    \"username\": \"neo4j\",\n    \"password\": \"password\",\n    \"auth\": true,\n    \"database\": \"\"\n  }\n}\n```\n\nEnsure that you also specify the `%%oc bolt` option when submitting queries to the Bolt endpoint.\n\nTo setup a new local Neo4J Desktop database for use with the graph notebook, check out the [Neo4J Desktop User Interface Guide](https://neo4j.com/developer/neo4j-desktop/).\n\n## Building From Source\n\nA pre-release distribution can be built from the graph-notebook repository via the following steps:\n\n``` bash\n# 1) Clone the repository and navigate into the clone directory\ngit clone https://github.com/aws/graph-notebook.git\ncd graph-notebook\n\n# 2) Create a new virtual environment\n\n# 2a) Option 1 - pyenv\npyenv install 3.10.13  # Only if not already installed; this can be any supported Python 3 version in Prerequisites\npyenv virtualenv 3.10.13 build-graph-notebook\npyenv local build-graph-notebook\n\n# 2b) Option 2 - venv\nrm -rf /tmp/venv\npython3 -m venv /tmp/venv\nsource /tmp/venv/bin/activate\n\n# 3) Install build dependencies\npip install --upgrade pip setuptools wheel twine\npip install \"jupyterlab>=3,<4\"\n\n# 4) Build the distribution\npython3 setup.py bdist_wheel\n```\n\nYou should now be able to find the built distribution at\n\n`./dist/graph_notebook-4.1.0-py3-none-any.whl`\n\nAnd use it by following the [installation](https://github.com/aws/graph-notebook#installation) steps, replacing\n\n``` python\npip install graph-notebook\n```\n\nwith\n\n``` python\npip install ./dist/graph_notebook-4.1.0-py3-none-any.whl\n```\n\n## Contributing Guidelines\n\nSee [CONTRIBUTING](https://github.com/aws/graph-notebook/blob/main/CONTRIBUTING.md) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2024-02-02T01:27:48Z", "2023-12-15T00:54:56Z", "2023-11-30T00:27:43Z", "2023-11-29T18:14:05Z", "2023-10-10T02:06:02Z", "2023-06-06T04:26:00Z", "2023-04-17T21:35:06Z", "2023-04-17T04:45:20Z", "2023-03-14T01:05:40Z", "2023-03-10T06:23:13Z", "2023-01-26T02:54:09Z", "2022-12-07T19:30:07Z", "2022-10-18T16:19:42Z", "2022-09-15T19:24:27Z", "2022-07-26T01:41:06Z", "2022-07-13T00:32:25Z", "2022-06-07T21:56:05Z", "2022-03-29T04:42:45Z", "2022-02-26T01:32:55Z", "2021-12-22T04:16:40Z", "2021-11-04T00:34:36Z", "2021-10-25T20:02:58Z", "2021-09-21T03:29:05Z", "2021-08-28T01:13:47Z", "2021-08-11T21:55:29Z", "2021-07-30T07:36:58Z", "2021-07-29T02:56:42Z", "2021-06-27T21:45:16Z", "2021-06-22T23:32:27Z", "2021-05-11T18:34:21Z"]}, {"name": "homebrew-aws", "description": "Homebrew is a package manager for macOS which provides easy installation and update management of additional software. This Tap (repository) contains the Formulae that are used in the macOS AMI that AWS offers.", "language": "Ruby", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# EC2 macOS Homebrew Tap\n\nAWS offers EC2-optimized macOS AMIs for developer use.  The EC2 Mac AMIs provided by AWS include this Tap by default, which provides a simple way to get these tools and updates to them.\n\n## What is Homebrew?\n\n[Homebrew](https://brew.sh) is a package manager for macOS, which provides easy installation and update management of [additional software](https://formulae.brew.sh/).\n\n## What is a Tap?\n\nA third-party (in relation to Homebrew) repository, which provides installable packages on macOS.\n\nSee more at [https://docs.brew.sh/Taps](https://docs.brew.sh/Taps).\n\n## What packages are available in this Tap?\n\nThis Tap (repository) contains the formulae that are used in the macOS AMI(s) that are offered by AWS for launching EC2 Mac instances.\n\nThis includes:\n\n| Name | Description | Type | Package Name|\n|------|-------------|------|-------------|\n| Amazon ENA | [EC2 Mac ENA Network Driver for macOS](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena.html) | Cask | amazon-ena-ethernet |\n| Amazon ENA DriverKit | [EC2 Mac ENA Network Driver for macOS Monterey and later (arm64 only)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena.html) | Cask | amazon-ena-ethernet-dext |\n| Amazon EFS | [Amazon Elastic File System](https://docs.aws.amazon.com/efs/latest/ug/using-amazon-efs-utils.html) | Keg | amazon-efs-utils |\n| Amazon SSM Agent | [Amazon SSM Agent](https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html)| Cask| amazon-ssm-agent |\n| EC2 macOS Init | Instance initialization and configuration, including performance optimization | Cask | ec2-macos-init |\n| EC2 System Monitor for macOS | For collecting system monitoring CloudWatch metrics on mac1.metal instances | Cask | ec2-macos-system-monitoring |\n| EC2 macOS Utils | Provides common command-line tool for customizing EC2 Mac instances | Cask | ec2-macos-utils | \n| EC2 Instance Connect | [EC2 Instance Connect](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-set-up.html) | Cask | ec2-instance-connect |\n\n## How do I install the packages from this Tap?\n\nThis Tap follows standard Homebrew commands, for more information, please refer to the [Homebrew Documentation](https://docs.brew.sh/). Updating homebrew itself is done via the following command:\n\n```sh\nbrew update\n```\n\nThe above command will update homebrew itself and this comamnd should always be run prior to attempts to install or update any packages with brew.\n\n### Installing the Tap\n\nThe macOS AMIs provided by AWS already come with Homebrew installed and this Tap \"pretapped\". To manually install the Tap, use the the `brew tap` command:\n\n```sh\nbrew tap aws/homebrew-aws\n```\n\n### Removing the Tap\n\nIf this Tap needs to be removed for any reason, Homebrew includes a specific [command](https://docs.brew.sh/Taps) for this. (*Note:* This will only remove the Tap and it will preserve anything that was previously installed)\n\n```sh\nbrew untap aws/homebrew-aws\n```\n\n### Updating Packages\n\nTo update a specific package (Keg or Cask) that has been installed from this tap. Kegs use the default `brew upgrade <package>` command, while Casks have their own sub-command: `brew upgrade --cask <package>`.\n\nFor example:\n\n| Type  | Update Command|\n|-------|--------|\n| Full System| `brew upgrade` |\n| Keg   |`brew upgrade amazon-efs-utils`| \n| Cask  |`brew upgrade --cask amazon-ena-ethernet-dext`|\n\n### Installing Packages\n\nThere are two primary ways to install software from the Tap. Kegs use the default `brew install <package>` command, while Casks have their own sub-command: `brew install --cask <package>`.\n\nFor example:\n\n| Type | Install Command |\n|------|-----------------|\n| Keg  | `brew install amazon-efs-utils` ||\n| Cask | `brew install --cask amazon-ena-ethernet-dext` |\n\n### Removing Packages\n\nRemoving software is similar to installing software. Kegs and Casks now use the same `brew remove <package>` command.\n\nFor example:\n\n| Type | Uninstall Command |\n|------|-----------------|\n| Keg  | `brew remove amazon-efs-utils` |\n| Cask | `brew remove amazon-ssm-agent` |\n\n## Documentation\n\nTo get more information about brew you can run `brew help` or `man brew` on a mac1.metal instance or check [Homebrew's documentation](https://docs.brew.sh) for Homebrew's complete documentation.\n\n## License\n\nThis project is licensed under the [Apache License, version 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n", "release_dates": []}, {"name": "homebrew-tap", "description": "Homebrew formulae that allows installation of AWS tools through the Homebrew package manager.", "language": "Ruby", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# homebrew-tap\n\nHomebrew formulae that allows installation of AWS tools through the [Homebrew](https://brew.sh/) package manager.\n\n\n## Installation\n\n```\nbrew tap aws/tap\nbrew install <FORMULA>\n```\n\n## Formulae\n\n| Repository | Formula | Description |\n| ---------- | ------- | ----------- |\n| container-tools | [formula](Formula/container-tools.rb) | Meta-package to install common container tools |\n| [copilot-cli](https://github.com/aws/copilot-cli) | [formula](Formula/copilot-cli.rb) | Build, release and operate production ready containerized applications on Amazon ECS |\n| [ec2-instance-selector](https://github.com/aws/amazon-ec2-instance-selector) | [formula](Formula/ec2-instance-selector.rb) | CLI tool and go library which recommends instance types based on resource criteria like vcpus and memory |\n| [ec2-metadata-mock](https://github.com/aws/amazon-ec2-metadata-mock) | [formula](Formula/ec2-metadata-mock.rb) | A tool to simulate Amazon EC2 instance metadata |\n| [ec2-spot-interrupter](https://github.com/aws/amazon-ec2-spot-interrupter) | [formula](Formula/ec2-spot-interrupter.rb) | CLI tool that triggers Amazon EC2 Spot Interruption Notifications and Rebalance Recommendations |\n| [eks-anywhere](https://github.com/aws/eks-anywhere) | [formula](Formula/eks-anywhere.rb) | CLI tool for managing Amazon EKS Anywhere clusters |\n| [eks-node-viewer](https://github.com/awslabs/eks-node-viewer/) | [formula](Formula/eks-node-viewer.rb) | CLI tool for visualizing dynamic node usage within an EKS cluster. |\n| [eksdemo](https://github.com/awslabs/eksdemo) | [formula](Formula/eksdemo.rb) | The easy button for learning, testing and demoing Amazon EKS |\n| k8s-tools | [formula](Formula/k8s-tools.rb) | Meta-package to install common Kubernetes tools |\n| [lightsailctl](https://github.com/aws/lightsailctl) | [formula](Formula/lightsailctl.rb) | Amazon Lightsail CLI Extensions |\n| [xray-daemon](https://github.com/aws/aws-xray-daemon) | [formula](Formula/xray-daemon.rb) | Relays traffic to AWS X-Ray API |\n| [aws-simple-ec2-cli](https://github.com/awslabs/aws-simple-ec2-cli) | [formula](Formula/aws-simple-ec2-cli.rb) | CLI tool that simplifies the process of launching, connecting and terminating EC2 instances |\n| [qldbshell](https://github.com/awslabs/amazon-qldb-shell) | [formula](Formula/qldbshell.rb) | A CLI Shell for interacting with and processing queries against AWS QLDB |\n| [emr-on-eks-custom-image](https://github.com/awslabs/amazon-emr-on-eks-custom-image-cli) | [formula](Formula/emr-on-eks-custom-image.rb) | A CLI tool to interact with EMR on EKS custom images.\n| [cbmc-viewer](https://github.com/awslabs/aws-viewer-for-cbmc) | [formula](Formula/cbmc-viewer.rb) | CBMC Viewer scans the output of CBMC and produces a summary that can be opened in any web browser to understand and debug CBMC findings.\n| [dynamodb-shell](https://github.com/awslabs/dynamodb-shell) | [formula](Formula/aws-ddbsh.rb) | A simple SQL CLI for DynamoDB\n\n## Migrated Formulae's\n| Repository | Migrated Tap | Formula | Description |\n| ---------- | ------------ | ------- | ----------- |\n| [aws-sam-cli](https://github.com/awslabs/aws-sam-cli) | `homebrew/core` | [formula](https://github.com/Homebrew/homebrew-core/blob/master/Formula/a/aws-sam-cli.rb) | CLI tool to build, test, debug, and deploy Serverless applications using AWS Lambda |\n| [smithy-cli](https://github.com/awslabs/smithy) | `smithy-lang/tap` |  [formula](https://github.com/smithy-lang/homebrew-tap/blob/main/Formula/smithy-cli.rb) | A CLI for building, validating, querying, and iterating on Smithy models\n\n\n## Documentation\n\n`brew help`, `man brew` or check [Homebrew's documentation](https://docs.brew.sh/)\n", "release_dates": []}, {"name": "http-desync-guardian", "description": "Analyze HTTP requests to minimize risks of HTTP Desync attacks (precursor for HTTP request smuggling/splitting). ", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<img src=\"docs/http-desync-guardian-logo.png\" width=\"200\">\n\n[![Apache 2 License](https://img.shields.io/github/license/awslabs/s2n.svg)](http://aws.amazon.com/apache-2-0/)\n[![Crate](https://img.shields.io/crates/v/http_desync_guardian.svg)](https://crates.io/crates/http_desync_guardian)\n![Clippy/Fmt](https://github.com/aws/http-desync-guardian/workflows/Clippy/Fmt/badge.svg)\n![Tests](https://github.com/aws/http-desync-guardian/workflows/Tests/badge.svg)\n[![Coverage Status](https://coveralls.io/repos/github/aws/http-desync-guardian/badge.svg?branch=master)](https://coveralls.io/github/aws/http-desync-guardian?branch=master)\n\n\n\nOverview\n========\n\n`HTTP/1.1` went through a long evolution since 1991 to 2014:\n\n* [HTTP/0.9](https://www.w3.org/Protocols/HTTP/AsImplemented.html) \u2013 1991\n* [HTTP/1.0](https://tools.ietf.org/html/rfc1945) \u2013 1996\n* HTTP/1.1\n  * [RFC 2068](https://tools.ietf.org/html/rfc2068) \u2013 1997\n  * [RFC 2616](https://tools.ietf.org/html/rfc2616) - 1999\n  * [RFC 7230](https://tools.ietf.org/html/rfc7230) - 2014\n\nThis means there is a variety of servers and clients, which might have different views on request boundaries, creating opportunities for desynchronization attacks (a.k.a. HTTP Desync). \t \n  \nIt might seem simple to follow the latest RFC recommendations. However, for large scale systems that have been there for a while, it may come with unacceptable availability impact.\t \n  \n`http_desync_guardian` library is designed to analyze HTTP requests to prevent HTTP Desync attacks, balancing security and availability. \nIt classifies requests into different [categories](/docs#request-classification) and provides recommendations on how each tier should be handled.\n\nIt can be used either for raw HTTP request headers or already parsed by an HTTP engine.\nConsumers may configure logging and metrics collection.\nLogging is rate limited and all user data is obfuscated. \n\nIf you think you might have found a security impacting issue, please follow [our Security Notification Process.](#security-issue-notifications)\n\nPriorities\n=======\n\n* **Uniformity across services is key.** This means request classification, logging, and metrics must happen under the hood and with minimally available settings (e.g., such as log file destination).\n* **Focus on reviewability.** The test suite must require no knowledge about the library/programming languages but only about HTTP protocol. So it's easy to review, contribute, and re-use.\n* **Security is efficient when it's easy for users.** Our goal is to make integration of the library as simple as possible.\n* **Ultralight.** The overhead must be minimal and impose no tangible tax on request handling (see [benchmarks](./benches)).\n\nSupported HTTP versions\n======\n\nThe main focus of this library is `HTTP/1.1`. See [tests](./tests) for all covered cases. Predecessors of `HTTP/1.1` don't support connection re-use which limits opportunities for HTTP Desync,\nhowever some proxies may upgrade such requests to `HTTP/1.1` and re-use backend connections, which may allow to craft malicious `HTTP/1.0` requests. \nThat's why they are analyzed using the same criteria as `HTTP/1.1`. For other protocol versions have the following exceptions:\n\n* `HTTP/0.9` requests are never considered `Compliant`, but are classified as `Acceptable`. If any of `Content-Length`/`Transfer-Encoding` is present then it's `Ambiguous`.\n* `HTTP/1.0` - the presence of `Transfer-Encoding` makes a request `Ambiguous`.\n* `HTTP/2+` is out of scope. But if your proxy downgrades `HTTP/2` to `HTTP/1.1`, make sure the outgoing request is analyzed. \n\nSee [documentation](./docs) to learn more.\n\nUsage from C\n=====\n\nThis library is designed to be primarily used from HTTP engines written in `C/C++`.  \n\n1. Install [cbindgen](https://github.com/eqrion/cbindgen#cbindgen-----): `cargo install --force cbindgen`\n1. Generate the header file: \n   * Run `cbindgen --output http_desync_guardian.h --lang c` for C.\n   * Run `cbindgen --output http_desync_guardian.h --lang c++` for C++.\n1. Run `cargo build --release`. The binaries are in `./target/release/libhttp_desync_guardian.*` files.\n\nLearn more: [generic](./misc/demo-c) and [Nginx](./misc/demo-nginx) examples.\n\n```c\n#include \"http_desync_guardian.h\"\n\n/* \n * http_engine_request_t - already parsed by the HTTP engine \n */\nstatic int check_request(http_engine_request_t *req) {\n    http_desync_guardian_request_t guardian_request = construct_http_desync_guardian_from(req); \n    http_desync_guardian_verdict_t verdict = {0};\n\n    http_desync_guardian_analyze_request(&guardian_request, &verdict);\n\n    switch (verdict.tier) {\n        case REQUEST_SAFETY_TIER_COMPLIANT:\n            // The request is good. green light\n            break;\n        case REQUEST_SAFETY_TIER_ACCEPTABLE:\n            // Reject, if mode == STRICTEST\n            // Otherwise, OK\n            break;\n        case REQUEST_SAFETY_TIER_AMBIGUOUS:\n            // The request is ambiguous.\n            // Reject, if mode == STRICTEST \n            // Otherwise send it, but don't reuse both FE/BE connections.\n            break;\n        case REQUEST_SAFETY_TIER_SEVERE:\n            // Send 400 and close the FE connection.\n            break;\n        default:\n            // unreachable code\n            abort();\n    }\n}\n```\n\nUsage from Rust\n====\n\nSee [benchmarks](./benches/benchmarks.rs) as an example of usage from Rust. \n\n## Security issue notifications\n\nIf you discover a potential security issue in `http_desync_guardian` we ask that you notify\nAWS Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue. \n\n## Security\n\nSee [CONTRIBUTING](./CONTRIBUTING.md#contributing-guidelines) for more information.\n\n## License\n\nThis project is licensed under the [Apache-2.0 License](./LICENSE).\n", "release_dates": []}, {"name": "iot-atlas", "description": "The content of the IoT Atlas", "language": "CSS", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "## IoT Atlas\n\nThis repository contains the content of the [IoT Atlas](http://iotatlas.net)\n\nThe IoT Atlas supports your project by explaining the why, what, and who of commonly used, modern IoT designs.\n\nIt would be great to have you join us and [contribute](https://github.com/aws/iot-atlas/blob/main/CONTRIBUTING.md) your ideas for designs, considerations, and examples. Please also read the [FAQ](https://github.com/aws/iot-atlas/blob/main/FAQ.md).\n\n## Local Development\n\nPlease review the [contributor development process](https://github.com/aws/iot-atlas/blob/main/src/README.md) for instructions on testing local content.\n\n## License Summary\n\nThe documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.\n\nThe sample code within this documentation is made available under a modified MIT license. See the LICENSE-SAMPLECODE file.\n", "release_dates": []}, {"name": "iot-expresslink", "description": null, "language": "Python", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "## AWS IoT ExpressLink\n\nWatch [this overview presentation](https://www.youtube.com/watch?v=6K9knXZpC8A) to get a better understanding of what AWS IoT ExpressLink provides.\n\nThis repository contains examples for integrating an AWS IoT ExpressLink module into an application.  These examples are intended to demonstrate how to perform the common operations for an IoT device.\n\nThe AWS IoT ExpressLink AT command specification can be found in the [Programmer's Manual](https://docs.aws.amazon.com/iot-expresslink).\n\nInformation about AWS IoT ExpressLink and partner's hardware modules can be found at AWS Partner Network's Device Catalog from the [main page](https://aws.amazon.com/iot-expresslink). Module datasheet, Getting Started Guide, and Product Information are also included in the device catalog.\n\n## Documentation\n\n* [AWS IoT ExpressLink Programmer's Guide](https://docs.aws.amazon.com/iot-expresslink/latest/programmersguide/elpg.html)\n* [AWS IoT ExpressLink Getting Started Guide](https://docs.aws.amazon.com/iot-expresslink/latest/gettingstartedguide/elgsg.html)\n* [AWS IoT ExpressLink Onboarding-by-Claim Customer/OEM Guide](https://docs.aws.amazon.com/iot-expresslink/latest/oemonboardingguide/oemog.html)\n\n## Prototyping SDK libraries\n### C\nExpressLink drivers and examples written in C on different MCU platforms:\n#### STM32 MCUs\nCMSIS pack for ExpressLink driver and examples, integrated with STM32CubeMX, the native configuration and code generation tool from STMicroelectronics.\nThe combination enables developers to quickly and easily create applications that connect to AWS IoT Core using ExpressLink modules with various STM32 MCUs.\nhttps://github.com/stm32-hotspot/I-CUBE-ExpressLink\n\n### Python, CircuitPython, MicroPython\n\nFor Python, including CircuitPython, MicroPython, and standard Python on Linux, macOS, and Windows:\nhttps://github.com/awslabs/aws-iot-expresslink-library-python\n\n### Arduino\n\nFor Arduino and all all microcontrollers that support the Arduino platform:\nhttps://github.com/awslabs/aws-iot-expresslink-library-arduino\n\n## Web Resources\n\n* [AWS IoT ExpressLink product page](https://aws.amazon.com/iot-expresslink/?nc=sn&loc=1)\n* [AWS Partner Device Catalog for qualified AWS IoT ExpressLink hardware](https://devices.amazonaws.com/search?page=1&sv=iotxplnk)\n\n## User Guides\n\nThe user guides can be found under the folder [cloud_templates/user_guides](cloud_templates/user_guides).\n\n### Espressif\n\n[Getting Started Guide for Espressif's AWS IoT ExpressLink Evaluation Kit on GitHub](https://github.com/espressif/esp-aws-expresslink-eval)\n\n### Realtek\n\n[Realtek's Ameba Z2 AWS IoT ExpressLink Evaluation Kit on GitHub](https://github.com/ambiot/ambz2_aws_iot_expresslink_eval)\n\n### u-blox\n\n[u-blox' USB-NORA-W256AWS AWS IoT ExpressLink Multiradio development kit](https://www.u-blox.com/en/product/usb-nora-w2?legacy=Current#Documentation-&-resources)\n\n[u-blox' SARA-R510AWS module LTE-M AWS IoT ExpressLink module](https://www.u-blox.com/en/product/sara-r510aws-module?legacy=Current#Documentation-&-resources)\n\n### Telit\n\n[Telit's Bravo AWS IoT ExpressLink development kit on GitHub](https://github.com/telit/bravo-aws-expresslink)\n\n## Hands-On Workshops and Resources\n\nThe **AWS IoT ExpressLink Demo Badge Workshop** is available at https://catalog.workshops.aws/aws-iot-expresslink-demo-badge/. This workshop ran as *IOT207-R* on November 28, and as *IOT207-R1* on December 2, at AWS re:Invent 2022.\n\nThe *Advanced Lab Modules* of the AWS re:Invent 2022 workshop used a Python library to quickly prototype an integration with AWS IoT ExpressLink using Python. The evolution of this early library is now available in this GitHub repository:\n[awslabs/aws-iot-expresslink-library-python](https://github.com/awslabs/aws-iot-expresslink-library-python)\n\n## Hardware Designs\n\nThe **AWS IoT ExpressLink Demo Badge hardware design**, with full schematics and PCB layout files, is available under the [hardware designs folder](./hardware-designs/).\n\n## Blog Posts and Application Notes\n\n* [Introducing AWS IoT ExpressLink, making it faster and easier to develop secure IoT devices](https://aws.amazon.com/blogs/iot/introducing-aws-iot-expresslink-making-it-faster-and-easier-to-develop-secure-iot-devices/) published 2021-11-30.\n* [AWS IoT ExpressLink is now generally available](https://aws.amazon.com/about-aws/whats-new/2022/06/aws-iot-expresslink-generally-available/) published 2022-06-22.\n\n## Twitch series: All Things AWS IoT ExpressLink\n\n* Live stream on AWS Twitch channel - https://twitch.tv/aws\n* Aired Tuesdays, 8am - 9am Pacific\n* 15 episodes from March 15 - June 21, 2022\n* Hosted by Dan Gross\n\n Episode    | Date       | Link                                       | Guest(s)                         | Partner  |\n ---------- | ---------- | ------------------------------------------ | -------------------------------- | -------- |\n Episode 1  | 2022-03-15 | https://www.twitch.tv/aws/video/1486749698 | Lucio, Michael                   |          |\n Episode 2  | 2022-03-22 | https://www.twitch.tv/aws/video/1486743174 | Lucio                            |          |\n Episdoe 3  | 2022-03-29 | https://www.twitch.tv/aws/video/1308163094 | Lucio, Magnus, Harald            | u-blox   |\n Episode 4  | 2022-04-05 | https://www.twitch.tv/aws/video/1314933253 | Lucio, Cobus, Joe                |          |\n Episode 5  | 2022-04-12 | https://www.twitch.tv/aws/video/1499049648 | Lucio, Harald, Magnus, Leo       | u-blox   |\n Episode 6  | 2022-04-19 | https://www.twitch.tv/aws/video/1507258985 | Cobus, Richard, Michael          |          |\n Episode 7  | 2022-04-26 | https://www.twitch.tv/aws/video/1507261412 | Lucio, Anton, Amey               | Espressif|\n Episode 8  | 2022-05-03 | https://www.twitch.tv/aws/video/1507263672 | Lucio, Joe                       |          |\n Episode 9  | 2022-05-10 | https://www.twitch.tv/aws/video/1507266090 | Lucio, Amey, Dhaval              | Espressif|\n Episode 10 | 2022-05-17 | https://www.twitch.tv/aws/video/1507267356 | Lucio, Yasser                    |          |\n Episode 11 | 2022-05-24 | https://www.twitch.tv/aws/video/1532014170 | Lucio, Amit, Prejith             | Infineon |\n Episode 12 | 2022-05-31 | https://www.twitch.tv/aws/video/1532015120 | Lucio, Joe                       |          |\n Episode 13 | 2022-06-07 | https://www.twitch.tv/aws/video/1532015794 | Lucio, Amit                      | Infineon |\n Episode 14 | 2022-06-14 | https://www.twitch.tv/aws/video/1532016601 | Lucio, Mike                      |          |\n Episode 15 | 2022-06-21 | https://www.twitch.tv/aws/video/1532017442 | Lucio, Steve, Magnus, Amey, Dave | All      |\n Episode 16 | 2022-11-22 | https://www.twitch.tv/aws/video/1690204748 | Lucio, Jefferson, Elleson, Shimis| Realtek |\n\n## YouTube Videos\n\n* [Playlist: Building a Weather Station](https://www.youtube.com/watch?v=hGBIzlp68bU&list=PLhr1KZpdzukdy_S7NpE9kkC75SXUCMYdO)\n* [Espressif's AWS IoT ExpressLink Solution](https://www.youtube.com/watch?v=NSGCVH0OU7w)\n* [u-blox AWS IoT ExpressLink](https://www.youtube.com/watch?v=4GiBnT0I0HE)\n* [AWS IoT ExpressLink and the u-blox NORA-W2](https://www.youtube.com/watch?v=PvyzQwVgCCw)\n* [Product Showcase: AWS IoT ExpressLink SARA-R5 Starter Kit](https://www.youtube.com/watch?v=nJNYUP0413c)\n* [AIROC\u2122 Cloud Connectivity Manager module & AWS IoT ExpressLink](https://www.youtube.com/watch?v=LEGDyNXPYfc)\n* [Innovation Coffee with AWS IoT ExpressLink](https://www.youtube.com/watch?v=K0saFj-6s6c)\n* [Connecting AWS IoT ExpressLink to an MQTT Broker running on AWS IoT Greengrass](https://www.youtube.com/watch?v=ii59MiFVIDI)\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThe resources in this repository are licensed under the MIT-0 License. See the LICENSE file.\n", "release_dates": []}, {"name": "iot-expresslink-plant-reference", "description": null, "language": "JavaScript", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "## AWS IoT ExpressLink Plant Reference\n\nThe plant demo will demonstrate how to integrate various AWS cloud features with ExpressLink. The plant demo uses several peripherals to monitor the environment of a plant and notify a user when certain variables are outside of the desired state for the plant.\n\n<br>\n<center><img src=\"docs/architectureDiagram.png\" alt=\"alt text\"/></center>\n</br>\n\n## Repository structure\n\nThe repository is separated into 2 folders: cdk and arduino. The arduino folder contains the code that needs to be flashed onto the host board. The cdk folder contains the CDK template for creating and deploying the cloud resources for the demo.\n\n## Hardware Setup\n\nThe demo is built on a P-NUCLEO-WB55 by STMicroelectionics; running the example on a different host board may require some modification to the serial communication and peripheral connections. The demo uses four sensors (photocell, temperature sensor, soil moisture sensor, and water level sensor) to monitor the plant's environment. In addition to these sensors the demo also includes a water pump for automatically watering the plant on a schedule.\n\n## ExpressLink Documentation\n\n* [AWS IoT ExpressLink Programmer's Guide](https://docs.aws.amazon.com/iot-expresslink/latest/programmersguide/elpg.html)\n* [AWS IoT ExpressLink Getting Started Guide](https://docs.aws.amazon.com/iot-expresslink/latest/gettingstartedguide/elgsg.html)\n* [AWS IoT ExpressLink Onboarding-by-Claim Customer/OEM Guide](https://docs.aws.amazon.com/iot-expresslink/latest/oemonboardingguide/oemog.html)\n* [More AWS IoT ExpressLink Examples](https://github.com/aws/iot-expresslink)\n\n## YouTube Videos\n\n* [Playlist: Building a Weather Station](https://www.youtube.com/watch?v=hGBIzlp68bU&list=PLhr1KZpdzukdy_S7NpE9kkC75SXUCMYdO)\n* [Espressif's AWS IoT ExpressLink Solution](https://www.youtube.com/watch?v=NSGCVH0OU7w)\n* [u-blox AWS IoT ExpressLink](https://www.youtube.com/watch?v=4GiBnT0I0HE)\n* [AWS IoT ExpressLink and the u-blox NORA-W2](https://www.youtube.com/watch?v=PvyzQwVgCCw)\n* [Product Showcase: AWS IoT ExpressLink SARA-R5 Starter Kit](https://www.youtube.com/watch?v=nJNYUP0413c)\n* [AIROC\u2122 Cloud Connectivity Manager module & AWS IoT ExpressLink](https://www.youtube.com/watch?v=LEGDyNXPYfc)\n* [Innovation Coffee with AWS IoT ExpressLink](https://www.youtube.com/watch?v=K0saFj-6s6c)\n\n## License\n\nThe resources in this repository are licensed under the MIT-0 License. See the LICENSE file.", "release_dates": []}, {"name": "Jobs-for-AWS-IoT-embedded-sdk", "description": "Client library for using AWS IoT Jobs service on embedded devices", "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "## AWS IoT Jobs library\n\n**[API Documentation Pages for current and previous releases of this library can be found here](https://aws.github.io/Jobs-for-AWS-IoT-embedded-sdk/)**\n\nThe AWS IoT Jobs library helps you notify connected IoT devices of a pending\n[Job](https://freertos.org/jobs/jobs-terminology.html). A Job can be used to\nmanage your fleet of devices, update firmware and security certificates on your\ndevices, or perform administrative tasks such as restarting devices and\nperforming diagnostics. It interacts with the\n[AWS IoT Jobs service](https://docs.aws.amazon.com/iot/latest/developerguide/iot-jobs.html)\nusing MQTT, a lightweight publish-subscribe protocol. This library provides a\nconvenience API to compose and recognize the MQTT topic strings used by the Jobs\nservice. The library is written in C compliant with ISO C90 and MISRA C:2012,\nand is distributed under the [MIT Open Source License](LICENSE).\n\nThis library has gone through code quality checks including verification that no\nfunction has a\n[GNU Complexity](https://www.gnu.org/software/complexity/manual/complexity.html)\nscore over 10, and checks against deviations from mandatory rules in the\n[MISRA coding standard](https://www.misra.org.uk). Deviations from the MISRA\nC:2012 guidelines are documented under [MISRA Deviations](MISRA.md). This\nlibrary has also undergone both static code analysis from\n[Coverity](https://scan.coverity.com/), and validation of memory safety with the\n[CBMC bounded model checker](https://www.cprover.org/cbmc/).\n\nSee memory requirements for this library\n[here](./docs/doxygen/include/size_table.md).\n\n**AWS IoT Jobs v1.3.0\n[source code](https://github.com/aws/Jobs-for-AWS-IoT-embedded-sdk/tree/v1.3.0/source)\nis part of the\n[FreeRTOS 202210.00 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202210.00-LTS)\nrelease.**\n\n**AWS IoT Jobs v1.1.0\n[source code](https://github.com/aws/Jobs-for-AWS-IoT-embedded-sdk/tree/v1.1.0/source)\nis part of the\n[FreeRTOS 202012.01 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202012.01-LTS)\nrelease.**\n\n## Building the Jobs library\n\nA compiler that supports **C99 or later** such as _gcc_ is required to build the\nlibrary.\n\nAdditionally, coreJSON is required for parsing. To build the library, first run:\n```bash\ngit clone https://github.com/FreeRTOS/coreJSON.git --depth 1 --branch v3.2.0\n```\n\nGiven an application in a file named `example.c`, _gcc_ can be used like so:\n\n```bash\ngcc -I source/include -I coreJSON/source/include example.c coreJSON/source/core_json.c source/jobs.c -o example\n```\n\n_gcc_ can also produce an object file to be linked later:\n\n```bash\ngcc -I source/include -I coreJSON/source/include -c source/jobs.c\n```\n\n## CBMC\n\nTo learn more about CBMC and proofs specifically, review the training material\n[here](https://model-checking.github.io/cbmc-training).\n\nThe `test/cbmc` directory contains CBMC proofs.\n\nIn order to run these proofs you will need to install CBMC and other tools by\nfollowing the instructions\n[here](https://model-checking.github.io/cbmc-training/installation.html).\n\n## Reference example\n\nThe AWS IoT Device SDK for Embedded C repository contains a demo using the jobs\nlibrary on a POSIX platform.\nhttps://github.com/aws/aws-iot-device-sdk-embedded-C/tree/main/demos/jobs/jobs_demo_mosquitto\n\n## Documentation\n\n### Existing Documentation\n\nFor pre-generated documentation, please see the documentation linked in the\nlocations below:\n\n|                                                           Location                                                           |\n| :--------------------------------------------------------------------------------------------------------------------------: |\n|     [AWS IoT Device SDK for Embedded C](https://github.com/aws/aws-iot-device-sdk-embedded-C#releases-and-documentation)     |\n| [FreeRTOS.org](https://freertos.org/Documentation/api-ref/jobs-for-aws-iot-embedded-sdk/docs/doxygen/output/html/index.html) |\n\nNote that the latest included version of the AWS IoT Jobs library may differ\nacross repositories.\n\n### Generating Documentation\n\nThe Doxygen references were created using Doxygen version 1.9.2. To generate the\nDoxygen pages, please run the following command from the root of this\nrepository:\n\n```shell\ndoxygen docs/doxygen/config.doxyfile\n```\n\n## Building unit tests\n\n### Platform Prerequisites\n\n- For running unit tests\n  - C99 compiler like gcc\n  - CMake 3.13.0 or later\n  - Ruby 2.0.0 or later is additionally required for the Unity test framework\n    (that we use).\n- For running the coverage target, lcov is additionally required.\n\n### Steps to build Unit Tests\n\n1. Create build directory: `mkdir build && cd build`\n\n1. Run _cmake_ while inside build directory: `cmake -S ../test`\n\n1. Run this command to build the library and unit tests: `make all`\n\n1. The generated test executables will be present in `build/bin/tests` folder.\n\n1. Run `ctest` to execute all tests and view the test run summary.\n\n## Contributing\n\nSee [CONTRIBUTING.md](./.github/CONTRIBUTING.md) for information on\ncontributing.\n", "release_dates": ["2023-11-21T20:13:51Z", "2022-10-14T16:09:37Z", "2021-11-11T23:48:25Z", "2021-07-23T01:41:33Z", "2021-02-27T01:42:55Z", "2020-12-14T17:38:59Z"]}, {"name": "jsii", "description": "jsii allows code in any language to naturally interact with JavaScript classes. It is the technology that enables the AWS Cloud Development Kit to deliver polyglot libraries from a single codebase!", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ![jsii](./logo/png/128.png)\n\n[![Join the chat at https://cdk.Dev](https://img.shields.io/static/v1?label=Slack&message=cdk.dev&color=brightgreen&logo=slack)](https://cdk.dev)\n[![All Contributors](https://img.shields.io/github/all-contributors/aws/jsii/main?label=%E2%9C%A8%20All%20Contributors)](#contributors-)\n[![Build Status](https://github.com/aws/jsii/workflows/Main/badge.svg)](https://github.com/aws/jsii/actions?query=workflow%3AMain+branch%3Amain)\n[![npm](https://img.shields.io/npm/v/jsii?logo=npm)](https://www.npmjs.com/package/jsii)\n[![docker](https://img.shields.io/badge/Docker-public.ecr.aws%2Fjsii%2Fsuperchain-brightgreen?logo=docker)](https://gallery.ecr.aws/jsii/superchain)\n\n## Overview\n\n`jsii` allows code in any language to naturally interact with JavaScript classes. It is the technology that enables the\n[AWS Cloud Development Kit][cdk] to deliver polyglot libraries from a single codebase!\n\n[cdk]: https://github.com/aws/aws-cdk\n\nA class library written in **TypeScript** can be used in projects authored in **TypeScript** or **Javascript** (as\nusual), but also in **Python**, **Java**, **C#** (and other languages from the _.NET_ family), ...\n\n## :question: Documentation\n\nHead over to our [documentation website](https://aws.github.io/jsii)!\n\nThe jsii toolchain is spread out on multiple repositories:\n- [aws/jsii-compiler](https://github.com/aws/jsii-compiler) is where the `jsii` compiler is maintained (except releases\n  in the `1.x` line, which are maintained in this repository)\n- [aws/jsii-rosetta](https://github.com/aws/jsii-rosetta) is where the `jsii-rosetta` sample code transliteration tool\n  is maintained (except releases in the `1.x` line, which are maintained in this repository)\n- [aws/jsii](https://github.com/aws/jsii) is where the rest of the toolchain is maintained, including:\n  - `@jsii/spec`, the package that defines the *`.jsii` assembly* specification\n  - `jsii-config`, an interactive tool to help configure your jsii package\n  - `jsii-pacmak`, the bindings generator for jsii packages\n  - `jsii-reflect`, a higher-level way to process *`.jsii` assemblies*\n  - The jsii runtime libraries for the supported jsii target languages\n  - `1.x` release lines of `jsii` and `jsii-rosetta`\n\n# :book: Blog Posts\n\nHere's a collection of blog posts (in chronological order) related to `jsii`:\n\n- **2020-01-11:** <a id=\"blog-mbonig\" /> [How to Create CDK Constructs][mbonig-2020-01-11], by [Matthew Bonig][@mbonig]\n- **2020-05-27:** <a id=\"blog-floydpink\" /> [Generate Python, Java, and .NET software libraries from a TypeScript\n  source][floydpink-2020-05-27], by [Hari Pachuveetil][@floydpink]\n- **2020-12-23:** <a id=\"blog-romainmuller\" /> [How the jsii open source framework meets developers where they are\n  ][romain-2020-12-23], by [Romain Marcadier][@RomainMuller]\n\n[mbonig-2020-01-11]: https://www.matthewbonig.com/2020/01/11/creating-constructs/\n[floydpink-2020-05-27]:\n  https://aws.amazon.com/fr/blogs/opensource/generate-python-java-dotnet-software-libraries-from-typescript-source/\n[romain-2020-12-23]:\n  https://aws.amazon.com/blogs/opensource/how-the-jsii-open-source-framework-meets-developers-where-they-are/\n[@mbonig]: http://www.matthewbonig.com/\n[@floydpink]: https://harimenon.com/\n[@romainmuller]: https://github.com/RomainMuller\n\n> :information_source: If you wrote blog posts about `jsii` and would like to have them referenced here, do not hesitate\n> to file a pull request to add the links here!\n\n# :gear: Contributing\n\nSee [CONTRIBUTING](./CONTRIBUTING.md).\n\n## Contributors \u2728\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/aws/aws-cdk\"><img src=\"https://avatars0.githubusercontent.com/u/43080478?v=4?s=100\" width=\"100px;\" alt=\"AWS CDK Automation\"/><br /><sub><b>AWS CDK Automation</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Aaws-cdk-automation\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Aaws-cdk-automation\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/costleya\"><img src=\"https://avatars2.githubusercontent.com/u/1572163?v=4?s=100\" width=\"100px;\" alt=\"Aaron Costley\"/><br /><sub><b>Aaron Costley</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acostleya+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=costleya\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Acostleya+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Acostleya\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ahodieb\"><img src=\"https://avatars1.githubusercontent.com/u/835502?v=4?s=100\" width=\"100px;\" alt=\"Abdallah Hodieb\"/><br /><sub><b>Abdallah Hodieb</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aahodieb+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://endoflineblog.com/\"><img src=\"https://avatars2.githubusercontent.com/u/460937?v=4?s=100\" width=\"100px;\" alt=\"Adam Ruka\"/><br /><sub><b>Adam Ruka</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Askinny85+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=skinny85\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Askinny85\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Askinny85\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/agdimech\"><img src=\"https://avatars.githubusercontent.com/u/51220968?v=4?s=100\" width=\"100px;\" alt=\"Adrian Dimech\"/><br /><sub><b>Adrian Dimech</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=agdimech\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://adrianhesketh.com/\"><img src=\"https://avatars.githubusercontent.com/u/1029947?v=4?s=100\" width=\"100px;\" alt=\"Adrian Hesketh\"/><br /><sub><b>Adrian Hesketh</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=a-h\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://softwhat.com/\"><img src=\"https://avatars0.githubusercontent.com/u/4362270?v=4?s=100\" width=\"100px;\" alt=\"Alex Pulver\"/><br /><sub><b>Alex Pulver</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aalexpulver+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://kichik.com/\"><img src=\"https://avatars.githubusercontent.com/u/1156773?v=4?s=100\" width=\"100px;\" alt=\"Amir Szekely\"/><br /><sub><b>Amir Szekely</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Akichik\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/andipabst\"><img src=\"https://avatars.githubusercontent.com/u/9639382?v=4?s=100\" width=\"100px;\" alt=\"Andi Pabst\"/><br /><sub><b>Andi Pabst</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aandipabst+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/rectalogic\"><img src=\"https://avatars.githubusercontent.com/u/11581?v=4?s=100\" width=\"100px;\" alt=\"Andrew Wason\"/><br /><sub><b>Andrew Wason</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Arectalogic+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=rectalogic\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/andrestone\"><img src=\"https://avatars.githubusercontent.com/u/7958086?v=4?s=100\" width=\"100px;\" alt=\"Andr\u00e9 Fontenele\"/><br /><sub><b>Andr\u00e9 Fontenele</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=andrestone\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.aslezak.com/\"><img src=\"https://avatars2.githubusercontent.com/u/6944605?v=4?s=100\" width=\"100px;\" alt=\"Andy Slezak\"/><br /><sub><b>Andy Slezak</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=amslezak\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://ansgar.dev\"><img src=\"https://avatars.githubusercontent.com/u/1112056?v=4?s=100\" width=\"100px;\" alt=\"Ansgar Mertens\"/><br /><sub><b>Ansgar Mertens</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Aansgarm\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/commits?author=ansgarm\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aansgarm+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/anshulguleria\"><img src=\"https://avatars3.githubusercontent.com/u/993508?v=4?s=100\" width=\"100px;\" alt=\"Anshul Guleria\"/><br /><sub><b>Anshul Guleria</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aanshulguleria+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/aripalo/\"><img src=\"https://avatars0.githubusercontent.com/u/679146?v=4?s=100\" width=\"100px;\" alt=\"Ari Palo\"/><br /><sub><b>Ari Palo</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aaripalo+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://armaan.tobaccowalla.com\"><img src=\"https://avatars.githubusercontent.com/u/13340433?v=4?s=100\" width=\"100px;\" alt=\"Armaan Tobaccowalla\"/><br /><sub><b>Armaan Tobaccowalla</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AArmaanT+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/BiDzej\"><img src=\"https://avatars1.githubusercontent.com/u/26255490?v=4?s=100\" width=\"100px;\" alt=\"Bart\u0142omiej Jurek\"/><br /><sub><b>Bart\u0142omiej Jurek</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ABiDzej+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://twiiter.com/benbridts\"><img src=\"https://avatars0.githubusercontent.com/u/1301221?v=4?s=100\" width=\"100px;\" alt=\"Ben Bridts\"/><br /><sub><b>Ben Bridts</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=benbridts\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/BenChaimberg\"><img src=\"https://avatars.githubusercontent.com/u/3698184?v=4?s=100\" width=\"100px;\" alt=\"Ben Chaimberg\"/><br /><sub><b>Ben Chaimberg</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=BenChaimberg\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/benfarr\"><img src=\"https://avatars0.githubusercontent.com/u/10361379?v=4?s=100\" width=\"100px;\" alt=\"Ben Farr\"/><br /><sub><b>Ben Farr</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=benfarr\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/BenWal\"><img src=\"https://avatars0.githubusercontent.com/u/2656067?v=4?s=100\" width=\"100px;\" alt=\"Ben Walters\"/><br /><sub><b>Ben Walters</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ABenWal+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://macher.dev\"><img src=\"https://avatars0.githubusercontent.com/u/32685580?v=4?s=100\" width=\"100px;\" alt=\"Benjamin Macher\"/><br /><sub><b>Benjamin Macher</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=bmacher\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bmaizels\"><img src=\"https://avatars1.githubusercontent.com/u/36682168?v=4?s=100\" width=\"100px;\" alt=\"Benjamin Maizels\"/><br /><sub><b>Benjamin Maizels</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=bmaizels\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Abmaizels\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://berviantoleo.my.id/\"><img src=\"https://avatars.githubusercontent.com/u/15927349?v=4?s=100\" width=\"100px;\" alt=\"Bervianto Leo Pratama\"/><br /><sub><b>Bervianto Leo Pratama</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Aberviantoleo\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://wcauchois.github.io/\"><img src=\"https://avatars1.githubusercontent.com/u/300544?v=4?s=100\" width=\"100px;\" alt=\"Bill Cauchois\"/><br /><sub><b>Bill Cauchois</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Awcauchois+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sirrus233\"><img src=\"https://avatars.githubusercontent.com/u/8885220?v=4?s=100\" width=\"100px;\" alt=\"Bradley Sherman\"/><br /><sub><b>Bradley Sherman</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=sirrus233\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bverhoeve\"><img src=\"https://avatars1.githubusercontent.com/u/46007524?v=4?s=100\" width=\"100px;\" alt=\"Brecht Verhoeve\"/><br /><sub><b>Brecht Verhoeve</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Abverhoeve+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://bdawg.org/\"><img src=\"https://avatars1.githubusercontent.com/u/92937?v=4?s=100\" width=\"100px;\" alt=\"Breland Miley\"/><br /><sub><b>Breland Miley</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=mindstorms6\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/CaerusKaru\"><img src=\"https://avatars3.githubusercontent.com/u/416563?v=4?s=100\" width=\"100px;\" alt=\"CaerusKaru\"/><br /><sub><b>CaerusKaru</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=CaerusKaru\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ACaerusKaru\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/comcalvi\"><img src=\"https://avatars.githubusercontent.com/u/66279577?v=4?s=100\" width=\"100px;\" alt=\"Calvin Combs\"/><br /><sub><b>Calvin Combs</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=comcalvi\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Acomcalvi\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://camilobermudez85.github.io/\"><img src=\"https://avatars0.githubusercontent.com/u/7834055?v=4?s=100\" width=\"100px;\" alt=\"Camilo Berm\u00fadez\"/><br /><sub><b>Camilo Berm\u00fadez</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acamilobermudez85+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/campionfellin\"><img src=\"https://avatars3.githubusercontent.com/u/11984923?v=4?s=100\" width=\"100px;\" alt=\"Campion Fellin\"/><br /><sub><b>Campion Fellin</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=campionfellin\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/carterv\"><img src=\"https://avatars2.githubusercontent.com/u/1551538?v=4?s=100\" width=\"100px;\" alt=\"Carter Van Deuren\"/><br /><sub><b>Carter Van Deuren</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acarterv+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/cgarvis\"><img src=\"https://avatars.githubusercontent.com/u/213125?v=4?s=100\" width=\"100px;\" alt=\"Chris Garvis\"/><br /><sub><b>Chris Garvis</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=cgarvis\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://christianmoore.me/\"><img src=\"https://avatars.githubusercontent.com/u/36210509?v=4?s=100\" width=\"100px;\" alt=\"Christian Moore\"/><br /><sub><b>Christian Moore</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ashamelesscookie+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ChristopheVico\"><img src=\"https://avatars.githubusercontent.com/u/56592817?v=4?s=100\" width=\"100px;\" alt=\"Christophe Vico\"/><br /><sub><b>Christophe Vico</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AChristopheVico+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/christophercurrie\"><img src=\"https://avatars0.githubusercontent.com/u/19510?v=4?s=100\" width=\"100px;\" alt=\"Christopher Currie\"/><br /><sub><b>Christopher Currie</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=christophercurrie\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Achristophercurrie+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://rybicki.io/\"><img src=\"https://avatars2.githubusercontent.com/u/5008987?v=4?s=100\" width=\"100px;\" alt=\"Christopher Rybicki\"/><br /><sub><b>Christopher Rybicki</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=Chriscbr\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3AChriscbr+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=Chriscbr\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/colifran\"><img src=\"https://avatars.githubusercontent.com/u/131073567?v=4?s=100\" width=\"100px;\" alt=\"Colin Francis\"/><br /><sub><b>Colin Francis</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Acolifran\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/CommanderRoot\"><img src=\"https://avatars.githubusercontent.com/u/4395417?v=4?s=100\" width=\"100px;\" alt=\"CommanderRoot\"/><br /><sub><b>CommanderRoot</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=CommanderRoot\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/corymhall\"><img src=\"https://avatars.githubusercontent.com/u/43035978?v=4?s=100\" width=\"100px;\" alt=\"Cory Hall\"/><br /><sub><b>Cory Hall</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acorymhall+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://mcristi.wordpress.com\"><img src=\"https://avatars.githubusercontent.com/u/95209?v=4?s=100\" width=\"100px;\" alt=\"Cristian M\u0103gheru\u0219an-Stanciu\"/><br /><sub><b>Cristian M\u0103gheru\u0219an-Stanciu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ACristim+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/CyrusNajmabadi\"><img src=\"https://avatars3.githubusercontent.com/u/4564579?v=4?s=100\" width=\"100px;\" alt=\"CyrusNajmabadi\"/><br /><sub><b>CyrusNajmabadi</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ACyrusNajmabadi+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ACyrusNajmabadi+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dsilbergleithcu-godaddy\"><img src=\"https://avatars.githubusercontent.com/u/78872820?v=4?s=100\" width=\"100px;\" alt=\"Damian Silbergleith\"/><br /><sub><b>Damian Silbergleith</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=dsilbergleithcu-godaddy\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Adsilbergleithcu-godaddy+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://danieldinu.com/\"><img src=\"https://avatars1.githubusercontent.com/u/236187?v=4?s=100\" width=\"100px;\" alt=\"Daniel Dinu\"/><br /><sub><b>Daniel Dinu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Addinu+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=ddinu\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://danielmschmidt.de/\"><img src=\"https://avatars.githubusercontent.com/u/1337046?v=4?s=100\" width=\"100px;\" alt=\"Daniel Schmidt\"/><br /><sub><b>Daniel Schmidt</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ADanielMSchmidt+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=DanielMSchmidt\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.udondan.com/\"><img src=\"https://avatars3.githubusercontent.com/u/6443408?v=4?s=100\" width=\"100px;\" alt=\"Daniel Schroeder\"/><br /><sub><b>Daniel Schroeder</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Audondan+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=udondan\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/commits?author=udondan\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Audondan+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Audondan\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/slotnick\"><img src=\"https://avatars3.githubusercontent.com/u/918175?v=4?s=100\" width=\"100px;\" alt=\"Dave Slotnick\"/><br /><sub><b>Dave Slotnick</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aslotnick+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dastbe\"><img src=\"https://avatars.githubusercontent.com/u/634735?v=4?s=100\" width=\"100px;\" alt=\"David Bell\"/><br /><sub><b>David Bell</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=dastbe\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://caremad.io/\"><img src=\"https://avatars3.githubusercontent.com/u/145979?v=4?s=100\" width=\"100px;\" alt=\"Donald Stufft\"/><br /><sub><b>Donald Stufft</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adstufft+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=dstufft\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Adstufft+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Adstufft\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dagnir\"><img src=\"https://avatars2.githubusercontent.com/u/261310?v=4?s=100\" width=\"100px;\" alt=\"Dongie Agnir\"/><br /><sub><b>Dongie Agnir</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=dagnir\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Adagnir\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://eduardorabelo.me/\"><img src=\"https://avatars.githubusercontent.com/u/829902?v=4?s=100\" width=\"100px;\" alt=\"Eduardo Rabelo\"/><br /><sub><b>Eduardo Rabelo</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=oieduardorabelo\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/edsenabr\"><img src=\"https://avatars3.githubusercontent.com/u/15689137?v=4?s=100\" width=\"100px;\" alt=\"Eduardo Sena S. Rosa\"/><br /><sub><b>Eduardo Sena S. Rosa</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aedsenabr+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://eladb.github.com/\"><img src=\"https://avatars3.githubusercontent.com/u/598796?v=4?s=100\" width=\"100px;\" alt=\"Elad Ben-Israel\"/><br /><sub><b>Elad Ben-Israel</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aeladb+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=eladb\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aeladb+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Aeladb\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Aeladb\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"#talk-eladb\" title=\"Talks\">\ud83d\udce2</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/iliapolo\"><img src=\"https://avatars0.githubusercontent.com/u/1428812?v=4?s=100\" width=\"100px;\" alt=\"Eli Polonsky\"/><br /><sub><b>Eli Polonsky</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ailiapolo+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=iliapolo\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Ailiapolo+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Ailiapolo\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Ailiapolo\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/eric-hc\"><img src=\"https://avatars.githubusercontent.com/u/1885157?v=4?s=100\" width=\"100px;\" alt=\"Eric Carboni\"/><br /><sub><b>Eric Carboni</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=eric-hc\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://ericzbeard.com/\"><img src=\"https://avatars0.githubusercontent.com/u/663183?v=4?s=100\" width=\"100px;\" alt=\"Eric Z. Beard\"/><br /><sub><b>Eric Z. Beard</b></sub></a><br /><a href=\"#projectManagement-ericzbeard\" title=\"Project Management\">\ud83d\udcc6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/McDoit\"><img src=\"https://avatars3.githubusercontent.com/u/16723686?v=4?s=100\" width=\"100px;\" alt=\"Erik Karlsson\"/><br /><sub><b>Erik Karlsson</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AMcDoit+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kozlove-aws\"><img src=\"https://avatars1.githubusercontent.com/u/68875428?v=4?s=100\" width=\"100px;\" alt=\"Eugene Kozlov\"/><br /><sub><b>Eugene Kozlov</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=kozlove-aws\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/FabioGentile\"><img src=\"https://avatars2.githubusercontent.com/u/7030345?v=4?s=100\" width=\"100px;\" alt=\"Fabio Gentile\"/><br /><sub><b>Fabio Gentile</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AFabioGentile+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/workeitel\"><img src=\"https://avatars1.githubusercontent.com/u/7794947?v=4?s=100\" width=\"100px;\" alt=\"Florian Eitel\"/><br /><sub><b>Florian Eitel</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aworkeitel+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/gshpychka\"><img src=\"https://avatars.githubusercontent.com/u/23005347?v=4?s=100\" width=\"100px;\" alt=\"Glib Shpychka\"/><br /><sub><b>Glib Shpychka</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Agshpychka+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.grahamlea.com/\"><img src=\"https://avatars0.githubusercontent.com/u/754403?v=4?s=100\" width=\"100px;\" alt=\"Graham Lea\"/><br /><sub><b>Graham Lea</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AGrahamLea+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3AGrahamLea\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/greglucas\"><img src=\"https://avatars.githubusercontent.com/u/12417828?v=4?s=100\" width=\"100px;\" alt=\"Greg Lucas\"/><br /><sub><b>Greg Lucas</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=greglucas\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/assyadh\"><img src=\"https://avatars0.githubusercontent.com/u/4091730?v=4?s=100\" width=\"100px;\" alt=\"Hamza Assyad\"/><br /><sub><b>Hamza Assyad</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aassyadh+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=assyadh\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aassyadh+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Aassyadh\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://harimenon.com/\"><img src=\"https://avatars2.githubusercontent.com/u/171072?v=4?s=100\" width=\"100px;\" alt=\"Hari Pachuveetil\"/><br /><sub><b>Hari Pachuveetil</b></sub></a><br /><a href=\"#blog-floydpink\" title=\"Blogposts\">\ud83d\udcdd</a> <a href=\"https://github.com/aws/jsii/commits?author=floydpink\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/wafuwafu13\"><img src=\"https://avatars.githubusercontent.com/u/50798936?v=4?s=100\" width=\"100px;\" alt=\"Hirotaka Tagawa / wafuwafu13\"/><br /><sub><b>Hirotaka Tagawa / wafuwafu13</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=wafuwafu13\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/SoManyHs\"><img src=\"https://avatars0.githubusercontent.com/u/29964746?v=4?s=100\" width=\"100px;\" alt=\"Hsing-Hui Hsu\"/><br /><sub><b>Hsing-Hui Hsu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=SoManyHs\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/commits?author=SoManyHs\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ASoManyHs+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3ASoManyHs\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://zepvn.com/\"><img src=\"https://avatars.githubusercontent.com/u/95884?v=4?s=100\" width=\"100px;\" alt=\"Huy Phan (Harry)\"/><br /><sub><b>Huy Phan (Harry)</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=huyphan\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://bandism.net/\"><img src=\"https://avatars.githubusercontent.com/u/22633385?v=4?s=100\" width=\"100px;\" alt=\"Ikko Ashimine\"/><br /><sub><b>Ikko Ashimine</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=eltociear\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Nycto\"><img src=\"https://avatars.githubusercontent.com/u/30517?v=4?s=100\" width=\"100px;\" alt=\"James\"/><br /><sub><b>James</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ANycto+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=Nycto\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/JKCT\"><img src=\"https://avatars.githubusercontent.com/u/24870481?v=4?s=100\" width=\"100px;\" alt=\"James Kelley\"/><br /><sub><b>James Kelley</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AJKCT+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jamesmead.org/\"><img src=\"https://avatars2.githubusercontent.com/u/3169?v=4?s=100\" width=\"100px;\" alt=\"James Mead\"/><br /><sub><b>James Mead</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=floehopper\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jamesiri\"><img src=\"https://avatars1.githubusercontent.com/u/22601145?v=4?s=100\" width=\"100px;\" alt=\"James Siri\"/><br /><sub><b>James Siri</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=jamesiri\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Ajamesiri\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jasdel\"><img src=\"https://avatars3.githubusercontent.com/u/961963?v=4?s=100\" width=\"100px;\" alt=\"Jason Del Ponte\"/><br /><sub><b>Jason Del Ponte</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajasdel+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Ajasdel\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://aws.amazon.com/\"><img src=\"https://avatars1.githubusercontent.com/u/193449?v=4?s=100\" width=\"100px;\" alt=\"Jason Fulghum\"/><br /><sub><b>Jason Fulghum</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Afulghum+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"#projectManagement-fulghum\" title=\"Project Management\">\ud83d\udcc6</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Afulghum\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jmalins\"><img src=\"https://avatars.githubusercontent.com/u/2001356?v=4?s=100\" width=\"100px;\" alt=\"Jeff Malins\"/><br /><sub><b>Jeff Malins</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=jmalins\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Jerry-AWS\"><img src=\"https://avatars3.githubusercontent.com/u/52084730?v=4?s=100\" width=\"100px;\" alt=\"Jerry Kindall\"/><br /><sub><b>Jerry Kindall</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=Jerry-AWS\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3AJerry-AWS+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://nmussy.github.io/\"><img src=\"https://avatars0.githubusercontent.com/u/2505696?v=4?s=100\" width=\"100px;\" alt=\"Jimmy Gaussen\"/><br /><sub><b>Jimmy Gaussen</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Anmussy+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://twitter.com/jowe\"><img src=\"https://avatars.githubusercontent.com/u/569011?v=4?s=100\" width=\"100px;\" alt=\"Johannes Weber\"/><br /><sub><b>Johannes Weber</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=johannes-weber\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jpantzlaff\"><img src=\"https://avatars.githubusercontent.com/u/33850400?v=4?s=100\" width=\"100px;\" alt=\"John Pantzlaff\"/><br /><sub><b>John Pantzlaff</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=jpantzlaff\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://sudolibre.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/20878393?v=4?s=100\" width=\"100px;\" alt=\"Jon Day\"/><br /><sub><b>Jon Day</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=sudolibre\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jsteinich\"><img src=\"https://avatars0.githubusercontent.com/u/3868754?v=4?s=100\" width=\"100px;\" alt=\"Jon Steinich\"/><br /><sub><b>Jon Steinich</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajsteinich+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Ajsteinich+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/commits?author=jsteinich\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://joekiller.com/\"><img src=\"https://avatars3.githubusercontent.com/u/1022919?v=4?s=100\" width=\"100px;\" alt=\"Joseph Lawson\"/><br /><sub><b>Joseph Lawson</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Ajoekiller\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jpmartin2\"><img src=\"https://avatars2.githubusercontent.com/u/2464249?v=4?s=100\" width=\"100px;\" alt=\"Joseph Martin\"/><br /><sub><b>Joseph Martin</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajpmartin2+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dxunix\"><img src=\"https://avatars3.githubusercontent.com/u/11489831?v=4?s=100\" width=\"100px;\" alt=\"Junix\"/><br /><sub><b>Junix</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adxunix+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jusdino\"><img src=\"https://avatars.githubusercontent.com/u/11840575?v=4?s=100\" width=\"100px;\" alt=\"Justin Frahm\"/><br /><sub><b>Justin Frahm</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajusdino+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/jsdtaylor\"><img src=\"https://avatars0.githubusercontent.com/u/15832750?v=4?s=100\" width=\"100px;\" alt=\"Justin Taylor\"/><br /><sub><b>Justin Taylor</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajsdtaylor+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kaizencc\"><img src=\"https://avatars.githubusercontent.com/u/36202692?v=4?s=100\" width=\"100px;\" alt=\"Kaizen Conroy\"/><br /><sub><b>Kaizen Conroy</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=kaizencc\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Akaizencc+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kaizencc\"><img src=\"https://avatars.githubusercontent.com/u/36202692?v=4?s=100\" width=\"100px;\" alt=\"Kaizen Conroy\"/><br /><sub><b>Kaizen Conroy</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=kaizencc\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/z3r0w0n\"><img src=\"https://avatars.githubusercontent.com/u/6740347?v=4?s=100\" width=\"100px;\" alt=\"Kaushik Borra\"/><br /><sub><b>Kaushik Borra</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Az3r0w0n+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/aws/aws-cdk\"><img src=\"https://avatars.githubusercontent.com/u/53584728?v=4?s=100\" width=\"100px;\" alt=\"Kendra Neil\"/><br /><sub><b>Kendra Neil</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ATheRealAmazonKendra\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://findable.no/\"><img src=\"https://avatars.githubusercontent.com/u/51441?v=4?s=100\" width=\"100px;\" alt=\"Knut O. Hellan\"/><br /><sub><b>Knut O. Hellan</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Akhellan+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kiiadi\"><img src=\"https://avatars3.githubusercontent.com/u/4661536?v=4?s=100\" width=\"100px;\" alt=\"Kyle Thomson\"/><br /><sub><b>Kyle Thomson</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=kiiadi\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Akiiadi\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://stackoverflow.com/users/2116873/pedreiro\"><img src=\"https://avatars3.githubusercontent.com/u/10764017?v=4?s=100\" width=\"100px;\" alt=\"Leandro Padua\"/><br /><sub><b>Leandro Padua</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aleandropadua+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://liangzhou.dev\"><img src=\"https://avatars.githubusercontent.com/u/1444104?v=4?s=100\" width=\"100px;\" alt=\"Liang Zhou\"/><br /><sub><b>Liang Zhou</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Alzhoucs+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=lzhoucs\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/madeline-k\"><img src=\"https://avatars.githubusercontent.com/u/80541297?v=4?s=100\" width=\"100px;\" alt=\"Madeline Kusters\"/><br /><sub><b>Madeline Kusters</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=madeline-k\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Amadeline-k+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/majasb\"><img src=\"https://avatars2.githubusercontent.com/u/142510?v=4?s=100\" width=\"100px;\" alt=\"Maja S Bratseth\"/><br /><sub><b>Maja S Bratseth</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Amajasb+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/marcosdiez\"><img src=\"https://avatars2.githubusercontent.com/u/297498?v=4?s=100\" width=\"100px;\" alt=\"Marcos Diez\"/><br /><sub><b>Marcos Diez</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Amarcosdiez+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://polothy.github.io\"><img src=\"https://avatars.githubusercontent.com/u/634657?v=4?s=100\" width=\"100px;\" alt=\"Mark Nielsen\"/><br /><sub><b>Mark Nielsen</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=polothy\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.matthewbonig.com/\"><img src=\"https://avatars2.githubusercontent.com/u/1559437?v=4?s=100\" width=\"100px;\" alt=\"Matthew Bonig\"/><br /><sub><b>Matthew Bonig</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ambonig+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"#blog-mbonig\" title=\"Blogposts\">\ud83d\udcdd</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mpiroc\"><img src=\"https://avatars2.githubusercontent.com/u/1623344?v=4?s=100\" width=\"100px;\" alt=\"Matthew Pirocchi\"/><br /><sub><b>Matthew Pirocchi</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=mpiroc\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Ampiroc+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Ampiroc\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://kane.mx\"><img src=\"https://avatars.githubusercontent.com/u/843303?v=4?s=100\" width=\"100px;\" alt=\"Meng Xin Zhu\"/><br /><sub><b>Meng Xin Zhu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Azxkane+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mneil\"><img src=\"https://avatars.githubusercontent.com/u/1605808?v=4?s=100\" width=\"100px;\" alt=\"Michael Neil\"/><br /><sub><b>Michael Neil</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Amneil\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mikelane\"><img src=\"https://avatars0.githubusercontent.com/u/6543713?v=4?s=100\" width=\"100px;\" alt=\"Mike Lane\"/><br /><sub><b>Mike Lane</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Amikelane+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://elastician.com/\"><img src=\"https://avatars3.githubusercontent.com/u/2056?v=4?s=100\" width=\"100px;\" alt=\"Mitch Garnaat\"/><br /><sub><b>Mitch Garnaat</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Agarnaat+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=garnaat\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Agarnaat+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Agarnaat\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/MrArnoldPalmer\"><img src=\"https://avatars0.githubusercontent.com/u/7221111?v=4?s=100\" width=\"100px;\" alt=\"Mitchell Valine\"/><br /><sub><b>Mitchell Valine</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AMrArnoldPalmer+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=MrArnoldPalmer\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3AMrArnoldPalmer+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3AMrArnoldPalmer\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3AMrArnoldPalmer\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/MohamadSoufan\"><img src=\"https://avatars3.githubusercontent.com/u/28849417?v=4?s=100\" width=\"100px;\" alt=\"Mohamad Soufan\"/><br /><sub><b>Mohamad Soufan</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=MohamadSoufan\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/moelasmar\"><img src=\"https://avatars.githubusercontent.com/u/71043312?v=4?s=100\" width=\"100px;\" alt=\"Mohamed Elasmar\"/><br /><sub><b>Mohamed Elasmar</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=moelasmar\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://moritzkornher.de/\"><img src=\"https://avatars.githubusercontent.com/u/379814?v=4?s=100\" width=\"100px;\" alt=\"Momo Kornher\"/><br /><sub><b>Momo Kornher</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=mrgrain\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mmogylenko\"><img src=\"https://avatars.githubusercontent.com/u/7536624?v=4?s=100\" width=\"100px;\" alt=\"Mykola Mogylenko\"/><br /><sub><b>Mykola Mogylenko</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ammogylenko+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Naumel\"><img src=\"https://avatars.githubusercontent.com/u/104374999?v=4?s=100\" width=\"100px;\" alt=\"Naumel\"/><br /><sub><b>Naumel</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3ANaumel\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/NetaNir\"><img src=\"https://avatars0.githubusercontent.com/u/8578043?v=4?s=100\" width=\"100px;\" alt=\"Neta Nir\"/><br /><sub><b>Neta Nir</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=NetaNir\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ANetaNir+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ANetaNir\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3ANetaNir\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/njlynch\"><img src=\"https://avatars3.githubusercontent.com/u/1376292?v=4?s=100\" width=\"100px;\" alt=\"Nick Lynch\"/><br /><sub><b>Nick Lynch</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Anjlynch+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=njlynch\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Anjlynch\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Anjlynch\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nija-at\"><img src=\"https://avatars2.githubusercontent.com/u/16217941?v=4?s=100\" width=\"100px;\" alt=\"Niranjan Jayakar\"/><br /><sub><b>Niranjan Jayakar</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Anija-at+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=nija-at\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Anija-at+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Anija-at\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Anija-at\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/NGL321\"><img src=\"https://avatars0.githubusercontent.com/u/4944099?v=4?s=100\" width=\"100px;\" alt=\"Noah Litov\"/><br /><sub><b>Noah Litov</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=NGL321\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ANGL321\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3ANGL321\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://otaviomacedo.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/288203?v=4?s=100\" width=\"100px;\" alt=\"Otavio Macedo\"/><br /><sub><b>Otavio Macedo</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=otaviomacedo\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aotaviomacedo+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Pidz-b\"><img src=\"https://avatars3.githubusercontent.com/u/47750432?v=4?s=100\" width=\"100px;\" alt=\"PIDZ - Bart \"/><br /><sub><b>PIDZ - Bart </b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3APidz-b+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/pahud\"><img src=\"https://avatars.githubusercontent.com/u/278432?v=4?s=100\" width=\"100px;\" alt=\"Pahud Hsieh\"/><br /><sub><b>Pahud Hsieh</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=pahud\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/peterwoodworth\"><img src=\"https://avatars.githubusercontent.com/u/44349620?v=4?s=100\" width=\"100px;\" alt=\"Peter Woodworth\"/><br /><sub><b>Peter Woodworth</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Apeterwoodworth\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/donicek\"><img src=\"https://avatars.githubusercontent.com/u/8548012?v=4?s=100\" width=\"100px;\" alt=\"Petr Kacer\"/><br /><sub><b>Petr Kacer</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adonicek+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://petrabarus.net/\"><img src=\"https://avatars3.githubusercontent.com/u/523289?v=4?s=100\" width=\"100px;\" alt=\"Petra Barus\"/><br /><sub><b>Petra Barus</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=petrabarus\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://philcali.me/\"><img src=\"https://avatars1.githubusercontent.com/u/105208?v=4?s=100\" width=\"100px;\" alt=\"Philip Cali\"/><br /><sub><b>Philip Cali</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aphilcali+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Kent1\"><img src=\"https://avatars1.githubusercontent.com/u/83018?v=4?s=100\" width=\"100px;\" alt=\"Quentin Loos\"/><br /><sub><b>Quentin Loos</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AKent1+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Console32\"><img src=\"https://avatars1.githubusercontent.com/u/4870099?v=4?s=100\" width=\"100px;\" alt=\"Raphael\"/><br /><sub><b>Raphael</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AConsole32+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/richardhboyd\"><img src=\"https://avatars0.githubusercontent.com/u/58230111?v=4?s=100\" width=\"100px;\" alt=\"Richard H Boyd\"/><br /><sub><b>Richard H Boyd</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Arichardhboyd+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://rix0r.nl/\"><img src=\"https://avatars2.githubusercontent.com/u/524162?v=4?s=100\" width=\"100px;\" alt=\"Rico Huijbers\"/><br /><sub><b>Rico Huijbers</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Arix0rrr+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=rix0rrr\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Arix0rrr+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Arix0rrr\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Arix0rrr\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://keybase.io/romainmuller\"><img src=\"https://avatars2.githubusercontent.com/u/411689?v=4?s=100\" width=\"100px;\" alt=\"Romain Marcadier\"/><br /><sub><b>Romain Marcadier</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ARomainMuller+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=RomainMuller\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#design-RomainMuller\" title=\"Design\">\ud83c\udfa8</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ARomainMuller+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ARomainMuller\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3ARomainMuller\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"#blog-RomainMuller\" title=\"Blogposts\">\ud83d\udcdd</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ryparker\"><img src=\"https://avatars.githubusercontent.com/u/17558268?v=4?s=100\" width=\"100px;\" alt=\"Ryan Parker\"/><br /><sub><b>Ryan Parker</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=ryparker\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/sadikkuzu/\"><img src=\"https://avatars2.githubusercontent.com/u/23168063?v=4?s=100\" width=\"100px;\" alt=\"SADIK KUZU\"/><br /><sub><b>SADIK KUZU</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Asadikkuzu\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/skarode96\"><img src=\"https://avatars2.githubusercontent.com/u/24491216?v=4?s=100\" width=\"100px;\" alt=\"SK\"/><br /><sub><b>SK</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Askarode96+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/spfink\"><img src=\"https://avatars1.githubusercontent.com/u/20525381?v=4?s=100\" width=\"100px;\" alt=\"Sam Fink\"/><br /><sub><b>Sam Fink</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=spfink\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Aspfink\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://punch.dev/\"><img src=\"https://avatars1.githubusercontent.com/u/38672686?v=4?s=100\" width=\"100px;\" alt=\"Sam Goodwin\"/><br /><sub><b>Sam Goodwin</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Asam-goodwin\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://skorfmann.com/\"><img src=\"https://avatars1.githubusercontent.com/u/136789?v=4?s=100\" width=\"100px;\" alt=\"Sebastian Korfmann\"/><br /><sub><b>Sebastian Korfmann</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Askorfmann+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=skorfmann\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Askorfmann+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://sepehrlaal.com/\"><img src=\"https://avatars.githubusercontent.com/u/5657848?v=4?s=100\" width=\"100px;\" alt=\"Sepehr Laal\"/><br /><sub><b>Sepehr Laal</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3A3p3r+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/khushail\"><img src=\"https://avatars.githubusercontent.com/u/117320115?v=4?s=100\" width=\"100px;\" alt=\"Shailja Khurana\"/><br /><sub><b>Shailja Khurana</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Akhushail\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://digitalsanctum.com/\"><img src=\"https://avatars3.githubusercontent.com/u/30923?v=4?s=100\" width=\"100px;\" alt=\"Shane Witbeck\"/><br /><sub><b>Shane Witbeck</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adigitalsanctum+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/shivlaks\"><img src=\"https://avatars0.githubusercontent.com/u/32604953?v=4?s=100\" width=\"100px;\" alt=\"Shiv Lakshminarayan\"/><br /><sub><b>Shiv Lakshminarayan</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=shivlaks\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Ashivlaks\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Ashivlaks\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/SomayaB\"><img src=\"https://avatars3.githubusercontent.com/u/23043132?v=4?s=100\" width=\"100px;\" alt=\"Somaya\"/><br /><sub><b>Somaya</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=SomayaB\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ASomayaB+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ASomayaB\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3ASomayaB\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/skuenzli\"><img src=\"https://avatars.githubusercontent.com/u/869201?v=4?s=100\" width=\"100px;\" alt=\"Stephen Kuenzli\"/><br /><sub><b>Stephen Kuenzli</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=skuenzli\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/touchez-du-bois\"><img src=\"https://avatars.githubusercontent.com/u/434017?v=4?s=100\" width=\"100px;\" alt=\"Takahiro Sugiura\"/><br /><sub><b>Takahiro Sugiura</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=touchez-du-bois\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://gitter.im/\"><img src=\"https://avatars2.githubusercontent.com/u/8518239?v=4?s=100\" width=\"100px;\" alt=\"The Gitter Badger\"/><br /><sub><b>The Gitter Badger</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=gitter-badger\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Agitter-badger\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://medium.com/@thomaspoignant\"><img src=\"https://avatars2.githubusercontent.com/u/17908063?v=4?s=100\" width=\"100px;\" alt=\"Thomas Poignant\"/><br /><sub><b>Thomas Poignant</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Athomaspoignant+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ThomasSteinbach\"><img src=\"https://avatars0.githubusercontent.com/u/1683246?v=4?s=100\" width=\"100px;\" alt=\"Thomas Steinbach\"/><br /><sub><b>Thomas Steinbach</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AThomasSteinbach+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/hoegertn\"><img src=\"https://avatars2.githubusercontent.com/u/1287829?v=4?s=100\" width=\"100px;\" alt=\"Thorsten Hoeger\"/><br /><sub><b>Thorsten Hoeger</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=hoegertn\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/serverlessunicorn\"><img src=\"https://avatars1.githubusercontent.com/u/54867311?v=4?s=100\" width=\"100px;\" alt=\"Tim Wagner\"/><br /><sub><b>Tim Wagner</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aserverlessunicorn+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aserverlessunicorn+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/TimothyJones\"><img src=\"https://avatars.githubusercontent.com/u/914369?v=4?s=100\" width=\"100px;\" alt=\"Timothy Jones\"/><br /><sub><b>Timothy Jones</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ATimothyJones+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/tobli\"><img src=\"https://avatars3.githubusercontent.com/u/540266?v=4?s=100\" width=\"100px;\" alt=\"Tobias Lidskog\"/><br /><sub><b>Tobias Lidskog</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=tobli\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/TomBonnerAtDerivitec\"><img src=\"https://avatars.githubusercontent.com/u/83637254?v=4?s=100\" width=\"100px;\" alt=\"Tom Bonner\"/><br /><sub><b>Tom Bonner</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ATomBonnerAtDerivitec+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://tompkel.net/\"><img src=\"https://avatars.githubusercontent.com/u/1083460?v=4?s=100\" width=\"100px;\" alt=\"Tom Keller\"/><br /><sub><b>Tom Keller</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Akellertk\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://ty.coghlan.dev/\"><img src=\"https://avatars2.githubusercontent.com/u/15920577?v=4?s=100\" width=\"100px;\" alt=\"Ty Coghlan\"/><br /><sub><b>Ty Coghlan</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AOphirr33+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/tvanhens\"><img src=\"https://avatars1.githubusercontent.com/u/5342795?v=4?s=100\" width=\"100px;\" alt=\"Tyler van Hensbergen\"/><br /><sub><b>Tyler van Hensbergen</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Atvanhens+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/vinayak-kukreja\"><img src=\"https://avatars.githubusercontent.com/u/78971045?v=4?s=100\" width=\"100px;\" alt=\"Vinayak Kukreja\"/><br /><sub><b>Vinayak Kukreja</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=vinayak-kukreja\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://ultidev.com/Products/\"><img src=\"https://avatars1.githubusercontent.com/u/757185?v=4?s=100\" width=\"100px;\" alt=\"Vlad Hrybok\"/><br /><sub><b>Vlad Hrybok</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Avgribok+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Lanayx\"><img src=\"https://avatars2.githubusercontent.com/u/3329606?v=4?s=100\" width=\"100px;\" alt=\"Vladimir Shchur\"/><br /><sub><b>Vladimir Shchur</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ALanayx+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Ragnoroct\"><img src=\"https://avatars.githubusercontent.com/u/19155205?v=4?s=100\" width=\"100px;\" alt=\"Will Bender\"/><br /><sub><b>Will Bender</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ARagnoroct+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://yanex.org/\"><img src=\"https://avatars2.githubusercontent.com/u/95996?v=4?s=100\" width=\"100px;\" alt=\"Yan Zhulanow\"/><br /><sub><b>Yan Zhulanow</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=yanex\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yuth\"><img src=\"https://avatars.githubusercontent.com/u/511386?v=4?s=100\" width=\"100px;\" alt=\"Yathi\"/><br /><sub><b>Yathi</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=yuth\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yglcode\"><img src=\"https://avatars.githubusercontent.com/u/11893614?v=4?s=100\" width=\"100px;\" alt=\"Yigong Liu\"/><br /><sub><b>Yigong Liu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ayglcode+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Ayglcode+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ZachBien\"><img src=\"https://avatars.githubusercontent.com/u/1245628?v=4?s=100\" width=\"100px;\" alt=\"Zach Bienenfeld\"/><br /><sub><b>Zach Bienenfeld</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AZachBien+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ajnarang\"><img src=\"https://avatars3.githubusercontent.com/u/52025281?v=4?s=100\" width=\"100px;\" alt=\"ajnarang\"/><br /><sub><b>ajnarang</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aajnarang+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/andyaaz\"><img src=\"https://avatars.githubusercontent.com/u/24879322?v=4?s=100\" width=\"100px;\" alt=\"andyan\"/><br /><sub><b>andyan</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=andyaaz\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/aniljava\"><img src=\"https://avatars.githubusercontent.com/u/412569?v=4?s=100\" width=\"100px;\" alt=\"aniljava\"/><br /><sub><b>aniljava</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=aniljava\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/arnogeurts-sqills\"><img src=\"https://avatars.githubusercontent.com/u/79304871?v=4?s=100\" width=\"100px;\" alt=\"arnogeurts-sqills\"/><br /><sub><b>arnogeurts-sqills</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aarnogeurts-sqills+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=arnogeurts-sqills\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/cn-cit\"><img src=\"https://avatars.githubusercontent.com/u/27255477?v=4?s=100\" width=\"100px;\" alt=\"cn-cit\"/><br /><sub><b>cn-cit</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acn-cit+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/deccy-mcc\"><img src=\"https://avatars0.githubusercontent.com/u/45844893?v=4?s=100\" width=\"100px;\" alt=\"deccy-mcc\"/><br /><sub><b>deccy-mcc</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adeccy-mcc+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/apps/dependabot-preview\"><img src=\"https://avatars3.githubusercontent.com/in/2141?v=4?s=100\" width=\"100px;\" alt=\"dependabot-preview[bot]\"/><br /><sub><b>dependabot-preview[bot]</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adependabot-preview[bot]+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Adependabot-preview[bot]\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/apps/dependabot\"><img src=\"https://avatars0.githubusercontent.com/in/29110?v=4?s=100\" width=\"100px;\" alt=\"dependabot[bot]\"/><br /><sub><b>dependabot[bot]</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Adependabot[bot]\" title=\"Maintenance\">\ud83d\udea7</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dheffx\"><img src=\"https://avatars0.githubusercontent.com/u/22029918?v=4?s=100\" width=\"100px;\" alt=\"dheffx\"/><br /><sub><b>dheffx</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adheffx+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/gregswdl\"><img src=\"https://avatars0.githubusercontent.com/u/47365273?v=4?s=100\" width=\"100px;\" alt=\"gregswdl\"/><br /><sub><b>gregswdl</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Agregswdl+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/guyroberts21\"><img src=\"https://avatars.githubusercontent.com/u/47118902?v=4?s=100\" width=\"100px;\" alt=\"guyroberts21\"/><br /><sub><b>guyroberts21</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=guyroberts21\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mattBrzezinski\"><img src=\"https://avatars.githubusercontent.com/u/4356074?v=4?s=100\" width=\"100px;\" alt=\"mattBrzezinski\"/><br /><sub><b>mattBrzezinski</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=mattBrzezinski\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mergify\"><img src=\"https://avatars.githubusercontent.com/u/18240476?v=4?s=100\" width=\"100px;\" alt=\"mergify\"/><br /><sub><b>mergify</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Amergify\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/apps/mergify\"><img src=\"https://avatars1.githubusercontent.com/in/10562?v=4?s=100\" width=\"100px;\" alt=\"mergify[bot]\"/><br /><sub><b>mergify[bot]</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Amergify[bot]\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nathannaveen\"><img src=\"https://avatars.githubusercontent.com/u/42319948?v=4?s=100\" width=\"100px;\" alt=\"nathannaveen\"/><br /><sub><b>nathannaveen</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Anathannaveen\" title=\"Maintenance\">\ud83d\udea7</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/seiyashima\"><img src=\"https://avatars2.githubusercontent.com/u/4947101?v=4?s=100\" width=\"100px;\" alt=\"seiyashima42\"/><br /><sub><b>seiyashima42</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aseiyashima+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=seiyashima\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/commits?author=seiyashima\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sullis\"><img src=\"https://avatars3.githubusercontent.com/u/30938?v=4?s=100\" width=\"100px;\" alt=\"sullis\"/><br /><sub><b>sullis</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=sullis\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/vaneek\"><img src=\"https://avatars1.githubusercontent.com/u/8113305?v=4?s=100\" width=\"100px;\" alt=\"vaneek\"/><br /><sub><b>vaneek</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Avaneek+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/wendysophie\"><img src=\"https://avatars.githubusercontent.com/u/54415551?v=4?s=100\" width=\"100px;\" alt=\"wendysophie\"/><br /><sub><b>wendysophie</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Awendysophie+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://ysuzuki19.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/42496938?v=4?s=100\" width=\"100px;\" alt=\"ysuzuki19\"/><br /><sub><b>ysuzuki19</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=ysuzuki19\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification.\nContributions of any kind welcome!\n\n## :balance_scale: License\n\n**jsii** is distributed under the [Apache License, Version 2.0][apache-2.0].\n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.\n\n[apache-2.0]: https://www.apache.org/licenses/LICENSE-2.0\n", "release_dates": ["2024-01-09T15:42:05Z", "2023-12-08T17:53:17Z", "2023-11-16T15:48:42Z", "2023-10-24T20:39:01Z", "2023-10-06T21:27:50Z", "2023-09-20T23:23:03Z", "2023-08-25T16:05:50Z", "2023-08-11T15:52:43Z", "2023-08-02T15:30:38Z", "2023-08-01T22:56:31Z", "2023-07-17T12:20:38Z", "2023-06-14T09:33:31Z", "2023-06-07T12:11:07Z", "2023-05-22T22:19:46Z", "2023-05-10T17:46:15Z", "2023-04-04T16:32:14Z", "2023-03-23T13:10:08Z", "2023-03-16T18:51:20Z", "2023-03-16T12:19:19Z", "2023-03-06T15:24:19Z", "2023-02-24T12:17:58Z", "2023-02-14T20:09:43Z", "2023-01-28T19:14:29Z", "2023-01-04T19:33:13Z", "2022-12-05T12:41:57Z", "2022-11-09T13:10:24Z", "2022-10-19T18:18:33Z", "2022-09-27T11:46:00Z", "2022-09-22T20:38:30Z", "2022-09-02T17:06:36Z"]}, {"name": "jsii-compiler", "description": "The jsii compiler for TypeScript", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ![jsii](https://raw.githubusercontent.com/aws/jsii-compiler/main/logo/png/128.png)\n\n[![Join the chat at https://cdk.Dev](https://img.shields.io/static/v1?label=Slack&message=cdk.dev&color=brightgreen&logo=slack)](https://cdk.dev)\n[![All Contributors](https://img.shields.io/github/all-contributors/aws/jsii/main?label=%E2%9C%A8%20All%20Contributors)](#contributors-)\n[![Build Status](https://github.com/aws/jsii-compiler/workflows/build/badge.svg)](https://github.com/aws/jsii-compiler/actions?query=workflow%3Abuild+branch%3Amain)\n[![npm](https://img.shields.io/npm/v/jsii?logo=npm)](https://www.npmjs.com/package/jsii)\n[![docker](https://img.shields.io/badge/docker-jsii%2Fsuperchain-brightgreen?logo=docker)](https://hub.docker.com/r/jsii/superchain)\n\n## Overview\n\n`jsii` allows code in any language to naturally interact with JavaScript classes. It is the technology that enables the\n[AWS Cloud Development Kit][cdk] to deliver polyglot libraries from a single codebase!\n\n[cdk]: https://github.com/aws/aws-cdk\n\nA class library written in **TypeScript** can be used in projects authored in **TypeScript** or **Javascript** (as\nusual), but also in **Python**, **Java**, **C#** (and other languages from the _.NET_ family), ...\n\n## :question: Documentation\n\nHead over to our [documentation website](https://aws.github.io/jsii)!\n\nThe jsii toolchain is spread out on multiple repositories:\n\n- [aws/jsii-compiler](https://github.com/aws/jsii-compiler) is where the `jsii` compiler is maintained (except releases\n  in the `1.x` line)\n- [aws/jsii-rosetta](https://github.com/aws/jsii-rosetta) is where the `jsii-rosetta` sample code transliteration tool\n  is maintained (except releases in the `1.x` line)\n- [aws/jsii](https://github.com/aws/jsii) is where the rest of the toolchain is maintained, including:\n  - `@jsii/spec`, the package that defines the *`.jsii` assembly* specification\n  - `jsii-config`, an interactive tool to help configure your jsii package\n  - `jsii-pacmak`, the bindings generator for jsii packages\n  - `jsii-reflect`, a higher-level way to process *`.jsii` assemblies*\n  - The jsii runtime libraries for the supported jsii target languages\n  - `1.x` release lines of `jsii` and `jsii-rosetta`\n\n## :book: Blog Posts\n\nHere's a collection of blog posts (in chronological order) related to `jsii`:\n\n- **2020-01-11:** <a id=\"blog-mbonig\" /> [How to Create CDK Constructs][mbonig-2020-01-11], by [Matthew Bonig][@mbonig]\n- **2020-05-27:** <a id=\"blog-floydpink\" /> [Generate Python, Java, and .NET software libraries from a TypeScript\n  source][floydpink-2020-05-27], by [Hari Pachuveetil][@floydpink]\n- **2020-12-23:** <a id=\"blog-romainmuller\" /> [How the jsii open source framework meets developers where they are\n  ][romain-2020-12-23], by [Romain Marcadier][@RomainMuller]\n\n[mbonig-2020-01-11]: https://www.matthewbonig.com/2020/01/11/creating-constructs/\n[floydpink-2020-05-27]:\n  https://aws.amazon.com/fr/blogs/opensource/generate-python-java-dotnet-software-libraries-from-typescript-source/\n[romain-2020-12-23]:\n  https://aws.amazon.com/blogs/opensource/how-the-jsii-open-source-framework-meets-developers-where-they-are/\n[@mbonig]: http://www.matthewbonig.com/\n[@floydpink]: https://harimenon.com/\n[@romainmuller]: https://github.com/RomainMuller\n\n> :information_source: If you wrote blog posts about `jsii` and would like to have them referenced here, do not hesitate\n> to file a pull request to add the links here!\n\n## :gear: Maintenance & Support\n\nThe applicable _Maintenance & Support policy_ can be reviewed in [SUPPORT.md](./SUPPORT.md).\n\nThe current status of `jsii` compiler releases is:\n\n| Release | Status      | Comment                                                                                 |\n| ------- | ----------- | --------------------------------------------------------------------------------------- |\n| `5.3.x` | Current     | ![npm](https://img.shields.io/npm/v/jsii/v5.3-latest?label=jsii%40v5.3-latest&logo=npm) |\n| `5.2.x` | Maintenance | ![npm](https://img.shields.io/npm/v/jsii/v5.2-latest?label=jsii%40v5.2-latest&logo=npm) |\n| `5.1.x` | Maintenance | ![npm](https://img.shields.io/npm/v/jsii/v5.1-latest?label=jsii%40v5.1-latest&logo=npm) |\n| `5.0.x` | Maintenance | ![npm](https://img.shields.io/npm/v/jsii/v5.0-latest?label=jsii%40v5.0-latest&logo=npm) |\n| `1.x`   | Maintenance | <https://github.com/aws/jsii>                                                           |\n\n## :balance_scale: License\n\n**jsii** is distributed under the [Apache License, Version 2.0][apache-2.0].\n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.\n\n[apache-2.0]: https://www.apache.org/licenses/LICENSE-2.0\n\n## :gear: Contributing\n\nSee [CONTRIBUTING](./CONTRIBUTING.md).\n\n### Contributors \u2728\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Hunter-Thompson\"><img src=\"https://avatars.githubusercontent.com/u/20844961?v=4?s=100\" width=\"100px;\" alt=\" Aatman \"/><br /><sub><b> Aatman </b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=Hunter-Thompson\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/costleya\"><img src=\"https://avatars2.githubusercontent.com/u/1572163?v=4?s=100\" width=\"100px;\" alt=\"Aaron Costley\"/><br /><sub><b>Aaron Costley</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acostleya+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=costleya\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Acostleya+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Acostleya\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ahodieb\"><img src=\"https://avatars1.githubusercontent.com/u/835502?v=4?s=100\" width=\"100px;\" alt=\"Abdallah Hodieb\"/><br /><sub><b>Abdallah Hodieb</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aahodieb+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://endoflineblog.com/\"><img src=\"https://avatars2.githubusercontent.com/u/460937?v=4?s=100\" width=\"100px;\" alt=\"Adam Ruka\"/><br /><sub><b>Adam Ruka</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Askinny85+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=skinny85\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Askinny85\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Askinny85\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/agdimech\"><img src=\"https://avatars.githubusercontent.com/u/51220968?v=4?s=100\" width=\"100px;\" alt=\"Adrian Dimech\"/><br /><sub><b>Adrian Dimech</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=agdimech\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://adrianhesketh.com/\"><img src=\"https://avatars.githubusercontent.com/u/1029947?v=4?s=100\" width=\"100px;\" alt=\"Adrian Hesketh\"/><br /><sub><b>Adrian Hesketh</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=a-h\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://softwhat.com/\"><img src=\"https://avatars0.githubusercontent.com/u/4362270?v=4?s=100\" width=\"100px;\" alt=\"Alex Pulver\"/><br /><sub><b>Alex Pulver</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aalexpulver+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://kichik.com/\"><img src=\"https://avatars.githubusercontent.com/u/1156773?v=4?s=100\" width=\"100px;\" alt=\"Amir Szekely\"/><br /><sub><b>Amir Szekely</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=kichik\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/andipabst\"><img src=\"https://avatars.githubusercontent.com/u/9639382?v=4?s=100\" width=\"100px;\" alt=\"Andi Pabst\"/><br /><sub><b>Andi Pabst</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aandipabst+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/rectalogic\"><img src=\"https://avatars.githubusercontent.com/u/11581?v=4?s=100\" width=\"100px;\" alt=\"Andrew Wason\"/><br /><sub><b>Andrew Wason</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Arectalogic+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=rectalogic\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/andrestone\"><img src=\"https://avatars.githubusercontent.com/u/7958086?v=4?s=100\" width=\"100px;\" alt=\"Andr\u00e9 Fontenele\"/><br /><sub><b>Andr\u00e9 Fontenele</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=andrestone\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.aslezak.com/\"><img src=\"https://avatars2.githubusercontent.com/u/6944605?v=4?s=100\" width=\"100px;\" alt=\"Andy Slezak\"/><br /><sub><b>Andy Slezak</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=amslezak\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://ansgar.dev\"><img src=\"https://avatars.githubusercontent.com/u/1112056?v=4?s=100\" width=\"100px;\" alt=\"Ansgar Mertens\"/><br /><sub><b>Ansgar Mertens</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Aansgarm\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/commits?author=ansgarm\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aansgarm+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/anshulguleria\"><img src=\"https://avatars3.githubusercontent.com/u/993508?v=4?s=100\" width=\"100px;\" alt=\"Anshul Guleria\"/><br /><sub><b>Anshul Guleria</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aanshulguleria+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/NextThread\"><img src=\"https://avatars.githubusercontent.com/u/89125578?v=4?s=100\" width=\"100px;\" alt=\"Anurag Roy\"/><br /><sub><b>Anurag Roy</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=NextThread\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/aripalo/\"><img src=\"https://avatars0.githubusercontent.com/u/679146?v=4?s=100\" width=\"100px;\" alt=\"Ari Palo\"/><br /><sub><b>Ari Palo</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aaripalo+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://armaan.tobaccowalla.com\"><img src=\"https://avatars.githubusercontent.com/u/13340433?v=4?s=100\" width=\"100px;\" alt=\"Armaan Tobaccowalla\"/><br /><sub><b>Armaan Tobaccowalla</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AArmaanT+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/BiDzej\"><img src=\"https://avatars1.githubusercontent.com/u/26255490?v=4?s=100\" width=\"100px;\" alt=\"Bart\u0142omiej Jurek\"/><br /><sub><b>Bart\u0142omiej Jurek</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ABiDzej+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://twiiter.com/benbridts\"><img src=\"https://avatars0.githubusercontent.com/u/1301221?v=4?s=100\" width=\"100px;\" alt=\"Ben Bridts\"/><br /><sub><b>Ben Bridts</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=benbridts\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/BenChaimberg\"><img src=\"https://avatars.githubusercontent.com/u/3698184?v=4?s=100\" width=\"100px;\" alt=\"Ben Chaimberg\"/><br /><sub><b>Ben Chaimberg</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=BenChaimberg\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/benfarr\"><img src=\"https://avatars0.githubusercontent.com/u/10361379?v=4?s=100\" width=\"100px;\" alt=\"Ben Farr\"/><br /><sub><b>Ben Farr</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=benfarr\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/BenWal\"><img src=\"https://avatars0.githubusercontent.com/u/2656067?v=4?s=100\" width=\"100px;\" alt=\"Ben Walters\"/><br /><sub><b>Ben Walters</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ABenWal+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://macher.dev\"><img src=\"https://avatars0.githubusercontent.com/u/32685580?v=4?s=100\" width=\"100px;\" alt=\"Benjamin Macher\"/><br /><sub><b>Benjamin Macher</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=bmacher\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bmaizels\"><img src=\"https://avatars1.githubusercontent.com/u/36682168?v=4?s=100\" width=\"100px;\" alt=\"Benjamin Maizels\"/><br /><sub><b>Benjamin Maizels</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=bmaizels\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Abmaizels\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://berviantoleo.my.id/\"><img src=\"https://avatars.githubusercontent.com/u/15927349?v=4?s=100\" width=\"100px;\" alt=\"Bervianto Leo Pratama\"/><br /><sub><b>Bervianto Leo Pratama</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=berviantoleo\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://wcauchois.github.io/\"><img src=\"https://avatars1.githubusercontent.com/u/300544?v=4?s=100\" width=\"100px;\" alt=\"Bill Cauchois\"/><br /><sub><b>Bill Cauchois</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Awcauchois+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sirrus233\"><img src=\"https://avatars.githubusercontent.com/u/8885220?v=4?s=100\" width=\"100px;\" alt=\"Bradley Sherman\"/><br /><sub><b>Bradley Sherman</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=sirrus233\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bverhoeve\"><img src=\"https://avatars1.githubusercontent.com/u/46007524?v=4?s=100\" width=\"100px;\" alt=\"Brecht Verhoeve\"/><br /><sub><b>Brecht Verhoeve</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Abverhoeve+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://bdawg.org/\"><img src=\"https://avatars1.githubusercontent.com/u/92937?v=4?s=100\" width=\"100px;\" alt=\"Breland Miley\"/><br /><sub><b>Breland Miley</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=mindstorms6\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://about.bryanmoffatt.com/\"><img src=\"https://avatars.githubusercontent.com/u/1418030?v=4?s=100\" width=\"100px;\" alt=\"Bryan Moffatt\"/><br /><sub><b>Bryan Moffatt</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=bmoffatt\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/CaerusKaru\"><img src=\"https://avatars3.githubusercontent.com/u/416563?v=4?s=100\" width=\"100px;\" alt=\"CaerusKaru\"/><br /><sub><b>CaerusKaru</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=CaerusKaru\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ACaerusKaru\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/comcalvi\"><img src=\"https://avatars.githubusercontent.com/u/66279577?v=4?s=100\" width=\"100px;\" alt=\"Calvin Combs\"/><br /><sub><b>Calvin Combs</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=comcalvi\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Acomcalvi\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://camilobermudez85.github.io/\"><img src=\"https://avatars0.githubusercontent.com/u/7834055?v=4?s=100\" width=\"100px;\" alt=\"Camilo Berm\u00fadez\"/><br /><sub><b>Camilo Berm\u00fadez</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acamilobermudez85+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/campionfellin\"><img src=\"https://avatars3.githubusercontent.com/u/11984923?v=4?s=100\" width=\"100px;\" alt=\"Campion Fellin\"/><br /><sub><b>Campion Fellin</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=campionfellin\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/carterv\"><img src=\"https://avatars2.githubusercontent.com/u/1551538?v=4?s=100\" width=\"100px;\" alt=\"Carter Van Deuren\"/><br /><sub><b>Carter Van Deuren</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acarterv+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/cgarvis\"><img src=\"https://avatars.githubusercontent.com/u/213125?v=4?s=100\" width=\"100px;\" alt=\"Chris Garvis\"/><br /><sub><b>Chris Garvis</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=cgarvis\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://christianmoore.me/\"><img src=\"https://avatars.githubusercontent.com/u/36210509?v=4?s=100\" width=\"100px;\" alt=\"Christian Moore\"/><br /><sub><b>Christian Moore</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ashamelesscookie+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ChristopheVico\"><img src=\"https://avatars.githubusercontent.com/u/56592817?v=4?s=100\" width=\"100px;\" alt=\"Christophe Vico\"/><br /><sub><b>Christophe Vico</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AChristopheVico+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/christophercurrie\"><img src=\"https://avatars0.githubusercontent.com/u/19510?v=4?s=100\" width=\"100px;\" alt=\"Christopher Currie\"/><br /><sub><b>Christopher Currie</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=christophercurrie\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Achristophercurrie+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://rybicki.io/\"><img src=\"https://avatars2.githubusercontent.com/u/5008987?v=4?s=100\" width=\"100px;\" alt=\"Christopher Rybicki\"/><br /><sub><b>Christopher Rybicki</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=Chriscbr\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3AChriscbr+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=Chriscbr\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/colifran\"><img src=\"https://avatars.githubusercontent.com/u/131073567?v=4?s=100\" width=\"100px;\" alt=\"Colin Francis\"/><br /><sub><b>Colin Francis</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=colifran\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/CommanderRoot\"><img src=\"https://avatars.githubusercontent.com/u/4395417?v=4?s=100\" width=\"100px;\" alt=\"CommanderRoot\"/><br /><sub><b>CommanderRoot</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=CommanderRoot\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/corymhall\"><img src=\"https://avatars.githubusercontent.com/u/43035978?v=4?s=100\" width=\"100px;\" alt=\"Cory Hall\"/><br /><sub><b>Cory Hall</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acorymhall+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://mcristi.wordpress.com\"><img src=\"https://avatars.githubusercontent.com/u/95209?v=4?s=100\" width=\"100px;\" alt=\"Cristian M\u0103gheru\u0219an-Stanciu\"/><br /><sub><b>Cristian M\u0103gheru\u0219an-Stanciu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ACristim+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/CyrusNajmabadi\"><img src=\"https://avatars3.githubusercontent.com/u/4564579?v=4?s=100\" width=\"100px;\" alt=\"CyrusNajmabadi\"/><br /><sub><b>CyrusNajmabadi</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ACyrusNajmabadi+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ACyrusNajmabadi+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dsilbergleithcu-godaddy\"><img src=\"https://avatars.githubusercontent.com/u/78872820?v=4?s=100\" width=\"100px;\" alt=\"Damian Silbergleith\"/><br /><sub><b>Damian Silbergleith</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=dsilbergleithcu-godaddy\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Adsilbergleithcu-godaddy+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://danieldinu.com/\"><img src=\"https://avatars1.githubusercontent.com/u/236187?v=4?s=100\" width=\"100px;\" alt=\"Daniel Dinu\"/><br /><sub><b>Daniel Dinu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Addinu+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=ddinu\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://danielmschmidt.de/\"><img src=\"https://avatars.githubusercontent.com/u/1337046?v=4?s=100\" width=\"100px;\" alt=\"Daniel Schmidt\"/><br /><sub><b>Daniel Schmidt</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ADanielMSchmidt+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=DanielMSchmidt\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.udondan.com/\"><img src=\"https://avatars3.githubusercontent.com/u/6443408?v=4?s=100\" width=\"100px;\" alt=\"Daniel Schroeder\"/><br /><sub><b>Daniel Schroeder</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Audondan+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=udondan\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/commits?author=udondan\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Audondan+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Audondan\" title=\"Maintenance\">\ud83d\udea7</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/slotnick\"><img src=\"https://avatars3.githubusercontent.com/u/918175?v=4?s=100\" width=\"100px;\" alt=\"Dave Slotnick\"/><br /><sub><b>Dave Slotnick</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aslotnick+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dastbe\"><img src=\"https://avatars.githubusercontent.com/u/634735?v=4?s=100\" width=\"100px;\" alt=\"David Bell\"/><br /><sub><b>David Bell</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=dastbe\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://caremad.io/\"><img src=\"https://avatars3.githubusercontent.com/u/145979?v=4?s=100\" width=\"100px;\" alt=\"Donald Stufft\"/><br /><sub><b>Donald Stufft</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adstufft+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=dstufft\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Adstufft+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Adstufft\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dagnir\"><img src=\"https://avatars2.githubusercontent.com/u/261310?v=4?s=100\" width=\"100px;\" alt=\"Dongie Agnir\"/><br /><sub><b>Dongie Agnir</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=dagnir\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Adagnir\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://eduardorabelo.me/\"><img src=\"https://avatars.githubusercontent.com/u/829902?v=4?s=100\" width=\"100px;\" alt=\"Eduardo Rabelo\"/><br /><sub><b>Eduardo Rabelo</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=oieduardorabelo\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/edsenabr\"><img src=\"https://avatars3.githubusercontent.com/u/15689137?v=4?s=100\" width=\"100px;\" alt=\"Eduardo Sena S. Rosa\"/><br /><sub><b>Eduardo Sena S. Rosa</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aedsenabr+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://eladb.github.com/\"><img src=\"https://avatars3.githubusercontent.com/u/598796?v=4?s=100\" width=\"100px;\" alt=\"Elad Ben-Israel\"/><br /><sub><b>Elad Ben-Israel</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aeladb+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=eladb\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aeladb+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Aeladb\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Aeladb\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"#talk-eladb\" title=\"Talks\">\ud83d\udce2</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/iliapolo\"><img src=\"https://avatars0.githubusercontent.com/u/1428812?v=4?s=100\" width=\"100px;\" alt=\"Eli Polonsky\"/><br /><sub><b>Eli Polonsky</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ailiapolo+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=iliapolo\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Ailiapolo+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Ailiapolo\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Ailiapolo\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/eric-hc\"><img src=\"https://avatars.githubusercontent.com/u/1885157?v=4?s=100\" width=\"100px;\" alt=\"Eric Carboni\"/><br /><sub><b>Eric Carboni</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=eric-hc\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://ericzbeard.com/\"><img src=\"https://avatars0.githubusercontent.com/u/663183?v=4?s=100\" width=\"100px;\" alt=\"Eric Z. Beard\"/><br /><sub><b>Eric Z. Beard</b></sub></a><br /><a href=\"#projectManagement-ericzbeard\" title=\"Project Management\">\ud83d\udcc6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/McDoit\"><img src=\"https://avatars3.githubusercontent.com/u/16723686?v=4?s=100\" width=\"100px;\" alt=\"Erik Karlsson\"/><br /><sub><b>Erik Karlsson</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AMcDoit+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kozlove-aws\"><img src=\"https://avatars1.githubusercontent.com/u/68875428?v=4?s=100\" width=\"100px;\" alt=\"Eugene Kozlov\"/><br /><sub><b>Eugene Kozlov</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=kozlove-aws\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/FabioGentile\"><img src=\"https://avatars2.githubusercontent.com/u/7030345?v=4?s=100\" width=\"100px;\" alt=\"Fabio Gentile\"/><br /><sub><b>Fabio Gentile</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AFabioGentile+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/workeitel\"><img src=\"https://avatars1.githubusercontent.com/u/7794947?v=4?s=100\" width=\"100px;\" alt=\"Florian Eitel\"/><br /><sub><b>Florian Eitel</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aworkeitel+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/gshpychka\"><img src=\"https://avatars.githubusercontent.com/u/23005347?v=4?s=100\" width=\"100px;\" alt=\"Glib Shpychka\"/><br /><sub><b>Glib Shpychka</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Agshpychka+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.grahamlea.com/\"><img src=\"https://avatars0.githubusercontent.com/u/754403?v=4?s=100\" width=\"100px;\" alt=\"Graham Lea\"/><br /><sub><b>Graham Lea</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AGrahamLea+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3AGrahamLea\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/greglucas\"><img src=\"https://avatars.githubusercontent.com/u/12417828?v=4?s=100\" width=\"100px;\" alt=\"Greg Lucas\"/><br /><sub><b>Greg Lucas</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=greglucas\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/assyadh\"><img src=\"https://avatars0.githubusercontent.com/u/4091730?v=4?s=100\" width=\"100px;\" alt=\"Hamza Assyad\"/><br /><sub><b>Hamza Assyad</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aassyadh+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=assyadh\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aassyadh+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Aassyadh\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://harimenon.com/\"><img src=\"https://avatars2.githubusercontent.com/u/171072?v=4?s=100\" width=\"100px;\" alt=\"Hari Pachuveetil\"/><br /><sub><b>Hari Pachuveetil</b></sub></a><br /><a href=\"#blog-floydpink\" title=\"Blogposts\">\ud83d\udcdd</a> <a href=\"https://github.com/aws/jsii/commits?author=floydpink\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/wafuwafu13\"><img src=\"https://avatars.githubusercontent.com/u/50798936?v=4?s=100\" width=\"100px;\" alt=\"Hirotaka Tagawa / wafuwafu13\"/><br /><sub><b>Hirotaka Tagawa / wafuwafu13</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=wafuwafu13\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/HBobertz\"><img src=\"https://avatars.githubusercontent.com/u/18233297?v=4?s=100\" width=\"100px;\" alt=\"Hogan Bobertz\"/><br /><sub><b>Hogan Bobertz</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=HBobertz\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/SoManyHs\"><img src=\"https://avatars0.githubusercontent.com/u/29964746?v=4?s=100\" width=\"100px;\" alt=\"Hsing-Hui Hsu\"/><br /><sub><b>Hsing-Hui Hsu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=SoManyHs\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/commits?author=SoManyHs\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ASoManyHs+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3ASoManyHs\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://zepvn.com/\"><img src=\"https://avatars.githubusercontent.com/u/95884?v=4?s=100\" width=\"100px;\" alt=\"Huy Phan (Harry)\"/><br /><sub><b>Huy Phan (Harry)</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=huyphan\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://bandism.net/\"><img src=\"https://avatars.githubusercontent.com/u/22633385?v=4?s=100\" width=\"100px;\" alt=\"Ikko Ashimine\"/><br /><sub><b>Ikko Ashimine</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=eltociear\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Nycto\"><img src=\"https://avatars.githubusercontent.com/u/30517?v=4?s=100\" width=\"100px;\" alt=\"James\"/><br /><sub><b>James</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ANycto+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=Nycto\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/JKCT\"><img src=\"https://avatars.githubusercontent.com/u/24870481?v=4?s=100\" width=\"100px;\" alt=\"James Kelley\"/><br /><sub><b>James Kelley</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AJKCT+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jamesmead.org/\"><img src=\"https://avatars2.githubusercontent.com/u/3169?v=4?s=100\" width=\"100px;\" alt=\"James Mead\"/><br /><sub><b>James Mead</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=floehopper\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jamesiri\"><img src=\"https://avatars1.githubusercontent.com/u/22601145?v=4?s=100\" width=\"100px;\" alt=\"James Siri\"/><br /><sub><b>James Siri</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=jamesiri\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Ajamesiri\" title=\"Maintenance\">\ud83d\udea7</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jasdel\"><img src=\"https://avatars3.githubusercontent.com/u/961963?v=4?s=100\" width=\"100px;\" alt=\"Jason Del Ponte\"/><br /><sub><b>Jason Del Ponte</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajasdel+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Ajasdel\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://aws.amazon.com/\"><img src=\"https://avatars1.githubusercontent.com/u/193449?v=4?s=100\" width=\"100px;\" alt=\"Jason Fulghum\"/><br /><sub><b>Jason Fulghum</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Afulghum+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"#projectManagement-fulghum\" title=\"Project Management\">\ud83d\udcc6</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Afulghum\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jmalins\"><img src=\"https://avatars.githubusercontent.com/u/2001356?v=4?s=100\" width=\"100px;\" alt=\"Jeff Malins\"/><br /><sub><b>Jeff Malins</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=jmalins\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Jerry-AWS\"><img src=\"https://avatars3.githubusercontent.com/u/52084730?v=4?s=100\" width=\"100px;\" alt=\"Jerry Kindall\"/><br /><sub><b>Jerry Kindall</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=Jerry-AWS\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3AJerry-AWS+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://nmussy.github.io/\"><img src=\"https://avatars0.githubusercontent.com/u/2505696?v=4?s=100\" width=\"100px;\" alt=\"Jimmy Gaussen\"/><br /><sub><b>Jimmy Gaussen</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Anmussy+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://twitter.com/jowe\"><img src=\"https://avatars.githubusercontent.com/u/569011?v=4?s=100\" width=\"100px;\" alt=\"Johannes Weber\"/><br /><sub><b>Johannes Weber</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=johannes-weber\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jpantzlaff\"><img src=\"https://avatars.githubusercontent.com/u/33850400?v=4?s=100\" width=\"100px;\" alt=\"John Pantzlaff\"/><br /><sub><b>John Pantzlaff</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=jpantzlaff\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://sudolibre.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/20878393?v=4?s=100\" width=\"100px;\" alt=\"Jon Day\"/><br /><sub><b>Jon Day</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=sudolibre\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jsteinich\"><img src=\"https://avatars0.githubusercontent.com/u/3868754?v=4?s=100\" width=\"100px;\" alt=\"Jon Steinich\"/><br /><sub><b>Jon Steinich</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajsteinich+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Ajsteinich+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/commits?author=jsteinich\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://joekiller.com/\"><img src=\"https://avatars3.githubusercontent.com/u/1022919?v=4?s=100\" width=\"100px;\" alt=\"Joseph Lawson\"/><br /><sub><b>Joseph Lawson</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Ajoekiller\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jpmartin2\"><img src=\"https://avatars2.githubusercontent.com/u/2464249?v=4?s=100\" width=\"100px;\" alt=\"Joseph Martin\"/><br /><sub><b>Joseph Martin</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajpmartin2+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dxunix\"><img src=\"https://avatars3.githubusercontent.com/u/11489831?v=4?s=100\" width=\"100px;\" alt=\"Junix\"/><br /><sub><b>Junix</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adxunix+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jusdino\"><img src=\"https://avatars.githubusercontent.com/u/11840575?v=4?s=100\" width=\"100px;\" alt=\"Justin Frahm\"/><br /><sub><b>Justin Frahm</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajusdino+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/jsdtaylor\"><img src=\"https://avatars0.githubusercontent.com/u/15832750?v=4?s=100\" width=\"100px;\" alt=\"Justin Taylor\"/><br /><sub><b>Justin Taylor</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajsdtaylor+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kaizencc\"><img src=\"https://avatars.githubusercontent.com/u/36202692?v=4?s=100\" width=\"100px;\" alt=\"Kaizen Conroy\"/><br /><sub><b>Kaizen Conroy</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=kaizencc\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/z3r0w0n\"><img src=\"https://avatars.githubusercontent.com/u/6740347?v=4?s=100\" width=\"100px;\" alt=\"Kaushik Borra\"/><br /><sub><b>Kaushik Borra</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Az3r0w0n+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/RyoshiKayo\"><img src=\"https://avatars.githubusercontent.com/u/24500457?v=4?s=100\" width=\"100px;\" alt=\"Kayo\"/><br /><sub><b>Kayo</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=RyoshiKayo\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/aws/aws-cdk\"><img src=\"https://avatars.githubusercontent.com/u/53584728?v=4?s=100\" width=\"100px;\" alt=\"Kendra Neil\"/><br /><sub><b>Kendra Neil</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=TheRealAmazonKendra\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://findable.no/\"><img src=\"https://avatars.githubusercontent.com/u/51441?v=4?s=100\" width=\"100px;\" alt=\"Knut O. Hellan\"/><br /><sub><b>Knut O. Hellan</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Akhellan+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kiiadi\"><img src=\"https://avatars3.githubusercontent.com/u/4661536?v=4?s=100\" width=\"100px;\" alt=\"Kyle Thomson\"/><br /><sub><b>Kyle Thomson</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=kiiadi\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Akiiadi\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://stackoverflow.com/users/2116873/pedreiro\"><img src=\"https://avatars3.githubusercontent.com/u/10764017?v=4?s=100\" width=\"100px;\" alt=\"Leandro Padua\"/><br /><sub><b>Leandro Padua</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aleandropadua+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://liangzhou.dev\"><img src=\"https://avatars.githubusercontent.com/u/1444104?v=4?s=100\" width=\"100px;\" alt=\"Liang Zhou\"/><br /><sub><b>Liang Zhou</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Alzhoucs+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=lzhoucs\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/madeline-k\"><img src=\"https://avatars.githubusercontent.com/u/80541297?v=4?s=100\" width=\"100px;\" alt=\"Madeline Kusters\"/><br /><sub><b>Madeline Kusters</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=madeline-k\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Amadeline-k+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/majasb\"><img src=\"https://avatars2.githubusercontent.com/u/142510?v=4?s=100\" width=\"100px;\" alt=\"Maja S Bratseth\"/><br /><sub><b>Maja S Bratseth</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Amajasb+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/marcosdiez\"><img src=\"https://avatars2.githubusercontent.com/u/297498?v=4?s=100\" width=\"100px;\" alt=\"Marcos Diez\"/><br /><sub><b>Marcos Diez</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Amarcosdiez+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://polothy.github.io\"><img src=\"https://avatars.githubusercontent.com/u/634657?v=4?s=100\" width=\"100px;\" alt=\"Mark Nielsen\"/><br /><sub><b>Mark Nielsen</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=polothy\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.matthewbonig.com/\"><img src=\"https://avatars2.githubusercontent.com/u/1559437?v=4?s=100\" width=\"100px;\" alt=\"Matthew Bonig\"/><br /><sub><b>Matthew Bonig</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ambonig+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"#blog-mbonig\" title=\"Blogposts\">\ud83d\udcdd</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mpiroc\"><img src=\"https://avatars2.githubusercontent.com/u/1623344?v=4?s=100\" width=\"100px;\" alt=\"Matthew Pirocchi\"/><br /><sub><b>Matthew Pirocchi</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=mpiroc\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Ampiroc+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Ampiroc\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://kane.mx\"><img src=\"https://avatars.githubusercontent.com/u/843303?v=4?s=100\" width=\"100px;\" alt=\"Meng Xin Zhu\"/><br /><sub><b>Meng Xin Zhu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Azxkane+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mneil\"><img src=\"https://avatars.githubusercontent.com/u/1605808?v=4?s=100\" width=\"100px;\" alt=\"Michael Neil\"/><br /><sub><b>Michael Neil</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Amneil\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mikelane\"><img src=\"https://avatars0.githubusercontent.com/u/6543713?v=4?s=100\" width=\"100px;\" alt=\"Mike Lane\"/><br /><sub><b>Mike Lane</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Amikelane+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://elastician.com/\"><img src=\"https://avatars3.githubusercontent.com/u/2056?v=4?s=100\" width=\"100px;\" alt=\"Mitch Garnaat\"/><br /><sub><b>Mitch Garnaat</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Agarnaat+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=garnaat\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Agarnaat+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Agarnaat\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/MrArnoldPalmer\"><img src=\"https://avatars0.githubusercontent.com/u/7221111?v=4?s=100\" width=\"100px;\" alt=\"Mitchell Valine\"/><br /><sub><b>Mitchell Valine</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AMrArnoldPalmer+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=MrArnoldPalmer\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3AMrArnoldPalmer+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3AMrArnoldPalmer\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3AMrArnoldPalmer\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/MohamadSoufan\"><img src=\"https://avatars3.githubusercontent.com/u/28849417?v=4?s=100\" width=\"100px;\" alt=\"Mohamad Soufan\"/><br /><sub><b>Mohamad Soufan</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=MohamadSoufan\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/moelasmar\"><img src=\"https://avatars.githubusercontent.com/u/71043312?v=4?s=100\" width=\"100px;\" alt=\"Mohamed Elasmar\"/><br /><sub><b>Mohamed Elasmar</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=moelasmar\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://moritzkornher.de/\"><img src=\"https://avatars.githubusercontent.com/u/379814?v=4?s=100\" width=\"100px;\" alt=\"Momo Kornher\"/><br /><sub><b>Momo Kornher</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=mrgrain\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mmogylenko\"><img src=\"https://avatars.githubusercontent.com/u/7536624?v=4?s=100\" width=\"100px;\" alt=\"Mykola Mogylenko\"/><br /><sub><b>Mykola Mogylenko</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ammogylenko+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Naumel\"><img src=\"https://avatars.githubusercontent.com/u/104374999?v=4?s=100\" width=\"100px;\" alt=\"Naumel\"/><br /><sub><b>Naumel</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3ANaumel\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/NetaNir\"><img src=\"https://avatars0.githubusercontent.com/u/8578043?v=4?s=100\" width=\"100px;\" alt=\"Neta Nir\"/><br /><sub><b>Neta Nir</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=NetaNir\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ANetaNir+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ANetaNir\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3ANetaNir\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/njlynch\"><img src=\"https://avatars3.githubusercontent.com/u/1376292?v=4?s=100\" width=\"100px;\" alt=\"Nick Lynch\"/><br /><sub><b>Nick Lynch</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Anjlynch+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=njlynch\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Anjlynch\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Anjlynch\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nija-at\"><img src=\"https://avatars2.githubusercontent.com/u/16217941?v=4?s=100\" width=\"100px;\" alt=\"Niranjan Jayakar\"/><br /><sub><b>Niranjan Jayakar</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Anija-at+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=nija-at\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Anija-at+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Anija-at\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Anija-at\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/NGL321\"><img src=\"https://avatars0.githubusercontent.com/u/4944099?v=4?s=100\" width=\"100px;\" alt=\"Noah Litov\"/><br /><sub><b>Noah Litov</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=NGL321\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ANGL321\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3ANGL321\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://otaviomacedo.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/288203?v=4?s=100\" width=\"100px;\" alt=\"Otavio Macedo\"/><br /><sub><b>Otavio Macedo</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=otaviomacedo\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aotaviomacedo+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Pidz-b\"><img src=\"https://avatars3.githubusercontent.com/u/47750432?v=4?s=100\" width=\"100px;\" alt=\"PIDZ - Bart \"/><br /><sub><b>PIDZ - Bart </b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3APidz-b+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/pahud\"><img src=\"https://avatars.githubusercontent.com/u/278432?v=4?s=100\" width=\"100px;\" alt=\"Pahud Hsieh\"/><br /><sub><b>Pahud Hsieh</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=pahud\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/peterwoodworth\"><img src=\"https://avatars.githubusercontent.com/u/44349620?v=4?s=100\" width=\"100px;\" alt=\"Peter Woodworth\"/><br /><sub><b>Peter Woodworth</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Apeterwoodworth\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/donicek\"><img src=\"https://avatars.githubusercontent.com/u/8548012?v=4?s=100\" width=\"100px;\" alt=\"Petr Kacer\"/><br /><sub><b>Petr Kacer</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adonicek+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://petrabarus.net/\"><img src=\"https://avatars3.githubusercontent.com/u/523289?v=4?s=100\" width=\"100px;\" alt=\"Petra Barus\"/><br /><sub><b>Petra Barus</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=petrabarus\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://philcali.me/\"><img src=\"https://avatars1.githubusercontent.com/u/105208?v=4?s=100\" width=\"100px;\" alt=\"Philip Cali\"/><br /><sub><b>Philip Cali</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aphilcali+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Kent1\"><img src=\"https://avatars1.githubusercontent.com/u/83018?v=4?s=100\" width=\"100px;\" alt=\"Quentin Loos\"/><br /><sub><b>Quentin Loos</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AKent1+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Console32\"><img src=\"https://avatars1.githubusercontent.com/u/4870099?v=4?s=100\" width=\"100px;\" alt=\"Raphael\"/><br /><sub><b>Raphael</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AConsole32+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/richardhboyd\"><img src=\"https://avatars0.githubusercontent.com/u/58230111?v=4?s=100\" width=\"100px;\" alt=\"Richard H Boyd\"/><br /><sub><b>Richard H Boyd</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Arichardhboyd+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://rix0r.nl/\"><img src=\"https://avatars2.githubusercontent.com/u/524162?v=4?s=100\" width=\"100px;\" alt=\"Rico Huijbers\"/><br /><sub><b>Rico Huijbers</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Arix0rrr+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=rix0rrr\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Arix0rrr+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Arix0rrr\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Arix0rrr\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://keybase.io/romainmuller\"><img src=\"https://avatars2.githubusercontent.com/u/411689?v=4?s=100\" width=\"100px;\" alt=\"Romain Marcadier\"/><br /><sub><b>Romain Marcadier</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ARomainMuller+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=RomainMuller\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#design-RomainMuller\" title=\"Design\">\ud83c\udfa8</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ARomainMuller+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ARomainMuller\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3ARomainMuller\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"#blog-RomainMuller\" title=\"Blogposts\">\ud83d\udcdd</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ryparker\"><img src=\"https://avatars.githubusercontent.com/u/17558268?v=4?s=100\" width=\"100px;\" alt=\"Ryan Parker\"/><br /><sub><b>Ryan Parker</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=ryparker\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/sadikkuzu/\"><img src=\"https://avatars2.githubusercontent.com/u/23168063?v=4?s=100\" width=\"100px;\" alt=\"SADIK KUZU\"/><br /><sub><b>SADIK KUZU</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Asadikkuzu\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/skarode96\"><img src=\"https://avatars2.githubusercontent.com/u/24491216?v=4?s=100\" width=\"100px;\" alt=\"SK\"/><br /><sub><b>SK</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Askarode96+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/spfink\"><img src=\"https://avatars1.githubusercontent.com/u/20525381?v=4?s=100\" width=\"100px;\" alt=\"Sam Fink\"/><br /><sub><b>Sam Fink</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=spfink\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Aspfink\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://punch.dev/\"><img src=\"https://avatars1.githubusercontent.com/u/38672686?v=4?s=100\" width=\"100px;\" alt=\"Sam Goodwin\"/><br /><sub><b>Sam Goodwin</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Asam-goodwin\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://skorfmann.com/\"><img src=\"https://avatars1.githubusercontent.com/u/136789?v=4?s=100\" width=\"100px;\" alt=\"Sebastian Korfmann\"/><br /><sub><b>Sebastian Korfmann</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Askorfmann+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=skorfmann\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Askorfmann+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://sepehrlaal.com/\"><img src=\"https://avatars.githubusercontent.com/u/5657848?v=4?s=100\" width=\"100px;\" alt=\"Sepehr Laal\"/><br /><sub><b>Sepehr Laal</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3A3p3r+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/khushail\"><img src=\"https://avatars.githubusercontent.com/u/117320115?v=4?s=100\" width=\"100px;\" alt=\"Shailja Khurana\"/><br /><sub><b>Shailja Khurana</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=khushail\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://digitalsanctum.com/\"><img src=\"https://avatars3.githubusercontent.com/u/30923?v=4?s=100\" width=\"100px;\" alt=\"Shane Witbeck\"/><br /><sub><b>Shane Witbeck</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adigitalsanctum+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/shivlaks\"><img src=\"https://avatars0.githubusercontent.com/u/32604953?v=4?s=100\" width=\"100px;\" alt=\"Shiv Lakshminarayan\"/><br /><sub><b>Shiv Lakshminarayan</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=shivlaks\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Ashivlaks\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3Ashivlaks\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/SomayaB\"><img src=\"https://avatars3.githubusercontent.com/u/23043132?v=4?s=100\" width=\"100px;\" alt=\"Somaya\"/><br /><sub><b>Somaya</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=SomayaB\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ASomayaB+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ASomayaB\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+reviewed-by%3ASomayaB\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/skuenzli\"><img src=\"https://avatars.githubusercontent.com/u/869201?v=4?s=100\" width=\"100px;\" alt=\"Stephen Kuenzli\"/><br /><sub><b>Stephen Kuenzli</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=skuenzli\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/touchez-du-bois\"><img src=\"https://avatars.githubusercontent.com/u/434017?v=4?s=100\" width=\"100px;\" alt=\"Takahiro Sugiura\"/><br /><sub><b>Takahiro Sugiura</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=touchez-du-bois\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://gitter.im/\"><img src=\"https://avatars2.githubusercontent.com/u/8518239?v=4?s=100\" width=\"100px;\" alt=\"The Gitter Badger\"/><br /><sub><b>The Gitter Badger</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=gitter-badger\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Agitter-badger\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://thomasmatecki.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/10324649?v=4?s=100\" width=\"100px;\" alt=\"Thomas Matecki\"/><br /><sub><b>Thomas Matecki</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=thomasmatecki\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://medium.com/@thomaspoignant\"><img src=\"https://avatars2.githubusercontent.com/u/17908063?v=4?s=100\" width=\"100px;\" alt=\"Thomas Poignant\"/><br /><sub><b>Thomas Poignant</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Athomaspoignant+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ThomasSteinbach\"><img src=\"https://avatars0.githubusercontent.com/u/1683246?v=4?s=100\" width=\"100px;\" alt=\"Thomas Steinbach\"/><br /><sub><b>Thomas Steinbach</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AThomasSteinbach+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/hoegertn\"><img src=\"https://avatars2.githubusercontent.com/u/1287829?v=4?s=100\" width=\"100px;\" alt=\"Thorsten Hoeger\"/><br /><sub><b>Thorsten Hoeger</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=hoegertn\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/serverlessunicorn\"><img src=\"https://avatars1.githubusercontent.com/u/54867311?v=4?s=100\" width=\"100px;\" alt=\"Tim Wagner\"/><br /><sub><b>Tim Wagner</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aserverlessunicorn+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aserverlessunicorn+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/TimothyJones\"><img src=\"https://avatars.githubusercontent.com/u/914369?v=4?s=100\" width=\"100px;\" alt=\"Timothy Jones\"/><br /><sub><b>Timothy Jones</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=TimothyJones\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/tobli\"><img src=\"https://avatars3.githubusercontent.com/u/540266?v=4?s=100\" width=\"100px;\" alt=\"Tobias Lidskog\"/><br /><sub><b>Tobias Lidskog</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=tobli\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/TomBonnerAtDerivitec\"><img src=\"https://avatars.githubusercontent.com/u/83637254?v=4?s=100\" width=\"100px;\" alt=\"Tom Bonner\"/><br /><sub><b>Tom Bonner</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ATomBonnerAtDerivitec+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://tompkel.net/\"><img src=\"https://avatars.githubusercontent.com/u/1083460?v=4?s=100\" width=\"100px;\" alt=\"Tom Keller\"/><br /><sub><b>Tom Keller</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=kellertk\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://ty.coghlan.dev/\"><img src=\"https://avatars2.githubusercontent.com/u/15920577?v=4?s=100\" width=\"100px;\" alt=\"Ty Coghlan\"/><br /><sub><b>Ty Coghlan</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AOphirr33+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/tvanhens\"><img src=\"https://avatars1.githubusercontent.com/u/5342795?v=4?s=100\" width=\"100px;\" alt=\"Tyler van Hensbergen\"/><br /><sub><b>Tyler van Hensbergen</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Atvanhens+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/vinayak-kukreja\"><img src=\"https://avatars.githubusercontent.com/u/78971045?v=4?s=100\" width=\"100px;\" alt=\"Vinayak Kukreja\"/><br /><sub><b>Vinayak Kukreja</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=vinayak-kukreja\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://ultidev.com/Products/\"><img src=\"https://avatars1.githubusercontent.com/u/757185?v=4?s=100\" width=\"100px;\" alt=\"Vlad Hrybok\"/><br /><sub><b>Vlad Hrybok</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Avgribok+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Lanayx\"><img src=\"https://avatars2.githubusercontent.com/u/3329606?v=4?s=100\" width=\"100px;\" alt=\"Vladimir Shchur\"/><br /><sub><b>Vladimir Shchur</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ALanayx+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Ragnoroct\"><img src=\"https://avatars.githubusercontent.com/u/19155205?v=4?s=100\" width=\"100px;\" alt=\"Will Bender\"/><br /><sub><b>Will Bender</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=Ragnoroct\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://yanex.org/\"><img src=\"https://avatars2.githubusercontent.com/u/95996?v=4?s=100\" width=\"100px;\" alt=\"Yan Zhulanow\"/><br /><sub><b>Yan Zhulanow</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=yanex\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yuth\"><img src=\"https://avatars.githubusercontent.com/u/511386?v=4?s=100\" width=\"100px;\" alt=\"Yathi\"/><br /><sub><b>Yathi</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=yuth\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yglcode\"><img src=\"https://avatars.githubusercontent.com/u/11893614?v=4?s=100\" width=\"100px;\" alt=\"Yigong Liu\"/><br /><sub><b>Yigong Liu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ayglcode+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Ayglcode+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ZachBien\"><img src=\"https://avatars.githubusercontent.com/u/1245628?v=4?s=100\" width=\"100px;\" alt=\"Zach Bienenfeld\"/><br /><sub><b>Zach Bienenfeld</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AZachBien+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ajnarang\"><img src=\"https://avatars3.githubusercontent.com/u/52025281?v=4?s=100\" width=\"100px;\" alt=\"ajnarang\"/><br /><sub><b>ajnarang</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aajnarang+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/andyaaz\"><img src=\"https://avatars.githubusercontent.com/u/24879322?v=4?s=100\" width=\"100px;\" alt=\"andyan\"/><br /><sub><b>andyan</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=andyaaz\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/aniljava\"><img src=\"https://avatars.githubusercontent.com/u/412569?v=4?s=100\" width=\"100px;\" alt=\"aniljava\"/><br /><sub><b>aniljava</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=aniljava\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/arnogeurts-sqills\"><img src=\"https://avatars.githubusercontent.com/u/79304871?v=4?s=100\" width=\"100px;\" alt=\"arnogeurts-sqills\"/><br /><sub><b>arnogeurts-sqills</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aarnogeurts-sqills+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=arnogeurts-sqills\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/cn-cit\"><img src=\"https://avatars.githubusercontent.com/u/27255477?v=4?s=100\" width=\"100px;\" alt=\"cn-cit\"/><br /><sub><b>cn-cit</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acn-cit+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/deccy-mcc\"><img src=\"https://avatars0.githubusercontent.com/u/45844893?v=4?s=100\" width=\"100px;\" alt=\"deccy-mcc\"/><br /><sub><b>deccy-mcc</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adeccy-mcc+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/apps/dependabot-preview\"><img src=\"https://avatars3.githubusercontent.com/in/2141?v=4?s=100\" width=\"100px;\" alt=\"dependabot-preview[bot]\"/><br /><sub><b>dependabot-preview[bot]</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adependabot-preview[bot]+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Adependabot-preview[bot]\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/apps/dependabot\"><img src=\"https://avatars0.githubusercontent.com/in/29110?v=4?s=100\" width=\"100px;\" alt=\"dependabot[bot]\"/><br /><sub><b>dependabot[bot]</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Adependabot[bot]\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dheffx\"><img src=\"https://avatars0.githubusercontent.com/u/22029918?v=4?s=100\" width=\"100px;\" alt=\"dheffx\"/><br /><sub><b>dheffx</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adheffx+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/gregswdl\"><img src=\"https://avatars0.githubusercontent.com/u/47365273?v=4?s=100\" width=\"100px;\" alt=\"gregswdl\"/><br /><sub><b>gregswdl</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Agregswdl+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/guyroberts21\"><img src=\"https://avatars.githubusercontent.com/u/47118902?v=4?s=100\" width=\"100px;\" alt=\"guyroberts21\"/><br /><sub><b>guyroberts21</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=guyroberts21\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mattBrzezinski\"><img src=\"https://avatars.githubusercontent.com/u/4356074?v=4?s=100\" width=\"100px;\" alt=\"mattBrzezinski\"/><br /><sub><b>mattBrzezinski</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=mattBrzezinski\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mergify\"><img src=\"https://avatars.githubusercontent.com/u/18240476?v=4?s=100\" width=\"100px;\" alt=\"mergify\"/><br /><sub><b>mergify</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Amergify\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/apps/mergify\"><img src=\"https://avatars1.githubusercontent.com/in/10562?v=4?s=100\" width=\"100px;\" alt=\"mergify[bot]\"/><br /><sub><b>mergify[bot]</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Amergify[bot]\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mikewrighton\"><img src=\"https://avatars.githubusercontent.com/u/8303460?v=4?s=100\" width=\"100px;\" alt=\"mikewrighton\"/><br /><sub><b>mikewrighton</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=mikewrighton\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nathannaveen\"><img src=\"https://avatars.githubusercontent.com/u/42319948?v=4?s=100\" width=\"100px;\" alt=\"nathannaveen\"/><br /><sub><b>nathannaveen</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Anathannaveen\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/paulhcsun\"><img src=\"https://avatars.githubusercontent.com/u/47882901?v=4?s=100\" width=\"100px;\" alt=\"paulhcsun\"/><br /><sub><b>paulhcsun</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=paulhcsun\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/seiyashima\"><img src=\"https://avatars2.githubusercontent.com/u/4947101?v=4?s=100\" width=\"100px;\" alt=\"seiyashima42\"/><br /><sub><b>seiyashima42</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aseiyashima+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/commits?author=seiyashima\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/commits?author=seiyashima\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sullis\"><img src=\"https://avatars3.githubusercontent.com/u/30938?v=4?s=100\" width=\"100px;\" alt=\"sullis\"/><br /><sub><b>sullis</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=sullis\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/vaneek\"><img src=\"https://avatars1.githubusercontent.com/u/8113305?v=4?s=100\" width=\"100px;\" alt=\"vaneek\"/><br /><sub><b>vaneek</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Avaneek+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/wendysophie\"><img src=\"https://avatars.githubusercontent.com/u/54415551?v=4?s=100\" width=\"100px;\" alt=\"wendysophie\"/><br /><sub><b>wendysophie</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Awendysophie+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://ysuzuki19.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/42496938?v=4?s=100\" width=\"100px;\" alt=\"ysuzuki19\"/><br /><sub><b>ysuzuki19</b></sub></a><br /><a href=\"https://github.com/aws/jsii/commits?author=ysuzuki19\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification.\nContributions of any kind welcome!\n", "release_dates": ["2024-02-26T10:11:30Z", "2024-02-26T05:08:05Z", "2024-02-26T00:10:32Z", "2024-02-19T10:11:15Z", "2024-02-19T05:08:11Z", "2024-02-12T10:11:44Z", "2024-02-12T05:07:53Z", "2024-02-12T00:10:34Z", "2024-02-05T10:12:03Z", "2024-02-05T05:08:02Z", "2024-02-05T00:10:59Z", "2024-01-29T15:09:08Z", "2024-01-22T15:09:22Z", "2024-01-22T10:11:13Z", "2024-01-22T05:09:18Z", "2024-01-22T00:11:05Z", "2024-01-15T15:09:15Z", "2024-01-15T10:11:01Z", "2024-01-15T05:09:26Z", "2024-01-15T00:11:11Z", "2024-01-08T00:10:55Z", "2024-01-01T00:11:16Z", "2023-12-25T00:10:47Z", "2023-12-18T16:33:31Z", "2023-12-18T10:11:06Z", "2023-12-18T05:09:16Z", "2023-12-18T00:10:37Z", "2023-12-11T10:11:14Z", "2023-12-11T05:09:22Z", "2023-12-11T00:10:52Z"]}, {"name": "jsii-rosetta", "description": "The jsii sample code transliterator", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ![jsii](https://raw.githubusercontent.com/aws/jsii/main/logo/png/128.png)\n\n[![Join the chat at https://cdk.Dev](https://img.shields.io/static/v1?label=Slack&message=cdk.dev&color=brightgreen&logo=slack)](https://cdk.dev)\n[![All Contributors](https://img.shields.io/github/all-contributors/aws/jsii/main?label=%E2%9C%A8%20All%20Contributors)](#contributors-)\n[![Build Status](https://github.com/aws/jsii-rosetta/workflows/build/badge.svg)](https://github.com/aws/jsii-rosetta/actions?query=workflow%3Abuild+branch%3Amain)\n[![npm](https://img.shields.io/npm/v/jsii-rosetta?logo=npm)](https://www.npmjs.com/package/jsii-rosetta)\n\n## Overview\n\n`jsii-rosetta` translates code samples contained in jsii libraries from TypeScript to supported *jsii* target languages.\nThis is what enables the [AWS Cloud Development Kit][cdk] to deliver polyglot documentation from a single codebase!\n\n`jsii-rosetta` leverages knowledge about jsii language translation conventions in order to produce translations. It only\nsupports a limited set of TypeScript language features (which can be reliably represented in other languages).\n\n[cdk]: https://github.com/aws/aws-cdk\n\n## :question: Documentation\n\nHead over to our [documentation website](https://aws.github.io/jsii)!\n\nThe jsii toolchain spreads out on multiple repositories:\n\n- [aws/jsii-compiler](https://github.com/aws/jsii-compiler) is where the `jsii` compiler is maintained (except releases\n  in the `1.x` line)\n- [aws/jsii-rosetta](https://github.com/aws/jsii-rosetta) is where the `jsii-rosetta` sample code transliteration tool\n  is maintained (except releases in the `1.x` line)\n- [aws/jsii](https://github.com/aws/jsii) is where the rest of the toolchain is maintained, including:\n  - `@jsii/spec`, the package that defines the *`.jsii` assembly* specification\n  - `jsii-config`, an interactive tool to help configure your jsii package\n  - `jsii-pacmak`, the bindings generator for jsii packages\n  - `jsii-reflect`, a higher-level way to process *`.jsii` assemblies*\n  - The jsii runtime libraries for the supported jsii target languages\n  - `1.x` release lines of `jsii` and `jsii-rosetta`\n\n## :gear: Maintenance & Support\n\nThe applicable *Maintenance & Support policy* can be reviewed in [SUPPORT.md](./SUPPORT.md).\n\nThe current status of `jsii-rosetta` releases is:\n\n| Release | Status      | Comment                                                                                                 |\n| ------- | ----------- | ------------------------------------------------------------------------------------------------------- |\n| `5.3.x` | Current     | ![npm](https://img.shields.io/npm/v/jsii-rosetta/v5.3-latest?label=jsii-rosetta%40v5.3-latest&logo=npm) |\n| `5.2.x` | Maintenance | ![npm](https://img.shields.io/npm/v/jsii-rosetta/v5.2-latest?label=jsii-rosetta%40v5.2-latest&logo=npm) |\n| `5.1.x` | Maintenance | ![npm](https://img.shields.io/npm/v/jsii-rosetta/v5.1-latest?label=jsii-rosetta%40v5.1-latest&logo=npm) |\n| `5.0.x` | Maintenance | ![npm](https://img.shields.io/npm/v/jsii-rosetta/v5.0-latest?label=jsii-rosetta%40v5.0-latest&logo=npm) |\n| `1.x`   | Maintenance | ![npm](https://img.shields.io/npm/v/jsii-rosetta/v1?label=jsii-rosetta%40v1&logo=npm)                   |\n\n## :gear: Contributing\n\nSee [CONTRIBUTING](./CONTRIBUTING.md).\n\n## :school_satchel: Getting Started\n\n## Rosetta for example authors\n\nThis section describes what to pay attention to when writing examples that will be converted\nby Rosetta.\n\n### Making examples compile\n\nThe translator can translate both code that completely compiles and typechecks, as well as code that doesn't.\n\nIn case of non-compiling samples the translations will be based off of grammatical parsing only. This has the downside\nthat we do not have the type information available to the exact thing in all instances. Specifically\nstruct types will not be able to be inferred from object literals. Have a look at the following piece of code:\n\n```ts\nsomeObject.someMethod('foo', {\n  bar: 3,\n});\n```\n\nIn non-TypeScript languages, it is important to know the type of the second\nargument to the method here. However, without access to the definition of\n`someMethod()`, it's impossible for Rosetta to know the type, and hence\nit cannot translate the example. It is therefore important to include necessary\nimports, variable declarations, etc, to give Rosetta enough information to figure\nout what's going on in this code, and the example should read like this:\n\n```ts\nimport * as myLib from 'some-library';\n\ndeclare const someObject: myLib.SomeClass;\n\nsomeObject.someMethod('foo', {\n  bar: 3,\n});\n```\n\n### Enforcing correct examples\n\nBy default, Rosetta will accept non-compiling examples. If you set\n`jsiiRosetta.strict` to `true` in your `package.json`,\nthe Rosetta command will fail if any example contains an error:\n\n```js\n/// package.json\n{\n  \"jsiiRosetta\": {\n    \"strict\": true\n  }\n}\n```\n\n### Fixtures\n\nTo avoid having to repeat common setup every time, code samples can use\n\"fixtures\": a source template where the example is inserted. A fixture must\ncontain the text `/// here` and typically looks like this:\n\n```ts\nconst * as module from '@some/dependency';\n\nclass MyClass {\n  constructor() {\n    const obj = new MyObject();\n\n    /// here\n  }\n}\n```\n\nThe example will be inserted at the location marked as `/// here` and will have\naccess to `module`, `obj` and `this`.  Any `import` statements found in the\nexample will automatically be hoisted at the top of the fixture, where they are\nguaranteed to be syntactically valid.\n\nThe default file loaded as a fixture is called `rosetta/default.ts-fixture` in\nthe package directory (if it exists).\n\nExamples can request an alternative fixture by specifying a `fixture` parameter\nas part of the code block fence:\n\n````text\n```ts fixture=some-fixture\n````\n\nOr opt out of using the default fixture by specifying `nofixture`:\n\n````text\n```ts nofixture\n````\n\nTo specify fixtures in an `@example` block, use an accompanying `@exampleMetadata` tag:\n\n````text\n/**\n * My cool class\n *\n * @exampleMetadata fixture=with-setup\n * @example\n *\n * new MyCoolClass();\n */\n````\n\n### Dependencies\n\nWhen compiling examples, Rosetta will make sure your package itself and all of\nits `dependencies` and `peerDependencies` are available in the dependency\nclosure that your examples will be compiled in.\n\nIf there are packages you want to use in an example that should *not* be part\nof your package's dependencies, declare them in `jsiiRosetta.exampleDependencies`\nin your `package.json`:\n\n```js\n/// package.json\n{\n  \"jsiiRosetta\": {\n    \"exampleDependencies\": {\n      \"@some-other/package\": \"^1.2.3\",\n      \"@yet-another/package\": \"*\",\n    }\n  }\n}\n```\n\nYou can also set up a directory with correct dependencies yourself, and pass\n`--directory` when running `jsii-rosetta extract`. We recommend using the\nautomatic closure building mechanism and specifying `exampleDependencies` though.\n\n## Rosetta for package publishers\n\nThis section describes how Rosetta integrates into your build process.\n\n### Extract\n\nRosetta has a number of subcommands. The most important one is `jsii-rosetta extract`.\n\nThe `jsii-rosetta extract` command will take one or more jsii assemblies,\nextract the snippets from them, will try to compile them with respect to a given\nhome directory, and finally store all translations in something called a\n\"tablet\".\n\nA couple of things to note here:\n\n- Snippets are always read from the jsii assembly. That means if you make\n  changes to examples in source files, you must first re-run `jsii` to\n  regenerate the assembly, before re-running `jsii-rosetta extract`.\n- The compilation directory will be used to resolve `import`s. Currently, you\n  are responsible for building a directory with the correct `node_modules`\n  directories in there so that a TypeScript compilation step will find all\n  libraries referenced in the examples. This is especially revelant if your\n  examples include libraries that depend on the *current* library: it is not\n  uncommon to write examples in library `A` showing how to use it in combination\n  with library `B`, where `B` depends on `A`. However, since by definition `B`\n  *cannot* be in the set of dependencies of `A`, you must build a directory with\n  both `B` and `A` in it somewhere in your filesystem and run Rosetta in that\n  directory.\n- \"Extract\" will compile samples in parallel. The more assemblies you give it\n  at the same time, the more efficient of a job it will be able to do.\n\nThe extract command will write a file named `.jsii.tabl.json` next to every\nassembly, containing translations for all samples found in the assembly. You\nshould include this file in your NPM package when you publish, so that\ndownstream consumers of the package have access to the translations.\n\nAn example invocation of `jsii-rosetta extract` looks like this:\n\n```sh\njsii-rosetta extract --directory some/dir $(find . -name .jsii)\n```\n\n#### Running in parallel\n\nSince TypeScript compilation takes a lot of time, much time can be gained by\nusing the CPUs in your system effectively.  `jsii-rosetta extract` will run the\ncompilations in parallel.\n\n`jsii-rosetta` will use a number of workers equal to half the number of CPU\ncores, up to a maximum of 16 workers. This default maximum can be overridden by\nsetting the `JSII_ROSETTA_MAX_WORKER_COUNT` environment variable.\n\nIf you get out of memory errors running too many workers, run a command like\nthis to raise the memory allowed for your workers:\n\n```sh\n/sbin/sysctl -w vm.max_map_count=2251954\n```\n\n#### Caching\n\nRosetta extract will translate all examples found in `.jsii` and write the\ntranslations to `.jsii.tabl.json`. From compilation to compilation, many of these\nexamples won't have changed. Since TypeScript compilation is a fairly expensive\nprocess, we would like to avoid doing unnecessary work as much as possible.\n\nTo that end, rosetta can reuse translations from a cache, and write\nnew translations into the same cache:\n\n```sh\njsii-rosetta extract \\\n  --directory some/dir \\\n  --cache cache.json \\\n  [--trim-cache] \\\n  $(find . -name .jsii)\n```\n\nThe `--trim-cache` flag will remove any old translations from the cache that\ndon't exist anymore in any of the given assemblies. This prevents the cache from\ngrowing endlessly over time (an equivalent `jsii-rosetta trim-cache` command is\navailable if your workflow involves running `extract` in multiple distinct\ninvocations and want to retain the cache between them).\n\n### Infuse\n\nThe `jsii-rosetta infuse` command increases the coverage of examples for classes\nin the assembly.\n\nIt finds classes in the assembly that don't have an example associated with them\nyet (as specified via the `@example` tag in the doc comments), but that are used\nin another example found elsewhere\u2014in either a `README` or an example of another\nclass\u2014it will copy the example to all classes involved.  This will make sure\nyour handwritten examples go as far as possible.\n\nNote that in order to do this, `infuse` will *modify* the assemblies it is\ngiven.\n\n`rosetta infuse` depends on the analysis perfomed by `rosetta extract`, and must\ntherefore be run after `extract`. It can also be run as part of `extract`, by\npassing the `--infuse` flag:\n\n```sh\njsii-rosetta extract \\\n  --directory some/dir \\\n  --infuse \\\n  $(find . -name .jsii)\n```\n\n### Translations and pacmak\n\n`jsii-pacmak` will read translation from tablets to substitute translated examples\ninto the generated source bindings. `pacmak` will automatically read individual\n`.jsii.tabl.json` files if present, and can additionally also read from a global\ntablet file.\n\nWhen a translation for a code sample cannot be found, `pacmak` can be configured\nto do one of the following:\n\n- Leave the sample untranslated (default)\n- Translate the sample in-place (this will slow down generation a lot, and you\n  will not have the fine control over the compilation environment that you would\n  have if you were to use the `extract` command)\n- Fail\n\nExample:\n\n```sh\njsii-pacmak \\\n  [--rosetta-tablet=global.json] \\\n  [--rosetta-unknown-snippets=verbatim|translate|fail]\n```\n\n### Data flow\n\nThe diagram below shows how data flows through the jsii tools when used together:\n\n```text\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           \u2502\n\u2502  Source   \u251c\u2500\u2500\u2500\u2510\n\u2502           \u2502   \u2502    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502    \u2551          \u2551    \u2502            \u2502     \u2551    rosetta    \u2551    \u2502          \u2502\n                \u251c\u2500\u2500\u2500\u25b6\u2551   jsii   \u2551\u2500\u2500\u2500\u25b6\u2502  assembly  \u2502\u2500\u2500\u2500\u2500\u25b6\u2551    extract    \u2551\u2500\u2500\u2500\u25b6\u2502  tablet  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502    \u2551          \u2551    \u2502            \u2502     \u2551               \u2551    \u2502          \u2502\n\u2502           \u2502   \u2502    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502  README   \u2502\u2500\u2500\u2500\u2518                           \u2502                                      \u2502\n\u2502           \u2502                               \u2502                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502           \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557          \u2502\n                                            \u2502           \u2551    rosetta    \u2551          \u2502\n                                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2551    infuse     \u2551\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                        \u2551               \u2551\n                                                        \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n                                                                \u2502\n                                            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                            \u2502                                       \u2502\n                                            \u25bc                                       \u25bc\n                                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                     \u2502            \u2502                           \u2502          \u2502\n                                     \u2502 assembly'  \u2502                           \u2502 tablet'  \u2502\n                                     \u2502            \u2502                           \u2502          \u2502\n                                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                            \u2502                                       \u2502\n                                            \u2502                                       \u2502\n                                            \u2502                                       \u25bc              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                            \u2502                               \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557     \u250c\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n                                            \u2502                               \u2551               \u2551     \u2502             \u2502\u2502\n                                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2551    pacmak     \u2551\u2500\u2500\u2500\u2500\u25b6\u2502  packages   \u2502\u2502\n                                                                            \u2551               \u2551     \u2502             \u251c\u2518\n                                                                            \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                               (potentially\n                                                                             live-translates)\n```\n\n## Advanced topics\n\n### Hiding code from samples\n\nIn order to make examples compile, boilerplate code may need to be added that detracts from the example at hand (such as\nvariable declarations and imports).\n\nThis package supports hiding parts of the original source after translation.\n\nTo mark special locations in the source tree, we can use one of three mechanisms:\n\n- Use a `void` expression statement to mark statement locations in the AST.\n- Use the `comma` operator combined with a `void` expression to mark expression locations in the AST.\n- Use special directive comments (`/// !hide`, `/// !show`) to mark locations that span AST nodes. This is less reliable\n  (because the source location of translated syntax sometimes will have to be estimated) but the only option if you want\n  to mark non-contiguous nodes (such as hide part of a class declaration but show statements inside the constructor).\n\nThe `void` expression keyword and or the `comma` operator feature are little-used JavaScript features that are reliably\nparsed by TypeScript and do not affect the semantics of the application in which they appear (so the program executes\nthe same with or without them).\n\nA handy mnemonic for this feature is that you can use it to \"send your code into the void\".\n\n#### Hiding statements\n\nStatement hiding looks like this:\n\n```ts\nbefore(); // will be shown\n\nvoid 0; // start hiding (the argument to 'void' doesn't matter)\nmiddle(); // will not be shown\nvoid 'show'; // stop hiding\n\nafter(); // will be shown again\n```\n\n#### Hiding expressions\n\nFor hiding expressions, we use `comma` expressions to attach a `void` statement to an expression value without changing\nthe meaning of the code.\n\nExample:\n\n```ts\nfoo(1, 2, (void 1, 3));\n```\n\nWill render as\n\n```ts\nfoo(1, 2)\n```\n\nAlso supports a visible ellipsis:\n\n```ts\nconst x = (void '...', 3);\n```\n\nRenders to:\n\n```ts\nx = ...\n```\n\n#### Hiding across AST nodes\n\nUse special comment directives:\n\n```ts\nbefore();\n/// !hide\nnotShown();\n/// !show\nafter();\n```\n\n## Contributors \u2728\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/costleya\"><img src=\"https://avatars2.githubusercontent.com/u/1572163?v=4?s=100\" width=\"100px;\" alt=\"Aaron Costley\"/><br /><sub><b>Aaron Costley</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acostleya+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=costleya\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Acostleya+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Acostleya\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ahodieb\"><img src=\"https://avatars1.githubusercontent.com/u/835502?v=4?s=100\" width=\"100px;\" alt=\"Abdallah Hodieb\"/><br /><sub><b>Abdallah Hodieb</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aahodieb+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://endoflineblog.com/\"><img src=\"https://avatars2.githubusercontent.com/u/460937?v=4?s=100\" width=\"100px;\" alt=\"Adam Ruka\"/><br /><sub><b>Adam Ruka</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Askinny85+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=skinny85\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Askinny85\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Askinny85\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/agdimech\"><img src=\"https://avatars.githubusercontent.com/u/51220968?v=4?s=100\" width=\"100px;\" alt=\"Adrian Dimech\"/><br /><sub><b>Adrian Dimech</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=agdimech\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://adrianhesketh.com/\"><img src=\"https://avatars.githubusercontent.com/u/1029947?v=4?s=100\" width=\"100px;\" alt=\"Adrian Hesketh\"/><br /><sub><b>Adrian Hesketh</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=a-h\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://softwhat.com/\"><img src=\"https://avatars0.githubusercontent.com/u/4362270?v=4?s=100\" width=\"100px;\" alt=\"Alex Pulver\"/><br /><sub><b>Alex Pulver</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aalexpulver+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://aws.amazon.com/\"><img src=\"https://avatars.githubusercontent.com/u/54958958?v=4?s=100\" width=\"100px;\" alt=\"Amazon GitHub Automation\"/><br /><sub><b>Amazon GitHub Automation</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=amazon-auto\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/andipabst\"><img src=\"https://avatars.githubusercontent.com/u/9639382?v=4?s=100\" width=\"100px;\" alt=\"Andi Pabst\"/><br /><sub><b>Andi Pabst</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aandipabst+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/rectalogic\"><img src=\"https://avatars.githubusercontent.com/u/11581?v=4?s=100\" width=\"100px;\" alt=\"Andrew Wason\"/><br /><sub><b>Andrew Wason</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Arectalogic+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=rectalogic\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.aslezak.com/\"><img src=\"https://avatars2.githubusercontent.com/u/6944605?v=4?s=100\" width=\"100px;\" alt=\"Andy Slezak\"/><br /><sub><b>Andy Slezak</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=amslezak\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://ansgar.dev\"><img src=\"https://avatars.githubusercontent.com/u/1112056?v=4?s=100\" width=\"100px;\" alt=\"Ansgar Mertens\"/><br /><sub><b>Ansgar Mertens</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Aansgarm\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=ansgarm\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aansgarm+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/anshulguleria\"><img src=\"https://avatars3.githubusercontent.com/u/993508?v=4?s=100\" width=\"100px;\" alt=\"Anshul Guleria\"/><br /><sub><b>Anshul Guleria</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aanshulguleria+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/aripalo/\"><img src=\"https://avatars0.githubusercontent.com/u/679146?v=4?s=100\" width=\"100px;\" alt=\"Ari Palo\"/><br /><sub><b>Ari Palo</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aaripalo+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://armaan.tobaccowalla.com\"><img src=\"https://avatars.githubusercontent.com/u/13340433?v=4?s=100\" width=\"100px;\" alt=\"Armaan Tobaccowalla\"/><br /><sub><b>Armaan Tobaccowalla</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AArmaanT+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/BiDzej\"><img src=\"https://avatars1.githubusercontent.com/u/26255490?v=4?s=100\" width=\"100px;\" alt=\"Bart\u0142omiej Jurek\"/><br /><sub><b>Bart\u0142omiej Jurek</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ABiDzej+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://twiiter.com/benbridts\"><img src=\"https://avatars0.githubusercontent.com/u/1301221?v=4?s=100\" width=\"100px;\" alt=\"Ben Bridts\"/><br /><sub><b>Ben Bridts</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=benbridts\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/BenChaimberg\"><img src=\"https://avatars.githubusercontent.com/u/3698184?v=4?s=100\" width=\"100px;\" alt=\"Ben Chaimberg\"/><br /><sub><b>Ben Chaimberg</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=BenChaimberg\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/benfarr\"><img src=\"https://avatars0.githubusercontent.com/u/10361379?v=4?s=100\" width=\"100px;\" alt=\"Ben Farr\"/><br /><sub><b>Ben Farr</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=benfarr\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/BenWal\"><img src=\"https://avatars0.githubusercontent.com/u/2656067?v=4?s=100\" width=\"100px;\" alt=\"Ben Walters\"/><br /><sub><b>Ben Walters</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ABenWal+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://macher.dev\"><img src=\"https://avatars0.githubusercontent.com/u/32685580?v=4?s=100\" width=\"100px;\" alt=\"Benjamin Macher\"/><br /><sub><b>Benjamin Macher</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=bmacher\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bmaizels\"><img src=\"https://avatars1.githubusercontent.com/u/36682168?v=4?s=100\" width=\"100px;\" alt=\"Benjamin Maizels\"/><br /><sub><b>Benjamin Maizels</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=bmaizels\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Abmaizels\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://berviantoleo.my.id/\"><img src=\"https://avatars.githubusercontent.com/u/15927349?v=4?s=100\" width=\"100px;\" alt=\"Bervianto Leo Pratama\"/><br /><sub><b>Bervianto Leo Pratama</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Aberviantoleo\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://wcauchois.github.io/\"><img src=\"https://avatars1.githubusercontent.com/u/300544?v=4?s=100\" width=\"100px;\" alt=\"Bill Cauchois\"/><br /><sub><b>Bill Cauchois</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Awcauchois+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bverhoeve\"><img src=\"https://avatars1.githubusercontent.com/u/46007524?v=4?s=100\" width=\"100px;\" alt=\"Brecht Verhoeve\"/><br /><sub><b>Brecht Verhoeve</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Abverhoeve+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://bdawg.org/\"><img src=\"https://avatars1.githubusercontent.com/u/92937?v=4?s=100\" width=\"100px;\" alt=\"Breland Miley\"/><br /><sub><b>Breland Miley</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=mindstorms6\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/CaerusKaru\"><img src=\"https://avatars3.githubusercontent.com/u/416563?v=4?s=100\" width=\"100px;\" alt=\"CaerusKaru\"/><br /><sub><b>CaerusKaru</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=CaerusKaru\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ACaerusKaru\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/comcalvi\"><img src=\"https://avatars.githubusercontent.com/u/66279577?v=4?s=100\" width=\"100px;\" alt=\"Calvin Combs\"/><br /><sub><b>Calvin Combs</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=comcalvi\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Acomcalvi\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://camilobermudez85.github.io/\"><img src=\"https://avatars0.githubusercontent.com/u/7834055?v=4?s=100\" width=\"100px;\" alt=\"Camilo Berm\u00fadez\"/><br /><sub><b>Camilo Berm\u00fadez</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acamilobermudez85+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/campionfellin\"><img src=\"https://avatars3.githubusercontent.com/u/11984923?v=4?s=100\" width=\"100px;\" alt=\"Campion Fellin\"/><br /><sub><b>Campion Fellin</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=campionfellin\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/carterv\"><img src=\"https://avatars2.githubusercontent.com/u/1551538?v=4?s=100\" width=\"100px;\" alt=\"Carter Van Deuren\"/><br /><sub><b>Carter Van Deuren</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acarterv+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/cgarvis\"><img src=\"https://avatars.githubusercontent.com/u/213125?v=4?s=100\" width=\"100px;\" alt=\"Chris Garvis\"/><br /><sub><b>Chris Garvis</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=cgarvis\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://christianmoore.me/\"><img src=\"https://avatars.githubusercontent.com/u/36210509?v=4?s=100\" width=\"100px;\" alt=\"Christian Moore\"/><br /><sub><b>Christian Moore</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ashamelesscookie+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ChristopheVico\"><img src=\"https://avatars.githubusercontent.com/u/56592817?v=4?s=100\" width=\"100px;\" alt=\"Christophe Vico\"/><br /><sub><b>Christophe Vico</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AChristopheVico+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/christophercurrie\"><img src=\"https://avatars0.githubusercontent.com/u/19510?v=4?s=100\" width=\"100px;\" alt=\"Christopher Currie\"/><br /><sub><b>Christopher Currie</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=christophercurrie\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Achristophercurrie+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://rybicki.io/\"><img src=\"https://avatars2.githubusercontent.com/u/5008987?v=4?s=100\" width=\"100px;\" alt=\"Christopher Rybicki\"/><br /><sub><b>Christopher Rybicki</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=Chriscbr\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3AChriscbr+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=Chriscbr\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/CommanderRoot\"><img src=\"https://avatars.githubusercontent.com/u/4395417?v=4?s=100\" width=\"100px;\" alt=\"CommanderRoot\"/><br /><sub><b>CommanderRoot</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=CommanderRoot\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/corymhall\"><img src=\"https://avatars.githubusercontent.com/u/43035978?v=4?s=100\" width=\"100px;\" alt=\"Cory Hall\"/><br /><sub><b>Cory Hall</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acorymhall+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://mcristi.wordpress.com\"><img src=\"https://avatars.githubusercontent.com/u/95209?v=4?s=100\" width=\"100px;\" alt=\"Cristian M\u0103gheru\u0219an-Stanciu\"/><br /><sub><b>Cristian M\u0103gheru\u0219an-Stanciu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ACristim+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/CyrusNajmabadi\"><img src=\"https://avatars3.githubusercontent.com/u/4564579?v=4?s=100\" width=\"100px;\" alt=\"CyrusNajmabadi\"/><br /><sub><b>CyrusNajmabadi</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ACyrusNajmabadi+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ACyrusNajmabadi+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dsilbergleithcu-godaddy\"><img src=\"https://avatars.githubusercontent.com/u/78872820?v=4?s=100\" width=\"100px;\" alt=\"Damian Silbergleith\"/><br /><sub><b>Damian Silbergleith</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=dsilbergleithcu-godaddy\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Adsilbergleithcu-godaddy+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://danieldinu.com/\"><img src=\"https://avatars1.githubusercontent.com/u/236187?v=4?s=100\" width=\"100px;\" alt=\"Daniel Dinu\"/><br /><sub><b>Daniel Dinu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Addinu+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=ddinu\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://danielmschmidt.de/\"><img src=\"https://avatars.githubusercontent.com/u/1337046?v=4?s=100\" width=\"100px;\" alt=\"Daniel Schmidt\"/><br /><sub><b>Daniel Schmidt</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ADanielMSchmidt+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=DanielMSchmidt\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.udondan.com/\"><img src=\"https://avatars3.githubusercontent.com/u/6443408?v=4?s=100\" width=\"100px;\" alt=\"Daniel Schroeder\"/><br /><sub><b>Daniel Schroeder</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Audondan+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=udondan\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=udondan\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Audondan+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Audondan\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/slotnick\"><img src=\"https://avatars3.githubusercontent.com/u/918175?v=4?s=100\" width=\"100px;\" alt=\"Dave Slotnick\"/><br /><sub><b>Dave Slotnick</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aslotnick+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dastbe\"><img src=\"https://avatars.githubusercontent.com/u/634735?v=4?s=100\" width=\"100px;\" alt=\"David Bell\"/><br /><sub><b>David Bell</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=dastbe\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://caremad.io/\"><img src=\"https://avatars3.githubusercontent.com/u/145979?v=4?s=100\" width=\"100px;\" alt=\"Donald Stufft\"/><br /><sub><b>Donald Stufft</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adstufft+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=dstufft\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Adstufft+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Adstufft\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dagnir\"><img src=\"https://avatars2.githubusercontent.com/u/261310?v=4?s=100\" width=\"100px;\" alt=\"Dongie Agnir\"/><br /><sub><b>Dongie Agnir</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=dagnir\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Adagnir\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://eduardorabelo.me/\"><img src=\"https://avatars.githubusercontent.com/u/829902?v=4?s=100\" width=\"100px;\" alt=\"Eduardo Rabelo\"/><br /><sub><b>Eduardo Rabelo</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=oieduardorabelo\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/edsenabr\"><img src=\"https://avatars3.githubusercontent.com/u/15689137?v=4?s=100\" width=\"100px;\" alt=\"Eduardo Sena S. Rosa\"/><br /><sub><b>Eduardo Sena S. Rosa</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aedsenabr+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://eladb.github.com/\"><img src=\"https://avatars3.githubusercontent.com/u/598796?v=4?s=100\" width=\"100px;\" alt=\"Elad Ben-Israel\"/><br /><sub><b>Elad Ben-Israel</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aeladb+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=eladb\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aeladb+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Aeladb\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Aeladb\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"#talk-eladb\" title=\"Talks\">\ud83d\udce2</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/iliapolo\"><img src=\"https://avatars0.githubusercontent.com/u/1428812?v=4?s=100\" width=\"100px;\" alt=\"Eli Polonsky\"/><br /><sub><b>Eli Polonsky</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ailiapolo+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=iliapolo\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Ailiapolo+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Ailiapolo\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Ailiapolo\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://ericzbeard.com/\"><img src=\"https://avatars0.githubusercontent.com/u/663183?v=4?s=100\" width=\"100px;\" alt=\"Eric Z. Beard\"/><br /><sub><b>Eric Z. Beard</b></sub></a><br /><a href=\"#projectManagement-ericzbeard\" title=\"Project Management\">\ud83d\udcc6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/McDoit\"><img src=\"https://avatars3.githubusercontent.com/u/16723686?v=4?s=100\" width=\"100px;\" alt=\"Erik Karlsson\"/><br /><sub><b>Erik Karlsson</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AMcDoit+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kozlove-aws\"><img src=\"https://avatars1.githubusercontent.com/u/68875428?v=4?s=100\" width=\"100px;\" alt=\"Eugene Kozlov\"/><br /><sub><b>Eugene Kozlov</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=kozlove-aws\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/FabioGentile\"><img src=\"https://avatars2.githubusercontent.com/u/7030345?v=4?s=100\" width=\"100px;\" alt=\"Fabio Gentile\"/><br /><sub><b>Fabio Gentile</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AFabioGentile+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/workeitel\"><img src=\"https://avatars1.githubusercontent.com/u/7794947?v=4?s=100\" width=\"100px;\" alt=\"Florian Eitel\"/><br /><sub><b>Florian Eitel</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aworkeitel+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/gshpychka\"><img src=\"https://avatars.githubusercontent.com/u/23005347?v=4?s=100\" width=\"100px;\" alt=\"Glib Shpychka\"/><br /><sub><b>Glib Shpychka</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Agshpychka+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.grahamlea.com/\"><img src=\"https://avatars0.githubusercontent.com/u/754403?v=4?s=100\" width=\"100px;\" alt=\"Graham Lea\"/><br /><sub><b>Graham Lea</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AGrahamLea+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3AGrahamLea\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/greglucas\"><img src=\"https://avatars.githubusercontent.com/u/12417828?v=4?s=100\" width=\"100px;\" alt=\"Greg Lucas\"/><br /><sub><b>Greg Lucas</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=greglucas\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/assyadh\"><img src=\"https://avatars0.githubusercontent.com/u/4091730?v=4?s=100\" width=\"100px;\" alt=\"Hamza Assyad\"/><br /><sub><b>Hamza Assyad</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aassyadh+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=assyadh\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aassyadh+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Aassyadh\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://harimenon.com/\"><img src=\"https://avatars2.githubusercontent.com/u/171072?v=4?s=100\" width=\"100px;\" alt=\"Hari Pachuveetil\"/><br /><sub><b>Hari Pachuveetil</b></sub></a><br /><a href=\"#blog-floydpink\" title=\"Blogposts\">\ud83d\udcdd</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=floydpink\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/SoManyHs\"><img src=\"https://avatars0.githubusercontent.com/u/29964746?v=4?s=100\" width=\"100px;\" alt=\"Hsing-Hui Hsu\"/><br /><sub><b>Hsing-Hui Hsu</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=SoManyHs\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=SoManyHs\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ASoManyHs+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3ASoManyHs\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://bandism.net/\"><img src=\"https://avatars.githubusercontent.com/u/22633385?v=4?s=100\" width=\"100px;\" alt=\"Ikko Ashimine\"/><br /><sub><b>Ikko Ashimine</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=eltociear\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Nycto\"><img src=\"https://avatars.githubusercontent.com/u/30517?v=4?s=100\" width=\"100px;\" alt=\"James\"/><br /><sub><b>James</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ANycto+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=Nycto\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/JKCT\"><img src=\"https://avatars.githubusercontent.com/u/24870481?v=4?s=100\" width=\"100px;\" alt=\"James Kelley\"/><br /><sub><b>James Kelley</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AJKCT+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jamesmead.org/\"><img src=\"https://avatars2.githubusercontent.com/u/3169?v=4?s=100\" width=\"100px;\" alt=\"James Mead\"/><br /><sub><b>James Mead</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=floehopper\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jamesiri\"><img src=\"https://avatars1.githubusercontent.com/u/22601145?v=4?s=100\" width=\"100px;\" alt=\"James Siri\"/><br /><sub><b>James Siri</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=jamesiri\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Ajamesiri\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jasdel\"><img src=\"https://avatars3.githubusercontent.com/u/961963?v=4?s=100\" width=\"100px;\" alt=\"Jason Del Ponte\"/><br /><sub><b>Jason Del Ponte</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajasdel+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Ajasdel\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://aws.amazon.com/\"><img src=\"https://avatars1.githubusercontent.com/u/193449?v=4?s=100\" width=\"100px;\" alt=\"Jason Fulghum\"/><br /><sub><b>Jason Fulghum</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Afulghum+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"#projectManagement-fulghum\" title=\"Project Management\">\ud83d\udcc6</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Afulghum\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jmalins\"><img src=\"https://avatars.githubusercontent.com/u/2001356?v=4?s=100\" width=\"100px;\" alt=\"Jeff Malins\"/><br /><sub><b>Jeff Malins</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=jmalins\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Jerry-AWS\"><img src=\"https://avatars3.githubusercontent.com/u/52084730?v=4?s=100\" width=\"100px;\" alt=\"Jerry Kindall\"/><br /><sub><b>Jerry Kindall</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=Jerry-AWS\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3AJerry-AWS+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://nmussy.github.io/\"><img src=\"https://avatars0.githubusercontent.com/u/2505696?v=4?s=100\" width=\"100px;\" alt=\"Jimmy Gaussen\"/><br /><sub><b>Jimmy Gaussen</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Anmussy+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://twitter.com/jowe\"><img src=\"https://avatars.githubusercontent.com/u/569011?v=4?s=100\" width=\"100px;\" alt=\"Johannes Weber\"/><br /><sub><b>Johannes Weber</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=johannes-weber\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jpantzlaff\"><img src=\"https://avatars.githubusercontent.com/u/33850400?v=4?s=100\" width=\"100px;\" alt=\"John Pantzlaff\"/><br /><sub><b>John Pantzlaff</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=jpantzlaff\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jsteinich\"><img src=\"https://avatars0.githubusercontent.com/u/3868754?v=4?s=100\" width=\"100px;\" alt=\"Jon Steinich\"/><br /><sub><b>Jon Steinich</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajsteinich+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Ajsteinich+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=jsteinich\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://joekiller.com/\"><img src=\"https://avatars3.githubusercontent.com/u/1022919?v=4?s=100\" width=\"100px;\" alt=\"Joseph Lawson\"/><br /><sub><b>Joseph Lawson</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Ajoekiller\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jpmartin2\"><img src=\"https://avatars2.githubusercontent.com/u/2464249?v=4?s=100\" width=\"100px;\" alt=\"Joseph Martin\"/><br /><sub><b>Joseph Martin</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajpmartin2+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dxunix\"><img src=\"https://avatars3.githubusercontent.com/u/11489831?v=4?s=100\" width=\"100px;\" alt=\"Junix\"/><br /><sub><b>Junix</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adxunix+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jusdino\"><img src=\"https://avatars.githubusercontent.com/u/11840575?v=4?s=100\" width=\"100px;\" alt=\"Justin Frahm\"/><br /><sub><b>Justin Frahm</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajusdino+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/jsdtaylor\"><img src=\"https://avatars0.githubusercontent.com/u/15832750?v=4?s=100\" width=\"100px;\" alt=\"Justin Taylor\"/><br /><sub><b>Justin Taylor</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ajsdtaylor+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kaizen3031593\"><img src=\"https://avatars.githubusercontent.com/u/36202692?v=4?s=100\" width=\"100px;\" alt=\"Kaizen Conroy\"/><br /><sub><b>Kaizen Conroy</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=kaizen3031593\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Akaizen3031593+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kaizencc\"><img src=\"https://avatars.githubusercontent.com/u/36202692?v=4?s=100\" width=\"100px;\" alt=\"Kaizen Conroy\"/><br /><sub><b>Kaizen Conroy</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=kaizencc\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/z3r0w0n\"><img src=\"https://avatars.githubusercontent.com/u/6740347?v=4?s=100\" width=\"100px;\" alt=\"Kaushik Borra\"/><br /><sub><b>Kaushik Borra</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Az3r0w0n+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/aws/aws-cdk\"><img src=\"https://avatars.githubusercontent.com/u/53584728?v=4?s=100\" width=\"100px;\" alt=\"Kendra Neil\"/><br /><sub><b>Kendra Neil</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=TheRealAmazonKendra\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/KhurramJalil\"><img src=\"https://avatars.githubusercontent.com/u/114917595?v=4?s=100\" width=\"100px;\" alt=\"Khurram Jalil\"/><br /><sub><b>Khurram Jalil</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=KhurramJalil\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://findable.no/\"><img src=\"https://avatars.githubusercontent.com/u/51441?v=4?s=100\" width=\"100px;\" alt=\"Knut O. Hellan\"/><br /><sub><b>Knut O. Hellan</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Akhellan+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kiiadi\"><img src=\"https://avatars3.githubusercontent.com/u/4661536?v=4?s=100\" width=\"100px;\" alt=\"Kyle Thomson\"/><br /><sub><b>Kyle Thomson</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=kiiadi\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Akiiadi\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://stackoverflow.com/users/2116873/pedreiro\"><img src=\"https://avatars3.githubusercontent.com/u/10764017?v=4?s=100\" width=\"100px;\" alt=\"Leandro Padua\"/><br /><sub><b>Leandro Padua</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aleandropadua+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://liangzhou.dev\"><img src=\"https://avatars.githubusercontent.com/u/1444104?v=4?s=100\" width=\"100px;\" alt=\"Liang Zhou\"/><br /><sub><b>Liang Zhou</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Alzhoucs+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=lzhoucs\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/madeline-k\"><img src=\"https://avatars.githubusercontent.com/u/80541297?v=4?s=100\" width=\"100px;\" alt=\"Madeline Kusters\"/><br /><sub><b>Madeline Kusters</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=madeline-k\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Amadeline-k+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/majasb\"><img src=\"https://avatars2.githubusercontent.com/u/142510?v=4?s=100\" width=\"100px;\" alt=\"Maja S Bratseth\"/><br /><sub><b>Maja S Bratseth</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Amajasb+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/marcosdiez\"><img src=\"https://avatars2.githubusercontent.com/u/297498?v=4?s=100\" width=\"100px;\" alt=\"Marcos Diez\"/><br /><sub><b>Marcos Diez</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Amarcosdiez+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://polothy.github.io\"><img src=\"https://avatars.githubusercontent.com/u/634657?v=4?s=100\" width=\"100px;\" alt=\"Mark Nielsen\"/><br /><sub><b>Mark Nielsen</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=polothy\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.matthewbonig.com/\"><img src=\"https://avatars2.githubusercontent.com/u/1559437?v=4?s=100\" width=\"100px;\" alt=\"Matthew Bonig\"/><br /><sub><b>Matthew Bonig</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ambonig+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"#blog-mbonig\" title=\"Blogposts\">\ud83d\udcdd</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mpiroc\"><img src=\"https://avatars2.githubusercontent.com/u/1623344?v=4?s=100\" width=\"100px;\" alt=\"Matthew Pirocchi\"/><br /><sub><b>Matthew Pirocchi</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=mpiroc\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Ampiroc+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Ampiroc\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://kane.mx\"><img src=\"https://avatars.githubusercontent.com/u/843303?v=4?s=100\" width=\"100px;\" alt=\"Meng Xin Zhu\"/><br /><sub><b>Meng Xin Zhu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Azxkane+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mneil\"><img src=\"https://avatars.githubusercontent.com/u/1605808?v=4?s=100\" width=\"100px;\" alt=\"Michael Neil\"/><br /><sub><b>Michael Neil</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Amneil\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mikelane\"><img src=\"https://avatars0.githubusercontent.com/u/6543713?v=4?s=100\" width=\"100px;\" alt=\"Mike Lane\"/><br /><sub><b>Mike Lane</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Amikelane+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://elastician.com/\"><img src=\"https://avatars3.githubusercontent.com/u/2056?v=4?s=100\" width=\"100px;\" alt=\"Mitch Garnaat\"/><br /><sub><b>Mitch Garnaat</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Agarnaat+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=garnaat\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Agarnaat+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Agarnaat\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/MrArnoldPalmer\"><img src=\"https://avatars0.githubusercontent.com/u/7221111?v=4?s=100\" width=\"100px;\" alt=\"Mitchell Valine\"/><br /><sub><b>Mitchell Valine</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AMrArnoldPalmer+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=MrArnoldPalmer\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3AMrArnoldPalmer+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3AMrArnoldPalmer\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3AMrArnoldPalmer\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/MohamadSoufan\"><img src=\"https://avatars3.githubusercontent.com/u/28849417?v=4?s=100\" width=\"100px;\" alt=\"Mohamad Soufan\"/><br /><sub><b>Mohamad Soufan</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=MohamadSoufan\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://moritzkornher.de/\"><img src=\"https://avatars.githubusercontent.com/u/379814?v=4?s=100\" width=\"100px;\" alt=\"Momo Kornher\"/><br /><sub><b>Momo Kornher</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=mrgrain\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mmogylenko\"><img src=\"https://avatars.githubusercontent.com/u/7536624?v=4?s=100\" width=\"100px;\" alt=\"Mykola Mogylenko\"/><br /><sub><b>Mykola Mogylenko</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ammogylenko+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Naumel\"><img src=\"https://avatars.githubusercontent.com/u/104374999?v=4?s=100\" width=\"100px;\" alt=\"Naumel\"/><br /><sub><b>Naumel</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3ANaumel\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/NetaNir\"><img src=\"https://avatars0.githubusercontent.com/u/8578043?v=4?s=100\" width=\"100px;\" alt=\"Neta Nir\"/><br /><sub><b>Neta Nir</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=NetaNir\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ANetaNir+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ANetaNir\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3ANetaNir\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/njlynch\"><img src=\"https://avatars3.githubusercontent.com/u/1376292?v=4?s=100\" width=\"100px;\" alt=\"Nick Lynch\"/><br /><sub><b>Nick Lynch</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Anjlynch+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=njlynch\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Anjlynch\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Anjlynch\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nija-at\"><img src=\"https://avatars2.githubusercontent.com/u/16217941?v=4?s=100\" width=\"100px;\" alt=\"Niranjan Jayakar\"/><br /><sub><b>Niranjan Jayakar</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Anija-at+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=nija-at\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Anija-at+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Anija-at\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Anija-at\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/NGL321\"><img src=\"https://avatars0.githubusercontent.com/u/4944099?v=4?s=100\" width=\"100px;\" alt=\"Noah Litov\"/><br /><sub><b>Noah Litov</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=NGL321\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ANGL321\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3ANGL321\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://otaviomacedo.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/288203?v=4?s=100\" width=\"100px;\" alt=\"Otavio Macedo\"/><br /><sub><b>Otavio Macedo</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=otaviomacedo\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aotaviomacedo+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Pidz-b\"><img src=\"https://avatars3.githubusercontent.com/u/47750432?v=4?s=100\" width=\"100px;\" alt=\"PIDZ - Bart \"/><br /><sub><b>PIDZ - Bart </b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3APidz-b+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/peterwoodworth\"><img src=\"https://avatars.githubusercontent.com/u/44349620?v=4?s=100\" width=\"100px;\" alt=\"Peter Woodworth\"/><br /><sub><b>Peter Woodworth</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Apeterwoodworth\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/donicek\"><img src=\"https://avatars.githubusercontent.com/u/8548012?v=4?s=100\" width=\"100px;\" alt=\"Petr Kacer\"/><br /><sub><b>Petr Kacer</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adonicek+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://petrabarus.net/\"><img src=\"https://avatars3.githubusercontent.com/u/523289?v=4?s=100\" width=\"100px;\" alt=\"Petra Barus\"/><br /><sub><b>Petra Barus</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=petrabarus\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://philcali.me/\"><img src=\"https://avatars1.githubusercontent.com/u/105208?v=4?s=100\" width=\"100px;\" alt=\"Philip Cali\"/><br /><sub><b>Philip Cali</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aphilcali+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Kent1\"><img src=\"https://avatars1.githubusercontent.com/u/83018?v=4?s=100\" width=\"100px;\" alt=\"Quentin Loos\"/><br /><sub><b>Quentin Loos</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AKent1+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Console32\"><img src=\"https://avatars1.githubusercontent.com/u/4870099?v=4?s=100\" width=\"100px;\" alt=\"Raphael\"/><br /><sub><b>Raphael</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AConsole32+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/richardhboyd\"><img src=\"https://avatars0.githubusercontent.com/u/58230111?v=4?s=100\" width=\"100px;\" alt=\"Richard H Boyd\"/><br /><sub><b>Richard H Boyd</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Arichardhboyd+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://rix0r.nl/\"><img src=\"https://avatars2.githubusercontent.com/u/524162?v=4?s=100\" width=\"100px;\" alt=\"Rico Huijbers\"/><br /><sub><b>Rico Huijbers</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Arix0rrr+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=rix0rrr\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Arix0rrr+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Arix0rrr\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Arix0rrr\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://keybase.io/romainmuller\"><img src=\"https://avatars2.githubusercontent.com/u/411689?v=4?s=100\" width=\"100px;\" alt=\"Romain Marcadier\"/><br /><sub><b>Romain Marcadier</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ARomainMuller+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=RomainMuller\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#design-RomainMuller\" title=\"Design\">\ud83c\udfa8</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ARomainMuller+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ARomainMuller\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3ARomainMuller\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"#blog-RomainMuller\" title=\"Blogposts\">\ud83d\udcdd</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/sadikkuzu/\"><img src=\"https://avatars2.githubusercontent.com/u/23168063?v=4?s=100\" width=\"100px;\" alt=\"SADIK KUZU\"/><br /><sub><b>SADIK KUZU</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Asadikkuzu\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/skarode96\"><img src=\"https://avatars2.githubusercontent.com/u/24491216?v=4?s=100\" width=\"100px;\" alt=\"SK\"/><br /><sub><b>SK</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Askarode96+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/spfink\"><img src=\"https://avatars1.githubusercontent.com/u/20525381?v=4?s=100\" width=\"100px;\" alt=\"Sam Fink\"/><br /><sub><b>Sam Fink</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=spfink\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Aspfink\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://punch.dev/\"><img src=\"https://avatars1.githubusercontent.com/u/38672686?v=4?s=100\" width=\"100px;\" alt=\"Sam Goodwin\"/><br /><sub><b>Sam Goodwin</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Asam-goodwin\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://skorfmann.com/\"><img src=\"https://avatars1.githubusercontent.com/u/136789?v=4?s=100\" width=\"100px;\" alt=\"Sebastian Korfmann\"/><br /><sub><b>Sebastian Korfmann</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Askorfmann+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=skorfmann\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Askorfmann+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://sepehrlaal.com/\"><img src=\"https://avatars.githubusercontent.com/u/5657848?v=4?s=100\" width=\"100px;\" alt=\"Sepehr Laal\"/><br /><sub><b>Sepehr Laal</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3A3p3r+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://digitalsanctum.com/\"><img src=\"https://avatars3.githubusercontent.com/u/30923?v=4?s=100\" width=\"100px;\" alt=\"Shane Witbeck\"/><br /><sub><b>Shane Witbeck</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adigitalsanctum+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/shivlaks\"><img src=\"https://avatars0.githubusercontent.com/u/32604953?v=4?s=100\" width=\"100px;\" alt=\"Shiv Lakshminarayan\"/><br /><sub><b>Shiv Lakshminarayan</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=shivlaks\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Ashivlaks\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3Ashivlaks\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/SomayaB\"><img src=\"https://avatars3.githubusercontent.com/u/23043132?v=4?s=100\" width=\"100px;\" alt=\"Somaya\"/><br /><sub><b>Somaya</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=SomayaB\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3ASomayaB+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3ASomayaB\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/aws/jsii-rosetta/pulls?q=is%3Apr+reviewed-by%3ASomayaB\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/skuenzli\"><img src=\"https://avatars.githubusercontent.com/u/869201?v=4?s=100\" width=\"100px;\" alt=\"Stephen Kuenzli\"/><br /><sub><b>Stephen Kuenzli</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=skuenzli\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/touchez-du-bois\"><img src=\"https://avatars.githubusercontent.com/u/434017?v=4?s=100\" width=\"100px;\" alt=\"Takahiro Sugiura\"/><br /><sub><b>Takahiro Sugiura</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=touchez-du-bois\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://gitter.im/\"><img src=\"https://avatars2.githubusercontent.com/u/8518239?v=4?s=100\" width=\"100px;\" alt=\"The Gitter Badger\"/><br /><sub><b>The Gitter Badger</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=gitter-badger\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Agitter-badger\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://medium.com/@thomaspoignant\"><img src=\"https://avatars2.githubusercontent.com/u/17908063?v=4?s=100\" width=\"100px;\" alt=\"Thomas Poignant\"/><br /><sub><b>Thomas Poignant</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Athomaspoignant+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ThomasSteinbach\"><img src=\"https://avatars0.githubusercontent.com/u/1683246?v=4?s=100\" width=\"100px;\" alt=\"Thomas Steinbach\"/><br /><sub><b>Thomas Steinbach</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AThomasSteinbach+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/hoegertn\"><img src=\"https://avatars2.githubusercontent.com/u/1287829?v=4?s=100\" width=\"100px;\" alt=\"Thorsten Hoeger\"/><br /><sub><b>Thorsten Hoeger</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=hoegertn\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/serverlessunicorn\"><img src=\"https://avatars1.githubusercontent.com/u/54867311?v=4?s=100\" width=\"100px;\" alt=\"Tim Wagner\"/><br /><sub><b>Tim Wagner</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aserverlessunicorn+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Aserverlessunicorn+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/tobli\"><img src=\"https://avatars3.githubusercontent.com/u/540266?v=4?s=100\" width=\"100px;\" alt=\"Tobias Lidskog\"/><br /><sub><b>Tobias Lidskog</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=tobli\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/TomBonnerAtDerivitec\"><img src=\"https://avatars.githubusercontent.com/u/83637254?v=4?s=100\" width=\"100px;\" alt=\"Tom Bonner\"/><br /><sub><b>Tom Bonner</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ATomBonnerAtDerivitec+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://ty.coghlan.dev/\"><img src=\"https://avatars2.githubusercontent.com/u/15920577?v=4?s=100\" width=\"100px;\" alt=\"Ty Coghlan\"/><br /><sub><b>Ty Coghlan</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AOphirr33+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/tvanhens\"><img src=\"https://avatars1.githubusercontent.com/u/5342795?v=4?s=100\" width=\"100px;\" alt=\"Tyler van Hensbergen\"/><br /><sub><b>Tyler van Hensbergen</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Atvanhens+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://ultidev.com/Products/\"><img src=\"https://avatars1.githubusercontent.com/u/757185?v=4?s=100\" width=\"100px;\" alt=\"Vlad Hrybok\"/><br /><sub><b>Vlad Hrybok</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Avgribok+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Lanayx\"><img src=\"https://avatars2.githubusercontent.com/u/3329606?v=4?s=100\" width=\"100px;\" alt=\"Vladimir Shchur\"/><br /><sub><b>Vladimir Shchur</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ALanayx+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Ragnoroct\"><img src=\"https://avatars.githubusercontent.com/u/19155205?v=4?s=100\" width=\"100px;\" alt=\"Will Bender\"/><br /><sub><b>Will Bender</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3ARagnoroct+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://yanex.org/\"><img src=\"https://avatars2.githubusercontent.com/u/95996?v=4?s=100\" width=\"100px;\" alt=\"Yan Zhulanow\"/><br /><sub><b>Yan Zhulanow</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=yanex\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yglcode\"><img src=\"https://avatars.githubusercontent.com/u/11893614?v=4?s=100\" width=\"100px;\" alt=\"Yigong Liu\"/><br /><sub><b>Yigong Liu</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Ayglcode+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/issues?q=author%3Ayglcode+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ZachBien\"><img src=\"https://avatars.githubusercontent.com/u/1245628?v=4?s=100\" width=\"100px;\" alt=\"Zach Bienenfeld\"/><br /><sub><b>Zach Bienenfeld</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3AZachBien+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ajnarang\"><img src=\"https://avatars3.githubusercontent.com/u/52025281?v=4?s=100\" width=\"100px;\" alt=\"ajnarang\"/><br /><sub><b>ajnarang</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aajnarang+label%3Afeature-request\" title=\"Feature requests\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/aniljava\"><img src=\"https://avatars.githubusercontent.com/u/412569?v=4?s=100\" width=\"100px;\" alt=\"aniljava\"/><br /><sub><b>aniljava</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=aniljava\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/arnogeurts-sqills\"><img src=\"https://avatars.githubusercontent.com/u/79304871?v=4?s=100\" width=\"100px;\" alt=\"arnogeurts-sqills\"/><br /><sub><b>arnogeurts-sqills</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aarnogeurts-sqills+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=arnogeurts-sqills\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/cn-cit\"><img src=\"https://avatars.githubusercontent.com/u/27255477?v=4?s=100\" width=\"100px;\" alt=\"cn-cit\"/><br /><sub><b>cn-cit</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Acn-cit+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/deccy-mcc\"><img src=\"https://avatars0.githubusercontent.com/u/45844893?v=4?s=100\" width=\"100px;\" alt=\"deccy-mcc\"/><br /><sub><b>deccy-mcc</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adeccy-mcc+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/apps/dependabot-preview\"><img src=\"https://avatars3.githubusercontent.com/in/2141?v=4?s=100\" width=\"100px;\" alt=\"dependabot-preview[bot]\"/><br /><sub><b>dependabot-preview[bot]</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adependabot-preview[bot]+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Adependabot-preview[bot]\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/apps/dependabot\"><img src=\"https://avatars0.githubusercontent.com/in/29110?v=4?s=100\" width=\"100px;\" alt=\"dependabot[bot]\"/><br /><sub><b>dependabot[bot]</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Adependabot[bot]\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dheffx\"><img src=\"https://avatars0.githubusercontent.com/u/22029918?v=4?s=100\" width=\"100px;\" alt=\"dheffx\"/><br /><sub><b>dheffx</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Adheffx+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/gregswdl\"><img src=\"https://avatars0.githubusercontent.com/u/47365273?v=4?s=100\" width=\"100px;\" alt=\"gregswdl\"/><br /><sub><b>gregswdl</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Agregswdl+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/guyroberts21\"><img src=\"https://avatars.githubusercontent.com/u/47118902?v=4?s=100\" width=\"100px;\" alt=\"guyroberts21\"/><br /><sub><b>guyroberts21</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=guyroberts21\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mattBrzezinski\"><img src=\"https://avatars.githubusercontent.com/u/4356074?v=4?s=100\" width=\"100px;\" alt=\"mattBrzezinski\"/><br /><sub><b>mattBrzezinski</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=mattBrzezinski\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mergify\"><img src=\"https://avatars.githubusercontent.com/u/18240476?v=4?s=100\" width=\"100px;\" alt=\"mergify\"/><br /><sub><b>mergify</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Amergify\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/apps/mergify\"><img src=\"https://avatars1.githubusercontent.com/in/10562?v=4?s=100\" width=\"100px;\" alt=\"mergify[bot]\"/><br /><sub><b>mergify[bot]</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Amergify[bot]\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nathannaveen\"><img src=\"https://avatars.githubusercontent.com/u/42319948?v=4?s=100\" width=\"100px;\" alt=\"nathannaveen\"/><br /><sub><b>nathannaveen</b></sub></a><br /><a href=\"https://github.com/aws/jsii/pulls?q=is%3Apr+author%3Anathannaveen\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/seiyashima\"><img src=\"https://avatars2.githubusercontent.com/u/4947101?v=4?s=100\" width=\"100px;\" alt=\"seiyashima42\"/><br /><sub><b>seiyashima42</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Aseiyashima+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=seiyashima\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/aws/jsii-rosetta/commits?author=seiyashima\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sullis\"><img src=\"https://avatars3.githubusercontent.com/u/30938?v=4?s=100\" width=\"100px;\" alt=\"sullis\"/><br /><sub><b>sullis</b></sub></a><br /><a href=\"https://github.com/aws/jsii-rosetta/commits?author=sullis\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/vaneek\"><img src=\"https://avatars1.githubusercontent.com/u/8113305?v=4?s=100\" width=\"100px;\" alt=\"vaneek\"/><br /><sub><b>vaneek</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Avaneek+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/wendysophie\"><img src=\"https://avatars.githubusercontent.com/u/54415551?v=4?s=100\" width=\"100px;\" alt=\"wendysophie\"/><br /><sub><b>wendysophie</b></sub></a><br /><a href=\"https://github.com/aws/jsii/issues?q=author%3Awendysophie+label%3Abug\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification.\nContributions of any kind welcome!\n\n## :balance_scale: License\n\n**jsii** is distributed under the [Apache License, Version 2.0][apache-2.0].\n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.\n\n[apache-2.0]: https://www.apache.org/licenses/LICENSE-2.0\n", "release_dates": ["2024-02-26T10:11:56Z", "2024-02-26T05:10:51Z", "2024-02-26T00:22:51Z", "2024-02-19T10:11:33Z", "2024-02-19T05:10:27Z", "2024-02-19T00:23:39Z", "2024-02-12T10:11:57Z", "2024-02-12T05:11:11Z", "2024-02-12T00:23:13Z", "2024-02-05T10:12:28Z", "2024-02-05T05:10:23Z", "2024-02-05T00:23:15Z", "2024-01-29T15:07:39Z", "2024-01-22T15:07:47Z", "2024-01-22T10:06:38Z", "2024-01-22T05:08:02Z", "2024-01-22T00:24:46Z", "2024-01-15T00:24:22Z", "2024-01-15T15:07:35Z", "2024-01-15T10:06:37Z", "2024-01-15T05:08:14Z", "2024-01-08T00:23:57Z", "2024-01-08T15:07:45Z", "2024-01-08T10:06:51Z", "2024-01-08T05:08:15Z", "2024-01-01T00:25:36Z", "2024-01-01T15:07:28Z", "2024-01-01T10:06:29Z", "2024-01-01T05:08:16Z", "2023-12-25T00:23:30Z"]}, {"name": "jsii-runtime-go", "description": "The jsii runtime for go", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ![jsii](https://github.com/aws/jsii/raw/main/logo/png/128.png)\n\n## The jsii runtime library for Go\n\nThis is the [jsii] runtime for go. This repository is used only for publishing\nthe go module. The source code is managed in [the main JSII repository][jsii].\nRefer to the [go-runtime readme there][readme] for details on building and\ntesting the module.\n\n[jsii]: https://github.com/aws/jsii\n[readme]: https://github.com/aws/jsii/blob/main/packages/%40jsii/go-runtime/README.md\n\n## :balance_scale: License\n\n**jsii** is distributed under the [Apache License, Version 2.0][apache-2.0].\n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.\n\n[apache-2.0]: https://www.apache.org/licenses/LICENSE-2.0\n", "release_dates": ["2021-02-24T14:42:08Z", "2021-02-23T08:35:28Z", "2021-02-11T16:09:33Z", "2021-02-03T15:54:52Z", "2021-02-03T08:13:45Z", "2021-01-31T09:52:47Z", "2021-01-21T05:08:14Z", "2020-12-01T18:25:14Z", "2020-12-01T18:23:04Z"]}, {"name": "karpenter-provider-aws", "description": "Karpenter is a Kubernetes Node Autoscaler built for flexibility, performance, and simplicity.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "[![CI](https://github.com/aws/karpenter-provider-aws/actions/workflows/ci.yaml/badge.svg?branch=main)](https://github.com/aws/karpenter/actions/workflows/ci.yaml)\n![GitHub stars](https://img.shields.io/github/stars/aws/karpenter-provider-aws)\n![GitHub forks](https://img.shields.io/github/forks/aws/karpenter-provider-aws)\n[![GitHub License](https://img.shields.io/badge/License-Apache%202.0-ff69b4.svg)](https://github.com/aws/karpenter-provider-aws/blob/main/LICENSE)\n[![Go Report Card](https://goreportcard.com/badge/github.com/aws/karpenter-provider-aws)](https://goreportcard.com/report/github.com/aws/karpenter)\n[![Coverage Status](https://coveralls.io/repos/github/aws/karpenter-provider-aws/badge.svg?branch=main)](https://coveralls.io/github/aws/karpenter?branch=main)\n[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/aws/karpenter-provider-aws/issues)\n\n![](website/static/banner.png)\n\nKarpenter is an open-source node provisioning project built for Kubernetes.\nKarpenter improves the efficiency and cost of running workloads on Kubernetes clusters by:\n\n* **Watching** for pods that the Kubernetes scheduler has marked as unschedulable\n* **Evaluating** scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods\n* **Provisioning** nodes that meet the requirements of the pods\n* **Removing** the nodes when the nodes are no longer needed\n\nCome discuss Karpenter in the [#karpenter](https://kubernetes.slack.com/archives/C02SFFZSA2K) channel, in the [Kubernetes slack](https://slack.k8s.io/) or join the [Karpenter working group](https://karpenter.sh/docs/contributing/working-group/) bi-weekly calls. If you want to contribute to the Karpenter project, please refer to the Karpenter docs.\n\nCheck out the [Docs](https://karpenter.sh/docs/) to learn more.\n\n## Talks\n- 09/08/2022 [Workload Consolidation with Karpenter](https://youtu.be/BnksdJ3oOEs)\n- 05/19/2022 [Scaling K8s Nodes Without Breaking the Bank or Your Sanity](https://www.youtube.com/watch?v=UBb8wbfSc34)\n- 03/25/2022 [Karpenter @ AWS Community Day 2022](https://youtu.be/sxDtmzbNHwE?t=3931)\n- 12/20/2021 [How To Auto-Scale Kubernetes Clusters With Karpenter](https://youtu.be/C-2v7HT-uSA)\n- 11/30/2021 [Karpenter vs Kubernetes Cluster Autoscaler](https://youtu.be/3QsVRHVdOnM)\n- 11/19/2021 [Karpenter @ Container Day](https://youtu.be/qxWJRUF6JJc)\n- 05/14/2021 [Groupless Autoscaling with Karpenter @ Kubecon](https://www.youtube.com/watch?v=43g8uPohTgc)\n- 05/04/2021 [Karpenter @ Container Day](https://youtu.be/MZ-4HzOC_ac?t=7137)\n", "release_dates": ["2024-02-29T20:42:01Z", "2024-02-20T01:07:08Z", "2024-02-09T17:53:54Z", "2024-02-08T23:58:48Z", "2024-02-06T19:33:50Z", "2024-01-30T23:25:23Z", "2024-01-26T18:30:59Z", "2024-01-11T20:00:24Z", "2023-12-19T21:16:42Z", "2023-12-19T19:32:18Z", "2023-12-05T19:00:44Z", "2023-12-01T21:08:14Z", "2023-11-29T21:11:17Z", "2023-11-22T19:35:28Z", "2023-11-10T23:21:26Z", "2023-10-31T18:35:37Z", "2023-10-31T00:34:43Z", "2023-10-31T00:16:20Z", "2023-10-12T00:02:56Z", "2023-09-27T21:14:28Z", "2023-08-31T19:06:15Z", "2023-08-11T01:29:11Z", "2023-07-19T20:47:06Z", "2023-07-17T22:00:21Z", "2023-07-06T18:57:01Z", "2023-06-22T20:30:58Z", "2023-06-14T21:54:33Z", "2023-06-12T20:11:26Z", "2023-05-30T22:20:22Z", "2023-05-18T22:50:54Z"]}, {"name": "libfabric", "description": "AWS Libfabric", "language": "C", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "[<img alt=\"libfabric master branch Travis CI status\" src=\"https://travis-ci.org/ofiwg/libfabric.svg?branch=master\"/>](https://travis-ci.org/ofiwg/libfabric)\n[<img alt=\"libfabric Coverity scan suild status\" src=\"https://scan.coverity.com/projects/4274/badge.svg\"/>](https://scan.coverity.com/projects/4274)\n[![libfabric release version](https://img.shields.io/github/release/ofiwg/libfabric.svg)](https://github.com/ofiwg/libfabric/releases/latest)\n\n# libfabric\n\nThe Open Fabrics Interfaces (OFI) is a framework focused on exporting fabric\ncommunication services to applications.\n\nSee [the OFI web site](http://libfabric.org) for more details, including a\ndescription and overview of the project, and detailed documentation of the\nLibfabric APIs.\n\n## Installing pre-built Libfabric packages\n\nOn OS X, the latest release of Libfabric can be installed using the\n[Homebrew](https://github.com/Homebrew/homebrew) package manager using the\nfollowing command:\n\n```\n$ brew install libfabric\n```\n\nLibfabric pre-built binaries may be available from other sources, such as Linux\ndistributions.\n\n## Building and installing Libfabric from source\n\nDistribution tarballs are available from the Github\n[releases](https://github.com/ofiwg/libfabric/releases) tab.\n\nIf you are building Libfabric from a developer Git clone, you must first run\nthe `autogen.sh` script. This will invoke the GNU Autotools to bootstrap\nLibfabric's configuration and build mechanisms. If you are building Libfabric\nfrom an official distribution tarball, there is no need to run `autogen.sh`;\nLibfabric distribution tarballs are already bootstrapped for you.\n\nLibfabric currently supports GNU/Linux, Free BSD, and OS X.\n\n### Configure options\n\nThe `configure` script has many built in options (see `./configure --help`).\nSome useful options are:\n\n```\n--prefix=<directory>\n```\n\nBy default `make install` will place the files in the `/usr` tree.\nThe `--prefix` option specifies that Libfabric files should be installed into\nthe tree specified by named `<directory>`. The executables will be located at\n`<directory>/bin`.\n\n```\n--with-valgrind=<directory>\n```\n\nDirectory where valgrind is installed. If valgrind is found, then valgrind\nannotations are enabled. This may incur a performance penalty.\n\n```\n--enable-debug\n```\n\nEnable debug code paths. This enables various extra checks and allows for using\nthe highest verbosity logging output that is normally compiled out in\nproduction builds.\n\n```\n--enable-<provider>=[yes|no|auto|dl|<directory>]\n--disable-<provider>\n```\n\nThis enables or disables the provider named `<provider>`. Valid options are:\n- auto (This is the default if the `--enable-<provider>` option isn't specified)\n\n  The provider will be enabled if all of its requirements are satisfied. If one\n  of the requirements cannot be satisfied, then the provider is disabled.\n- yes (This is the default if the `--enable-<provider>` option is specified)\n\n  The configure script will abort if the provider cannot be enabled (e.g., due\n  to some of its requirements not being available.\n- no\n\n  Disable the provider. This is synonymous with `--disable-<provider>`.\n- dl\n\n  Enable the provider and build it as a loadable library.\n- \\<directory\\>\n\n  Enable the provider and use the installation given in `<directory>`.\n\n### Examples\n\nConsider the following example:\n\n```\n$ ./configure --prefix=/opt/libfabric --disable-sockets && make -j 32 && sudo make install\n```\nThis will tell Libfabric to disable the `sockets` provider, and install\nLibfabric in the `/opt/libfabric` tree. All other providers will be enabled if\npossible and all debug features will be disabled.\n\nAlternatively:\n\n```\n$ ./configure --prefix=/opt/libfabric --enable-debug --enable-psm=dl && make -j 32 && sudo make install\n```\n\nThis will tell Libfabric to enable the `psm` provider as a loadable library,\nenable all debug code paths, and install Libfabric to the `/opt/libfabric`\ntree. All other providers will be enabled if possible.\n\n\n## Validate installation\n\nThe fi_info utility can be used to validate the libfabric and provider\ninstallation, as well as provide details about provider support and available\ninterfaces.  See `fi_info(1)` man page for details on using the fi_info\nutility.  fi_info is installed as part of the libfabric package.\n\nA more comprehensive test package is available via the fabtests package.\n\n\n## Providers\n\n### gni\n\n***\n\nThe `gni` provider runs on Cray XC (TM) systems utilizing the user-space\nGeneric Network Interface (`uGNI`) which provides low-level access to\nthe Aries interconnect.  The Aries interconnect is designed for\nlow-latency one-sided messaging and also includes direct hardware\nsupport for common atomic operations and optimized collectives.\n\nSee the `fi_gni(7)` man page for more details.\n\n#### Dependencies\n\n- The `gni` provider requires `gcc` version 4.9 or higher.\n\n### mxm\n\n***\n\nThe MXM provider has been deprecated and was removed after the 1.4.0 release.\n\n### psm\n\n***\n\nThe `psm` provider runs over the PSM 1.x interface that is currently supported\nby the Intel TrueScale Fabric. PSM provides tag-matching message queue\nfunctions that are optimized for MPI implementations.  PSM also has limited\nActive Message support, which is not officially published but is quite stable\nand well documented in the source code (part of the OFED release). The `psm`\nprovider makes use of both the tag-matching message queue functions and the\nActive Message functions to support a variety of Libfabric data transfer APIs,\nincluding tagged message queue, message queue, RMA, and atomic\noperations.\n\nThe `psm` provider can work with the `psm2-compat` library, which exposes\na PSM 1.x interface over the Intel Omni-Path Fabric.\n\nSee the `fi_psm(7)` man page for more details.\n\n### psm2\n\n***\n\nThe `psm2` provider runs over the PSM 2.x interface that is supported\nby the Intel Omni-Path Fabric. PSM 2.x has all the PSM 1.x features plus a set\nof new functions with enhanced capabilities. Since PSM 1.x and PSM 2.x are not\nABI compatible, the `psm2` provider only works with PSM 2.x and doesn't support\nIntel TrueScale Fabric.\n\nSee the `fi_psm2(7)` man page for more details.\n\n### rxm\n\nThe `ofi_rxm` provider is an utility provider that supports RDM endpoints emulated\nover MSG endpoints of a core provider.\n\nSee [`fi_rxm`(7)](https://ofiwg.github.io/libfabric/master/man/fi_rxm.7.html) for more information.\n\n### sockets\n\n***\n\nThe sockets provider has been deprecated in favor of the tcp, udp, and\nutility providers, which provide improved performance and stability.\n\nThe `sockets` provider is a general purpose provider that can be used on any\nsystem that supports TCP sockets.  The provider is not intended to provide\nperformance improvements over regular TCP sockets, but rather to allow\ndevelopers to write, test, and debug application code even on platforms\nthat do not have high-performance fabric hardware.  The sockets provider\nsupports all Libfabric provider requirements and interfaces.\n\nSee the `fi_sockets(7)` man page for more details.\n\n### tcp\n\n***\n\nThe tcp provider is an optimized socket based provider that supports\nreliable connected endpoints.  It is intended to be used directly by\napps that need MSG endpoint support, or in conjunction with the rxm\nprovider for apps that need RDM endpoints.  The tcp provider targets\nreplacing the sockets provider for applications using standard\nnetworking hardware.\n\nSee the `fi_tcp(7)` man page for more details.\n\n\n### udp\n\n***\n\nThe `udp` provider is a basic provider that can be used on any system that\nsupports UDP sockets.  The provider is not intended to provide performance\nimprovements over regular UDP sockets, but rather to allow application and\nprovider developers to write, test, and debug their code.  The `udp` provider\nforms the foundation of a utility provider that enables the implementation of\nLibfabric features over any hardware.\n\nSee the `fi_udp(7)` man page for more details.\n\n### usnic\n\n***\n\nThe `usnic` provider is designed to run over the Cisco VIC (virtualized NIC)\nhardware on Cisco UCS servers. It utilizes the Cisco usnic (userspace NIC)\ncapabilities of the VIC to enable ultra low latency and other offload\ncapabilities on Ethernet networks.\n\nSee the `fi_usnic(7)` man page for more details.\n\n#### Dependencies\n\n- The `usnic` provider depends on library files from either `libnl` version 1\n  (sometimes known as `libnl` or `libnl1`) or version 3 (sometimes known as\n  `libnl3`). If you are compiling Libfabric from source and want to enable\n  usNIC support, you will also need the matching `libnl` header files (e.g.,\n  if you are building with `libnl` version 3, you need both the header and\n  library files from version 3).\n\n#### Configure options\n\n```\n--with-libnl=<directory>\n```\n\nIf specified, look for libnl support. If it is not found then the `usnic`\nprovider will not be built. If `<directory>` is specified, then check in the\ndirectory and check for `libnl` version 3. If version 3 is not found, then\ncheck for version 1. If no `<directory>` argument is specified, then this\noption is redundant with `--with-usnic`.\n\n### verbs\n\n***\n\nThe verbs provider enables applications using OFI to be run over any verbs\nhardware (Infiniband, iWarp, etc). It uses the Linux Verbs API for network\ntransport and provides a translation of OFI calls to appropriate verbs API calls.\nIt uses librdmacm for communication management and libibverbs for other control\nand data transfer operations.\n\nSee the `fi_verbs(7)` man page for more details.\n\n#### Dependencies\n\n- The verbs provider requires libibverbs (v1.1.8 or newer) and librdmacm (v1.0.16\n  or newer). If you are compiling Libfabric from source and want to enable verbs\n  support, you will also need the matching header files for the above two libraries.\n  If the libraries and header files are not in default paths, specify them in CFLAGS,\n  LDFLAGS and LD_LIBRARY_PATH environment variables.\n\n### bgq\n\n***\n\nThe `bgq` provider is a native provider that directly utilizes the hardware\ninterfaces of the Blue Gene/Q system to implement aspects of the libfabric\ninterface to fully support MPICH3 CH4.\n\nSee the `fi_bgq(7)` man page for more details\n\n#### Dependencies\n\n- The `bgq` provider depends on the system programming interfaces (SPI) and\n  the hardware interfaces (HWI) located in the Blue Gene/Q driver installation.\n  Additionally, the open source Blue Gene/Q system files are required.\n\n#### Configure options\n\n```\n--with-bgq-progress=(auto|manual)\n```\n\nIf specified, set the progress mode enabled in FABRIC_DIRECT (default is FI_PROGRESS_MANUAL).\n\n```\n--with-bgq-mr=(basic|scalable)\n```\n\nIf specified, set the memory registration mode (default is FI_MR_BASIC).\n\n### Network Direct\n\n***\n\nThe Network Direct provider enables applications using OFI to be run over\nany verbs hardware (Infiniband, iWarp and etc). It uses the Microsoft Network\nDirect SPI for network transport and provides a translation of OFI calls to\nappropriate Network Direct API calls.\nThe Network Direct providers allows to OFI-based applications utilize\nzero-copy data transfers between applications, kernel-bypass I/O generation and\none-sided data transfer operations on Microsoft Windows OS.\nAn application is able to use OFI with Network Direct provider enabled on\nWindows OS to expose the capabilities of the networking devices if the hardware\nvendors of the devices implemented the Network Direct service provider interface\n(SPI) for their hardware.\n\nSee the `fi_netdir(7)` man page for more details.\n\n#### Dependencies\n\n- The Network Direct provider requires Network Direct SPI. If you are compiling\n  Libfabric from source and want to enable Network Direct support, you will also\n  need the matching header files for the Network Direct SPI.\n  If the libraries and header files are not in default paths (the default path is\n  root of provier directory, i.e. \\prov\\netdir\\NetDirect, where NetDirect contains\n  the header files), specify them in the configuration properties of the VS project.\n\n### mlx\n\n***\n\nThe MLX provider enables applications using OFI to be run over UCX\ncommunication library. It uses libucp for connections control and data transfer operations.\nSupported UCP API version: 1.2\n\nSee the `fi_mlx(7)` man page for more details.\n\n#### Dependencies\n\n- The MLX provider requires UCP API 1.2 capable libucp and libucs (tested with hpcx v1.8.0, v1.9.7).\n  If you are compiling Libfabric from source and want to enable MLX\n  support, you will also need the matching header files for UCX.\n  If the libraries and header files are not in default paths, specify them using:\n\n```\n--with-mlx=<path to local UCX installation>\n```\n\n### shm\n\n***\n\nThe shm provider enables applications using OFI to be run over shared memory.\n\nSee the `fi_shm(7)` man page for more details.\n\n#### Dependencies\n\n- The shared memory provider only works on Linux platforms and makes use of\n  kernel support for 'cross-memory attach' (CMA) data copies for large\n  transfers.\n\n### efa\n\n***\n\nThe `efa` provider enables the use of libfabric-enabled applications on [Amazon\nEC2 Elastic Fabric Adapter (EFA)](https://aws.amazon.com/hpc/efa/), a\ncustom-built OS bypass hardware interface for inter-instance communication on\nEC2.\n\nSee [`fi_efa`(7)](https://ofiwg.github.io/libfabric/master/man/fi_efa.7.html) for more information.\n\n## WINDOWS Instructions\n\nEven though windows isn't fully supported yet it is possible to compile and link your library.\n\n- 1. first you need the NetDirect provider:\n  Network Direct SDK/DDK may be obtained as a nuget package (preferred) from:\n\n  https://www.nuget.org/packages/NetworkDirect\n\n  or downloaded from:\n\n  https://www.microsoft.com/en-us/download/details.aspx?id=36043\n  on page press Download button and select NetworkDirect_DDK.zip.\n\n  Extract header files from downloaded\n  NetworkDirect_DDK.zip:`\\NetDirect\\include\\` file into `<libfabricroot>\\prov\\netdir\\NetDirect\\`,\n  or add path to NetDirect headers into VS include paths\n\n- 2. compiling:\n  libfabric has 6 Visual Studio solution configurations:\n\n      1-2: Debug/Release ICC (restricted support for Intel Compiler XE 15.0 only)\n      3-4: Debug/Release v140 (VS 2015 tool set)\n      5-6: Debug/Release v141 (VS 2017 tool set)\n\n  make sure you choose the correct target fitting your compiler.\n  By default the library will be compiled to `<libfabricroot>\\x64\\<yourconfigchoice>`\n\n- 3. linking your library\n  - right click your project and select properties.\n  - choose C/C++ > General and add `<libfabricroot>\\include` to \"Additional include Directories\"\n  - choose Linker > Input and add `<libfabricroot>\\x64\\<yourconfigchoice>\\libfabric.lib` to \"Additional Dependencies\"\n  - depending on what you are building you may also need to copy `libfabric.dll` into the targetfolder of your own project.\n", "release_dates": ["2023-12-15T22:31:53Z", "2023-11-11T01:53:14Z", "2023-10-02T16:58:59Z", "2023-09-11T21:59:41Z", "2023-08-31T18:48:59Z", "2023-08-31T18:46:01Z", "2023-05-03T01:34:40Z", "2022-12-08T20:05:01Z", "2022-12-05T20:01:20Z", "2022-11-08T22:18:07Z", "2022-08-22T14:54:34Z", "2022-07-08T00:43:13Z", "2022-07-03T15:43:26Z", "2022-07-01T16:10:24Z", "2022-06-14T19:42:31Z", "2022-03-23T14:42:42Z", "2021-10-26T21:15:55Z", "2021-08-18T18:42:09Z", "2021-05-07T22:19:12Z", "2021-04-26T22:10:30Z", "2020-12-04T19:26:26Z", "2020-06-09T18:22:12Z", "2023-10-12T18:06:17Z", "2020-06-03T03:55:13Z", "2020-05-07T16:54:36Z", "2020-01-14T00:38:44Z", "2020-01-06T23:00:05Z", "2019-12-06T01:23:23Z", "2019-11-11T21:53:14Z", "2019-10-23T20:12:49Z"]}, {"name": "lightsail-setup-instance-https", "description": "These scripts are run when Lightsail SetupInstanceHttps API is invoked.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Overview\n\nThese scripts are run when Lightsail [`SetupInstanceHttps`][api] API is invoked.\n\nThis feature is currently available for WordPress instances on Amazon Lightsail. [Learn more.][more]\n\n[api]: https://docs.aws.amazon.com/lightsail/2016-11-28/api-reference/API_SetupInstanceHttps.html\n[more]: https://lightsail.aws.amazon.com/ls/docs/en_us/articles/amazon-lightsail-tutorial-launching-and-configuring-wordpress\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "lightsailctl", "description": "Amazon Lightsail CLI Extensions", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Amazon Lightsail CLI Extensions\n\nThis project is the source code of `lightsailctl`, a tool that\naugments [Amazon Lightsail features in AWS CLI.][lscli]\n\n## Usage\n\n`lightsailctl` is executed **automatically** by AWS CLI when certain\nsubcommands are used, such as `aws lightsail push-container-image`.\n\n```sh\n$ lightsailctl --plugin -h\n\nUsage of `lightsailctl --plugin`:\n  --input payload\n        plugin payload\n  --input-stdin\n        receive plugin payload on stdin\n```\n\n## Installing\n\n### Homebrew \ud83c\udf7b\n\n```sh\nbrew install aws/tap/lightsailctl\n```\n\n### From Source\n\n`lightsailctl` is written in Go, so please [install Go.][getgo]\n\nIf all you want is to install `lightsailctl` binary, then do the following:\n\n```sh\ngo install github.com/aws/lightsailctl@latest\n```\n\n> **Note:** the executable is installed in the directory named by the `GOBIN`\n> environment variable, which defaults to `$GOPATH/bin` or `$HOME/go/bin` if\n> the `GOPATH` environment variable is not set (on Windows that is `%GOPATH%\\bin`\n> or `%USERPROFILE%\\go\\bin` respectively).\n\nKeep reading if you want to work with `lightsailctl` source code locally.\n\nAfter you clone this repo and open your terminal app in it, you'll be\nable to test and build this code like so:\n\n```sh\ngo test ./...\ngo install ./...\n```\n\n## Under The Hood\n\nLet's consider this command and see what actually happens:\n\n```sh\naws lightsail push-container-image \\\n --service-name hello \\\n --image hello-world:latest \\\n --label www\n```\n\nThe above command pushes a local container image with tag\n`hello-world:latest` to make it available in Lightsail container\nservice deployments for service `hello`.\n\nThis container image pushing logic requires a number of steps that are\noutsourced from AWS CLI to `lightsailctl`.\n\nHere's a shell invocation of `ligtsailctl` that approximates what AWS\nCLI does when the command above is invoked:\n\n```sh\n$ echo '{\n  \"inputVersion\": \"1\",\n  \"operation\": \"PushContainerImage\",\n  \"payload\": {\n    \"service\": \"hello\",\n    \"label\":   \"www\",\n    \"image\":   \"hello-world:latest\"\n  }\n}' | lightsailctl --plugin --input-stdin\n\n85fcec7ef3ef: Layer already exists \n3e5288f7a70f: Layer already exists \n56bc37de0858: Layer already exists \n1c91bf69a08b: Layer already exists \ncb42413394c4: Layer already exists \nDigest: sha256:0b159cd1ee1203dad901967ac55eee18c24da84ba3be384690304be93538bea8\nImage \"hello-world:latest\" registered.\nRefer to this image as \":hello.www.73\" in deployments.\n```\n\n## Security Disclosures\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md#security-issue-notifications) for\nmore information.\n\n## Giving Feedback and Contributing\n\nAside from the security feedback covered above, do you have any\nfeedback, bug reports, questions or feature ideas?\n\nYou are welcome to write up an [issue][issue] for us.\n\nPlease read about [Contributing Guidelines.](CONTRIBUTING.md)\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n[lscli]: https://docs.aws.amazon.com/cli/latest/reference/lightsail/index.html\n[getgo]: https://golang.org/doc/install\n[issue]: https://github.com/aws/lightsailctl/issues/new\n", "release_dates": ["2023-07-12T20:57:49Z", "2023-02-23T22:21:07Z", "2022-07-06T18:56:45Z", "2021-10-08T22:24:30Z", "2021-09-28T00:31:38Z", "2021-02-05T00:35:35Z"]}, {"name": "lumberyard", "description": "Amazon Lumberyard is a free AAA game engine deeply integrated with AWS and Twitch \u2013 with full source.", "language": "C++", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "![lmbr](http://d2tinsms4add52.cloudfront.net/github/readme_header.jpg)   \n\n------------\n\n**Amazon Lumberyard is no longer offered. This archive provides source for convenience only; you can also view the For continued support, visit our [documentation archive on GitHub](https://github.com/awsdocs/amazon-lumberyard-user-guide/) or [contact Lumberyard customer support ](https://pages.awscloud.com/Amazon-Game-Tech-Contact-Us.html). We recommend Open 3D Engine, Lumberyard\u2019s Apache-licensed successor. O3DE is maintained by the open-source community, including developers from Adobe, AWS, Epic, Microsoft, Intel, Lightspeed Studios, Niantic, and others. Visit O3DE's [website](https://www.o3de.org/ \"website\"), [Discord](https://discord.gg/o3de), and [Github](https://github.com/o3de/o3de).**\n\n------------\n \n# Amazon Lumberyard\nAmazon Lumberyard is a free, AAA game engine that gives you the tools you need to create high quality games.\n\n## Acquiring Lumberyard source \nEach release of Lumberyard exists as a separate branch in GitHub. You can get Lumberyard from GitHub using the following steps:\n\n### Fork the repository\nForking creates a copy of the Lumberyard repository in your GitHub account. Your fork becomes the remote repository into which you can push changes.\n\n### Create a branch\nThe GitHub workflow assumes your master branch is always deployable. Create a branch for your local project or fixes.\n\nFor more information about branching, see the [GitHub documentation](https://guides.github.com/introduction/flow/).\n\n### Clone the repository\nCloning the repository copies your fork onto your computer. To clone the repository, click the \"Clone or download\" button on the GitHub website, and copy the resultant URL to the clipboard. In a command line window, type ```git clone [URL]```, where ```[URL]``` is the URL that you copied in the previous step.\n\nFor more information about cloning a repository, see the [GitHub documentation](https:// help.github.com/articles/cloning-a-repository/).\n\n\n### Downloading additive  files\nOnce the repository exists locally on your machine, manually execute ```git_bootstrap.exe``` found at the root of the repository. This application will perform a download operation for __Lumberyard binaries that are required prior to using or building the engine__. This program uses AWS services to download the binaries. Monitor the health of AWS services on the [AWS Service Health Dashboard](https://status.aws.amazon.com/).\n\n### Running the Setup Assistant\n```git_bootstrap.exe``` will launch the Setup Assistant when it completes. Setup Assistant lets you configure your environment and launch the Lumberyard Editor.\n\n## Lumberyard Documentation\nFull Lumberyard documentation can be found here:\nhttps://github.com/awsdocs/amazon-lumberyard-user-guide/ \n<br>We also have tutorials available at https://www.youtube.com/c/AmazonLumberyardTutorials/\n\n## License\nYour use of Lumberyard is governed by the AWS Customer Agreement at https://aws.amazon.com/agreement/ and Lumberyard Service Terms at https://aws.amazon.com/serviceterms/#57._Amazon_Lumberyard_Engine.\n\nFor complete copyright and license terms please see the LICENSE.txt file at the root of this distribution (the \"License\").  As a reminder, here are some key pieces to keep in mind when submitting changes/fixes and creating your own forks:\n-\tIf you submit a change/fix, we can use it without restriction, and other Lumberyard users can use it under the License.\n-\tOnly share forks in this GitHub repo (i.e., forks must be parented to https://github.com/aws/lumberyard).\n-\tYour forks are governed by the License, and you must include the License.txt file with your fork.  Please also add a note at the top explaining your modifications.\n-\tIf you use someone else\u2019s fork from this repo, your use is subject to the License.    \n-\tYour fork may not enable the use of third-party compute, storage or database services.  \n-\tIt's fine to connect to third-party platform services like Steamworks, Apple GameCenter, console platform services, etc.  \nTo learn more, please see our FAQs https://aws.amazon.com/lumberyard/faq/#licensing.\n", "release_dates": []}, {"name": "mit-0", "description": "The \"MIT No Attribution\" (MIT-0) License", "language": null, "license": null, "readme": "# The \"MIT No Attribution\" (MIT-0) License\n\nThe \"MIT No Attribution\" or \"MIT-0\" license is a modification to the\nusual MIT license that removes the requirement for attribution. It is\nroughly the same as an MIT license consisting of a sandwich of\n(paragraph, line, paragraph), but it is missing the middle line of the\n'sandwich'.\n\n## Rationale\n\nThis license has proven useful for code that is intended for\ndevelopers to use as reference, teaching samples, examples, or\ntemplates that other developers may modify for their own purposes.\n\nIn many of these cases, the initial developer may not want to impose\neven the cost of attribution, or the use cases may not be conducive to\nattribution.\n\nThe CC0 and various \"do what you want\" licenses and various public\ndomain dedications may be less attractive to the initial developer for\nvarious reasons (i.e., a license is preferable to a public domain\ndedication).  The MIT license with all the attribution requirement\nlanguage removed fills this need.\n\n## License Text\n\n\n```\nMIT No Attribution\n\nCopyright <YEAR> <COPYRIGHT HOLDER>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this\nsoftware and associated documentation files (the \"Software\"), to deal in the Software\nwithout restriction, including without limitation the rights to use, copy, modify,\nmerge, publish, distribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\nINCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\nPARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n```\n\n## See also\n\nWhile researching prior to publishing this repo, we discovered\n<https://romanrm.net/mit-zero>, which is an independent prior\nformulation and publication of the same idea.\n", "release_dates": []}, {"name": "mynah-ui", "description": null, "language": "SCSS", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Mynah UI\n### *A Data & Event Drivent Chat Interface Library for Browsers and Webviews*\n\nAs you can understand from the title too, **Mynah UI** is a **_data and event_** driven chat interface for browsers and webviews on IDEs or any platform which supports latest web technologies which is also used by Amazon Q for [VSCode](https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.aws-toolkit-vscode) and [JetBrains](https://plugins.jetbrains.com/plugin/11349-aws-toolkit--amazon-q-codewhisperer-and-more) comes together with AWS Toolkit extension.\n\nSince it is not depending on any framework or a UI library, you can use it inside any web based project without a hassle. Reason behind that decision is to keep it highly configurable for theming to support various use cases. It works standalone, you just need to tell it where to render on the DOMTree.\n\nPlease continue with one of the following guides:\n\n## How to setup, configure and use Mynah UI\n### [Startup Guide](./docs/STARTUP.md)\n### [Constructor Properties](./docs/PROPERTIES.md)\n### [Configuration](./docs/CONFIG.md)\n### [How to use MynahUI](./docs/USAGE.md)\n### [Data Model](./docs/DATAMODEL.md)\n### [Styling Configuration](./docs/STYLING.md)\n\n<p align=\"center\">\n  <img src=\"./docs/img/splash.gif\" alt=\"MynahUI\" style=\"max-width:1024px; width:100%;border: 1px solid #e0e0e0;\">\n</p>\n\n## Supported browsers\n**Mynah UI** <em>-because of it's extensive css structure-</em> only supports ever-green browsers including webkit based WebUI renderers.\n\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2024-02-27T16:05:17Z", "2024-02-27T09:02:19Z", "2024-02-26T10:29:02Z", "2024-02-21T12:05:41Z", "2024-02-14T14:30:44Z", "2024-02-13T11:21:22Z", "2024-02-06T12:45:05Z", "2024-01-31T15:35:12Z", "2024-01-18T14:09:17Z", "2024-01-10T11:40:13Z", "2024-01-02T13:29:39Z", "2023-12-19T13:20:32Z", "2023-12-13T13:57:47Z", "2023-11-30T20:18:34Z", "2023-11-28T12:39:05Z", "2023-11-28T11:01:49Z", "2023-11-27T23:00:07Z", "2023-11-27T22:41:26Z", "2023-11-23T16:46:40Z", "2023-11-23T11:37:36Z", "2023-11-23T00:15:26Z", "2023-11-22T17:13:46Z", "2023-11-22T14:13:34Z", "2023-11-22T12:55:02Z", "2023-11-22T12:09:57Z", "2023-11-21T13:51:55Z", "2023-11-20T17:46:09Z", "2023-11-18T00:13:43Z", "2023-11-17T16:27:49Z", "2023-11-16T23:17:32Z"]}, {"name": "neptune-export", "description": null, "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Neptune Export\n\n[![CI Status](https://github.com/aws/neptune-export/actions/workflows/maven_build_and_verify.yml/badge.svg)](https://github.com/aws/neptune-export/actions/workflows/maven_build_and_verify.yml)\n[![Code Coverage](https://codecov.io/gh/aws/neptune-export/branch/develop/graph/badge.svg)](https://codecov.io/gh/aws/neptune-export)\n\nExports Amazon Neptune property graph data to CSV or JSON, or RDF graph data to Turtle.\n\n## Usage\n\n  - [`export-pg`](docs/export-pg.md)\n  - [`create-pg-config`](docs/create-pg-config.md)\n  - [`export-pg-from-config`](docs/export-pg-from-config.md)\n  - [`export-pg-from-queries`](docs/export-pg-from-queries.md)\n  - [`export-rdf`](docs/export-rdf.md)\n\n### Topics\n\n  - [Neptune-Export service](#neptune-export-service)\n  - [Best practices](#best-practices)\n  - [Exporting to the Bulk Loader CSV Format](#exporting-to-the-bulk-loader-csv-format)\n  - [Exporting the Results of User-Supplied Queries](#exporting-the-results-of-user-supplied-queries)\n  - [Exporting an RDF Graph](#exporting-an-rdf-graph)\n  - [Exporting to an Amazon Kinesis Data Stream](#exporting-to-an-amazon-kinesis-data-stream)\n  - [Building neptune-export](#building-neptune-export)\n  - [Security](#security)\n  - [Deploying neptune-export as an AWS Lambda Function](#deploying-neptune-export-as-an-aws-lambda-function)\n\n## Neptune-Export service\n\nYou can now deploy _neptune-export_ as a service inside your Neptune VPC. Use [these CloudFormation templates](https://docs.aws.amazon.com/neptune/latest/userguide/export-service.html) to install the Neptune-Export service.\n\n## Standalone usage\n\n_neptune-export_ is also runnable as an adhoc command. See [prerequisites](https://docs.aws.amazon.com/neptune/latest/userguide/export-utility.html).\n\n## Best practices\n\n### Export from a cloned cluster\n\n_neptune-export_ cannot guarantee the consistency of exported data if you export from a Neptune cluster whose data is changing while the export is taking place. Therefore, we recommend exporting from a clone of your cluster. This ensures the export takes place against a static version of the data at the point in time the database was cloned. Further, exporting from a clone ensures the export doesn\u2019t impact the query performance of the original cluster.\n\n_neptune-export_ makes it easy to export from a clone. Simply supply a `--clone-cluster` option with the command. You can also use the `--clone-cluster-replica-count` option to specify the number of read replicas to be added to the cloned cluster, and the `--clone-cluster-instance-type` parameter to tell _neptune-export_ which instance type \u2013 e.g. `db.r5.2xlarge` \u2013  to use for each instance in the cloned cluster (by default, _neptune-export_ will use the same instance type as the primary in the original cluster.)\n\nIf you clone your cluster using the `--clone-cluster` option, _neptune-export_ will ignore any `--concurrency` option supplied in the params, and will instead work out a concurrency setting based on the number of instances in the cloned cluster and their instance types.\n\nIf you use the cluster cloning features of _neptune-export_, you must ensure the AWS Identity and Access Management identity with which the process runs can perform the following actions:\n\n  - rds:AddTagsToResource\n  - rds:DescribeDBClusters\n  - rds:DescribeDBInstances\n  - rds:ListTagsForResource\n  - rds:DescribeDBClusterParameters\n  - rds:DescribeDBParameters\n  - rds:ModifyDBParameterGroup\n  - rds:ModifyDBClusterParameterGroup\n  - rds:RestoreDBClusterToPointInTime\n  - rds:DeleteDBInstance\n  - rds:DeleteDBClusterParameterGroup\n  - rds:DeleteDBParameterGroup\n  - rds:DeleteDBCluster\n  - rds:CreateDBInstance\n  - rds:CreateDBClusterParameterGroup\n  - rds:CreateDBParameterGroup\n\n\n### Use a config file\n\nUse the `export-pg-from-config` command in preference to `export-pg` when exporting property graphs from Neptune. The `export-pg` command makes two passes over your data: the first to generate metadata, the second to create the data files. This first pass takes place on a single thread, and for very large datasets can take many hours \u2013 often much longer than the export itself.\n\nThe preferred approach is to generate the metadata once using `create-pg-config`, store the config file in S3, and then refer to it from `export-pg-from-config` using the `--config-file` option.\n\n### Supply approximate node and edge counts\n\nWhen performing a parallel export (`--concurrency` is larger than one), _neptune-export_ must first query your database to determine the number of nodes and edges to be exported. These numbers are then used to calculate ranges for each query in a set of parallel queries. Counting the nodes and edges in a large dataset can take many minutes.\n\n_neptune-export_ now includes `--approx-node-count` and `--approx-edge-count` options that allow you to supply estimates for the number of nodes and edges you expect to export. By specifying approximate counts you can reduce the export time, because _neptune-export_ will no longer have to query the database to count the nodes and edges.\n\nThe numbers you supply need only be approximate \u2013 it doesn\u2019t matter if you\u2019re within ten percent of the real counts. One way of calculating these numbers is to use the counts from a previous export, adjusted based on the approximate number of additions and deletions that have taken place in the interim. \n\n## Exporting to the Bulk Loader CSV Format\n\nWhen exporting to the [CSV format](https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-format-gremlin.html) used by the [Amazon Neptune bulk loader](https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html), _neptune-export_ generates CSV files based on a schema derived from scanning your graph. This schema is persisted in a JSON file. There are three ways in which you can use the tool to generate bulk load files:\n\n - [`export-pg`](docs/export-pg.md) \u2013 This command makes two passes over your data: the first to generate the schema, the second to create the data files. By scanning all nodes and edges in the first pass, the tool captures the superset of properties for each label, identifies the datatype for each property, and identifies any properties for which at least one vertex or edge has multiple values. If exporting to CSV, these latter properties are exported to CSV as array types. If exporting to JSON, these property values are exported as array nodes.\n - [`create-pg-config`](docs/create-pg-config.md) \u2013 This command makes a single pass over your data to generate the schema config file.\n - [`export-pg-from-config`](docs/export-pg-from-config.md) \u2013 This command makes a single pass over your data to create the CSV or JSON files. It uses a preexisting schema config file.\n \n### Generating schema\n\n[`export-pg`](docs/export-pg.md) and [`create-pg-config`](docs/create-pg-config.md) both generate schema JSON files describing the properties associated with each node and edge label. By default, these commands will scan the entire database. For large datasets, this can take a long time. \n\nBoth commands also allow you to sample a range of nodes and edges in order to create this schema. If you are confident that sampling your data will yield the same schema as scanning the entire dataset, specify the `--sample` option with these commands. If, however, you have reason to believe the same property on different nodes or edges could yield different datatypes, or different cardinalities, or that nodes or edges with the same labels could contain different sets of properties, you should consider retaining the default behaviour of a full scan.\n\nOnce you have generated a schema file, either with `export-pg` or `create-pg-config`, you can reuse it for subsequent exports in `export-pg-from-config`. You can also modify the file to restrict the labels and properties that will be exported.\n\n### Label filters\n\nAll three commands allow you to supply vertex and edge label filters. \n\n - If you supply label filters to the [`export-pg`](docs/export-pg.md) command, the schema file and the exported data files will contain data only for the labels specified in the filters.\n - If you supply label filters to the [`create-pg-config`](docs/create-pg-config.md) command, the schema file will contain data only for the labels specified in the filters.\n - If you supply label filters to the [`export-pg-from-config`](docs/export-pg-from-config.md) command, the exported data files will contain data for the intersection of labels in the config file and the labels specified in the command filters.\n \n### Token-only export\n\nFor some offline use cases you may want to export only the structural data in the graph: that is, just the labels and IDs of vertices and edges. `export-pg` allows you to specify a `--tokens-only` option with the value `nodes`, `edges` or `both`. A token-only export does not generate a schema, nor does it export any property data: for vertices it simply exports `~id` and `~label`; for edges, it exports `~id`, `~from`, `~to` and `~label`. You can still use label filters to determine exactly which vertices and edges will be exported.\n \n### Parallel export\n\nThe [`export-pg`](docs/export-pg.md) and [`export-pg-from-config`](docs/export-pg-from-config.md) commands support parallel export. You can supply a concurrency level, which determines the number of client threads used to perform the parallel export, and, optionally, a range or batch size, which determines how many nodes or edges will be queried by each thread at a time. If you specify a concurrency level, but don't supply a range, the tool will calculate a range such that each thread queries _(1/concurrency level) * number of nodes/edges_ nodes or edges.\n\nIf using parallel export, we recommend setting the concurrency level to the number of vCPUs on your Neptune instance.\n\nYou can load balance requests across multiple instances in your cluster (or even multiple clusters) by supplying multiple `--endpoint` options.\n\n### Long-running queries\n\n_neptune-export_ uses long-running queries to generate the schema and the data files. You may need to increase the `neptune_query_timeout` [DB parameter](https://docs.aws.amazon.com/neptune/latest/userguide/parameters.html) in order to run the tool against large datasets.\n\nFor large datasets, we recommend running this tool against a standalone database instance that has been restored from a snapshot of your database.\n\n### Serializer\n\nThe latest version of _neptune-export_ uses the [GraphBinary](http://tinkerpop.apache.org/docs/3.4.0/upgrade/#_graphbinary) serialization format introduced in Gremlin 3.4.x. Previous versions of _neptune-export_ used Gryo. To revert to using Gryo, supply `--serializer GRYO_V3D0`.\n\n### Character Encoding\n\n_neptune-export_ attempts to use the JVM system default text encoding for all output files. This can be configured manually if needed by setting the `file.encoding` system property.\n\n```java -Dfile.encoding=UTF8 -jar neptune-export.jar ...```\n\n## Exporting the Results of User-Supplied Queries\n\n_neptune-export_'s [`export-pg-from-queries`](docs/export-pg-from-queries.md) command allows you to supply groups of Gremlin queries and export the results to CSV or JSON.\n\nEvery user-supplied query should return a resultset whose every result comprises a Map. Typically, these are queries that return a `valueMap()` or a projection created using `project().by().by()...`.\n\nQueries are grouped into _named groups_. All the queries in a named group should return the same columns. Named groups allow you to 'shard' large queries and execute them in parallel (using the `--concurrency` option). **Note that query sharding is not done automatically, so if you just supply one query, you will get no benefit from increasing the concurrency level past one.** The resulting CSV or JSON files will be written to a directory named after the group.\n\nIf there is a possibility that individual rows in a query's resultset will contain different keys, use the `--two-pass-analysis` flag to force _neptune-export_ to determine the superset of keys or column headers for the query.\n\nYou can supply multiple named groups using multiple `--queries` options. Each group comprises a name, an equals sign, and then a semi-colon-delimited list of Gremlin queries. Surround the list of queries in double quotes. For example:\n\n`-q person=\"g.V().hasLabel('Person').range(0,100000).valueMap();g.V().hasLabel('Person').range(100000,-1).valueMap()\"`\n\nSharding queries for concurrent execution can create a large number of queries, especially with a high concurrency level. In order to avoid inputting all of these queries as command-line arguments, you can also supply them in a JSON file with the `--queriesFile` option. The JSON file should be formatted like this:\n\n```json\n[\n  {\n    \"name\": \"NamedQueryGroup\",\n    \"queries\": [\"list\", \"of\", \"sharded\", \"queries\", \"in\", \"group\"]\n  },\n  ...\n]\n```\n\nThis file can be given as a local path, or over https or s3.\n\n### Split Queries\n\nThe `--split-queries` option may be used to automatically shard queries. When invoked, the tool will calculate ranges in the same manner as the `export-pg` command's [parallel export](#parallel-export), and then split each query into `--concurrency` number of shards.\n\nThe sharded queries use injected `range()` steps at the beginning of the query to divide the ranges. For example, `g.V().hasLabel(\"person\")` may be sharded as:\n```groovy\ng.V().range(0, 250).hasLabel(\"person\")\ng.V().range(250, 500).hasLabel(\"person\")\ng.V().range(500, 750).hasLabel(\"person\")\ng.V().range(750, -1).hasLabel(\"person\")\n```\n\nThis `range()`-based sharding may not be uniformly balanced, and may lead produce different results with certain queries. Any gremlin steps which operate on the entire input stream at once (such as `order()`, `dedup()`, and `group()`) should be used with caution as this sharding inevitably alters their inputs.\n\nFor any queries which are incompatible with `range()`-based sharding, or in situations where more precise balancing is required, it is recommended to avoid using `--split-queries` and instead provide a `--queriesFile` with pre-sharded queries.\n\n### Parallel execution of queries\n\nIf using parallel export, we recommend setting the concurrency level to the number of vCPUs on your Neptune instance. When _neptune-export_ executes named groups of queries in parallel, it simply flattens all the queries into a queue, and spins up a pool of worker threads according to the concurrency level you have specified using `--concurrency`. Worker threads continue to take queries from the queue until the queue is exhausted.\n\n### Batching\n\nQueries whose results contain very large rows can sometimes trigger a `CorruptedFrameException`. If this happens, you can either adjust the batch size (`--batch-size`) to reduce the number of results returned to the client in a batch (the default is 64), or increase the frame size (`--max-content-length`, default value 65536).\n\n## Exporting an RDF Graph\n              \nAt present _neptune-export_ supports exporting an RDF dataset to Turtle, NQuads, and NTriples with a single-threaded long-running query.\n\n### Exporting Named Graphs\n\nThe default scope for `export-rdf` is to export the entire dataset (union of all named graphs). Use the `--named-graph <NamedGraphURI>` argument to limit the scope to a single named graph. This can only be used with the default `graph` scope (`--rdf-export-scope graph`).\n\n### Exporting from User-Supplied SPARQL query\n\nTo export the results from a SPARQL query, use the `--rdf-export-scope query` and `--sparql <SPARQL Query>` arguments.\n\n## Security\n  \n### Encryption in transit\n\nBy default, _neptune-export_ connects to your database using SSL. If your target does not support SSL connections, use the `--disable-ssl` flag.\n\n(SSL used to be an opt-in feature for _neptune-export_, with a `--use-ssl` option for turning SSL on. This behaviour has now changed: SSL is on by default, but can be turned off using `--disable-ssl`. The `--use-ssl` option now no longer has any effect.)\n\nIf you are using a load balancer or a proxy server (such as HAProxy), you must [use SSL termination and have your own SSL certificate on the proxy server](https://docs.aws.amazon.com/neptune/latest/userguide/security-ssl.html).\n\n### IAM DB authentication\n\n_neptune-export_ supports exporting from databases that have [IAM database authentication](https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth.html) enabled. Supply the `--use-iam-auth` option with each command. Remember to set the **SERVICE_REGION** environment variable \u2013 e.g. `export SERVICE_REGION=us-east-1`.\n\n_neptune-export_ also supports connecting through a load balancer to a Neptune database with IAM DB authetication enabled. However, this feature is only currently supported for property graphs, with support for RDF graphs coming soon.\n\nIf you are connecting through a load balancer, and have IAM DB authentication enabled, you must also supply either an `--nlb-endpoint` option (if using a network load balancer) or an `--alb-endpoint` option (if using an application load balancer), and an `--lb-port`.\n\nFor details on using a load balancer with a database with IAM DB authentication enabled, see [Connecting to Amazon Neptune from Clients Outside the Neptune VPC](https://github.com/aws-samples/aws-dbs-refarch-graph/tree/master/src/connecting-using-a-load-balancer). \n\n## Exporting to an Amazon Kinesis Data Stream\n\nWhen exporting to an Amazon Kinesis Data Stream, records are [aggregated](https://docs.aws.amazon.com/streams/latest/dev/kinesis-kpl-concepts.html#kinesis-kpl-concepts-aggretation) by default \u2013 that is, multiple exported records are packed into a single Kinesis Data Streams record. In your stream client your will need to [deaggregate](https://docs.aws.amazon.com/streams/latest/dev/kinesis-kpl-consumer-deaggregation.html) the records. If you are using a Python client, you can use the record deaggregation module from the [Kinesis Aggregation/Deaggregation Modules for Python](https://pypi.org/project/aws-kinesis-agg/).\n\nYou can turn off stream record aggregation when you export to a Kinesis Data Stream using the `--disable-stream-aggregation` option.\n   \n## Building neptune-export\n\nTo build the jar, run:\n\n`mvn clean install`\n\n## Deploying neptune-export as an AWS Lambda Function\n\nThe _neptune-export_ jar can be deployed as an AWS Lambda function. To access Neptune, you will either have to [configure the function to access resources inside your VPC](https://docs.aws.amazon.com/lambda/latest/dg/vpc.html), or [expose the Neptune endpoints via a load balancer](https://github.com/aws-samples/aws-dbs-refarch-graph/tree/master/src/connecting-using-a-load-balancer).\n\nBe mindful of the [AWS Lambda limits](https://docs.aws.amazon.com/lambda/latest/dg/limits.html), particularly with regard to function timeouts (max 15 minutes) and _/tmp_ directory storage (512 MB). Large exports can easily exceed these limits.\n\nWhen deployed as a Lambda function, _neptune-export_ will automatically copy the export files to an S3 bucket of your choosing. Optionally, it can also write a completion file to a separate S3 location (useful for triggering additional Lambda functions). You must configure your function with an IAM role that has write access to these S3 locations.\n\nThe Lambda function expects a number of parameters, which you can supply either as [environment variables](https://docs.aws.amazon.com/lambda/latest/dg/env_variables.html) or via a JSON input parameter. Fields in the JSON input parameter override any environment variables you have set up.\n\n| Environment Variable | JSON Field | Description ||\n| ---- | ---- | ---- | ---- |\n| `COMMAND` | `command` | _neptune-export_ command and command-line options: e.g. `export-pg -e <neptune_endpoint>` | Mandatory |\n| `OUTPUT_S3_PATH` | `outputS3Path` | S3 location to which exported files will be written | Mandatory |\n| `CONFIG_FILE_S3_PATH` | `configFileS3Path` | S3 location of a JSON config file to be used when exporting a property graph from a config file | Optional |\n| `COMPLETION_FILE_S3_PATH` | `completionFileS3Path` | S3 location to which a completion file should be written once all export files have been copied to S3 | Optional |\n| `SSE_KMS_KEY_ID` | `sseKmsKeyId` | ID of the customer managed AWS-KMS symmetric encryption key to used for server-side encryption when exporting to S3 | Optional |\n\n## Samples\n\n### [AWS CDK Wrapper for Machine Learning](https://github.com/aws-samples/aws-cdk-wrapper-for-amazon-neptune-export-for-fake-news-detection)\n\nA CDK Wrapper around Neptune Export and Neptune ML CloudFormation stacks to run [fake news detection jobs](https://github.com/aws-samples/amazon-neptune-ml-fake-news-detection).\n\n\n", "release_dates": ["2024-01-15T18:53:28Z", "2023-11-30T20:40:47Z", "2023-11-15T17:27:35Z", "2023-11-03T22:00:50Z", "2023-11-01T00:34:52Z", "2023-09-28T02:12:05Z", "2023-08-01T00:43:27Z", "2023-06-05T20:53:41Z", "2023-05-31T22:09:31Z", "2023-04-28T01:35:21Z", "2023-04-08T02:47:09Z", "2023-03-30T22:38:19Z", "2023-03-01T04:35:51Z"]}, {"name": "neptune-gremlin-client", "description": "A Java Gremlin client for Amazon Neptune that allows you to change the endpoints used by the client as it is running.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Neptune Gremlin Client\n\nA Java Gremlin client for Amazon Neptune that allows you to change the endpoints used by the client as it is running. Includes an endpoint refresh agent that can get cluster topology details, and update the client on a periodic basis. You can supply your own custom endpoint selectors to configure the client for a subset of instances in your cluster based on tags, instance types, instance IDs, Availability Zones, etc.\n\nThe client also provides support for connecting to Neptune via a proxy such as a network or application load balancer, as an alternative to using an endpoint refresh agent and custom endpoint selectors.\n\nSee [Migrating from version 1 of the Neptune Gremlin Client](#migrating-from-version-1-of-the-neptune-gremlin-client) if you are migrating an application from version 1.x.x of the Neptune Gremlin Client.\n\n## Example\n\nThe following example shows how to build a `GremlinClient` that connects to and round-robins requests across all available Neptune serverless instances that have been tagged \"analytics\". The list of endpoints that match this selection criteria is refreshed every 60 seconds. The refresh agent that updates the list of endpoints uses an AWS Lambda proxy function to retrieve details of the Neptune database's cluster topology.\n\n```\nEndpointsSelector selector = (cluster) ->\n        new EndpointCollection(\n                cluster.getInstances().stream()\n                        .filter(i -> i.hasTag(\"workload\", \"analytics\"))\n                        .filter(i -> i.getInstanceType().equals(\"db.serverless\"))\n                        .filter(NeptuneInstanceMetadata::isAvailable)\n                        .collect(Collectors.toList()));\n\nClusterEndpointsRefreshAgent refreshAgent = \n        ClusterEndpointsRefreshAgent.lambdaProxy(\"neptune-endpoints-info-lambda\");\n\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .enableIamAuth(true)\n        .addContactPoints(refreshAgent.getEndpoints(selector))\n        .create();\n\nGremlinClient client = cluster.connect();\n\nrefreshAgent.startPollingNeptuneAPI(\n        RefreshTask.refresh(client, selector),\n        60,\n        TimeUnit.SECONDS);\n\nDriverRemoteConnection connection = DriverRemoteConnection.using(client);\nGraphTraversalSource g = AnonymousTraversalSource.traversal().withRemote(connection);\n\nfor (int i = 0; i < 100; i++) {\n    List<Map<Object, Object>> results = g.V().limit(10).valueMap(true).toList();\n    for (Map<Object, Object> result : results) {\n        //Do nothing\n    }\n}\n\nrefreshAgent.close();\nclient.close();\ncluster.close();\n```\n\n## Dependencies\n\n### Maven\n\n```\n<dependency>\n    <groupId>software.amazon.neptune</groupId>\n    <artifactId>gremlin-client</artifactId>\n    <version>2.0.1</version>\n</dependency>\n```\n\n## Table of Contents\n\n  - [Overview](#overview)\n    - [Distributing requests across an Amazon Neptune cluster](#distributing-requests-across-an-amazon-neptune-cluster)\n  - [Creating a GremlinCluster and GremlinClient](#creating-a-gremlincluster-and-gremlinclient)\n    - [Configuration](#configuration)\n  - [Using a ClusterEndpointsRefreshAgent](#using-a-clusterendpointsrefreshagent)\n    - [Using an AWS Lambda proxy to retrieve cluster topology](#using-an-aws-lambda-proxy-to-retrieve-cluster-topology)\n      - [Installing the neptune-endpoints-info AWS Lambda function](#installing-the-neptune-endpoints-info-aws-lambda-function)\n      - [Lambda proxy environment variables](#lambda-proxy-environment-variables)\n      - [Suspending endpoints using the AWS Lambda proxy](#suspending-endpoints-using-the-aws-lambda-proxy)\n    - [Using a ClusterEndpointsRefreshAgent to query the Neptune Management API directly](#using-a-clusterendpointsrefreshagent-to-query-the-neptune-management-api-directly)\n    - [ClusterEndpointsRefreshAgent credentials](#clusterendpointsrefreshagent-credentials)\n      - [Accessing the Neptune Management API or Lambda proxy across accounts](#accessing-the-neptune-management-api-or-lambda-proxy-across-accounts)\n    - [Using a ClusterEndpointsRefreshAgent in an AWS Lambda function](#using-a-clusterendpointsrefreshagent-in-an-aws-lambda-function)\n  - [EndpointsSelector](#endpointsselector)\n    - [EndpointsType](#endpointstype)\n  - [Connecting to an IAM auth enabled Neptune database](#connecting-to-an-iam-auth-enabled-neptune-database)\n    - [Service region](#service-region)\n  - [Connecting via a proxy](#connecting-via-a-proxy)\n    - [Configuring proxy connections for an IAM auth enabled Neptune database](#configuring-proxy-connections-for-an-iam-auth-enabled-neptune-database)\n    - [Removing the Host header before sending a Sigv4 signed request to a proxy](#removing-the-host-header-before-sending-a-sigv4-signed-request-to-a-proxy)\n  - [Using a load balancer with host-based routing](#using-a-load-balancer-with-host-based-routing)\n    - [Using an AWS Application Load Balancer](#using-an-aws-application-load-balancer)\n      - [Create target groups](#step-1-create-target-groups)\n      - [Create an Application Load Balancer](#step-2-create-an-application-load-balancer)\n      - [Preserve host headers](#step-3-preserve-host-headers)\n      - [Configure host-based routing](#step-4-configure-host-based-routing)\n      - [Configure the Neptune Gremlin Client](#step-5-configure-the-neptune-gremlin-client)\n  - [Metrics](#metrics)\n    - [Scheduling metrics](#schdeuling-metrics)\n    - [Metrics handlers](#metrics-handlers)\n  - [Usage](#usage)\n    - [Use database instance tags to control endpoint visibility](#use-database-instance-tags-to-control-endpoint-visibility)\n      - [Prewarm replicas](#prewarm-replicas)\n      - [Register targets with load balancer](#register-targets-with-load-balancer)\n    - [Backoff and retry](#backoff-and-retry)\n      - [RetryUtils](#retryutils)\n      - [Backoff and retry when creating a  GremlinCluster and GremlinClient](#backoff-and-retry-when-creating-a-gremlincluster-and-gremlinclient)\n      - [Backoff and retry when submitting a query](#backoff-and-retry-when-submitting-a-query)\n    - [Connection timeouts](#connection-timeouts)\n      - [Force refresh of endpoints when waiting to acquire a connection](#force-refresh-of-endpoints-when-waiting-to-acquire-a-connection)\n    - [Transactions](#transactions)\n    - [Migrating from version 1 of the Neptune Gremlin Client](#migrating-from-version-1-of-the-neptune-gremlin-client)\n  - [Demo](#demo)\n    - [CustomSelectorsDemo](#customselectorsdemo)\n    - [RefreshAgentDemo](#refreshagentdemo)\n    - [RetryDemo](#retrydemo)\n    - [TxDemo](#txdemo)\n\n## Overview\n\nWith the Neptune Gremlin Client you create a `GremlinCluster` and `GremlinClient` much as you would create a `Cluster` and `Client` with the Tinkerpop Java driver. The Neptune Gremlin Client is designed to be a near-drop-in replacement for the Java driver. Internally, it uses the Java driver to connect to Neptune and issue queries.\n\nYou populate a `GremlinCluster` with one or more endpoints, or contact points, when you first create a client, but you can also refresh this list of endpoints from your code whenever you want. The `GremlinClient` exposes a `refreshEndpoints()` method that allows you to supply a new set of endpoints. This allows a running application to adapt to changes in your Neptune database's cluster topology.\n\nThe easiest way to automatically refresh the list of endpoints is to use a `ClusterEndpointsRefreshAgent`. The agent can be configured to periodically discover the database cluster's current topology, select a set of endpoints, and update the client.\n\nA `ClusterEndpointsRefreshAgent` can be configured to get the database cluster's topology directly from the [Neptune Management API](https://docs.aws.amazon.com/neptune/latest/userguide/api.html), or from an AWS Lambda proxy function, which fetches and caches the cluster topology from the Management API on behalf of multiple clients. Unless you have a very small number of client instances (1-5) in your application, we recommend using a Lambda proxy to get the cluster toplogy. This reduces the risk of the Management API throttling requests from many clients.\n\nYour application can then use an `EndpointsSelector` to select an appropriate set of endpoints from the current cluster topology.\n\nThe following diagram shows how an application can use a `GremlinClient`, `ClusterEndpointsRefreshAgent`, and AWS Lambda proxy function to access a Neptune database:\n\n![Accessing Neptune using AWS Lambda proxy](./images/lambda-proxy-architecture.png)\n\nThe following diagram shows how an application can use a `GremlinClient`, and a `ClusterEndpointsRefreshAgent` that gets cluster topology information directly from the Neptune Management API, to access a Neptune database:\n\n![Accessing Neptune using Neptune Management API](./images/management-api-architecture.png)\n\n### Distributing requests across an Amazon Neptune cluster\n\nOne of the benefits of the Neptune Gremlin Client is that it helps you distribute requests evenly across multiple read replicas in a Neptune cluster. \n\nIf you're building an application that needs to distribute requests across replicas, your first choice will typically be the [reader endpoint](https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-endpoints.html#feature-overview-reader-endpoints), which balances _connections_ across replicas. The reader endpoint continues to balance connections across replicas even if you change the cluster topology by adding or removing replicas, or promoting a replica to become the new primary.\n\nHowever, in some circumstances using the reader endpoint can result in an uneven use of cluster resources. The reader endpoint works by periodically changing the host that the DNS entry points to. If a client opens a lot of connections before the DNS entry changes, all the connection requests are sent to a single Neptune instance. The same thing happens if DNS caching occurs in the application layer: the client ends up using the same replica over and over again. If an application opens a lot of connections to the reader endpoint at the same time, many of those connections can end up being tied to a single replica.\n\nThe Neptune Gremlin Client more fairly distributes connections and requests across a set of instances in a Neptune cluster. The client works by creating a connection pool for each [_instance endpoint_](https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-endpoints.html#feature-overview-instance-endpoints) in a given list of endpoints, and distributing requests (queries, not connections) in a round-robin fashion across these connection pools, thereby ensuring a more even distribution of work, and higher read throughput. \n\nNote that the Neptune Gremlin Client will only round-robin requests across multiple read replicas if you supply it with a list of replica _instance endpoints_. If you supply it with the reader endpoint, you may continue to see connections and requests unevenly distributed across the cluster.\n\n## Creating a GremlinCluster and GremlinClient\n\nYou create a `GremlinCluster` and `GremlinClient` using a `NeptuneGremlinClusterBuilder`:\n\n```\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .addContactPoints(\"replica-endpoint-1\", \"replica-endpoint-2\", \"replica-endpoint-3\")\n        .create();       \n \nGremlinClient client = cluster.connect();\n \nDriverRemoteConnection connection = DriverRemoteConnection.using(client);\nGraphTraversalSource g = AnonymousTraversalSource.traversal().withRemote(connection);\n \n// Use g throughout the lifetime of your application to submit queries to Neptune\n \nclient.close();\ncluster.close();\n```\n\nThe `NeptuneGremlinClusterBuilder` is configured to use port 8182, and enable SSL by default.\n \nUse the `GraphTraversalSource` created here throughout the lifetime of your application, and across threads \u2013 just as you would with the TinkerPop Java driver client. The `GremlinClient` ensures that requests are distributed across the current set of endpoints in a round-robin fashion.\n \nThe `GremlinClient` has a `refreshEndpoints()` method that allows you to submit a fresh list of endpoint addresses. When the list of endpoints changes, subsequent requests will be distributed across the new set of endpoints.\n \nOnce you have a reference to a `GremlinClient`, you can call this `refreshEndpoints()` method whenever you discover the cluster topology has changed. You could subscribe to SNS events, for example, and refresh the list whenever an instance is added or removed, or when you detect a failover. \n\nTo update the list of endpoint addresses:\n \n```\nclient.refreshEndpoints(new EndpointCollection(Arrays.asList(\n                    new DatabaseEndpoint().withAddress(\"new-replica-endpoint-1\"),\n                    new DatabaseEndpoint().withAddress(\"new-replica-endpoint-2\"),\n                    new DatabaseEndpoint().withAddress(\"new-replica-endpoint-3\")\n            )));\n```\n\nFrom version 2.0.5 onwards, you can use:\n\n```\nclient.refreshEndpoints(\n                new DatabaseEndpoint().withAddress(\"new-replica-endpoint-1\"),\n                new DatabaseEndpoint().withAddress(\"new-replica-endpoint-2\"),\n                new DatabaseEndpoint().withAddress(\"new-replica-endpoint-3\")\n        );\n```\n \nYou can also use a `ClusterEndpointsRefreshAgent` to update the endpoints automatically on a periodic basis.\n\nBecause the cluster topology can change at any moment as a result of both planned and unplanned events, you should [wrap all queries with an exception handler](#backoff-and-retry). Should a query fail because the underlying client connection has been closed, you can attempt a retry.\n\n### Configuration\n\nMost of the best practices for [using the TinkerPop Gremlin Java client with Amazon Neptune](https://docs.aws.amazon.com/neptune/latest/userguide/best-practices-gremlin-java-client.html) apply to the Neptune Gremlin Client.\n\nOne important point to note is that with the Neptune Gremlin Client, all connection and connection pool settings specified using the `NeptuneGremlinClusterBuilder` apply on a _per endpoint_ basis. For example, if you configure the `NeptuneGremlinClusterBuilder` with three endpoints, then it will create a client with three connection pools. Each connection pool will be configured separately with the connection pool settings specified using the `NeptuneGremlinClusterBuilder`.\n\nOld versions of the TinkerPop Gremlin Java client configured with a `minConnectionPoolSize` smaller than the `maxConnectionPoolSize` could sometimes appear to hang if they needed to add a new connection to the pool to handle an increase in traffic. If the thread used to schedule the creation of a new connection was already doing other work, it sometimes happened that the new connection would never be created, thereby blocking the client from sending any further requests. To mitigate this, we used to recommend configuring the client with `minConnectionPoolSize` equal to `maxConnectionPoolSize`, so that all connections in the pool were created eagerly.\n\nThis issue has been addressed in newer versions of the TinkerPop Gremlin Java client (on which the Neptune Gremlin Client depends), so the former advice no longer applies. Consider setting `minConnectionPoolSize` (per endpoint) to accomodate your steady traffic, and `maxConnectionPoolSize` the peak in your traffic. The exact values will depend on your workload, and may require some experimentation. If in doubt, leave the builder to use the default values (`2` and `8` respectively).\n\nIf you are using the Neptune Gremlin Client in an AWS Lambda function, consider setting both `minConnectionPoolSize` and `maxConnectionPoolSize` to `1`. Because concurrent client requests to your Lambda functions are handled by different function instances running in separate execution contexts, there's no need to maintain a pool of connections to handle concurrent requests inside each function instance.\n\n## Using a ClusterEndpointsRefreshAgent\n\nThe `ClusterEndpointsRefreshAgent` allows you to schedule endpoint updates to a `GremlinClient`. The agent can be configured to periodically discover the database cluster's current topology, select a set of endpoints using an `EndpointsSelector`, and update a client.\n\nA `ClusterEndpointsRefreshAgent` can be configured to get the database cluster's topology directly from the [Neptune Management API](https://docs.aws.amazon.com/neptune/latest/userguide/api.html), or from an AWS Lambda proxy function, which fetches and caches the cluster topology from the Management API on behalf of multiple clients. **Unless you have a very small number of client instances (1-5) in your application, you should use a Lambda proxy to get the cluster toplogy.** This reduces the risk of the Management API throttling requests from many clients.\n\n### Using an AWS Lambda proxy to retrieve cluster topology\n\nThe following example shows how to create a `ClusterEndpointsRefreshAgent` that queries an AWS Lambda proxy function to discover the database cluster's current topology. The proxy function periodically fetches and caches the cluster topology from the Management API on behalf of multiple clients. When the agent gets the cluster topology from the Lambda function, it then updates a `GremlinClient` with the current set of read replica endpoints. Notice how the builder's `addContactPoints()` method uses `refreshAgent.getEndpoints(selector)` to get an initial list of endpoints from the refresh agent using the selector.\n\n\n```\nEndpointsSelector selector = EndpointsType.ReadReplicas;\n\nClusterEndpointsRefreshAgent refreshAgent = \n        ClusterEndpointsRefreshAgent.lambdaProxy(\"neptune-endpoints-info-lambda\");\n\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .addContactPoints(refreshAgent.getEndpoints(selector))\n        .create();       \n \nGremlinClient client = cluster.connect();\n\nrefreshAgent.startPollingNeptuneAPI(\n        RefreshTask.refresh(client, selector),\n        60,\n        TimeUnit.SECONDS);\n        \n```\n\nWhen you use a `ClusterEndpointsRefreshAgent` to query an AWS Lambda proxy function for cluster topology information, the identity under which you're running the agent must be authorized to perform `lambda:InvokeFunction` for the proxy Lambda function. See [ClusterEndpointsRefreshAgent credentials](#clusterendpointsrefreshagent-credentials) for details of supplying credentials to the refresh agent.\n\n#### Installing the neptune-endpoints-info AWS Lambda function\n\n  1. Build the AWS Lambda proxy from [source](./neptune-endpoints-info-lambda), or download the [latest release](https://github.com/aws/neptune-gremlin-client/releases/latest), and put it an Amazon S3 bucket. \n  2. Install the Lambda proxy in your account using [this CloudFormation template](./cloudformation-templates/neptune-endpoints-info-lambda.json). The template includes parameters for the current Neptune cluster ID, and the S3 source for the Lambda proxy jar (from step 1).\n  3. Ensure all parts of your application are using the latest Neptune Gremlin Client.\n  4. The Neptune Gremlin Client should be configured to fetch the cluster topology information from the Lambda proxy using the `ClusterEndpointsRefreshAgent.lambdaProxy()` method, as per the [example above](#using-an-aws-lambda-proxy-to-retrieve-cluster-topology).\n  \n#### Lambda proxy environment variables\n\nThe AWS Lambda proxy has the following environment variables:\n\n  - `clusterId` \u2013 The cluster ID of the Amazon Neptune cluster to be polled for endpoint information.\n  - `pollingIntervalSeconds` \u2013 The number of seconds between polls.\n  - `suspended` \u2013 Determines whether specific endpoints will be suspended (see the next section). Valid values are: `none`, `all`, `writer`, `reader`. \n  \n#### Suspending endpoints using the AWS Lambda proxy\n\nThe Lambda proxy has a `suspended` environment variable that accepts a comma-separated list of the following values: `all`, `writer`, `reader`, `<endpoint_address>`, `<instance_id>`. You can use this environment variable to _suspend_ specific types of endpoint and specific instance endpoints. Suspended endpoints will not be chosen by the client when it applies a selector to the cluster topology.\n\nTo suspend a particular endpoint type or specific instance endpoint, change the variable value, save the change, and once it has propagated (this may take up to a minute), all clients that use the Lambda proxy will see the specified endpoints as being suspended. Setting the value to `reader`, for example, will ensure that all instances currently in a reader role will be seen as suspended. Setting the value to `writer,neptune-db-2-05d7a510` will suspend both the primary instance (the writer) and the instance with the instance id `neptune-db-2-05d7a510`.\n\nYou can also suspend specific instance endpoints using [Amazon Neptune tags](https://docs.aws.amazon.com/neptune/latest/userguide/tagging.html). To suspend a specific instance, attach a tag with the key `neptune:suspended`, and value `true` to the instance. To remove the suspension, delete the tag or set its value to `false`.\n\nAn endpoint will be considered suspended if it has been tagged as suspended, or if it is included in a group referred to by the `suspended` environment variable, or if it is directly referred to using its endpoint address or instance id in the `suspended` environment variable. For example, if the `suspended` environment variable is set to `all`, and an instance has also been tagged as suspended, it will be considered suspended. If the tag is susequently removed, the instance will still be considered suspended, because it belongs to the `all` group. If an instance is tagged `neptune:suspended` with the value `false`, but its id is included in the `suspended` environment variable, the instance will be suspended. \n\nYou can use this feature to prevent traffic to your cluster while you perform maintenance, upgrade or migration activities. Suspended endpoints apply back pressure in the client, preventing it from sending queries to the database cluster. To manage this back pressure, your application will have to handle an `EndpointsUnavailableException`. This exception can occur in two different places:\n \n  - When you call `NeptuneGremlinClusterBuilder.create()`.\n  - When you submit a query using an existing `GraphTraversalSource`.\n \nThe `EndpointsUnavailableException` will appear as the root cause: invariably, it is wrapped in a `RemoteConnectionexception`, or similar. The only reason you should see an `EndpointsUnavailableException` is because the endpoints have been suspended. \n\nSuspended endpoints are hidden from endpoint selectors. If an endpoint is suspended, it will not be considered for selection, even if it matches the selection criteria.\n\n### Using a ClusterEndpointsRefreshAgent to query the Neptune Management API directly\n\nThe following example shows how to create a `ClusterEndpointsRefreshAgent` that queries the [Neptune Management API](https://docs.aws.amazon.com/neptune/latest/userguide/api.html) to discover the database cluster's current topology. The agent then updates a `GremlinClient` with the current set of read replica endpoints. Notice how the builder's `addContactPoints()` method uses `refreshAgent.getEndpoints(selector)` to get an initial list of endpoints from the refresh agent using the selector.\n\n```\nEndpointsSelector selector = EndpointsType.ReadReplicas;\n\nClusterEndpointsRefreshAgent refreshAgent = \n        ClusterEndpointsRefreshAgent.managementApi(\"cluster-id\");\n\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .addContactPoints(refreshAgent.getEndpoints(selector))\n        .create();       \n \nGremlinClient client = cluster.connect();\n\nrefreshAgent.startPollingNeptuneAPI(\n        RefreshTask.refresh(client, selector),\n        60,\n        TimeUnit.SECONDS);\n        \n```\n\nWhen you use a `ClusterEndpointsRefreshAgent` to query the Neptune Management API directly, the identity under which you're running the agent must be authorized to perform `rds:DescribeDBClusters` for your Neptune cluster, and `rds:DescribeDBInstances` and `rds:ListTagsForResource` for `db:*`. See [ClusterEndpointsRefreshAgent credentials](#clusterendpointsrefreshagent-credentials) for details of supplying credentials to the refresh agent.\n\n(`rds:DescribeDBInstances` and `rds:ListTagsForResource` require permissions for `db:*` because a `db` resource type can't be restricted by cluster name. A `db` resource can be restricted by instance name, but this is not particularly useful here because the refresh agent is looking for instances that may have been created _after_ the IAM policy was formulated.)\n\nWhen the Neptune Management API experiences a high rate of requests, it starts throttling API calls. If you have a lot of clients frequently polling for endpoint information, your application can very quickly experience throttling (in the form of HTTP 400 throttling exceptions).\n\nBecause of this throttling behaviour, if your application uses a lot of concurrent `GremlinClient` and `ClusterEndpointsRefreshAgent` instances, instead of querying the Management API directly, you should [proxy endpoint refresh requests through an AWS Lambda function](#using-an-aws-lambda-proxy-to-retrieve-cluster-topology). The Lambda function can periodically query the Management API and then cache the results on behalf of its clients.\n\n### ClusterEndpointsRefreshAgent credentials \n\nWhen you create a `ClusterEndpointsRefreshAgent` using one of the `lambaProxy` or `managementApi` factory methods, you can supply the credentials necessary to invoke the AWS Lambda proxy, or the Neptune Management API, as appropriate. These can be a separate set of credentials from the [credentials used to query your Neptune database](#connecting-to-an-iam-auth-enabled-neptune-database).\n\nYou can supply the name of a named profile in a local profile configuration file:\n\n```\nString profileName = \"my-profile\";\n\n// Using a Lambda proxy\nClusterEndpointsRefreshAgent lambdaProxyRefreshAgent = \n        ClusterEndpointsRefreshAgent.lambdaProxy(\"neptune-endpoints-info-lambda\", \"eu-west-1\", profileName);\n        \n// Querying the Neptune Management API\nClusterEndpointsRefreshAgent managementApiRefreshAgent = \n        ClusterEndpointsRefreshAgent.managementApi(\"my-cluster-id\", \"eu-west-1\", profileName);        \n```\n\nOr you can supply an implementation of `AWSCredentialsProvider`:\n\n```\nAWSCredentialsProvider credentialsProvider = \n        new ProfileCredentialsProvider(\"my-profile\")\n\n// Using a Lambda proxy\nClusterEndpointsRefreshAgent lambdaProxyRefreshAgent = \n        ClusterEndpointsRefreshAgent.lambdaProxy(\"neptune-endpoints-info-lambda\", \"eu-west-1\", credentialsProvider);       \n\n// Querying the Neptune Management API\nClusterEndpointsRefreshAgent managementApiRefreshAgent = \n        ClusterEndpointsRefreshAgent.managementApi(\"my-cluster-id\", \"eu-west-1\", credentialsProvider);        \n```\n\n### Using a ClusterEndpointsRefreshAgent in an AWS Lambda function \n\nIf you're building a serverless application that uses AWS Lambda functions to query Neptune, and you're using a Neptune Gremlin Client and a refresh agent in those Lambda functions, you must ensure the refresh agent wakes up and refreshes in a timely manner. You do this by calling the `awake()` method on the functions's `ClusterEndpointsRefreshAgent` instance (available from version 2.0.2 onwards). Do this with each function invocation in the functions's handler. For example:\n\n```\npublic void handleRequest(InputStream input, OutputStream output, Context context) throws IOException {\n\n        refreshAgent.awake();\n        \n        // Rest of the handler code\n}\n```\n\nThe refresh agent schedules its refreshes on a background thread. A Lambda context \u2013 the container in which a function executes \u2013 survives across multiple invocations of the function, but in between invocations it is effectively asleep. If a refresh is scheduled to occur while the Lambda context is asleep, the refresh will not take place. As a result, changes in the Neptune cluster topology that might be expected to propogate to the Lambda proxy in several seconds (depending on the refresh interval you specify) can take several minutes to appear \u2013 appearing only when a refresh coincides with a period when the context is awake.\n\nBy calling `awake()` at the beginning of every function invocation, you ensure that refreshes occur in a timely manner.\n\n\n\n#### Accessing the Neptune Management API or Lambda proxy across accounts\n\nNormally your application, Neptune database and Lambda proxy will be in the same account. If, however, you need to assume a cross-account role to contact the Lambda proxy or Neptune Management API, you can use an `STSAssumeRoleSessionCredentialsProvider` to create a temporary session for authentication to a resource in another account.\n\nBefore you access the Management API or Lambda proxy across accounts, follow the instructions in [this tutorial](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html) to delegate access to the resources across AWS accounts using IAM roles. In line with this tutorial, you'll need to:\n\n  1. Create a managed policy and role in the resource account (the account containing the Neptune database cluster or the AWS Lambda proxy) that allows trusted users to access the resource.\n  2. Grant access to this role to the identity under which you're running the refresh agent.\n  3. Create an `STSAssumeRoleSessionCredentialsProvider` that can assume the role, and which can be passed to the refresh agent.\n\nIf you want to access the Neptune Management API across accounts, the managed policy document that you create in the Neptune account in [Step 1](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html#tutorial_cross-account-with-roles-1) of the tutorial should look like this :\n\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"rds:DescribeDBClusters\"\n      ],\n      \"Resource\": \"<NEPTUNE_CLUSTER_ARN>\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"rds:DescribeDBInstances\",\n        \"rds:ListTagsForResource\"\n      ],\n      \"Resource\": \"arn:${Partition}:rds:${Region}:${Account}:db:*\"\n    }\n  ]\n}\n```\n\nIn the above example, replace `<NEPTUNE_CLUSTER_ARN>` with the ARN of your Neptune database cluster, and the `${Partition}`, `${Region}` and `${Account}` placeholders with the relevant values for your account.\n\nIf you want to access the Lambda proxy across accounts, the managed policy document that you create in the Neptune account in [Step 1](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html#tutorial_cross-account-with-roles-1) of the tutorial should look like this:\n\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"lambda:InvokeFunction\"\n      ],\n      \"Resource\": \"<LAMBDA_ARN>\"\n    }\n  ]\n}\n```\n\nIn the above example, replace `<LAMBDA_ARN>` with the ARN of your Lambda proxy function.\n\nOnce you've completed [Step 2](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html#tutorial_cross-account-with-roles-2) of the tutorial, you can then create a refresh agent using an `STSAssumeRoleSessionCredentialsProvider`.\n\nThe following example shows how to access a Lambda proxy across accounts. Replace `<CROSS_ACCOUNT_ROLE_ARN>` with the ARN of the role created in [Step 1](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html#tutorial_cross-account-with-roles-1) of the tutorial:\n\n \n```\nString lambdaName = \"neptune-endpoinst-info\";\nString lambdaRegion = \"eu-west-1\";\nString crossAccountRoleArn = \"<CROSS_ACCOUNT_ROLE_ARN>\";\n\nSTSAssumeRoleSessionCredentialsProvider credentialsProvider =\n        new STSAssumeRoleSessionCredentialsProvider.Builder(crossAccountRoleArn, \"AssumeRoleSession1\")\n        .build();\n\nClusterEndpointsRefreshAgent refreshAgent = \n        ClusterEndpointsRefreshAgent.lambdaProxy(lambdaName, lambdaRegion, credentialsProvider);\n```\n\nThe following example shows hot to access the Neptune aMnagement API across accounts. Replace `<CROSS_ACCOUNT_ROLE_ARN>` with the ARN of the role created in [Step 1](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html#tutorial_cross-account-with-roles-1) of the tutorial:\n\n \n```\nString clusterId = \"my-cluster-id\";\nString neptuneRegion = \"eu-west-1\";\nString crossAccountRoleArn = \"<CROSS_ACCOUNT_ROLE_ARN>\";\n\nSTSAssumeRoleSessionCredentialsProvider credentialsProvider =\n        new STSAssumeRoleSessionCredentialsProvider.Builder(crossAccountRoleArn, \"AssumeRoleSession1\")\n        .build();\n\nClusterEndpointsRefreshAgent refreshAgent = \n        ClusterEndpointsRefreshAgent.managementApi(clusterId, neptuneRegion, credentialsProvider);\n```\n\nIf [AWS STS Regional endpoints](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_enable-regions.html) have been enabled in your account, you may want to configure the credentials provider for Regional STS endpoint access:\n\n```\nEndpointConfiguration regionEndpointConfig = new EndpointConfiguration(\"https://sts.eu-west-1.amazonaws.com\", \"eu-west-1\");\n\nAWSSecurityTokenService stsRegionalClient = AWSSecurityTokenServiceClientBuilder.standard()\n        .withEndpointConfiguration(regionEndpointConfig)\n        .build();\n        \nSTSAssumeRoleSessionCredentialsProvider credentialsProvider =\n        new STSAssumeRoleSessionCredentialsProvider.Builder(crossAccountRoleArn, \"AssumeRoleSession1\")\n        .withStsClient(stsRegionalClient)\n        .build();\n``` \n\nRemember to call `close()` on the credentials provider when it is no longer needed. This shuts down the thread that performs asynchronous credential refreshing.\n\n## EndpointsSelector\n\nThe `EndpointsSelector` interface allows you to create objects that encapuslate custom endpoint selection logic. When a selector's `getEndpoints()` method is invoked, it is passed a `NeptuneClusterMetadata` object that contains details about the database cluster's topology. In your `getEndpoints()` implementation, you can then filter instances in the cluster by properties such as role (reader or writer), instance ID, instance type, tags, and Availability Zone. \n\nThe following example shows how to create an `EndpointsSelector` that returns the endpoints of available Neptune serverless instances in a cluster that have been tagged \"analytics\":\n\n```\nEndpointsSelector selector = (cluster) ->\n        new EndpointCollection(\n                cluster.getInstances().stream()\n                        .filter(i -> i.hasTag(\"workload\", \"analytics\"))\n                        .filter(i -> i.getInstanceType().equals(\"db.serverless\"))\n                        .filter(NeptuneInstanceMetadata::isAvailable)\n                        .collect(Collectors.toList()));\n\n```\n\nThe `isAvailable()` property of a `NeptuneInstanceMetadata` object indicates whether an endpoint is _likely_ to be available. An endpoint is considered likely to be available if the instance itself is in one of the following states: `available`, `backing-up`, `modifying`, `upgrading`.\n\nThe full list of database instances states can be found [here](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/accessing-monitoring.html). Note that not all of these states are relevant to Amazon Neptune (for example, `converting-to-vpc` does not apply to Amazon Neptune database instances).\n\nWe say that `isAvailable()` indicates that an endpoint is _likely_ available. There is no guarantee that the endpoint is actually available. For example, while an instance is `upgrading`, there can be short periods when the endpoint is _not_ available. During the upgrade process, instances are sometimes restarted, and while this is happening the instance endpoint will not be available, even though the state is `upgrading`.\n\nIf your selection criteria returns an empty list of endpoints, you may want to fall back to using the [cluster or reader endpoints](https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-endpoints.html). That way, your client will always have at least one endpoint, even if because of some issue outside of its control, it cannot currently connect to the database cluster. The following example falls back to the reader endpoint if there are no instances matching the selection criteria:\n\n```\nEndpointsSelector writerSelector = (cluster) -> {\n                List<NeptuneInstanceMetadata> endpoints = cluster.getInstances().stream()\n                        .filter(i -> i.hasTag(\"workload\", \"analytics\"))\n                        .filter(i -> i.getInstanceType().equals(\"db.serverless\"))\n                        .filter(NeptuneInstanceMetadata::isAvailable)\n                        .collect(Collectors.toList());\n                return endpoints.isEmpty() ?\n                        new EndpointCollection(Collections.singletonList(cluster.getReaderEndpoint())) :\n                        new EndpointCollection(endpoints);\n            };\n```\n### EndpointsType\n\nThe `EndpointsType` enum provides implementations of `EndpointsSelector` for some common use cases:\n\n  * `EndpointsType.All` \u2013  Returns all available instance (writer and read replicas) endpoints, or, if there are no available instance endpoints, the [reader endpoint](https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-endpoints.html#feature-overview-reader-endpoints).\n  * `EndpointsType.Primary` \u2013 Returns the primary (writer) instance endpoint if it is available, or the [cluster endpoint](https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-endpoints.html#feature-overview-cluster-endpoints) if the primary instance endpoint is not available.\n  * `EndpointsType.ReadReplicas` \u2013 Returns all available read replica instance endpoints, or, if there are no replica instance endpoints, the [reader endpoint](https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-endpoints.html#feature-overview-reader-endpoints).\n  * `EndpointsType.ClusterEndpoint` \u2013 Returns the [cluster endpoint](https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-endpoints.html#feature-overview-cluster-endpoints).\n  * `EndpointsType.ReaderEndpoint` \u2013 Returns the [reader endpoint](https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-endpoints.html#feature-overview-reader-endpoints).\n \n\nThe following example shows how to use the `ReadReplicas` enum value to create and refresh a client:\n\n```\nEndpointsSelector selector = EndpointsType.ReadReplicas;\n\nClusterEndpointsRefreshAgent refreshAgent = \n        new ClusterEndpointsRefreshAgent(\"cluster-id\");\n\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .addContactPoints(refreshAgent.getEndpoints(selector))\n        .create();       \n \nGremlinClient client = cluster.connect();\n\nrefreshAgent.startPollingNeptuneAPI(\n        RefreshTask.refresh(client, selector),\n        60,\n        TimeUnit.SECONDS);\n        \n```\n\n## Connecting to an IAM auth enabled Neptune database\n \nWhen the Neptune Gremlin Client connects to an IAM auth enabled database it uses a `DefaultAWSCredentialsProviderChain` to supply credentials to the signing process. You can modify this behavior in a couple of different ways.\n\nTo customize which profile is sourced from a local credentials file, use the `iamProfile()` builder method: \n\n```\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .enableIamAuth(true)\n        .addContactPoint(\"reader-1\")\n        .iamProfile(\"profile-name\")\n        .create();\n```\n\nOr you can supply your own `AwsCredentialsProvider` implementation:\n\n```\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .enableIamAuth(true)\n        .addContactPoint(\"reader-1\")\n        .credentials(new ProfileCredentialsProvider(\"profile-name\"))\n        .create();\n```\n\nThe client includes a load balancer-aware handshake interceptor that will [sign requests and adjust HTTP headers as necessary](https://github.com/aws-samples/aws-dbs-refarch-graph/tree/master/src/connecting-using-a-load-balancer). However, you can replace this interceptor with your own implementation:\n\n```\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n\t\t\t\t.enableIamAuth(true)\n        .addContactPoint(\"reader-1\")\n        .handshakeInterceptor(r -> { \n              NeptuneNettyHttpSigV4Signer sigV4Signer = new NeptuneNettyHttpSigV4Signer(\"eu-west-1\", new DefaultAWSCredentialsProviderChain());\n              sigV4Signer.signRequest(r);\n              return r;\n        })\n        .create();\n```\n\n### Service region\n\nIf you have IAM database authentication enabled for your Neptune database, you _must_ specify the Neptune service region when connecting from your client.\n\nBy default, the Neptune Gremlin Client will attempt to source this region parameter from several different places:\n\n  - The **SERVICE_REGION** environment variable.\n  - The **SERVICE_REGION** system property.\n  - The **AWS_REGION** Lambda environment variable (this assumes Neptune is in the same region as the Lambda function).\n  - Using the `Regions.getCurrentRegion()` method from the [AWS SDK for Java](https://aws.amazon.com/blogs/developer/determining-an-applications-current-region/) (this assumes Neptune is in the current region).\n\nYou can also specify the service region when creating a `GremlinCluster` using the `NeptuneGremlinClusterBuilder.serviceRegion()` builder method:\n\n```\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .enableIamAuth(true)\n        .addContactPoint(\"reader-1\")\n        .serviceRegion(\"eu-west-1\")\n        .create();\n```\n\n## Connecting via a proxy\n\nAs an _alternative_ to connecting directly to Neptune and using a refresh agent to adjust the pool of connections to match the cluster's current topology, you can connect via a proxy, such as a load balancer. To connect via a proxy, use the `proxyAddress()` and `proxyPort()` builder methods. If the connection to the proxy does not require SSL, disable SSL:\n\n```\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .enableSsl(false)\n        .proxyAddress(\"http://my-proxy\")\n        .proxyPort(80)\n        .serviceRegion(\"eu-west-1\")\n        .create();\n```\n\nIf your Neptune database does _not_ have IAM auth enabled, you do not need to add any additional contact points.\n\n### Configuring proxy connections for an IAM auth enabled Neptune database\n\nIf your Neptune database has IAM auth enabled, HTTP requests to the database must be signed using AWS Signature Version 4. Unless your proxy implements the signing process, you will have to sign requests in the client. The client must sign the request using the Neptune endpoint that will recieve the request, and include an HTTP `Host` header whose value is `<neptune-endpoint-dns:port>`. \n\nUse the `enableIamAuth()` and `serviceRegion()` builder methods to sign requests.\n\nWhen using a proxy, besides specifying the proxy address and port, you will also have to specify the ultimate Neptune endpoint to which a request will be directed. You do this using the `addContactPoint()` builder method. _The endpoint you specify here must match the Neptune endpoint to which the proxy forwards requests._ There's no point using a refresh agent to supply a list of endpoints to the client unless the proxy can forward requests to the correct Neptune endpoint based on the value of the `Host` header in the request sent to the proxy.\n\nHere's an example of creating a client that connects through a proxy to an IAM auth-enabled Neptune database, where the proxy has been configured to forward requests to the database's cluster endpoint:\n\n```\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .enableIamAuth(true)\n        .serviceRegion(\"eu-west-1\")\n        .addContactPoint(\"my-db.cluster-cktfjywp6uxn.eu-west-1.neptune.amazonaws.com\") // cluster endpoint\n        .proxyAddress(\"my-proxy\")\n        .proxyPort(80)\n        .create();\n```\n\n### Removing the Host header before sending a Sigv4 signed request to a proxy\n\nIn some circumstances, you may need to remove the HTTP `Host` header after signing the request, but before sending it to the proxy. For example, your proxy may add a `Host` header to the request: if that's the case, you don't want the request when it arrives at the Neptune endpoint to contain _two_ `Host` headers:\n\n```\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .enableIamAuth(true)\n        .serviceRegion(\"eu-west-1\")\n        .addContactPoint(\"my-db.cluster-cktfjywp6uxn.eu-west-1.neptune.amazonaws.com\") // cluster endpoint\n        .proxyAddress(\"my-proxy\")\n        .proxyPort(80)\n        .proxyRemoveHostHeader(true)\n        .serviceRegion(\"eu-west-1\")\n        .create();\n```\n\n## Using a load balancer with host-based routing\n\nAs of Neptune Gremlin Client version 2.0.1 you can use the Neptune Gremlin Client with a load balancer that supports host-based routing to balance requests across instances in your database cluster as selected by an `EndpointsSelector`. This allows you to use custom endpoint selection logic in the client and still distribute requests across the instances in the selection set.\n\nFor this solution to work you must set up host-based routing in your load balancer. \n\n### Using an AWS Application Load Balancer\n\nThe following steps describe how to create an AWS Application Load Balancer in your Neptune VPC and configure it for host-based routing to individual database endpoints. Note, you **must** configure the [Preserve host headers](#step-3-preserve-host-headers) attribute on your laod balancer for this solution to work with an IAM-auth eneabled database.\n\n#### Limitations\n\nThe solution presented here uses a fixed set of target groups, one per database instance endpoint. If your database cluster topology changes, you may need to remove some of those target groups and add others. If you don't keep the target groups up-to-date with the cluster topology, topology changes that trigger a refresh of the connection pools and connection selection logic in your Neptune Gremlin Client instances may lead to connection errors in the client.\n\n#### Step 1. Create target groups\n\nCreate a [separate target group](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html) for each instance in your cluster. \n\n##### Specify group details\n\nUnder **Basic configuration**:\n\n  - Select **IP addresses** as the target type.\n  - Give the target group a name that clearly indicates which instance it is associated with.\n  - Select the `HTTPS` protocol and port `8182` (or whatever port your database cluster is using).\n  - Select the Neptune VPC.\n\nUnder **Health checks**:\n\n  - The health check protocol should already be set to `HTTPS`.\n  - Enter `/status` for the health check path.\n  - If your Neptune database is secured with IAM database authentication, you'll need to update the **Advanced health check settings**: set the success codes to `200-403`. \n  \nClick **Next**.\n  \n##### Register targets\n\nUnder **IP addresses**:\n\n  - Add the IPv4 address of the instance endpoint to the target group and click the **Include as pending below** button (the port should already be set to `8182`). You can find the private IP address of the instance endpoint using `dig +short <endpoint>` from your terminal.\n\nClick **Create target group**.\n\nRepeat this process for each instance in your database cluster.\n\n#### Step 2. Create an Application Load Balancer\n\n##### Compare and select load balancer type\n\nUnder **Load balancer types** choose **Application Load Balancer** and click **Create**.\n\n##### Create Application Load Balancer\n\nUnder **Network mapping**:\n\n  - Select the Neptune VPC.\n  - If you've configured the load balancer to be internet facing, you must choose at least two subnets in the Neptune VPC with routes to an internet gateway.\n  \nUnder **Security groups**\n\n  - Select one or more security groups that allow access to your load balancer from your clients.\n  \nUnder **Listeners and routing**\n\n  - Select the protocol clients will use to access the load balancer. If you select `HTTPS` you'll also have to configure the **Secure listener settings**.\n  - At this point, you'll need to select one of your target groups to act as the target for the default action. \n  \nClick **Create load balancer**\n\n#### Step 3. Preserve host headers\n\nOnce you've created your load balancer, there's one further change you must make, which is to configure it to [preserve host headers](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#host-header-preservation):\n\n  - On the load balancer's **Attributes** tab, click **Edit**.\n  - Under the **Packet handling** section, enable **Preserve host header**.\n  - Click **Save changes**.\n  \n#### Step 4. Configure host-based routing\n\nYou can now set up [host-based routing rules](https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/) for all your target groups.\n\n  - Open the **Rules** tab for your load balancer's listener and click the **Manage rules** button.\n  - Insert a rule for each of your target groups. Specify a `Host header... is` condition with the host name of the target group's instance endpoint, and a `Forward to...` action.\n  - Consider modifying the default rule to return `404 - Not Found` for connection requests that cannot be routed to an existing target group.\n  \n#### Step 5. Configure the Neptune Gremlin Client\n\nAs pointed out in the [limitations](#limitations) section, this solution works with a fixed set of target groups, and therefore a fixed cluster topology. Because of this, there is little value in using a refresh agent to keep the client up-to-date with changes in the cluster topology. (If you automate the process of keeping the target groups and load balancer host-based routing rules up-to-date with changes in the cluster topology, then there certainly _is_ value in using a refresh agent.) You can, however, supply multiple instance endpoints when building a client, so as to ensure that requests are distributed across those endpoints.\n\nHere's an example of creating a client that connects through an ALB to an IAM auth-enabled Neptune database. The ALB accepts traffic on port 80, so we disable SSL (connections from the ALB to Neptune, however, do use SSL). The ALB has been configured to use host-based routing, so we can provide multiple instance endpoints to the builder:\n\n```\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .enableIamAuth(true)\n        .enableSsl(false)\n        .serviceRegion(\"eu-west-1\")\n        .addContactPoints(\"replica-1-endpoint\", \"replica-2-endpoint\", \"replica-3-endpoint\") // replica endpoints\n        .proxyAddress(\"alb-endpoint\")\n        .proxyPort(80)\n        .create();\n```\n\n## Metrics\n\nFrom version 2.0.3 onwards, the Neptune Gremlin Client can collect metrics about the attempts to connect to each endpoint, and the requests to each endpoint. Metrics are emitted each time the endpoints of a `GremlinClient` are refreshed (using a refresh agent, for example).\n\nTo enable metrics collection, set `enableMetrics()` to `true` when building a cluster:\n\n```\nGremlinCluster cluster =  NeptuneGremlinClusterBuilder.build()\n        .enableMetrics(true)\n        \n        ... // other builder methods\n        \n        .create();               \n```\n\nYou can also enable or disable metrics using a `org.apache.tinkerpop.gremlin.driver.MetricsConfig.enableMetrics` environment variable or system property. If either the environment variable or system property is set to `false`, then metrics will be disabled, irrespective of the builder configuration. \n\nWhen you enable metrics, connection and request metrics will be written to the log at the `INFO` logging level. Here's an example:\n\n```\nINFO MetricsLogger - Connection metrics: [duration: 15346ms, totalConnectionAttempts:38368, endpoints: [db-1.abcdefghijklm.eu-west-2.neptune.amazonaws.com [total: 19184, succeeded: 19184, unavailable: 0, closing: 0, dead: 0, npe: 0, nha: 0, minMillis: 0, maxMillis: 2, avgMillis: 0.12], db-2.abcdefghijklm.eu-west-2.neptune.amazonaws.com [total: 19184, succeeded: 19184, unavailable: 0, closing: 0, dead: 0, npe: 0, nha: 0, minMillis: 0, maxMillis: 1, avgMillis: 0.12]]]\n\nINFO MetricsLogger - Request metrics: [duration: 15346ms, totalRequests:38368, endpoints: [db-1.abcdefghijklm.eu-west-2.neptune.amazonaws.com [count: 19184, ratePerSec: 1249.935, minMillis: 0, maxMillis: 16, avgMillis: 0.17], db-2.abcdefghijklm.eu-west-2.neptune.amazonaws.com [count: 19184, ratePerSec: 1249.935, minMillis: 0, maxMillis: 3, avgMillis: 0.17]] (dropped: 0, skipped: 0)]\n```\n\nConnection metrics capture the time taken to attempt to acquire a connection. Using the connection metrics, you can determine whether requests are using connections that are equally distributed across the database endpoints.\n\nRequest metrics capture the average latencies for requests to each of the endpoints. Using the request metrics, you can determine whether some endpoints are returning responses more slowly than others.\n\n### Scheduling metrics\n\nMetrics are only emitted when the endpoints of a `GremlinClient` are refreshed. If you've enabled metrics when building a cluster, and you're using a refresh agent to keep your clients up-to-date with changes in your Neptune database's cluster topology, connection and request metrics will be emitted automatically. If you're not using a refresh agent (perhaps you've supplied a static list of endpoints when building a cluster, and therefore don't need to refresh them periodically), however, the client won't emit any metrics. You have two options to force metrics to be emitted periodically.\n\nThe simplest option is to 'monitor' your client using a refresh agent. In the following example, the monitor will cause the client to emit metrics every 15 seconds:\n\n```\nGremlinClient client = cluster.connect();\n\n\nClusterEndpointsRefreshAgent refreshAgent = \n        ClusterEndpointsRefreshAgent.monitor(client, 15, TimeUnit.SECONDS);\n\n// Application code\n\nrefreshAgent.close()\n```\n\nRemember to close the refresh agent when your application has finished with it.\n\nIf you don't want to use a refresh agent to monitor a client, you can call `refresh()` directly on the client instance instead:\n\n```\nGremlinClient client = cluster.connect();\n\n\nclient.refresh(); // Call this periodically to emit metrics\n```\n\nWith this second approach, you'll have to schedule the periodic call to `refresh()` in your application code.\n\n### Metrics handlers\n\nWhen you enable metrics, metrics will automatically be written to the log whenever the endpoints of a `GremlinClient` are refreshed. Besides logging metrics, you also have the option of supplying a `MetricsHandler`:\n\n```\nGremlinCluster cluster =  NeptuneGremlinClusterBuilder.build()\n        .enableMetrics(true)\n        .addMetricsHandler((connectionMetrics, requestMetrics) -> {\n                // Your handler code   \n        })\n        \n        ...\n        \n        .create();\n```\n\nUsing a `MetricsHandler` you can, for example, publish metrics to Amazon CloudWatch.\n\n\n## Usage\n\n### Use database instance tags to control endpoint visibility\n\nUsing a custom `EndpointsSelector` you can select database instance endpoints based on the presence or absence of specific [database instance tags](https://docs.aws.amazon.com/neptune/latest/userguide/tagging.html) and tag values. You can employ this feature to manage the visibility of instance endpoints to clients while you undertake some out-of-band operations. The examples below describe two operational patterns that use this feature.\n\n(Note that pre version 2.0.1 of the Neptune Gremlin Client, calls from a refresh agent to the Neptune Management API would cache tags for individual instances. This meant that changes to a database instance's tags would not be reflected in subsequent calls to get the cluster topology. This behaviour has changed in 2.0.1: the tags associated with a database instance now remain current with that instance.)\n\n#### Prewarm replicas\n\nNeptune's [auto-scaling feature](https://docs.aws.amazon.com/neptune/latest/userguide/manage-console-autoscaling.html) allows you to automatically adjust the number of Neptune replicas in a database cluster to meet your connectivity and workload requirements. Autoscaling can add replicas to your cluster on a sceduled basis or whenever the CPU utilization on existing instances exceeds a threshhold you specify in the autoscaling configuration.\n\nNew replicas, however, will start with a cold [buffer cache](https://aws.amazon.com/blogs/database/part-1-accelerate-graph-query-performance-with-caching-in-amazon-neptune/). For some applications, the additional query latency caused by a cold cache can impact the user experience. In these circumstances, you may want to warm the cache before 'releasing' the replica to clients. In this way, you improve the performance of the first queries directed to the replica, but at the expense of the replica taking a little longer to become available to service queries.\n\nNeptune's `neptune_autoscaling_config` parameter allows you to specify one or more tags to be attached to newly provisioned autoscaled readers. You can use this feature in combination with a custom `EndpointsSelector` to hide cold replicas from clients.\n\nHere's an example `neptune_autoscaling_config` JSON parameter that tags autoscaled readers with a \"buffer-cache\" tag with the value \"cold\":\n\n```\n\"{\n  \\\"tags\\\": [\n    { \\\"key\\\" : \\\"buffer-cache\\\", \\\"value\\\" : \\\"cold\\\" }\n  ],\n  \\\"maintenanceWindow\\\" : \\\"wed:12:03-wed:12:33\\\",\n  \\\"dbInstanceClass\\\" : \\\"db.r5.xlarge\\\"\n}\"\n```\n\nYou can use this in combination with the following custom `EndpointsSelector`, which selects reader instances that _do not_ have a \"buffer-cache\" tag with the value \"cold\":\n\n```\nEndpointsSelector selector = (cluster) ->\n        new EndpointCollection(\n                cluster.getInstances().stream()\n                        .filter(NeptuneInstanceMetadata::isReader)\n                        .filter(i -> !i.hasTag(\"buffer-cache\", \"cold\")) // ignore readers with cold caches\n                        .collect(Collectors.toList()));\n```\n\nFor this solution to work, you now need a process that can identify a newly provisioned reader, warm it with a query workload, and then delete its \"buffer-cache\" tag. When the tag is deleted, the instance will become visible to the application's Neptune Gremlin Clients.\n\nThe process you use for detecting and warming readers is out of scope for this documentation. Note, however, that if you are already using an [AWS Lambda proxy](#using-an-aws-lambda-proxy-to-retrieve-cluster-topology) in your application to retrieve cluster topology information, then you already have a source you can poll to discover cold readers (i.e. readers that _do_ have a \"buffer-cache\" tag with the value \"cold\").\n\n\n#### Register targets with load balancer\n\nThe solution described elsewhere in this documentation for using an [AWS Application Load Balancer with the Neptune Gremlin Client](#using-an-aws-application-load-balancer) is limited insofar as it assumes a static cluster topology. If you want to use this solution with a refresh agent and `EndpointsSelector` that adapts to changes in the cluster toplogy, then you will need to introduce a process that can update a load balancer's target groups and host-based routing rules whenever instances are added to or removed from the cluster. Further, you will want to ensure that clients can only select an instance endpoint once it has been registered with the load balancer.\n\nOne way to ensure that a Neptune Gremlin Client instance only uses endpoints that have been registered with an ALB, is to tag instances that have been registered with the load balancer, and to use a custom `EndpointsSelector` that filters based on this tag. The following custom `EndpointsSelector` only selects reader instances that have an \"alb-status\" tag with the value \"registered\":\n\n```\nEndpointsSelector selector = (cluster) ->\n        new EndpointCollection(\n                cluster.getInstances().stream()\n                        .filter(NeptuneInstanceMetadata::isReader)\n                        .filter(i -> i.hasTag(\"alb-status\", \"registered\"))\n                        .collect(Collectors.toList()));\n```\n\nFor this solution to work, you now need a process that can identify whenever an instance is added to or removed from the cluster, and which updates the target groups and host-based routing rules accordingly. This process should then tag the database instance after it has been registered with the load balancer.\n\nThe process you use for detecting cluster changes and updating target groups and routing rules is out of scope for this documentation. Note, however, that if you are already using an [AWS Lambda proxy](#using-an-aws-lambda-proxy-to-retrieve-cluster-topology) in your application to retrieve cluster topology information, then you already have a source you can poll to discover new, unregistered instances (i.e. instances that _do not_ have an \"alb-status\" tag with the value \"registered\").\n\n\n### Backoff and retry\n\nConnection issues occur: database instances failover or restart during upgrades, and intermittent issues in the network can break connections. Write queries sometimes throw an error, because of a `ConcurrentModificationException` or `ConstraintViolationException`, or because the primary has failed over, and the instance to which the query is sent can no longer support writes, triggering a `ReadOnlyViolationException`.\n\nAs a good practice you should consider implementing a backoff and retry strategy to handle these occurences and help your application recover and make forward progress. \n\n  - **Connection issues** \u2013 The TinkerPop Java driver, on which the Neptune Gremlin Client depends, automatically attempts to remediate connection issues. For example, if a host is considered unavailable, the driver will start a background task that tries to create a fresh connection. Because the driver handles reconnects automatically, if a connection issue occurs while your application is submitting a query, all your application has to do is backoff and retry the query.\n  - `ConcurrentModificationException` \u2013 The Neptune transaction semantics mean that concurrent transactions can sometimes fail with a `ConcurrentModificationException`. In these situations, an [exponential backoff-and-retry mechanism can help resolve collisions](https://docs.aws.amazon.com/neptune/latest/userguide/transactions-exceptions.html). \n  - `ConstraintViolationException` \u2013 This can sometimes occur if you attempt to create an edge between vertices that have only recently been committed. If one or other of the vertices is not yet visible to the current transaction, a `ConstraintViolationException` can occur. You can retry the query in the expectation that the necessary items will become visible.\n  - `ReadOnlyViolationException` \u2013 This occurs if the primary has failed over to another instance. By backing off and retrying the query, you give the Neptune Gremlin Client the opportunity to refresh its endpoint information.\n  \nThere are two places in your application where you should consider implementing a backoff-and-retry strategy:\n\n  - **Creating a `GremlinCluster` and `GremlinClient`** \u2013 In many applications you need create a `GremlinClient` only once, when the application starts. This client then lasts for the lifetime of the application. The situtaion is slightly different with serverless applications built using AWS Lambda functions. If you use Lambda functions to host your database access, each instance of a function will create its own `GremlinClient`.\n  - **Submitting a query** \u2013 If you do use a backoff-and-retry strategy to handle write request issues, consider implementing idempotent queries for create and update requests. \n  \n#### RetryUtils\n\nThe Neptune Gremlin Client includes a `RetryUtils` utility class with a `isRetryableException(Exception e)` method. The method encapsulates what we've learned running the Neptune Gremlin Client in long-running, high-throughput scenarios. The `Result` of this method indicates whether an exception represents a connection issue or query exception that would allow for an operation to be retried: \n\n\n```\ntry {\n    \n    // Your code\n    \n} catch (Exception e){\n    RetryUtils.Result result = RetryUtils.isRetryableException(e);\n    boolean isRetryable = result.isRetryable();\n}\n```\n\n#### Backoff and retry when creating a GremlinCluster and GremlinClient\n\nThe following example uses [Retry4j](https://github.com/elennick/retry4j) (which is included with the Neptune Gremlin Client) for retries, and `RetryUtils.isRetryableException()` for determining whether an exception represents a connection issue or query exception that would allow for an operation to be retried.\n\nIn this example, the `GremlinCluster`, `GremlinClient`, and `GraphTraversalSource` are created inside a `Callable`, which is executed by a Retry4j `CallExecutor`. The `Callable` returns a `ClusterContext` \u2013 a simple container object provided by the Neptune Gremlin Client to hold the cluster, client and traversal source. The `ClusterContext` implements `Autocloseable`, and its `close()` method closes both the client and the cluster.\n\nThe `ClusterEndpointsRefreshAgent` is created _outside_ the backoff-and-retry code. This allows a single `ClusterEndpointsRefreshAgent` to be used to populate multiple clusters with endpoint information. \n\n```\nString lambdaProxy = \"neptune-endpoints-info\";\n\nEndpointsSelector selector = EndpointsType.Primary;\n\nClusterEndpointsRefreshAgent refreshAgent =\n        ClusterEndpointsRefreshAgent.lambdaProxy(lambdaProxy);\n\nRetryConfig retryConfig = new RetryConfigBuilder()\n        .retryOnCustomExceptionLogic(\n                e -> RetryUtils.isRetryableException(e).isRetryable())\n        .withExponentialBackoff()\n        .withMaxNumberOfTries(5)\n        .withDelayBetweenTries(1, ChronoUnit.SECONDS)\n        .build();\n\nCallExecutor executor = new CallExecutorBuilder()\n        .config(retryConfig)\n        .build();\n\nStatus<ClusterContext> status = executor.execute((Callable<ClusterContext>) () -> {\n\n    // Create cluster, client, and graph traversal source\n\n    GremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n            .addContactPoints(refreshAgent.getEndpoints(selector))\n            .create();\n\n    GremlinClient client = cluster.connect();\n\n    DriverRemoteConnection connection = DriverRemoteConnection.using(client);\n    GraphTraversalSource g = AnonymousTraversalSource.traversal().withRemote(connection);\n\n    return new ClusterContext(cluster, client, g);\n});\n\nClusterContext clusterContext = status.getResult();\n\nrefreshAgent.startPollingNeptuneAPI(\n        RefreshTask.refresh(clusterContext.client(), selector),\n        15,\n        TimeUnit.SECONDS\n);\n\n// Use clusterContext.graphTraversalSource() for all queries (across threads)\n// And then, when the app closes...\n\nrefreshAgent.close();\nclusterContext.close();\n```\n\n#### Backoff and retry when submitting a query\n\nThe following example uses [Retry4j](https://github.com/elennick/retry4j) (which is included with the Neptune Gremlin Client) for retries, and `RetryUtils.isRetryableException()` for determing whether an exception represents a connection issue or query exception that would allow for an operation to be retried.\n\nIn this example, the query is constructed and submitted inside a `Callable`, which is executed by a Retry4j `CallExecutor`. The `GraphTraversalSource` has been created _prior_ to submitting any query (possibly using the [example code shown above](#backoff-and-retry-when-creating-a-gremlincluster-and-gremlinclient)), and can be reused across queries and threads. You should use a separate `CallExecutor` per thread, however.\n\nNote that with each retry, the code creates a new `Traversal` instance spawned from the `GraphTraversalSource`. In other words, retry the entire query.\n\n```\n//GraphTraversalSource can be provided by ClusterContext \u2013 see previous example\nGraphTraversalSource g = ...\n\nRetryConfig retryConfig = new RetryConfigBuilder()\n        .retryOnCustomExceptionLogic(\n                e -> RetryUtils.isRetryableException(e).isRetryable())\n        .withExponentialBackoff()\n        .withMaxNumberOfTries(5)\n        .withDelayBetweenTries(1, ChronoUnit.SECONDS)\n        .build();\n\n\n// You can reuse GraphTraversalSource and RetryConfig across threads,\n// but use a CallExecutor per thread.\n\nCallExecutor<Edge> executor = new CallExecutorBuilder<Edge>()\n        .config(retryConfig)\n        .build();\n\nCallable<Edge> query = () -> g.addV().as(\"v1\")\n        .addV().as(\"v2\")\n        .addE(\"edge\").from(\"v1\").to(\"v2\")\n        .next();\n\ntry {\n\n    Edge result = executor.execute(query).getResult();\n\n    // Do something with result\n\n} catch (RetriesExhaustedException e) {\n    // Attempted backoff and retry, but this has failed\n} catch (Exception e) {\n    // Handle unexpected exceptions\n}\n```\n\n### Connection timeouts \n\nWhenever you submit a Gremlin request to a `GremlinClient`, the client repeatedly tries to acquire a connection until it either succeeds, a `ConnectionException` occurs, or a timeout threshold is exceeded.\n\nThe `GremlinClient.chooseConnection()` method (which is invoked internally whenever the application submits a request via the client) respects the `maxWaitForConnection` value specified when you create a `GremlinCluster`. The following example creates a `GremlinClient` whose `chooseConnection()` method will throw a `TimeoutException`after 10 seconds if it can't acquire a connection:\n\n```\nGremlinCluster cluster = GremlinClusterBuilder.build()\n        .maxWaitForConnection(10000)\n        ...\n        .create();\n\nGremlinClient client = cluster.connect();\n```\n\nIf you don't specify a `maxWaitForConnection` value, the `GremlinCluster` uses a default value of `16,000` milliseconds.\n\nWhenever a `GremlinClient` attempts to acquire a connection, it iterates through the connection pools associated with the endpoints with which it has been configured, looking for the first healthy connection. By default, it waits 5 milliseconds between attempts to get a connection. You can configure this interval using the `acquireConnectionBackoffMillis()` builder method.\n\nIf you have [suspended the database endpoints](#suspending-endpoints-using-the-aws-lambda-proxy) (via a Lambda proxy), instead of throwing a `TimeoutException`, the client will throw an `EndpointsUnavailableException` after the `maxWaitForConnection` interval.\n\n#### Force refresh of endpoints when waiting to acquire a connection\n\nSometimes the reason the client is not able to acquire a connection is because it has a stale view of the cluster topology. In these circumstances, you may want the client to immediately refresh its view of the cluster topology, rather than wait for the refresh agent's next scheduled refresh. \n\nThe `NeptuneGremlinClusterBuilder` provides an `eagerRefreshWaitTimeMillis()` builder method that allows you to specify the maximum time the client will wait to acquire a connection before triggering an eager refresh of the endpoints. If you set `eagerRefreshWaitTimeMillis`, ensure the value is less than `maxWaitForConnection`. If you set `eagerRefreshWaitTimeMillis` greater than `maxWaitForConnection`, the client will simply throw a `TimeoutException` after the `maxWaitForConnection` interval. \n\nBy default, `eagerRefreshWaitTimeMillis` is not configured.\n\nIf you do configure `eagerRefreshWaitTimeMillis`, you must also supply an event handler using the `onEagerRefresh()` builder method. The handler is an implementation of the `OnEagerRefresh` interface. Its `getEndpoints()` method is passed an`EagerRefreshContext` (currently empty, but there to hold context information in future versions of the Neptune Gremlin Client) and must return an `EndpointCollection`.\n\nThe following example shows how to create a `GremlinClient` that will refresh its endpoints after 5 seconds have passed trying to acquire a connection:\n\n```\nEndpointsType selector = EndpointsType.ReadReplicas;\n\nClusterEndpointsRefreshAgent refreshAgent =\n        ClusterEndpointsRefreshAgent.lambdaProxy(lambdaProxy);\n        \nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .addContactPoints(refreshAgent.getEndpoints(selector))\n        .eagerRefreshWaitTimeMillis(5000)\n        .onEagerRefresh(ctx -> refreshAgent.getEndpoints(selector))\n        .maxWaitForConnection(20000)\n        .create();       \n \nGremlinClient client = cluster.connect();\n\nrefreshAgent.startPollingNeptuneAPI(\n        RefreshTask.refresh(client, selector),\n        60,\n        TimeUnit.SECONDS);\n```\n\nThe refresh agent here is configured to find the endpoint addresses of all database instances in a Neptune cluster that are currently acting as readers. The `NeptuneGremlinClusterBuilder` creates a `GremlinCluster` whose contact points (i.e. its endpoint addresses) are initialized via a first invocation of the refresh agent. But the builder also configures the client so that after 5 seconds have passed attempting to acquire a connection from its currently configured endpoint addresses, it refreshes those addresses, again using the agent. The client is also configured to timeout attempts to get a connection after 20 seconds. At the end of the example we also configure the refresh agent to refresh the `GremlinClient` every minute, irrespective of any failures or successes.\n\nWith this setup, then, the `GremlinClient` will refresh its endpoint addresses once every minute. It will also refresh its endpoints after 5 seonds have passed  attempting to get a connection. If any attempt to get a connection takes longer than 20 seconds, the client will throw a `TimeoutException`.\n\nThe `eagerRefreshWaitTimeMillis` value is evaluated on a per-request basis. However, a `GremlinClient` is capable of concurrently handling many requests. The client ensures that multiple eager refresh events cannot be triggered at the same time. Further, it imposes a backoff period between eager refresh events, so as to prevent the Neptune Management API or a Lambda proxy being overwhelmed with cluster topology requests. By default, this backoff period is 5 seconds. You can configure it using the `eagerRefreshBackoffMillis()` builder method. \n\n### Transactions\n\nThe Neptune Gremlin Client supports Gremlin transactions, as long as the transactions are issued against a writer endpoint:\n\n```\nEndpointsType selector = EndpointsType.ClusterEndpoint;\n\nClusterEndpointsRefreshAgent refreshAgent = \n        new ClusterEndpointsRefreshAgent(\"my-cluster-id\");\n\t\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .addContactPoints(refreshAgent.getEndpoints(selector))\n        .create();\n    \nGremlinClient client = cluster.connect();\n\nDriverRemoteConnection connection = DriverRemoteConnection.using(client);\n\nTransaction tx = traversal().withRemote(connection).tx();\n\nGraphTraversalSource g = tx.begin();\n\ntry {\n        String id1 = UUID.randomUUID().toString();\n        String id2 = UUID.randomUUID().toString();\n    \n        g.addV(\"testNode\").property(T.id, id1).iterate();\n        g.addV(\"testNode\").property(T.id, id2).iterate();\n        g.addE(\"testEdge\").from(__.V(id1)).to(__.V(id2)).iterate();\n        \n        tx.commit();\n\n} catch (Exception e) {\n    tx.rollback();\n}\n\n\nrefreshAgent.close();\nclient.close();\ncluster.close();\n```\n\nIf you attempt to issue transactions against a read replica, the client returns an error:\n\n```\norg.apache.tinkerpop.gremlin.driver.exception.ResponseException: {\"detailedMessage\":\"Gremlin update operation attempted on a read-only replica.\",\"requestId\":\"05074a8e-c9ef-42b7-9f3e-1c388cd35ae0\",\"code\":\"ReadOnlyViolationException\"}\n```\n\n### Migrating from version 1 of the Neptune Gremlin Client\n\nNeptune Gremlin Client 2.x.x includes the following breaking changes:\n\n  - The `EndpointsSelector.getEndpoints()` method now accepts a `NeptuneClusterMetadata` object and returns an `EndpointCollection` (version 1 returned a collection of String addresses).\n  - The `GremlinClient.refreshEndpoints()` method now accepts an `EndpointCollection` (version 1 accepted a collection of String addresses).\n  - `NeptuneInstanceProperties` has been renamed `NeptuneInstanceMetadata`.\n  - You can no longer supply a list of selectors when creating a `ClusterEndpointsRefreshAgent`: selectors are applied lazily whenever the agent is triggered.\n  - To supply an initial list of endpoints to the `NeptuneGremlinClusterBuilder.addContactPoints()` method, use `refreshAgent.getEndpoints()` with an appropriate selector (version 1 used `getAddresses()`).\n  - The `ClusterEndpointsRefreshAgent.startPollingNeptuneAPI()` now accepts a collection of `RefreshTask` objects. Each task encapsulates a client and a selector. This way, you can update multiple clients, each with its own selection logic, using a single refresh agent.\n  - The `NeptuneGremlinClusterBuilder` now uses `proxyPort()`, `proxyEndpoint()` and `proxyRemoveHostHeader()` builder methods to configure connections through a proxy (e.g. a load balancer). These methods replace `loadBalancerPort()`, `networkLoadBalancerEndpoint()` and `applicationLoadBalancerEndpoint()`.\n  - The `NeptuneGremlinClusterBuilder.refreshOnErrorThreshold()` and `NeptuneGremlinClusterBuilder.refreshOnErrorEventHandler()` builder methods have been replaced with `eagerRefreshWaitTimeMillis()` and `onEagerRefresh()`. Note that `refreshOnErrorThreshold()` specified a count of consecutive failure attempts, whereas `eagerRefreshWaitTimeMillis` specifies a wait time in milliseconds. When migrating, set `eagerRefreshWaitTimeMillis` equal to `refreshOnErrorThreshold * 5`. \n\nThe following example shows a solution built using Neptune Gremlin Client version 1.0.2:\n\n```\nString clusterId = \"my-cluster-id\";\n\nEndpointsSelector selector = (clusterEndpoint, readerEndpoint, instances) ->\n        instances.stream()\n                .filter(NeptuneInstanceProperties::isReader)\n                .filter(i -> i.hasTag(\"workload\", \"analytics\"))\n                .filter(NeptuneInstanceProperties::isAvailable)\n                .map(NeptuneInstanceProperties::getEndpoint)\n                .collect(Collectors.toList());\n\nClusterEndpointsRefreshAgent refreshAgent = new ClusterEndpointsRefreshAgent(\n        clusterId,\n        selector);\n\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .enableIamAuth(true)\n        .addContactPoints(refreshAgent.getAddresses().get(selector))\n        .refreshOnErrorThreshold(1000)\n        .refreshOnErrorEventHandler(() -> refreshAgent.getAddresses().get(selector))\n        .create();\n\nGremlinClient client = cluster.connect();\n\nrefreshAgent.startPollingNeptuneAPI(\n        (OnNewAddresses) addresses -> client.refreshEndpoints(addresses.get(selector)),\n        60,\n        TimeUnit.SECONDS);\n\nDriverRemoteConnection connection = DriverRemoteConnection.using(client);\nGraphTraversalSource g = AnonymousTraversalSource.traversal().withRemote(connection);\n\nfor (int i = 0; i < 100; i++) {\n    List<Map<Object, Object>> results = g.V().limit(10).valueMap(true).toList();\n    for (Map<Object, Object> result : results) {\n        //Do nothing\n    }\n}\n\nrefreshAgent.close();\nclient.close();\ncluster.close();\n```\n\nThe code below shows the solution updated to Neptune Gremlin Client version 2.x.x:\n\n```\nString lambdaProxyName = \"neptune-endpoints-info\";\n\n// Selector accepts NeptuneClusterMetadata and returns EndpointCollection\n// NeptuneInstanceProperties renamed to NeptuneInstanceMetadata\nEndpointsSelector selector = (cluster) ->\n        new EndpointCollection(\n                cluster.getInstances().stream()\n                .filter(NeptuneInstanceMetadata::isReader)\n                .filter(i -> i.hasTag(\"workload\", \"analytics\"))\n                .filter(NeptuneInstanceMetadata::isAvailable)\n                .collect(Collectors.toList())\n        );\n        \n\n// Prefer AWS Lambda proxy \u2013 no selector passed to factory method\nClusterEndpointsRefreshAgent refreshAgent = \n        ClusterEndpointsRefreshAgent.lambdaProxy(lambdaProxyName);\n \n// Use refreshAgent.getEndpoints(selector) to populate contact points\nGremlinCluster cluster = NeptuneGremlinClusterBuilder.build()\n        .enableIamAuth(true)\n        .addContactPoints(refreshAgent.getEndpoints(selector))\n        .eagerRefreshWaitTimeMillis(5000) // 1000 * 5 millis\n        .onEagerRefresh(() -> refreshAgent.getEndpoints(selector))\n        .create();\n\nGremlinClient client = cluster.connect();\n\n// Accepts single RefreshTask or collection of RefreshTasks\nrefreshAgent.startPollingNeptuneAPI(\n        RefreshTask.refresh(client, selector),\n        60,\n        TimeUnit.SECONDS);\n\nDriverRemoteConnection connection = DriverRemoteConnection.using(client);\nGraphTraversalSource g = AnonymousTraversalSource.traversal().withRemote(connection);\n\nfor (int i = 0; i < 100; i++) {\n    List<Map<Object, Object>> results = g.V().limit(10).valueMap(true).toList();\n    for (Map<Object, Object> result : results) {\n        //Do nothing\n    }\n}\n\nrefreshAgent.close();\nclient.close();\ncluster.close();\n```\n\nThe revised version above shows using an AWS Lambda proxy to refresh the endpoints. If you prefer to keep using the Neptune Management API directly, you can create a `ClusterEndpointsRefreshAgent` like this:\n\n```\nClusterEndpointsRefreshAgent refreshAgent = \n        ClusterEndpointsRefreshAgent.managementApi(clusterId);\n```\n\n## Demos\n \nThe demo includes several sample scenarios. To run them, compile the `gremlin-client-demo.jar` and then install it on an EC2 instance that allows connections to your Neptune cluster.\n\nAll of the demos use a `ClusterEndpointsRefreshAgent` to get the database cluster topology and refresh the endpoints in a `GremlinClient`. If you supply a `--cluster-id` parameter, the refresh agent will [query the Neptune Management API directly](#using-a-clusterendpointsrefreshagent-to-query-the-neptune-management-api-directly). If you supply a `--lambda-proxy` name instead, the refresh agent will [query an AWS Lambda proxy](#using-an-aws-lambda-proxy-to-retrieve-cluster-topology) for endpoint information (you will need to [install a Lambda proxy first](#installing-the-neptune-endpoints-info-aws-lambda-function)).\n\nIf you use a `--cluster-id` parameter, the identity under which you're running the demo must be authorized to perform `rds:DescribeDBClusters` for your Neptune cluster, and `rds:DescribeDBInstances` and `rds:ListTagsForResource` for `db:*`.\n\nIf you use a `--lambda-proxy` parameter, the identity under which you're running the demo must be authorized to perform `lambda:InvokeFunction` for the proxy Lambda function. \n\n### CustomSelectorsDemo\n\nThis demo shows how to use [custom `EndpointSelector` implementations](#endpointsselector) to filter the cluster topology for writer and reader endpoints. It uses a single `ClusterTopologyRefreshAgent` to refresh both a writer `GremlinClient` and a reader `GremlinClient`. \n\n```\njava -jar gremlin-client-demo.jar custom-selectors-demo \\\n  --cluster-id <my-cluster-id> \n```\n\n### RefreshAgentDemo\n \nThis demo uses a `ClusterTopologyRefreshAgent` to query for the current cluster topology every 15 seconds. The `GremlinClient` adapts accordingly.\n \n```\njava -jar gremlin-client-demo.jar refresh-agent-demo \\\n  --cluster-id <my-cluster-id>\n```\n \nWith this demo, try triggering a failover in the cluster. After approx 15 seconds you should see a new endpoint added to the client, and the old endpoint removed. While the failover is occurring, you may see some queries fail: I've used a simple exception handler to log these errors.\n\n### RetryDemo\n \nThis demo uses [Retry4j](https://github.com/elennick/retry4j) and `RetryUtils.isRetryableException()` to wrap the creation of a `GremlinCluster` and `GremlinClient`, and the submission of individual queries, with a backoff-and-retry strategy.\n \n```\njava -jar gremlin-client-demo.jar retry-demo \\\n  --cluster-id <my-cluster-id>\n```\n \nWith this demo, try triggering a failover in the cluster. After approx 15 seconds you should start seeing some exceptions and retry attempts in the console. Most of the operations should succeed after one or more retries. Some small number may fail after the maximum number of retries.\n \n\n### TxDemo\n \nThis demo demonstrates using the Neptune Gremlin client to issue transactions.\n \n```\njava -jar gremlin-client-demo.jar tx-demo \\\n  --cluster-id <my-cluster-id>\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2023-12-14T14:26:22Z", "2023-09-06T08:32:40Z", "2023-07-19T09:33:12Z", "2023-07-12T09:29:22Z", "2023-06-02T14:01:36Z", "2023-05-03T10:44:53Z"]}, {"name": "obs-cdi", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "\ufeffBuild Instructions for OBS Studio CDI Plugin\n---\n\nThese are the build instructions for the OBS Studio CDI plugin.\u00a0The broad steps are to build CDI, build OBS Studio, then build the OBS Studio CDI plugin.\u00a0You must build OBS Studio from source in order to have a development environment with all the needed headers. This plugin will enable CDI as an OBS Studio source and output.\n\n**OBS Studio 30.0.2** is the latest version this plugin supports. Newer versions may require changes to the information in this guide and/or the plugin.\n\n**CDI outputs**: For YCbCr outputs, OBS Studio's pixel format must be set to ```I444```. For RGB outputs, the pixel format must be set to ```BGRA (8-bit)```. Audio is supported from 1-8 channels, as limited by OBS Studio. We have tested this plugin with various frame rates and raster sizes but find that 1080p60 performs the best.\n\n**CDI sources**: No additional configuration of OBS Studio is required. Up to 8 audio channels are supported, as limited by OBS Studio.\n\n<!-- @import \"[TOC]\" {cmd=\"toc\" depthFrom=1 depthTo=6 orderedList=false} -->\n\n<!-- code_chunk_output -->\n\n- [Installing on Windows](#installing-on-windows)\n  - [Launch Windows EC2 Instance](#launch-windows-ec2-instance)\n  - [Download and Build AWS CDI-SDK and Dependencies on Windows](#download-and-build-aws-cdi-sdk-and-dependencies-on-windows)\n  - [Download and Build OBS Studio and Dependencies on Windows](#download-and-build-obs-studio-and-dependencies-on-windows)\n    - [Download and Build the OBS Studio CDI Plugin on Windows](#download-and-build-the-obs-studio-cdi-plugin-on-windows)\n- [Debuggging the OBS Studio CDI Plugin on Windows](#debuggging-the-obs-studio-cdi-plugin-on-windows)\n- [Installing on Linux](#installing-on-linux)\n  - [Configure Rocky Linux 9 for GUI](#configure-rocky-linux-9-for-gui)\n  - [Install NVIDIA driver on Linux](#install-nvidia-driver-on-linux)\n  - [Install NiceDCV for remote GUI access on Linux](#install-nicedcv-for-remote-gui-access-on-linux)\n  - [Download and Build AWS CDI-SDK and Dependencies on Linux](#download-and-build-aws-cdi-sdk-and-dependencies-on-linux)\n  - [Download and Build OBS Studio and Dependencies on Linux](#download-and-build-obs-studio-and-dependencies-on-linux)\n    - [Install OBS Studio dependencies](#install-obs-studio-dependencies)\n  - [Download and Build the OBS Studio CDI Plugin on Linux](#download-and-build-the-obs-studio-cdi-plugin-on-linux)\n    - [Install plugin dependencies](#install-plugin-dependencies)\n    - [Optional: install Visual Studio Code](#optional-install-visual-studio-code)\n    - [Download plugin](#download-plugin)\n    - [Build plugin](#build-plugin)\n  - [Debugging the OBS Studio CDI Plugin on Linux](#debugging-the-obs-studio-cdi-plugin-on-linux)\n- [CDI Output Configuration](#cdi-output-configuration)\n  - [CDI Source Configuration](#cdi-source-configuration)\n  - [OBS Studio CDI Plugin Logging](#obs-studio-cdi-plugin-logging)\n\n<!-- /code_chunk_output -->\n\n# Installing on Windows\n\n## Launch Windows EC2 Instance\n\n**Note**: These steps have been verified to work for both Debug and Release builds.\n\nLaunch an EC2 instance with EFA as the primary network adapter. A minimum of using a c5n.8xlarge or g4dn.8xlarge is currently required.\n\n-   Use Windows Server 2019 base AMI\n-   Use public IP\n-   Use at least 100GB for EBS volume due to size of OBS/QT install\n\nConfigure security settings for Windows RDP and use RDP client to connect to the instance.\n\n## Download and Build AWS CDI-SDK and Dependencies on Windows\n\nFollow the instructions at\u00a0[INStALL_GUIDE_WINDOWS.md](https://github.com/aws/aws-cdi-sdk/blob/mainline/INSTALL_GUIDE_WINDOWS.md)\u00a0with the following changes:\n\nInstall Microsoft Visual Studio 2022 instead of 2019. To install use Chocolatey from Powershell using:\n\n```choco install visualstudio2022-workload-nativedesktop -y```\n\nAfter you have downloaded the aws-cdi-sdk, and before running the ```install.ps1``` script, disable the CloudWatch metrics in ```aws-cdi-sdk/src/cdi/configuration.h``` by commenting out lines for defining ```CLOUDWATCH_METRICS_ENABLED``` and `METRICS_GATHER_SERVICE_ENABLED`. After commenting out, those lines should look like:\n\n```\n//#define CLOUDWATCH_METRICS_ENABLED\n//#define METRICS_GATHERING_SERVICE_ENABLED\n```\n\nThen, follow the remaining steps to build and install the AWS CDI-SDK using ```install.ps1```. The script will download dependencies, install the EFA driver and build the CDI-SDK.\n\nThe OBS Studio CDI plugin requires the ```Debug_DLL``` or ```Release_DLL``` variant of the AWS CDI-SDK.\n\nTo build it, Use Visual Studio to open the ```aws-cdi-sdk/proj/cdi_proj.sln``` Visual Studio solution file. For build type select either ```Debug_DLL``` or ```Release_DLL``` from the dropdown and then use Build \u2192 Build Solution to build it.\n\n## Download and Build OBS Studio and Dependencies on Windows\n\nFollow the instructions at\u00a0[Windows build directions](https://obsproject.com/wiki/install-instructions#windows-build-directions). A few additional notes are below:\n\nWhen creating the OBS Studio Visual Studio project files, use a **Visual Studio Powershell** to run these commands:\n\n ```\n cd obs-studio\ncmake --preset windows-x64\n ```\n\nBuild OBS Studio as a ```Debug``` or ```Release``` build: Build \u2192 Build Solution\n\n**NOTE**: Not all sub-projects always build successfully, but the necessary ones do. Look at the output tab when building is done to verify this. Then, run OBS Studio which is located at ```build_x64/rundir/Debug/bin/64bit/obs64.exe``` or ```build_x64/rundir/Release/bin/64bit/obs64.exe``` to verify it built properly.\n\n### Download and Build the OBS Studio CDI Plugin on Windows\n\nIn a **Visual Studio Powershell**, navigate to the location you would like the OBS CDI plugin to download to. Download the OBS Studio CDI plugin repository using:\n\n```\ngit clone https://github.com/aws/obs-cdi.git\n```\n\nThen, use the commands shown below to generate the Visual Studio solution and project files. Set **CDI_DIR** to that path where the CDI-SDK was installed and set **CMAKE_INSTALL_PREFIX** to the path where the built OBS Studio was installed. Default paths are shown.\n\n```\ncd obs-cdi\nmkdir build\ncmake -B ./build --preset windows-x64 -DCDI_DIR=\"C:/CDI/aws-cdi-sdk\" -DCMAKE_INSTALL_PREFIX=\"C:/obs-studio\"\n```\n\nIf successful, output should look something like:\n\n```\n-- Build files have been written to: C:/obs-cdi/build\n```\n\nUsing Visual Studio, open the solution file ```obs-cdi.sln``` that was generated in the build folder.\n\nBuild the plugin as a ```Debug``` or ```Release``` build: Build \u2192 Build Solution\n\n**Note: For performance reasons, the Debug variant is not able to support 1080p@60 with bit depths greater than 8-bits. However, the Release variant does support all bit depths up to and including 1080p@60.**\n\nNote: The project includes a post build script that copies all the necessary files into the right places in the OBS Studio rundir.\n\nIn Windows firewall allowed applications, allow the ```obs64.exe``` executable.\n\n# Debuggging the OBS Studio CDI Plugin on Windows\n\nOpen the obs-cdi Visual Studio solution in Visual Studio. The default execution target is ```ALL_BUILD```.\n\nIn the Solution Explorer window, expand the ```CmakePredefinedTargets``` and right-click on the ```ALL_BUILD``` project. Select ```Properties```. On the ```ALL_BUILD``` Propety Pages window, select ```Configuration Properties``` and change the settings shown below to point to where you installed OBS (examples shown):\n\nDebug Command:\n ```C:/obs-studio/build_x64/rundir/Debug/bin/64bit/obs64.exe```\n\nWorking Directory:\n ```C:/obs-studio/build_x64/rundir/Debug/bin/64bit```\n\nYou can now set breakpoints and launch OBS Studio, which will load the CDI plugin, from within Visual Studio.\n\n# Installing on Linux\n\n**Note**: These steps have been verified to work for both Debug and Release builds on **Rocky Linux 9**.\n\nLaunch an EC2 instance with EFA as the primary network adapter. A minimum of using a g4dn.8xlarge is currently required.\n\n-   Use Rocky Linux 9 base AMI\n-   Use public IP\n-   Use at least 100GB for EBS volume due to size of OBS/QT install\n\n## Configure Rocky Linux 9 for GUI\n\nssh to the instance using \"rocky\" as username and the key file used when the instance was created. Then run these steps:\n\n```\nsudo dnf upgrade -y\nsudo dnf groupinstall \"Server with GUI\" -y\nsudo systemctl set-default graphical\n\n# Create a password for rocky user:\nsudo passwd rocky\n```\n\n## Install NVIDIA driver on Linux\n\nSteps shown below were create using [this guide](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-nvidia-driver.html).\n\n**Install dependencies**\n\n```\nsudo dnf install -y make gcc kernel-devel kernel-headers elfutils-libelf-devel elfutils-devel libglvnd-devel.x86_64\nsudo dnf config-manager --set-enabled crb\nsudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm\nsudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-next-release-latest-9.noarch.rpm\nsudo dnf install -y wget java-17-openjdk java-17-openjdk-devel apr apr-util mesa-libGLU xcb-util-image xcb-util-keysyms xcb-util-renderutil xcb-util-wm\n```\n\n**Blacklist the Nouveau driver**\n```\ncat << EOF | sudo tee --append /etc/modprobe.d/blacklist-nouveau.conf\nblacklist nouveau\noptions nouveau modeset=0\nEOF\n\nsudo touch /etc/modprobe.d/blacklist-nouveau.conf\nsudo echo 'omit_drivers+=\" nouveau \"' | sudo tee --append /etc/dracut.conf.d/blacklist-nouveau.conf > /dev/null\nsudo dracut --regenerate-all --force\n\n# Enable console mode and reboot.\nsudo systemctl set-default multi-user.target\nsudo reboot\n```\n\n**Download and install the NVIDIA driver**\n\nInstall **aws cli** using [this guide](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html). Then download the latest driver and install using these commands:\n\n```\naws s3 cp --recursive s3://ec2-linux-nvidia-drivers/latest/ .\nchmod +x NVIDIA-Linux-x86_64*.run\nsudo /bin/sh ./NVIDIA-Linux-x86_64*.run\n\n# Enable graphical mode and configure NVIDIA.\nsudo systemctl set-default graphical.target\n```\n\nOptionally, instead of installing **aws cli** you can use **wget** to download the driver. To find the filename of the latest driver use [this link](\"https://s3.amazonaws.com/ec2-linux-nvidia-drivers/\").\n\nThen, look for **\\<Key\\>latest/**. An example is shown below that shows the key and the command used to download the driver:\n\n```\n<Key>latest/NVIDIA-Linux-x86_64-550.54.14-grid-aws.run</Key>\n\nwget \"s3://ec2-linux-nvidia-drivers/latest/NVIDIA-Linux-x86_64-550.54.14-grid-aws.run\"\n```\n\n**Disable GSP**\n\n```\nsudo touch /etc/modprobe.d/nvidia.conf\nsudo echo \"options nvidia NVreg_EnableGpuFirmware=0\" | sudo tee --append /etc/modprobe.d/nvidia.conf\n\nsudo nvidia-persistenced\nsudo nvidia-smi -ac 5001,1590\nsudo reboot\n```\n\n## Install NiceDCV for remote GUI access on Linux\n\n**Note**: Using Windows RDP is not recommended. I was not able to get to work correctly with OBS Studio's source rendering window (it would not render).\n\nThe install steps shown below were based on the [DCV Linux Install Guide](https://docs.aws.amazon.com/dcv/latest/adminguide/setting-up-installing-linux.html) to install NiceDCV for **console** sessions.\n\n**Note**: Must enable S3 for dcv-license by adding permission to AMI associated with the instance. Details are [here](https://docs.aws.amazon.com/dcv/latest/adminguide/setting-up-license.html).\n\nUpdate the security group settings to allow incoming TCP traffic on port 8443.\n\n**Configure firewall**\n\nCan either disable the firewall or allow TCP traffic on port 8443.\n\n```\n# Disable firewall:\nsudo setenforce 0\nsudo sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/sysconfig/selinux\nsudo sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config\nsudo systemctl stop firewalld\nsudo systemctl disable firewalld\n\n# Or allow TCP traffic on port 8443:\nsudo firewall-cmd --permanent --add-port=3389/tcp\nsudo firewall-cmd --reload\n```\n\n**Configure X Server**\n```\nsudo systemctl set-default graphical.target\nsudo yum install glx-utils\nsudo nvidia-xconfig --preserve-busid --enable-all-gpus\nsudo rm -rf /etc/X11/XF86Config*\nsudo systemctl isolate multi-user.target\nsudo systemctl isolate graphical.target\n```\n\n**Install the NiceDCV server**\n\n```\nsudo rpm --import https://d1uj6qtbmh3dt5.cloudfront.net/NICE-GPG-KEY\nwget https://d1uj6qtbmh3dt5.cloudfront.net/nice-dcv-el9-x86_64.tgz\ntar -xvzf nice-dcv-el9-x86_64.tgz\ncd nice-dcv-2023.1-16388-el9-x86_64\nsudo yum install nice-dcv-server*.rpm\nsudo systemctl isolate multi-user.target\nsudo systemctl isolate graphical.target\n```\n\n**Start the NiceDCV server**\n```\nsudo systemctl start dcvserver\nsudo systemctl enable dcvserver\n```\n\n**Create a DCV console session**\n\nUse the **rocky** user to create a session either [automatically on startup](https://docs.aws.amazon.com/dcv/latest/adminguide/managing-sessions-start.html#managing-sessions-start-auto) or manually using the steps below:\n\n```\nsudo dcv create-session console --type console --owner rocky\ndcv list-sessions\n```\n\n**Connect from client**\n\nAfter all the steps above were completed, I had to then re-run these commands (once) on the instance for the connection to work:\n```\nsudo systemctl isolate multi-user.target\nsudo systemctl isolate graphical.target\n```\n\nInstall a NiceDCV client on your client system and then, use the connection string below to connect to the instance, replacing **remote_ip** with the IP address of your instance:\n```\n<remote_ip>:8443?transport=auto#console\n```\n\n**Note**: When prompted, use \"rocky\" as the user. For second part of login, can select **Other User** and use the \"rocky\" user's password.\n\n## Download and Build AWS CDI-SDK and Dependencies on Linux\n\nFollow the instructions at\u00a0[INSTALL_GUIDE_LINUX.md](https://github.com/aws/aws-cdi-sdk/blob/mainline/INSTALL_GUIDE_LINUX.md)\u00a0with the following changes:\n\nSkip the **Install AWS CloudWatch and AWS CLI** section.\n\nFor the **Build CDI libraries and test applications** section, replace the build steps with the commands shown below. This will build a debug variant with CloudWatch metrics gathering disabled.\n\n```\ncd aws-cdi-sdk/\nmake DEBUG=y NO_MONITORING=y\n```\n\n## Download and Build OBS Studio and Dependencies on Linux\n\nThe steps below for **Rocky Linux 9** were created using the instructions at\u00a0[Red Hat-based directions](https://github.com/obsproject/obs-studio/wiki/build-instructions-for-linux#red-hat-based).\n\n\n### Install OBS Studio dependencies\n\n```\n# Install rpm fusion\nsudo dnf install --nogpgcheck https://dl.fedoraproject.org/pub/epel/epel-release-latest-$(rpm -E %rhel).noarch.rpm\nsudo dnf install --nogpgcheck https://mirrors.rpmfusion.org/free/el/rpmfusion-free-release-$(rpm -E %rhel).noarch.rpm\nsudo dnf install --nogpgcheck https://mirrors.rpmfusion.org/nonfree/el/rpmfusion-nonfree-release-$(rpm -E %rhel).noarch.rpm\n\n# Install dev tools\nsudo dnf groupinstall \"Development Tools\" -y\nsudo dnf install alsa-lib-devel asio-devel cmake ffmpeg-free-devel fontconfig-devel freetype-devel gcc gcc-c++ gcc-objc git glib2-devel json-devel libavcodec-free-devel libavdevice-free-devel libcurl-devel libdrm-devel libglvnd-devel -y\n\n# This package was not found, so used RPM.\n# sudo dnf install libjansson-devel\nwget https://dl.rockylinux.org/pub/rocky/9/devel/x86_64/os/Packages/j/jansson-devel-2.14-1.el9.x86_64.rpm\nsudo dnf install jansson-devel-2.14-1.el9.x86_64.rpm -y\n\nsudo dnf install libuuid-devel libva-devel libv4l-devel libX11-devel libXcomposite-devel libXinerama-devel luajit-devel mbedtls-devel pciutils-devel pciutils-devel pipewire-devel pulseaudio-libs-devel python3-devel -y\nsudo dnf install qt6-qtbase-devel qt6-qtbase-private-devel qt6-qtsvg-devel qt6-qtwayland-devel -y\n\n# This package was not found and I could not find a RPM. Doesn't seem to be required.\n# sudo dnf install qt6-qtx11extras-devel -y\n\nsudo dnf install speexdsp-devel swig systemd-devel vlc-devel wayland-devel websocketpp-devel x264-devel -y\n\n# Additional packages that I had to install.\nsudo dnf install libxkbcommon-devel libqrcodegencpp-devel oneVPL-devel srt-devel librist-devel -y\n```\n\n**Upgrade from default cmake is required**\n\nHad to upgrade to a newer version of cmake. Default version is 3.20, while OBS Studio requires 3.22 or later.\n\n```\nwget https://github.com/Kitware/CMake/releases/download/v3.28.3/cmake-3.28.3.tar.gz\ntar -xvf cmake*.gz\ncd cmake-3.28.3\n./bootstrap\nmake -j10\nsudo make install -j10\n\n# Add this to ~/.bashrc so the new cmake is used by default.\nexport PATH=\"/usr/local/bin:$PATH\"\n```\n\n**Download and build OBS Studio**\nOBS Studio version 30.0.2 was tested. Newer versions may require addtional changes to this document and files. Download the files and checkout version 30.0.2 using:\n\n```\ngit clone --recursive https://github.com/obsproject/obs-studio.git\ncd obs-studio\ngit checkout -b 30.0.2 30.0.2\n```\n\nTo build, modify the **CMAKE_INSTALL_PREFIX** path shown below as desired. **Note**: You must use the same path when building the OBS Studio CDI plugin.\n\n```\nmkdir build && cd build\ncmake -DLINUX_PORTABLE=ON -DCMAKE_INSTALL_PREFIX=\"${HOME}/obs-studio-portable\" -DENABLE_BROWSER=OFF -DENABLE_AJA=OFF -DENABLE_WEBRTC=OFF -DENABLE_WEBSOCKET=OFF -DCMAKE_BUILD_TYPE=Debug ..\nmake -j10\nmake install\n```\n\nEnsure OBS Studio runs before building and installing the OBS Studio CDI plugin. The default path is shown in the example below:\n```\ncd ~/obs-studio-portable/bin/64bit\n./obs\n```\n\n\n## Download and Build the OBS Studio CDI Plugin on Linux\n\n### Install plugin dependencies\n\n```\nsudo dnf install ninja-build\n```\n\n### Optional: install Visual Studio Code\n\nIf you prefer to use Visual Studio Code to build and debug the plugin, you can install it using [this guide](https://code.visualstudio.com/docs/setup/linux). A Visual Studio Code workspace file **obs-cdi.code-workspace** and associated **.vscode** folder are included with the plugin.\n\n### Download plugin\n\nIn a terminal, navigate to the location you would like the OBS CDI plugin to download to. Download the OBS Studio CDI plugin repository using:\n\n```\ngit clone https://github.com/aws/obs-cdi.git\n```\n\n### Build plugin\n\nTo generate the makefiles used to build a Debug variant of the plugin, use the commands shown below. Set **CDI_DIR** to that path where the CDI-SDK was installed and set **CMAKE_INSTALL_PREFIX** to the path where the built OBS Studio was installed. Default paths are shown.\n\n```\nmkdir build\ncmake -B ./build --preset linux-x86_64 -DCMAKE_BUILD_TYPE=Debug -DCDI_DIR=\"/home/rocky/CDI/aws-cdi-sdk\" -DCMAKE_INSTALL_PREFIX=\"/home/rocky/obs-studio-portable\"\n```\n\nIf successful, output should look something like:\n\n```\n-- Build files have been written to: /home/rocky/obs-cdi/build\n```\n\nNow, build the plugin and install in OBS Studio using:\n\n```\ncd build\nninja\nninja install\n```\n\n**Note**: You can also use Visual Code to perform these steps. Open the **obs-cdi.code-workspace** Visual Code workspace file and set **CDI_DIR** and **CMAKE_INSTALL_PREFIX** using **Terminal->Configure Task**. This will open the **launch.json** file so you can edit it.\n\n## Debugging the OBS Studio CDI Plugin on Linux\n\nOpen the **obs-cdi.code-workspace** Visual Code workspace file in Visual Code.\n\nSelect **Run->Open Configurations**. The **launch.json** file will be displayed in the editor. For **\"program\"**, set the full path to the obs binary and set **\"cwd\"** to the obs folder. For example:\n\n```\n    \"program\": \"/home/rocky/obs-studio-portable/bin/64bit/obs\",\n    ...\n    \"cwd\": \"/home/rocky/obs-studio-portable/bin/64bit\",\n```\n\nYou can now set breakpoints and launch OBS Studio, which will load the CDI plugin, from within Visual Code.\n\n# CDI Output Configuration\n\nBefore turning on the CDI output, make sure the OBS Studio video and audio settings are compatible with the plugin.\n\nSettings \u2192 Video\n```\nBase (Canvas) Resolution \u2192 1920x1080\nOutput (Scaled) Resolution \u2192 1920x1080\nFPS \u2192 60\n```\n\nSettings \u2192 Advanced \u2192 Video\n```\nColor Format \u2192 \"I444\" for YCbCr output or \"BGRA (8-bit)\" for RGB output\nColor Space \u2192 709\nColor Range \u2192 Full\n```\n\nSettings \u2192 Audio\n```\nSample Rate \u2192 48khz\nChannels \u2192 Default is Stereo, but supports all the channel formats\n```\n\nTo configure your CDI settings, use Menu \u2192 Tools \u2192 AWS CDI Output Settings\n\n```\nMain Output Name - Name for the output - defaults to \u201cOBS\u201d\nDestination IP - the IP address of your CDI receiver\nDestination Port - the destination port of your CDI receiver\nLocal EFA Adapter IP - your local IP address assigned to the EFA adapter\u00a0\nVideo Stream ID - CDI video stream identifier (0-65535). Default is 1.\nAudio Stream ID - CDI audio stream identifier (0-65535). Default is 2.\nVideo Sampling - YCbCr 4:2:2, 4:4:4 and RGB.\nRGB Alpha Used - Only available for RGB output.\nBit Depth - 8, 10 and 12-bit.\n```\n\n## CDI Source Configuration\n\nUse the CDI Source Properties to set the following configuration settings:\n\n```\nLocal EFA Adapter IP - The local IP address assigned to the EFA adapter\nLocal Bind IP - If using a single adapter, leave blank. Otherwise  the IP address of the adapter to bind to\nPort - The port to listen to for the CDI connection\nEnable Audio - Check to enable audio (default is enabled)\n```\n\n## OBS Studio CDI Plugin Logging\n\n**NOTE**: This plugin will generate log messages in the default OBS log folder located in Windows at ```C:\\Users\\<username>\\AppData\\Roaming\\OBS\\logs``` and Linux at ```~/.config/obs-studio/logs```. This log file can get very large if there is not a valid CDI target to connect to.\u00a0 The log will fill with messages about trying to connect, so it is recommended your CDI source and/or receiver is setup before selecting the OBS Studio CDI source or enabling the OBS Studio CDI output.\n", "release_dates": []}, {"name": "opsworks-cookbooks", "description": "Chef Cookbooks for the AWS OpsWorks Service", "language": "Ruby", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "opsworks-cookbooks\n==================\n\n**This repo contains cookbooks used by AWS OpsWorks for Chef versions 11.10, 11.4 and 0.9.**\n\nTo get started with AWS OpsWorks cookbooks for all versions of Chef see the [cookbook documentation](https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook.html).\n\nIf you want to override any template (like the Rails database.yml or the Apache\nvhost definition), this is the place to look for the originals.\n\nDo not reuse built-in cookbook names for custom or community cookbooks. Custom\ncookbooks that have the same name as built-in cookbooks might fail.\n\nChef version 12\n------------------------------------\n\n**For Chef 12.2 Windows and Chef 12 Linux there are no built-in cookbooks**\n\nChef versions 11.10, 11.4 and 0.9\n----------------------------------\n\nThese branches contain the cookbooks that are used by official OpsWorks releases:\n\n- **Chef 11.10**: [release-chef-11.10](https://github.com/aws/opsworks-cookbooks/tree/release-chef-11.10)\n- **Chef 11.4**: [release-chef-11.4](https://github.com/aws/opsworks-cookbooks/tree/release-chef-11.4) (deprecated)\n- **Chef 0.9**: [release-chef-0.9](https://github.com/aws/opsworks-cookbooks/tree/release-chef-0.9) (deprecated)\n\nThese branches reflect the upcoming changes for the next release:\n\n- **Chef 11.10**: [master-chef-11.10](https://github.com/aws/opsworks-cookbooks/tree/master-chef-11.10)\n- **Chef 11.4**: [master-chef-11.4](https://github.com/aws/opsworks-cookbooks/tree/master-chef-11.4) (deprecated)\n- **Chef 0.9**: [master-chef-0.9](https://github.com/aws/opsworks-cookbooks/tree/master-chef-0.9) (deprecated)\n\nThe `master` branch is not used since AWS OpsWorks supports multiple configuration managers.\n\nSee also <https://aws.amazon.com/opsworks/>\n\nLICENSE: Unless otherwise stated, cookbooks/recipes originated by Amazon Web Services are licensed\nunder the [Apache 2.0 license](http://aws.amazon.com/apache2.0/). See the LICENSE file. Some files\nare just imported and authored by others. Their license will of course apply.\n", "release_dates": []}, {"name": "ota-for-aws-iot-embedded-sdk", "description": null, "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# AWS IoT Over-the-air Update Library\n\n## Getting Started With OTA\nAs mentioned in the previous 'Upcoming changes' section, the new \"core\" OTA\nlibraries have been released. These modular and composable libraries can be\nutilized to implement an OTA 'orchestrator' which sequences the libraries to\nachieve Over-The-Air Update functionality. The composable nature of the OTA\norchestrator will allow for new backing services, both supported by AWS and\nnot.\n\n**This library will remain available however it will not be developed further.** Support will instead be focused on\nthe new composable libraries and example orchestrators.\n\nFor more information, see the following:\n1. FreeRTOS.org webpage explaining the [modular OTA concept](https://www.freertos.org/freertos-core/over-the-air-updates/index.html)\n1. Example: [Simple OTA Orchestrator](https://www.freertos.org/freertos-core/over-the-air-updates/mqtt-simple-orchestrator.html)\n2. Example: [OTA Agent Orchestrator](https://www.freertos.org/freertos-core/over-the-air-updates/mqtt-ota-agent-orchestrator.html)\n    1. This one is written to ease the transition of applications using this SDK.\n\nAnd for the composable libraries, see:\n1. [Jobs Library](https://github.com/aws/Jobs-for-AWS-IoT-embedded-sdk/tree/main) which also contains additional support for [AWS IoT OTA jobs]()\n2. [MQTT Streaming Library](https://github.com/aws/aws-iot-core-mqtt-file-streams-embedded-c) for file block downloads over MQTT\n3. [coreHTTP](https://github.com/FreeRTOS/coreHTTP) for file block downloads over HTTP\n\n## Description\n\n**[API Documentation Pages for current and previous releases of this library can be found here](https://aws.github.io/ota-for-aws-iot-embedded-sdk/)**\n\nThe OTA library enables you to manage the notification of a newly available\nupdate, download the update, and perform cryptographic verification of the\nfirmware update. Using the library, you can logically separate firmware updates\nfrom the application running on your devices. The OTA library can share a\nnetwork connection with the application, saving memory in resource-constrained\ndevices. In addition, the OTA library lets you define application-specific logic\nfor testing, committing, or rolling back a firmware update. The library supports\ndifferent application protocols like Message Queuing Telemetry Transport (MQTT)\nand Hypertext Transfer Protocol (HTTP), and provides various configuration\noptions you can fine tune depending on network type and conditions. This library\nis distributed under the [MIT Open Source License](LICENSE).\n\nThis library has gone through code quality checks including verification that no\nfunction has a\n[GNU Complexity](https://www.gnu.org/software/complexity/manual/complexity.html)\nscore over 10. This library has also undergone static code analysis from\n[Coverity static analysis](https://scan.coverity.com/).\n\nSee memory requirements for this library\n[here](./docs/doxygen/include/size_table.md).\n\n**AWS IoT Over-the-air Update Library v3.4.0\n[source code](https://github.com/aws/ota-for-aws-iot-embedded-sdk/tree/v3.4.0/source)\nis part of the\n[FreeRTOS 202210.00 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202210.00-LTS)\nrelease.**\n\n**AWS IoT Over-the-air Update Library v3.3.0\n[source code](https://github.com/aws/ota-for-aws-iot-embedded-sdk/tree/v3.3.0/source)\nis part of the\n[FreeRTOS 202012.01 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202012.01-LTS)\nrelease.**\n\n## Upcoming Changes\n\nThis library will be deprecated in 2024. Please see [Getting Started With OTA](#getting-started-with-ota)\n\n## AWS IoT Over-the-air Updates Config File\n\nThe AWS IoT Over-the-air Updates library exposes configuration macros that are\nrequired for building the library. A list of all the configurations and their\ndefault values are defined in\n[ota_config_defaults.h](source/include/ota_config_defaults.h). To provide custom\nvalues for the configuration macros, a custom config file named `ota_config.h`\ncan be provided by the user application to the library.\n\nBy default, a `ota_config.h` custom config is required to build the library. To\ndisable this requirement and build the library with default configuration\nvalues, provide `OTA_DO_NOT_USE_CUSTOM_CONFIG` as a compile time preprocessor\nmacro.\n\n## Building the Library\n\nThe [otaFilePaths.cmake](otaFilePaths.cmake) file contains the information of\nall source files and the header include paths required to build the AWS IoT\nOver-the-air Updates library.\n\nAs mentioned in the previous section, either a custom config file (i.e.\n`ota_config.h`) OR the `OTA_DO_NOT_USE_CUSTOM_CONFIG` macro needs to be provided\nto build the AWS IoT Over-the-air Updates library.\n\nFor a CMake example of building the AWS IoT Over-the-air Updates library with\nthe `otaFilePaths.cmake` file, refer to the `coverity_analysis` library target\nin the [test/CMakeLists.txt](test/CMakeLists.txt) file.\n\n## Building Unit Tests\n\n### Checkout CMock Submodule\n\nBy default, the submodules in this repository are configured with `update=none`\nin [.gitmodules](.gitmodules) to avoid increasing clone time and disk space\nusage of other repositories (like\n[AWS IoT Device SDK for Embedded C](https://github.com/aws/aws-iot-device-sdk-embedded-C)\nthat submodules this repository).\n\nTo build unit tests, the submodule dependency of CMock is required. Use the\nfollowing command to clone the submodule:\n\n```\ngit submodule update --checkout --init --recursive test/unit-test/CMock source/dependency/coreJSON source/dependency/3rdparty/tinycbor\n```\n\n### Platform Prerequisites\n\n- Linux\n- For building the library, **CMake 3.13.0** or later and a **C90 compiler**.\n- For running unit tests, **Ruby 2.0.0** or later is additionally required for\n  the CMock test framework (that we use).\n- For running the coverage target, **gcov** and **lcov** are additionally\n  required.\n\n### Steps to build unit tests\n\n1. Go to the root directory of this repository. (Make sure that the **CMock**\n   submodule is cloned as described [above](#checkout-cmock-submodule).)\n\n1. Run the _cmake_ command: `cmake -S test -B build`\n\n1. Run this command to build the library and unit tests: `make -C build all`\n\n1. The generated test executables will be present in `build/bin/tests` folder.\n\n1. Run `cd build && ctest` to execute all tests and view the test run summary.\n\n## Migration Guide\n\n### How to migrate from v2.0.0 (Release Candidate) to v3.4.0\n\nThe following table lists equivalent API function signatures in v2.0.0 (Release\nCandidate) and v3.4.0 declared in [ota.h](source/include/ota.h)\n\n|             v2.0.0 (Release Candidate)             |                                   v3.4.0                                    |                                                                               Notes                                                                               |\n| :------------------------------------------------: | :-------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------: |\n| `OtaState_t OTA_Shutdown( uint32_t ticksToWait );` | `OtaState_t OTA_Shutdown( uint32_t ticksToWait, uint8_t unsubscribeFlag );` | `unsubscribeFlag` indicates if unsubscribe operations should be performed from the job topics when shutdown is called. Set this as 1 to unsubscribe, 0 otherwise. |\n\n### How to migrate from version 1.0.0 to version 3.4.0 for OTA applications\n\nRefer to\n[OTA Migration document](https://docs.aws.amazon.com/freertos/latest/portingguide/porting-migration-ota.html)\nfor the summary of updates to the API.\n[Migration document for OTA PAL](https://docs.aws.amazon.com/freertos/latest/portingguide/porting-migration-ota-pal.html)\nalso provides a summary of updates required for upgrading the OTA-PAL to work\nwith v3.4.0 of the library.\n\n## Porting\n\nIn order to support AWS IoT Over-the-air Updates on your device, it is necessary\nto provide the following components:\n\n1. [Port for the OTA Portable Abstraction Layer (PAL).](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/aws/ota-for-aws-iot-embedded-sdk/docs/doxygen/output/html/ota_porting.html#ota_porting_pal)\n\n1. [OS Interface](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/aws/ota-for-aws-iot-embedded-sdk/docs/doxygen/output/html/ota_porting.html#ota_porting_os)\n\n1. [MQTT Interface](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/aws/ota-for-aws-iot-embedded-sdk/docs/doxygen/output/html/ota_porting.html#ota_porting_mqtt)\n\nFor enabling data transfer over HTTP dataplane the following component should\nalso be provided:\n\n1. [HTTP Interface](https://docs.aws.amazon.com/embedded-csdk/202103.00/lib-ref/libraries/aws/ota-for-aws-iot-embedded-sdk/docs/doxygen/output/html/ota_porting.html#ota_porting_http)\n\n**NOTE** When using OTA over HTTP dataplane, MQTT is required for control plane\noperations and should also be provided.\n\n## CBMC\n\nTo learn more about CBMC and proofs specifically, review the training material\n[here](https://model-checking.github.io/cbmc-training).\n\nThe `test/cbmc/proofs` directory contains CBMC proofs.\n\nIn order to run these proofs you will need to install CBMC and other tools by\nfollowing the instructions\n[here](https://model-checking.github.io/cbmc-training/installation.html).\n\n### CBMC Locally\n\nTo run a single CBMC proof locally, you can build the Makefile in any of the\nCBMC proofs. The Makefile is located in the\n`test/cbmc/proof/<the proof you want>/` directory.\n\nRunning `make` will produce a HTML-based report nearly identical to the one\nproduced by the CI step.\n\n**A couple notes about CBMC Proofs**\n\n- macOS doesn't implement POSIX message queues (`mqueue.h`);\n- It is possible that macOS fails to recognize your loop unwinding identifiers\n  for function from the C standard libraries. For this case, you'll want to use\n  the `__builtin___<function>_chk` identifier, e.g., instead of using `memcpy`\n  add `__builtin___memcpy_chk`.\n  - For example, the **requestJob_Mqtt** proof fails on macOS with the following\n    error:\n\n```\nLoop unwinding failures\n[trace] __builtin___strncpy_chk.unwind.0 in line 36 in file <builtin-library-__builtin___strncpy_chk>\n```\n\nTo solve this issue, replace `strncpy` with `__builtin___strncpy_ch` on\n[this line](https://github.com/aws/ota-for-aws-iot-embedded-sdk/blob/main/test/cbmc/proofs/requestJob_Mqtt/Makefile#L16).\n\n## Reference examples\n\nPlease refer to the demos of the AWS IoT Over-the-air Updates library in the\nfollowing location for reference examples on POSIX and FreeRTOS:\n\n| Platform |                                                     Location                                                     |\n| :------: | :--------------------------------------------------------------------------------------------------------------: |\n|  POSIX   |  [AWS IoT Device SDK for Embedded C](https://github.com/aws/aws-iot-device-sdk-embedded-C/tree/main/demos/ota)   |\n| FreeRTOS | [FreeRTOS/FreeRTOS](https://github.com/FreeRTOS/FreeRTOS/tree/main/FreeRTOS-Plus/Demo/AWS/Ota_Windows_Simulator) |\n| FreeRTOS |        [FreeRTOS AWS Reference Integrations](https://github.com/aws/amazon-freertos/tree/main/demos/ota)         |\n\n## Documentation\n\n### Existing Documentation\n\nFor pre-generated documentation, please see the documentation linked in the\nlocations below:\n\n|                                                          Location                                                           |\n| :-------------------------------------------------------------------------------------------------------------------------: |\n|    [AWS IoT Device SDK for Embedded C](https://github.com/aws/aws-iot-device-sdk-embedded-C#releases-and-documentation)     |\n| [FreeRTOS.org](https://freertos.org/Documentation/api-ref/ota-for-aws-iot-embedded-sdk/docs/doxygen/output/html/index.html) |\n\nNote that the latest included version of coreMQTT may differ across\nrepositories.\n\n### Generating documentation\n\nThe Doxygen references were created using Doxygen version 1.9.2. To generate the\nDoxygen pages, please run the following command from the root of this\nrepository:\n\n```shell\ndoxygen docs/doxygen/config.doxyfile\n```\n\n## Contributing\n\nSee [CONTRIBUTING.md](./.github/CONTRIBUTING.md) for information on\ncontributing.\n", "release_dates": ["2022-10-14T17:29:28Z", "2021-12-16T00:51:18Z", "2021-11-12T02:49:47Z", "2021-08-16T17:25:02Z", "2021-02-28T00:14:14Z", "2020-12-15T00:17:23Z"]}, {"name": "personalize-kafka-connector", "description": null, "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Kafka Connector for Amazon Personalize\n\nAmazon Personalize is a fully managed machine learning service that uses your data to generate item recommendations for your users. When you import data, you can choose to import records in bulk, or with our [real-time APIs](https://docs.aws.amazon.com/personalize/latest/dg/data-prep.html). For customers that use Kafka, we've set up the Amazon Personalize Kafka Connector to send real-time data from kafka topics to your Amazon Personalize Dataset.\n\n## Usage\n\n### Prerequisites:\n1. Follow the steps listed in the [Amazon Personalize Documentation](https://docs.aws.amazon.com/personalize/latest/dg/personalize-workflow-full.html) to make sure you have the required Amazon Personalize resources created.\n2. Create a new, or use an existing, self-managed kafka cluster.\n\n### Setup:\n1. Clone this repository, and run the ```mvn clean package``` command locally to create the uber jar/zip file.\n2. Place the jar/extracted zip in your Kafka plugin directory. Please refer to the [Kafka Documentation](https://kafka.apache.org/documentation/#connectconfigs_plugin.path) for the plugin path configuration.\n3. Create a configuration file for the connector. Please refer to the Configuration section below for more details.\n4. Create connector by following the [Kafka Connector Rest API documentation](https://kafka.apache.org/0100/documentation.html#connect_rest). Once the connector starts running, it will ingest data from your Kafka topic into the Amazon Personalize Dataset you specified in your configuration file.\n6. To verify the data is populated in Amazon Personalize as expected, [run an analysis](https://docs.aws.amazon.com/personalize/latest/dg/analyzing-data.html) on the ingested dataset or follow the steps to [export a dataset](https://docs.aws.amazon.com/personalize/latest/dg/export-data.html) from Amazon Personalize.\n7. Congrats! Your connector setup is complete \ud83c\udf89\n\n## Configuration\n\nThis is a sample configuration which is required for configuring your connector.\n\nAdd the following details:\n1. Preferred AWS region.\n2. The Amazon Personalize Dataset type for data ingestion, should be \"items\", \"users\" or \"events\".\n3. The Amazon Personalize Dataset [event tracking ID](https://docs.aws.amazon.com/personalize/latest/dg/importing-interactions.html#event-tracker-console), which is required if the Dataset Type is \"events\".\n4. The Amazon Personalize Dataset ARN, which is required if the Dataset Type is \"items\" or \"users\".\n\n```\n    aws.region.name=<AWS region>\n    # Amazon Personalize Data type for which this connector sending data. Valid Values : events, users, items. Default value is events if not specified.\n    amazon.personalize.data.type = events/users/items\n    # Amazon Personalize Event Tracking Id , it is required when Data Type value configured as events\n    amazon.personalize.event.tracking.id = <Amazon Personalize Event Tracking ID>\n    # ARN for Items/Users Dataset. Required when Data type configured as items or users\n    amazon.personalize.dataset.arn = <ARN for Item/User Dataset>\n```\n\n## AWS Credentials\n\nThe following sections provide information on how to configure AWS credentials for the Amazon Personalize Kafka Connector.\n\n### Credentials Provider Chain\n\nAWS credentials should be provided through one of the methods provided in the [AWS SDK for Java documentation](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default).\n\n\n### Using Trusted account credentials\n\nThis connector can assume a role and use credentials from a separate trusted account. If this is intended, you need to provide addititional configuration parameters, specified below.\n\n    amazon.personalize.credentials.provider.class=com.aws.auth.AssumeRoleCredentialsProvider\n    amazon.personalize.sts.role.arn=<ARN for assume role>\n    amazon.personalize.sts.role.external.id=<STS External Id>\n    amazon.personalize.sts.role.session.name=<Session Name for assumed role>\n\n## Expected Data Formats for Different Data Set\n\nThe Amazon Personalize Kafka Connector expects data in a specific format.\n\nIf you want to transform data to match the required data format, use the transformations techniques provided within the [Apache Kafka documentation](https://kafka.apache.org/documentation/#connect_included_transformation).\n\n### Expected Data format for events data type.\n\nSee the [Amazon Personalize Event API documentation](https://docs.aws.amazon.com/personalize/latest/dg/API_UBS_Event.html) for more details.\n\n    { \n         \"eventId\": \"string\",\n         \"eventType\": \"string\",\n         \"eventValue\": number,\n         \"impression\": [ \"string\" ],\n         \"itemId\": \"string\",\n         \"eventAttributionSource\": \"string\",\n         \"properties\": \"string\",\n         \"recommendationId\": \"string\",\n         \"sentAt\": number,\n         \"sessionId\": \"string\",\n         \"userId\": \"string\"\n    }\n\n### Expected Data format for users data type.\n\nSee the [Amazon Personalize User API documentation](https://docs.aws.amazon.com/personalize/latest/dg/API_UBS_User.html) for more details.\n\n        {\n            \"userId\": \"string\",\n            \"properties\": \"string\"\n        }\n\n### Expected Data format for items data type.\n\nSee the [Amazon Personalize Item API documentation](https://docs.aws.amazon.com/personalize/latest/dg/API_UBS_Item.html) for more details.\n\n        {\n            \"itemId\": \"string\",\n            \"properties\": \"string\"\n        }\n\n## Custom Transformations\n\nSince the properties field needs to be a \"stringified\" JSON containing fields related to your Amazon Personalize Dataset, we have created a Custom Transformation to create the properties field to transform data from a regular JSON value such as:\n\n    {\n        \"itemId\" :\"item1\",\n        \"numberOfRatings\": \"12\",\n        \"numberOfLikes\" : \"5\"\n    }\n\ninto a \"stringified\" JSON:\n\n    {\n        \"itemId\" :\"item1\",\n        \"properties\":\"{\"numberOfRatings\": \"12\",\"numberOfLikes\" : \"5\"}\"    \n    }\n\nHere are sample transformation properties to use the Custom Transformation.\n\n    transforms = addproperties\n    transforms.addproperties.type = com.aws.transform.CombineFieldsToJSONTransformation$Value\n    transforms.addproperties.fieldsToInclude = numberOfRatings,numberofLikes\n    transforms.addproperties.targetFieldName = properties\n\n## Service quotas and limit\n\nAmazon Personalize has limits on the rate at which you can send data. Update your configuration in order to conform to the limits. More details on limits can be found in our [documentation](https://docs.aws.amazon.com/personalize/latest/dg/limits.html#limits-table)\n\nPlease add the following configuration to throttle or limit data sent to Amazon Personalize if you have changed the service limit and quotas for your AWS account.\n\n    record.rate.limit = <Maximum rate of Put* API (PutEvents/PutItems/PutUsers) requests / number of tasks>\n\nThe current defaults, preconfigured in the connector plugin, are 1000 requests/second for an Event Dataset and 10 requests/second for Users and Items Dataset.\n\n## License\n\nThe Amazon Personalize Kafka Connector is available under [Apache License, Version 2.0](https://aws.amazon.com/apache2.0).\n\n\n----\n\nCopyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n", "release_dates": []}, {"name": "pg_tle", "description": "Framework for building trusted language extensions for PostgreSQL", "language": "C", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Trusted Language Extensions for PostgreSQL (pg_tle)\n\nTrusted Language Extensions (TLE) for PostgreSQL (`pg_tle`) is an open source project that lets developers extend and deploy new PostgreSQL functionality with lower administrative and technical overhead. Developers can use Trusted Language Extensions for PostgreSQL to create and install extensions on restricted filesystems and work with PostgreSQL internals through a SQL API.\n\n* [Documentation](./docs/)\n* [Overview](#overview)\n* [Getting started](#getting-started)\n* [Help & feedback](#help--feedback)\n* [Contributing](#contributing)\n* [Security](#security)\n* [License](#license)\n\n## Overview\n\nPostgreSQL provides an [extension framework](https://www.postgresql.org/docs/current/extend-extensions.html) for adding more functionality to PostgreSQL without having to fork the codebase. This powerful mechanism lets developers build new functionality for PostgreSQL, such as new data types, the ability to communicate with other database systems, and more. It also lets developers consolidate code that is functionally related and apply a version to each change. This makes it easier to bundle and distribute software across many unique PostgreSQL databases.\n\nInstalling a new PostgreSQL extension involves having access to the underlying filesystem. Many managed service providers or systems running databases in containers disallow users from accessing the filesystem for security and safety reasons. This makes it challenging to add new extensions in these environments, as users either need to request for a managed service provider to build an extension or rebuild a container image.\n\nTrusted Language Extensions for PostgreSQL, or `pg_tle`, is an extension to help developers install and manage extensions in environments that do not provide access to the filesystem. PostgreSQL provides \"trusted languages\" for development that have certain safety attributes, including restrictions on accessing the filesystem directly and certain networking properties. With these security guarantees in place, a PostgreSQL administrator can let unprivileged users write stored procedures in their preferred programming languages, such as PL/pgSQL, JavaScript, or Perl. PostgreSQL also provides the ability to mark an extension as \"trusted\" and let unprivileged users install and use extensions that do not contain code that could potentially impact the security of a system.\n\n## Getting started\n\nTo get started with `pg_tle`, follow the [installation](./docs/01_install.md) instructions.\n\nOnce you have installed `pg_tle`, we recommend writing your first TLE using the [quickstart](./docs/02_quickstart.md).\n\nYou can also find detailed information about the `pg_tle` [extension management API](./docs/03_managing_extensions.md) and [available hooks](./docs/04_hooks.md).\n\nThere are examples for writing TLEs in several languages, including:\n\n* [SQL](./docs/05_sql_examples.md)\n* [PL/pgSQL](./docs/06_plpgsql_examples.md)\n* [JavaScript](./docs/07_plv8_examples.md)\n* [Perl](./docs/08_plperl_examples.md)\n\n## Help & feedback\n\nHave a question? Have a feature request? We recommend trying the following things (in this order):\n\n* [Documentation](./docs/)\n* [Search open issues](https://github.com/aws/pg_tle/issues/)\n* [Open a new issue](https://github.com/aws/pg_tle/issues/new/choose)\n\n## Contributing\n\nWe welcome and encourage contributions to `pg_tle`!\n\nSee our [contribution guide](CONTRIBUTING.md) for more information on how to report issues, set up a development environment, and submit code.\n\nWe also recommend you read through the [architecture guide](./docs/30_architecture.md) to understand the `pg_tle` design principles!\n\nWe adhere to the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2023-09-07T18:02:20Z", "2023-07-24T18:24:01Z"]}, {"name": "porting-advisor-for-graviton", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "Porting Advisor for Graviton\n=============================\n\nThis is a fork of [Porting advisor](https://github.com/arm-hpc/porting-advisor), an open source project by the ARM High Performance Computing group. Originally, it was coded as a Python module that analyzed some known incompatibilities for C and Fortran code.\n\n It is a command line tool that analyzes source code for known code patterns and dependency libraries. It then generates a report with any incompatibilities with our Graviton processors. This tool provides suggestions of minimal required and/or recommended versions to run on Graviton instances for both language runtime and dependency libraries. It can run on non-ARM based machines (no Graviton processor needed). This tool does not work on binaries, just source code. It does not make any code modifications, it doesn\u2019t make API level recommendations, nor does it send data back to AWS.\n\n**PLEASE NOTE: Even though we do our best to find known incompatibilities, we still recommend to perform the appropriate tests to your application on a Graviton instance before going to Production.**\n\n This tool scans all files in a source tree, regardless of whether they are included by the build system or not. As such it may erroneously report issues in files that appear in the source tree but are excluded by the build system. Currently, the tool supports the following languages/dependencies:\n\n* Python 3+\n    * Python version\n    * PIP version\n    * Dependency versions in requirements.txt file\n* Java 8+\n    * Java version\n    * Dependency versions in pom.xml file\n    * JAR scanning for native method  calls (requires JAVA to be installed)\n* Go 1.11+\n    * Go version\n    * Dependency versions on go.mod file\n* C, C++, Fortran\n    * Inline assembly with no corresponding aarch64 inline assembly.\n    * Assembly source files with no corresponding aarch64 assembly source files.\n    * Missing aarch64 architecture detection in autoconf config.guess scripts.\n    * Linking against libraries that are not available on the aarch64 architecture.\n    * Use of architecture specific intrinsic.\n    * Preprocessor errors that trigger when compiling on aarch64.\n    * Use of old Visual C++ runtime (Windows specific).\n    * The following types of issues are detected, but not reported by default:\n        * Compiler specific code guarded by compiler specific pre-defined macros.\n    * The following types of cross-compile specific issues are detected, but not reported by default.\n        * Architecture detection that depends on the host rather than the target.\n        * Use of build artifacts in the build process.\n\n\nFor more information on how to modify issues reported, use the tool\u2019s built-in help:\n\n```bash\n./porting-advisor-linux-x86_64 -\u2013help\n```\n\nIf you run into any issues, see our [CONTRIBUTING](CONTRIBUTING.md#reporting-bugsfeature-requests) file.\n\n# How to run:\n\n## As a container\n\nBy using this option, you don't need to worry about Python or Java versions, or any other dependency that the tool needs. This is the quickest way to get started.\n\n**Pre-requisites**\n\n- Docker or [containerd](https://github.com/containerd/containerd) + [nerdctl](https://github.com/containerd/nerdctl) + [buildkit](https://github.com/moby/buildkit)\n\n**Build container image**\n\n**NOTE:** if using containerd, you can substitute `docker` with `nerdctl`\n\n```bash\ndocker build -t porting-advisor .\n```\n\n**NOTE:** on Windows you might need to run these commands to avoid bash scripts having their line ends changed to CRLF:\n\n```shell\ngit config core.autocrlf false\ngit reset --hard\n```\n\n**Run container image**\n\nAfter building the image, we can run the tool as a container. We use `-v` to mount a volume from our host machine to the container.\n\nWe can run it directly to console:\n\n```bash\ndocker run --rm -v my/repo/path:/repo porting-advisor /repo\n```\n\nOr generate a report:\n\n```bash\ndocker run --rm -v my/repo/path:/repo -v my/output:/output porting-advisor /repo --output /output/report.html\n```\n\nWindows example:\n\n```shell\ndocker run --rm -v /c/Users/myuser/repo:/repo -v /c/Users/myuser/output:/output porting-advisor /repo --output /output/report.html\n```\n\n## As a Python script\n\n**Pre-requisites**\n\n- Python 3.10 or above (with PIP3 and venv module installed).\n- (Optionally) Open JDK 17 (or above) and Maven 3.5 (or above) if you want to scan JAR files for native methods.\n\n**Enable Python Environment**\n\nLinux/Mac:\n```bash\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\nPowershell:\n```shell\npython -m venv .venv\n.\\.venv\\Scripts\\Activate.ps1\n```\n\n**Install requirements**\n```bash\npip3 install -r requirements.txt\n```\n\n**Run tool (console output)**\n```bash\npython3 src/porting-advisor.py ~/my/path/to/my/repo\n```\n\n**Run tool (HTML report)**\n```bash\npython3 src/porting-advisor.py ~/my/path/to/my/repo --output report.html\n```\n\n## As a binary\n\n### Generating the binary\n\n**Pre-requisites**\n\n- Python 3.10 or above (with PIP3 and venv module installed).\n- (Optionally) Open JDK 17 (or above) and Maven 3.5 (or above) if you want the binary to be able to scan JAR files for native methods.\n\nThe `build.sh` script will generate a self-contained binary (for Linux/MacOS). It will be output to a folder called `dist`.\n\nBy default, it will generate a binary named like `porting-advisor-linux-x86_64`. You can customize generated filename by setting environment variable `FILE_NAME`.\n\n```bash\n./build.sh\n```\n\nFor Windows, the `Build.ps1` will generate a folder with an EXE and all the files it requires to run.\n\n```shell\n.\\Build.ps1\n```\n\n**Running the binary**\n\n**Pre-requisites**\n\nOnce you have the binary generated, it will only require Java 11 Runtime (or above) if you want to scan JAR files for native methods. Otherwise, the file is self-contained and doesn't need Python to run.\n\nDefault behaviour, console output:\n```bash\n$ ./porting-advisor-linux-x86_64 ~/my/path/to/my/repo\n```\n\nGenerating HTML report:\n```bash\n$ ./porting-advisor-linux-x86_64 ~/my/path/to/my/repo --output report.html\n```\n\nGenerating a report of just dependencies (this creates an Excel file with just the dependencies we found on the repo, no suggestions provided):\n```bash\n$ ./porting-advisor-linux-x86_64 ~/my/path/to/my/repo --output dependencies.xlsx --output-format dependencies\n```\n\n### Sample console report output:\n\n```bash\n./dist/porting-advisor-linux-x86_64 ./sample-projects/\n| Elapsed Time: 0:00:03\n\nPorting Advisor for Graviton v1.0.0\nReport date: 2023-01-06 23:48:20\n\n13 files scanned.\ndetected java code. we recommend using Corretto. see https://aws.amazon.com/corretto/ for more details.\ndetected python code. if you need pip, version 19.3 or above is recommended. we detected that you have version 22.2.1.\ndetected python code. min version 3.7.5 is required. we detected that you have version 3.10.6. see https://github.com/aws/aws-graviton-getting-started/blob/main/python.md for more details.\n./sample-projects/java-samples/pom.xml: dependency library: leveldbjni-all is not supported on Graviton\n./sample-projects/java-samples/pom.xml: using dependency library snappy-java version 1.1.3. upgrade to at least version 1.1.4\n./sample-projects/java-samples/pom.xml: using dependency library zstd-jni version 1.1.0. upgrade to at least version 1.2.0\n./sample-projects/python-samples/incompatible/requirements.txt:3: using dependency library OpenBLAS version 0.3.16. upgrade to at least version 0.3.17\ndetected go code. min version 1.16 is required. version 1.18 or above is recommended. we detected that you have version 1.15. see https://github.com/aws/aws-graviton-getting-started/blob/main/golang.md for more details.\n./sample-projects/java-samples/pom.xml: using dependency library hadoop-lzo. this library requires a manual build  more info at: https://github.com/aws/aws-graviton-getting-started/blob/main/java.md#building-multi-arch-jars\n./sample-projects/python-samples/incompatible/requirements.txt:5: dependency library NumPy is present. min version 1.19.0 is required.\ndetected java code. min version 8 is required. version 11 or above is recommended. see https://github.com/aws/aws-graviton-getting-started/blob/main/java.md for more details.\n\nUse --output FILENAME.html to generate an HTML report.\n```", "release_dates": ["2023-08-04T00:30:34Z", "2023-04-28T21:28:47Z", "2023-03-08T06:04:40Z", "2023-01-20T22:38:47Z"]}, {"name": "porting-assistant-dotnet-client", "description": "The 'Porting Assistant for .NET' is a standalone compatibility analyzer that helps customers to port their .NET Framework (\u201c.NET\u201d) applications to .NET Core on Linux.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![Porting Assistant for .NET](./logo.png \"Porting Assistant for .NET\")\n\n# Porting Assistant for .NET SDK\n![Build Test](https://github.com/aws/porting-assistant-dotnet-client/workflows/Build%20Test/badge.svg)\n \nPorting Assistant for .NET provides tools such as Porting Assistant for .NET standalone tool and Porting Assistant for .NET Visual Studio IDE Extension. Both tools provide functionality that scans .NET Framework applications and generates a .NET compatibility assessment, helping customers port their applications to Linux faster.\n \nPorting Assistant for .NET tools quickly scans .NET Framework applications to identify incompatibilities with .NET, finds known replacements, and generates detailed compatibility assessment reports. This reduces the manual effort involved in modernizing applications to Linux.\n \n**PortingAssistant.Client**  SDK package provides interfaces to analyze .NET applications, find the incompatibilities, and port applications to .NET. Please note that current support for porting is limited.\n \nFor more information about Porting Assistant and to try the tool, please refer to the documenation: https://aws.amazon.com/porting-assistant-dotnet/\n\n## Getting Started\n\n* Add the Porting Assistant NuGet package source into your Nuget configuration. \n   * [https://s3-us-west-2.amazonaws.com/aws.portingassistant.dotnet.download/nuget/index.json](https://s3-us-west-2.amazonaws.com/aws.portingassistant.dotnet.download/nuget/index.json)\n   \n* Add **PortingAssistant.Client** to your project as a Nuget Package.\n\n* Follow the example below to see how the library can be integrated into your application for analyzing and porting an application.\n\n```csharp\n   var solutionPath = \"/user/projects/TestProject/TestProject.sln\";\n   var outputPath = \"/tmp/\";\n   \n   /* Create configuration object */\n   var configuration = new PortingAssistantConfiguration();\n\n   /* Create PortingAssistatntClient object */\n   var portingAssistantBuilder = PortingAssistantBuilder.Build(configuration, logConfig => logConfig.AddConsole());\n\n   var portingAssistantClient = portingAssistantBuilder.GetPortingAssistant();\n\n   /* For exporting the assessment results into a file */\n   var reportExporter = portingAssistantBuilder.GetReportExporter();\n\n   var analyzerSettings = new AnalyzerSettings\n   {\n       TargetFramework = \"netcoreapp3.1\",\n    };\n\n   /* Analyze the solution */\n   var analyzeResults =  await portingAssistantClient.AnalyzeSolutionAsync(solutionPath, analyzerSettings);\n\n   /* Generate JSON output */\n   reportExporter.GenerateJsonReport(analyzeResults, outputPath);\n   \n\n   var filteredProjects = new List<string> {\"projectname1\", \"projectname2\"};\n\n   /* Porting the application to .NET project */\n   var projects = analyzeResults.SolutionDetails.Projects.Where(p => !filteredProjects.Contains(p.ProjectName)).ToList();\n   var PortingProjectResults = analyzeResults.ProjectAnalysisResults\n       .Where(project => !filteredProjects.Contains(project.ProjectName));\n\n   var FilteredRecommendedActions = PortingProjectResults\n       .SelectMany(project => project.PackageAnalysisResults.Values\n       .Where(package =>\n       {\n            var comp = package.Result.CompatibilityResults.GetValueOrDefault(analyzerSettings.TargetFramework);\n            return comp.Compatibility != Compatibility.COMPATIBLE && comp.CompatibleVersions.Count != 0;\n        })\n        .SelectMany(package => package.Result.Recommendations.RecommendedActions));\n\n   var portingRequest = new PortingRequest\n   {\n       Projects = projects, //By default all projects are ported\n       SolutionPath = solutionPath,\n       TargetFramework = analyzerSettings.TargetFramework,\n       RecommendedActions = FilteredRecommendedActions.ToList(),\n       IncludeCodeFix = true\n   };\n\n   var portingResults =  portingAssistantClient.ApplyPortingChanges(portingRequest);\n\n   /* Generate JSON output */\n   reportExporter.GenerateJsonReport(portingResults, solutionPath, outputPath);          \n```\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues\nfor tracking bugs and feature requests.\n\n* If it turns out that you may have found a bug,\n  please open an [issue](https://github.com/aws/porting-assistant-dotnet-client/issues/new)\n  \n* Send us an email to: aws-porting-assistant-support@amazon.com\n  \n## How to use this code?\n* Clone the Git repository.\n* Load the solution `PortingAssistant.Client.sln` using Visual Studio or Rider. \n* Create a \"Run/Debug\" Configuration for the \"PortingAssistant.Client\" project.\n* Provide command line arguments for a solution path and output path, then run the application.\n\n## Other Packages\n[Codelyzer](https://github.com/aws/codelyzer): Porting Assistant uses Codelyzer to get package and API information used for finding compatibilities and replacements.\n\n[Porting Assistant for .NET Datastore](https://github.com/aws/porting-assistant-dotnet-datastore): The repository containing the data set and recommendations used in compatibility assessment.\n\n[Code translation assistant](https://github.com/aws/cta): The repository used to apply code translations\n\n\n## Contributing\n* [Adding Recommendations](https://github.com/aws/porting-assistant-dotnet-datastore/blob/master/RECOMMENDATIONS.md)\n\n* We welcome community contributions and pull requests. See\n[CONTRIBUTING](./CONTRIBUTING.md) for information on how to set up a development\nenvironment and submit code.\n\n# Additional Resources\n \n[Porting Assistant for .NET](https://docs.aws.amazon.com/portingassistant/index.html)\n \n[AWS Developer Center - Explore .NET on AWS](https://aws.amazon.com/developer/language/net/)\nFind all the .NET code samples, step-by-step guides, videos, blog content, tools, and information about live events that you need in one place.\n \n[AWS Developer Blog - .NET](https://aws.amazon.com/blogs/developer/category/programing-language/dot-net/)\nCome see what .NET developers at AWS are up to!  Learn about new .NET software announcements, guides, and how-to's.\n\n## Thank you\n* [CsprojToVs2017](https://github.com/hvanbakel/CsprojToVs2017) - CsprojToVs2017 helps convert project files from from the legacy format to the Visual Studio 2017/2019 format.\n* [Buildalyzer](https://github.com/daveaglick/Buildalyzer) - Buildalyzer lets you run MSBuild from your own code and returns information about the project.\n* [Nuget.Client](https://github.com/NuGet/NuGet.Client) - Nuget.Client provides tools to interface with Nuget.org and parse Nuget configuration files.\n* [Portability Analyzer](https://github.com/microsoft/dotnet-apiport) - Portability Analyzer analyzes assembly files to access API compatibility with various versions of .NET. Porting Assistant for .NET makes use of recommendations and data provided by Portability Analyzer.\n* [The .NET Compiler Platform (\"Roslyn\")](https://github.com/dotnet/roslyn) - Roslyn provides open-source C# and Visual Basic compilers with rich code analysis APIs. \n* [.NET SDKs](https://dotnet.microsoft.com/) - .NET SDKs is a set of libraries and tools that allow developers to create .NET applications and libraries.\n* [THIRD-PARTY](./THIRD-PARTY) - This project would not be possible without additional dependencies listed in [THIRD-PARTY](./THIRD-PARTY).\n\n# License\n\nLibraries in this repository are licensed under the Apache 2.0 License.\n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.  \n\n", "release_dates": ["2023-05-04T03:53:40Z", "2022-10-26T00:55:51Z", "2022-07-19T00:27:32Z"]}, {"name": "porting-assistant-dotnet-datastore", "description": "The 'Porting Assistant for .NET' is a standalone compatibility analyzer that helps customers to port their .NET Framework (\u201c.NET\u201d) applications to .NET Core on Linux.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Porting Assistant for .NET\nPorting Assistant for .NET is an analysis tool that scans .NET Framework applications and generates a .NET Core compatibility assessment, helping customers port their applications to Linux faster.\n\nPorting Assistant for .NET quickly scans .NET Framework applications to identify incompatibilities with .NET Core, finds known replacements, and generates detailed compatibility assessment reports. This reduces the manual effort involved in modernizing the applications to Linux.\n\n### Porting Assistant Dotnet DataStore\nThis repository contains the location of data store in S3. The data store contains data files that show package and API compatibility with .NET Core and .NET Standard. \n\n* Porting assistant uses these data files for finding the compatibility of packages and their APIs.\n* For each NuGet package in nuget.org, a separate Json file is defined. Each file captures compatibility information of all its packaged versions.\n* It shows compatibility information for all the NuGet packages available in nuget.org - 211k unique packages, 2.5 millions packaged versions (The numbers could change over time).\n\n### Format of the data file (Json format)\n#### Package Details\n| Attribute Name | Description |\n| :----------- | :---------- |\n| Name | Name of the package. <br/> Example: `\"Name\": \"AWSSDK.EC2\"`|\n| Format | Version of the Json document.<br/>Example: `\"Format\": \"1.0\"` |\n| Versions |  List of packaged versions of this package. <br/>Example: `\"Versions\": [ \"4.0\",\"4.4\"]` |\n| Targets |  Targets: Compatible targets based on packaged versions. <br/>Example: `{ \"netcoreapp3.1\" => [ \"4.0\", \"4.1\"] }, { \"netcoreapp5.0 => [\"4.0\"] }`. <br/>\"netcoreapp3.1\" is supported for package versions - 4.0 and 4.1. |\n| API details | List of APIs. Refer API Details. | \n| License details | License details. Refer License Details |\n\n\n#### API Details\n| Attribute Name | Description |\n| :------ | :---------- |\n| Name | Name of the Api. <br/>Example: `\"Name\": \".ctor\"` |\n| Signature | Signature of the Api. <br/>Example: `\"Signature\": \"AutoMapper.AutoMapperConfigurationException.AutoMapperConfigurationException(string)\"`  |\n| Namespace | Namespace.  <br/>Example: `\"NameSpace\": \"AutoMapper\"` |\n| ClassName | Class name.  <br/>Example: `\"ClassName\": \"AutoMapper.AutoMapperConfigurationException\"` |\n| Parameters | Parameters. <br/>Example: `\"Parameters\": [ \"string\" ]`  |\n| ReturnType | Return type. <br/>Example: `\"ReturnType\": \"Void\"` |\n| Targets |  Targets: Compatible targets based on packaged versions. <br/>Example: `{ \"netcoreapp3.1\" => [ \"4.0\", \"4.1\"] }, { \"netcoreapp5.0 => [\"4.0\"] }`. <br/>\"netcoreapp3.1\" is supported for package versions - 4.0 and 4.1. |\n\n#### License Details\n| Attribute Name | Description |\n| :------ | :---------- |\n| License | License type. <br/>Example: `\"License\": { \"MIT\": [ \"4.0.0\" ], \"Apache\": [ \"4.4.0\" ] }`  |\n| Title |  Title of the package. <br/>Example: `\"Title\": { \"This package is used to analyse C# code\": [ \"4.0.0\", \"4.4.0\" ] }` |\n| Url |  Url of the license. <br/>Example: `\"Url\": { \"https://licenses.nuget.org/MIT\": [ \"4.0.0\", \"4.4.0\" ] }` |\n| Description | Description of the package. <br/>Example: `\"Details\": { \"This package is used to analyse C# code\": [ \"4.0.0\", \"4.4.0\" ] }  |\n\n## Contributing\n* [Adding Recommendations](https://github.com/aws/porting-assistant-dotnet-datastore/blob/master/RECOMMENDATIONS.md)\n\n## Security\n* See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## Thank you\n* [Portability Analyzer](https://github.com/microsoft/dotnet-apiport) - Portability Analyzer analyzes assembly files to access API compatibility with various versions of .NET. Porting Assistant for .NET makes use of recommendations and data provided by Portability Analyzer.\n* [THIRD-PARTY](./THIRD-PARTY) - This project would not be possible without additional dependencies listed in [THIRD-PARTY](./THIRD-PARTY).\n\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "porting-assistant-dotnet-ui", "description": "Porting Assistant for .NET is an analysis tool that scans .NET Framework applications and generates a .NET Core compatibility assessment, helping customers port their applications to Linux faster.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# UI for Porting Assistant for .NET Standalone Tool\n\nPorting Assistant for .NET is an analysis tool that scans .NET Framework applications and generates a .NET Core compatibility assessment, helping customers port their applications to Linux faster.\n\nPorting Assistant for .NET quickly scans .NET Framework applications to identify incompatibilities with .NET Core, finds known replacements, and generates detailed compatibility assessment reports. This reduces the manual effort involved in modernizing applications to Linux.\n\n**PortingAssistant.UI** package provides the source code of Porting Assistant standalone tool's UI.\n\nFor more information about Porting Assistant and to try the tool, please refer to the documenation: https://aws.amazon.com/porting-assistant-dotnet/\n\n## Introduction\n\nPorting Assistant for .NET standalone tool is an electron application. The UI is a react application that is running on an Electron application.\n\n## Repository Structure\n\nWe are using lerna to manage multiple packages in a single repository.\n\nAll code is located within the `./packages` folder.\n\n\n* `./packages/csharp` - C# code that interfaces with Porting Assistant for .NET client\n* `./packages/electron` - Electron related code. Including electron.js and electron build scripts.\n* `./packages/react` - The React app that electron runs.\n* `./packages/integration-test` - Integration tests.\n\n## Developing Porting Assistant for .NET\n\n### Prereq\n\nWe require the following:\n\n* Node 14 (newer versions are not supported)\n* .NET Core 3.1\n* [.NET Runtime 6.0.12 ](https://dotnet.microsoft.com/en-us/download/dotnet/thank-you/runtime-6.0.12-windows-x64-installer?cid=getdotnetcore)\n\n### Getting started developing\n\n#### Single commands\n\nTo start a local dev environment from scratch.\n\n\n```\nnpm install && npm run build && npm start\n```\n\n\n#### Add a new dependency\n\n\n1. Run `lerna add some-dependency --scope @porting-assistant/some-package`. For example `lerna add redux --scope @porting-assistant/react`.\n\nYou can also run `npm install` inside a package. But you will need to run `lerna bootstrap` afterwards.\n\n#### Steps to package a dev exe.\n\n\n1. Build the apps\n\n```\nnpm run build\n```\n\n\n\n1. Package the apps, by default we package for Windows only.\n\n```\nnpm run build:exe:dev\n```\n\n\n\n1. Find the exe in the ./dist/ folder.\n\n### AWS UI\n\nPorting Assistant for .NET standalone tool makes use of the newly released AWS UI design framework for UI components. AWS UI contains a collection of React components that help create intuitive, responsive, and accessible user experiences for web applications. It is available on NPM (Package manager of Node.JS). This work is available under the terms of the [Apache 2.0 open source license](http://#).\n\nAWS UI\u2019s source code and documentation has not been opensourced or released yet. For now the best way to obtain the list of available components and parameters for the components is to look into the package within node_modules. If you need additional help with AWS UI please file an issue, we will be happy to provide the help you need.\n\n\n#### Steps to find list of components\n\n\n1. Run npm \n\n```\nnpm install\n```\n\n1. Go into `packages/react/node_modules/@awsui/components-react/` to check the list of components.\n2. The typescript typing files will also allow typescript intellisense / plugins to perform auto completion within IDEs.\n\n# Additional Resources\n\n[Porting Assistant for .NET](https://docs.aws.amazon.com/portingassistant/index.html)\n\n[AWS Developer Center - Explore .NET on AWS](https://aws.amazon.com/developer/language/net/)\nFind all the .NET code samples, step-by-step guides, videos, blog content, tools, and information about live events that you need in one place.\n\n[AWS Developer Blog - .NET](https://aws.amazon.com/blogs/developer/category/programing-language/dot-net/)\nCome see what .NET developers at AWS are up to! Learn about new .NET software announcements, guides, and how-to's.\n\n\n# License\n\nLibraries in this repository are licensed under the Apache 2.0 License.\n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.\n", "release_dates": ["2023-05-04T22:43:59Z"]}, {"name": "porting-assistant-dotnet-visual-studio-ide-extension", "description": "Porting Assistant for .NET is an analysis tool that scans .NET Framework applications and generates a .NET Core compatibility assessment, helping customers port their applications to Linux faster.", "language": "C#", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Porting Assistant for .NET Visual Studio IDE Extension\n \nPorting Assistant for .NET is an analysis tool that scans .NET Framework applications and generates a .NET Core compatibility assessment, helping customers port their applications to Linux faster.\n \nPorting Assistant for .NET quickly scans .NET Framework applications to identify incompatibilities with .NET Core, finds known replacements, and generates detailed compatibility assessment reports. This reduces the manual effort involved in modernizing applications to Linux.\n \n**Porting Assistant for .NET Visual Studio IDE Extension**  package provides a Visual Studio IDE extension implementation to analyze .NET applications, find incompatibilities, and port applications to .NET Core. Please note that current support for porting is limited.\n \nFor more information about Porting Assistant for .NET Visual Studio IDE Extension and to try the tool, please refer to the documentation: https://aws.amazon.com/porting-assistant-dotnet/\n\n## Getting Started\n\n* Add the Porting Assistant NuGet package source into your Nuget configuration. \n   * [https://s3-us-west-2.amazonaws.com/aws.portingassistant.dotnet.download/nuget/index.json](https://s3-us-west-2.amazonaws.com/aws.portingassistant.dotnet.download/nuget/index.json)\n   \n* Add **PortingAssistant.Client** to your project as a Nuget Package.\n\n## Getting Help\n\nPlease use these community resources for getting help. We use the GitHub issues\nfor tracking bugs and feature requests.\n\n* If it turns out that you may have found a bug,\n  please open an [issue](https://github.com/aws/porting-assistant-dotnet-visual-studio-ide-extension/issues/new)\n  \n* Send us an email to: aws-porting-assistant-support@amazon.com\n  \n## How to use this code?\n\n### Prerequisites\n\nWe require the following:\n\n* Visual Studio 2019 or later\n* .NET 6\n\n### Steps to build a dev vsix package\n* Unload the Visual Studio Client project you do not want to build for. For example if you want to install on Visual Studio 2022, unload the Porting AssistantVSExtensionClient2019 project.\n* Build the solution using the release configuration\n* Find the dev vsix package in the bin\\Release directory of the Visual Studio client version you are building for\n  * PortingAssistantVSExtensionClient2019\\bin\\Release\n  * PortingAssistantVSExtensionClient2022\\bin\\Release\n* Run the vsix to install the extension\n\n### Getting started with development\n\n* Clone the Git repository.\n* Install git-secrets https://github.com/awslabs/git-secrets\n* Run git config core.hooksPath hooks\n* Load the solution `PortingAssistantVSExtension.sln` using Visual Studio or Rider. \n* Unload the Visual Studio Client project you do not want to build for. For example if you want to install on Visual Studio 2022, unload the Porting AssistantVSExtensionClient2019 project.\n* Create a \"Run/Debug\" Configuration for the \"PortingAssistantVSExtension.sln\" solution.\n* Select 'Set Startup Projects'>'Multiple Startup Project' and set one of the following combinations as startup projects: \n  * PortingAssistantExtensionServer and PortingAssistantVSExtensionClient2019\n  * PortingAssistantExtensionServer and PortingAssistantVSExtensionClient2022\n* Provide command line arguments for configuration, then run the application.\n* It should start a new Visual Studio Client with the Extension setup.\n* Open the solution to analyze/port in the new Visual Studio Client.\n\n## Other Packages\n[PortingAssistant.Client](https://github.com/aws/porting-assistant-dotnet-client): PortingAssistant.Client package provides interfaces to analyze .NET applications, find the incompatibilities, and port applications to .NET Core. Please note that current support for porting is limited.\n\n[Codelyzer](https://github.com/aws/codelyzer): Porting Assistant uses Codelyzer to get package and API information used for finding compatibilities and replacements.\n\n[Porting Assistant for .NET Datastore](https://github.com/aws/porting-assistant-dotnet-datastore): The repository containing the data set and recommendations used in compatibility assessment.\n\n[Code translation assistant](https://github.com/aws/cta): The repository used to apply code translations.\n\n\n## Contributing\n* [Adding Recommendations](https://github.com/aws/porting-assistant-dotnet-datastore/blob/master/RECOMMENDATIONS.md)\n\n* We welcome community contributions and pull requests. See\n[CONTRIBUTING](./CONTRIBUTING.md) for information on how to set up a development\nenvironment and submit code.\n\n# Additional Resources\n \n[Porting Assistant for .NET](https://docs.aws.amazon.com/portingassistant/index.html)\n \n[AWS Developer Center - Explore .NET on AWS](https://aws.amazon.com/developer/language/net/)\nFind all the .NET code samples, step-by-step guides, videos, blog content, tools, and information about live events that you need in one place.\n \n[AWS Developer Blog - .NET](https://aws.amazon.com/blogs/developer/category/programing-language/dot-net/)\nCome see what .NET developers at AWS are up to!  Learn about new .NET software announcements, guides, and how-to's.\n\n[Learn more about developing extensions for Visual Studio](https://learn.microsoft.com/en-us/visualstudio/extensibility/starting-to-develop-visual-studio-extensions?view=vs-2022)\n\n## Thank you\n* [Omnisharp](https://github.com/OmniSharp/csharp-language-server-protocol) - Omnisharp is an implementation of the Language Server Protocol written entirely in C# for .NET.\n* [CsprojToVs2017](https://github.com/hvanbakel/CsprojToVs2017) - CsprojToVs2017 helps convert project files from from the legacy format to the Visual Studio 2017/2019 format.\n* [Buildalyzer](https://github.com/daveaglick/Buildalyzer) - Buildalyzer lets you run MSBuild from your own code and returns information about the project.\n* [Nuget.Client](https://github.com/NuGet/NuGet.Client) - Nuget.Client provides tools to interface with Nuget.org and parse Nuget configuration files.\n* [Portability Analyzer](https://github.com/microsoft/dotnet-apiport) - Portability Analyzer analyzes assembly files to access API compatibility with various versions of .NET. Porting Assistant for .NET makes use of recommendations and data provided by Portability Analyzer.\n* [The .NET Compiler Platform (\"Roslyn\")](https://github.com/dotnet/roslyn) - Roslyn provides open-source C# and Visual Basic compilers with rich code analysis APIs. \n* [.NET SDKs](https://dotnet.microsoft.com/) - .NET SDKs is a set of libraries and tools that allow developers to create .NET applications and libraries.\n* [THIRD-PARTY](./LICENSE-THIRD-PARTY.txt) - This project would not be possible without additional dependencies listed in [THIRD-PARTY](./LICENSE-THIRD-PARTY.txt).\n\n# License\n\nLibraries in this repository are licensed under the Apache 2.0 License.\n\nSee [LICENSE](./LICENSE) and [NOTICE](./NOTICE) for more information.  \n", "release_dates": []}, {"name": "postgresql-logfdw", "description": null, "language": "C", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# log_fdw\n\nThis is a PostgreSQL extension built using Foreign-Data Wrapper facility to\nenable reading log files via SQL. It basically provides SQL interface to create\nforeign tables for each PostgreSQL log file through which the file contents can\nbe read and analyzed. Only superusers are allowed to create this extension.\n\n## SQL functions\nTo create foreign table, use:\n```\ncreate_foreign_table_for_log_file(IN table_name TEXT, IN server_name TEXT, IN log_file_name TEXT)\n```\nTo list files and their sizes present in PostgreSQL log directory, use:\n```\nlist_postgres_log_files(OUT file_name TEXT, OUT file_size_bytes BIGINT)\n```\nNote that `list_postgres_log_files()` function is a wrapper around PostgreSQL's\ncore function [pg_ls_logdir](https://www.postgresql.org/docs/current/functions-admin.html#FUNCTIONS-ADMIN-GENFILE)\nand exists for compatibility reasons.\n\nBy default, use of this extension's functions is restricted to superusers.\nAccess may be granted by superusers to others using GRANT as needed.\nFor instance, following are the minimal things that one needs to do for\nenabling others to use the extension's functions:\n```\nCREATE ROLE foo; -- a non-superuser\nGRANT pg_monitor TO foo; -- do this only when list_postgres_log_files() is used because the underlying function pg_ls_logdir() needs it\nGRANT CREATE ON SCHEMA bar TO foo; -- to create foreign tables in schema named bar\nGRANT USAGE ON FOREIGN SERVER log_fdw_server TO foo; -- to use log_fdw foreign server\nSET ROLE foo;\nSELECT * FROM create_foreign_table_for_log_file('log_file_tbl', 'log_fdw_server', 'log_file.csv');\n```\n\n## Quick install instructions\n\nClone the repository from https://github.com/aws/postgresql-logfdw:\n\n```\ngit clone https://github.com/aws/postgresql-logfdw.git\n``` \nRun `make clean` and `make install` to install the extension. Remember to set\n`PATH` environment variable to point to `pg_config`. Alternatively, copy the\nextension source code to `contrib` directory under PostgreSQL source tree and\ninstall it.\n\n## Usage\n\n### Create extension:\n\n```\npostgres=# create extension log_fdw;\nCREATE EXTENSION\n```\n\n### See functions created by extension:\n\n```\npostgres=# \\df\n                                                      List of functions\n Schema |               Name                | Result data type |                  Argument data types                  | Type \n--------+-----------------------------------+------------------+-------------------------------------------------------+------\n public | create_foreign_table_for_log_file | void             | table_name text, server_name text, log_file_name text | func\n public | list_postgres_log_files           | SETOF record     | OUT file_name text, OUT file_size_bytes bigint        | func\n public | log_fdw_handler                   | fdw_handler      |                                                       | func\n public | log_fdw_validator                 | void             | text[], oid                                           | func\n(4 rows)\n```\n\n```\npostgres=# SELECT * FROM list_postgres_log_files() LIMIT 10;\n         file_name         | file_size_bytes \n---------------------------+-----------------\n postgresql-2022-10-13.csv |               0\n postgresql-2022-11-14.log |            8006\n postgresql-2022-11-01.csv |            4025\n postgresql-2022-10-27.csv |               0\n postgresql-2022-10-24.log |               0\n postgresql-2022-11-05.log |               0\n postgresql-2022-11-23.log |          789872\n postgresql-2022-11-07.csv |               0\n postgresql-2022-11-04.csv |            3943\n postgresql-2022-11-16.log |               0\n(10 rows)\n```\n\n```\npostgres=# SELECT * FROM list_postgres_log_files() ORDER BY 1 DESC LIMIT 2;\n         file_name         | file_size_bytes \n---------------------------+-----------------\n postgresql-2022-11-28.log |            1754\n postgresql-2022-11-28.csv |            1948\n(2 rows)\n```\n\n### Create server:\n\n```\npostgres=# CREATE SERVER log_fdw_server FOREIGN DATA WRAPPER log_fdw;\nCREATE SERVER\n```\n\n### Create foreign tables from csv files and log files:\n\n```\npostgres=# SELECT * FROM create_foreign_table_for_log_file('postgresql_2022_11_28_csv','log_fdw_server','postgresql-2022-11-28.csv');\n create_foreign_table_for_log_file \n-----------------------------------\n \n(1 row)\n```\n\n```\npostgres=# SELECT * FROM create_foreign_table_for_log_file('postgresql_2022_11_28_log','log_fdw_server','postgresql-2022-11-28.log');\n create_foreign_table_for_log_file \n-----------------------------------\n \n(1 row)\n```\n\n### See foreign tables created:\n\n```\npostgres=# \\detr\n            List of foreign tables\n Schema |           Table           |  Server  \n--------+---------------------------+----------------\n public | postgresql_2022_11_28_csv | log_fdw_server\n public | postgresql_2022_11_28_log | log_fdw_server\n(2 rows)\ufffc\n```\n\n### Read log file contents via foreign tables created:\n\nSELECT * FROM postgresql_2022_11_14_log LIMIT 2;\n\n```\npostgres=# \\x\nExpanded display is on.\npostgres=# select * from postgresql_2022_11_28_log limit 2;\n-[ RECORD 1 ]---------------------------------------------------------------------------------------------------------------------------\nlog_entry | 2022-11-28 20:37:51.767 UTC   14170  637e8d69.375a 7  2022-11-23 21:15:21 UTC  0 00000LOG:  received fast shutdown request\n-[ RECORD 2 ]---------------------------------------------------------------------------------------------------------------------------\nlog_entry | 2022-11-28 20:37:51.769 UTC   14170  637e8d69.375a 8  2022-11-23 21:15:21 UTC  0 00000LOG:  aborting any active transactions\n```\n\nSELECT * FROM postgresql_2022_11_28_csv LIMIT 2;\n\n```\npostgres=# select * from postgresql_2022_11_28_csv limit 2;\n-[ RECORD 1 ]----------+---------------------------------\nlog_time               | 2022-11-28 20:37:51.767+00\nuser_name              | \ndatabase_name          | \nprocess_id             | 14170\nconnection_from        | \nsession_id             | 637e8d69.375a\nsession_line_num       | 5\ncommand_tag            | \nsession_start_time     | 2022-11-23 21:15:21+00\nvirtual_transaction_id | \ntransaction_id         | 0\nerror_severity         | LOG\nsql_state_code         | 00000\nmessage                | received fast shutdown request\ndetail                 | \nhint                   | \ninternal_query         | \ninternal_query_pos     | \ncontext                | \nquery                  | \nquery_pos              | \nlocation               | \napplication_name       | \nbackend_type           | postmaster\nleader_pid             | \nquery_id               | 0\n-[ RECORD 2 ]----------+---------------------------------\nlog_time               | 2022-11-28 20:37:51.769+00\nuser_name              | \ndatabase_name          | \nprocess_id             | 14170\nconnection_from        | \nsession_id             | 637e8d69.375a\nsession_line_num       | 6\ncommand_tag            | \nsession_start_time     | 2022-11-23 21:15:21+00\nvirtual_transaction_id | \ntransaction_id         | 0\nerror_severity         | LOG\nsql_state_code         | 00000\nmessage                | aborting any active transactions\ndetail                 | \nhint                   | \ninternal_query         | \ninternal_query_pos     | \ncontext                | \nquery                  | \nquery_pos              | \nlocation               | \napplication_name       | \nbackend_type           | postmaster\nleader_pid             | \nquery_id               | 0\n```\n\n### Remove extension:\n\nDROP EXTENSION log_fdw CASCADE;\n\n```\npostgres=# DROP EXTENSION log_fdw CASCADE;\nNOTICE:  drop cascades to 3 other objects\nDETAIL:  drop cascades to server log_fdw_server\ndrop cascades to foreign table postgresql_2022_11_28_csv\ndrop cascades to foreign table postgresql_2022_11_28_log\nDROP EXTENSION\n```\n\n## Compatibility with PostgreSQL\n\nThis extension currently works well with PostgreSQL version 14, 15 and 16devel.\n\n## LICENSE\n\nSee [LICENSE](https://github.com/aws/postgresql-logfdw/blob/main/LICENSE) for\ndetailed information.\n\n## Contributing\n\nSee [CODE_OF_CONDUCT](https://github.com/aws/postgresql-logfdw/blob/main/CODE_OF_CONDUCT.md)\nand [CONTRIBUTING](https://github.com/aws/postgresql-logfdw/blob/main/CONTRIBUTING.md)\nfor detailed information.\n", "release_dates": []}, {"name": "public-suffix-list", "description": "A fork of the Public Suffix List, for organizing/collating AWS contributions onto the PSL.", "language": "Go", "license": null, "readme": "## What is this repository?\n\nThis repository is a fork of the [Public Suffix List](https://publicsuffix.org), created so that AWS's engineering teams can streamline AWS-specific updates to the PSL. Branches in this repository represent 'work in progress' that will be contributed to the PSL when ready. This repository is **not** intended for public usage and does not represent the current state of the PSL.", "release_dates": []}, {"name": "random-cut-forest-by-aws", "description": "An implementation of the Random Cut Forest data structure for sketching streaming data, with support for anomaly detection, density estimation, imputation, and more.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Random Cut Forest by AWS\n\nThis repository contains implementations of the Random Cut Forest (RCF) probabilistic data structure.\nRCFs were originally developed at Amazon to use in a nonparametric anomaly detection algorithm for\nstreaming data. Later new algorithms based on RCFs were developed for density estimation, imputation,\nand forecasting.\n\nThe different directories correspond to equivalent implementations in different languages, and bindings to\nto those base implementations, using language specific features for greater flexibility of use. \n\nRandomCutForest in the randomcutforest-core package provides an estimation (say anomaly score, or extrapolation over a forecast horizon)\nand using that raw estimation can be challenging. The randomcutforest-parkservices package provides\nseveral capabilities (ThresholdedRandomCutForest, RCFCaster, respectively) for distilling the scores to a determination of\na potential anomaly or calibrated forecast respectively.\nThe package randomcutforest-examples showcases several example scenarios for using the repository. \nThey also provide examples for some of the parameter settings. Many of these examples are built in tests.\n\n## Documentation\n\n* Guha, S., Mishra, N., Roy, G., & Schrijvers, O. (2016, June). Robust random cut forest based anomaly detection on streams. In *International conference on machine learning* (pp. 2712-2721).\n\n## Code of Conduct\n\nThis project has adopted an [Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\n\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public GitHub issue.\n\n\n## Licensing\n\nSee the [LICENSE](./LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\n\n## Copyright\n\nCopyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n", "release_dates": ["2024-01-04T01:17:34Z", "2023-07-24T20:12:48Z", "2023-05-22T20:59:04Z", "2023-04-05T20:10:07Z", "2023-03-15T18:36:57Z", "2023-03-07T10:39:43Z", "2022-07-12T22:18:53Z", "2022-07-12T22:18:29Z", "2022-07-12T22:18:02Z", "2022-07-12T22:17:14Z", "2022-07-12T22:16:35Z", "2021-08-10T20:59:25Z", "2021-07-23T23:39:59Z", "2021-06-24T16:02:51Z"]}, {"name": "redshift-test-drive", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Redshift Test Drive\n\n## Introduction\nRedshift Test Drive is an amalgamation of Redshift Replay and Node Config. The Redshift Replay consits of Workload Replicator, Replay Analysis and External Object Replicator.\n\n## Prerequisites\nInstall the following packages before cloning the repository:\n<br>1. Install git\n<br> \n```\nyum install git\n```\n <br>2. Install pip3:\n <br>\n ```\n yum install pip3\n ```\n <br>3. Install make:\n <br>\n ```\n yum install make\n ```\n\n ## Preparations\n 01. Clone the git repository using the following command:\n \n ```\n git clone https://github.com/aws/redshift-test-drive\n cd redshift-test-drive/\n export REDSHIFT_TEST_DRIVE_ROOT=$(pwd)\n ```\n 02. Execute the following command from the root directory to install all the required packages:\n ```\n cd $REDSHIFT_TEST_DRIVE_ROOT && make setup\n ```\n 03. Refer to the Table of Content which will point out the different tools and README links of your interest.\n\n\n<br>\n\n### Table of Content\nThe following table provides links to all tools, locations & READMEs in the repository\n\n\n\n| Index |                             Tool                              | Description | README links|\n| ----- |:-------------------------------------------------------------:|-------| :-------: |\n| 01|                 [Workload Replicator](/core)                  |Workload Replicator is an open source tool which helps customers to mimic their workloads on clusters |[README](/core/README.md)|\n| 02|           [Replay Analysis](/tools/ReplayAnalysis)            |Replay Analysis utility enhances auditing in the Workload Replicator process to extract information about the errors that occurred, the validity of the run, and the performance of the replay. This is also a user interface in which customers can choose multiple replays to analyze, validate, and compare using the extracted audit logs.|[README](/tools/ReplayAnalysis/README.md)|\n|03 | [External Object Replicator](/tools/ExternalObjectReplicator) |External Object Replicator replicates COPY manifest objects, and Spectrum object in the customer cluster|[README](/tools/ExternalObjectReplicator/README.md)|\n|04|            [Node Config](/tools/NodeConfigCompare)            | Node Configuration Comparison utility answers a very common question on which instance type and number of nodes should we choose for your workload on Amazon Redshift.|[README](/tools/NodeConfigCompare/README.md)\n\n## FAQs\nQ. I'm experiencing issues with boto3 appearing as `ValueError: Invalid endpoint: https://s3..amazonaws.com` or something to that effect, how do I fix this?\n\nA. `aws configure` command is a pre-requisite step for most tools within Test drive. Make sure you run `aws configure` and configure the default region.\n\n----\nQ. My make commands are failing with `make: *** No rule to make target `, how do I fix this?\n\nA. Make sure you are in the right directory for execution. Make commands are made possible through the Makefile found in the root directory. If you followed the setup instructions, this is aliased to `REDSHIFT_TEST_DRIVE_ROOT` in your shell.\n\n----\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": []}, {"name": "res", "description": "Research and Engineering Studio (RES) is an AWS supported open source product that enables IT administrators to provide an easy-to-use web portal for scientists and engineers to run technical computing workloads on AWS.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Research and Engineering Studio on AWS (RES)\n\nResearch and Engineering Studio on AWS (RES) is an open source, easy-to-use web-based portal for administrators to create and manage secure cloud-based research and engineering environments. Using RES, scientists and engineers can visualize data and run interactive applications without the need for cloud expertise. \n  \t\t  \nWith just a few clicks, scientists and engineers can create and connect to Windows and Linux virtual desktops that come with pre-installed applications, shared data, and collaboration tools they need. With RES, administrators can define permissions, set budgets, and monitor resource utilization through a single web interface.\n  \t\t  \nRES virtual desktops are powered by Amazon EC2 instances and NICE DCV. RES is available at no additional charge. You pay only for the AWS resources needed to run your applications.\n \n## Table of contents\n- [Public Documentation](#public-documentation)\n- [Public Installation](#public-installation)\n- [Troubleshooting](#troubleshooting)\n- [Getting Help](#getting-help)\n- [More Resources](#more-resources)\n\n## Public Documentation\n\nRefer to [Research and Engineering Studio Documentation](https://docs.aws.amazon.com/res/latest/ug/).\n \n## Public Installation\n\nRefer to [Research and Engineering Studio Installation](https://docs.aws.amazon.com/res/latest/ug/deploy-the-product.html).\n\n## Troubleshooting\n\nRefer to [Research and Engineering Studio Troubleshooting Wiki](https://github.com/aws/res/wiki/Troubleshooting)\n\n## Getting Help\n  \t\t  \nIf you have a [support plan](https://aws.amazon.com/premiumsupport/) with AWS Support, please create a new support case.\n\nYou can also [open an issue](https://github.com/aws/res/issues/new/choose) and choose from one of our templates for guidance, bug reports, or feature requests. Please check for open [similar issues](https://github.com/aws/res/issues) before opening another one.\n  \t\t  \n## More Resources\n  \t\t   \t\t  \n* [Changelog](https://github.com/aws/res/blob/mainline/CHANGELOG.md)\n* [Amazon Web Services Discussion Forums](https://repost.aws/) \n* [AWS Support](https://console.aws.amazon.com/support/home#/)", "release_dates": ["2024-02-16T01:50:50Z", "2024-01-30T18:42:41Z", "2023-11-16T19:49:46Z"]}, {"name": "rolesanywhere-credential-helper", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## AWS IAM Roles Anywhere Credential Helper\nrolesanywhere-credential-helper implements the [signing process](https://docs.aws.amazon.com/rolesanywhere/latest/userguide/authentication-sign-process.html) for IAM Roles Anywhere's [CreateSession](https://docs.aws.amazon.com/rolesanywhere/latest/userguide/authentication-create-session.html) API and returns temporary credentials in a standard JSON format that is compatible with the `credential_process` feature available across the language SDKs. More information can be found [here](https://docs.aws.amazon.com/rolesanywhere/latest/userguide/credential-helper.html). It is released and licensed under the Apache License 2.0.\n\n## Building\n\n### Dependencies\nIn order to build the source code, you will need to install git, gcc, GNU make, and golang.\n\n#### Linux\n\nOn Debian-based systems, you can do so using `sudo apt-get install git build-essential golang-go`. For other Linux distributions, replace `apt-get` with the package manager on your system.\n\n#### Darwin\n\nYou can download Apple clang through the [following link](https://developer.apple.com/download/) if you don't already have it installed on your system. You can install git, make, and golang through Homebrew through `brew install git`, `brew install make` and `brew install go`, respectively.\n\n#### Windows\n\nIn order to get gcc on Windows, one option is to use [MinGW-w64](https://www.mingw-w64.org/downloads/). After obtaining gcc, you can install golang through the [installer](https://go.dev/doc/install). Lastly, you can install git and make through `Chocolatey` with `choco install git` and `choco install make`, respectively.\n\n### Build\n\nAfter obtaining these tools, and making sure they are on your `PATH`, you can build the package (assuming you are currently at the package root):\n\n```\nmake release\n```\n\nAfter building, you should see the `aws_signing_helper` binary built for your system at `build/bin/aws_signing_helper`. Usage can be found in [AWS's documentation](https://docs.aws.amazon.com/rolesanywhere/latest/userguide/credential-helper.html). A later section also goes into how you can use the scripts provided in this repository to test out the credential helper binary.\n\n### Scripts\n\nThe project also comes with two bash scripts at its root, called `generate-certs.sh` and `generate-credential-process-data.sh`. The former script is used strictly for unit testing, and it generates certificate and private key data with different parameters that are supported by IAM Roles Anywhere. You can run the bash script using `/bin/bash generate-certs.sh`, and you will see the generated certificates and keys under the `tst/certs` directory. The latter script is used both for unit testing and can also be used for testing the `credential-process` command after having built the binary. It will create a CA certificate/private key as well as a leaf certificate/private key. When testing IAM Roles Anywhere, you will have to upload the CA certificate a trust anchor and create a profile within Roles Anywhere before using the binary along with the leaf certificate/private key to call `credential-process` (more instructions can be found in the next section). You can run the bash script using `/bin/bash generate-credential-process-data.sh`, and you will see the generated certificate hierarchy (and corresponding keys) under the `credential-process-data` directory. Note that the unit tests that require these fixtures to exist will run the bash script themselves, before executing those tests that depend on the fixtures existing. Please note that these scripts currently only work on Unix-based systems and require `openssl` to be installed.\n\n## Diagnostic Command Tools\n\n### read-certificate-data\n\nReads a certificate. Either the path to the certificate on disk or PKCS#11 URI to identify the certificate is provided with the `--certificate` parameter, or the `--cert-selector` flag is provided to select a certificate within an OS certificate store. Further details about the `--cert-selector` flag are provided below.\n\nIf there are multiple certificates that match a given `--cert-selector` or PKCS#11 URI (as specified through the `--certificate` parameter), information about each of them is printed. For PKCS#11, URIs for each matched certificate is also printed in the hopes that it will be useful in uniquely identifying a certificate. \n\n#### cert-selector flag\n\nIf you use Windows or MacOS, the credential helper also supports leveraging private keys and certificates that are in their OS-specific secure stores. In Windows, both CNG and Cryptography are supported, while on MacOS, Keychain Access is supported. Through the `--cert-selector` flag, it is possible to specify which certificate (and associated private key) to use in calling `CreateSession`. The credential helper will then delegate signing operations to the keys within those secure stores, without those keys ever having to leave those stores. It is important to note that on Windows, only the user's \"MY\" certificate store will be searched by the credential helper, while for MacOS, Keychains on the search list will be searched.\n\nThe `--cert-selector` flag allows one to search for a specific certificate (and associated private key) through the certificate Subject, Issuer, and Serial Number. The corresponding keys are `x509Subject`, `x509Issuer`, and `x509Serial`, respectively. These keys can be specified either through a JSON file format or through the command line. An example of both approaches can be found below.\n\nIf you would like to use a JSON file, it should look something like this:\n\n```\n[\n  {\n    \"Key\": \"x509Subject\",\n    \"Value\": \"CN=Subject\"\n  },\n  {\n    \"Key\": \"x509Issuer\",\n    \"Value\": \"CN=Issuer\"\n  },\n  {\n    \"Key\": \"x509Serial\",\n    \"Value\": \"15D19632234BF759A32802C0DA88F9E8AFC8702D\"\n  }\n]\n```\n\nIf the above is placed in a file called `selector.json`, it can be specified with the `--cert-selector` flag through `file://path/to/selector.json`. The very same certificate selector argument can be specified through the command line as follows:\n\n```\n--cert-selector Key=x509Subject,Value=CN=Subject Key=x509Issuer,Value=CN=Issuer Key=x509Serial,Value=15D19632234BF759A32802C0DA88F9E8AFC8702D\n```\n\nThe example given here is quite simple (the Subject and Issuer each contain only a single RDN), so it may not be obvious, but the Subject and Issuer values roughly follow the [RFC 2253](https://www.rfc-editor.org/rfc/rfc2253.html) Distinguished Names syntax.\n\n### sign-string\n\nSigns a fixed strings: `\"AWS Roles Anywhere Credential Helper Signing Test\" || SIGN_STRING_TEST_VERSION || SHA256(\"IAM RA\" || PUBLIC_KEY_BYTE_ARRAY)`. Useful for validating your private key and digest. Either the path to the private key must be provided with the `--private-key` parameter, or a certificate selector must be provided through the `--cert-selector` parameter (if you want to use the OS certificate store integration). Other parameters that can be used are `--digest`, which must be one of `SHA256 (*default*) | SHA384 | SHA512`, and `--format`, which must be one of `text (*default*) | json | bin`.\n\n### credential-process\n\nVends temporary credentials by sending a `CreateSession` request to the Roles Anywhere service. The request is signed by the private key whose path can be provided with the `--private-key` parameter. Currently, only plaintext private keys are supported. Other parameters include `--certificate` (the path to the end-entity certificate), `--role-arn` (the ARN of the role to obtain temporary credentials for), `--profile-arn` (the ARN of the profile that provides a mapping for the specified role), and `--trust-anchor-arn` (the ARN of the trust anchor used to authenticate). Optional parameters that can be used are `--debug` (to provide debugging output about the request sent), `--no-verify-ssl` (to skip verification of the SSL certificate on the endpoint called), `--intermediates` (the path to intermediate certificates), `--with-proxy` (to make the binary proxy aware), `--endpoint` (the endpoint to call), `--region` (the region to scope the request to), and `--session-duration` (the duration of the vended session). Instead of passing in paths to the plaintext private key on your file system, another option could be to use the [PKCS#11 integration](#pkcs11-integration) (using the `--pkcs11-pin` flag to locate objects in PKCS#11 tokens) or (depending on your OS) use the `--cert-selector` flag. More details about the `--cert-selector` flag can be found in [this section](#cert-selector-flag). \n\nNote that if more than one certificate matches the `--cert-selector` parameter within the OS-specific secure store, the `credential-process` command will fail. To find the list of certificates that match a given `--cert-selector` parameter, you can use the same flag with the `read-certificate-data` command.\n\nAlso note that in Windows, if you would like the credential helper to search a system certificate store other than \"MY\" (\"MY\" will be the default) in the `CERT_SYSTEM_STORE_CURRENT_USER` context, you can specify the name of the certificate store through the `--system-store-name` flag. It's not possible for the credential helper to search multiple Windows system certificate stores at once currently. But it will indirectly search certificate stores in the `CERT_SYSTEM_STORE_LOCAL_MACHINE` context since all current user certificate stores will inherit contents of local machine certificate stores. The only exception to this rule is the Current User/Personal (\"MY\") store. Please see the [Microsoft documentation](https://learn.microsoft.com/en-us/windows-hardware/drivers/install/local-machine-and-current-user-certificate-stores?source=recommendations) for more details. \n\nWhen `credential-process` is used, AWS SDKs store the returned AWS credentials in memory. AWS SDKs will keep track of the credential expiration and generate new AWS session credentials via the credential process, provided the certificate has not expired or been revoked.\n\nWhen the AWS CLI uses a `credential-process`, the AWS CLI calls the `credential-process` for every CLI command issued, which will result in the creation of a new role session and a slight delay when excuting commands. To avoid this delay from getting new credentials when using the AWS CLI, you can use `serve` or `update`.\n\n#### MacOS Keychain Guidance\n\nIf you would like to secure keys through MacOS Keychain and use them with IAM Roles Anywhere, you may want to consider creating a new Keychain that only the credential helper can access and store your keys there. The steps to do this are listed below. Note that the commands should be executed in bash.\n\nFirst, create the new Keychain:\n\n```\nsecurity create-keychain -p ${CREDENTIAL_HELPER_KEYCHAIN_PASSWORD} credential-helper.keychain\n```\n\nIn the above command line, `${CREDENTIAL_HELPER_KEYCHAIN_PASSWORD}` should contain the password you want the new Keychain to have. Next, unlock the Keychain:\n\n```\nsecurity unlock-keychain -p ${CREDENTIAL_HELPER_KEYCHAIN_PASSWORD} credential-helper.keychain\n```\n\nOnce again, you will have to specify the password to the Keychain, but this time it will be used to unlock it. Next, modify the Keychain search list to include your newly created Keychain:\n\n```\nEXISTING_KEYCHAINS=$(security list-keychains | cut -d '\"' -f2) security list-keychains -s credential-helper.keychain $(echo ${EXISTING_KEYCHAINS} | awk -v ORS=\" \" '{print $1}')\n```\n\nThe above command line will extract existing Keychains in the search list and add the newly created Keychain to the top of it. Lastly, add your PFX file (that contains your client certificate and associated private key) to the Keychain:\n\n```\nsecurity import /path/to/identity.pfx -T /path/to/aws_signing_helper -P ${UNWRAPPING_PASSWORD} -k credential-helper.keychain\n```\n\nThe above command line will import your client certificate and private key that are in a PFX file (which will be unwrapped using the `UNWRAPPING_PASSWORD` environment variable) into the newly created Keychain and only allow for the credential helper to access it. It's important to note that since the credential helper isn't signed, it isn't trusted by MacOS. To get around this, you may have to specify your Keychain password whenever the credential helper wants to use the private key to perform a signing operation. If you don't want to have to specify the password each time, you can choose to always allow the credential helper to use the Keychain item.\n\nAlso note that the above steps can be done through [MacOS Keychain APIs](https://developer.apple.com/documentation/security/keychain_services/keychains), as well as through the [Keychain Access application](https://support.apple.com/guide/keychain-access/welcome/mac).\n\n#### Windows CNG Guidance\n\nIf you would like to secure keys through Windows CNG and use them with IAM Roles Anywhere, it should be sufficient to to import your certificate (and associated private key) into your user's \"MY\" certificate store.\n\nAdd your certificate (and associated private key) to the certificate store by importing e.g. a PFX file through the below command line in Command Prompt:\n\n```\ncertutil -user -p %UNWRAPPING_PASSWORD% -importPFX \"MY\" \\path\\to\\identity.pfx\n```\n\nThe above command will import the PFX file into the user's \"MY\" certificate store. The `UNWRAPPING_PASSWORD` environment variable should contain the password to unwrap the PFX file.\n\nAlso note that the above step can be done through a [Powershell cmdlet](https://learn.microsoft.com/en-us/powershell/module/pki/import-pfxcertificate?view=windowsserver2022-ps) or through [Windows CNG/Cryptography APIs](https://learn.microsoft.com/en-us/windows/win32/api/wincrypt/nf-wincrypt-pfximportcertstore).\n\n#### PKCS#11 Integration\n\nAs you should expect from all applications which use keys and certificates, you can simply give a\n[PKCS#11 URI](https://datatracker.ietf.org/doc/html/rfc7512) in place of a filename in order to\nuse certificates and/or keys from hardware or software PKCS#11 tokens / HSMs. A hybrid mode\nusing a certificate from a file but only the key in the token is also supported. Some examples:\n\n  * `--certificate 'pkcs11:manufacturer=piv_II;id=%01'`\n  * `--certificate 'pkcs11:object=My%20RA%20key'`\n  * `--certificate client-cert.pem --private-key 'pkcs11:model=SoftHSM%20v2;object=My%20RA%20key'`\n\nSome documentation which may assist with finding the correct URI for\nyour key can be found [here](https://www.infradead.org/openconnect/pkcs11.html). Otherwise, you \ncan also potentially scope down your PKCS#11 URI by using the `read-certificate-data` diagnostic \ncommand. \n\nMost Linux and similar *nix systems use\n[p11-kit](https://p11-glue.github.io/p11-glue/p11-kit/manual/config.html)\nto provide consistent system-wide and per-user configuration of\navailable PKCS#11 providers. Any properly packaged provider module\nwill register itself with p11-kit and will be automatically visible\nthrough the `p11-kit-proxy.{dylib, dll, so}` provider which is used by default.\n\nIf you have a poorly packaged provider module from a vendor, then\nafter you have filed a bug, you can manually create a p11-kit [module\nfile](https://p11-glue.github.io/p11-glue/p11-kit/manual/pkcs11-conf.html)\nfor it.\n\nFor systems or containers which lack p11-kit, a specific PKCS#11\nprovider library can be specified using the `--pkcs11-lib` parameter.\n\nThe other relevant parameter is `--reuse-pin`. This is a boolean parameter that can \nbe specified if the private key object you would like to use to sign data has the \n`CKA_ALWAYS_AUTHENTICATE` attribute set and the `CKU_CONTEXT_SPECIFIC` PIN for the \nobject matches the `CKU_USER` PIN. If this parameter isn't set, you will be prompted \nto provide the `CKU_CONTEXT_SPECIFIC` PIN for the object through the console. If this \nparameter is set and the `CKU_USER` PIN doesn't match the `CKU_CONTEXT_SPECIFIC` PIN, \nthe credential helper application will fall back to prompting you. In an unattended \nscenario, this flag is very helpful. There is currently no way in which to specify \nthe `CKU_CONTEXT_SPECIFIC` PIN without being prompted for it, so you are out of luck \nfor the time being when it comes to unattended workloads if the `CKU_CONTEXT_SPECIFIC` \nPIN of the private key object you want to use is different from the `CKU_USER` PIN of \nthe token that it belongs to. \n\nThe searching methodology used to find objects within PKCS#11 tokens can largely be found \n[here](https://datatracker.ietf.org/doc/html/draft-woodhouse-cert-best-practice-01). Do note \nthat there are some slight differences in how objects are found in the credential helper \napplication. \n\n#### Other Notes\n\n##### YubiKey Attestation Certificates\n\nNote that if you're using a YubiKey device with PIV support, when a key pair \nand certificate exist in slots 9a or 9c (PIV authentication and digital signature, \nrespectively), the YubiKey will automatically generate an attestation certificate \nfor the slot. Testing has shown that the attestation certificate can't be deleted. \nIn this case, if you attempt to use the `CKA_ID` (the `id` path attribute in a URI) \nof your certificate to identify it in your supplied PKCS#11 URI, there will be \ntwo certificates that match. One way in which you can disambiguate between the \ntwo in your PKCS#11 URI can be through `CKA_LABEL` (the `object` path attribute \nin a URI). Attestation certificates in either of these two slots can be \nidentified through the hard-coded labels, `X.509 Certificate for PIV Attestation \n9a` or `X.509 Certificate for PIV Attestation 9c`. \n\n##### Implementation Note\n\nDue to this package's use of a dependency to integrate with PKCS#11 modules, we are unable \nto guarantee that PINs are zeroized in memory after they are no longer needed. We will continue \nto explore options to overcome this. Customers are encouraged to study the impact of this limitation \nand determine whether compensating controls are warranted for their system and threat model.\n\n### update\n\nUpdates temporary credentials in the [credential file](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html). Parameters for this command include those for the `credential-process` command, as well as `--profile`, which specifies the named profile for which credentials should be updated (if the profile doesn't already exist, it will be created), and `--once`, which specifies that credentials should be updated only once. Both arguments are optional. If `--profile` isn't specified, the default profile will have its credentials updated, and if `--once` isn't specified, credentials will be continuously updated. In this case, credentials will be updated through a call to `CreateSession` five minutes before the previous set of credentials are set to expire. Please note that running the `update` command multiple times, creating multiple processes, may not work as intended. There may be issues with concurrent writes to the credentials file.\n\nBecause when you use `update` credentials are written to a credential file on disk, it's important to understand that any user or process who can read the credential file may be able to read and use those AWS credentials. If using `update` to update any profile other than default, your application must be reference the correct profile to use. AWS SDKs will request new AWS credentials from the from the credential file as required.\n\n\n### serve\n\nVends temporary credentials through an endpoint running on localhost. Parameters for this command include those for the `credential-process` command, as well as an optional `--port`, to specify the port on which the local endpoint will be exposed. By default, the port will be `9911`. Once again, credentials will be updated through a call to `CreateSession` five minutes before the previous set of credentials are set to expire. Note that the URIs and request headers are the same as those used in [IMDSv2](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html) (only the address of the endpoint changes from `169.254.169.254` to `127.0.0.1`). In order to make the credentials served from the local endpoint available to the SDK, set the `AWS_EC2_METADATA_SERVICE_ENDPOINT` environment variable appropriately.\n\nWhen you use `serve` AWS SDKs will be able to discover the credentials from the credential helper using their [credential providers](https://docs.aws.amazon.com/sdkref/latest/guide/standardized-credentials.html) without any changes to code or configuration.  AWS SDKs will request new AWS credentials from the credential helper's server listening on 127.0.0.1 as required. \n\nWhen using `serve` it is important to understand that processes running on a system that can reach 127.0.0.1 will be able to retrieve AWS credentials from the credential helper. \n\n### Scripts\n\nThe project also comes with two bash scripts at its root, called `generate-certs.sh` and `generate-credential-process-data.sh`. Note that these scripts currently only work on Unix-based systems and require `openssl` to be installed.\n\n#### generate-certs.sh\n\nUsed by unit tests to generate test certificates and private keys supported by IAM Roles Anywhere. The test data is stored in the tst/certs directory.\n\n#### generate-credential-process-data.sh\n\nUsed by unit tests and for manual testing of the credential-process command. Creates a CA certificate/private key pair as well as a leaf certificate/private key. Test data is stored in the credential-process-data directory. When testing IAM Roles Anywhere, you will have to upload the CA certificate as a trust anchor and create a profile within Roles Anywhere before using the binary along with the leaf certificate/private key to call credential-process.\n\n### Example Usage\n```\n/bin/sh generate-credential-process-data.sh\n\nTA_ARN=$(aws rolesanywhere create-trust-anchor \\\n    --name \"Test TA\" \\\n    --source \"sourceType=CERTIFICATE_BUNDLE,sourceData={x509CertificateData=$(cat credential-process-data/root-cert.pem)}\" \\\n    --enabled | jq -r '.trustAnchor.trustAnchorArn')\n\nPROFILE_ARN=$(aws rolesanywhere create-profile \\\n    --name \"Test Profile\" \\\n    --role-arns '[\"<your-role-arn>\"]' \\\n    --enabled | jq -r '.profile.profileArn')\n\n/path/to/aws_signing_helper credential-process \\\n    --certificate credential-process-data/client-cert.pem \\\n    --private-key credential-process-data/client-key.pem \\\n    --role-arn <your-role-arn> \\\n    --trust-anchor-arn ${TA_ARN} \\\n    --profile-arn ${PROFILE_ARN}\n```\n\nIn the above example, you will have to create a role with a trust policy as documented [here](https://docs.aws.amazon.com/rolesanywhere/latest/userguide/trust-model.html). After having done so, record the role ARN and use it both when creating a profile and when obtaining temporary security credentials through `credential-process`.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2023-10-12T20:31:49Z", "2023-09-20T15:56:28Z", "2023-08-16T19:19:33Z", "2023-07-26T00:59:13Z", "2023-01-17T18:52:50Z", "2022-12-05T22:53:58Z", "2022-11-11T15:39:46Z"]}, {"name": "s2n-netbench", "description": "An efficiency, performance, and correctness analysis tool for transport protocols.", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# s2n-netbench\n\nAn efficiency, performance, and correctness analysis tool for transport protocols.\n\n## Why does this exist?\n\nThere are many transport protocols and several implementations of each. This tool exists to provide users with the ability to perform a direct comparison and decide the best implementation for their workloads.\n\nHere are a few examples of questions that s2n-netbench aims to answer:\n\n* What is the cost of encrypting traffic?\n    * How much more capacity will I need when deploying this?\n* What transport protocol performs best\n    * in a data center?\n    * in networks with high packet loss?\n    * in networks with high latency?\n    * with many concurrent, multiplexed streams?\n* Which implementation of \"X\" protocol is best for my workload?\n* What is the optimal configuration of the transport's settings for my workload?\n* How does certificate chain length affect handshake throughput?\n* Is implementation \"X\" interoperable with implementation \"Y\" of \"Z\" protocol?\n\n## Quickstart\nA basic use of s2n-netbench is demonstrated in the `netbench-run.sh` script. This script will\n- compile all necessary s2n-netbench utilities\n- generate scenario files\n- execute the `request-response.json` scenario using `s2n-quic` and `s2n-tls` drivers\n- execute the `connect.json` scenario using `s2n-quic` and `s2n-tls` drivers\n- collect statistics from the drivers using `netbench-collector`\n- generate a report in the `./target/netbench/report` directory\n\nFrom the main `netbench` folder, run the following commands\n```\n./scripts/netbench-run\ncd target/netbench/report\npython3 -m http.server 9000\n```\nThen navigate to `localhost:9000` in a browser to view the netbench results.\n\nNote that this script does not support bpftrace, as it runs without any of the\nelevated permissions required for bpf programs.\n\n## How it works\n\n### netbench-scenarios\n`netbench` provides tools to write [scenarios](./netbench-scenarios/) that describe application workloads. An example of a scenario is a simple request/response pattern between a client and server:\n\n```rust\nuse netbench_scenario::prelude::*;\n\nconfig!({\n    /// The size of the client's request to the server\n    let request_size: Byte = 1.kilobytes();\n\n    /// The size of the server's response to the client\n    let response_size: Byte = 10.megabytes();\n});\n\npub fn scenario(config: Config) -> Scenario {\n    let Config {\n        request_size,\n        response_size,\n    } = config;\n\n    Scenario::build(|scenario| {\n        let server = scenario.create_server();\n\n        scenario.create_client(|client| {\n            client.connect_to(server, |conn| {\n                conn.open_bidirectional_stream(\n                    |local| {\n                        local.send(request_size);\n                        local.receive(response_size);\n                    },\n                    |remote| {\n                        remote.receive(request_size);\n                        remote.send(response_size);\n                    },\n                );\n            });\n        });\n    })\n}\n```\n\nThis scenario generates a json file of instructions. These instructions are protocol and language independent, which means they can easily be executed by a [\"netbench driver\"](./netbench-driver/), written in any language or runtime.\n\n### netbench-driver\nNetbench drivers are responsible for executing netbench scenarios. Each transport protocol has a `client` and `server` implementation. Each of these implementations is a self-container binary that consumes a `scenario.json` file. Implemented drivers include:\n\n* `TCP`\n* [`native-tls`](https://crates.io/crates/native-tls)\n    * OpenSSL on Linux\n    * Secure Transport on macOS\n    * SChannel on Windows\n* `s2n-quic`\n* `s2n-tls`\n\n### netbench-collector\nDriver metrics are collected with the [`netbench-collector`](./netbench-collector/) utility. There are two implementation of this available - a generic utility and a bpftrace utility. The generic utility uses the `proc fs` to gather information about the process, while the `bpftrace` implementation is able to collect a wider variety of statistics through ebpf probes.\n\nThe collector binary takes a `netbench-driver` as an argument. The driver binary is spawned as a child process. The collector will continuously gather metrics from the driver and emit those metrics to `stdout`.\n\n### netbench-cli\n`netbench-cli` is used to visualize the results of the scenarios. This reports use [vega](https://vega.github.io/) which is \"a declarative format for creating, saving, and sharing visualization designs\".\n\n`report` is used to generate individual `.json` reports. These can be visualized by pasting them into the [vega editor](https://vega.github.io/editor/).\n\n`report-tree` is used to to generate a human-readable `.html` report. Given a directory structure like the following\n```\nrequest-response/ # scenario\n\u251c\u2500 tls/ # driver\n\u2502  \u251c\u2500 client.json\n\u2502  \u251c\u2500 server.json\n\u251c\u2500 quic/\n   \u251c\u2500 client.json\n   \u251c\u2500 server.json\n```\n`report-tree` will generate the individual `reports` and package them into a human readable `index.html` file that can be used to view graphs of the results.\n\nA [sample report can be found here](https://dnglbrstg7yg.cloudfront.net/8e1890f04727ef7d3acdcb521c5b3cda257778f0/netbench/index.html#request_response/clients.json).\n\nNote that you will not be able to open the report directly since the report relies on the jsdelivr cdn. This request will fail when the URL is a local file scheme with a [CORS request not HTTP](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS/Errors/CORSRequestNotHttp) error.\n\nTo get around this, use a local server.\n```\n# assuming the report is in ./report\ncd report\n# start a local server on port 9000\npython3 -m http.server 9000\n```\nIn a browser, navigate to `localhost:9000` to view the netbench report.\n", "release_dates": []}, {"name": "s2n-quic", "description": "An implementation of the IETF QUIC protocol", "language": "Rust", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# s2n-quic\n\n`s2n-quic` is a Rust implementation of the [IETF QUIC protocol](https://quicwg.org/), featuring:\n\n- a simple, easy-to-use API. See [an example](https://github.com/aws/s2n-quic/blob/main/examples/echo/src/bin/quic_echo_server.rs) of an s2n-quic echo server built with just a few API calls\n- high configurability using [providers](https://docs.rs/s2n-quic/latest/s2n_quic/provider/index.html) for granular control of functionality\n- extensive automated testing, including fuzz testing, integration testing, unit testing, snapshot testing, efficiency testing, performance benchmarking, interoperability testing and [more](https://github.com/aws/s2n-quic/blob/main/docs/ci.md)\n- integration with [s2n-tls](https://github.com/aws/s2n-tls), AWS's simple, small, fast and secure TLS implementation, as well as [rustls](https://crates.io/crates/rustls)\n- thorough [compliance coverage tracking](https://github.com/aws/s2n-quic/blob/main/docs/ci.md#compliance) of normative language in relevant standards\n- and much more, including [CUBIC congestion controller](https://www.rfc-editor.org/rfc/rfc8312.html) support, [packet pacing](https://www.rfc-editor.org/rfc/rfc9002.html#name-pacing), [Generic Segmentation Offload](https://lwn.net/Articles/188489/) support, [Path MTU discovery](https://www.rfc-editor.org/rfc/rfc8899.html), and unique [connection identifiers](https://www.rfc-editor.org/rfc/rfc9000.html#name-connection-id) detached from the address\n\nSee the [API documentation](https://docs.rs/s2n-quic) and [examples](https://github.com/aws/s2n-quic/tree/main/examples) to get started with `s2n-quic`.\n\n[![Crates.io][crates-badge]][crates-url]\n[![docs.rs][docs-badge]][docs-url]\n[![Apache 2.0 Licensed][license-badge]][license-url]\n[![Build Status][actions-badge]][actions-url]\n[![Dependencies][dependencies-badge]][dependencies-url]\n[![MSRV][msrv-badge]][msrv-url]\n\n## Installation\n\n`s2n-quic` is available on `crates.io` and can be added to a project like so:\n\n```toml\n[dependencies]\ns2n-quic = \"1\"\n```\n\n**NOTE**: On unix-like systems, [`s2n-tls`](https://github.com/aws/s2n-tls) will be used as the default TLS provider.\nOn linux systems,  [`aws-lc-rs`](https://github.com/awslabs/aws-lc-rs) will be used for cryptographic\noperations. A C compiler and CMake may be required on these systems for installation.\n\n## Example\n\nThe following implements a basic echo server and client. The client connects to the server and pipes its `stdin` on a stream. The server listens for new streams and pipes any data it receives back to the client. The client will then pipe all stream data to `stdout`.\n\n### Server\n\n```rust\n// src/bin/server.rs\nuse s2n_quic::Server;\nuse std::{error::Error, path::Path};\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn Error>> {\n    let mut server = Server::builder()\n        .with_tls((Path::new(\"cert.pem\"), Path::new(\"key.pem\")))?\n        .with_io(\"127.0.0.1:4433\")?\n        .start()?;\n\n    while let Some(mut connection) = server.accept().await {\n        // spawn a new task for the connection\n        tokio::spawn(async move {\n            while let Ok(Some(mut stream)) = connection.accept_bidirectional_stream().await {\n                // spawn a new task for the stream\n                tokio::spawn(async move {\n                    // echo any data back to the stream\n                    while let Ok(Some(data)) = stream.receive().await {\n                        stream.send(data).await.expect(\"stream should be open\");\n                    }\n                });\n            }\n        });\n    }\n\n    Ok(())\n}\n```\n\n### Client\n\n```rust\n// src/bin/client.rs\nuse s2n_quic::{client::Connect, Client};\nuse std::{error::Error, path::Path, net::SocketAddr};\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn Error>> {\n    let client = Client::builder()\n        .with_tls(Path::new(\"cert.pem\"))?\n        .with_io(\"0.0.0.0:0\")?\n        .start()?;\n\n    let addr: SocketAddr = \"127.0.0.1:4433\".parse()?;\n    let connect = Connect::new(addr).with_server_name(\"localhost\");\n    let mut connection = client.connect(connect).await?;\n\n    // ensure the connection doesn't time out with inactivity\n    connection.keep_alive(true)?;\n\n    // open a new stream and split the receiving and sending sides\n    let stream = connection.open_bidirectional_stream().await?;\n    let (mut receive_stream, mut send_stream) = stream.split();\n\n    // spawn a task that copies responses from the server to stdout\n    tokio::spawn(async move {\n        let mut stdout = tokio::io::stdout();\n        let _ = tokio::io::copy(&mut receive_stream, &mut stdout).await;\n    });\n\n    // copy data from stdin and send it to the server\n    let mut stdin = tokio::io::stdin();\n    tokio::io::copy(&mut stdin, &mut send_stream).await?;\n\n    Ok(())\n}\n```\n\n## Minimum Supported Rust Version (MSRV)\n\n`s2n-quic` will maintain a rolling MSRV (minimum supported rust version) policy of at least 6 months. The current s2n-quic version is not guaranteed to build on Rust versions earlier than the MSRV.\n\nThe current MSRV is [1.71.0][msrv-url].\n\n## Security issue notifications\n\nIf you discover a potential security issue in s2n-quic we ask that you notify\nAWS Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\nIf you package or distribute s2n-quic, or use s2n-quic as part of a large multi-user service, you may be eligible for pre-notification of future s2n-quic releases. Please contact s2n-pre-notification@amazon.com.\n\n## License\n\nThis project is licensed under the [Apache-2.0 License][license-url].\n\n[crates-badge]: https://img.shields.io/crates/v/s2n-quic.svg\n[crates-url]: https://crates.io/crates/s2n-quic\n[license-badge]: https://img.shields.io/badge/license-apache-blue.svg\n[license-url]: https://aws.amazon.com/apache-2-0/\n[actions-badge]: https://github.com/aws/s2n-quic/workflows/ci/badge.svg\n[actions-url]: https://github.com/aws/s2n-quic/actions/workflows/ci.yml?query=branch%3Amain\n[docs-badge]: https://img.shields.io/docsrs/s2n-quic.svg\n[docs-url]: https://docs.rs/s2n-quic\n[dependencies-badge]: https://img.shields.io/librariesio/release/cargo/s2n-quic.svg\n[dependencies-url]: https://crates.io/crates/s2n-quic/dependencies\n[msrv-badge]: https://img.shields.io/badge/MSRV-1.71.0-green\n[msrv-url]: https://blog.rust-lang.org/2023/07/13/Rust-1.71.0.html\n", "release_dates": ["2024-02-13T20:33:12Z", "2023-12-08T01:28:45Z", "2023-11-06T22:48:54Z", "2023-10-17T01:03:42Z", "2023-10-02T22:24:15Z", "2023-09-14T02:37:44Z", "2023-08-29T21:43:11Z", "2023-08-21T18:06:12Z", "2023-07-24T22:28:57Z", "2023-07-05T22:20:00Z", "2023-06-30T21:49:43Z", "2023-06-13T21:31:23Z", "2023-05-18T01:18:30Z", "2023-05-03T22:20:00Z", "2023-04-21T05:29:42Z", "2023-04-03T22:14:15Z", "2023-03-22T22:43:10Z", "2023-02-07T21:35:26Z", "2023-01-10T19:35:13Z", "2022-12-13T18:32:06Z", "2022-11-28T17:54:51Z", "2022-11-10T18:27:41Z", "2022-10-21T21:29:46Z", "2022-10-06T03:27:07Z", "2022-10-03T18:07:41Z", "2022-08-30T03:49:09Z", "2022-08-17T20:54:19Z", "2022-08-03T20:15:07Z", "2022-07-21T17:28:38Z", "2022-06-28T19:02:36Z"]}, {"name": "s2n-tls", "description": "An implementation of the TLS/SSL protocols", "language": "C", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<img src=\"docs/images/s2n_logo_github.png\" alt=\"s2n\">\n\ns2n-tls is a C99 implementation of the TLS/SSL protocols that is designed to be simple, small, fast, and with security as a priority. It is released and licensed under the Apache License 2.0.\n\n> s2n-tls is short for \"signal to noise\" and is a nod to the almost magical act of encryption \u2014 disguising meaningful signals, like your critical data, as seemingly random noise.\n>\n> -- [s2n-tls announcement](https://aws.amazon.com/blogs/security/introducing-s2n-a-new-open-source-tls-implementation/)\n\n[![Build Status](https://codebuild.us-west-2.amazonaws.com/badges?uuid=eyJlbmNyeXB0ZWREYXRhIjoiMndlTzJNbHVxWEo3Nm82alp4eGdGNm4rTWdxZDVYU2VTbitIR0ZLbHVtcFFGOW5majk5QnhqaUp3ZEkydG1ueWg0NGlhRE43a1ZnUzZaQTVnSm91TzFFPSIsIml2UGFyYW1ldGVyU3BlYyI6IlJLbW42NENlYXhJNy80QnYiLCJtYXRlcmlhbFNldFNlcmlhbCI6MX0%3D&branch=main)](https://github.com/aws/s2n-tls/)\n[![Apache 2 License](https://img.shields.io/github/license/aws/s2n-tls.svg)](http://aws.amazon.com/apache-2-0/)\n[![C99](https://img.shields.io/badge/language-C99-blue.svg)](http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1256.pdf)\n[![Github forks](https://img.shields.io/github/forks/aws/s2n-tls.svg)](https://github.com/aws/s2n-tls/network)\n[![Github stars](https://img.shields.io/github/stars/aws/s2n-tls.svg)](https://github.com/aws/s2n-tls/stargazers)\n\n## Quickstart for Ubuntu\n\n```bash\n# clone s2n-tls\ngit clone https://github.com/aws/s2n-tls.git\ncd s2n-tls\n\n# install build dependencies\nsudo apt update\nsudo apt install cmake\n\n# install a libcrypto\nsudo apt install libssl-dev\n\n# build s2n-tls\ncmake . -Bbuild \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=./s2n-tls-install\ncmake --build build -j $(nproc)\nCTEST_PARALLEL_LEVEL=$(nproc) ctest --test-dir build\ncmake --install build\n```\n\nSee the [s2n-tls build documentation](docs/BUILD.md) for further guidance on building s2n-tls for your platform.\n\n## Have a Question?\nIf you think you might have found a security impacting issue, please follow our [Security Notification Process.](#security-issue-notifications)\n\nIf you have any questions about submitting PRs, s2n-tls API usage, or something similar, please open an issue.\n\n## Documentation\n\ns2n-tls uses [Doxygen](https://doxygen.nl/index.html) to document its public API. The latest s2n-tls documentation can be found on [GitHub pages](https://aws.github.io/s2n-tls/doxygen/). The [Usage Guide](https://aws.github.io/s2n-tls/usage-guide/) explains how different TLS features can be configured and used.\n\nDocumentation for older versions or branches of s2n-tls can be generated locally. To generate the documentation, install doxygen and run `doxygen docs/doxygen/Doxyfile`. The doxygen documentation can now be found at `docs/doxygen/output/html/index.html`.\n\nDoxygen installation instructions are available at the [Doxygen](https://doxygen.nl/download.html) webpage.\n\n## Using s2n-tls\n\nThe s2n-tls I/O APIs are designed to be intuitive to developers familiar with the widely-used POSIX I/O APIs, and s2n-tls supports blocking, non-blocking, and full-duplex I/O. Additionally there are no locks or mutexes within s2n-tls.\n\n```c\n/* Create a server mode connection handle */\nstruct s2n_connection *conn = s2n_connection_new(S2N_SERVER);\nif (conn == NULL) {\n    ... error ...\n}\n\n/* Associate a connection with a file descriptor */\nif (s2n_connection_set_fd(conn, fd) < 0) {\n    ... error ...\n}\n\n/* Negotiate the TLS handshake */\ns2n_blocked_status blocked;\nif (s2n_negotiate(conn, &blocked) < 0) {\n    ... error ...\n}\n\n/* Write data to the connection */\nint bytes_written;\nbytes_written = s2n_send(conn, \"Hello World\", sizeof(\"Hello World\"), &blocked);\n```\n\nFor details on building the s2n-tls library and how to use s2n-tls in an application you are developing, see the [Usage Guide](https://aws.github.io/s2n-tls/usage-guide).\n\n## s2n-tls features\n\ns2n-tls implements SSLv3, TLS1.0, TLS1.1, TLS1.2, and TLS1.3. For encryption, s2n-tls supports 128-bit and 256-bit AES in the CBC and GCM modes, ChaCha20, 3DES, and RC4. For forward secrecy, s2n-tls supports both DHE and ECDHE. s2n-tls also supports the Server Name Indicator (SNI), Application-Layer Protocol Negotiation (ALPN), and Online Certificate Status Protocol (OCSP) TLS extensions. SSLv3, RC4, 3DES, and DHE are each disabled by default for security reasons.\n\nAs it can be difficult to keep track of which encryption algorithms and protocols are best to use, s2n-tls features a simple API to use the latest \"default\" set of preferences. If you prefer to remain on a specific version for backwards compatibility, that is also supported.\n\n```c\n/* Use the latest s2n-tls \"default\" set of ciphersuite and protocol preferences */\ns2n_config_set_cipher_preferences(config, \"default\");\n\n/* Use a specific set of preferences, update when you're ready */\ns2n_config_set_cipher_preferences(config, \"20150306\")\n```\n\n## s2n-tls safety mechanisms\n\nInternally s2n-tls takes a systematic approach to data protection and includes several mechanisms designed to improve safety.\n\n##### Small and auditable code base\nIgnoring tests, blank lines and comments, s2n-tls is about 6,000 lines of code. s2n's code is also structured and written with a focus on reviewability. All s2n-tls code is subject to code review, and we plan to complete security evaluations of s2n-tls on an annual basis.\n\nTo date there have been two external code-level reviews of s2n-tls, including one by a commercial security vendor. s2n-tls has also been shared with some trusted members of the broader cryptography, security, and Open Source communities. Any issues discovered are always recorded in the s2n-tls issue tracker.\n\n##### Static analysis, fuzz-testing and penetration testing\n\nIn addition to code reviews, s2n-tls is subject to regular static analysis, fuzz-testing, and penetration testing. Several penetration tests have occurred, including two by commercial vendors.\n\n##### Unit tests and end-to-end testing\n\ns2n-tls includes positive and negative unit tests and end-to-end test cases.\n\nUnit test coverage can be viewed [here](https://dx1inn44oyl7n.cloudfront.net/main/index.html). Note that this represents unit coverage for a particular build. Since that build won't necessarily support all s2n-tls features, test coverage may be artificially lowered.\n\n##### Erase on read\ns2n-tls encrypts or erases plaintext data as quickly as possible. For example, decrypted data buffers are erased as they are read by the application.\n\n##### Built-in memory protection\ns2n-tls uses operating system features to protect data from being swapped to disk or appearing in core dumps.\n\n##### Minimalist feature adoption\ns2n-tls avoids implementing rarely used options and extensions, as well as features with a history of triggering protocol-level vulnerabilities. For example there is no support for session renegotiation or DTLS.\n\n##### Compartmentalized random number generation\nThe security of TLS and its associated encryption algorithms depends upon secure random number generation. s2n-tls provides every thread with two separate random number generators. One for \"public\" randomly generated data that may appear in the clear, and one for \"private\" data that should remain secret. This approach lessens the risk of potential predictability weaknesses in random number generation algorithms from leaking information across contexts.\n\n##### Modularized encryption\ns2n-tls has been structured so that different encryption libraries may be used. Today s2n-tls supports OpenSSL (versions 1.0.2, 1.1.1 and 3.0.x), LibreSSL, BoringSSL, AWS-LC, and the Apple Common Crypto framework to perform the underlying cryptographic operations.\n\n##### Timing blinding\ns2n-tls includes structured support for blinding time-based side-channels that may leak sensitive data. For example, if s2n-tls fails to parse a TLS record or handshake message, s2n-tls will add a randomized delay of between 10 and 30 seconds, granular to nanoseconds, before responding. This raises the complexity of real-world timing side-channel attacks by a factor of at least tens of trillions.\n\n##### Table based state-machines\ns2n-tls uses simple tables to drive the TLS/SSL state machines, making it difficult for invalid out-of-order states to arise.\n\n##### C safety\ns2n-tls is written in C, but makes light use of standard C library functions and wraps all memory handling, string handling, and serialization in systematic boundary-enforcing checks.\n\n## Security issue notifications\nIf you discover a potential security issue in s2n-tls we ask that you notify\nAWS Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\nIf you package or distribute s2n-tls, or use s2n-tls as part of a large multi-user service, you may be eligible for pre-notification of future s2n-tls releases. Please contact s2n-pre-notification@amazon.com.\n\n## Contributing to s2n-tls\nIf you are interested in contributing to s2n-tls, please see our [development guide](https://github.com/aws/s2n-tls/blob/main/docs/DEVELOPMENT-GUIDE.md).\n\n## Language Bindings for s2n-tls\nSee our [language bindings list](https://github.com/aws/s2n-tls/blob/main/docs/BINDINGS.md) for language bindings for s2n-tls that we're aware of.\n", "release_dates": ["2024-02-23T00:30:15Z", "2024-02-14T19:36:11Z", "2024-01-30T19:50:38Z", "2024-01-24T19:24:38Z", "2023-12-22T02:32:20Z", "2023-12-06T18:55:36Z", "2023-11-01T23:18:14Z", "2023-10-17T19:59:51Z", "2023-10-05T21:11:47Z", "2023-10-05T15:29:21Z", "2023-10-02T16:19:56Z", "2023-09-11T18:42:53Z", "2023-08-25T19:06:34Z", "2023-08-14T22:00:34Z", "2023-07-28T18:31:01Z", "2023-07-14T17:46:24Z", "2023-06-23T19:35:51Z", "2023-06-01T18:51:35Z", "2023-05-09T18:17:22Z", "2023-04-27T21:26:08Z", "2023-04-04T18:52:50Z", "2023-03-23T22:16:11Z", "2023-03-13T19:02:30Z", "2023-03-01T17:58:01Z", "2023-02-15T20:46:18Z", "2023-02-08T23:37:33Z", "2023-01-31T21:43:31Z", "2023-01-27T23:14:33Z", "2023-01-18T22:52:11Z", "2023-01-05T20:07:23Z"]}, {"name": "sagemaker-chainer-container", "description": "Docker container for running Chainer scripts to train and host Chainer models on SageMaker", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2020-03-25T11:29:47Z", "2020-01-08T11:29:26Z", "2019-12-18T11:38:57Z", "2019-10-28T11:40:24Z", "2019-10-22T11:37:19Z", "2019-09-06T20:38:38Z", "2019-09-05T12:01:26Z", "2019-08-26T12:50:28Z", "2019-08-21T13:04:19Z", "2019-08-19T21:25:05Z", "2019-06-25T05:30:47Z"]}, {"name": "sagemaker-containers", "description": "WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2020-06-22T16:26:15Z", "2020-06-18T16:20:00Z", "2020-04-22T16:24:39Z", "2020-04-01T21:28:16Z", "2020-04-01T04:38:12Z", "2020-03-31T23:06:37Z", "2020-03-25T16:24:27Z", "2020-03-24T18:09:06Z", "2020-03-20T16:34:59Z", "2020-02-20T16:23:50Z", "2020-02-19T23:57:51Z", "2020-02-17T16:23:46Z", "2020-02-07T18:51:15Z", "2020-02-06T19:42:59Z", "2019-12-18T16:23:55Z", "2019-11-30T03:32:30Z", "2019-11-25T00:48:16Z", "2019-11-15T19:59:42Z", "2019-10-29T18:46:06Z", "2019-10-24T16:22:41Z", "2019-09-25T16:23:55Z", "2019-09-24T16:24:09Z", "2019-09-23T16:24:04Z", "2019-09-19T21:10:39Z", "2019-07-31T16:23:14Z", "2019-07-30T16:23:17Z", "2019-07-22T16:23:22Z", "2019-07-18T16:23:11Z", "2019-06-27T16:23:09Z", "2019-06-24T16:23:09Z"]}, {"name": "sagemaker-distribution", "description": "A set of Docker images that include popular frameworks for machine learning, data science and visualization.", "language": "Dockerfile", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Amazon SageMaker Distribution\n\nAmazon SageMaker Distribution is a set of Docker images that include popular frameworks for machine learning, data\nscience and visualization.\n\nThese images come in two variants, CPU and GPU, and include deep learning frameworks like PyTorch, TensorFlow and\nKeras; popular Python packages like numpy, scikit-learn and pandas; and IDEs like Jupyter Lab. The distribution contains\nthe _latest_ versions of all these packages _such that_ they are _mutually compatible_.\n\nThis project follows semver (more on that below) and comes with a helper tool to automate new releases of the\ndistribution.\n\n## Getting started\n\nIf you just want to use the images, you do _not_ need to use this GitHub repository. Instead, you can pull pre-built\nand ready-to-use images from our [AWS ECR Gallery repository](https://gallery.ecr.aws/sagemaker/sagemaker-distribution).\n\n### Dependency versions included in a particular Amazon SageMaker Distribution version\n\nIf you want to check what packages are installed in a given version of Amazon SageMaker Distribution, you can find that\nin the relevant _RELEASE.md_ file in the [build_artifacts](build_artifacts) directory.\n\n### Versioning strategy\n\nAmazon SageMaker Distribution supports semantic versioning as described on [semver.org](https://semver.org/). A major \nversion upgrade of Amazon SageMaker Distribution allows major version upgrades of all its dependencies, and similarly\nfor minor and patch version upgrades. However, it is important to note that Amazon SageMaker Distribution\u2019s ability to\nfollow semver guidelines is currently dependent on how its dependencies adhere to them.\n\nSome dependencies, such as Python, will be treated differently. Amazon SageMaker Distribution will allow a minor\nupgrade of Python (say, 3.10 to 3.11) only during a major upgrade (say, 4.8 to 5.0).\n\n### Image tags\n\nOur current image tagging scheme is: `<AMAZON_SAGEMAKER_DISTRIBUTION_VERSION_NUMBER>-<CPU_OR_GPU>`. For example, the CPU\nversion of Amazon SageMaker Distribution's _v0.1.2_ will carry the following tags:\n\n1. `0.1.2-cpu`: Once an image is tagged with such a patch version, that tag will _not_ be assigned to any other image\nin future.\n1. `0.1-cpu`: this, and the two below, _can_ change when new versions of Amazon SageMaker Distribution are released.\n1. `0-cpu`\n1. `latest-cpu`\n\nSo, if you want to stay on the latest software as and when release by Amazon SageMaker Distribution, you can use\n`latest-cpu` and do a `docker pull latest-cpu` when needed. If you use, say, `0.1.2-cpu`, the underlying distribution\nwill remain the same over time.\n\n### Package Staleness Report\n\nIf you want to generate/view the staleness report for each of the individual packages in a given \nSageMaker distribution image version, then run the following command:\n\n```\nVERSION=<Insert SageMaker Distribution version in semver format here. example: 0.4.2>\npython ./src/main.py generate-staleness-report --target-patch-version $VERSION\n```\n\n\n\n## Example use cases\n\nHere are some examples on how you can try out one of our images.\n\n### _Local_ environment, such as your laptop\n\nThe easiest way to get it running on your laptop is through the Docker CLI:\n\n```shell\nexport ECR_IMAGE_ID='INSERT_IMAGE_YOU_WANT_TO_USE'\ndocker run -it \\\n    -p 8888:8888 \\\n    --user `id -u`:`id -g` \\\n    -v `pwd`/sample-notebooks:/home/sagemaker-user/sample-notebooks \\\n    $ECR_IMAGE_ID jupyter-lab --no-browser --ip=0.0.0.0\n```\n\n(If you have access to Nvidia GPUs, you can pass `--gpus=all` to the Docker command.)\n\nIn the image, we also have entrypoints built in, that automatically starts IDE server and automatically restarts\nIDE server in case of minor IDE server interruptions or crashes. For example, to start JupyterLab server using the\nentrypoint built in:\n```shell\nexport ECR_IMAGE_ID='INSERT_IMAGE_YOU_WANT_TO_USE'\ndocker run -it \\\n    -p 8888:8888 \\\n    --entrypoint entrypoint-jupyter-server \\\n    --user `id -u`:`id -g` \\\n    -v `pwd`/sample-notebooks:/home/sagemaker-user/sample-notebooks \\\n    $ECR_IMAGE_ID\n```\n\nIn the console output, you'll then see a URL similar to `http://127.0.0.1:8888/lab?token=foo`. Just open that URL in\nyour browser, create a Jupyter Lab notebook or open a terminal, and start hacking.\n\nNote that the sample command above bind mounts a directory in `pwd` inside the container. That way, if you were to\nre-create the container (say, to use a different version or CPU/GPU variant), any files you created within that\ndirectory (such as Jupyter Lab notebooks) will persist.\n\n### Amazon SageMaker Studio\n\n> [Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html) is a web-based, integrated \n> development environment (IDE) for machine learning that lets you build, train, debug, deploy, and monitor your\n> machine learning models.\n\nTo use the sagemaker-distribution image in SageMaker Studio, select `SageMaker Distribution v{Major_version} {CPU/GPU}` using the [SageMaker Studio Launcher](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-launcher.html).\n\n### \"I want to directly use the Conda environment, not via a Docker image\"\n\nAmazon SageMaker Distribution supports full reproducibility of Conda environments, so you don't necessarily need to use\nDocker. Just find the version number you want to use in the [build_artifacts](build_artifacts) directory, open one of\n_cpu.env.out_ or _gpu.env.out_ and follow the instructions in the first 2 lines.\n\n### Customizing image\n\nIf you'd like to create a new Docker image on top of what we offer, we recommend you use `micromamba install ...` instead of `pip install ...`.\n\nFor example:\n```\nFROM public.ecr.aws/sagemaker/sagemaker-distribution:latest-cpu\nUSER $ROOT\nRUN apt-get install -y vim\nUSER $MAMBA_USER\nRUN micromamba install sagemaker-inference --freeze-installed --yes --channel conda-forge --name base\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": []}, {"name": "sagemaker-example-notebooks-testing", "description": "A continuous integration (CI) system for \ud83d\udcd3 Jupyter notebooks, built using \ud83e\udde0 Amazon SageMaker.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": ":warning: **Warning**: This repository is experimental software designed for trial use. It may change significantly in the future and there is no guarantee of support. Please do use it and give us feedback on what we could improve, but take its experimental nature into account.\n\n![SageMaker](https://github.com/aws/sagemaker-example-notebooks-testing/raw/main/branding/icon/sagemaker-banner.png)\n\n# SageMaker Example Notebooks Testing\n\nA continuous integration (CI) system for Jupyter notebooks, built using Amazon SageMaker.\n\n## :books: Background\n\n[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service for data science and machine learning (ML) workflows.\nYou can use Amazon SageMaker to simplify the process of building, training, and deploying ML models.\n\nThe [SageMaker example notebooks](https://sagemaker-examples.readthedocs.io/en/latest/) are Jupyter notebooks that demonstrate the usage of Amazon SageMaker.\n\nThis **SageMaker Example Notebooks Testing** repository contains the code for the open-source CI system for the example notebooks.\nIt can be repurposed to build a CI system for your own Jupyter notebooks on SageMaker.\n\n## :balance_scale: License\n\nThis library is licensed under the [Apache 2.0 License](http://aws.amazon.com/apache2.0/).\nFor more details, please take a look at the [LICENSE](https://github.com/aws/sagemaker-example-notebooks-testing/blob/main/LICENSE) file.\n\n## :handshake: Contributing\n\nContributions are welcome!\nPlease read our [contributing guidelines](https://github.com/aws/sagemaker-example-notebooks-testing/blob/main/CONTRIBUTING.md)\nif you'd like to open an issue or submit a pull request.\n", "release_dates": []}, {"name": "sagemaker-experiments", "description": "Experiment tracking and metric logging for Amazon SageMaker notebooks and model training.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2023-05-17T17:31:23Z", "2023-05-09T17:28:47Z", "2023-03-02T22:42:11Z", "2022-12-08T18:27:41Z", "2022-11-23T22:37:37Z", "2022-10-13T17:22:15Z", "2022-09-08T17:18:43Z", "2022-05-11T17:18:04Z", "2022-05-09T17:26:19Z", "2022-05-04T17:16:22Z", "2021-08-10T17:14:57Z", "2021-07-30T17:15:33Z", "2021-06-10T17:27:49Z", "2021-06-07T17:41:13Z", "2021-05-06T17:45:35Z", "2021-04-13T17:28:38Z", "2021-03-17T18:01:15Z", "2021-02-18T17:31:56Z", "2021-02-15T17:32:28Z", "2021-01-25T18:29:19Z", "2020-12-01T21:06:57Z", "2020-07-31T18:11:45Z", "2020-07-31T17:15:47Z", "2020-07-29T17:12:40Z", "2020-07-27T17:10:03Z", "2020-07-15T17:10:54Z", "2020-07-10T17:10:50Z", "2020-07-09T17:10:57Z", "2020-07-02T17:10:17Z", "2020-06-17T17:10:12Z"]}, {"name": "sagemaker-feature-store-spark", "description": null, "language": "Scala", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "SageMaker FeatureStore Spark is an open source Spark library for [Amazon SageMaker FeatureStore](https://aws.amazon.com/sagemaker/feature-store/).\n\nWith this spark connector, you can easily ingest data to FeatureGroup's online and offline store from Spark `DataFrame`.\n\n## Installation\n\n### Scala\n\nThe library is compatible with Scala >= 2.12, and Spark >= 3.0.0. \nIf your application is on EMR, pleas use emr-6.x.\n\nTODO: Add instructions here how to install the library from Maven.\n\nAfter the library is imported, you can build your application into a jar and submit the application using `spark-shell` or `spark-submit`.\n\n#### EMR\n\nOnce you import the spark library as a dependency in your application, you shoud be able to submit the spark job according to this EMR [documentation](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-submit-step.html).  \n\n### Python\n\nThe library is compatible with Python >= 3.6, and PySpark >= 3.0.0.\nIf your application is on EMR, pleas use emr-6.x.\n\nPlease also make sure that the environment has PySpark and Numpy installed.\n\nThe spark library is available on [PyPi](https://pypi.org/project/sagemaker-feature-store-pyspark/)\nBefore installation, it is recommended to set `SPARK_HOME` environment variable to the path where your Spark is installed, because during installation the library will automatically copy some depedent jars to `SPARK_HOME`.\nFor EMR, the library installation will handle the path automatically, so there is no need to specify `SPARK_HOME` if you're installing on EMR.\n\nTo install the library:\n```\nsudo -E pip3 install sagemaker-feature-store-pyspark --no-binary :all:\n```\n\n#### EMR\n\nCreate a custom jar step of EMR to start the library installation\n\nIf your EMR has single node:\n```\nJar Location: command-runner.jar\nArguments: sudo -E pip3 install sagemaker-feature-store-pyspark \u2014no-binary :all:\n```\nThis will only install the library on `Driver` node.\n\nTo distribute the library to all executor nodes, you can create a installation script and add a custom bootstrap while creating EMR cluster.\n\nFor more information, pleas take a look at [EMR bootstramp](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html).\nSince bootstrap action is executed before all EMR applications are installed, so dependent jars cannot be automatically loaded to `SPARK_HOME`.\nSo when submitting your application, please specify dependent jars using:\n```\n--jars `feature-store-pyspark-dependency-jars`\n```\n\n#### SageMaker Notebook\n\nSince SageMaker Notebook instances are using older version of Spark library which is not compatible with the spark version of FeatureStore Spark library. The Spark on SageMaker Notebook instance has to be uninstalled first and reinstall with a newer version.\n\nSo, add a cell like this in your notebook:\n```\n# Install a newer versiion of Spark which is compatible with spark library\n!pip3 install pyspark==3.1.1\n```\n\nAfter finish executing the notebook, you can restore the original version which is Spark-2.4.0.\n\n## Getting Started\n\n`FeatureStoreManager` is the interface for all Spark library operations, such as data ingestion and loading feature definitions.\n\n### Scala\n\nTo ingest a DataFrame into FeatureStore:\n\n```\nimport com.amazonaws.services.sagemaker.featurestore.sparksdk.FeatureStoreManager\n\nval featureGroupArn = <your-feature-group-arn>\nval featureStoreManager = new FeatureStoreManager()\nfeatureStoreManager.ingestData(dataFrame, featureGroupArn, directOfflineStore = true)\n```\nIf `directOfflineStore` is specified to true, the spark library will ingest data directly to OfflineStore without using FeatureStoreRuntime API which is going to cut the cost on FeatureStore WCU, the default value for this flag is false.\n\nTo load feature definitions:\n\n```\nval featureDefinitions = featureStoreManager.loadFeatureDefinitionsFromSchema(inputDataFrame)\n```\n\nAfter the feature definitions are retured, you can create feature groups using `CreateFeatureGroup` API.\n\n### Python\n\nTo ingest a DataFrame into FeatureStore:\n\n```\nfrom feature_store_pyspark.FeatureStoreManager import FeatureStoreManager\n\nfeature_group_arn = <your-feature-group-arn>\nfeature_store_manager = FeatureStoreManager()\nfeature_store_manager.ingest_data(input_data_frame=user_data_frame, feature_group_arn=feature_group_arn, direct_offline_store=True)\n```\nIf `direct_offline_store` is specified to true, the spark library will ingest data directly to OfflineStore without using FeatureStoreRuntime API which is going to cut the cost on FeatureStore WCU, the default value for this flag is false.\n\nTo load feature definitions:\n\n```\nfeature_definitions = feature_store_manager.load_feature_definitions_from_schema(user_data_frame)\n```\n\nAfter the feature definitions are retured, you can create feature groups using `CreateFeatureGroup` API.\n\n## Development\n\n### New Features\n\nThis library is built in Scala, and Python methods are actually calling Scala via JVM through `wrapper.py`. To add more features, please make sure you finish the implementation in Scala first and perhaps you need to update the wrapper so that functionality of Scala and Python are in sync.\n\n### Test Coverage\n\nBoth Scala and Python versions have unit test covered. In addition to that, we have integration test for Python version which verifies there is no regressions in terms of functionality.\n\n#### Scala Package Build\n\nIt is a good practice to keep our code always formatted correctly, we used `scalafmt` to auto format the code. So, please run `sbt scalafmtAll` to format your code.\n\nTo get the test coverage report and format check result, run `sbt jacoco scalafmtCheckAll`.\n\n#### Python Package Build\n\nWe are using `tox` for test purposes, you can check the build by running `tox`. To configure or figure out the command we run by tox, please checkout `tox.ini`.\n\n#### Integration Test\n\nThe test execution script and test itself are included in `pyspark-sdk/integration_test`, to run the test: \n\n1. Fetch the credential from our spark test account first.\n2. Run the test execution script `run-spark-integration-test`\n\n#### Github Repository Automated Testing\n\nThe Github repository is connected to the CodeBuild project in our spark test account. Modify the command steps in `ci/buildspec.yml` according to your demands.\n\n## More Reference\n\n[Spark Application on EMR](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-application.html)\n\n[Add Spark EMR Steps](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-submit-step.html)\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2023-06-05T23:36:42Z", "2023-03-02T18:47:44Z", "2022-12-08T22:11:00Z", "2021-12-16T23:55:00Z"]}, {"name": "sagemaker-huggingface-inference-toolkit", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<div style=\"display:flex; text-align:center;\">\n<img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" width=\"100\"/> \n<img src=\"https://github.com/aws/sagemaker-inference-toolkit/raw/master/branding/icon/sagemaker-banner.png\" width=\"450\"/>\n</div>\n\n\n\n\n# SageMaker Hugging Face Inference Toolkit \n\n[![Latest Version](https://img.shields.io/pypi/v/sagemaker_huggingface_inference_toolkit.svg)](https://pypi.python.org/pypi/sagemaker_huggingface_inference_toolkit) [![Supported Python Versions](https://img.shields.io/pypi/pyversions/sagemaker_huggingface_inference_toolkit.svg)](https://pypi.python.org/pypi/sagemaker_huggingface_inference_toolkit) [![Code Style: Black](https://img.shields.io/badge/code_style-black-000000.svg)](https://github.com/python/black)\n\n\nSageMaker Hugging Face Inference Toolkit is an open-source library for serving \ud83e\udd17 Transformers and Diffusers models on Amazon SageMaker. This library provides default pre-processing, predict and postprocessing for certain \ud83e\udd17 Transformers and Diffusers models and tasks. It utilizes the [SageMaker Inference Toolkit](https://github.com/aws/sagemaker-inference-toolkit) for starting up the model server, which is responsible for handling inference requests.\n\nFor Training, see [Run training on Amazon SageMaker](https://huggingface.co/docs/sagemaker/train).\n\nFor the Dockerfiles used for building SageMaker Hugging Face Containers, see [AWS Deep Learning Containers](https://github.com/aws/deep-learning-containers/tree/master/huggingface).\n\nFor information on running Hugging Face jobs on Amazon SageMaker, please refer to the [\ud83e\udd17 Transformers documentation](https://huggingface.co/docs/sagemaker).\n\nFor notebook examples: [SageMaker Notebook Examples](https://github.com/huggingface/notebooks/tree/master/sagemaker).\n\n---\n## \ud83d\udcbb  Getting Started with \ud83e\udd17 Inference Toolkit\n\n_needs to be adjusted -> currently pseudo code_\n\n**Install Amazon SageMaker Python SDK**\n\n```bash\npip install sagemaker --upgrade\n```\n\n**Create a Amazon SageMaker endpoint with a trained model.**\n\n```python\nfrom sagemaker.huggingface import HuggingFaceModel\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n    transformers_version='4.6',\n    pytorch_version='1.7',\n    py_version='py36',\n    model_data='s3://my-trained-model/artifacts/model.tar.gz',\n    role=role,\n)\n# deploy model to SageMaker Inference\nhuggingface_model.deploy(initial_instance_count=1,instance_type=\"ml.m5.xlarge\")\n```\n\n\n**Create a Amazon SageMaker endpoint with a model from the [\ud83e\udd17 Hub](https://huggingface.co/models).**  \n_note: This is an experimental feature, where the model will be loaded after the endpoint is created. Not all sagemaker features are supported, e.g. MME_\n```python\nfrom sagemaker.huggingface import HuggingFaceModel\n# Hub Model configuration. https://huggingface.co/models\nhub = {\n  'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad',\n  'HF_TASK':'question-answering'\n}\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n    transformers_version='4.6',\n    pytorch_version='1.7',\n    py_version='py36',\n    env=hub,\n    role=role,\n)\n# deploy model to SageMaker Inference\nhuggingface_model.deploy(initial_instance_count=1,instance_type=\"ml.m5.xlarge\")\n```\n\n---\n\n## \ud83d\udee0\ufe0f Environment variables\n\nThe SageMaker Hugging Face Inference Toolkit implements various additional environment variables to simplify your deployment experience. A full list of environment variables is given below.\n\n#### `HF_TASK`\n\nThe `HF_TASK` environment variable defines the task for the used \ud83e\udd17 Transformers pipeline. A full list of tasks can be find [here](https://huggingface.co/transformers/main_classes/pipelines.html).\n\n```bash\nHF_TASK=\"question-answering\"\n```\n\n#### `HF_MODEL_ID`\n\nThe `HF_MODEL_ID` environment variable defines the model id, which will be automatically loaded from [huggingface.co/models](https://huggingface.co/models) when creating or SageMaker Endpoint. The \ud83e\udd17 Hub provides +10 000 models all available through this environment variable.\n\n```bash\nHF_MODEL_ID=\"distilbert-base-uncased-finetuned-sst-2-english\"\n```\n\n#### `HF_MODEL_REVISION`\n\nThe `HF_MODEL_REVISION` is an extension to `HF_MODEL_ID` and allows you to define/pin a revision of the model to make sure you always load the same model on your SageMaker Endpoint.\n\n```bash\nHF_MODEL_REVISION=\"03b4d196c19d0a73c7e0322684e97db1ec397613\"\n```\n\n#### `HF_API_TOKEN`\n\nThe `HF_API_TOKEN` environment variable defines the your Hugging Face authorization token. The `HF_API_TOKEN` is used as a HTTP bearer authorization for remote files, like private models. You can find your token at your [settings page](https://huggingface.co/settings/token).\n\n```bash\nHF_API_TOKEN=\"api_XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n```\n\n#### `HF_TRUST_REMOTE_CODE`\n\nThe `HF_TRUST_REMOTE_CODE` environment variable defines wether or not to allow for custom models defined on the Hub in their own modeling files. Allowed values are `\"True\"` and `\"False\"`\n\n```bash\nHF_TRUST_REMOTE_CODE=\"True\"\n```\n\n#### `HF_OPTIMUM_BATCH_SIZE`\n\nThe `HF_OPTIMUM_BATCH_SIZE` environment variable defines the batch size, which is used when compiling the model to Neuron. The default value is `1`. Not required when model is already converted. \n\n```bash\nHF_OPTIMUM_BATCH_SIZE=\"1\"\n```\n\n#### `HF_OPTIMUM_SEQUENCE_LENGTH`\n\nThe `HF_OPTIMUM_SEQUENCE_LENGTH` environment variable defines the sequence length, which is used when compiling the model to Neuron. There is no default value. Not required when model is already converted. \n\n```bash\nHF_OPTIMUM_SEQUENCE_LENGTH=\"128\"\n```\n\n---\n\n## \ud83e\uddd1\ud83c\udffb\u200d\ud83d\udcbb User defined code/modules\n\nThe Hugging Face Inference Toolkit allows user to override the default methods of the `HuggingFaceHandlerService`. Therefore, they need to create a folder named `code/` with an `inference.py` file in it. You can find an example for it in [sagemaker/17_customer_inference_script](https://github.com/huggingface/notebooks/blob/master/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb).\nFor example:  \n```bash\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n  |- inference.py\n  |- requirements.txt \n```\nIn this example, `pytorch_model.bin` is the model file saved from training, `inference.py` is the custom inference module, and `requirements.txt` is a requirements file to add additional dependencies.\nThe custom module can override the following methods:  \n\n* `model_fn(model_dir)`: overrides the default method for loading the model, the return value `model` will be used in the `predict()` for predicitions. It receives argument the `model_dir`, the path to your unzipped `model.tar.gz`.\n* `transform_fn(model, data, content_type, accept_type)`: overrides the default transform function with a custom implementation. Customers using this would have to implement `preprocess`, `predict` and `postprocess` steps in the `transform_fn`. **NOTE: This method can't be combined with `input_fn`, `predict_fn` or `output_fn` mentioned below.** \n* `input_fn(input_data, content_type)`: overrides the default method for preprocessing, the return value `data` will be used in the `predict()` method for predicitions. The input is `input_data`, the raw body of your request and `content_type`, the content type form the request Header.\n* `predict_fn(processed_data, model)`: overrides the default method for predictions, the return value `predictions` will be used in the `postprocess()` method. The input is `processed_data`, the result of the `preprocess()` method.\n* `output_fn(prediction, accept)`: overrides the default method for postprocessing, the return value `result` will be the respond of your request(e.g.`JSON`). The inputs are `predictions`, the result of the `predict()` method and `accept` the return accept type from the HTTP Request, e.g. `application/json`\n\n\n\n\n---\n## \ud83e\udd1d Contributing\n\nPlease read [CONTRIBUTING.md](https://github.com/aws/sagemaker-huggingface-inference-toolkit/blob/main/CONTRIBUTING.md)\nfor details on our code of conduct, and the process for submitting pull\nrequests to us.\n\n---\n## \ud83d\udcdc  License\n\nSageMaker Hugging Face Inference Toolkit is licensed under the Apache 2.0 License.\n\n---\n\n## \ud83e\uddd1\ud83c\udffb\u200d\ud83d\udcbb Development Environment\n\nInstall all test and development packages with \n\n```bash\npip3 install -e \".[test,dev]\"\n```\n## Run Model Locally\n\n1. manually change `MMS_CONFIG_FILE`\n```\nwget -O sagemaker-mms.properties https://raw.githubusercontent.com/aws/deep-learning-containers/master/huggingface/build_artifacts/inference/config.properties\n```\n\n2. Run Container, e.g. `text-to-image`\n```\nHF_MODEL_ID=\"stabilityai/stable-diffusion-xl-base-1.0\" HF_TASK=\"text-to-image\" python src/sagemaker_huggingface_inference_toolkit/serving.py\n```\n3. Adjust `handler_service.py` and comment out `if content_type in content_types.UTF8_TYPES:` thats needed for SageMaker but cannot be used locally\n\n3. Send request \n```\ncurl --request POST \\\n  --url http://localhost:8080/invocations \\\n  --header 'Accept: image/png' \\\n  --header 'Content-Type: application/json' \\\n  --data '\"{\\\"inputs\\\": \\\"Camera\\\"}\" \\\n  --output image.png\n```", "release_dates": ["2023-08-15T11:30:15Z", "2023-06-22T19:40:07Z", "2022-12-12T21:54:54Z", "2022-10-25T16:38:42Z", "2022-03-25T07:40:05Z", "2022-02-10T08:41:24Z", "2021-12-16T12:35:10Z", "2021-06-25T18:32:02Z"]}, {"name": "sagemaker-inference-toolkit", "description": "Serve machine learning models within a \ud83d\udc33 Docker container using \ud83e\udde0 Amazon SageMaker.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![SageMaker](https://github.com/aws/sagemaker-inference-toolkit/raw/master/branding/icon/sagemaker-banner.png)\n\n# SageMaker Inference Toolkit\n\n[![Latest Version](https://img.shields.io/pypi/v/sagemaker-inference.svg)](https://pypi.python.org/pypi/sagemaker-inference) [![Supported Python Versions](https://img.shields.io/pypi/pyversions/sagemaker-inference.svg)](https://pypi.python.org/pypi/sagemaker-inference) [![Code Style: Black](https://img.shields.io/badge/code_style-black-000000.svg)](https://github.com/python/black)\n\nServe machine learning models within a Docker container using Amazon\nSageMaker.\n\n## :books: Background\n\n[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service for data science and machine learning (ML) workflows.\nYou can use Amazon SageMaker to simplify the process of building, training, and deploying ML models.\n\nOnce you have a trained model, you can include it in a [Docker container](https://www.docker.com/resources/what-container) that runs your inference code.\nA container provides an effectively isolated environment, ensuring a consistent runtime regardless of where the container is deployed.\nContainerizing your model and code enables fast and reliable deployment of your model.\n\nThe **SageMaker Inference Toolkit** implements a model serving stack and can be easily added to any Docker container, making it [deployable to SageMaker](https://aws.amazon.com/sagemaker/deploy/).\nThis library's serving stack is built on [Multi Model Server](https://github.com/awslabs/multi-model-server), and it can serve your own models or those you trained on SageMaker using [machine learning frameworks with native SageMaker support](https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html).\nIf you use a [prebuilt SageMaker Docker image for inference](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html), this library may already be included.\n\nFor more information, see the Amazon SageMaker Developer Guide sections on [building your own container with Multi Model Server](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html) and [using your own models](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html).\n\n## :hammer_and_wrench: Installation\n\nTo install this library in your Docker image, add the following line to your [Dockerfile](https://docs.docker.com/engine/reference/builder/):\n\n``` dockerfile\nRUN pip3 install multi-model-server sagemaker-inference\n```\n\n[Here is an example](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_bring_your_own/container/Dockerfile) of a Dockerfile that installs SageMaker Inference Toolkit.\n\n## :computer: Usage\n\n### Implementation Steps\n\nTo use the SageMaker Inference Toolkit, you need to do the following:\n\n1.  Implement an inference handler, which is responsible for loading the model and providing input, predict, and output functions.\n    ([Here is an example](https://github.com/aws/sagemaker-pytorch-serving-container/blob/master/src/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py) of an inference handler.)\n\n    ``` python\n    from sagemaker_inference import content_types, decoder, default_inference_handler, encoder, errors\n\n    class DefaultPytorchInferenceHandler(default_inference_handler.DefaultInferenceHandler):\n\n        def default_model_fn(self, model_dir, context=None):\n            \"\"\"Loads a model. For PyTorch, a default function to load a model cannot be provided.\n            Users should provide customized model_fn() in script.\n\n            Args:\n                model_dir: a directory where model is saved.\n                context (obj): the request context (default: None).\n\n            Returns: A PyTorch model.\n            \"\"\"\n            raise NotImplementedError(textwrap.dedent(\"\"\"\n            Please provide a model_fn implementation.\n            See documentation for model_fn at https://github.com/aws/sagemaker-python-sdk\n            \"\"\"))\n\n        def default_input_fn(self, input_data, content_type, context=None):\n            \"\"\"A default input_fn that can handle JSON, CSV and NPZ formats.\n\n            Args:\n                input_data: the request payload serialized in the content_type format\n                content_type: the request content_type\n                context (obj): the request context (default: None).\n\n            Returns: input_data deserialized into torch.FloatTensor or torch.cuda.FloatTensor depending if cuda is available.\n            \"\"\"\n            return decoder.decode(input_data, content_type)\n\n        def default_predict_fn(self, data, model, context=None):\n            \"\"\"A default predict_fn for PyTorch. Calls a model on data deserialized in input_fn.\n            Runs prediction on GPU if cuda is available.\n\n            Args:\n                data: input data (torch.Tensor) for prediction deserialized by input_fn\n                model: PyTorch model loaded in memory by model_fn\n                context (obj): the request context (default: None).\n\n            Returns: a prediction\n            \"\"\"\n            return model(input_data)\n\n        def default_output_fn(self, prediction, accept, context=None):\n            \"\"\"A default output_fn for PyTorch. Serializes predictions from predict_fn to JSON, CSV or NPY format.\n\n            Args:\n                prediction: a prediction result from predict_fn\n                accept: type which the output data needs to be serialized\n                context (obj): the request context (default: None).\n\n            Returns: output data serialized\n            \"\"\"\n            return encoder.encode(prediction, accept)\n    ```\n    Note, passing context as an argument to the handler functions is optional. Customer can choose to omit context from the function declaration if it's not needed in the runtime. For example, the following handler function declarations will also work:\n\n    ```\n    def default_model_fn(self, model_dir)\n\n    def default_input_fn(self, input_data, content_type)\n\n    def default_predict_fn(self, data, model)\n\n    def default_output_fn(self, prediction, accept)\n    ``` \n\n2.  Implement a handler service that is executed by the model server.\n    ([Here is an example](https://github.com/aws/sagemaker-pytorch-serving-container/blob/master/src/sagemaker_pytorch_serving_container/handler_service.py) of a handler service.)\n    For more information on how to define your `HANDLER_SERVICE` file, see [the MMS custom service documentation](https://github.com/awslabs/multi-model-server/blob/master/docs/custom_service.md).\n\n    ``` python\n    from sagemaker_inference.default_handler_service import DefaultHandlerService\n    from sagemaker_inference.transformer import Transformer\n    from sagemaker_pytorch_serving_container.default_inference_handler import DefaultPytorchInferenceHandler\n\n\n    class HandlerService(DefaultHandlerService):\n        \"\"\"Handler service that is executed by the model server.\n        Determines specific default inference handlers to use based on model being used.\n        This class extends ``DefaultHandlerService``, which define the following:\n            - The ``handle`` method is invoked for all incoming inference requests to the model server.\n            - The ``initialize`` method is invoked at model server start up.\n        Based on: https://github.com/awslabs/multi-model-server/blob/master/docs/custom_service.md\n        \"\"\"\n        def __init__(self):\n            transformer = Transformer(default_inference_handler=DefaultPytorchInferenceHandler())\n            super(HandlerService, self).__init__(transformer=transformer)\n    ```\n\n3.  Implement a serving entrypoint, which starts the model server.\n    ([Here is an example](https://github.com/aws/sagemaker-pytorch-serving-container/blob/master/src/sagemaker_pytorch_serving_container/serving.py) of a serving entrypoint.)\n\n    ``` python\n    from sagemaker_inference import model_server\n\n    model_server.start_model_server(handler_service=HANDLER_SERVICE)\n    ```\n\n4.  Define the location of the entrypoint in your Dockerfile.\n\n    ``` dockerfile\n    ENTRYPOINT [\"python\", \"/usr/local/bin/entrypoint.py\"]\n    ```\n\n### Complete Example\n\n[Here is a complete example](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/multi_model_bring_your_own) demonstrating usage of the SageMaker Inference Toolkit in your own container for deployment to a multi-model endpoint.\n\n## :scroll: License\n\nThis library is licensed under the [Apache 2.0 License](http://aws.amazon.com/apache2.0/).\nFor more details, please take a look at the [LICENSE](https://github.com/aws-samples/sagemaker-inference-toolkit/blob/master/LICENSE) file.\n\n## :handshake: Contributing\n\nContributions are welcome!\nPlease read our [contributing guidelines](https://github.com/aws/sagemaker-inference-toolkit/blob/master/CONTRIBUTING.md)\nif you'd like to open an issue or submit a pull request.\n", "release_dates": ["2023-10-25T09:38:14Z", "2023-07-25T09:30:35Z", "2023-06-20T09:30:04Z", "2023-04-04T09:32:26Z", "2023-03-22T09:30:15Z", "2023-03-17T20:15:12Z", "2023-02-01T00:20:13Z", "2022-10-14T19:28:29Z", "2022-09-08T09:41:48Z", "2022-08-22T09:42:24Z", "2022-05-12T23:00:11Z", "2022-05-10T09:40:37Z", "2022-02-02T09:42:06Z", "2022-02-01T09:39:20Z", "2022-01-08T00:31:04Z", "2021-12-23T18:20:07Z", "2021-12-10T19:32:35Z", "2021-12-09T19:45:49Z", "2021-01-30T00:40:17Z", "2021-01-18T09:26:14Z", "2020-10-15T09:26:47Z", "2020-08-04T09:26:36Z", "2020-07-31T18:23:24Z", "2020-07-30T09:26:25Z", "2020-07-27T09:26:54Z", "2020-06-29T17:04:44Z", "2020-06-16T09:25:38Z", "2020-05-25T09:25:23Z", "2020-05-11T19:20:13Z", "2020-05-07T21:05:37Z"]}, {"name": "sagemaker-jumpstart-industry-pack", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": []}, {"name": "sagemaker-mxnet-inference-toolkit", "description": "Toolkit for allowing inference and serving with MXNet in SageMaker. Dockerfiles used for building SageMaker MXNet Containers are at https://github.com/aws/deep-learning-containers. ", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2022-05-23T11:19:34Z", "2022-01-08T01:31:37Z", "2021-12-17T16:38:09Z", "2021-12-16T19:34:56Z", "2021-04-12T11:19:36Z", "2020-09-14T20:55:14Z", "2020-08-17T11:19:24Z", "2020-08-06T11:20:38Z", "2020-06-25T11:19:26Z", "2020-06-16T23:38:22Z", "2020-05-20T11:19:38Z", "2020-05-19T11:19:33Z", "2020-05-18T11:19:42Z", "2020-05-14T11:19:28Z", "2020-05-12T20:00:48Z", "2020-04-30T11:18:58Z", "2020-04-28T11:18:28Z", "2020-04-01T22:01:54Z", "2020-04-01T11:18:23Z", "2020-03-30T11:18:27Z", "2020-03-24T11:18:24Z", "2020-03-11T11:18:33Z", "2020-03-09T11:18:19Z", "2020-03-04T23:21:52Z", "2020-02-20T11:18:24Z", "2020-02-19T11:18:20Z", "2020-02-17T11:18:34Z", "2020-02-12T05:43:02Z", "2020-01-22T18:45:32Z", "2019-10-22T12:41:59Z"]}, {"name": "sagemaker-mxnet-training-toolkit", "description": "Toolkit for running MXNet training scripts on SageMaker. Dockerfiles used for building SageMaker MXNet Containers are at https://github.com/aws/deep-learning-containers. ", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2020-09-14T20:59:43Z", "2020-07-27T12:17:58Z", "2020-07-08T12:17:52Z", "2020-06-25T12:17:51Z", "2020-06-17T20:07:54Z", "2020-06-17T12:17:50Z", "2020-06-09T23:19:48Z", "2020-05-12T20:01:08Z", "2020-05-07T12:18:23Z", "2020-04-30T12:18:45Z", "2020-04-27T12:18:22Z", "2020-04-07T12:18:28Z", "2020-04-06T12:18:32Z", "2020-04-02T12:18:35Z", "2020-04-01T12:18:35Z", "2020-03-30T12:18:27Z", "2020-03-23T12:18:24Z", "2020-03-18T12:18:31Z", "2020-03-16T12:18:30Z", "2020-03-12T12:18:31Z", "2020-03-11T12:18:21Z", "2020-02-20T12:18:19Z", "2020-02-17T12:18:20Z", "2020-02-13T12:18:20Z", "2020-02-11T12:18:30Z", "2020-02-10T12:18:22Z", "2020-02-05T21:17:01Z", "2020-01-11T02:12:44Z", "2019-10-28T14:24:19Z", "2019-10-22T14:22:47Z"]}, {"name": "sagemaker-python-sdk", "description": "A library for training and deploying machine learning models on Amazon SageMaker", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2024-02-28T19:54:47Z", "2024-02-24T01:31:17Z", "2024-02-15T22:26:12Z", "2024-02-06T22:33:31Z", "2024-02-05T20:27:40Z", "2024-02-01T05:06:38Z", "2024-02-01T18:20:07Z", "2024-01-31T23:30:55Z", "2024-01-25T17:59:23Z", "2024-01-23T20:32:23Z", "2024-01-09T17:53:35Z", "2023-12-28T23:57:48Z", "2023-12-22T18:29:04Z", "2023-12-21T19:52:38Z", "2023-12-20T02:50:52Z", "2023-12-14T20:08:49Z", "2023-12-13T18:40:45Z", "2023-11-30T02:10:39Z", "2023-11-27T09:49:04Z", "2023-11-07T06:46:10Z", "2023-10-27T01:26:02Z", "2023-10-26T03:34:47Z", "2023-10-25T05:21:53Z", "2023-10-19T20:38:05Z", "2023-10-18T17:09:11Z", "2023-10-13T18:20:46Z", "2023-10-11T21:30:56Z", "2023-10-05T22:51:44Z", "2023-10-04T22:26:56Z", "2023-10-03T01:14:50Z"]}, {"name": "sagemaker-pytorch-inference-toolkit", "description": "Toolkit for allowing inference and serving with PyTorch on SageMaker. Dockerfiles used for building SageMaker Pytorch Containers are at https://github.com/aws/deep-learning-containers.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2023-12-18T14:24:19Z", "2023-10-23T14:24:44Z", "2023-10-19T14:25:02Z", "2023-10-17T14:25:05Z", "2023-10-10T14:25:36Z", "2023-08-07T14:28:26Z", "2023-07-24T14:28:20Z", "2023-05-29T14:28:41Z", "2023-03-22T14:27:05Z", "2023-03-20T14:27:10Z", "2022-11-29T14:28:08Z", "2022-11-07T21:42:33Z", "2022-04-07T14:27:21Z", "2022-04-04T14:28:45Z", "2022-01-13T14:27:07Z", "2021-10-26T14:24:51Z", "2021-10-04T14:23:53Z", "2021-03-17T22:21:58Z", "2020-09-03T14:20:38Z", "2020-08-24T14:20:46Z", "2020-08-12T14:20:51Z", "2020-08-06T14:20:49Z", "2020-08-03T14:21:03Z", "2020-06-25T14:20:41Z", "2020-06-18T14:20:32Z", "2020-06-16T01:44:00Z", "2020-05-12T14:20:36Z", "2020-05-04T14:19:01Z", "2020-04-30T14:19:10Z", "2020-04-28T14:18:43Z"]}, {"name": "sagemaker-pytorch-training-toolkit", "description": "Toolkit for running PyTorch training scripts on SageMaker. Dockerfiles used for building SageMaker Pytorch Containers are at https://github.com/aws/deep-learning-containers.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2023-10-23T15:20:39Z", "2023-04-28T04:21:27Z", "2022-10-20T19:23:55Z", "2022-09-21T15:19:34Z", "2022-08-26T17:50:55Z", "2022-08-15T23:30:55Z", "2022-08-03T02:07:13Z", "2022-07-21T21:18:39Z", "2022-07-08T23:25:45Z", "2022-07-08T22:37:28Z", "2020-12-11T03:59:16Z", "2020-08-31T15:22:07Z", "2020-06-25T15:20:26Z", "2020-06-16T20:34:35Z", "2020-06-05T00:35:42Z", "2020-05-12T15:21:57Z", "2020-05-07T15:21:45Z", "2020-05-05T15:20:28Z", "2020-05-04T15:20:36Z", "2020-04-27T15:25:58Z", "2020-04-16T15:20:19Z", "2020-04-07T15:20:02Z", "2020-04-02T15:20:22Z", "2020-03-23T19:55:38Z", "2020-03-12T15:20:15Z", "2020-03-11T15:19:29Z", "2020-03-10T15:19:35Z", "2020-03-09T15:19:36Z", "2020-02-27T22:07:28Z", "2020-02-09T01:19:35Z"]}, {"name": "sagemaker-rl-container", "description": "A set of dockerfiles that provide Reinforcement Learning solutions for use in SageMaker. ", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": []}, {"name": "sagemaker-scikit-learn-container", "description": "Support code for building and running Amazon SageMaker compatible Docker containers based on the open source framework Scikit-learn (http://scikit-learn.org/stable/)", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2023-02-27T21:43:53Z", "2022-08-05T08:07:11Z", "2022-08-05T08:13:01Z", "2022-08-05T08:11:07Z"]}, {"name": "sagemaker-scikit-learn-extension", "description": "A library of additional estimators and SageMaker tools based on scikit-learn", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2022-02-17T19:58:27Z", "2021-09-23T17:35:45Z", "2021-08-16T21:16:20Z", "2021-05-21T15:05:11Z", "2021-04-13T19:22:11Z", "2020-10-21T16:55:18Z", "2020-08-13T14:12:57Z", "2020-07-29T17:23:17Z", "2020-07-21T17:49:32Z", "2020-02-24T21:19:05Z", "2019-12-03T18:59:36Z"]}, {"name": "sagemaker-spark", "description": "A Spark library for Amazon SageMaker.", "language": "Scala", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# <img alt=\"SageMaker\" src=\"branding/icon/sagemaker-banner.png\" height=\"100\">\n\n# SageMaker Spark\n[![codecov](https://codecov.io/gh/aws/sagemaker-spark/branch/master/graph/badge.svg)](https://codecov.io/gh/aws/sagemaker-spark)\n\nSageMaker Spark is an open source Spark library for [Amazon SageMaker](https://aws.amazon.com/sagemaker/). With SageMaker Spark you construct Spark ML `Pipeline`s using Amazon SageMaker stages. These pipelines interleave native Spark ML stages and stages that interact with SageMaker training and model hosting.\n\nWith SageMaker Spark, you can train on Amazon SageMaker from Spark `DataFrame`s using **Amazon-provided ML algorithms**\nlike K-Means clustering or XGBoost, and make predictions on `DataFrame`s against\nSageMaker endpoints hosting your trained models, and, if you have **your own ML algorithms** built\ninto SageMaker compatible Docker containers, you can use SageMaker Spark to train and infer on `DataFrame`s with your\nown algorithms -- **all at Spark scale.**\n\n## Table of Contents\n* [Getting SageMaker Spark](#getting-sagemaker-spark)\n  * [Scala](#scala)\n* [Running SageMaker Spark](#running-sagemaker-spark)\n  * [Running SageMaker Spark Applications with spark-shell or <code>spark-submit</code>](#running-sagemaker-spark-applications-with-spark-shell-or-spark-submit)\n  * [Running SageMaker Spark Applications on EMR](#running-sagemaker-spark-applications-on-emr)\n  * [Python](#python)\n  * [S3 FileSystem Schemes](#s3-filesystem-schemes)\n  * [API Documentation](#api-documentation)\n* [Getting Started: K-Means Clustering on SageMaker with SageMaker Spark SDK](#getting-started-k-means-clustering-on-sagemaker-with-sagemaker-spark-sdk)\n* [Example: Using SageMaker Spark with Any SageMaker Algorithm](#example-using-sagemaker-spark-with-any-sagemaker-algorithm)\n* [Example: Using SageMakerEstimator and SageMakerModel in a Spark Pipeline](#example-using-sagemakerestimator-and-sagemakermodel-in-a-spark-pipeline)\n* [Example: Using Multiple SageMakerEstimators and SageMakerModels in a Spark Pipeline](#example-using-multiple-sagemakerestimators-and-sagemakermodels-in-a-spark-pipeline)\n* [Example: Creating a SageMakerModel](#example-creating-a-sagemakermodel)\n  * [SageMakerModel From an Endpoint](#sagemakermodel-from-an-endpoint)\n  * [SageMakerModel From Model Data in S3](#sagemakermodel-from-model-data-in-s3)\n  * [SageMakerModel From a Previously Completed Training Job](#sagemakermodel-from-a-previously-completed-training-job)\n* [Example: Tearing Down Amazon SageMaker Endpoints](#example-tearing-down-amazon-sagemaker-endpoints)\n* [Configuring an IAM Role](#configuring-an-iam-role)\n* [SageMaker Spark: In-Depth](#sagemaker-spark-in-depth)\n  * [The Amazon Record format](#the-amazon-record-format)\n  * [Serializing and Deserializing for Inference](#serializing-and-deserializing-for-inference)\n* [License](#license)\n\n## Getting SageMaker Spark\n\n### Scala\n\nSageMaker Spark for Scala is available in the Maven central repository:\n\n```\n<dependency>\n    <groupId>com.amazonaws</groupId>\n    <artifactId>sagemaker-spark_2.11</artifactId>\n    <version>spark_2.2.0-1.0</version>\n</dependency>\n```\n\nOr, if your project depends on Spark 2.1:\n\n```\n<dependency>\n    <groupId>com.amazonaws</groupId>\n    <artifactId>sagemaker-spark_2.11</artifactId>\n    <version>spark_2.1.1-1.0</version>\n</dependency>\n```\n\nYou can also build SageMaker Spark from source. See [sagemaker-spark-sdk](sagemaker-spark-sdk) for more on\nbuilding SageMaker Spark from source.\n\n### Python\n\nSee the [sagemaker-pyspark-sdk](sagemaker-pyspark-sdk) for more on installing and running SageMaker PySpark.\n\n## Running SageMaker Spark\n\nSageMaker Spark depends on hadoop-aws-2.8.1. To run Spark applications that depend on SageMaker Spark, you need to\nbuild Spark with Hadoop 2.8. However, if you are running Spark applications on EMR, you can use Spark built with Hadoop 2.7.\n\nApache Spark currently distributes binaries built against Hadoop-2.7, but not 2.8.\nSee the [Spark documentation](https://spark.apache.org/docs/2.2.0/hadoop-provided.html) for more on building Spark\nwith Hadoop 2.8.\n\nSageMaker Spark needs to be added to both the driver and executor classpaths.\n\n### Running SageMaker Spark Applications with `spark-shell` or `spark-submit`\n\nYou can submit SageMaker Spark and the AWS Java Client as dependencies with the \"--jars\" flag, or take a dependency\non SageMaker Spark in Maven using the \"--package\" flag.\n\n1. Install Hadoop-2.8. [https://hadoop.apache.org/docs/r2.8.0/](https://hadoop.apache.org/docs/r2.8.0/)\n2. Build Spark 2.2 with Hadoop-2.8. The [Spark documentation](https://spark.apache.org/docs/2.2.0/hadoop-provided.html)\nhas guidance on building Spark with your own Hadoop installation.\n3. Run ```spark-shell``` or ```spark-submit``` with the `--packages` flag:\n\n```\nspark-shell --packages com.amazonaws:sagemaker-spark_2.11:spark_2.2.0-1.0\n```\n\n### Running SageMaker Spark Applications on EMR\n\nYou can run SageMaker Spark applications on an EMR cluster just like any other Spark application by\nsubmitting your Spark application jar and the SageMaker Spark dependency jars with the --jars or --packages flags.\n\nSageMaker Spark is pre-installed on EMR releases since 5.11.0. You can run your SageMaker Spark application\non EMR by submitting your Spark application jar and any additional dependencies your Spark application uses.\n\nSageMaker Spark applications have also been verified to be compatible with EMR-5.6.0 (which runs Spark 2.1) and EMR-5-8.0\n(which runs Spark 2.2). When submitting your Spark application to an earlier EMR release, use the `--packages` flag to\ndepend on a recent version of the AWS Java SDK:  \n\n```\nspark-submit\n  --packages com.amazonaws:aws-java-sdk:1.11.613 \\\n  --deploy-mode cluster \\\n  --conf spark.driver.userClassPathFirst=true \\\n  --conf spark.executor.userClassPathFirst=true \\\n  --jars SageMakerSparkApplicationJar.jar,...\n  ...\n```\n\nThe `spark.driver.userClassPathFirst=true` and `spark.executor.userClassPathFirst=true` properties are required so that\nthe Spark cluster will use the AWS Java SDK dependencies with SageMaker, rather than the AWS Java SDK installed on these\nearlier EMR clusters.\n\nFor more on running Spark application on EMR, see the\n[EMR Documentation](http://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-submit-step.html) on submitting a step.\n\n### Python\n\nSee the [sagemaker-pyspark-sdk](sagemaker-pyspark-sdk) for more on installing and running SageMaker PySpark.\n\n### S3 FileSystem Schemes\n\nEMR allows you to read and write data using the EMR FileSystem (EMRFS), accessed through Spark with \"s3://\":\n\n```scala\nspark.read.format(\"libsvm\").load(\"s3://my-bucket/my-prefix\")\n```\n\nIn other execution environments, you can use the S3A schema to use the S3A FileSystem \"s3a://\" to read and write data:\n\n```scala\nspark.read.format(\"libsvm\").load(\"s3a://my-bucket/my-prefix\")\n```\n\nIn the code examples in this README, we use \"s3://\" to use the [EMRFS](http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-fs.html),\nor \"s3a://\" to use the [S3A system](https://wiki.apache.org/hadoop/AmazonS3), which is recommended over \"s3n://\".\n\n### API Documentation\n\nYou can view the [Scala API Documentation for SageMaker Spark here.](https://aws.github.io/sagemaker-spark/)\n\nYou can view the [PySpark API Documentation for SageMaker Spark here.](http://sagemaker-pyspark.readthedocs.io/en/latest/)\n\n## Getting Started: K-Means Clustering on SageMaker with SageMaker Spark SDK\n \nThis example walks through using SageMaker Spark to train on a Spark DataFrame using a SageMaker-provided algorithm,\nhost the resulting model on SageMaker Spark, and making predictions on a Spark DataFrame using that hosted model.\n\nWe'll cluster handwritten digits in the MNIST dataset, which we've made available in LibSVM format at \n`s3://sagemaker-sample-data-us-east-1/spark/mnist/train/mnist_train.libsvm`.\n\nYou can start a Spark shell with SageMaker Spark\n\n```\nspark-shell --packages com.amazonaws:sagemaker-spark_2.11:spark_2.1.1-1.0\n```\n\n1. Create your Spark Session and load your training and test data into DataFrames:\n```scala\nval spark = SparkSession.builder.getOrCreate\n\n// load mnist data as a dataframe from libsvm. replace this region with your own.\nval region = \"us-east-1\"\nval trainingData = spark.read.format(\"libsvm\")\n  .option(\"numFeatures\", \"784\")\n  .load(s\"s3://sagemaker-sample-data-$region/spark/mnist/train/\")\n\nval testData = spark.read.format(\"libsvm\")\n  .option(\"numFeatures\", \"784\")\n  .load(s\"s3://sagemaker-sample-data-$region/spark/mnist/test/\")\n```\n\nThe `DataFrame` consists of a column named \"label\" of Doubles, indicating the digit for each example,\nand a column named \"features\" of Vectors:\n\n```scala\ntrainingData.show\n\n+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  5.0|(784,[152,153,154...|\n|  0.0|(784,[127,128,129...|\n|  4.0|(784,[160,161,162...|\n|  1.0|(784,[158,159,160...|\n|  9.0|(784,[208,209,210...|\n|  2.0|(784,[155,156,157...|\n|  1.0|(784,[124,125,126...|\n|  3.0|(784,[151,152,153...|\n|  1.0|(784,[152,153,154...|\n|  4.0|(784,[134,135,161...|\n|  3.0|(784,[123,124,125...|\n|  5.0|(784,[216,217,218...|\n|  3.0|(784,[143,144,145...|\n|  6.0|(784,[72,73,74,99...|\n|  1.0|(784,[151,152,153...|\n|  7.0|(784,[211,212,213...|\n|  2.0|(784,[151,152,153...|\n|  8.0|(784,[159,160,161...|\n|  6.0|(784,[100,101,102...|\n|  9.0|(784,[209,210,211...|\n+-----+--------------------+\n```\n\n2. Construct a `KMeansSageMakerEstimator`, which extends `SageMakerEstimator`, which is a Spark `Estimator`.\nYou need to pass in an Amazon SageMaker-compatible\nIAM Role that Amazon SageMaker will use to make AWS service calls on your behalf (or configure SageMaker Spark\nto [get this from Spark Config](#configuring-iam-role-and-s3-buckets)). Consult the API Documentation for a\ncomplete list of parameters.\n\nIn this example, we are setting the \"k\" and \"feature_dim\" hyperparameters, corresponding to the number\nof clusters we want and to the number of dimensions in our training dataset, respectively.\n\n```scala\n\n// Replace this IAM Role ARN with your own.\nval roleArn = \"arn:aws:iam::account-id:role/rolename\"\n\nval estimator = new KMeansSageMakerEstimator(\n  sagemakerRole = IAMRole(roleArn),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1)\n  .setK(10).setFeatureDim(784)\n```\n\n3. To train and host your model, call `fit()` on your training `DataFrame`:\n\n```scala\nval model = estimator.fit(trainingData)\n```\n\nWhat happens in this call to `fit()`?\n\n1. SageMaker Spark serializes your `DataFrame` and uploads the\nserialized training data to S3. For the K-Means algorithm, SageMaker Spark converts the `DataFrame` to the [Amazon Record\nformat](#the-amazon-record-format).\nSageMaker Spark will create an S3 bucket for you that your IAM role can access if you do not provide an S3 Bucket in\nthe constructor.\n2. SageMaker Spark sends a `CreateTrainingJobRequest` to Amazon SageMaker to run a Training Job with one `p2.xlarge` on the data in S3, configured with the\nvalues you pass in to the `SageMakerEstimator`, and polls for completion of the Training Job.\nIn this example, we are sending a CreateTrainingJob request to run a k-means clustering Training Job on Amazon SageMaker\non serialized data we uploaded from your `DataFrame`. When training completes, the Amazon SageMaker service puts\na serialized model in an S3 bucket you own (or the default bucket created by SageMaker Spark).\n3. After training completes, SageMaker Spark sends a `CreateModelRequest`, a `CreateEndpointConfigRequest`, and a\n`CreateEndpointRequest` and polls for completion, each configured with the values you pass in to the SageMakerEstimator.\nThis Endpoint will initially be backed by one `c4.xlarge`.\n\n4. To make inferences using the Endpoint hosting our model, call `transform()` on the `SageMakerModel` returned by `fit()`.\n\n```scala\nval transformedData = model.transform(testData)\ntransformedData.show\n+-----+--------------------+-------------------+---------------+\n|label|            features|distance_to_cluster|closest_cluster|\n+-----+--------------------+-------------------+---------------+\n|  5.0|(784,[152,153,154...|  1767.897705078125|            4.0|\n|  0.0|(784,[127,128,129...|  1392.157470703125|            5.0|\n|  4.0|(784,[160,161,162...| 1671.5711669921875|            9.0|\n|  1.0|(784,[158,159,160...| 1182.6082763671875|            6.0|\n|  9.0|(784,[208,209,210...| 1390.4002685546875|            0.0|\n|  2.0|(784,[155,156,157...|  1713.988037109375|            1.0|\n|  1.0|(784,[124,125,126...| 1246.3016357421875|            2.0|\n|  3.0|(784,[151,152,153...|  1753.229248046875|            4.0|\n|  1.0|(784,[152,153,154...|  978.8394165039062|            2.0|\n|  4.0|(784,[134,135,161...|  1623.176513671875|            3.0|\n|  3.0|(784,[123,124,125...|  1533.863525390625|            4.0|\n|  5.0|(784,[216,217,218...|  1469.357177734375|            6.0|\n|  3.0|(784,[143,144,145...|  1736.765869140625|            4.0|\n|  6.0|(784,[72,73,74,99...|   1473.69384765625|            8.0|\n|  1.0|(784,[151,152,153...|    944.88720703125|            2.0|\n|  7.0|(784,[211,212,213...| 1285.9071044921875|            3.0|\n|  2.0|(784,[151,152,153...| 1635.0125732421875|            1.0|\n|  8.0|(784,[159,160,161...| 1436.3162841796875|            6.0|\n|  6.0|(784,[100,101,102...| 1499.7366943359375|            7.0|\n|  9.0|(784,[209,210,211...| 1364.6319580078125|            6.0|\n+-----+--------------------+-------------------+---------------+\n\n```\n\nIn this call to `transform()`, the `SageMakerModel` serializes chunks of the input `DataFrame` and sends them to the\nEndpoint using the SageMakerRuntime `InvokeEndpoint` API. The `SageMakerModel` deserializes the Endpoint's responses,\nwhich contain predictions, and appends the prediction columns to the input `DataFrame`.\n\n## Example: Using SageMaker Spark with Any SageMaker Algorithm\n\nThe `SageMakerEstimator` is an `org.apache.spark.ml.Estimator` that trains a model on Amazon SageMaker.\n\nSageMaker Spark provides several classes that extend `SageMakerEstimator` to run particular algorithms, like `KMeansSageMakerEstimator`\nto run the SageMaker-provided k-means algorithm, or `XGBoostSageMakerEstimator` to run the SageMaker-provided XGBoost\nalgorithm. These classes are just `SageMakerEstimator`s with certain default values passed in. You can use SageMaker Spark with\nany algorithm that runs on Amazon SageMaker by creating a SageMakerEstimator.\n\nInstead of creating a KMeansSageMakerEstimator, you can create an equivalent SageMakerEstimator:\n\n```scala\nval estimator = new SageMakerEstimator(\n  trainingImage =\n    \"382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1\",\n  modelImage =\n    \"382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1\",\n  requestRowSerializer = new ProtobufRequestRowSerializer(),\n  responseRowDeserializer = new KMeansProtobufResponseRowDeserializer(),\n  hyperParameters = Map(\"k\" -> \"10\", \"feature_dim\" -> \"784\"),\n  sagemakerRole = IAMRole(roleArn),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1,\n  trainingSparkDataFormat = \"sagemaker\")\n```\n\n* `trainingImage` identifies the Docker registry path to the training image containing your custom code. In this case,\nthis points to the us-east-1 k-means image.\n* `modelImage` identifies the Docker registry path to the image containing inference code. Amazon SageMaker k-means \nuses the same image to train and to host trained models.\n* `requestRowSerializer` implements `com.amazonaws.services.sagemaker.sparksdk.transformation.RequestRowSerializer`.\nA `RequestRowSerializer` serializes `org.apache.spark.sql.Row`s in the input `DataFrame` to send them to the model hosted in Amazon SageMaker for inference.\nThis is passed to the SageMakerModel returned by `fit`. In this case, we pass in a `RequestRowSerializer` that serializes\n`Row`s to the Amazon Record protobuf format. See [Serializing and Deserializing for Inference](#serializing-and-deserializing-for-inference)\nfor more information on how SageMaker Spark makes inferences. \n* `responseRowDeserializer` Implements\n`com.amazonaws.services.sagemaker.sparksdk.transformation.ResponseRowDeserializer`. A `ResponseRowDeserializer` deserializes\nresponses containing predictions from the Endpoint back into columns in a `DataFrame`.\n* `hyperParameters` is a `Map[String, String]` that the `trainingImage` will use to set training hyperparameters.\n* `trainingSparkDataFormat` specifies the data format that Spark uses when uploading training data from a `DataFrame`\nto S3.\n\nSageMaker Spark needs the trainingSparkDataFormat to tell Spark how to write the DataFrame to S3 for the `trainingImage` to\ntrain on. In this example, \"sagemaker\" tells Spark to write the data as\nRecordIO-encoded [Amazon Records](#the-amazon-record-format), but your own algorithm may take another data format.\nYou can pass in any format that Spark supports as long as your `trainingImage` can train using that data format,\nsuch as \"csv\", \"parquet\", \"com.databricks.spark.csv\", or \"libsvm.\"\n\nSageMaker Spark also needs a `RequestRowSerializer` to serialize Spark `Row`s to a\ndata format the `modelImage` can deserialize, and a `ResponseRowDeserializer` to deserialize responses that contain\npredictions from the `modelImage` back into Spark `Row`s. See [Serializing and Deserializing for Inference](#serializing-and-deserializing-for-inference)\nfor more details.\n\n## Example: Using SageMakerEstimator and SageMakerModel in a Spark Pipeline\n\n`SageMakerEstimator`s and `SageMakerModel`s can be used in `Pipeline`s. In this\nexample, we run `org.apache.spark.ml.feature.PCA` on our Spark cluster, then train and infer using Amazon SageMaker's\nK-Means on the output column from `PCA`:\n\n```scala\nval pcaEstimator = new PCA()\n  .setInputCol(\"features\")\n  .setOutputCol(\"projectedFeatures\")\n  .setK(50)\n\nval kMeansSageMakerEstimator = new KMeansSageMakerEstimator(\n  sagemakerRole = IAMRole(roleArn),\n  requestRowSerializer =\n    new ProtobufRequestRowSerializer(featuresColumnName = \"projectedFeatures\"),\n  trainingSparkDataFormatOptions = Map(\"featuresColumnName\" -> \"projectedFeatures\"),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1)\n  .setK(10).setFeatureDim(50)\n\nval pipeline = new Pipeline().setStages(Array(pcaEstimator, kMeansSageMakerEstimator))\n\n// train\nval pipelineModel = pipeline.fit(trainingData)\n\nval transformedData = pipelineModel.transform(testData)\ntransformedData.show()\n\n+-----+--------------------+--------------------+-------------------+---------------+\n|label|            features|   projectedFeatures|distance_to_cluster|closest_cluster|\n+-----+--------------------+--------------------+-------------------+---------------+\n|  5.0|(784,[152,153,154...|[880.731433034386...|     1500.470703125|            0.0|\n|  0.0|(784,[127,128,129...|[1768.51722024166...|      1142.18359375|            4.0|\n|  4.0|(784,[160,161,162...|[704.949236329314...|  1386.246826171875|            9.0|\n|  1.0|(784,[158,159,160...|[-42.328192193771...| 1277.0736083984375|            5.0|\n|  9.0|(784,[208,209,210...|[374.043902028333...|   1211.00927734375|            3.0|\n|  2.0|(784,[155,156,157...|[941.267714528850...|  1496.157958984375|            8.0|\n|  1.0|(784,[124,125,126...|[30.2848596410594...| 1327.6766357421875|            5.0|\n|  3.0|(784,[151,152,153...|[1270.14374062052...| 1570.7674560546875|            0.0|\n|  1.0|(784,[152,153,154...|[-112.10792566485...|     1037.568359375|            5.0|\n|  4.0|(784,[134,135,161...|[452.068280676606...| 1165.1236572265625|            3.0|\n|  3.0|(784,[123,124,125...|[610.596447285397...|  1325.953369140625|            7.0|\n|  5.0|(784,[216,217,218...|[142.959601818422...| 1353.4930419921875|            5.0|\n|  3.0|(784,[143,144,145...|[1036.71862533658...| 1460.4315185546875|            7.0|\n|  6.0|(784,[72,73,74,99...|[996.740157435754...| 1159.8631591796875|            2.0|\n|  1.0|(784,[151,152,153...|[-107.26076167417...|   960.963623046875|            5.0|\n|  7.0|(784,[211,212,213...|[619.771820430940...|   1245.13623046875|            6.0|\n|  2.0|(784,[151,152,153...|[850.152101817161...|  1304.437744140625|            8.0|\n|  8.0|(784,[159,160,161...|[370.041887230547...| 1192.4781494140625|            0.0|\n|  6.0|(784,[100,101,102...|[546.674328209335...|    1277.0908203125|            2.0|\n|  9.0|(784,[209,210,211...|[-29.259112927426...| 1245.8182373046875|            6.0|\n+-----+--------------------+--------------------+-------------------+---------------+\n```\n\n* `requestRowSerializer =\n      new ProtobufRequestRowSerializer(featuresColumnName = \"projectedFeatures\")` tells the `SageMakerModel` returned\n      by `fit()` to infer on the features in the \"projectedFeatures\" column\n* `trainingSparkDataFormatOptions = Map(\"featuresColumnName\" -> \"projectedFeatures\")` tells the `SageMakerProtobufWriter`\n that Spark is using to write the `DataFrame` as format \"sagemaker\" to serialize the \"projectedFeatures\" column when\n writing Amazon Records for training.\n\n\n## Example: Using Multiple SageMakerEstimators and SageMakerModels in a Spark Pipeline\n\nWe can use multiple `SageMakerEstimator`s and `SageMakerModel`s in a pipeline. Here, we use\nSageMaker's PCA algorithm to reduce a dataset with 50 dimensions to a dataset with 20 dimensions, then\nuse SageMaker's K-Means algorithm to train on the 20-dimension data.\n\n```scala\nval pcaEstimator = new PCASageMakerEstimator(sagemakerRole = IAMRole(sagemakerRole),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1\n  responseRowDeserializer = new PCAProtobufResponseRowDeserializer(\n    projectionColumnName = \"projectionDim20\"),\n  trainingInputS3DataPath = S3DataPath(trainingBucket, inputPrefix),\n  trainingOutputS3DataPath = S3DataPath(trainingBucket, outputPrefix),\n  endpointCreationPolicy = EndpointCreationPolicy.CREATE_ON_TRANSFORM)\n  .setNumComponents(20).setFeatureDim(50)\n\nval kmeansEstimator = new KMeansSageMakerEstimator(sagemakerRole = IAMRole(sagemakerRole),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1\n  trainingSparkDataFormatOptions = Map(\"featuresColumnName\" -> \"projectionDim20\"),\n  requestRowSerializer = new ProtobufRequestRowSerializer(\n    featuresColumnName = \"projectionDim20\"),\n  responseRowDeserializer = new KMeansProtobufResponseRowDeserializer(),\n  trainingInputS3DataPath = S3DataPath(trainingBucket, inputPrefix),\n  trainingOutputS3DataPath = S3DataPath(trainingBucket, outputPrefix),\n  endpointCreationPolicy = EndpointCreationPolicy.CREATE_ON_TRANSFORM)\n  .setK(10).setFeatureDim(20)\n\nval pipeline = new Pipeline().setStages(Array(pcaEstimator, kmeansEstimator))\n\nval model = pipeline.fit(dataset)\n\n// For expediency, transforming the training dataset:\nval transformedData = model.transform(dataset)\ntransformedData.show()\n\n+-----+--------------------+--------------------+-------------------+---------------+\n|label|            features|     projectionDim20|distance_to_cluster|closest_cluster|\n+-----+--------------------+--------------------+-------------------+---------------+\n|  1.0|[-0.7927307,-11.2...|[5.50362682342529...|  45.03189468383789|            1.0|\n|  1.0|[-3.762671,-5.853...|[-2.1558122634887...|  41.79889678955078|            1.0|\n|  1.0|[-2.0988898,-2.40...|[4.53881502151489...| 50.824703216552734|            1.0|\n|  1.0|[-2.81075,-3.6481...|[0.97894239425659...|  52.78211975097656|            1.0|\n|  1.0|[-2.14356,-4.0369...|[2.25758934020996...|  48.99141311645508|            1.0|\n|  1.0|[-5.3773708,-15.3...|[-3.2523036003112...|  21.99374771118164|            1.0|\n|  1.0|[-1.0369565,-16.5...|[-17.643878936767...| 29.127044677734375|            3.0|\n|  1.0|[-2.019725,-3.226...|[1.41068196296691...|   51.7830696105957|            1.0|\n|  1.0|[-4.3821997,-0.98...|[-0.8335087299346...| 53.921058654785156|            1.0|\n|  1.0|[-7.075208,-34.31...|[11.4329795837402...|  35.12031173706055|            3.0|\n|  1.0|[-3.90454,-4.8401...|[-1.4304646253585...|  50.00594711303711|            1.0|\n|  1.0|[0.9607103,-13.50...|[1.13785743713378...|  28.71956443786621|            1.0|\n|  1.0|[-4.5025017,-15.2...|[2.66747045516967...| 25.419822692871094|            1.0|\n|  1.0|[0.041773,-27.148...|[7.58121681213378...| 30.303693771362305|            3.0|\n|  1.0|[-10.1477266,-39....|[-12.086886405944...|   35.9030647277832|            2.0|\n|  1.0|[-3.09143,-6.4892...|[1.79180252552032...|  39.34271240234375|            1.0|\n|  1.0|[-13.5285917,-32....|[7.62783145904541...| 35.040035247802734|            2.0|\n|  1.0|[-4.189806,-16.04...|[1.41141772270202...| 25.123626708984375|            1.0|\n|  1.0|[-12.77831508,-62...|[0.11281073093414...|  63.91242599487305|            2.0|\n|  1.0|[-9.3934507,-12.5...|[-9.4945802688598...| 20.913305282592773|            1.0|\n+-----+--------------------+--------------------+-------------------+---------------+\n\n```\n* `responseRowDeserializer = new PCAProtobufResponseRowDeserializer(\nprojectionColumnName = \"projectionDim20\")` tells the `SageMakerModel` attached to the PCA endpoint to deserialize\nresponses (which contain the lower-dimensional projections of the features vectors) into the column named \"projectionDim20\"\n* `endpointCreationPolicy = EndpointCreationPolicy.CREATE_ON_TRANSFORM` tells the `SageMakerEstimator` to delay SageMaker\n Endpoint creation until it is needed to transform a `DataFrame`. \n* `trainingSparkDataFormatOptions = Map(\"featuresColumnName\" -> \"projectionDim20\"),\n   requestRowSerializer = new ProtobufRequestRowSerializer(\n       featuresColumnName = \"projectionDim20\")` these lines tell the `KMeansSageMakerEstimator`\n       to respectively train and infer on the features in the \"projectionDim20\" column.\n\n## Example: Creating a SageMakerModel\n\nSageMaker Spark supports attaching `SageMakerModel`s to an existing SageMaker endpoint, or to an Endpoint created by\nreference to model data in S3, or to a previously completed Training Job.\n\nThis allows you to use SageMaker Spark just for model hosting and inference on Spark-scale `DataFrame`s without running\na new Training Job.\n\n### SageMakerModel From an Endpoint\n\nYou can attach a `SageMakerModel` to an endpoint that has already been created. Supposing an endpoint with name\n\"my-endpoint-name\" is already in service and hosting a SageMaker K-Means model:\n\n```scala\nval model = SageMakerModel\n  .fromEndpoint(endpointName = \"my-endpoint-name\",\n                requestRowSerializer = new ProtobufRequestRowSerializer(\n                  featuresColumnName = \"MyFeaturesColumn\"),\n                responseRowDeserializer = new KMeansProtobufResponseRowDeserializer(\n                  distanceToClusterColumnName = \"DistanceToCluster\",\n                  closestClusterColumnName = \"ClusterLabel\"\n                ))\n```\n\nThis `SageMakerModel` will, upon a call to `transform()`, serialize the column named\n\"MyFeaturesColumn\" for inference, and append the columns \"DistanceToCluster\" and \"ClusterLabel\" to the `DataFrame`.\n\n### SageMakerModel From Model Data in S3\n\nYou can create a SageMakerModel and an Endpoint by referring directly to your model data in S3:\n\n```scala\nval model = SageMakerModel\n  .fromModelS3Path(modelPath = \"s3://my-model-bucket/my-model-data/model.tar.gz\",\n                   modelExecutionRoleARN = \"arn:aws:iam::account-id:role/rolename\"\n                   modelImage = 382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1\",\n                   endpointInstanceType = \"ml.c4.xlarge\",\n                   endpointInitialInstanceCount = 1\n                   requestRowSerializer = new ProtobufRequestRowSerializer(),\n                   responseRowDeserializer = new KMeansProtobufResponseRowDeserializer()\n                  )\n```\n\n### SageMakerModel From a Previously Completed Training Job\n\nYou can create a SageMakerModel and an Endpoint by referring to a previously-completed training job:\n\n```scala\nval model = SageMakerModel\n  .fromTrainingJob(trainingJobName = \"my-training-job-name\",\n                   modelExecutionRoleARN = \"arn:aws:iam::account-id:role/rolename\"\n                   modelImage = 382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1\",\n                   endpointInstanceType = \"ml.c4.xlarge\",\n                   endpointInitialInstanceCount = 1\n                   requestRowSerializer = new ProtobufRequestRowSerializer(),\n                   responseRowDeserializer = new KMeansProtobufResponseRowDeserializer()\n                  )\n\n```\n\n## Example: Tearing Down Amazon SageMaker Endpoints\n\nSageMaker Spark provides a utility for deleting Endpoints created by a SageMakerModel:\n\n```scala\nval sagemakerClient = AmazonSageMakerClientBuilder.defaultClient\nval cleanup = new SageMakerResourceCleanup(sagemakerClient)\ncleanup.deleteResources(model.getCreatedResources)\n\n```\n\n## Configuring an IAM Role\n\nSageMaker Spark allows you to add your IAM Role ARN to your Spark Config so that you don't have to keep passing in\n`IAMRole(\"arn:aws:iam::account-id:role/rolename\")`.\n\nAdd an entry to your Spark Config with key `com.amazonaws.services.sagemaker.sparksdk.sagemakerrole` whose value is your\nAmazon SageMaker-compatible IAM Role. `SageMakerEstimator` will look for this role if it is not supplied in the constructor.\n\n## SageMaker Spark: In-Depth\n\n### The Amazon Record format\n\n`KMeansSageMakerEstimator`, `PCASageMakerEstimator`, and `LinearLearnerSageMakerEstimator` all serialize `DataFrame`s\nto the Amazon Record protobuf format with each Record encoded in\n[RecordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html).\nThey do this by passing in \"sagemaker\" to the `trainingSparkDataFormat` constructor argument, which configures Spark\nto use the `SageMakerProtobufWriter` to serialize Spark `DataFrame`s.\n\nWriting a `DataFrame` using the \"sagemaker\"\nformat serializes a column named \"label\", expected to contain\n`Double`s, and a column named \"features\", expected to contain a Sparse or Dense `org.apache.mllib.linalg.Vector`.\nIf the features column contains a `SparseVector`, SageMaker Spark sparsely-encodes the `Vector` into the Amazon Record.\nIf the features column contains a `DenseVector`, SageMaker Spark densely-encodes the `Vector` into the Amazon Record.\n\nYou can choose which columns the `SageMakerEstimator` chooses as its \"label\" and \"features\" columns by passing in \na `trainingSparkDataFormatOptions` `Map[String, String]` with keys \"labelColumnName\" and \"featuresColumnName\" and with\nvalues corresponding to the names of your chosen label and features columns.\n\nYou can also write Amazon Records using SageMaker Spark by using the \"sagemaker\" format directly:\n\n```scala\nmyDataFrame.write\n    .format(\"sagemaker\")\n    .option(\"labelColumnName\", \"myLabelColumn\")\n    .option(\"featuresColumnName\", \"myFeaturesColumn\")\n    .save(\"s3://my-s3-bucket/my-s3-prefix\")\n```\n\nBy default, `SageMakerEstimator` deletes the RecordIO-encoded Amazon Records in S3 following training on Amazon \nSageMaker. You can choose to allow the data to persist in S3 by passing in `deleteStagingDataAfterTraining = true` to \n`SageMakerEstimator`.\n\nSee the [AWS Documentation on Amazon Records](https://aws.amazon.com/sagemaker/latest/dg/cdf-training.html) for\nmore information on Amazon Records.\n\n### Serializing and Deserializing for Inference\n\n`SageMakerEstimator.fit()` returns a `SageMakerModel`, which transforms a `DataFrame` by calling `InvokeEndpoint` on\nan Amazon SageMaker Endpoint. `InvokeEndpointRequest`s carry serialized `Row`s as their payload.`Row`s in the `DataFrame`\nare serialized for predictions against an Endpoint using a `RequestRowSerializer`. Responses from an Endpoint containing\npredictions are deserialized into Spark `Row`s and appended as columns in a `DataFrame` using a `ResponseRowDeserializer.`\n\nInternally, `SageMakerModel.transform` calls `mapPartitions` to distribute the work\nof serializing Spark `Row`s, constructing and sending `InvokeEndpointRequest`s to an Endpoint, and deserializing\n`InvokeEndpointResponse`s across a Spark cluster. Because each `InvokeEndpointRequest` can carry only 5MB, each \nSpark partition creates a\n`com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator` to iterate over its partition,\nsending prediction requests to the Endpoint in 5MB increments.\n\n`RequestRowSerializer.serializeRow()` converts a `Row` to an `Array[Byte]`.\nThe `RequestBatchIterator` appends these byte arrays to\nform the request body of an `InvokeEndpointRequest`.\n\nFor example, the\n`com.amazonaws.services.sagemaker.sparksdk.transformation.ProtobufRequestRowSerializer` creates one\nRecordIO-encoded Amazon Record per input row by serializing the \"features\" column in each row, and wrapping each\nAmazon Record in the RecordIO header.\n\n`ResponseRowDeserializer.deserializeResponse()` converts an `Array[Byte]` containing predictions from an Endpoint to \nan `Iterator[Row]`to appends columns containing these predictions to the `DataFrame` being transformed by the\n`SageMakerModel`.\n\nFor comparison, SageMaker's XGBoost uses LibSVM-formatted data for inference (as well as training), and responds with a comma-delimited list of predictions.\nAccordingly, SageMaker Spark uses `com.amazonaws.services.sagemaker.sparksdk.transformation.LibSVMRequestRowSerializer`\nto serialize rows into LibSVM-formatted data, and uses `com.amazonaws.services.sagemaker.sparksdk.transformation.XGBoostCSVResponseRowDeserializer`\nto deserialize the response into a column of predictions.\n\nTo support your own model image's data formats for inference, you can implement your own `RequestRowSerializer` and `ResponseRowDeserializer`.\n\n## License\n\nSageMaker Spark is licensed under [Apache-2.0](https://github.com/aws/sagemaker-spark/LICENSE.txt).\n", "release_dates": ["2024-02-22T02:36:14Z", "2022-08-26T16:19:59Z", "2022-08-26T07:55:05Z", "2022-08-23T17:52:39Z", "2021-04-23T18:00:53Z", "2020-09-14T16:26:22Z", "2020-08-10T16:25:48Z", "2020-07-23T16:25:59Z", "2020-07-21T16:27:38Z", "2020-06-18T16:28:23Z", "2020-05-25T16:27:48Z", "2020-04-21T19:21:04Z", "2020-03-31T16:27:33Z", "2020-03-26T16:54:13Z", "2020-03-25T20:26:26Z", "2020-02-04T16:25:49Z", "2019-12-12T16:25:47Z", "2019-10-22T00:16:52Z", "2019-08-22T20:16:07Z", "2019-05-06T16:26:26Z", "2019-04-30T16:26:11Z", "2019-04-24T16:26:21Z", "2019-04-12T23:26:34Z", "2018-11-06T04:36:51Z", "2018-09-27T22:22:45Z", "2018-08-23T23:06:35Z", "2018-07-26T00:25:18Z", "2018-07-12T18:36:56Z", "2018-06-21T22:01:53Z", "2018-05-24T21:04:42Z"]}, {"name": "sagemaker-spark-container", "description": "The SageMaker Spark Container is a Docker image used to run data processing workloads with the Spark framework on Amazon SageMaker.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# SageMaker Spark Container\n\n## Spark Overview\nApache Spark\u2122 is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.\n\n## SageMaker Spark Container\nThe SageMaker Spark Container is a Docker image used to run batch data processing workloads on Amazon SageMaker using the Apache Spark framework. The \ncontainer images in this repository are used to build the pre-built container images that are used when running Spark jobs on Amazon SageMaker using the SageMaker Python SDK. The pre-built images are available in the Amazon Elastic Container Registry (Amazon ECR), and this repository serves as a reference for those wishing to build their own customized Spark containers for use in Amazon SageMaker.\n\nFor the list of available Spark images, see [Available SageMaker Spark Images](available_images.md).\n\n## License\nThis project is licensed under the Apache-2.0 License.\n\n\n## Usage in the SageMaker Python SDK\n\nThe simplest way to get started with the SageMaker Spark Container is to use the pre-built images via the SageMaker Python SDK.\n\n[Amazon SageMaker Processing \u2014 sagemaker 2.5.3 documentation](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#amazon-sagemaker-processing)\n\n## Getting Started With Development\n\nTo get started building and testing the SageMaker Spark container, you will have to setup a local development environment.\n\nSee instructions in [DEVELOPMENT.md](./DEVELOPMENT.md)\n\n## Contributing\nTo contribute to this project, please read through [CONTRIBUTING.md](./CONTRIBUTING.md)\n\n", "release_dates": ["2023-10-09T20:51:37Z", "2023-07-27T19:13:32Z", "2023-06-14T20:27:45Z", "2023-04-17T18:25:14Z", "2022-09-21T23:44:36Z", "2022-05-26T19:38:36Z", "2021-12-12T23:59:43Z", "2022-04-27T18:12:02Z", "2022-04-26T21:58:46Z", "2021-12-12T23:49:29Z", "2021-12-13T00:12:05Z", "2021-10-05T22:23:57Z", "2021-03-03T16:34:03Z", "2021-03-03T16:33:03Z", "2020-12-08T17:51:26Z", "2020-12-08T17:48:06Z", "2020-12-01T16:40:40Z", "2020-12-01T16:37:26Z", "2020-09-14T18:58:39Z"]}, {"name": "sagemaker-sparkml-serving-container", "description": "This code is used to build & run a Docker container for performing predictions against a  Spark ML Pipeline.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "Sagemaker SparkML Serving Container\n===================================\n\nSageMaker SparkML Serving Container lets you deploy an Apache Spark ML Pipeline in [Amazon SageMaker](https://aws.amazon.com/sagemaker/) for real-time, batch prediction and inference pipeline use-cases. The container can be used to deploy a Spark ML Pipeline outside of SageMaker as well. It is powered by open-source [MLeap](https://github.com/combust/mleap) library.\n\nBuild Status\n============\n\n| Image Build Status                         | PR Build Status                         |\n|--------------------------------------------|-----------------------------------------|\n| ![CodeBuild](https://tinyurl.com/y8wdoq3h) | ![Travis](https://tinyurl.com/y79gh9of) |\n\nOverview\n========\n\n[Apache Spark](http://spark.apache.org/) is a unified analytics engine for large scale data processing. Apache Spark comes with a Machine Learning library called [MLlib](http://spark.apache.org/mllib/) which lets you build ML pipelines using most of the standard feature transformers & algorithms. Apache Spark is well suited for batch processing use-cases and is not the preferred solution for low latency online inference scenarios. In order to perform low latency online prediction, SageMaker SparkML Serving Container leverages an open source library called MLeap.\n\nMLeap is focussed towards deploying Apache Spark based ML pipelines to production for low latency online inference use-cases. It provides a serialization format for exporting a Spark ML pipeline and a runtime engine to execute prediction against the serialized pipeline. SageMaker SparkML Serving provides a RESTful web service using [Spring Boot](https://spring.io/projects/spring-boot) which internally calls MLeap runtime engine for execution.\n\nSageMaker SparkML Serving Container is primarily built on the underlying Spring Boot based web service and it provides a layer to build a SageMaker compatible Docker image. In addition to using it in SageMaker, you can build the Dockerfile or download SageMaker provided Docker images to perform inference against an MLeap serialized Spark ML Pipeline locally or outside of SageMaker.  \n\nSupported Spark/MLeap version\n=============================\n\nCurrently SageMaker SparkML Serving is powered by MLeap 0.20.0 and it is tested with Spark major version - 3.3.\n\nTable of Contents\n=================\n\n* How to use\n* Using the Docker image for performing inference with SageMaker\n* Using the Docker image for performing inference locally\n\nHow to use\n==========\n\nSageMaker SparkML Serving Container takes a code-free approach for performing inference. You need to pass a schema specifying the structure of input columns and output column. The web server will return you the contents of the output column in a specific format depending on `content-type` and `Accept`.\n\nProcedure to pass the schema\n----------------------------\n\nThere are two ways to pass the input schema to the serving container. You can either pass it as an environment variable or pass the schema with every request. In case there is a schema passed via the environment variable as well as it is passed via request, the one in the request will be considered. This functionality is provided to enable you the capability to override the default schema passed through an environment variable for certain specific requests.\n\nIn order to pass the schema via an environment variable, it should be passed with the key : `SAGEMAKER_SPARKML_SCHEMA`.\n\n### Format of the schema\n\nThe schema should be passed in the following format:\n\n```\n{\n  \"input\": [\n    {\n      \"name\": \"name_1\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name_2\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"name_3\",\n      \"type\": \"double\"\n    }\n  ],\n  \"output\": {\n    \"name\": \"prediction\",\n    \"type\": \"double\"\n  }\n}\n```\n\nThe `input` field takes a list of mappings and `output` field is a single mapping. Each mapping in the `input` field corresponds to one column in the `Dataframe` that was serialized with MLeap as part of the Spark job. `output` is required for you to specify the output column that you want in response after the `Dataframe` is transformed. If you have built an ML pipeline with a training algorithm at the end (e.g. Random Forest), most likely you'd be interested in the column `prediction`. The column name passed here (via the key `name`) should be exactly same as the name of the columns in the `Dataframe`. You can query for any field that was present in the `Dataframe` which was serialized with MLeap via the `output` field.\n\nSupported data types and data structures\n----------------------------------------\nSageMaker SparkML Serving Container supports most of the primitive data types to be the `type` field in `input` and `output`. `type` can be: `boolean`, `byte`, `short`, `int`, `long`, `double`, `float` and `string`. \n\nEach column can have data structures of three types: a single value (`basic`), a Spark `DenseVector` (`vector`) and a Spark `Array` (`array`). This means each column can be a single `int` (or any of the aforementioned data types) or an `Array` of `int` or a `DenseVector` of `int`.\nIf a column is of type `basic`, then you do not need to pass any additional information. Otherwise, if one or more columns in `input` or `output` is of the type `vector` or `array`, then you need to pass the information with a new key `struct` like this:\n\n```\n{\n  \"input\": [\n    {\n      \"name\": \"name_1\",\n      \"type\": \"int\",\n      \"struct\": \"vector\"\n    },\n    {\n      \"name\": \"name_2\",\n      \"type\": \"string\",\n      \"type\": \"basic\"  # This line is optional\n    },\n    {\n      \"name\": \"name_3\",\n      \"type\": \"double\",\n      \"struct\": \"array\"\n    }\n  ],\n  \"output\": {\n    \"name\": \"features\",\n    \"type\": \"double\",\n    \"struct\": \"vector\"\n  }\n}\n```\n\n\nRequest Structure\n-----------------\nSageMaker SparkML Serving Container can parse requests in both `text/csv` and `application/json` format. In case the schema is passed via an environment variable, the request should just contain the payload unless you want to override the schema for a specific request.\n\n### CSV\nFor CSV, the request should be passed with `content-type` as `text/csv` and schema should be passed via environment variable. In case of CSV input, each input column is treated as `basic` type because you can not have nested data structures in CSV. If your input payload contains one or more columns with `struct` as `vector` or `array`, you have to pass the request payload using JSON.\n\nSample CSV request:\n\n```\nfeature_1,feature_2,feature_3\n```\n\nString values do not need to be passed with quotes around it. There should not be any space around the comma and the order of the field should match one-to-one with the `input` field of the schema.\n\n### JSON\nFor JSON, the request should be passed with `content-type` as `application/json`. The schema can be passed either via an environment variable or as part of the payload.\n\n#### Schema is passed via environment variable\nIf schema is passed via an environment variable, then the input should be formatted like this:\n\n```\n\n# If individual columns are basic type\n\"data\": [feature_1, \"feature_2\", feature_3]\n\n# If one or more individual columns is vector or array\n\"data\": [[feature_11, feature_12], \"feature_2\", [feature_31, feature_32]]\n```\n\nAs with standard JSON, string input values has to be encoded with quotes.\n\n#### Schema is passed as part of the request\n\nFor JSON input, the schema can be passed as part of the input payload as well. All the other rules apply for this as well i.e. if a column is `basic`, then you do not need to pass the `struct` field in the mapping for that column. For this, a sample request would look like the following:\n\n```\n{\n  \"schema\": {\n    \"input\": [\n      {\n        \"name\": \"name_1\",\n        \"type\": \"int\",\n        \"struct\": \"vector\"\n      },\n      {\n        \"name\": \"name_2\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"name_3\",\n        \"type\": \"double\",\n        \"struct\": \"array\"\n      }\n    ],\n    \"output\": {\n      \"name\": \"features\",\n      \"type\": \"double\",\n      \"struct\": \"vector\"\n    }\n  },\n  \"data\": [[feature_11, feature_12, feature_13], \"feature_2\", [feature_31, feature_32]]\n}\n```\n\nOutput structure\n----------------\nSageMaker SparkML Serving Container can return output in three formats: CSV (`Accept` should be `text/csv`), JSON (`Accept` should be `application/jsonlines`) and JSON for text data (`Accept` should be `application/jsonlines;data=text`). \nDefault output format is CSV (in case there is no `Accept` parameter passed in the HTTP request).\n\n### Sample output\n\n#### CSV\n\n```\nout_1,out_2,out_3\n```\n\n#### JSON\n\n```\n{\"features\": [out_1, out_2, \"out_3\"]}\n```\n\n#### JSON for text data\nThis format is expected to be used for output which is text (e.g. `Tokenizer`). The `struct` in output in this case will most likely an `array` or `vector` and it is concatenated with space instead of comma.\n\n```\n{\"source\": \"sagemaker sparkml serving\"}\n\nor\n\n{\"source\": \"feature_1 feature_2 feature_3\"}\n```\n\nThis container is expected to be used in conjunction with other [SageMaker built-in algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html) for inference pipeline and the output formats resemble the structure those algorithms can work seamlessly with.\n\nExample Notebooks\n-----------------\n\nYou can find examples of how to use this in an end-to-end fashion here: [1](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/sparkml_serving_emr_mleap_abalone), [2](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/inference_pipeline_sparkml_xgboost_abalone) and [3](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/inference_pipeline_sparkml_blazingtext_dbpedia).\n\nUsing the Docker image for performing inference with SageMaker\n==============================================================\n\nSageMaker SparkML Serving Container is built to work seamlessly with SageMaker for real time inference, batch transformation and inference pipeline use-cases. \n\nWith AWS SDK\n------------\nIf you are using AWS Java SDK or Boto to call SageMaker APIs, then you can pass the SageMaker provided Docker images for this container in all region as part of the [`CreateModel`](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateModel.html) API call in the `PrimaryContainer` or `Containers` field. The schema should be passed using the `Environment` field of the API. As the schema has quotes, it should be encoded properly so that the JSON parser in the server can parse it during inference. For example, if you are using Boto, you can use Python's `json` library to do a `json.dumps` on the `dict` that holds the schema before passing it via Boto.\n\nCalling `CreateModel` is required for creating a `Model` in SageMaker with this Docker container and the serialized pipeline artifacts which is the stepping stone for all the use cases mentioned above.\n\nSageMaker works with Docker images stored in [Amazon ECR](https://aws.amazon.com/ecr/). SageMaker team has prepared and uploaded the Docker images for SageMaker SparkML Serving Container in all regions where SageMaker operates. \nRegion to ECR container URL mapping can be found below. For a mapping from Region to Region Name, please see [here](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html).\n\n* us-west-1 = 746614075791.dkr.ecr.us-west-1.amazonaws.com/sagemaker-sparkml-serving:3.3\n* us-west-2 = 246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-sparkml-serving:3.3\n* us-east-1 = 683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sparkml-serving:3.3\n* us-east-2 = 257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-sparkml-serving:3.3\n* ap-northeast-1 = 354813040037.dkr.ecr.ap-northeast-1.amazonaws.com/sagemaker-sparkml-serving:3.3\n* ap-northeast-2 = 366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-sparkml-serving:3.3\n* ap-southeast-1 = 121021644041.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-sparkml-serving:3.3\n* ap-southeast-2 = 783357654285.dkr.ecr.ap-southeast-2.amazonaws.com/sagemaker-sparkml-serving:3.3\n* ap-south-1 = 720646828776.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-sparkml-serving:3.3\n* eu-west-1 = 141502667606.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-sparkml-serving:3.3\n* eu-west-2 = 764974769150.dkr.ecr.eu-west-2.amazonaws.com/sagemaker-sparkml-serving:3.3\n* eu-central-1 = 492215442770.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-sparkml-serving:3.3\n* ca-central-1 = 341280168497.dkr.ecr.ca-central-1.amazonaws.com/sagemaker-sparkml-serving:3.3\n* us-gov-west-1 = 414596584902.dkr.ecr.us-gov-west-1.amazonaws.com/sagemaker-sparkml-serving:3.3\n\nWith [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk)\n------------------------------------------------------------------------\n\nIf you are using SageMaker Python SDK, you can create an instance of `SparkMLModel` class with only the serialized pipeline artifacts and call `deploy()` method on it to create an inference endpoint or use the model created as part of the method for a batch transformation job.\n\nUsing it in an inference pipeline\n---------------------------------\n\nFor using it as one of the containers in an inference pipeline, you need to pass the container as one of the containers in the `Containers` field if you are using AWS SDK. If you are using SageMaker Python SDK, then you need to pass an instance of `SparkMLModel` as one of the models in the `PipelineModel` instance that you will create. For more information on this, please see the documentation on SageMaker Python SDK.\n\n\nUsing the Docker image for performing inference locally\n=======================================================\n\nYou can also build and test this container locally or deploy it outside of SageMaker to perform predictions against an MLeap serialized Spark ML Pipeline.\n\n### Installing Docker\n\nFirst you need to ensure that have installed [Docker](https://www.docker.com/) on your development environment and you have it running with `docker start`.\n\n#### Building the image locally\n\nIn order to build the Docker image, you need to run a single Docker command:\n\n```\ndocker build -t sagemaker-sparkml-serving:3.3 .\n```\n\n#### Running the image locally\n\nIn order to run the Docker image, you need to run the following command. Please make sure that the serialized model artifact is present in `/tmp/model` or change the location to where it is stored in the following command. \nThe command will start the server on port 8080 and will also pass the schema as an environment variable to the Docker container. Alternatively, you can edit the `Dockerfile` to add `ENV SAGEMAKER_SPARKML_SCHEMA=schema` as well before building the Docker image.\n\n```\ndocker run -p 8080:8080 -e SAGEMAKER_SPARKML_SCHEMA=schema -v /tmp/model:/opt/ml/model sagemaker-sparkml-serving:3.3 serve\n```\n\n#### Invoking with a payload\n\nOnce the container starts to run, you can invoke it with a payload like this. Remember from our last schema definition that `feature_2` is a string. Note the difference in input for that.\n\n```\ncurl -i -H \"content-type:text/csv\" -d \"feature_1,feature_2,feature_3\" http://localhost:8080/invocations\n\nor \n\ncurl -i -H \"content-type:application/json\" -d \"{\\\"data\\\":[feature_1,\\\"feature_2\\\",feature_3]}\" http://localhost:8080/invocations\n```\n\nThe `Dockerfile` can be found at the root directory of the package. SageMaker SparkML Serving Container tags the Docker images using the Spark major version it is compatible with. Right now, it only supports Spark 3.3.0 and as a result, the Docker image is tagged with 3.3. \n\nIn order to save the effort of building the Docker image everytime you are making a code change, you can also install [Maven](http://maven.apache.org/) and run `mvn clean package` at your project root to verify if the code is compiling fine and unit tests are running without any issue. \n\n\nPublicly available Docker images from SageMaker\n===============================================\n\nIf you are not making any changes to the underlying code that powers this Docker container, you can also download one of the already built Docker images from SageMaker provided [Amazon ECR](https://aws.amazon.com/ecr/) repositories.\n\nIn order to download the image from the repository in `us-west-2` (US West - Oregon) region:\n\n* Make sure you have [Docker](https://www.docker.com/) installed in your development environment. Start the Docker client.\n* Install [AWS CLI](https://aws.amazon.com/cli/).\n* Authenticate your Docker client with `aws ecr get-login` with the following command:\n\n```\naws ecr get-login --region us-west-2 --registry-ids 246618743249 --no-include-email\n```\n\n* Download the Docker image with the following command:\n\n```\ndocker pull 246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-sparkml-serving:3.3\n```\n\nFor running the Docker image, please see the Running the image locally section from above.\n\nFor other regions, please see the region to ECR repository mapping provided above and download the image based on the region you are operating.\n\nLicense\n=======\nThis library is licensed under the Apache 2.0 License.\n\n", "release_dates": ["2022-10-11T18:01:06Z"]}, {"name": "sagemaker-tensorflow-extensions", "description": "SageMaker specific extensions to TensorFlow. ", "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2024-02-05T19:26:32Z", "2024-01-29T21:26:38Z", "2024-01-29T23:37:18Z", "2023-07-25T17:32:57Z", "2023-03-25T02:56:31Z", "2022-12-01T22:46:31Z", "2022-11-28T23:58:40Z", "2022-09-22T23:40:55Z", "2022-05-25T16:39:03Z", "2022-02-24T08:22:25Z", "2022-02-15T02:13:20Z", "2022-02-11T00:15:43Z", "2021-09-14T13:02:53Z", "2021-08-21T01:36:21Z", "2021-08-16T12:46:45Z", "2021-05-19T12:48:10Z", "2021-02-24T02:53:43Z", "2021-02-24T02:49:40Z", "2021-02-23T20:59:21Z", "2021-02-23T20:56:52Z", "2021-02-08T12:44:30Z", "2021-02-08T12:39:57Z", "2021-02-03T12:44:32Z", "2020-12-22T12:44:30Z", "2020-12-22T12:40:07Z", "2020-12-21T12:41:16Z", "2020-12-15T19:37:45Z", "2020-12-14T22:04:57Z", "2020-09-17T21:19:31Z", "2020-08-06T12:41:09Z"]}, {"name": "sagemaker-tensorflow-serving-container", "description": "A TensorFlow Serving solution for use in SageMaker. This repo is now deprecated.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ![image](https://user-images.githubusercontent.com/56273942/202568467-0ee721bb-1424-4efd-88fc-31b4f2a59dc6.png) DEPRECATED\n\n## Announcement:\nAs of September 13th, 2023, this repository is deprecated. The contents of this repository will remain available but we will no longer provide updates or accept new contributions and pull requests.\n\n# <img alt=\"SageMaker\" src=\"branding/icon/sagemaker-banner.png\" height=\"100\">\n\n# SageMaker TensorFlow Serving Container\n\nSageMaker TensorFlow Serving Container is an a open source project that builds\ndocker images for running TensorFlow Serving on\n[Amazon SageMaker](https://aws.amazon.com/documentation/sagemaker/).\n\nSupported versions of TensorFlow: ``1.4.1``, ``1.5.0``, ``1.6.0``, ``1.7.0``, ``1.8.0``, ``1.9.0``, ``1.10.0``, ``1.11.0``, ``1.12.0``, ``1.13.1``, ``1.14.0``, ``1.15.0``, ``2.0.0``.\n\nSupported versions of TensorFlow for Elastic Inference: ``1.11.0``, ``1.12.0``, ``1.13.1``, ``1.14.0``.\n\nECR repositories for SageMaker built TensorFlow Serving Container:\n\n- `'tensorflow-inference'` for any new version starting with ``1.13.0`` in the following AWS accounts:\n  - `\"871362719292\"` in `\"ap-east-1\"`;\n  - `\"217643126080\"` in `\"me-south-1\"`;\n  - `\"886529160074\"` in `\"us-iso-east-1\"`;\n  - `\"763104351884\"` in other SageMaker public regions.\n- `'sagemaker-tensorflow-serving'` for ``1.4.1``, ``1.5.0``, ``1.6.0``, ``1.7.0``, ``1.8.0``, ``1.9.0``, ``1.10.0``, ``1.11.0``, ``1.12.0`` versions in the following AWS accounts:\n  - `\"057415533634\"` in `\"ap-east-1\"`;\n  - `\"724002660598\"` in `\"me-south-1\"`;\n  - `\"520713654638\"` in other SageMaker public regions.\n\nECR repositories for SageMaker built TensorFlow Serving Container for Elastic Inference:\n\n- `'tensorflow-inference-eia'` for any new version starting with ``1.14.0`` in the same AWS accounts as TensorFlow Serving Container for newer TensorFlow versions listed above;\n- `'sagemaker-tensorflow-serving-eia'` for ``1.11.0``, ``1.12.0``, ``1.13.1`` versions in the same AWS accounts as TensorFlow Serving Container for older TensorFlow versions listed above.\n\nThis documentation covers building and testing these docker images.\n\nFor information about using TensorFlow Serving on SageMaker, see:\n[Deploying to TensorFlow Serving Endpoints](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html)\nin the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) documentation.\n\nFor notebook examples, see: [Amazon SageMaker Examples](https://github.com/awslabs/amazon-sagemaker-examples).\n\n## Table of Contents\n\n1. [Getting Started](#getting-started)\n2. [Building your image](#building-your-image)\n3. [Running the tests](#running-the-tests)\n4. [Pre/Post-Processing](#pre/post-processing)\n5. [Deploying a TensorFlow Serving Model](#deploying-a-tensorflow-serving-model)\n6. [Enable Batching](#enabling-batching)\n7. [Configurable SageMaker Environment Variables](#configurable-sagemaker-environment-variables)\n8. [Deploying to Multi-Model Endpoint](#deploying-to-multi-model-endpoint)\n\n## Getting Started\n\n### Prerequisites\n\nMake sure you have installed all of the following prerequisites on your\ndevelopment machine:\n\n- [Docker](https://www.docker.com/)\n- [AWS CLI](https://aws.amazon.com/cli/)\n\nFor testing, you will also need:\n\n- [Python 3.6](https://www.python.org/)\n- [tox](https://tox.readthedocs.io/en/latest/)\n- [npm](https://npmjs.org/)\n- [jshint](https://jshint.com/about/)\n\nTo test GPU images locally, you will also need:\n\n- [nvidia-docker](https://github.com/NVIDIA/nvidia-docker)\n\n**Note:** Some of the build and tests scripts interact with resources in your AWS account. Be sure to\nset your default AWS credentials and region using `aws configure` before using these scripts.\n\n## Building your image\n\nAmazon SageMaker uses Docker containers to run all training jobs and inference endpoints.\n\nThe Docker images are built from the Dockerfiles in\n[docker/](https://github.com/aws/sagemaker-tensorflow-serving-container/tree/master/docker).\n\nThe Dockerfiles are grouped based on the version of TensorFlow Serving they support. Each supported\nprocessor type (e.g. \"cpu\", \"gpu\", \"ei\") has a different Dockerfile in each group.\n\nTo build an image, run the `./scripts/build.sh` script:\n\n```bash\n./scripts/build.sh --version 1.13 --arch cpu\n./scripts/build.sh --version 1.13 --arch gpu\n./scripts/build.sh --version 1.13 --arch eia\n```\n\n\nIf your are testing locally, building the image is enough. But if you want to your updated image\nin SageMaker, you need to publish it to an ECR repository in your account. The\n`./scripts/publish.sh` script makes that easy:\n\n```bash\n./scripts/publish.sh --version 1.13 --arch cpu\n./scripts/publish.sh --version 1.13 --arch gpu\n./scripts/publish.sh --version 1.13 --arch eia\n```\n\nNote: this will publish to ECR in your default region. Use the `--region` argument to\nspecify a different region.\n\n### Running your image in local docker\n\nYou can also run your container locally in Docker to test different models and input\ninference requests by hand. Standard `docker run` commands (or `nvidia-docker run` for\nGPU images) will work for this, or you can use the provided `start.sh`\nand `stop.sh` scripts:\n\n```bash\n./scripts/start.sh [--version x.xx] [--arch cpu|gpu|eia|...]\n./scripts/stop.sh [--version x.xx] [--arch cpu|gpu|eia|...]\n```\n\nWhen the container is running, you can send test requests to it using any HTTP client. Here's\nand an example using the `curl` command:\n\n```bash\ncurl -X POST --data-binary @test/resources/inputs/test.json \\\n     -H 'Content-Type: application/json' \\\n     -H 'X-Amzn-SageMaker-Custom-Attributes: tfs-model-name=half_plus_three' \\\n     http://localhost:8080/invocations\n```\n\nAdditional `curl` examples can be found in `./scripts/curl.sh`.\n\n## Running the tests\n\nThe package includes automated tests and code checks. The tests use Docker to run the container\nimage locally, and do not access resources in AWS. You can run the tests and static code\ncheckers using `tox`:\n\n```bash\ntox\n```\n\nTo run local tests against a single container or with other options, you can use the following command:\n\n```bash\npython -m pytest test/integration/local\n    [--docker-name-base <docker_name_base>]\n    [--framework-version <framework_version>]\n    [--processor-type <processor_type>]\n```\n\nTo test against Elastic Inference with Accelerator, you will need an AWS account, publish your built image to ECR repository and run the following command:\n\n    tox -e py36 -- test/integration/sagemaker/test_ei.py\n        [--repo <ECR_repository_name>]\n        [--instance-types <instance_type>,...]\n        [--accelerator-type <accelerator_type>]\n        [--versions <version>,...]\n\nFor example:\n\n    tox -e py36 -- test/integration/sagemaker/test_ei.py \\\n        --repo sagemaker-tensorflow-serving-eia \\\n        --instance_type ml.m5.xlarge \\\n        --accelerator-type ml.eia1.medium \\\n        --versions 1.13.0\n\n\n## Pre/Post-Processing\n\n**NOTE: There is currently no support for pre-/post-processing with multi-model containers.**\n\nSageMaker TensorFlow Serving Container supports the following Content-Types for requests:\n\n* `application/json` (default)\n* `text/csv`\n* `application/jsonlines`\n\nAnd the following content types for responses:\n\n* `application/json` (default)\n* `application/jsonlines`\n\nThe container will convert data in these formats to [TensorFlow Serving REST API](https://www.tensorflow.org/tfx/serving/api_rest) requests,\nand will send these requests to the default serving signature of your SavedModel bundle.\n\nYou can also add customized Python code to process your input and output data. To use this feature, you need to:\n1. Add a python file named `inference.py` to the code directory inside your model archive.\n2. In `inference.py`, implement either a pair of `input_handler` and `output_handler` functions or a single `handler` function. Note that if `handler` function is implemented, `input_handler` and `output_handler` will be ignored.\n\nTo implement pre/post-processing handler(s), you will need to make use of the `Context` object created by Python service. The `Context` is a `namedtuple` with following attributes:\n- `model_name (string)`: the name of the model you will to use for inference, for example 'half_plus_three'\n- `model_version (string)`: version of the model, for example '5'\n- `method (string)`: inference method, for example, 'predict', 'classify' or 'regress', for more information on methods, please see [Classify and Regress API](https://www.tensorflow.org/tfx/serving/api_rest#classify_and_regress_api) and [Predict API](https://www.tensorflow.org/tfx/serving/api_rest#predict_api)\n- `rest_uri (string)`: the TFS REST uri generated by the Python service, for example, 'http://localhost:8501/v1/models/half_plus_three:predict'\n- `grpc_port (string)`: the GRPC port number generated by the Python service, for example, '9000'\n- `custom_attributes (string)`: content of 'X-Amzn-SageMaker-Custom-Attributes' header from the original request, for example, 'tfs-model-name=half_plus_three,tfs-method=predict'\n- `request_content_type (string)`: the original request content type, defaulted to 'application/json' if not provided\n- `accept_header (string)`: the original request accept type, defaulted to 'application/json' if not provided\n- `content_length (int)`: content length of the original request\n\nHere's a code example implementing `input_handler` and `output_handler`. By providing these, the Python service will post the request to TFS REST uri with the data pre-processed by `input_handler` and pass the response to `output_handler` for post-processing.\n\n```python\nimport json\n\ndef input_handler(data, context):\n    \"\"\" Pre-process request input before it is sent to TensorFlow Serving REST API\n    Args:\n        data (obj): the request data, in format of dict or string\n        context (Context): an object containing request and configuration details\n    Returns:\n        (dict): a JSON-serializable dict that contains request body and headers\n    \"\"\"\n    if context.request_content_type == 'application/json':\n        # pass through json (assumes it's correctly formed)\n        d = data.read().decode('utf-8')\n        return d if len(d) else ''\n\n    if context.request_content_type == 'text/csv':\n        # very simple csv handler\n        return json.dumps({\n            'instances': [float(x) for x in data.read().decode('utf-8').split(',')]\n        })\n\n    raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(\n        context.request_content_type or \"unknown\"))\n\n\ndef output_handler(data, context):\n    \"\"\"Post-process TensorFlow Serving output before it is returned to the client.\n    Args:\n        data (obj): the TensorFlow serving response\n        context (Context): an object containing request and configuration details\n    Returns:\n        (bytes, string): data to return to client, response content type\n    \"\"\"\n    if data.status_code != 200:\n        raise ValueError(data.content.decode('utf-8'))\n\n    response_content_type = context.accept_header\n    prediction = data.content\n    return prediction, response_content_type\n```\n\nHere's another code example implementing `input_handler` and `output_handler` to format image data into a TFS request that expects image data as an encoded string rather than as a numeric tensor:\n\n```python\nimport base64\nimport io\nimport json\nimport requests\n\ndef input_handler(data, context):\n    \"\"\" Pre-process request input before it is sent to TensorFlow Serving REST API\n\n    Args:\n        data (obj): the request data stream\n        context (Context): an object containing request and configuration details\n\n    Returns:\n        (dict): a JSON-serializable dict that contains request body and headers\n    \"\"\"\n\n    if context.request_content_type == 'application/x-image':\n        payload = data.read()\n        encoded_image = base64.b64encode(payload).decode('utf-8')\n        instance = [{\"b64\": encoded_image}]\n        return json.dumps({\"instances\": instance})\n    else:\n        _return_error(415, 'Unsupported content type \"{}\"'.format(\n            context.request_content_type or 'Unknown'))\n\n\ndef output_handler(response, context):\n    \"\"\"Post-process TensorFlow Serving output before it is returned to the client.\n\n    Args:\n        response (obj): the TensorFlow serving response\n        context (Context): an object containing request and configuration details\n\n    Returns:\n        (bytes, string): data to return to client, response content type\n    \"\"\"\n    if response.status_code != 200:\n        _return_error(response.status_code, response.content.decode('utf-8'))\n    response_content_type = context.accept_header\n    prediction = response.content\n    return prediction, response_content_type\n\n\ndef _return_error(code, message):\n    raise ValueError('Error: {}, {}'.format(str(code), message))\n```\n\nThe `input_handler` above creates requests that match the input of the following TensorFlow Serving SignatureDef, displayed\nusing the TensorFlow `saved_model_cli`:\n\n```\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['image_bytes'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1)\n        name: input_tensor:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['classes'] tensor_info:\n        dtype: DT_INT64\n        shape: (-1)\n        name: ArgMax:0\n    outputs['probabilities'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1001)\n        name: softmax_tensor:0\n  Method name is: tensorflow/serving/predict\n```\n\n\nThere are occasions when you might want to have complete control over the request handler. For example, making TFS request (REST or GRPC) to one model, and then making a request to a second model. In this case, you may implement the `handler` instead of the `input_handler` and `output_handler` pair:\n\n```python\nimport json\nimport requests\n\n\ndef handler(data, context):\n    \"\"\"Handle request.\n    Args:\n        data (obj): the request data\n        context (Context): an object containing request and configuration details\n    Returns:\n        (bytes, string): data to return to client, (optional) response content type\n    \"\"\"\n    processed_input = _process_input(data, context)\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\ndef _process_input(data, context):\n    if context.request_content_type == 'application/json':\n        # pass through json (assumes it's correctly formed)\n        d = data.read().decode('utf-8')\n        return d if len(d) else ''\n\n    if context.request_content_type == 'text/csv':\n        # very simple csv handler\n        return json.dumps({\n            'instances': [float(x) for x in data.read().decode('utf-8').split(',')]\n        })\n\n    raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(\n        context.request_content_type or \"unknown\"))\n\n\ndef _process_output(data, context):\n    if data.status_code != 200:\n        raise ValueError(data.content.decode('utf-8'))\n\n    response_content_type = context.accept_header\n    prediction = data.content\n    return prediction, response_content_type\n```\n\nYou can also bring in external dependencies to help with your data processing. There are 2 ways to do this:\n1. If your model archive contains `code/requirements.txt`, the container will install the Python dependencies at runtime using `pip install -r`.\n2. If you are working in a network-isolation situation or if you don't want to install dependencies at runtime everytime your Endpoint starts or Batch Transform job runs, you may want to put pre-downloaded dependencies under `code/lib` directory in your model archive, the container will then add the modules to the Python path. Note that if both `code/lib` and `code/requirements.txt` are present in the model archive, the `requirements.txt` will be ignored.\n\nYour untarred model directory structure may look like this if you are using `requirements.txt`:\n\n        model1\n            |--[model_version_number]\n                |--variables\n                |--saved_model.pb\n        model2\n            |--[model_version_number]\n                |--assets\n                |--variables\n                |--saved_model.pb\n        code\n            |--inference.py\n            |--requirements.txt\n\nYour untarred model directory structure may look like this if you have downloaded modules under `code/lib`:\n\n        model1\n            |--[model_version_number]\n                |--variables\n                |--saved_model.pb\n        model2\n            |--[model_version_number]\n                |--assets\n                |--variables\n                |--saved_model.pb\n        code\n            |--lib\n                |--external_module\n            |--inference.py\n\n## Deploying a TensorFlow Serving Model\n\nTo use your TensorFlow Serving model on SageMaker, you first need to create a SageMaker Model. After creating a SageMaker Model, you can use it to create [SageMaker Batch Transform Jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)\n for offline inference, or create [SageMaker Endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html) for real-time inference.\n\n\n### Creating a SageMaker Model\n\nA SageMaker Model contains references to a `model.tar.gz` file in S3 containing serialized model data, and a Docker image used to serve predictions with that model.\n\nYou must package the contents in a model directory (including models, inference.py and external modules) in .tar.gz format in a file named \"model.tar.gz\" and upload it to S3. If you're on a Unix-based operating system, you can create a \"model.tar.gz\" using the `tar` utility:\n\n```\ntar -czvf model.tar.gz 12345 code\n```\n\nwhere \"12345\" is your TensorFlow serving model version which contains your SavedModel.\n\nAfter uploading your `model.tar.gz` to an S3 URI, such as `s3://your-bucket/your-models/model.tar.gz`, create a [SageMaker Model](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateModel.html) which will be used to generate inferences. Set `PrimaryContainer.ModelDataUrl` to the S3 URI where you uploaded the `model.tar.gz`, and set `PrimaryContainer.Image` to an image following this format:\n\n```\n520713654638.dkr.ecr.{REGION}.amazonaws.com/sagemaker-tensorflow-serving:{SAGEMAKER_TENSORFLOW_SERVING_VERSION}-{cpu|gpu}\n```\n\n```\n763104351884.dkr.ecr.{REGION}.amazonaws.com/tensorflow-inference:{TENSORFLOW_INFERENCE_VERSION}-{cpu|gpu}\n```\n\nFor those using Elastic Inference set the image following this format instead:\n\n```\n520713654638.dkr.ecr.{REGION}.amazonaws.com/sagemaker-tensorflow-serving-eia:{SAGEMAKER_TENSORFLOW_SERVING_EIA_VERSION}-cpu\n```\n\n```\n763104351884.dkr.ecr.{REGION}.amazonaws.com/tensorflow-inference-eia:{TENSORFLOW_INFERENCE_EIA_VERSION}-cpu\n```\n\nWhere `REGION` is your AWS region, such as \"us-east-1\" or \"eu-west-1\"; `SAGEMAKER_TENSORFLOW_SERVING_VERSION`, `SAGEMAKER_TENSORFLOW_SERVING_EIA_VERSION`, `TENSORFLOW_INFERENCE_VERSION`, `TENSORFLOW_INFERENCE_EIA_VERSION` are one of the supported versions mentioned above; and \"gpu\" for use on GPU-based instance types like ml.p3.2xlarge, or \"cpu\" for use on CPU-based instances like `ml.c5.xlarge`.\n\nThe code examples below show how to create a SageMaker Model from a `model.tar.gz` containing a TensorFlow Serving model using the AWS CLI (though you can use any language supported by the [AWS SDK](https://aws.amazon.com/tools/)) and the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk).\n\n#### AWS CLI\n```bash\ntimestamp() {\n  date +%Y-%m-%d-%H-%M-%S\n}\n\n\nMODEL_NAME=\"image-classification-tfs-$(timestamp)\"\nMODEL_DATA_URL=\"s3://my-sagemaker-bucket/model/model.tar.gz\"\n\naws s3 cp model.tar.gz $MODEL_DATA_URL\n\nREGION=\"us-west-2\"\nTFS_VERSION=\"1.12.0\"\nPROCESSOR_TYPE=\"gpu\"\nIMAGE=\"520713654638.dkr.ecr.$REGION.amazonaws.com/sagemaker-tensorflow-serving:$TFS_VERSION-$PROCESSOR_TYPE\"\n\n# See the following document for more on SageMaker Roles:\n# https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html\nROLE_ARN=\"[SageMaker-compatible IAM Role ARN]\"\n\naws sagemaker create-model \\\n    --model-name $MODEL_NAME \\\n    --primary-container Image=$IMAGE,ModelDataUrl=$MODEL_DATA_URL \\\n    --execution-role-arn $ROLE_ARN\n```\n\n#### SageMaker Python SDK\n\n```python\nimport os\nimport sagemaker\nfrom sagemaker.tensorflow.serving import Model\n\nsagemaker_session = sagemaker.Session()\nrole = 'arn:aws:iam::038453126632:role/service-role/AmazonSageMaker-ExecutionRole-20180718T141171'\nbucket = 'am-datasets'\nprefix = 'sagemaker/high-throughput-tfs-batch-transform'\ns3_path = 's3://{}/{}'.format(bucket, prefix)\n\nmodel_data = sagemaker_session.upload_data('model.tar.gz',\n                                           bucket,\n                                           os.path.join(prefix, 'model'))\n\n# The \"Model\" object doesn't create a SageMaker Model until a Transform Job or Endpoint is created.\ntensorflow_serving_model = Model(model_data=model_data,\n                                 role=role,\n                                 framework_version='1.13',\n                                 sagemaker_session=sagemaker_session)\n\n```\n\nAfter creating a SageMaker Model, you can refer to the model name to create Transform Jobs and Endpoints. Code examples are given below.\n\n### Creating a Batch Transform Job\n\nA Batch Transform job runs an offline-inference job using your TensorFlow Serving model. Input data in S3 is converted to HTTP requests,\nand responses are saved to an output bucket in S3.\n\n#### CLI\n```bash\nTRANSFORM_JOB_NAME=\"tfs-transform-job\"\nTRANSFORM_S3_INPUT=\"s3://my-sagemaker-input-bucket/sagemaker-transform-input-data/\"\nTRANSFORM_S3_OUTPUT=\"s3://my-sagemaker-output-bucket/sagemaker-transform-output-data/\"\n\nTRANSFORM_INPUT_DATA_SOURCE={S3DataSource={S3DataType=\"S3Prefix\",S3Uri=$TRANSFORM_S3_INPUT}}\nCONTENT_TYPE=\"application/x-image\"\n\nINSTANCE_TYPE=\"ml.p2.xlarge\"\nINSTANCE_COUNT=2\n\nMAX_PAYLOAD_IN_MB=1\nMAX_CONCURRENT_TRANSFORMS=16\n\naws sagemaker create-transform-job \\\n    --model-name $MODEL_NAME \\\n    --transform-input DataSource=$TRANSFORM_INPUT_DATA_SOURCE,ContentType=$CONTENT_TYPE \\\n    --transform-output S3OutputPath=$TRANSFORM_S3_OUTPUT \\\n    --transform-resources InstanceType=$INSTANCE_TYPE,InstanceCount=$INSTANCE_COUNT \\\n    --max-payload-in-mb $MAX_PAYLOAD_IN_MB \\\n    --max-concurrent-transforms $MAX_CONCURRENT_TRANSFORMS \\\n    --transform-job-name $JOB_NAME\n```\n\n#### SageMaker Python SDK\n\n```python\noutput_path = 's3://my-sagemaker-output-bucket/sagemaker-transform-output-data/'\ntensorflow_serving_transformer = tensorflow_serving_model.transformer(\n                                     framework_version = '1.12',\n                                     instance_count=2,\n                                     instance_type='ml.p2.xlarge',\n                                     max_concurrent_transforms=16,\n                                     max_payload=1,\n                                     output_path=output_path)\n\ninput_path = 's3://my-sagemaker-input-bucket/sagemaker-transform-input-data/'\ntensorflow_serving_transformer.transform(input_path, content_type='application/x-image')\n```\n\n### Creating an Endpoint\n\nA SageMaker Endpoint hosts your TensorFlow Serving model for real-time inference. The [InvokeEndpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/API_runtime_InvokeEndpoint.html) API\nis used to send data for predictions to your TensorFlow Serving model.\n\n#### AWS CLI\n\n```bash\nENDPOINT_CONFIG_NAME=\"my-endpoint-config\"\nVARIANT_NAME=\"TFS\"\nINITIAL_INSTANCE_COUNT=1\nINSTANCE_TYPE=\"ml.p2.xlarge\"\naws sagemaker create-endpoint-config \\\n    --endpoint-config-name $ENDPOINT_CONFIG_NAME \\\n    --production-variants VariantName=$VARIANT_NAME,ModelName=$MODEL_NAME,InitialInstanceCount=$INITIAL_INSTANCE_COUNT,InstanceType=$INSTANCE_TYPE\n\nENDPOINT_NAME=\"my-tfs-endpoint\"\naws sagemaker create-endpoint \\\n    --endpoint-name $ENDPOINT_NAME \\\n    --endpoint-config-name $ENDPOINT_CONFIG_NAME\n\nBODY=\"fileb://myfile.jpeg\"\nCONTENT_TYPE='application/x-image'\nOUTFILE=\"response.json\"\naws sagemaker-runtime invoke-endpoint \\\n    --endpoint-name $ENDPOINT_NAME \\\n    --content-type=$CONTENT_TYPE \\\n    --body $BODY \\\n    $OUTFILE\n```\n\n#### SageMaker Python SDK\n\n```python\npredictor = tensorflow_serving_model.deploy(initial_instance_count=1,\n                                            framework_version='1.12',\n                                            instance_type='ml.p2.xlarge')\nprediction = predictor.predict(data)\n```\n\n## Enabling Batching\n\nYou can configure SageMaker TensorFlow Serving Container to batch multiple records together before\nperforming an inference. This uses [TensorFlow Serving's](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md)\nunderlying batching feature.\n\nYou may be able to significantly improve throughput, especially on GPU instances, by\nenabling and configuring batching. To get the best performance, it may be necessary to tune batching parameters,\nespecially the batch size and batch timeout, to your model, input data, and instance type.\n\nYou can set the following environment variables on a SageMaker Model or Transform Job to enable\nand configure batching:\n\n```bash\n# Configures whether to enable record batching.\n# Defaults to false.\nSAGEMAKER_TFS_ENABLE_BATCHING=\"true\"\n\n# Configures how many records\n# Corresponds to \"max_batch_size\" in TensorFlow Serving.\n# Defaults to 8.\nSAGEMAKER_TFS_MAX_BATCH_SIZE=\"32\"\n\n# Configures how long to wait for a full batch, in microseconds.\n# Corresponds to \"batch_timeout_micros\" in TensorFlow Serving.\n# Defaults to 1000 (1ms).\nSAGEMAKER_TFS_BATCH_TIMEOUT_MICROS=\"100000\"\n\n# Configures how many batches to process concurrently.\n# Corresponds to \"num_batch_threads\" in TensorFlow Serving\n# Defaults to number of CPUs.\nSAGEMAKER_TFS_NUM_BATCH_THREADS=\"16\"\n\n# Configures number of batches that can be enqueued.\n# Corresponds to \"max_enqueued_batches\" in TensorFlow Serving.\n# Defaults to number of CPUs for real-time inference,\n# or arbitrarily large for batch transform (because batch transform).\nSAGEMAKER_TFS_MAX_ENQUEUED_BATCHES=\"10000\"\n```\n\n## Configurable SageMaker Environment Variables\nThe following environment variables can be set on a SageMaker Model or Transform Job if further configuration is required:\n\n[Configures](https://docs.gunicorn.org/en/stable/settings.html#loglevel)\nthe logging level for Gunicorn.\n```bash\n# Defaults to \"info\"\nSAGEMAKER_GUNICORN_LOGLEVEL=\"debug\"\n```\n[Configures](https://docs.gunicorn.org/en/stable/settings.html#timeout)\nhow long a Gunicorn worker may be silent before it is killed and restarted.\n```bash\n# Defaults to 30.\nSAGEMAKER_GUNICORN_TIMEOUT_SECONDS=\"60\"\n```\n[Configures](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_read_timeout)\nthe timeout for reading a response from the proxied server.\nNote: If SAGEMAKER_GUNICORN_TIMEOUT_SECONDS is greater, \nSAGEMAKER_NGINX_PROXY_READ_TIMEOUT_SECONDS will be set to the \nvalue of SAGEMAKER_GUNICORN_TIMEOUT_SECONDS.\n```bash\n# Defaults to 60.\nSAGEMAKER_NGINX_PROXY_READ_TIMEOUT_SECONDS=\"120\"\n```\n\n## Deploying to Multi-Model Endpoint\n\nSageMaker TensorFlow Serving container (version 1.5.0 and 2.1.0, CPU) now supports Multi-Model Endpoint. With this feature, you can deploy different models (not just different versions of a model) to a single endpoint.\nTo deploy a Multi-Model endpoint with TFS container, please start the container with environment variable ``SAGEMAKER_MULTI_MODEL=True``.\n\n### Multi-Model Interfaces\nWe provide four different interfaces for user to interact with a Multi-Model Mode container:\n\n    +---------------------+---------------------------------+---------------------------------------------+\n    | Functionality       | Request                         | Response/Actions                            |\n    +---------------------+---------------------------------+---------------------------------------------+\n    | List A Single Model | GET /models/{model_name}        | Information about the specified model       |\n    +---------------------+---------------------------------+---------------------------------------------+\n    | List All Models     | GET /models                     | List of Information about all loaded models |\n    +---------------------+---------------------------------+---------------------------------------------+\n    |                     | POST /models                    | Load model with \"model_name\" from           |\n    |                     | data = {                        | specified url                               |\n    | Load A Model        |     \"model_name\": <model-name>, |                                             |\n    |                     |     \"url\": <path to model data> |                                             |\n    |                     | }                               |                                             |\n    +---------------------+---------------------------------+---------------------------------------------+\n    | Make Invocations    | POST /models/{model_name}/invoke| Return inference result from                |\n    |                     | data = <invocation payload>     | the specified model                         |\n    +---------------------+---------------------------------+---------------------------------------------+\n    | Unload A Model      | DELETE /models/{model_name}     | Unload the specified model                  |\n    +---------------------+---------------------------------+---------------------------------------------+\n\n### Maximum Number of Models\nAlso please note the environment variable ``SAGEMAKER_SAFE_PORT_RANGE`` will limit the number of models that can be loaded to the endpoint at the same time.\nOnly 90% of the ports will be utilized and each loaded model will be allocated with 2 ports (one for REST API and the other for GRPC).\nFor example, if the ``SAGEMAKER_SAFE_PORT_RANGE`` is between 9000 to 9999, the maximum number of models that can be loaded to the endpoint at the same time would be 499 ((9999 - 9000) * 0.9 / 2).\n\n### Using Multi-Model Endpoint with Pre/Post-Processing\nMulti-Model Endpoint can be used together with Pre/Post-Processing. Each model will need its own ``inference.py`` otherwise default handlers will be used. An example of the directory structure of Multi-Model Endpoint and Pre/Post-Processing would look like this:\n\n        /opt/ml/models/model1/model\n            |--[model_version_number]\n                |--variables\n                |--saved_model.pb\n        /opt/ml/models/model2/model\n            |--[model_version_number]\n                |--assets\n                |--variables\n                |--saved_model.pb\n            code\n                |--lib\n                    |--external_module\n                |--inference.py\n\n## Contributing\n\nPlease read [CONTRIBUTING.md](https://github.com/aws/sagemaker-tensorflow-serving-container/blob/master/CONTRIBUTING.md)\nfor details on our code of conduct, and the process for submitting pull requests to us.\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n", "release_dates": ["2021-06-30T15:31:27Z", "2021-04-26T15:38:13Z", "2021-04-13T15:30:03Z", "2021-04-07T15:29:55Z", "2021-03-23T15:29:47Z", "2020-07-29T15:48:39Z", "2020-07-23T15:41:50Z", "2020-07-17T15:41:12Z", "2020-06-25T15:45:34Z", "2020-06-11T15:38:47Z", "2020-05-13T15:44:38Z", "2020-04-20T15:35:32Z", "2020-04-16T15:35:57Z", "2020-04-03T15:40:52Z", "2020-04-01T15:35:22Z", "2020-03-26T15:45:19Z", "2020-02-18T15:46:57Z", "2020-02-17T15:51:47Z", "2020-02-04T15:44:12Z", "2020-01-20T15:50:32Z", "2020-01-10T15:53:27Z", "2020-01-08T15:37:16Z", "2020-01-03T15:30:07Z", "2020-01-02T15:44:49Z", "2019-12-17T15:32:44Z", "2019-12-13T15:53:18Z", "2019-11-25T15:46:25Z", "2019-10-25T17:47:56Z", "2019-10-22T15:42:37Z", "2019-10-16T15:37:13Z"]}, {"name": "sagemaker-tensorflow-training-toolkit", "description": "Toolkit for running TensorFlow training scripts on SageMaker. Dockerfiles used for building SageMaker TensorFlow Containers are at https://github.com/aws/deep-learning-containers. ", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2022-12-06T00:11:29Z", "2022-07-08T09:12:31Z", "2022-06-04T05:55:20Z", "2020-12-17T21:01:11Z", "2020-12-11T04:09:14Z", "2020-12-08T23:38:11Z", "2020-12-08T23:34:28Z", "2020-11-06T20:50:13Z", "2020-11-06T20:50:34Z", "2020-10-19T13:20:28Z", "2020-10-15T14:20:47Z", "2020-08-27T13:20:33Z", "2020-08-23T20:22:26Z", "2020-08-23T20:23:21Z", "2020-08-06T03:49:23Z", "2020-07-02T02:21:01Z", "2020-07-01T14:19:17Z", "2020-06-29T14:20:17Z", "2020-06-18T13:20:04Z", "2020-06-18T14:20:38Z", "2020-06-15T13:19:56Z", "2020-06-11T14:20:41Z", "2020-06-10T04:51:29Z", "2020-06-10T23:46:33Z", "2020-06-10T05:03:16Z", "2020-05-18T17:01:11Z", "2020-05-13T13:19:56Z", "2020-05-12T20:10:39Z", "2020-05-09T00:22:17Z", "2020-05-05T14:20:40Z"]}, {"name": "sagemaker-training-toolkit", "description": "Train machine learning models within a \ud83d\udc33 Docker container using \ud83e\udde0 Amazon SageMaker.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "![SageMaker](https://github.com/aws/sagemaker-training-toolkit/raw/master/branding/icon/sagemaker-banner.png)\n\n# SageMaker Training Toolkit\n\n[![Latest Version](https://img.shields.io/pypi/v/sagemaker-training.svg)](https://pypi.python.org/pypi/sagemaker-training) [![Supported Python Versions](https://img.shields.io/pypi/pyversions/sagemaker-training.svg)](https://pypi.python.org/pypi/sagemaker-training) [![Code Style: Black](https://img.shields.io/badge/code_style-black-000000.svg)](https://github.com/python/black)\n\nTrain machine learning models within a Docker container using Amazon SageMaker.\n\n\n## :books: Background\n\n[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service for data science and machine learning (ML) workflows.\nYou can use Amazon SageMaker to simplify the process of building, training, and deploying ML models.\n\nTo train a model, you can include your training script and dependencies in a [Docker container](https://www.docker.com/resources/what-container) that runs your training code.\nA container provides an effectively isolated environment, ensuring a consistent runtime and reliable training process. \n\nThe **SageMaker Training Toolkit** can be easily added to any Docker container, making it compatible with SageMaker for [training models](https://aws.amazon.com/sagemaker/train/).\nIf you use a [prebuilt SageMaker Docker image for training](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html), this library may already be included.\n\nFor more information, see the Amazon SageMaker Developer Guide sections on [using Docker containers for training](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html).\n\n## :hammer_and_wrench: Installation\n\nTo install this library in your Docker image, add the following line to your [Dockerfile](https://docs.docker.com/engine/reference/builder/):\n\n``` dockerfile\nRUN pip3 install sagemaker-training\n```\n\n## :computer: Usage\n\nThe following are brief how-to guides.\nFor complete, working examples of custom training containers built with the SageMaker Training Toolkit, please see [the example notebooks](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/custom-training-containers).\n\n### Create a Docker image and train a model\n\n1. Write a training script (eg. `train.py`).\n\n2. [Define a container with a Dockerfile](https://docs.docker.com/get-started/part2/#define-a-container-with-dockerfile) that includes the training script and any dependencies.\n\n    The training script must be located in the `/opt/ml/code` directory.\n    The environment variable `SAGEMAKER_PROGRAM` defines which file inside the `/opt/ml/code` directory to use as the training entry point.\n    When training starts, the interpreter executes the entry point defined by `SAGEMAKER_PROGRAM`.\n    Python and shell scripts are both supported.\n    \n    ``` docker\n    FROM yourbaseimage:tag\n  \n    # install the SageMaker Training Toolkit \n    RUN pip3 install sagemaker-training\n\n    # copy the training script inside the container\n    COPY train.py /opt/ml/code/train.py\n\n    # define train.py as the script entry point\n    ENV SAGEMAKER_PROGRAM train.py\n    ```\n\n3. Build and tag the Docker image.\n\n    ``` shell\n    docker build -t custom-training-container .\n    ```\n\n4. Use the Docker image to start a training job using the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk).\n\n    ``` python\n    from sagemaker.estimator import Estimator\n\n    estimator = Estimator(image_name=\"custom-training-container\",\n                          role=\"SageMakerRole\",\n                          train_instance_count=1,\n                          train_instance_type=\"local\")\n\n    estimator.fit()\n    ```\n    \n    To train a model using the image on SageMaker, [push the image to ECR](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html) and start a SageMaker training job with the image URI.\n    \n\n### Pass arguments to the entry point using hyperparameters\n\nAny hyperparameters provided by the training job are passed to the entry point as script arguments.\nThe SageMaker Python SDK uses this feature to pass special hyperparameters to the training job, including `sagemaker_program` and `sagemaker_submit_directory`.\nThe complete list of SageMaker hyperparameters is available [here](https://github.com/aws/sagemaker-training-toolkit/blob/master/src/sagemaker_training/params.py).\n\n1. Implement an argument parser in the entry point script. For example, in a Python script:\n\n    ``` python\n    import argparse\n\n    if __name__ == \"__main__\":\n      parser = argparse.ArgumentParser()\n\n      parser.add_argument(\"--learning-rate\", type=int, default=1)\n      parser.add_argument(\"--batch-size\", type=int, default=64)\n      parser.add_argument(\"--communicator\", type=str)\n      parser.add_argument(\"--frequency\", type=int, default=20)\n\n      args = parser.parse_args()\n      ...\n    ```\n\n2. Start a training job with hyperparameters.\n\n    ``` python\n    {\"HyperParameters\": {\"batch-size\": 256, \"learning-rate\": 0.0001, \"communicator\": \"pure_nccl\"}}\n    ```\n\n### Read additional information using environment variables\n\nAn entry point often needs additional information not available in `hyperparameters`.\nThe SageMaker Training Toolkit writes this information as environment variables that are available from within the script.\nFor example, this training job includes the channels `training` and `testing`:\n\n``` python\nfrom sagemaker.pytorch import PyTorch\n\nestimator = PyTorch(entry_point=\"train.py\", ...)\n\nestimator.fit({\"training\": \"s3://bucket/path/to/training/data\", \n               \"testing\": \"s3://bucket/path/to/testing/data\"})\n```\n\nThe environment variables `SM_CHANNEL_TRAINING` and `SM_CHANNEL_TESTING` provide the paths to the channels:\n\n``` python\nimport argparse\nimport os\n\nif __name__ == \"__main__\":\n  parser = argparse.ArgumentParser()\n\n  ...\n\n  # reads input channels training and testing from the environment variables\n  parser.add_argument(\"--training\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n  parser.add_argument(\"--testing\", type=str, default=os.environ[\"SM_CHANNEL_TESTING\"])\n\n  args = parser.parse_args()\n\n  ...\n```\n\nWhen training starts, SageMaker Training Toolkit will print all available environment variables. Please see the [reference on environment variables](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md) for a full list of provided environment variables.\n\n### Get information about the container environment\n\nTo get information about the container environment, initialize an `Environment` object.\n`Environment` provides access to aspects of the environment relevant to training jobs, including hyperparameters, system characteristics, filesystem locations, environment variables and configuration settings.\nIt is a read-only snapshot of the container environment during training, and it doesn't contain any form of state.\n\n``` python\nfrom sagemaker_training import environment\n\nenv = environment.Environment()\n\n# get the path of the channel \"training\" from the `inputdataconfig.json` file\ntraining_dir = env.channel_input_dirs[\"training\"]\n\n# get a the hyperparameter \"training_data_file\" from `hyperparameters.json` file\nfile_name = env.hyperparameters[\"training_data_file\"]\n\n# get the folder where the model should be saved\nmodel_dir = env.model_dir\n\n# train the model\ndata = np.load(os.path.join(training_dir, file_name))\nx_train, y_train = data[\"features\"], keras.utils.to_categorical(data[\"labels\"])\nmodel = ResNet50(weights=\"imagenet\")\n...\nmodel.fit(x_train, y_train)\n\n#save the model to the model_dir at the end of training\nmodel.save(os.path.join(model_dir, \"saved_model\"))\n```\n\n### Execute the entry point\n\nTo execute the entry point, call `entry_point.run()`.\n\n``` python\nfrom sagemaker_training import entry_point, environment\n\nenv = environment.Environment()\n\n# read hyperparameters as script arguments\nargs = env.to_cmd_args()\n\n# get the environment variables\nenv_vars = env.to_env_vars()\n\n# execute the entry point\nentry_point.run(uri=env.module_dir,\n                user_entry_point=env.user_entry_point,\n                args=args,\n                env_vars=env_vars)\n\n```\n\nIf the entry point execution fails, `trainer.train()` will write the error message to `/opt/ml/output/failure`. Otherwise, it will write to the file `/opt/ml/success`.\n\n## :scroll: License\n\nThis library is licensed under the [Apache 2.0 License](http://aws.amazon.com/apache2.0/).\nFor more details, please take a look at the [LICENSE](https://github.com/aws/sagemaker-training-toolkit/blob/master/LICENSE) file.\n\n## :handshake: Contributing\n\nContributions are welcome!\nPlease read our [contributing guidelines](https://github.com/aws/sagemaker-training-toolkit/blob/master/CONTRIBUTING.md)\nif you'd like to open an issue or submit a pull request.\n", "release_dates": ["2023-10-31T18:03:17Z", "2023-10-23T16:46:13Z", "2023-10-19T16:46:00Z", "2023-10-17T16:46:51Z", "2023-08-08T16:46:43Z", "2023-06-19T16:46:08Z", "2023-06-15T16:46:32Z", "2023-04-26T16:47:04Z", "2023-04-10T16:46:48Z", "2023-04-05T16:46:25Z", "2023-03-09T21:06:23Z", "2023-03-02T16:45:25Z", "2023-02-22T16:47:15Z", "2023-01-24T05:20:34Z", "2023-01-23T16:47:01Z", "2023-01-18T01:53:38Z", "2023-01-16T16:46:04Z", "2022-12-13T03:03:40Z", "2022-12-06T17:48:50Z", "2022-11-29T23:06:06Z", "2022-10-27T20:32:37Z", "2022-10-20T18:48:38Z", "2022-10-17T16:31:44Z", "2022-09-26T16:33:47Z", "2022-09-12T20:19:00Z", "2022-09-10T00:18:49Z", "2022-08-18T15:17:45Z", "2022-08-17T16:28:21Z", "2022-08-15T22:48:06Z", "2022-08-11T23:49:43Z"]}, {"name": "sagemaker-xgboost-container", "description": "This is the Docker container based on open source framework XGBoost (https://xgboost.readthedocs.io/en/latest/) to allow customers use their own XGBoost scripts in SageMaker.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": null, "release_dates": ["2023-03-06T05:24:08Z", "2022-08-05T08:16:59Z", "2022-08-05T08:17:53Z", "2022-08-05T08:19:06Z", "2022-08-05T08:19:51Z", "2022-08-05T08:21:12Z"]}, {"name": "secrets-store-csi-driver-provider-aws", "description": "The AWS provider for the Secrets Store CSI Driver allows you to fetch secrets from AWS Secrets Manager and AWS Systems Manager Parameter Store, and mount them into Kubernetes pods.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS Secrets Manager and Config Provider for Secret Store CSI Driver\n\n![badge](https://github.com/aws/secrets-store-csi-driver-provider-aws/actions/workflows/go.yml/badge.svg)\n[![codecov](https://codecov.io/gh/aws/secrets-store-csi-driver-provider-aws/branch/main/graph/badge.svg?token=S7ZDTT1F8K)](https://codecov.io/gh/aws/secrets-store-csi-driver-provider-aws)\n\nAWS offers two services to manage secrets and parameters conveniently in your code. AWS [Secrets Manager](https://aws.amazon.com/secrets-manager/) allows you to easily rotate, manage, and retrieve database credentials, API keys, certificates, and other secrets throughout their lifecycle. AWS [Systems Manager Parameter Store](https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html) provides hierarchical storage for configuration data. The AWS provider for the [Secrets Store CSI Driver](https://github.com/kubernetes-sigs/secrets-store-csi-driver) allows you to make secrets stored in Secrets Manager and parameters stored in Parameter Store appear as files mounted in Kubernetes pods.\n\n## Installation\n\n### Requirements\n* Amazon Elastic Kubernetes Service (EKS) 1.17+ running an EC2 node group (Fargate node groups are not supported **[^1]**)\n* [Secrets Store CSI driver installed](https://secrets-store-csi-driver.sigs.k8s.io/getting-started/installation.html):\n    ```shell\n    helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts\n    helm install -n kube-system csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver\n    ```\n  **Note** that older versions of the driver may require the ```--set grpcSupportedProviders=\"aws\"``` flag on the install step.\n* IAM Roles for Service Accounts ([IRSA](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html)) as described in the usage section below.\n\n[^1]: The CSI Secret Store driver runs as a DaemonSet, and as described in the [AWS documentation](https://docs.aws.amazon.com/eks/latest/userguide/fargate.html#fargate-considerations), DaemonSet is not supported on Fargate. \n\n### Installing the AWS Provider\nTo install the Secrets Manager and Config Provider use the YAML file in the deployment directory:\n```shell\nkubectl apply -f https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/deployment/aws-provider-installer.yaml\n```\n\n## Usage\n\nSet the region name and name of your cluster to use in the bash commands that follow:\n```bash\nREGION=<REGION>\nCLUSTERNAME=<CLUSTERNAME>\n```\nWhere **&lt;REGION&gt;** is the region in which your Kubernetes cluster is running and **&lt;CLUSTERNAME&gt;** is the name of your cluster.\n\nNow create a test secret:\n```shell\naws --region \"$REGION\" secretsmanager  create-secret --name MySecret --secret-string '{\"username\":\"memeuser\", \"password\":\"hunter2\"}'\n```\nCreate an access policy for the pod scoped down to just the secrets it should have and save the policy ARN in a shell variable:\n```shell\nPOLICY_ARN=$(aws --region \"$REGION\" --query Policy.Arn --output text iam create-policy --policy-name nginx-deployment-policy --policy-document '{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [ {\n        \"Effect\": \"Allow\",\n        \"Action\": [\"secretsmanager:GetSecretValue\", \"secretsmanager:DescribeSecret\"],\n        \"Resource\": [\"arn:*:secretsmanager:*:*:secret:MySecret-??????\"]\n    } ]\n}')\n```\n**Note**, when using SSM parameters the permission \"ssm:GetParameters\" is needed in the policy. To simplify this example we use wild card matches above but you could lock this down further using the full ARN from the output of create-secret above.\n\nCreate the IAM OIDC provider for the cluster if you have not already done so:\n```shell\neksctl utils associate-iam-oidc-provider --region=\"$REGION\" --cluster=\"$CLUSTERNAME\" --approve # Only run this once\n```\nNext create the service account to be used by the pod and associate the above IAM policy with that service account. For this example we use *nginx-deployment-sa* for the service account name:\n```shell\neksctl create iamserviceaccount --name nginx-deployment-sa --region=\"$REGION\" --cluster \"$CLUSTERNAME\" --attach-policy-arn \"$POLICY_ARN\" --approve --override-existing-serviceaccounts\n```\nNow create the SecretProviderClass which tells the AWS provider which secrets are to be mounted in the pod. The ExampleSecretProviderClass.yaml in the [examples](./examples) directory will mount \"MySecret\" created above:\n```shell\nkubectl apply -f https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/examples/ExampleSecretProviderClass.yaml\n```\nFinally we can deploy our pod. The ExampleDeployment.yaml in the examples directory contains a sample nginx deployment that mounts the secrets under /mnt/secrets-store in the pod:\n```shell\nkubectl apply -f https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/examples/ExampleDeployment.yaml\n```\n\nTo verify the secret has been mounted properly, See the example below:\n\n```shell\nkubectl exec -it $(kubectl get pods | awk '/nginx-deployment/{print $1}' | head -1) cat /mnt/secrets-store/MySecret; echo\n```\n### Troubleshooting\nMost errors can be viewed by describing the pod deployment. For the deployment, find the pod names using get pods (use -n **&lt;NAMESPACE&gt;** if you are not using the default namespace):\n```shell\nkubectl get pods\n```\nThen describe the pod (substitute the pod ID from above for **&lt;PODID&gt;**, as before use -n if you are not using the default namespace):\n```shell\nkubectl describe pod/<PODID>\n```\nAdditional information may be available in the provider logs:\n```shell\nkubectl -n kube-system get pods\nkubectl -n kube-system logs pod/<PODID>\n```\nWhere **&lt;PODID&gt;** in this case is the id of the *csi-secrets-store-provider-aws* pod.\n\n### SecretProviderClass options\nThe SecretProviderClass has the following format:\n```yaml\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: <NAME>\nspec:\n  provider: aws\n  parameters:\n```\nThe parameters section contains the details of the mount request and contain one of the three fields:\n* objects: This is a string containing a YAML declaration (described below) of the secrets to be mounted. This is most easily written using a YAML multi-line string or pipe character. For example:\n    ```yaml\n      parameters:\n        objects: |\n            - objectName: \"MySecret\"\n              objectType: \"secretsmanager\"\n    ```\n* region: An optional field to specify the AWS region to use when retrieving secrets from Secrets Manager or Parameter Store. If this field is missing, the provider will lookup the region from the annotation on the node. This lookup adds overhead to mount requests so clusters using large numbers of pods will benefit from providing the region here.\n* failoverRegion: An optional field to specify a secondary AWS region to use when retrieving secrets. See the Automated Failover Regions section in this readme for more information.\n* pathTranslation: An optional field to specify a substitution character to use when the path separator character (slash on Linux) is used in the file name. If a Secret or parameter name contains the path separator failures will occur when the provider tries to create a mounted file using the name. When not specified the underscore character is used, thus My/Path/Secret will be mounted as My_Path_Secret. This pathTranslation value can either be the string \"False\" or a single character string. When set to \"False\", no character substitution is performed.\n\nThe primary objects field of the SecretProviderClass can contain the following sub-fields:\n* objectName: This field is required. It specifies the name of the secret or parameter to be fetched. For Secrets Manager this is the [SecretId](https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html#API_GetSecretValue_RequestParameters) parameter and can be either the friendly name or full ARN of the secret. For SSM Parameter Store, this must be the [Name](https://docs.aws.amazon.com/systems-manager/latest/APIReference/API_GetParameter.html#API_GetParameter_RequestParameters) of the parameter and can not be a full ARN.\n* objectType: This field is optional when using a Secrets Manager ARN for objectName, otherwise it is required. This field can be either \"secretsmanager\" or \"ssmparameter\".\n* objectAlias: This optional field specifies the file name under which the secret will be mounted. When not specified the file name defaults to objectName.\n* objectVersion: This field is optional, and generally not recommended since updates to the secret require updating this field. For Secrets Manager this is the [VersionId](https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html#API_GetSecretValue_RequestParameters). For SSM Parameter Store, this is the optional [version number](https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-versions.html#reference-parameter-version).\n* objectVersionLabel: This optional field specifies the alias used for the version. Most applications should not use this field since the most recent version of the secret is used by default. For Secrets Manager this is the [VersionStage](https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html#API_GetSecretValue_RequestParameters). For SSM Parameter Store this is the optional [Parameter Label](https://docs.amazonaws.cn/en_us/systems-manager/latest/userguide/sysman-paramstore-labels.html).\n\n* failoverObject: An optional field when using the failoverRegion feature. See the Automated Failover Regions section in this readme for more information. The failover object can contain the following sub-fields:\n  * objectName: This field is required if failoverObject is present. Specifies the name of the secret or parameter to be fetched from the failover region. See the primary objectName field for more information.\n  * objectVersion: This field is optional and defines the objectVersion for the failover region.  If specified, it must match the primary region's objectVersion. See the primary objectVersion field for more information.\n  * objectVersionLabel: This optional field specifies the alias used for the version of the failoverObject. See the primary objectVersionLabel field for more information. \n\n* jmesPath: This optional field specifies the specific key-value pairs to extract from a JSON-formatted secret. You can use this field to mount key-value pairs from a properly formatted secret value as individual secrets. For example: Consider a secret \"MySecret\" with JSON content as follows:\n\n    ```shell\n        {\n            \"username\": \"testuser\"\n            \"password\": \"testpassword\"\n        }\n     ```\n  To mount the username and password key pairs of this secret as individual secrets, use the jmesPath field as follows:\n\n  ```yaml:\n        objects: |\n            - objectName: \"MySecret\"\n              objectType: \"secretsmanager\"\n              jmesPath:\n                  - path: \"username\"\n                    objectAlias: \"MySecretUsername\"\n                  - path: \"password\"\n                    objectAlias: \"MySecretPassword\"\n  ```\n  If either the 'path' or the 'objectAlias' fields contain a hyphen, then they must be escaped with a single quote:\n  \n  ```\n  - path: '\"hyphenated-path\"'\n    objectAlias: '\"hyphenated-alias\"'\n  ```\n  \n  If you use the jmesPath field,  you must provide the following two sub-fields:\n  * path: This required field is the [JMES path](https://jmespath.org/specification.html) to use for retrieval\n  * objectAlias: This required field specifies the file name under which the key-value pair secret will be mounted. \n\n## Additional Considerations\n\n### Rotation\nWhen using the optional alpha [rotation reconciler](https://secrets-store-csi-driver.sigs.k8s.io/topics/secret-auto-rotation.html) feature of the Secrets Store CSI driver the driver will periodically remount the secrets in the SecretProviderClass. This will cause additional API calls which results in additional charges. Applications should use a reasonable poll interval that works with their rotation strategy. A one hour poll interval is recommended as a default to reduce excessive API costs.\n\nAnyone wishing to test out the rotation reconciler feature can enable it using helm:\n```bash\nhelm upgrade -n kube-system csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver --set enableSecretRotation=true --set rotationPollInterval=3600s\n```\n\n### Automated Failover Regions\nIn order to provide availability during connectivity outages or for disaster recovery configurations, this provider supports an automated failover feature to fetch secrets or parameters from a secondary region. To define an automated failover region, define the failoverRegion in the SecretProviderClass.yaml file:\n```yaml\nspec:\n  provider: aws\n  parameters:\n    region: us-east-1\n    failoverRegion: us-east-2\n```\n\nWhen the failoverRegion is defined, the driver will attempt to get the secret value from both regions.\n* If both regions successfully retrieve the secret value, then the mount will contain the secret value of the secret in the primary region.\n* If one region returns a non-client error (code 5XX), and the other region succeeds, then the mount will contain the secret value of the non-failing region.\n* If either region returns a client error (code 4XX), then the mount will fail, and the cause of the error must be resolved before the mount will succeed.\n\n It is possible to use different secrets or parameters between the primary and failover regions.  This example will use different ARNs depending on which region it is pulling from:\n ```yaml\n- objectName: \"arn:aws:secretsmanager:us-east-1:123456789012:secret:PrimarySecret-12345\"\n  failoverObject: \n    objectName: \"arn:aws:secretsmanager:us-east-2:123456789012:secret:FailoverSecret-12345\" \n  objectAlias: testArn\n```\nIf 'failoverObject' is defined, then objectAlias is required.\n\n\n### Private Builds\nYou can pull down this git repository and build and install this plugin into your account's [AWS ECR](https://aws.amazon.com/ecr/) registry using the following steps. First clone the repository:\n```shell\ngit clone https://github.com/aws/secrets-store-csi-driver-provider-aws\ncd secrets-store-csi-driver-provider-aws\n```\nNext, set your region and repository name in bash shell variables to be used later:\n```bash\nexport REGION=<REGION>\nexport PRIVREPO=<ACCOUNT>.dkr.ecr.$REGION.amazonaws.com/secrets-store-csi-driver-provider-aws\n```\nWhere **&lt;REGION&gt;** is the AWS region in which your Kubernetes cluster is running, and **&lt;ACCOUNT&gt;** is your AWS account Id. Next create your ECR repository if you have not already done so:\n```bash\naws --region $REGION ecr create-repository --repository-name secrets-store-csi-driver-provider-aws # Only do this once\n```\nNow run make to build the plugin and push it into your account's repo:\n```\nmake\n```\nOnce the image is in your repo you can install it into your cluster from your repo rather than the public repo:\n```bash\nenvsubst < deployment/private-installer.yaml | kubectl apply -f -\n```\n\n### Security Considerations\n\nThe AWS Secrets Manager and Config Provider provides compatibility for legacy applications that access secrets as mounted files in the pod. Security conscious applications should use the native AWS APIs to fetch secrets and optionally cache them in memory rather than storing them in the file system.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": ["2024-01-31T22:00:32Z", "2023-11-15T21:52:15Z", "2023-07-13T19:15:43Z", "2023-06-12T15:50:09Z", "2023-03-21T23:24:48Z", "2023-03-06T18:07:55Z", "2023-03-03T21:55:31Z", "2022-12-30T21:16:39Z", "2022-09-24T00:39:00Z"]}, {"name": "selective-search-with-mutual-information-cotraining", "description": null, "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Mutual Information Co-training\n\nThis repository is the source code for the paper:\n\n**MICO: Selective Search with Mutual Information Co-training**\n\nIn Proceedings of the International Conference on Computational Linguistics (COLING) , 2022\n\n*Zhanyu Wang, Xiao Zhang, Hyokun Yun, Choon Hui Teo and Trishul Chilimb*\n\n## Introduction\nThis is the package of Mutual Information Co-training (MICO) for End2End topic sharding. MICO uses BERT to generate sentence representations, and performs query routing and document assignment with the representations. The document assignment module in MICO outputs almost equal-sized clusters, and the query routing module routes the queries to the cluster containing most (if not all) of its relevant documents. MICO achieves very high performance for topic sharding. \n\nThis package can be tested through the example usage below.\n\n## Usage\nYou can save the command below as a bash file and run it in the current folder. You can also find and run it in `./example/scripts/run_mico.sh`. It will take less than 5 minutes to finish running.\n\nThe results will be saved in `./results/`. In the folder `example_pair_BERT-finetune_layer-1_CLS_TOKEN_maxlen64_bs64_lr-bert5e-6_lr2e-4_warmup1000_entropy5_seed1` for this example experiment, we can see the final evaluation metrics saved in `metrics.json`. The document assigned to the clusters are saved in `clustered_docs.json` in a dictionary. The log files for training and evaluation are `*.log`. The model is saved as `*.pt`. The folder `./log` contains Tensorboard results for visualization. \n\nThe `dataset_name` in the training command is set as `example` since we have an example dataset saved in `../example/data/example_dataset/`. You can change the `train_folder_path` and `test_folder_path` according to your needs.\n\nDuring training, the `batch_size` is for each GPU card. If the current choice of `batch_size` is good on a machine with one GPU, we do not need to change it when switching to machines with more than one GPU (each with the same GPU memory). This is because we use the `DistributedDataParallel` function in `PyTorch` to support multi-GPU training: we assign one sub-process for each GPU and it maintains its own dataloader and counts its own epoch number (hence people usually focus on the iteration number instead of the epoch number). For a 4-GPU machine, finishing one epoch for each process means training the model for 4 epochs in total. For a GPU with 16GB memory, setting `batch_size=64` is good for the first try.\n\nDuring testing, we use `DataParallel` in `PyTorch` for better efficiency (we only go through the dataset once with multi-GPU, much less than using `DistributedDataParallel`), and the `batch_size` is across all GPUs. Usually for testing, you can set a much larger `batch_size` than the one used in training, e.g., for four GPUs (each with 16GB memory), we can use `batch_size=2048`. You can also test the trained model directly by setting `--eval_only`.\n\n    #!/bin/bash\n\n    dataset_name=example\n    train_folder_path=./example/data/${dataset_name}_train_csv/\n    test_folder_path=./example/data/${dataset_name}_test_csv/\n\n    batch_size=64\n    selected_layer_idx=-1\n    pooling_strategy=CLS_TOKEN\n    max_length=64\n    lr=2e-4\n    lr_bert=5e-6\n    entropy_weight=5\n    num_warmup_steps=1000\n    seed=1\n\n    model_path=./example/results/${dataset_name}_pair_BERT-finetune_layer${selected_layer_idx}\\\n    _${pooling_strategy}\\\n    _maxlen${max_length}\\\n    _bs${batch_size}\\\n    _lr-bert${lr_bert}\\\n    _lr${lr}\\\n    _warmup${num_warmup_steps}\\\n    _entropy${entropy_weight}\\\n    _seed${seed}/\n\n    python -u ./main.py \\\n        --model_path=${model_path} \\\n        --train_folder_path=${train_folder_path} \\\n        --test_folder_path=${test_folder_path} \\\n        --dim_input=768 \\\n        --number_clusters=64 \\\n        --dim_hidden=8 \\\n        --num_layers_posterior=0 \\\n        --batch_size=${batch_size} \\\n        --lr=${lr} \\\n        --num_warmup_steps=${num_warmup_steps} \\\n        --lr_prior=0.1 \\\n        --num_steps_prior=1 \\\n        --init=0.0 \\\n        --clip=1.0 \\\n        --epochs=1 \\\n        --log_interval=10 \\\n        --check_val_test_interval=10000 \\\n        --save_per_num_epoch=100 \\\n        --num_bad_epochs=10 \\\n        --seed=${seed} \\\n        --entropy_weight=${entropy_weight} \\\n        --num_workers=0 \\\n        --cuda \\\n        --lr_bert=${lr_bert} \\\n        --max_length=${max_length} \\\n        --pooling_strategy=${pooling_strategy} \\\n        --selected_layer_idx=${selected_layer_idx} \n\n\n\n## Visualize results with Tensorboard\nTo visualize the curves of the metrics calculated during training and evaluation, please use Tensorboard (for `Pytorch` we use `TensorboardX` which is installed in the setting up section.) \n\nThe results for each experiment is saved in the folder specified by `--model_path` in the bash commands. We also have log files in text format in that folder. After running the following command, you can open your browser and type `localhost:14095` to view the training results.\n\n    # start tensorboard\n    tensorboard --logdir=./results/ --port=14095 serve\n\n## Memory profiling\nAlthough we have adopted several techniques to decrease the memory usage, it is still possible that one encounters memory problem when running with large scale dataset. You can try this memory profiling method to estimate how much memory you will need for running MICO. \n\nSome tips: \n1. Setting `num_worker=0` is a good way to save memory and it almost does not affect the training speed. \n2. Running MICO on more GPUs will create more sub-process automatically, and each sub-process may consume much memory. Therefore, the memory usage increases linearly with the GPU number. If needed, you can set `export CUDA_VISIBLE_DEVICES=0` to only use 1 GPU in training to save memory.\n\nTo use the memory profiling method below, please make sure that the python package `memory_profiler` is installed. (If not, you can install it with `pip install memory_profiler`.) It can track the memory usage of the Python codes. For more details, please see https://pypi.org/project/memory-profiler/.\n\nTo use it to track the memory usage, you can try the command below.\n\n    mprof run --interval=10 --multiprocess --include-children './your_bash_file.sh'\n\nDuring the bash file running, you can plot the memory usage over time by the command below. Please replace `mprofile_***.dat` with the name of the profile results you want to plot (the lastest `dat` file will be used if the file is not specified). The figure will be saved as `memory_profile_result.png`.\n\n    mprof plot -o memory_profile_result.png --backend agg mprofile_***.dat\n\n## Setting up a new EC2 machine\nFor setting up a new EC2 machine to run the scripts, please use the codes below\n\n    wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh\n    bash ./Anaconda3-2021.05-Linux-x86_64.sh\n    source ~/.bashrc  \n    conda install pytorch=1.7.1 cudatoolkit=9.2 -c pytorch\n    pip install -r requirements.txt\n    pip install memory_profiler\n\nAfter download the data, you can replace the two folders (for training and testing data) in `./example/data/` by the two large scale datasets. Then, you can modify and run the script `./example/scripts/run_mico.sh`.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": []}, {"name": "serverless-application-model", "description": "The AWS Serverless Application Model (AWS SAM) transform is a AWS CloudFormation macro that transforms SAM templates into CloudFormation templates.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# AWS SAM transform\n\n[![Tests](https://github.com/aws/serverless-application-model/actions/workflows/build.yml/badge.svg)](https://github.com/aws/serverless-application-model/actions/workflows/build.yml)\n[![Update schema](https://github.com/aws/serverless-application-model/actions/workflows/schema.yml/badge.svg)](https://github.com/aws/serverless-application-model/actions/workflows/schema.yml)\n[![PyPI](https://img.shields.io/pypi/v/aws-sam-translator?label=PyPI)](https://pypi.org/project/aws-sam-translator/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/aws-sam-translator?label=Python)](https://pypi.org/project/aws-sam-translator/)\n[![Contribute with Gitpod](https://img.shields.io/badge/Contribute%20with-Gitpod-908a85?logo=gitpod)](https://gitpod.io/#https://github.com/aws/serverless-application-model.git)\n\nThe [AWS Serverless Application Model](https://aws.amazon.com/serverless/sam/) (AWS SAM) transform is a [AWS CloudFormation macro](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-macros.html) that transforms [SAM templates](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html) into [CloudFormation templates](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html).\n\nTo use the SAM transform, add `AWS::Serverless-2016-10-31` to the [`Transform` section](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-section-structure.html) of your CloudFormation template.\n\nBenefits of using the SAM transform include:\n\n- Built-in best practices and sane defaults.\n- Local testing and debugging with the [AWS SAM CLI](https://github.com/aws/aws-sam-cli).\n- Extension of the CloudFormation template syntax.\n\n## Getting started\n\nSave the following as `template.yaml`:\n\n```yaml\nTransform: AWS::Serverless-2016-10-31\nResources:\n  MyFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Runtime: nodejs18.x\n      Handler: index.handler\n      InlineCode: |\n        exports.handler = async (event) => {\n          console.log(event);\n        }\n```\n\nAnd deploy it with the [SAM CLI](https://github.com/aws/aws-sam-cli):\n\n```bash\nsam sync --stack-name sam-app\n```\n\nThe [`AWS::Serverless::Function`](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-resource-function.html) resource will create a [AWS Lambda](https://aws.amazon.com/lambda/) function that logs [events](https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-concepts.html#gettingstarted-concepts-event) it receives.\n\nUnder the hood, the template is transformed into the JSON equivalent of the following CloudFormation template:\n\n```yaml\nResources:\n  MyFunction:\n    Type: AWS::Lambda::Function\n    Properties:\n      Code:\n        ZipFile: |\n          exports.handler = async (event) => {\n            console.log(event);\n          }\n      Handler: index.handler\n      Role: !GetAtt MyFunctionRole.Arn\n      Runtime: nodejs18.x\n      Tags:\n        - Key: lambda:createdBy\n          Value: SAM\n  MyFunctionRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Action:\n              - sts:AssumeRole\n            Effect: Allow\n            Principal:\n              Service:\n                - lambda.amazonaws.com\n      ManagedPolicyArns:\n        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n      Tags:\n        - Key: lambda:createdBy\n          Value: SAM\n```\n\nFor a more thorough introduction, see the [this tutorial](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-getting-started-hello-world.html) in the [Developer Guide](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html).\n\n## Contributing\n\n### Setting up development environment\n\nYou'll need to have Python 3.8+ installed.\n\nCreate a [virtual environment](https://docs.python.org/3/library/venv.html):\n\n```bash\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\nSet up dependencies:\n\n```bash\nmake init\n```\n\nRun tests:\n\n```bash\nmake pr\n ```\n \nSee [`DEVELOPMENT_GUIDE.md`](DEVELOPMENT_GUIDE.md) for further development instructions, and [`CONTRIBUTING.md`](CONTRIBUTING.md) for the contributing guidelines.\n\n## Getting help\n\nThe best way to interact with the team is through GitHub. You can either [create an issue](https://github.com/aws/serverless-application-model/issues/new/choose) or [start a discussion](https://github.com/aws/serverless-application-model/discussions).\n\nYou can also join the [`#samdev` channel](https://join.slack.com/t/awsdevelopers/shared_invite/zt-yryddays-C9fkWrmguDv0h2EEDzCqvw) on Slack.\n\n## Learn more\n\n### Workshops and tutorials\n\n- [The Complete AWS SAM Workshop](https://catalog.workshops.aws/complete-aws-sam)\n- [AWS Serverless Developer Experience Workshop](https://catalog.us-east-1.prod.workshops.aws/workshops/9a27e484-7336-4ed0-8f90-f2747e4ac65c/en-US)\n- [Deploying a \"Hello, World!\" application](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-getting-started-hello-world.html)\n- [Testing in the cloud using the SAM CLI](https://aws.amazon.com/blogs/compute/accelerating-serverless-development-with-aws-sam-accelerate/)\n\n### Documentation\n\n- [SAM Developer Guide](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html)\n- [SAM template specification](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification.html)\n- [SAM connectors](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/managing-permissions-connectors.html)\n- [SAM policy templates](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-policy-templates.html)\n", "release_dates": ["2024-02-15T18:57:31Z", "2024-02-03T00:34:08Z", "2024-01-05T00:48:58Z", "2023-12-05T23:00:15Z", "2023-11-23T00:51:36Z", "2023-11-16T17:32:12Z", "2023-10-23T23:02:08Z", "2023-10-13T20:56:25Z", "2023-10-06T20:50:58Z", "2023-09-28T21:08:45Z", "2023-09-19T20:53:47Z", "2023-08-31T22:46:32Z", "2023-08-10T00:02:13Z", "2023-07-29T00:59:44Z", "2023-07-05T21:28:34Z", "2023-06-22T18:37:23Z", "2023-06-15T18:31:37Z", "2023-05-25T18:59:18Z", "2023-05-16T19:47:44Z", "2023-04-27T19:59:45Z", "2023-04-19T23:10:08Z", "2023-04-06T15:54:41Z", "2023-03-27T22:25:05Z", "2023-03-14T22:35:59Z", "2023-03-10T20:13:47Z", "2023-02-28T21:48:18Z", "2023-02-27T19:17:32Z", "2023-02-08T21:36:34Z", "2023-01-24T20:08:36Z", "2023-01-23T23:35:06Z"]}, {"name": "serverless-java-container", "description": "A Java wrapper to run Spring, Spring Boot, Jersey, and other apps inside AWS Lambda.", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Serverless Java container [![Build Status](https://github.com/aws/serverless-java-container/workflows/Continuous%20Integration/badge.svg)](https://github.com/aws/serverless-java-container/actions) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.amazonaws.serverless/aws-serverless-java-container/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.amazonaws.serverless/aws-serverless-java-container) [![Help](http://img.shields.io/badge/help-gitter-E91E63.svg?style=flat-square)](https://gitter.im/aws/serverless-java-container)\nThe `aws-serverless-java-container` makes it easy to run Java applications written with frameworks such as [Spring](https://spring.io/), [Spring Boot](https://projects.spring.io/spring-boot/), [Apache Struts](http://struts.apache.org/), [Jersey](https://jersey.java.net/), or [Spark](http://sparkjava.com/) in [AWS Lambda](https://aws.amazon.com/lambda/).\n\nServerless Java Container natively supports API Gateway's proxy integration models for requests and responses, you can create and inject custom models for methods that use custom mappings.\n\nCurrently the following versions are maintained:\n\n| Version                  | Branch | Java Enterprise support | Spring versions | JAX-RS/ Jersey version | Struts support | Spark support |\n|--------------------------|--------|-------------------------|-----------------|------------------------|----------------|---------------|\n| 1.x (stable)             | [1.x](https://github.com/aws/serverless-java-container/tree/1.x)    | Java EE (javax.*)       | 5.x (Boot 2.x)  | 2.x                    | :white_check_mark: | :white_check_mark: |\n| 2.x (under development)  | [main](https://github.com/aws/serverless-java-container/tree/main)   | Jakarta EE (jakarta.*)  | 6.x (Boot 3.x)  | 3.x                    | :x:            | :x:           |\n\nFollow the quick start guides in [our wiki](https://github.com/aws/serverless-java-container/wiki) to integrate Serverless Java Container with your project:\n* [Spring quick start](https://github.com/aws/serverless-java-container/wiki/Quick-start---Spring)\n* [Spring Boot 2 quick start](https://github.com/aws/serverless-java-container/wiki/Quick-start---Spring-Boot2)\n* [Spring Boot 3 quick start](https://github.com/aws/serverless-java-container/wiki/Quick-start---Spring-Boot3)\n* [Apache Struts quick start](https://github.com/aws/serverless-java-container/wiki/Quick-start---Struts)\n* [Jersey quick start](https://github.com/aws/serverless-java-container/wiki/Quick-start---Jersey)\n* [Spark quick start](https://github.com/aws/serverless-java-container/wiki/Quick-start---Spark)\n\nBelow is the most basic AWS Lambda handler example that launches a Spring application. You can also take a look at the [samples](https://github.com/aws/serverless-java-container/tree/master/samples) in this repository, our main wiki page includes a [step-by-step guide](https://github.com/aws/serverless-java-container/wiki#deploying-the-sample-applications) on how to deploy the various sample applications using Maven and [SAM](https://github.com/awslabs/serverless-application-model).\n\n```java\npublic class StreamLambdaHandler implements RequestStreamHandler {\n    private static final SpringLambdaContainerHandler<AwsProxyRequest, AwsProxyResponse> handler;\n\n    static {\n        try {\n            handler = SpringLambdaContainerHandler.getAwsProxyHandler(PetStoreSpringAppConfig.class);\n        } catch (ContainerInitializationException e) {\n            // if we fail here. We re-throw the exception to force another cold start\n            e.printStackTrace();\n            throw new RuntimeException(\"Could not initialize Spring framework\", e);\n        }\n    }\n\n    @Override\n    public void handleRequest(InputStream inputStream, OutputStream outputStream, Context context)\n            throws IOException {\n        handler.proxyStream(inputStream, outputStream, context);\n    }\n}\n``` \n\n## Public Examples\n\n### Blogs\n\n- [Re-platforming Java applications using the updated AWS Serverless Java Container](https://aws.amazon.com/blogs/compute/re-platforming-java-applications-using-the-updated-aws-serverless-java-container/)\n\n### Workshops\n\n- [Java on AWS Lambda](https://catalog.workshops.aws/java-on-aws-lambda) From Serverful to Serverless Java with AWS Lambda in 2 hours\n\n### Videos\n\n- [Spring on AWS Lambda](https://www.youtube.com/watch?v=A1rYiHTy9Lg&list=PLCOG9xkUD90IDm9tcY-5nMK6X6g8SD-Sz) YouTube Playlist from [@plantpowerjames](https://twitter.com/plantpowerjames)\n\n### Java samples with different frameworks \n\n- [Dagger, Micronaut, Quarkus, Spring Boot](https://github.com/aws-samples/serverless-java-frameworks-samples/)", "release_dates": ["2024-02-06T07:29:44Z", "2024-01-21T18:35:48Z", "2023-07-20T12:42:22Z", "2023-05-19T15:48:43Z", "2023-04-24T08:30:09Z", "2023-03-25T07:48:05Z", "2022-12-14T09:32:48Z", "2022-09-21T19:26:57Z", "2022-06-10T13:16:45Z", "2022-04-27T20:23:03Z", "2022-03-30T09:07:15Z", "2022-02-16T11:33:22Z", "2021-12-17T13:21:32Z", "2021-07-28T05:28:55Z", "2020-10-06T22:27:24Z", "2020-07-15T20:02:15Z", "2020-04-08T22:51:58Z", "2019-09-30T15:28:38Z", "2019-06-27T22:23:13Z", "2019-02-26T01:09:41Z", "2018-12-26T21:38:09Z", "2018-10-24T20:52:56Z", "2018-08-02T19:45:32Z", "2018-06-22T19:08:22Z", "2018-06-21T17:29:35Z", "2018-06-20T22:02:19Z", "2018-04-06T19:21:59Z", "2018-03-15T21:05:41Z", "2018-02-28T00:27:08Z", "2018-01-31T23:14:16Z"]}, {"name": "session-manager-plugin", "description": "This plugin helps you to use the AWS Command Line Interface (AWS CLI) to start and end sessions to your managed instances", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "\n# Session Manager Plugin\n\nThis plugin helps you to use the AWS Command Line Interface (AWS CLI) to start and end sessions to your managed instances. Session Manager is a capability of AWS Systems Manager.\n\n## Overview\n\nSession Manager is a fully managed AWS Systems Manager capability that lets you manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, on-premises instances and virtual machines. Session Manager provides secure and auditable instance management without the need to open inbound ports. When you use the Session Manager plugin with the AWS CLI to start a session, the plugin builds the websocket connection to your managed instances.\n\n### Prerequisites\n\nBefore using Session Manager, make sure your environment meets the following requirements. [Complete Session Manager prerequisites](http://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-prerequisites.html).\n\n### Starting a session\n\nFor information about starting a session using the AWS CLI, see [Starting a session (AWS CLI)](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-sessions-start.html#sessions-start-cli).\n\n### Troubleshooting\n\nFor information about troubleshooting, see [Troubleshooting Session Manager](http://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-troubleshooting.html).\n\n\n### Working with Docker\n\nTo build the Session Manager plugin in a `Docker` container, complete the following steps:\n\n1. Install [`docker`](https://docs.docker.com/engine/install/centos/)\n\n2. Build the `docker` image\n```\ndocker build -t session-manager-plugin-image .\n```\n3. Build the plugin\n```\ndocker run -it --rm --name session-manager-plugin -v `pwd`:/session-manager-plugin session-manager-plugin-image make release\n```\n\n### Working with Linux\n\nTo build the binaries required to install the Session Manager plugin, complete the following steps.\n\n1. Install `golang`\n\n2. Install `rpm-build` and `rpmdevtools`\n\n3. Install `gcc 8.3+` and `glibc 2.27+`\n\n4. Run `make release` to build the plugin for Linux, Debian, macOS and Windows.\n\n5. Change to the directory of your local machine's operating system architecture and open the `session-manager-plugin` directory. Then follow the installation procedure that applies to your local machine. For more information, see [Install the Session Manager plugin for the AWS CLI](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html). If the machine you're building the plugin on differs from the machine you plan to install the plugin on you will need to copy the `session-manager-plugin` binary to the appropriate directory for that operating system.\n\n```\nLinux - /usr/local/sessionmanagerplugin/bin/session-manager-plugin\n\nmacOS - /usr/local/sessionmanagerplugin/bin/session-manager-plugin\n\nWindows - C:\\Program Files\\Amazon\\SessionManagerPlugin\\bin\\session-manager-plugin.exe\n```\n\nThe `ssmcli` binary is available for some operating systems for testing purposes only. The following is an example command using this binary.\n\n```\n./ssmcli start-session --instance-id i-1234567890abcdef0 --region us-east-2\n```\n\n### Directory structure\n\nSource code\n\n* `sessionmanagerplugin/session` contains the source code for core functionalities\n* `communicator/` contains the source code for websocket related operations\n* `vendor/src` contains the vendor package source code\n* `packaging/` contains rpm and dpkg artifacts\n* `Tools/src` contains build scripts\n\n## Feedback\n\nThank you for helping us to improve the Session Manager plugin. Please send your questions or comments to the [Systems Manager Forum](https://forums.aws.amazon.com/forum.jspa?forumID=185&start=0)\n\n## License\n\nThe session-manager-plugin is licensed under the Apache 2.0 License.\n", "release_dates": ["2024-01-10T21:40:00Z", "2023-12-05T00:33:53Z", "2023-08-08T21:21:30Z", "2023-03-16T17:27:24Z", "2022-10-13T23:48:30Z", "2022-06-17T21:14:27Z", "2022-05-27T21:10:40Z", "2022-05-19T23:12:02Z", "2022-03-31T20:03:53Z", "2022-01-11T22:04:13Z", "2021-10-28T21:28:43Z", "2021-08-19T19:31:35Z", "2021-08-12T19:42:42Z", "2021-06-10T01:23:17Z"]}, {"name": "shim-loggers-for-containerd", "description": "Shim logger repository for streaming container logs when using Containerd", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Shim loggers for containerd\n\n[![Go Report Card](https://goreportcard.com/badge/github.com/aws/shim-loggers-for-containerd)](https://goreportcard.com/report/github.com/aws/shim-loggers-for-containerd)\n\nShim loggers for containerd is a collection of [containerd](https://github.com/containerd/containerd) compatible logger\nimplementations that send container logs to various destinations. The following destinations are currently supported:\n\n* [Amazon CloudWatch Logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html)\n* [Splunk](https://www.splunk.com/en_us/central-log-management.html)\n* [Fluentd](http://www.fluentd.org/)\n\n## Build\n\nMake sure you have [golang](https://golang.org) installed. Then simply run `make build` to build the respective binaries. You might need to execute `make get-deps` to install some of the dependencies.\n\n## Usage\n\nContainerd supports shim plugins that redirect container output to a custom binary on Linux using STDIO URIs with\n[runc v2 runtime](https://github.com/containerd/containerd/tree/release/1.3/runtime/v2). These loggers can be used\neither programmatically or with the [ctr](https://github.com/projectatomic/containerd/blob/master/docs/cli.md) tool.\n\n### When using the `NewTask` API\n\nWhen using the [`NewTask`](https://github.com/containerd/containerd/blob/release/1.3/container.go#L208) API\nto start a container, simply provide the path to the built binary file `shim-loggers-for-containerd` and required\narguments. Note it's a good practice to clean up container resources with\n[`Delete`](https://github.com/containerd/containerd/blob/release/1.3/task.go#L287) API call after container exited\nas the container IO pipes are not closed if the shim process is still running.\n\nExample:\n\n```go\nNewTask(context, cio.BinaryIO(\"/usr/bin/shim-loggers-for-containerd\", args))\n```\n\n### When using the `ctr` tool\n\nWhen using [ctr](https://github.com/projectatomic/containerd/blob/master/docs/cli.md) tool to run\na container, provide the URI path to the binary file `shim-loggers-for-containerd` and required arguments as part of\nthe path.\n\nExample:\n\n```bash\nctr run \\\n    --runtime io.containerd.runc.v2 \\\n    --log-uri \"binary:///usr/bin/shim-loggers-for-containerd?--log-driver=awslogs&--arg1=value1&-args2=value2\" \\\n    docker.io/library/redis:alpine \\\n    redis\n```\n\n## Arguments\n\n### Common arguments\n\nThe following list of arguments apply to all of the shim logger binaries in this repo:\n\n|Name|Required|Description|\n|-|-|-|\n| log-driver | Yes | The name of the shim logger. Can be any of `awslogs`, `splunk` or `fluentd`. |\n| container-id | Yes | The container id |\n| container-name | Yes | The name of the container |\n| mode | No | Either `blocking` or `non-blocking`. In the `non-blocking` mode, log events are buffered and the application continues to execute even if these logs can't be drained or sent to the destination. Logs could also be lost when the buffer is full. |\n| max-buffer-size | No | Only supported in `non-blocking` mode. Set to `1m` (1MiB) by default. Example values: `200`, `4k`, `1m` etc. |\n| uid | No | Set a custom uid for the shim logger process. `0` is not supported. |\n| gid | No | Set a custom gid for the shim logger process. `0` is not supported. |\n| cleanup-time | No | Set a custom time for the shim logger process clean up itself. Set to `5s` (5 seconds) by default. Note the maximum supported value is 12 seconds, since containerd shim sets shim logger cleanup timeout value as 12 seconds. See [reference](https://github.com/containerd/containerd/commit/0dc7c8595627e38ca2b83d17a062b51f384c2025). |\n| container-image-id | No | The container image id. This is part of the docker config variables that can be logged by splunk log driver. |\n| container-image-name | No | The container image name. This is part of the docker config variables that can be logged by splunk log driver. |\n| container-env | No | The container environment variables map in json format. This is part of the docker config variables that can be logged by splunk log driver. |\n| container-labels | No | The container labels map in json format. This is part of the docker config variables that can be logged by splunk log driver. |\n\n### Windows specific arguments\n\nThe following list of arguments apply to Windows shim logger binaries in this repo:\n\n|Name|Required|Description|\n|-|-|-|\n| log-file-dir | No | Only supported in Windows. Will be the path where shim logger's log files are written. By default it is `\\ProgramData\\Amazon\\ECS\\log\\shim-logger`\n| proxy-variable | No | Only supported in Windows. The proxy variable will set the `HTTP_PROXY` and `HTTPS_PROXY` environment variables.\n\n### Additional log driver options\n\n#### Amazon CloudWatch Logs\n\nThe following additional arguments are supported for the `awslogs` shim logger binary, which can be used to send container logs to [Amazon CloudWatch Logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html).\n\n| Name                         | Required | Description                                                                                                                                                                                                                                     |\n|------------------------------|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| awslogs-group                | Yes      | The [log group](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CloudWatchLogsConcepts.html) in which the log stream for the container will be created.                                                                                |\n| awslogs-stream               | Yes      | The [log stream name](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CloudWatchLogsConcepts.html) to stream container logs to.                                                                                                        |\n| awslogs-region               | Yes      | The region name in which the log group and log stream needs to be created in.                                                                                                                                                                   |\n| awslogs-credentials-endpoint | Yes      | The endpoint from which credentials are retrieved from to connect to Amazon CloudWatch Logs.                                                                                                                                                    |\n| awslogs-create-group         | No       | Set to `false` by default. If the provided log group name does not exist and this value is set to `false`, the binary will directly exit with an error                                                                                          |\n| awslogs-create-stream        | No       | Set to `true` by default. The log stream will always be created unless this value specified to `false` explicitly. If the value is `false` and the log stream does not exist, logging will fail silently instead of failing the container task. |\n| awslogs-multiline-pattern    | No       | Matches the behavior of the [`awslogs` Docker log driver](https://docs.docker.com/config/containers/logging/awslogs/#amazon-cloudwatch-logs-options#awslogs-multiline-pattern).                                                                 |\n| awslogs-datetime-format      | No       | Matches the behavior of the [`awslogs` Docker log driver](https://docs.docker.com/config/containers/logging/awslogs/#amazon-cloudwatch-logs-options#awslogs-datetime-format)                                                                    |\n| awslogs-endpoint             | No       | Matches the behavior of the [`awslogs` Docker log driver](https://docs.docker.com/config/containers/logging/awslogs/#awslogs-endpoint)                                                                                                          |\n\n#### Splunk\n\nThe following additional arguments are supported for the `splunk` shim logger binary, which can be used to send container logs to [splunk](https://www.splunk.com/en_us/central-log-management.html).\nYou can find a description of what these parameters are used for [here](https://docs.docker.com/config/containers/logging/splunk/).\n\n|Name|Required|\n|-|-|\n| splunk-token | Yes |\n| splunk-url | Yes |\n| splunk-source | No |\n| splunk-sourcetype | No |\n| splunk-index | No |\n| splunk-capath | No |\n| splunk-caname | No |\n| splunk-insecureskipverify | No |\n| splunk-format | No |\n| splunk-verify-connection | No |\n| splunk-gzip | No |\n| splunk-gzip-level | No |\n| splunk-tag | No |\n| labels | No |\n| env | No |\n| env-regex | No |\n\n#### Fluentd\n\nThe following additional arguments are supported for the `fluentd` shim logger binary, which can be used to send container logs to  [Fluentd](https://www.fluentd.org). Note that all of these are optional arguments.\n\n| Name                         | Required | Description                                                                                                                                                   |\n|------------------------------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| fluentd-address              | No       | The address of the Fluentd server to connect to. By default, the `localhost:24224` address is used.                                                           |\n| fluentd-async-connect        | No       | Specifies if the logger connects to Fluentd in background. Defaults to `false`.                                                                               |\n| fluentd-sub-second-precision | No       | Generates logs in nanoseconds. Defaults to `true`. Note that this is in contrast to the default behaviour of fluentd log driver where it defaults to `false`. |\n| fluentd-buffer-limit         | No       | Sets the number of events buffered on the memory in bytes. Defaults to `1048576` (1MB).                                                                       |\n| fluentd-tag                  | No       | Specifies the tag used for log messages. Defaults to the first 12 characters of container ID.                                                                 |\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n", "release_dates": []}, {"name": "SigV4-for-AWS-IoT-embedded-sdk", "description": "AWS library to sign AWS HTTP requests with Signature Version 4 Signing Process.", "language": "C", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# AWS SigV4 Library\n\n**[API Documentation Pages for current and previous releases of this library can be found here](https://aws.github.io/SigV4-for-AWS-IoT-embedded-sdk/)**\n\nThe AWS SigV4 Library is a standalone library for generating authorization\nheaders and signatures according to the specifications of the\n[Signature Version 4](https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html)\nsigning process. Authorization headers are required for authentication when\nsending HTTP requests to AWS. This library can optionally be used by\napplications sending direct HTTP requests to AWS services requiring SigV4\nauthentication. This library has no dependencies on any additional libraries\nother than the standard C library. This library is distributed under the MIT\nOpen Source License.\n\nThis library has gone through code quality checks including verification that no\nfunction has a GNU Complexity score over 8, and checks against deviations from\nmandatory rules in the MISRA coding standard. Deviations from the MISRA C:2012\nguidelines are documented under MISRA Deviations. This library has also\nundergone static code analysis using Coverity static analysis, and validation of\nmemory safety through the CBMC automated reasoning tool.\n\nSee memory requirements for this library [here][memory_table].\n\n[memory_table]: ./docs/doxygen/include/size_table.md\n\n**AWS SigV4 v1.2.0\n[source code](https://github.com/aws/Sigv4-for-AWS-IoT-embedded-sdk/tree/v1.2.0/source)\nis part of the\n[FreeRTOS 202210.00 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202210.00-LTS)\nrelease.**\n\n## AWS SigV4 Library Config File\n\nThe AWS SigV4 library exposes build configuration macros that are required for\nbuilding the library. A list of all the configurations and their default values\nare defined in [sigv4_config_defaults.h][default_config]. To provide custom\nvalues for the configuration macros, a config file named `sigv4_config.h` can be\nprovided by the application to the library.\n\n[default_config]: source/include/sigv4_config_defaults.h\n\nBy default, a `sigv4_config.h` config file is required to build the library. To\ndisable this requirement and build the library with default configuration\nvalues, provide `SIGV4_DO_NOT_USE_CUSTOM_CONFIG` as a compile time preprocessor\nmacro.\n\n**Thus, the SigV4 library can be built by either**:\n\n- Defining a `sigv4_config.h` file in the application, and adding it to the\n  include directories list of the library.\n\n**OR**\n\n- Defining the `SIGV4_DO_NOT_USE_CUSTOM_CONFIG` preprocessor macro for the\n  library build.\n\n## Building the SigV4 Library\n\nThe [sigv4FilePaths.cmake](sigv4FilePaths.cmake) file contains information of\nall the source files and header include paths required to build the SigV4\nlibrary.\n\nAs mentioned in the previous section, either a custom config file (i.e.\n`sigv4_config.h`) or `SIGV4_DO_NOT_USE_CUSTOM_CONFIG` macro needs to be provided\nto build the SigV4 library.\n\nTo use CMake, please refer to the\n[sigV4FilePaths.cmake](https://github.com/aws/SigV4-for-AWS-IoT-embedded-sdk/blob/main/sigv4FilePaths.cmake)\nfile, which contains the relevant information regarding source files and header\ninclude paths required to build this library.\n\n## Building Unit Tests\n\n### Platform Prerequisites\n\n- For running unit tests:\n  - **C90 compiler** like gcc.\n  - **CMake 3.13.0 or later**.\n  - **Ruby 2.0.0 or later** is additionally required for the CMock test\n    framework (that we use).\n- For running the coverage target, **gcov** and **lcov** are additionally\n  required.\n\n### Steps to build **Unit Tests**\n\n1. Go to the root directory of this repository.\n\n1. Run the _cmake_ command: `cmake -S test -B build -DBUILD_UNIT_TESTS=ON`.\n\n1. Run this command to build the library and unit tests: `make -C build all`.\n\n1. The generated test executables will be present in `build/bin/tests` folder.\n\n1. Run `cd build && ctest` to execute all tests and view the test run summary.\n\n## CBMC\n\nTo learn more about CBMC and proofs specifically, review the training material\n[here](https://model-checking.github.io/cbmc-training).\n\nThe `test/cbmc/proofs` directory contains CBMC proofs.\n\nIn order to run these proofs you will need to install CBMC and other tools by\nfollowing the instructions\n[here](https://model-checking.github.io/cbmc-training/installation.html).\n\n## Reference examples\n\nThe AWS IoT Embedded C-SDK repository contains\n[HTTP demos](https://github.com/aws/aws-iot-device-sdk-embedded-C/tree/main/demos/http)\nshowing the use of the AWS SigV4 Library on a POSIX platform to authenticate\nHTTP requests to AWS S3 service.\n\n## Generating documentation\n\nThe Doxygen references found in this repository were created using Doxygen\nversion 1.9.2. To generate these Doxygen pages, please run the following command\nfrom the root of this repository:\n\n```shell\ndoxygen docs/doxygen/config.doxyfile\n```\n\n## Contributing\n\nSee [CONTRIBUTING.md](.github/CONTRIBUTING.md) for information on contributing.\n", "release_dates": ["2022-10-14T20:16:34Z", "2021-12-15T20:24:31Z", "2021-08-16T23:11:46Z"]}, {"name": "smithy-go", "description": "Smithy code generators for Go (in development)", "language": "Java", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Smithy Go\n\n[![Go Build Status](https://github.com/aws/smithy-go/actions/workflows/go.yml/badge.svg?branch=main)](https://github.com/aws/smithy-go/actions/workflows/go.yml)[![Codegen Build Status](https://github.com/aws/smithy-go/actions/workflows/codegen.yml/badge.svg?branch=main)](https://github.com/aws/smithy-go/actions/workflows/codegen.yml)\n\n[Smithy](https://smithy.io/) code generators for Go.\n\n**WARNING: All interfaces are subject to change.**\n\n## Can I use this?\n\nIn order to generate a usable smithy client you must provide a [protocol definition](https://github.com/aws/smithy-go/blob/main/codegen/smithy-go-codegen/src/main/java/software/amazon/smithy/go/codegen/integration/ProtocolGenerator.java),\nsuch as [AWS restJson1](https://smithy.io/2.0/aws/protocols/aws-restjson1-protocol.html),\nin order to generate transport mechanisms and serialization/deserialization\ncode (\"serde\") accordingly.\n\nThe code generator does not currently support any protocols out of the box,\ntherefore the useability of this project on its own is currently limited.\nSupport for all [AWS protocols](https://smithy.io/2.0/aws/protocols/index.html)\nexists in [aws-sdk-go-v2](https://github.com/aws/aws-sdk-go-v2). We are\ntracking the movement of those out of the SDK into smithy-go in\n[#458](https://github.com/aws/smithy-go/issues/458), but there's currently no\ntimeline for doing so.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n\n", "release_dates": ["2024-02-21T19:26:40Z", "2024-02-13T19:24:19Z", "2023-12-07T19:49:35Z", "2023-11-30T01:34:00Z", "2023-11-29T19:04:08Z", "2023-11-15T19:31:51Z", "2023-10-31T18:40:39Z", "2023-10-06T18:28:18Z", "2023-08-18T18:33:45Z", "2023-08-07T18:32:01Z", "2023-07-31T18:28:36Z", "2022-12-02T19:22:11Z", "2022-10-24T19:12:30Z", "2022-09-14T23:24:19Z", "2022-09-02T18:25:46Z", "2022-08-31T18:31:23Z", "2022-08-29T19:28:02Z", "2022-08-09T18:30:45Z", "2022-06-29T18:30:40Z", "2022-06-07T18:23:21Z", "2022-03-24T18:22:19Z", "2022-03-08T01:54:28Z", "2022-02-24T19:40:31Z", "2022-01-14T22:47:20Z", "2022-01-07T22:26:45Z", "2021-11-05T23:02:48Z", "2021-10-21T20:57:13Z", "2021-08-27T21:51:53Z", "2021-08-04T19:41:59Z", "2021-07-15T21:14:27Z"]}, {"name": "studio-lab-examples", "description": "Example notebooks for working with SageMaker Studio Lab. Sign up for an account at the link below!", "language": "Jupyter Notebook", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<p align=\"center\">\n  <img src=\"_static/sagemaker-studio-lab-banner.svg\" width=\"300px\">\n</p>\n\n## SageMaker Studio Lab Examples\n\nExample Jupyter notebooks that demonstrate how to build AI/ML learning environment using Amazon SageMaker Studio Lab.\n\n## :books: Background\n\nSageMaker Studio Lab is a service for individual data scientist who wants to develop the career toward AI/ML practitioner. You can start your ML journey for free.\n\nThis repository introduces you to the way to set up Studio Lab according to your interest area, such as computer vision, natural language processing, etc. And also, we show how to deploy your project to the [Amazon SageMaker](https://github.com/aws/amazon-sagemaker-examples) to become the AI/ML practitioner.\n\n## :hammer_and_wrench: Setup\n\nPlease follow the [Onboard to Amazon SageMaker Studio Lab](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-lab-onboard.html).\n\n1. Request a Studio Lab account\n2. Create a Studio Lab account\n3. Sign in to Studio Lab\n\nIf you would like to localize the user interface, please follow [the instruction for user interface localization](custom-environments/localization).\n\n## :computer: Usage\n\n1. **Read**: You can read the notebook in Studio Lab without Studio Lab account. Please feel free to click **Open in Studio Lab** button in Examples section.\n2. **Run**: You can run the notebook by copying the notebook or `git clone` the repository to your Studio Lab project.\n3. **Share**: You can share the notebooks through the Git repository such as GitHub. [If you add Open in Studio Lab button](open-in-studio-lab), the readers can copy the notebook or clone the repository by clicking button.\n\n## :notebook: Examples\n\n### [Computer Vision](computer-vision/)\n\n| No | Title | Open in Studio Lab |\n|----|-------|--------------------|\n|   1|Train an image classification model with PyTorch | [![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/computer-vision/kmnist/cv-kminst.ipynb) |\n|   2| Weather Classification for Disaster Risk Reduction with DenseNet-161 | [![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/computer-vision/weather-data/weather-image-classification-pytorch.ipynb) |\n\n### [Natural Language Processing](natural-language-processing/)\n\n| No | Title | Open in Studio Lab |\n|----|-------|--------------------|\n|   1|Finetune T5 locally for machine translation on COVID-19 Health Service Announcements with Hugging Face | [![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/natural-language-processing/NLP_Disaster_Recovery_Translation.ipynb) |\n\n### [Geospatial Data Science](geospatial-data-science/)\n\n| No | Title | Open in Studio Lab |\n|----|-------|--------------------|\n|   1|Getting Started With Geospatial Data Analysis | [![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/geospatial-data-science/CA_data/geospatial_analysis.ipynb) |\n|   2|Exploratory Analysis for NOAA Weather and Climate Dataset | [![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/geospatial-data-science/NOAA_Exploratory_Analysis/EDA_weather_climate.ipynb) |\n\n### [Generative Deep Learning](generative-ai/)\n\n| No | Title | Open in Studio Lab |\n|----|-------|--------------------|\n|   1|Introduction to JumpStart - Text to Image | [![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/generative-ai/stable-diffusion-finetune/Amazon_JumpStart_Text_To_Image.ipynb) |\n|   2|Prompting Mistral 7B Instruct | [![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/generative-ai/mistral/prompting-mistral7B.ipynb) |\n\n### [Connect To AWS](connect-to-aws/)\n\n| No | Title | Open in Studio Lab |\n|----|-------|--------------------|\n|   1|Using SageMaker Studio Lab with AWS Resources | [![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/connect-to-aws/Access_AWS_from_Studio_Lab.ipynb) |\n|   2|Deploy A Hugging Face Pretrained Model to Amazon SageMaker Serverless Endpoint - Boto3 | [![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/connect-to-aws/Access_AWS_from_Studio_Lab_Deployment.ipynb)\n\n### [Custom Environments](custom-environments/)\n\nWe provide `.yml` files to set up various programming language / framework environments. To use the `.yml` file, please proceed with the following instruction.\n\n1. Click this button right here --> [![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/aws/studio-lab-examples/blob/main/custom-environments/custom_environment.ipynb)\n2. Click the `Copy to Project` button\n   * Sign-in and `Start runtime` is needed before it.\n3. When prompted, select `Clone Entire Repo`\n4. Click `Clone` after confirming `Open README files.` is checked.\n   * When `No Conda environment file found` shown, please `Dismiss`.\n5. Once opening `README.md` preview, please move to `Custom Environments` section and click the programming language / specific framework environment link as you need to open `.yml` file.\n6. Right click the opened `.yml` file tab and select `Show in File Browser`.\n7. Right click the `.yml` file in the file browser and select `Build Conda Environment`.\n8. Once command completed, please run notebook in the same folder to check the environment. When prompted `Select Kearnel`, please select the created environment.\n\n#### Programming language environment\n\n* [R environment](custom-environments/R/R.yml)\n* [Julia environment](custom-environments/julia/1-install-julia.ipynb)\n\n#### Specific framework environment\n\n* [AutoGluon (CPU) environment](custom-environments/AutoGluon/autogluon_cpu.yml)\n   * [AutoGluon](https://auto.gluon.ai/stable/index.html) is AutoML library for quick prototype by state-of-the-art method without expert knowledge. \n* [fast.ai environment](custom-environments/fastai/fastai.yml)\n   * [fast.ai](https://www.fast.ai/) is deep learning library which provides state-of-the-art results with high-level API for practitioners and low-level API for expert.\n* [SciPy environment](custom-environments/SciPy/scipy.yml)\n   * [SciPy](https://scipy.org/) is an open-source software for mathematics, science, and engineering.\n* [Diffusers environment](custom-environments/diffusers/diffusers.yml)\n   * [diffusers](https://github.com/huggingface/diffusers) provides pretrained diffusion models across multiple modalities, such as vision and audio, and serves as a modular toolbox for inference and training of diffusion models.\n* [RAPIDS environment](https://studiolab.sagemaker.aws/import/github/rapidsai-community/rapids-smsl/blob/main/rapids-smsl.ipynb) (external link)\n   * [RAPIDS](https://rapids.ai/index.html) provides GPU accelerated data science libraries.\n* [Geospatial environment](custom-environments/Geospatial/geospatial.yml)\n   * Geospatial environment is built from basic libraries for geospatial analysis such as [geopandas](https://geopandas.org/en/stable/), [shapely](https://github.com/shapely/shapely), and [folium](https://python-visualization.github.io/folium/quickstart.html#Getting-Started), etc.\n* [Medical image AI environment](https://github.com/aws/studio-lab-examples/blob/main/custom-environments/medical-image-ai/environment.yml)\n   * Medial image AI environment is built from basic libraries for medical image analysis such as [itkwidgets](https://github.com/InsightSoftwareConsortium/itkwidgets), [monai](https://monai.io/). \n* [Gradio environment](https://github.com/aws/studio-lab-examples/blob/main/custom-environments/Gradio/environment.yml)\n   * [Gradio](https://www.gradio.app/) is an application that is suitable for demonstrating your model through an interactive interface.\n\n\n### Community contents\n\nHere are some more examples from the community.\n\n[Studio Lab Examples in GitHub](https://github.com/topics/amazon-sagemaker-lab).\n\nPlease add `amazon-sagemaker-lab` tag to your repositories that use Studio Lab! We will pick up the popular repositories in here or our blog.\n\n## :balance_scale: License\n\nThis project is licensed under the [Apache-2.0 License](LICENSE).\n\n## :handshake: Contributing\n\nAlthough we're extremely excited to receive contributions from the community, we're still working on the best mechanism to take in examples from external sources. Please bear with us in the short-term if pull requests take longer than expected or are closed.\n\nPlease read our [contributing guidelines](CONTRIBUTING.md) if you'd like to open an issue or submit a pull request.\n\n## \ud83d\udd0e References\n\n* [SageMaker Studio Lab](https://studiolab.sagemaker.aws/)\n* [SageMaker Studio Lab document](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-lab.html)\n* [Stack Overflow](https://stackoverflow.com/questions/tagged/amazon-sagemaker)\n* Regional examples\n   * :jp: [SageMaker Studio Lab Community Japan](https://github.com/aws-sagemaker-jp/awesome-studio-lab-jp)\n", "release_dates": []}, {"name": "telegraf", "description": "The plugin-driven server agent for collecting & reporting metrics.", "language": "Go", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "\n# Telegraf\n*This is a fork of [telegraf](https://github.com/influxdata/telegraf/) project*\n\n![tiger](TelegrafTiger.png \"tiger\")\n\n[![Circle CI](https://circleci.com/gh/influxdata/telegraf.svg?style=svg)](https://circleci.com/gh/influxdata/telegraf) [![Docker pulls](https://img.shields.io/docker/pulls/library/telegraf.svg)](https://hub.docker.com/_/telegraf/)\n[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://www.influxdata.com/slack)\n\nTelegraf is an agent for collecting, processing, aggregating, and writing metrics. Based on a\nplugin system to enable developers in the community to easily add support for additional\nmetric collection. There are four distinct types of plugins:\n\n1. [Input Plugins](/docs/INPUTS.md) collect metrics from the system, services, or 3rd party APIs\n2. [Processor Plugins](/docs/PROCESSORS.md) transform, decorate, and/or filter metrics\n3. [Aggregator Plugins](/docs/AGGREGATORS.md) create aggregate metrics (e.g. mean, min, max, quantiles, etc.)\n4. [Output Plugins](/docs/OUTPUTS.md) write metrics to various destinations\n\nNew plugins are designed to be easy to contribute, pull requests are welcomed, and we work to\nincorporate as many pull requests as possible. Consider looking at the\n[list of external plugins](EXTERNAL_PLUGINS.md) as well.\n\n## Minimum Requirements\n\nTelegraf shares the same [minimum requirements][] as Go:\n\n- Linux kernel version 2.6.23 or later\n- Windows 7 or later\n- FreeBSD 11.2 or later\n- MacOS 10.11 El Capitan or later\n\n[minimum requirements]: https://github.com/golang/go/wiki/MinimumRequirements#minimum-requirements\n\n## Obtaining Telegraf\n\nView the [changelog](/CHANGELOG.md) for the latest updates and changes by version.\n\n### Binary Downloads\n\nBinary downloads are available from the [InfluxData downloads](https://www.influxdata.com/downloads)\npage or from each [GitHub Releases](https://github.com/influxdata/telegraf/releases) page.\n\n### Package Repository\n\nInfluxData also provides a package repo that contains both DEB and RPM downloads.\n\nFor deb-based platforms (e.g. Ubuntu and Debian) run the following to add the\nrepo key and setup a new sources.list entry:\n\n```shell\nwget -qO- https://repos.influxdata.com/influxdb.key | sudo tee /etc/apt/trusted.gpg.d/influxdb.asc >/dev/null\nsource /etc/os-release\necho \"deb https://repos.influxdata.com/${ID} ${VERSION_CODENAME} stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\nsudo apt-get update && sudo apt-get install telegraf\n```\n\nFor RPM-based platforms (e.g. RHEL, CentOS) use the following to create a repo\nfile and install telegraf:\n\n```shell\ncat <<EOF | sudo tee /etc/yum.repos.d/influxdb.repo\n[influxdb]\nname = InfluxDB Repository - RHEL $releasever\nbaseurl = https://repos.influxdata.com/rhel/\\$releasever/\\$basearch/stable\nenabled = 1\ngpgcheck = 1\ngpgkey = https://repos.influxdata.com/influxdb.key\nEOF\nsudo yum install telegraf\n```\n\n### Build From Source\n\nTelegraf requires Go version 1.17 or newer, the Makefile requires GNU make.\n\n1. [Install Go](https://golang.org/doc/install) >=1.17 (1.17.2 recommended)\n2. Clone the Telegraf repository:\n\n   ```shell\n   git clone https://github.com/influxdata/telegraf.git\n   ```\n\n3. Run `make` from the source directory\n\n   ```shell\n   cd telegraf\n   make\n   ```\n\n### Nightly Builds\n\n[Nightly](/docs/NIGHTLIES.md) builds are available, generated from the master branch.\n\n### 3rd Party Builds\n\nBuilds for other platforms or package formats are provided by members of theTelegraf community.\nThese packages are not built, tested, or supported by the Telegraf project or InfluxData. Please\nget in touch with the package author if support is needed:\n\n- [Ansible Role](https://github.com/rossmcdonald/telegraf)\n- [Chocolatey](https://chocolatey.org/packages/telegraf) by [ripclawffb](https://chocolatey.org/profiles/ripclawffb)\n- [Scoop](https://github.com/ScoopInstaller/Main/blob/master/bucket/telegraf.json)\n- [Snap](https://snapcraft.io/telegraf) by Laurent Sesqu\u00e8s (sajoupa)\n\n## Getting Started\n\nSee usage with:\n\n```shell\ntelegraf --help\n```\n\n### Generate a telegraf config file\n\n```shell\ntelegraf config > telegraf.conf\n```\n\n### Generate config with only cpu input & influxdb output plugins defined\n\n```shell\ntelegraf --section-filter agent:inputs:outputs --input-filter cpu --output-filter influxdb config\n```\n\n### Run a single telegraf collection, outputting metrics to stdout\n\n```shell\ntelegraf --config telegraf.conf --test\n```\n\n### Run telegraf with all plugins defined in config file\n\n```shell\ntelegraf --config telegraf.conf\n```\n\n### Run telegraf, enabling the cpu & memory input, and influxdb output plugins\n\n```shell\ntelegraf --config telegraf.conf --input-filter cpu:mem --output-filter influxdb\n```\n\n## Documentation\n\n[Latest Release Documentation](https://docs.influxdata.com/telegraf/latest/)\n\nFor documentation on the latest development code see the [documentation index](/docs).\n\n- [Input Plugins](/docs/INPUTS.md)\n- [Output Plugins](/docs/OUTPUTS.md)\n- [Processor Plugins](/docs/PROCESSORS.md)\n- [Aggregator Plugins](/docs/AGGREGATORS.md)\n\n## Contributing\n\nThere are many ways to contribute:\n\n- Fix and [report bugs](https://github.com/influxdata/telegraf/issues/new)\n- [Improve documentation](https://github.com/influxdata/telegraf/issues?q=is%3Aopen+label%3Adocumentation)\n- [Review code and feature proposals](https://github.com/influxdata/telegraf/pulls)\n- Answer questions and discuss here on github and on the [Community Site](https://community.influxdata.com/)\n- [Contribute plugins](CONTRIBUTING.md)\n- [Contribute external plugins](docs/EXTERNAL_PLUGINS.md)\n", "release_dates": ["2023-11-03T15:38:46Z", "2022-04-22T14:10:19Z", "2022-04-22T13:59:57Z", "2022-04-22T13:33:54Z", "2022-04-22T13:24:28Z"]}, {"name": "thinkbox-cm-library", "description": null, "language": "CMake", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ThinkboxCMLibrary\n\n## Overview\n\nThinkboxCMLibrary provides utility CMake functions for Thinkbox's 3d tools projects.\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to ThinkboxCMLibrary are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to ThinkboxCMLibrary can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-frost", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Frost\n\n## Overview\n\nThe Frost particle mesher code.\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to Frost are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to Frost can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-frost-mx", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# FrostMX\n\n## Overview\n\nThe Frost plugin for 3ds Max.\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to FrostMX are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to FrostMX can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-krakatoa", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Krakatoa\n\n## Overview\n\nKrakatoa is a fast particle renderer for use in visual effects applications. More information can be found [here](https://aws.amazon.com/thinkbox-krakatoa/).\n\nThis repository contains subdirectories with two different projects. The `Krakatoa` subdirectory contains the underlying code for the Krakatoa renderer itself. The `KrakatoaSR` subdirectory contains a C++ API for interacting with Krakatoa.\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to Krakatoa are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to Krakatoa can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-krakatoa-mx", "description": null, "language": "MAXScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# KrakatoaMX\n\n## Overview\n\nThe Krakatoa plugin for 3ds Max.\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to KrakatoaMX are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to KrakatoaMX can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-krakatoa-my", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# KrakatoaMY\n\n## Overview\n\nKrakatoaMY is a Maya plugin that enables using the Krakatoa renderer in Autodesk Maya.\n\nThe official documentation for the plugin can be found [here](https://docs.thinkboxsoftware.com/products/krakatoa/2.12/1_Documentation/manual/kmy/index.html).\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to KrakatoaMY are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to KrakatoaMY can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-library", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ThinkboxLibrary\r\n\r\n## Overview\r\n\r\nThinkboxLibrary is a general purpose C++ library with a focus on computer graphics. It makes up the backbone of all of Thinkbox's 3D Graphics Tools.\r\n\r\n## Table of Contents\r\n\r\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\r\n- [Security issue notifications](#security-issue-notifications)\r\n- [Contributing](#contributing)\r\n- [Code of Conduct](#code-of-conduct)\r\n- [Licensing](#licensing)\r\n\r\n## Reporting Bugs/Feature Requests\r\n\r\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\r\n\r\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\r\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\r\n\r\n- A reproducible test case or series of steps\r\n- The version of our code being used\r\n- Any modifications you've made relevant to the bug\r\n- Anything unusual about your environment or deployment\r\n\r\n## Security issue notifications\r\n\r\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\r\n\r\n## Contributing\r\n\r\nContributions to ThinkboxLibrary are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\r\nwe are happy to accept your contribution. Information on contributing to ThinkboxLibrary can be found\r\n[in CONTRIBUTING.md](CONTRIBUTING.md).\r\n\r\n## Code of Conduct\r\n\r\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\r\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\r\nopensource-codeofconduct@amazon.com with any additional questions or comments.\r\n\r\n## Licensing\r\n\r\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\r\n\r\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\r\n", "release_dates": []}, {"name": "thinkbox-magma", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Magma\n\n## Overview\n\nMagma is a graphical programming language. It is designed for visual effects applications, primarily for particle shading for the Krakatoa renderer.\n\nMagma is divided into two projects. MagmaCore is found in the `MagmaCore/` subdirectory. This is the implementation of the Magma language itself. MagmaMY is found in the `MagmaMY/` subdirectory. This is an extension of Magma for use in Maya plugins.\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to Magma are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to Magma can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-mx-library", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ThinkboxMXLibrary\n\n## Overview\n\nShared code for Thinkbox 3ds Max plugins.\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to ThinkboxMXLibrary are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to ThinkboxMXLibrary can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-my-library", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# ThinkboxMYLibrary\n\n## Overview\n\nThinkboxMYLibrary is a C++ library to aid in the development of Maya plugins.\n\nThe MayaSDKConan subdirectory contains the necessary files for creating a local conan package from the Maya SDK.\nMaya must be installed on the local system in order to build that package.\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to ThinkboxMYLibrary are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to ThinkboxMYLibrary can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-node-view", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# NodeView\n\n## Overview\n\nNodeView is a Qt component for displaying graph structures in a GUI.\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to NodeView are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to NodeView can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-stoke", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Stoke\n\n## Overview\n\nThe core Stoke code.\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to Stoke are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to Stoke can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-stoke-mx", "description": null, "language": "MAXScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# StokeMX\n\n## Overview\n\nThe Stoke plugin for 3ds Max.\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to StokeMX are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to StokeMX can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-xmesh", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# XMesh\n\n## Overview\n\nXMesh contains code to support certain features of XMesh plugins such as [ThinkboxXMeshMY](). Most of the code for reading from and writing to the XMesh file format can be found in [ThinkboxThinkboxLibrary]().\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to XMesh are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to XMesh can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-xmesh-mx", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# XMeshMX\n\n## Overview\n\nThe XMesh plugins for 3ds Max.\n\n## Table of Contents\n\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\n- [Security issue notifications](#security-issue-notifications)\n- [Contributing](#contributing)\n- [Code of Conduct](#code-of-conduct)\n- [Licensing](#licensing)\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n- A reproducible test case or series of steps\n- The version of our code being used\n- Any modifications you've made relevant to the bug\n- Anything unusual about your environment or deployment\n\n## Security issue notifications\n\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n## Contributing\n\nContributions to XMeshMX are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\nwe are happy to accept your contribution. Information on contributing to XMeshMX can be found\n[in CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\n", "release_dates": []}, {"name": "thinkbox-xmesh-my", "description": null, "language": "C++", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# XMeshMY\r\n\r\n## Overview\r\n\r\nXMeshMY is a Maya plugin that enables loading and saving geometry in the XMesh file format in Autodesk Maya.\r\n\r\nThe official documentation for the plugin can be found [here](https://docs.thinkboxsoftware.com/products/xmesh/1.10/1_Documentation/manual/xmesh_my/xmeshmy_index.html).\r\n\r\n## Table of Contents\r\n\r\n- [Reporting Bugs/Feature Requests](#reporting-bugs/feature-requests)\r\n- [Security issue notifications](#security-issue-notifications)\r\n- [Contributing](#contributing)\r\n- [Code of Conduct](#code-of-conduct)\r\n- [Licensing](#licensing)\r\n\r\n## Reporting Bugs/Feature Requests\r\n\r\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\r\n\r\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\r\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\r\n\r\n- A reproducible test case or series of steps\r\n- The version of our code being used\r\n- Any modifications you've made relevant to the bug\r\n- Anything unusual about your environment or deployment\r\n\r\n## Security issue notifications\r\n\r\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\r\n\r\n## Contributing\r\n\r\nContributions to XMeshMY are encouraged. If you want to fix a problem, or want to enhance the library in any way, then\r\nwe are happy to accept your contribution. Information on contributing to XMeshMY can be found\r\n[in CONTRIBUTING.md](CONTRIBUTING.md).\r\n\r\n## Code of Conduct\r\n\r\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\r\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\r\nopensource-codeofconduct@amazon.com with any additional questions or comments.\r\n\r\n## Licensing\r\n\r\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\r\n\r\nWe may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes.\r\n", "release_dates": []}, {"name": "to-smote-or-not", "description": null, "language": "Python", "license": {"key": "mit-0", "name": "MIT No Attribution", "spdx_id": "MIT-0", "url": "https://api.github.com/licenses/mit-0", "node_id": "MDc6TGljZW5zZTQx"}, "readme": "# To SMOTE, or not to SMOTE?\nThis package includes the code required to repeat the experiments in the paper and to analyze \nthe results.\n\n> To SMOTE, or not to SMOTE?\n> \n> Yotam Elor and Hadar Averbuch-Elor\n\n## Installation\n```\n# Create a new conda environment and activate it\nconda create --name to-SMOTE-or-not -y python=3.7\nconda activate to-SMOTE-or-not\n# Install dependencies\npip install -r requirements.txt\n```\n\n## Running experiments\nThe data is not included with this package. See an example of running a single experiment with a dataset from \n`imblanaced-learn`\n```python\n# Load the data\nimport pandas as pd\nimport numpy as np\nfrom imblearn.datasets import fetch_datasets\ndata = fetch_datasets()[\"mammography\"]\nx = pd.DataFrame(data[\"data\"])\ny = np.array(data[\"target\"]).reshape((-1, 1))\n\n# Run the experiment\nfrom experiment import experiment\nfrom classifiers import CLASSIFIER_HPS\nfrom oversamplers import OVERSAMPLER_HPS\nresults = experiment(\n    x=x,\n    y=y,\n    oversampler={\n        \"type\": \"smote\",\n        \"ratio\": 0.4,\n        \"params\": OVERSAMPLER_HPS[\"smote\"][0],\n    },\n    classifier={\n        \"type\": \"cat\",  # Catboost\n        \"params\": CLASSIFIER_HPS[\"cat\"][0]\n    },\n    seed=0,\n    normalize=False,\n    clean_early_stopping=False,\n    consistent=True,\n    repeats=1\n)\n\n# Print the results nicely\nimport json\nprint(json.dumps(results, indent=4))\n```\nTo run all the experiments in our study, wrap the above in loops, for example\n```python\nfor dataset in datasets:\n    x, y = load_dataset(dataset)  # this functionality is not provided\n    for seed in range(7):\n        for classifier, classifier_hp_configs in CLASSIFIER_HPS.items():\n            for classifier_hp in classifier_hp_configs:\n                for oversampler, oversampler_hp_configs in OVERSAMPLER_HPS.items():\n                    for oversampler_hp in oversampler_hp_configs:\n                        for ratio in [0.1, 0.2, 0.3, 0.4, 0.5]:\n                            results = experiment(\n                                x=x,\n                                y=y,\n                                oversampler={\n                                    \"type\": oversampler,\n                                    \"ratio\": ratio,\n                                    \"params\": oversampler_hp,\n                                },\n                                classifier={\n                                    \"type\": classifier,\n                                    \"params\": classifier_hp\n                                },\n                                seed=seed,\n                                normalize=...,\n                                clean_early_stopping=...,\n                                consistent=...,\n                                repeats=...\n                            )\n```\n## Analyze\nRead the results from the compressed csv file. As the results file is large, it is tracked using\n[git-lfs](https://git-lfs.github.com/). You might need to download it manually or install git-lfs.\n```python\nimport os\nimport pandas as pd\ndata_path = os.path.join(os.path.dirname(__file__), \"../data/results.gz\")\ndf = pd.read_csv(data_path)\n```\nDrop nans and filter experiments with consistent classifiers, no normalization and a single\nvalidation fold\n```python\ndf = df.dropna()\ndf = df[\n    (df[\"consistent\"] == True)\n    & (df[\"normalize\"] == False)\n    & (df[\"clean_early_stopping\"] == False)\n    & (df[\"repeats\"] == 1)\n]\n```\n\nSelect the best HP configurations according to AUC validation scores. `opt_metric` is the key\nused to select the best configuration. For example, for a-priori HPs use `opt_metric=\"test.roc_auc\"`\nand for validation-HPs use `opt_metric=\"validation.roc_auc\"`. Additionaly calculate average score and rank\n```python\nfrom analyze import filter_optimal_hps\ndf = filter_optimal_hps(\n    df, opt_metric=\"validation.roc_auc\", output_metrics=[\"test.roc_auc\"]\n)\nprint(df)\n```\nPlot the results\n```python\nfrom analyze import avg_plots\navg_plots(df, \"test.roc_auc\")\n```\n## Citation\n```\n@misc{elor2022smote,\n    title={To SMOTE, or not to SMOTE?}, \n    author={Yotam Elor and Hadar Averbuch-Elor},\n    year={2022},\n    eprint={2201.08528},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the MIT-0 License. See the LICENSE file.\n\n", "release_dates": []}, {"name": "Trusted-Advisor-Tools", "description": "The sample functions provided help to automate AWS Trusted Advisor best practices using Amazon Cloudwatch events and AWS Lambda.", "language": "Python", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Trusted Advisor Tools\n\n### Overview\nAWS Trusted Advisor provides real time guidance to help users provision their resources following AWS best practices. You can now create configurable, rule-based events for automated actions based on AWS Trusted Advisor\u2019s library of best-practice checks using Amazon EventBridge.\nThe sample functions provided help to automate Trusted Advisor best practices.\n\n### Setup and Usage\n\nSetup and usage instructions are present for each tool in its respective directory: <br />\n[Stop Amazon EC2 instances with low utilization](LowUtilizationEC2Instances/) <br />\n[Create snapshots for EBS volumes with no recent backup](AmazonEBSSnapshots/) <br />\n[Delete exposed IAM Keys and monitor usage](ExposedAccessKeys/)<br />\n[Enable S3 bucket Versioning](S3BucketVersioning/)<br />\n\nMore information about Trusted Advisor is available here: https://aws.amazon.com/premiumsupport/trustedadvisor/\n\n### License\nTrusted Advisor Tools is licensed under the Apache 2.0 License.\n", "release_dates": []}, {"name": "uefi", "description": "UEFI", "language": "Nix", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# UEFI\n\nThis repository contains the changes that need to be applied on top of\n[edk2](https://github.com/tianocore/edk2) in order to run x86_64 guests on\nNitro-based EC2 instances. We use [Nix](https://nixos.org/download.html) for\ncreating reproducible builds of the UEFI binaries to ensure that the same UEFI\nbinaries that are used with instance launches can be reproduced on any environment.\nEC2 customers running instances with AMD SEV-SNP support can match their\nrunning UEFI firmware with the binaries released here and even reproduce the\nbinaries themselves.\n\n## How to build\n\nAmazon EC2 instances that have AMD SEV-SNP enabled will use UEFI binaries built\nin this repository as instance boot firmware. The Github workflow that is run\non every new release uses Nix to build the binary. However, the binary can also\nbe generated manually after installing [Nix](https://nixos.org/download.html)\nby running the command:\n\n```\nnix-build --pure\n```\n\nThis will produce the `result/ovmf_img.fd` binary which can be matched\nagainst running and released UEFI binaries.\n\n## How to generate a measurement\nThe [sev-snp-measure](https://github.com/virtee/sev-snp-measure) tool\ncan be used to generate measurements, e.g.  for a guest with 4 vCPUs:\n\n```\n./sev-snp-measure.py --mode snp --vcpus=4 --vmm-type=ec2 --ovmf=ovmf_img.fd\n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the [BSD-2-Clause-Patent License](LICENSE).\n\n", "release_dates": ["2024-01-12T16:16:42Z", "2023-05-16T10:11:26Z", "2023-04-21T08:39:24Z"]}, {"name": "universal-test-runner", "description": ":runner: A universal test runner for any language and test framework.", "language": "TypeScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# universal-test-runner\n\n[![npm version](https://img.shields.io/npm/v/@aws/universal-test-runner)](https://www.npmjs.com/package/@aws/universal-test-runner)\n[![npm downloads](https://img.shields.io/npm/dm/@aws/universal-test-runner)](https://npm-stat.com/charts.html?package=%40aws%2Funiversal-test-runner)\n[![build status](https://github.com/aws/universal-test-runner/actions/workflows/build.yml/badge.svg)](https://github.com/aws/universal-test-runner/actions/workflows/build.yml)\n[![codecov](https://codecov.io/github/aws/universal-test-runner/branch/main/graph/badge.svg?token=6TIM7H9HC5)](https://codecov.io/github/aws/universal-test-runner)\n\nA universal test runner for any language and test framework.\n\n**\u2757\ufe0f NB: universal-test-runner is currently working towards a 1.0.0 release.\nSee the [1.0.0 milestone](https://github.com/aws/universal-test-runner/milestone/1) \nfor all progress towards 1.0.0. \u2757\ufe0f**\n\n## \ud83c\udf0e What is universal-test-runner?\n\nuniversal-test-runner is a command-line tool that uses the [Test Execution\nProtocol](./protocol/README.md) to run tests for any programming language and\nany test framework.\n\n**Installation**: Install globally using npm:\n\n```\n$ npm install -g @aws/universal-test-runner \n```\n\nNow the `run-tests` executable is available to be used.\n\n**Usage**: For example, to run a single test named \"test1\" in a\nproject using [jest](https://jestjs.io/), you can run the following:\n\n```\n$ export TEP_TESTS_TO_RUN=\"test1\" # set by IDE or CI/CD system\n\n$ run-tests jest\nPASS  ./index.test.js\n  Suite1\n    \u2713 test1 (2 ms)\n    \u25cb skipped test2\n    \u25cb skipped test3\n\nTest Suites: 1 passed, 1 total\nTests:       2 skipped, 1 passed, 3 total\nSnapshots:   0 total\nTime:        0.288 s, estimated 1 s\nRan all test suites with tests matching \"test1\".\n```\n\nHow about running a test named \"test1\", but for a project using\n[pytest](https://pytest.org)? Easy -- we can use the same command!\n\n```\n$ export TEP_TESTS_TO_RUN=\"test1\" # set by IDE or CI/CD system\n\n$ run-tests pytest\n================== test session starts =====================\nplatform darwin -- Python 3.10.9, pytest-7.1.3, pluggy-1.0.0\ncollected 3 items / 2 deselected / 1 selected\n\ntest_example.py .\n```\n\nAs shown in these examples, the Test Execution Protocol is used to establish a\nunified interface for passing arguments to different test frameworks and\nrunners.  Check out [RFC 0001](./protocol/rfcs/0001/README.md) for the\nmotivation behind universal-test-runner and the Test Execution Protocol, and\nwhy having a common interface is so useful.\n\n## \ud83e\udd14 When should I use universal-test-runner?\n\nYou should install universal-test-runner in the following cases:\n\n* Your IDE or CI/CD system tells you to, in order for it to support running tests according to the [Test Execution Protocol](./protocol/README.md)\n* You're developing an adapter for universal-test-runner, and you want to test your adapter\n* You're writing an IDE plugin or CI/CD integration that implements the Test Execution Protocol, and you need a protocol-aware runner to test your integration\n\n## \ud83d\udcc8 Framework and build tool support\n\nFirst-party test adapter support is provided for the following frameworks/build tools:\n\n* jest: https://jestjs.io/\n* pytest: https://pytest.org\n* maven: https://maven.apache.org/\n* gradle: https://gradle.org/\n* dotnet: https://learn.microsoft.com/en-us/dotnet/core/tools/\n\nSee the [1.0.0 milestone](https://github.com/aws/universal-test-runner/milestone/1)\nfor all frameworks and build tools we plan to support for v1.0.0.\n\n## \ud83d\udce6 Packages in this monorepo\n\nThe only package you should install and depend on is\n[`@aws/universal-test-runner`](./packages/universal-test-runner),\nwhich follows [semantic versioning](https://semver.org/).\n\nThe other packages are either internal utilities or adapters that have unstable\nAPIs and won't necessarily follow semver. You should avoid depending on them\ndirectly.\n\n## \ud83c\udff0 Architecture\n\nuniversal-test-runner is a \"Test Execution Protocol-aware\" runner that uses an\nadapter model to provide support for test frameworks.  The runner itself is not\naware of any frameworks, but delegates to the appropriate adapter in order to\nexecute tests for a specific framework.  For more details on the architecture,\nsee the following documentation:\n* [RFC 1](./protocol/rfcs/0001/README.md) for an explanation of the Test Execution Protocol\n* universal-test-runner [architecture documentation](./docs/architecture.md), for a description of the runner, the adapters, and how they interact\n\n## \ud83d\udd0b Custom adapters\n\nIt's possible to write custom adapters and pass them to universal-test-runner,\nproviding support for new frameworks or custom testing setups. See the docs on\n[writing custom adapters](#-writing-adapters) for how to implement one.\n\nIf you write a custom adapter, please host it in its own GitHub repo and\npublish it to npm; then [open a pull request](https://github.com/aws/universal-test-runner/compare)\nto add it to our [list of known third-party adapters](./docs/third-party-adapters.md),\nso everyone can benefit. (Note that we won't be adding the source code of\nthird-party adapters directly to this repo.)\n\nExample of using a third-party adapter from npm:\n\n```\nnpm install -g my-awesome-adapter\nrun-tests my-awesome-adapter\n```\n\nIf you have a specific project setup that you don't think merits a generic\nthird-party adapter, you can pass an adapter from a local file:\n\n```\nrun-tests ./my-local-adapter.js\n```\n\nIf universal-test-runner doesn't suit your needs exactly, you can use it as an\nexample of how to write your own Test Execution Protocol-aware runner. See the\n[writing custom runners](#-custom-runners) and the \n[Test Execution Protocol](./protocol/README.md) docs for more info.\n\n## \ud83d\udc69\u200d\ud83d\udcbb Writing adapters\n\nTest adapters are responsible for executing tests as specified by the Test\nExecution Protocol, and reporting the status of the test execution back to\nuniversal-test-runner. The runner will do all the work of parsing the protocol\nenvironment variables, and then invoke the `executeTests` function exposed by\nthe adapter.\n\n* The `executeTests` function must accept an input object of type [`AdapterInput`](./packages/universal-test-runner-types/src/index.ts)\n* The `executeTests` function must return an ouput object of type [`AdapterOutput`](./packages/universal-test-runner-types/src/index.ts) or `Promise<AdapterOutput>`.\n  * If the adapter executes the tests successfully, and the test run passes, `executeTests` should return an exitCode of `0` (or a promise resolved with an exitCode of `0`).\n  * If the adapter executes the tests successfully, and the test run fails, `executesTests` should return a non-zero exitCode (or a promise resolved with a non-zero exitCode).\n  * If the adapter cannot execute the tests due to an unrecoverable error, `executeTests` should throw an error (or return a rejected promise).\n\nTwo simple adapters are shown below, with the details of test execution omitted:\n\n```javascript\n// adapter.js\nexport function executeTests({ testNamesToRun }) {\n  const pass = doTestExecution(testNamesToRun)\n  return { exitCode: pass ? 0 : 1 }\n}\n```\n\n```javascript\n// adapter.js\nexport function executeTests({ testNamesToRun }) {\n  return new Promise((resolve, reject) => {\n    doTestExecution(testNamesToRun, (err, pass) => {\n      if (err) {\n        return reject(err)\n      }\n      resolve({ exitCode: pass ? 0 : 1 });\n    })\n  })\n}\n```\n\nThe adapter is passed to the runner as follows:\n\n```\nrun-tests ./adapter.js\n```\n\nAdapters can also accept a second argument called `context` of type\n[`RunnerContext`](./packages/universal-test-runner-types/src/index.ts):\n- `context.cwd`: prefer this value over using `process.cwd()` in your adapter. This allows the runner to execute the adapter in a different working directory from where the runner is executed, if needed, while still allowing the adapter to produce any artifacts (like reports) in the correct location.\n- `context.extraArgs`: Any unparsed, tokenized values passed to the runner after the end-of-argument marker `--`. Allows adapters to accommodate arbitrary flags being passed through the runner to the adapter.\n  - For example, you could pass a custom jest config to the jest adapter by running `run-tests jest -- --config ./path/to/config.js`\n- `context.logLevel`: The log level passed by the user to the runner when invoked from the command line, of type [`LogLevel`](./packages/universal-test-runner-types/src/index.ts). If adapters log anything when being executed, they should set the log level of their logger according to this value, where level rank from highest to lowest is `error`, `warn`, `info`, `debug`. Logs should only be written if their level rank is at least the rank of the specified log level, e.g. if the log level is `warn`, only logs of rank `error` and `warn` should be written.\n\nHere's an abridged example of an adapter using context:\n\n```javascript\nconst path = require('path')\n\nexport function executeTests({ testNamesToRun }, { cwd, extraArgs, logLevel }) {\n  // Use the log level specified by the runner\n  logger.setLogLevel(context.logLevel)\n\n  logger.info('Running tests...')\n\n  // Pass unparsed args to the underlying framework, if the adapter needs to support arbitrary user flags\n  const [pass, report] = doTestExecution(extraArgs, testNamesToRun)\n\n  logger.info('Running writing report...')\n\n  // If cwd is needed, prefer context.cwd over process.cwd()\n  report.write(path.join(cwd, 'reports', 'junit.xml'))\n\n  if (!pass) {\n    logger.error('Tests failed!')\n  }\n\n  return { exitCode: pass ? 0 : 1 }\n}\n```\n\n### Publishing adapters to npm\n\nStructure your adapter as described above, and make sure the `main` field in\nyour package.json points to a file that exports an `executeTests` function.\n\n## \ud83c\udfc3\u200d\u2640\ufe0f Custom runners\n\nTODO\n\n## Node.js support\n\nAll Active and Maintenance LTS versions of Node.js versions are supported.\nPlease see the Node.js [release schedule](https://github.com/nodejs/release#release-schedule)\nfor full details on the LTS process. We'll support the newest Current versions\nscheduled for LTS one month after their first release, and maintain support for\nold Maintenance versions for at least one month after they go end-of-life.\n\nFor example, as of writing, Node.js 14 and 16 are Maintenance LTS releases, and\nNode.js 18 is Active LTS, so all three are supported. Once Node.js 14 goes\nend-of-life in May 2023, universal-test-runner will support it until June 2023,\nafter which it will be removed from CI builds. Node.js 20 will become Current\nin April 2023, and LTS in October 2023, so universal-test-runner will support\nit as early as June 2023.\n\n(In the case that universal-test-runner's dependencies don't permit extended\nMaintenance support or early Current support, these one-month paddings may not\nbe possible, e.g. Jest drops support for Node.js 14 as soon as it goes\nend-of-life, or can't run on Node.js 20 until it goes LTS.)\n\n## \ud83d\udd00 Contributing\n\nPlease see the [contributing guide](./CONTRIBUTING.md) for all the logistical\ndetails. Read the existing issues and pull requests before starting to make any\ncode changes; large changes that haven't been discussed will be rejected\noutright in favour of small, incremental changes, so please open an issue early\nto discuss anything that may be non-trivial before development starts.\n\nAll changes to the Test Execution Protocol must follow the [RFC process](./protocol/README.md).\n\n### Local development setup\n\n[Fork](https://github.com/aws/universal-test-runner/fork) the repository, and then clone your fork:\n\n```\ngit clone https://github.com/<USERNAME>/universal-test-runner\ncd universal-test-runner\n```\n\nMake sure you're using the correct Node.js version (install nvm [here](https://github.com/nvm-sh/nvm) if needed):\n\n```\nnvm use\n```\n\n(Note that npm@8 or greater is required since this project uses \n[npm workspaces](https://docs.npmjs.com/cli/v9/using-npm/workspaces?v=true).\nNode.js 16 and up by default ship with versions of npm that support \nworkspaces.)\n\nInstall dependencies and build code:\n\n```\nnpm install\nnpm run compile\n```\n\nRun tests\n\n```\nnpm test\n```\n\nRun integration tests (you may have to install some of the frameworks and build tools manually to get these to pass locally):\n\n```\nnpm run test:integ\n```\n\nMake your changes and commit them. This project follows the \n[conventional commits](https://www.conventionalcommits.org/en/v1.0.0/#specification)\nspecification in order to support automatic [semantic versioning](https://semver.org/) \nand changelog generation, so a commit message hook will verify that you've\nformatted your commit message correctly. To run the pre-commit hook, you'll\nhave to install [TruffleHog](https://github.com/trufflesecurity/trufflehog).\nPush the changes to your fork, and open a pull request.\n", "release_dates": []}, {"name": "zone-aware-controllers-for-k8s", "description": "Kubernetes controllers for zone (AZ) aware rollouts and disruptions.", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Zone Aware Controllers for K8s\n\n## Introduction\n\nKubernetes controllers for zone (AZ) aware rollouts and disruptions.\n\n## Controllers\n\n### ZoneAwareUpdates (ZAU)\n\nThe ZoneAwareUpdate (ZAU) controller enables faster deployments for a StatefulSet whose pods are deployed across multiple availability zones. At each control loop, it applies zone-aware logic to check for pods in an old revision and deletes them so that they can be updated to a new revision.\n\n#### Update Strategy\n\nThe controller exponentially increases the number of pods simultaneously deleted, deploying slowly at first and accelerating as confidence is gained in the new revision. For example, it will start by updating a single pod, then 2, then 4 and so on. The number of pods deleted in an iteration will never exceed the configured `MaxUnavailable` value.\n\nThe controller also never update pods from different zones at the same time, and when moving to subsequent zones it continues to increase the number of pods to be deleted until `MaxUnavailable` is reached.\n\nAfter deleting pods, the controller will wait for them to transition to `Ready` state before updating the next set of pods.\n\nWhen a rollback (or new rollout) is initiated before a deployment finishes, it is important to delete the most recently updated pods first to move away as fast as possible from a faulty revision. To achieve that, the controller always deletes pods in a specific order, using the zone ascending alphabetical order in conjunction with the pod decreasing ordinal order, as shown in the figure below:\n\n```\n             >>>>>----------------- Update Sequence (MaxUnavailable = 4) ------------------->\n\npod #   [[28], [27, 22], [19, 17, 15, 10], [8, 6, 1]], [[29, 26, 23, 20], [16, 14, 11, 7], [5, 2]], ...\n        |                                           |  |                                         |\n        '---------------- zone-1 -------------------'  '---------------- zone-2 -----------------'\n```\n\n\nSome applications don't necessarily need to have pods updated exponentially. For those, it's possible to disable exponential updates by setting the `ExponentialFactor` to zero.\n\n```\n          >>>>>---------- Update Sequence (MaxUnavailable = 4, ExponentialFactor = 0) -------->\n\npod #   [[28, 27, 22, 19], [17, 15, 10, 8], [6, 1]], [[29, 26, 23, 20], [16, 14, 11, 7], [5, 2]], ...\n        |                                         |  |                                         |\n        '---------------- zone-1 -----------------'  '---------------- zone-2 -----------------'\n```\n\n#### Usage\n\nTo have the rollout of a StatefulSet's pods coordinated by ZAU controller, the StatefulSet update strategy should be changed to [`OnDelete`](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies) and a `ZoneAwareUpdate` resource defined into the same namespace as the StatefulSet.\n\n```yaml\napiVersion: zonecontrol.k8s.aws/v1\nkind: ZoneAwareUpdate\nmetadata:\n  name: <zau-name>\nspec:\n  statefulset: <sts-name>\n  maxUnavailable: 2\n```\n\n`maxUnavailable` can be an absolute number or a percentage of total Pods. For example, in case your application is evenly distributed accross 3 zones, it's possible to update all pods at once in each zone by setting `maxUnavailable` to at leat 33% and exponentialFactor to 0:\n\n```yaml\napiVersion: zonecontrol.k8s.aws/v1\nkind: ZoneAwareUpdate\nmetadata:\n  name: <zau-name>\nspec:\n  statefulset: <sts-name>\n  maxUnavailable: 33%\n  exponentialFactor: 0\n```\n\nIt's also possible to specify the name of a Amazon CloudWatch aggregate alarm that will pause the rollout when in alarm state. This can be used to prevent deployments from preceeding in case of canary failures, for example.\n\n```yaml\napiVersion: zonecontrol.k8s.aws/v1\nkind: ZoneAwareUpdate\nmetadata:\n  name: <zau-name>\nspec:\n  statefulset: <sts-name>\n  maxUnavailable: 2\n  pauseRolloutAlarm: <cw-aggregate-alarm-name>\n  ignoreAlarm: false\n```\n\n### ZoneDisruptionBudgets (ZDB)\n\nThe ZoneDisruptionBudget (ZDB) admission webhook controller extends the PodDisruptionBudgets (PDB) concept, allowing multiple disruptions only if the pods being disrupted are in the same zone.\n\nSimilar to the k8s' [DisruptionController](https://github.com/kubernetes/kubernetes/blob/d7123a65248e25b86018ba8220b671cd483d6797/pkg/controller/disruption/disruption.go#L555), the ZoneDisruptionBudget (ZDB) Controller is responsible for watching for changes to ZDBs and for keeping their status up to date, checking at each control-loop (https://kubernetes.io/docs/concepts/architecture/controller/) which pods are unavailable to calculate the number of disruptions allowed per zone at any time.\n\nA validation admission webhook is used to intercept requests to the eviction API, accepting or rejecting them based on ZDB's status, allowing multiple pods disruptions in zone-1, while blocking evictions from other zones.\n\n#### Usage\n\nA `ZoneDisruptionBudget` has three fields:\n\n- A label selector `.spec.selector` to specify the set of pods to which it applies. This field is required.\n- `.spec.maxUnavailable` that defines the maximun number of pods in the same zone that can be unavailable after the eviction. It can be either an absolute number or a percentage.\n\n```yaml\napiVersion: zonecontrol.k8s.aws/v1\nkind: ZoneDisruptionBudget\nmetadata:\n  name: <zdb-name>\nspec:\n  selector: <pod-selector>\n  maxUnavailable: 10%\n```\n\n## Installation\n\nThe controllers were built using the [kubebuilder](https://github.com/kubernetes-sigs/kubebuilder) framework. The kubebuilder based `Makefile` is available to use for development and deployment.\n\nTo build and push the controllers image to your container registry:\n\n```\nmake docker-build docker-push IMG=<image-url>\n```\n\nTo deploy the controllers to the K8s cluster specified in `~/.kube/config`:\n\n```\nmake deploy IMG=<image-url>\n```\n\nThe controllers will be deployed to the `zone-aware-controllers-system` namespace by default. The namespace used can be changed in `./config/default/kustomization.yaml` file.\n\nFinally, to undeploy the controllers:\n\n```\nmake undeploy \n```\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis project is licensed under the Apache-2.0 License.", "release_dates": []}]