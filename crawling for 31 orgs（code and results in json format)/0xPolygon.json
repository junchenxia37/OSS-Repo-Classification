[{"name": "account-abstraction-invoker", "description": "Modular account abstraction with EIP-3074.", "language": "TypeScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Account Abstraction Invoker\n\nExample account abstraction (EIP-3074) invoker contract. For demonstration purposes only.\n\n## About\n\nAccount Abstraction Invoker uses [`AUTH`](https://eips.ethereum.org/EIPS/eip-3074#auth-0xf6) and [`AUTHCALL`](https://eips.ethereum.org/EIPS/eip-3074#authcall-0xf7) opcodes introduced in [EIP-3074 ](https://eips.ethereum.org/EIPS/eip-3074) to delegate control of the externally owned account (EOA) to itself (smart contract). This adds more functionality to EOAs, such as batching capabilities, allowing for gas sponsoring, expirations, scripting, and beyond.\n\nUse cases are showcased in the [tests](test/AccountAbstractionInvoker.ts). The invoker works with \u2728 _all_ \u2728 contracts:\n\n<img alt=\"Sponsoring example\" src=\"./img/sponsoring-example.png\" width=\"693px\" />\n\n[Commit](https://eips.ethereum.org/EIPS/eip-3074#understanding-commit) is EIP-712 hash of the this [structure](scripts/signing/README.md). This means the invoker inherits the security of EIP-712, in addition to following the [Secure Invoker](https://eips.ethereum.org/EIPS/eip-3074#secure-invokers) recommendations and implementing additional security measures.\n\n## Requirements\n\n- Network with EIP-3074\n\n## Instructions\n\n### Quickstart\n\n```bash\ngit clone https://github.com/0xPolygon/account-abstraction-invoker\ncd account-abstraction-invoker\nyarn\n```\n\n### Setup\n\n```bash\ncp .env.example .env\n```\n\n- Set RPC URL in `.env`\n\n    Hardhat accounts `0` and `1` are included in `.env` for your convinience. Do not send real funds to those accounts.\n\n- Change chain ID in `hardhat.config.ts`\n\n### Test\n\nHardhat does not support EIP-3074 at the moment. All testing is done on a live network.\n\n```bash\nyarn hardhat test\n```\n\nTo redeploy contracts, set environment variable `REDEPLOY=true`. Otherwise, last deployed contracts will be used.\n\n## Acknowledgements\n\n- This example was based on Maarten Zuidhoorn's [EIP-3074 (Batch) Transaction Invoker](https://github.com/Mrtenz/transaction-invoker)\n- Reentrancy Guard by [OpenZeppelin](https://github.com/OpenZeppelin/openzeppelin-contracts)", "release_dates": []}, {"name": "account-binary-search-tree", "description": null, "language": "Solidity", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Account Binary Search Tree\n\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n[![CI Status](https://github.com/gretzke/account-binary-search-tree/actions/workflows/tests.yml/badge.svg)](https://github.com/gretzke/account-binary-search-tree/actions)\n\n## Contents\n\n- [Description](#description)\n- [Usage](#usage)\n- [API](#api)\n- [Build and Test](#build-and-test)\n- [Etherscan Verification](#etherscan-verification)\n- [Performance Optimizations](#performance-optimizations)\n\n## Description\n\nThis project demonstrates an implementation of a self balancing red-black binary search tree for usecases where accounts/addresses need to be sorted by their balance. This could be used inside of a game where players are sorted by their points or a staking implementation where stakers need to be sorted by their stake.\n\nIf an array or linked list was used for such a use case the insertion and deletion cost would be `O(n)` worst case which is not suitable for a smart contract implementation. With this self balancing binary implementation the insertion and deletion cost can be reduced to `O(log n)`. This library is based on [Rob Hitchens's (B9Labs) order statistics tree](https://github.com/rob-Hitchens/OrderStatisticsTree).\n\nDor more information on red-black trees see https://en.wikipedia.org/wiki/Red%E2%80%93black_tree\n\n## Usage\n\nThese contracts were originally developed for Polygon's [Core Contracts](https://github.com/0xPolygon/core-contracts), where they are used to maintain the list of validators and sort them by stake. You can read more about this implementation [here](https://github.com/0xPolygon/core-contracts/tree/main/contracts/libs#queue-pool-and-tree-libs-in-more-detail).\n\nAs you can see in both the example implementation in this repo and in Polygon's Core Contracts, you will generally use a two-contract approach, using `AccountStorage.sol` as a lib inside of a queue implemented to use it. (In the case of this repo, `DemoContract.sol`.)\n\nIn addition, the structs used throughout the tree are located in `contracts/interfaces/IAccount.sol`.\n\nThe example in this repo sorts accounts in a tree based on the amount of native asset (e.g. ETH on Ethereum mainnet) staked in the `DemoContract.sol` contract. We hope this example also helps illustrate how the tree could be used easily for ERC20 tokens or NFTs.\n\nThere are also basic tests to demonstrate basic usage.\n\n## API\n\nAll structs and functions have detailed natspec, more detail on these can be found there. This will be a brief overview of the function API for `AccountStorage`. Note that all functions are `internal`. This contract is meant to be consumed as a library. Here is the function API:\n\n- `function get(AccountTree storage self, address account) returns (Account storage)`\n- `function balanceOf(AccountTree storage self, address account) returns (uint256 balance)`\n- `function first(AccountTree storage self) returns (address _key)`\n- `function last(AccountTree storage self) returns (address _key)`\n- `function next(AccountTree storage self, address target) returns (address cursor)`\n- `function prev(AccountTree storage self, address target) returns (address cursor)`\n- `function exists(AccountTree storage self, address key) returns (bool)`\n- `function isEmpty(address key) returns (bool)`\n- `function getNode(AccountTree storage self, address key) returns (address _returnKey, address _parent, address _left, address _right, bool _red)`\n- `function insert(AccountTree storage self, address key, Account memory account)`\n- `function remove(AccountTree storage self, address key)`\n- `function treeMinimum(AccountTree storage self, address key) returns (address)`\n- `function treeMaximum(AccountTree storage self, address key) returns (address)`\n- `function rotateLeft(AccountTree storage self, address key)`\n- `function rotateRight(AccountTree storage self, address key)`\n- `function insertFixup(AccountTree storage self, address key)`\n- `function replaceParent(AccountTree storage self, address a, address b)`\n- `function removeFixup(AccountTree storage self, address key)`\n\nIn addition, the struct API for `Account`, used in `insert()` is:\n\n```\nstruct Account {\n  uint256 balance;\n  bool isActive;\n}\n```\n\n## Build and Test\n\nOn the project root, run:\n\n```bash\n$ npm i                 # install dependencies\n$ npm run compile       # compile contracts and generate typechain\n$ npm test              # run contract tests\n```\n\nTo run foundry tests for the library:\n\n```bash\n$ forge build           # compile contracts\n$ forge test            # run library tests\n```\n\noptional:\n\n```bash\n$ npm run coverage      # run test coverage tool\n```\n\n## Etherscan verification\n\nTo try out Etherscan verification, you first need to deploy a contract to an Ethereum network that's supported by Etherscan, such as Goerli.\n\nIn this project, copy the .env.example file to a file named .env, and then edit it to fill in the details. Enter your Etherscan API key, your Infura API key, and the mnemonic phrase of the account which will send the deployment transaction. With a valid .env file in place, first deploy your contract:\n\n```shell\nnpx hardhat run scripts/deploy.ts --network goerli\n```\n\nThen, copy the deployment address and paste it in to replace `DEPLOYED_CONTRACT_ADDRESS` in this command:\n\n```shell\nnpx hardhat verify --network goerli DEPLOYED_CONTRACT_ADDRESS \"Hello, Hardhat!\"\n```\n\n# Performance optimizations\n\nFor faster runs of your tests and scripts, consider skipping ts-node's type checking by setting the environment variable `TS_NODE_TRANSPILE_ONLY` to `1` in hardhat's environment. For more details see [the documentation](https://hardhat.org/guides/typescript.html#performance-optimizations).\n", "release_dates": []}, {"name": "agglayer", "description": null, "language": "Go", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Agglayer\n\nAgglayer is a web service that receives ZKPs from different CDK chains and checks the soundness of them before sending the ZKP to L1 for verification.\n\nTo find out more about Polygon, visit the [official website](https://wiki.polygon.technology/docs/cdk/).\n\nWARNING: This is a work in progress so architectural changes may happen in the future. The code is still being audited, so please contact the Polygon team if you would like to use it in production.\n\n## Getting Started\n\n### Prerequisites\n\nThis is an example of how to list things you need to use the software and how to install them.\n* docker\n* docker compose\n\n## Usage\n\n### Running in local with Docker\n\nRun\n```\nmake run-docker\n```\n\n## Key Signing configuration\n\n* Install polygon-cli `go install github.com/maticnetwork/polygon-cli@latest`\n* Create a new signature `polygon-cli signer create --kms GCP --gcp-project-id gcp-project --key-id mykey-tmp`\n* Install gcloud cli https://cloud.google.com/sdk/docs/install\n* Setup ADC `gcloud auth application-default login`\n* Configure `KMSKeyName` in `agglayer.toml`\n\n## Production setup\n\nCurrently only one instance of agglayer can be running at the same time, so it should be automatically started in the case of failure using a containerized setup or an OS level service manager/monitoring system.\n\n### Installation\n\n1. Clone the repo\n   ```sh\n   git clone https://github.com/0xPolygon/agglayer.git\n   ```\n3. Install Golang dependencies\n   ```sh\n   go install .\n   ```\n\n### Prerequisites\n\n* For each CDK chain it's necessary to configure it's corresponding RPC node, synced with the target CDK, this node is for checking the state root after executions of L2 batches.\n* It's recommended to have a durable HA PostgresDB for storage, prefer AWS Aurora Postgres or Cloud SQL for postgres in GCP.\n\n### Configuration of `agglayer.toml`\n    * Configure `[FullNodeRPCs]` to point to the corresponding L2 full node.\n    * Configure `[L1]` to point to the corresponding L1 chain.\n    * Configure the `[DB]` section with the managed database details.\n\n## License\nCopyright (c) 2024 PT Services DMCC\n\nLicensed under GNU Affero General Public License v3.0 or later ([LICENSE](https://spdx.org/licenses/AGPL-3.0-or-later.html))\n\nThe SPDX license identifier for this project is `AGPL-3.0-or-later`.\n\n### Contribution\n\nUnless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the AGPL-3.0-or-later, shall be licensed as above, without any additional terms or conditions.\n", "release_dates": []}, {"name": "bn254-test", "description": null, "language": "Go", "license": null, "readme": null, "release_dates": []}, {"name": "bnsnark1", "description": null, "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# bnsnark1\n\nGo mcl wrapper\n\n## Usage\n\nLook at the tests for the usage.\nMcl is used via static `*.a` files located inside `mclherumi/lib`.\n\n\n", "release_dates": []}, {"name": "cdk-data-availability", "description": null, "language": "Go", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# CDK Data Availability\n\n### Data Availability Layer for CDK Validium\n\nThe cdk-data-availability project is a specialized Data Availability Node (DA Node) that is part of Polygon's CDK (Chain Development Kit) Validium.\n\n## Overview of Validium\n\nFor a full overview of the Polygon CDK Validium, please reference the [CDK documentation](https://wiki.polygon.technology/docs/cdk/).\n\nThe CDK Validium solution is made up of several components; start with the [CDK Validium Node](https://github.com/0xPolygon/cdk-validium-node). For quick reference, the complete list of components are outlined below:\n\n| Component                                                                     | Description                                                          |\n| ----------------------------------------------------------------------------- | -------------------------------------------------------------------- |\n| [CDK Validium Node](https://github.com/0xPolygon/cdk-validium-node)           | Node implementation for the CDK networks in Validium mode            |\n| [CDK Validium Contracts](https://github.com/0xPolygon/cdk-validium-contracts) | Smart contract implementation for the CDK networks in Validium mode |\n| [CDK Data Availability](https://github.com/0xPolygon/cdk-data-availability)   | Data availability implementation for the CDK networks          |\n| [Prover / Executor](https://github.com/0xPolygonHermez/zkevm-prover)          | zkEVM engine and prover implementation                               |\n| [Bridge Service](https://github.com/0xPolygonHermez/zkevm-bridge-service)     | Bridge service implementation for CDK networks                       |\n| [Bridge UI](https://github.com/0xPolygonHermez/zkevm-bridge-ui)               | UI for the CDK networks bridge                                       |\n\n---\n\n## Introduction\n\nAs blockchain networks grow, the volume of data that needs to be stored and validated increases, posing challenges in scalability and efficiency. Storing all data on-chain can lead to bloated blockchains, slow transactions, and high fees.\n\nData Availability Nodes facilitate a separation between transaction execution and data storage. They allow transaction data to reside off-chain while remaining accessible for validation. This significantly improves scalability and reduces costs. Within the framework of Polygon's CDK, Data Availability Committees (DAC) members run DA nodes to ensure the security, accessibility, and reliability of off-chain data.\n\nTo learn more about how the data availability layer works in the validium, please see the CDK documentation [here](https://wiki.polygon.technology/docs/cdk/dac-overview/).\n\n### Off-Chain Data\n\nThe off-chain data is stored in a distributed manner and managed by a data availability committee, ensuring that it is available for validation. The data availability committee is defined as a core smart contract, available [here](https://github.com/0xPolygon/cdk-validium-contracts/blob/main/contracts/CDKDataCommittee.sol). This is crucial for the Validium model, where data computation happens off-chain but needs to be verifiable on-chain.\n\n### Running\n\nInstructions on how to run this software can be found [here](./docs/running.md)\n\n## License\n\nThe cdk-validium-node project is licensed under the [GNU Affero General Public License](LICENSE) free software license.\n", "release_dates": []}, {"name": "cdk-validium-contracts", "description": null, "language": "JavaScript", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Polygon CDK Validium Contracts\n### Core Contracts for the Polygon CDK Validium\n\nThe cdk-validium-contracts repository contains the smart contract implementations designed for use with CDK chains configured with Validium.\n\n## Overview of Validium\n\nFor a full overview of the Polygon CDK Validium, please reference the [CDK documentation](https://wiki.polygon.technology/docs/cdk/).\n\nThe CDK Validium solution is made up of several components; start with the [CDK Validium Node](https://github.com/0xPolygon/cdk-validium-node). For quick reference, the complete list of components are outlined below:\n\n| Component                                                                     | Description                                                          |\n| ----------------------------------------------------------------------------- | -------------------------------------------------------------------- |\n| [CDK Validium Node](https://github.com/0xPolygon/cdk-validium-node)           | Node implementation for the CDK networks in Validium mode            |\n| [CDK Validium Contracts](https://github.com/0xPolygon/cdk-validium-contracts) | Smart contract implementation for the CDK networks in Validium mode |\n| [CDK Data Availability](https://github.com/0xPolygon/cdk-data-availability)   | Data availability implementation for the CDK networks          |\n| [Prover / Executor](https://github.com/0xPolygonHermez/zkevm-prover)          | zkEVM engine and prover implementation                               |\n| [Bridge Service](https://github.com/0xPolygonHermez/zkevm-bridge-service)     | Bridge service implementation for CDK networks                       |\n| [Bridge UI](https://github.com/0xPolygonHermez/zkevm-bridge-ui)               | UI for the CDK networks bridge                                       |\n\n---\n\n## Important Note\n\nThe private keys and mnemonics included in this repository are intended solely for internal testing. **Do not use them in production environments.**\n\n## Prerequisites\n\n- Node.js version: 16.x\n- npm version: 7.x\n\n## Repository Structure\n\n- `contracts`: Core contracts\n  - `PolygonZkEVMBridge.sol`: Facilitates asset transfers between chains\n    - `PolygonZkEVMGlobalExitRoot.sol`: Manages the global exit root on L1\n    - `PolygonZkEVMGlobalExitRootL2.sol`: Manages the global exit root on L2\n  - `CDKValidium.sol`: Consensus algorithm for Validium CDK chains\n- `docs`: Specifications and useful resources\n- `test`: Contract test suites\n\n## Activate Github Hook\n\nTo activate the GitHub hook, run the following command:\n\n```bash\ngit config --local core.hooksPath .githooks/\n```\n\n## Install\n\n```bash\nnpm i\n```\n\n## Run Tests\n\nExecute the test suite with:\n\n```bash\nnpm run test\n```\n\n## Linting\n\nTo check for linting errors, run:\n\n```bash\nnpm run lint\n```\n\nTo automatically fix linting errors, run:\n\n```bash\nnpm run lint:fix\n```\n\n## Build Docker Image\n\nTo build the Docker image, run:\n\n```bash\nnpm run docker:contracts\n```\n\nThis will create a new Docker image named `hermeznetwork/geth-cdk-validium-contracts`, which includes a Geth node with the deployed contracts. The deployment output can be found at `docker/deploymentOutput/deploy_output.json`.\n\nTo run the Docker container, use:\n\n```bash\ndocker run -p 8545:8545 hermeznetwork/geth-cdk-validium-contracts\n```\n\n## Note\n\nFor testing purposes, the following private keys are being used. These keys are not intended for production use:\n\n- **Private key**: 0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80\n- **Address**: 0xf39fd6e51aad88f6f4ce6ab8827279cfffb92266\n- **Private key**: 0xdfd01798f92667dbf91df722434e8fbe96af0211d4d1b82bbbbc8f1def7a814f\n- **Address**: 0xc949254d682d8c9ad5682521675b8f43b102aec4\n\n## License\n\nThe cdk-validium-contracts project is licensed under the [GNU Affero General Public License](LICENSE) free software license.\n", "release_dates": []}, {"name": "cdk-validium-node", "description": "Go implementation of a node that operates the Polygon zkEVM Network", "language": "Go", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "readme": "# Polygon CDK Validium Node\n\nFor a full overview of the CDK-Validium please reference the [CDK documentation](https://wiki.polygon.technology/docs/cdk/).\n\nThe CDK-Validium solution is made up of several components, start with the [CDK Validium Node](https://github.com/0xPolygon/cdk-validium-node). However, for quick reference, the complete list of components are outlined below:\n\n| Component                                                                     | Description                                                          |\n| ----------------------------------------------------------------------------- | -------------------------------------------------------------------- |\n| [CDK Validium Node](https://github.com/0xPolygon/cdk-validium-node)           | Node implementation for the CDK networks in Validium mode            |\n| [CDK Validium Contracts](https://github.com/0xPolygon/cdk-validium-contracts) | Smart contract implementation for the CDK networks in Validium mode |\n| [CDK Data Availability](https://github.com/0xPolygon/cdk-data-availability)   | Data availability implementation for the CDK networks          |\n| [Prover / Executor](https://github.com/0xPolygonHermez/zkevm-prover)          | zkEVM engine and prover implementation                               |\n| [Bridge Service](https://github.com/0xPolygonHermez/zkevm-bridge-service)     | Bridge service implementation for CDK networks                       |\n| [Bridge UI](https://github.com/0xPolygonHermez/zkevm-bridge-ui)               | UI for the CDK networks bridge                                       |\n\nUnderstanding the underlying protocol is crucial when working with an implementation. This project is based on the Polygon zkEVM network, which is designed to bring scalability to Ethereum-compatible blockchains.\n\nFor an in-depth understanding of the protocol\u2019s specifications, please refer to the [zkEVM Protocol Overview](https://wiki.polygon.technology/docs/zkevm/)\n\n## Run a CDK Validium\n\n> This repo is a fork of the [zkevm-node](https://github.com/0xPolygonHermez/zkevm-node), more information and code diff explained [here](./docs/diff/diff.md)\n\n### Development\n\n> ARM devices (such as Apple M1 and M2) are not supported\n\nFor a streamlined development experience, it\u2019s highly recommended to utilize the make utility for tasks such as building and testing the code. To view a comprehensive list of available commands, simply execute `make help` in your terminal.\n\nThis step by step guide will result in a local environment that has everything needed to test and develop on a CDK Validium, but note that:\n\n- everything will be run on a ephemeral and local L1 network, once the environment is shutdown, all progress will be lost\n- ZK Proofs are mocked\n- Bridge service and UI is not included as part of this setup, instead there is a pre-funded account\n\n#### Steps\n\n1. Clone this GitHub repository to your local machine:\n\n```\ngit clone https://github.com/0xPolygon/cdk-validium-node.git\n```\n\n2. Navigate to the cloned directory:\n\n```\ncd cdk-validium-node\n```\n\n3. Build the Docker image using the provided Dockerfile:\n\n```\nmake build-docker\n```\n\n4. Navigate to the test directory:\n\n```\ncd test\n```\n\n5. Run all needed components:\n\n```\nmake run\n```\n\n#### Usage\n\n- L2 RPC endpoint: `http://localhost:8123`\n- L2 Chain ID: 1001\n- L1 RPC endpoint: `http:localhost:8545`\n- L1 Chain ID: 1337\n- Pre funded account private key: `0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80`\n\n#### Troubleshooting\n\nEverything is run using docker, so if anything is not working, first thing is to identify what containers are running or not:\n\n```\ndocker compose ps\n```\n\nThen check the logs:\n\n```\ndocker logs <problematic container, example: cdk-validium-sync>\n```\n\nAditionaly, it can be worth checking the DBs:\n\n- StateDB: `psql -h localhost -p 5432 -U state_user state_db`, password: `state_password`\n- PoolDB: `psql -h localhost -p 5433 -U pool_user pool_db`, password: `pool_password`\n\n#### Advanced config\n\nIn order to go beyond the default configuration, you can edit the config files:\n\n- `./test/config/test.node.config.toml`: configuration of the node, documented [here](./docs/config-file/node-config-doc.md)\n- `./test/config/test.genesis.config.toml`: configuration of the network, documented [here](./docs/config-file/custom_network-config-doc.md)\n- `./test/config/test.prover.config.json`: configuration of the prover/executor\n\n## Key Components\n\n### Aggregator\n\nThe Aggregator is responsible for submitting validity proofs of the L2 state to L1. To do so, it fetches the batches sequencced by the sequencer, and interacts with the provers to generate the ZeroKnowledge Proofs (ZKPs).\n\nTo do so in a efficient way, the Aggregator will:\n\n- Orchestrate communication with one or multiple provers\n- Aggregate proofs from many batches, a single proof can verify multiple batches\n- Send the aggregated proof to L1 using the EthTxManager\n\n### Prover\n\nThe Prover is tasked with generating proofs for the batched transactions. These proofs are essential for the subsequent validation of the transactions on the Ethereum mainnet. In general:\n\n- ZKP Generation: Creates cryptographic proofs for each batch of transactions or for a combination of batches (proof aggregation).\n- Optimization: Utilizes parallel computing to speed up the proof generation process.\n- Ethereum Mainnet Preparation: Formats the proofs for validation on the Ethereum mainnet.\n\nNote that this software is not implemented in this repo, but in [this one](https://github.com/0xPolygonHermez/zkevm-prover)\n\n### Sequencer\n\nThe Sequencer is responsible for ordering transactions, in other words, making the state move forward:\n\n- Transaction Ordering: Get transactions from the pool and adds them into the state.\n- State Transition: Collaborates with the Executor to process transactions and update the state.\n- Trusted finality: Once the sequencer has added a transaction into the state, it shares this information with other nodes, making the transaction final. Other nodes will need to trust that this transaction is added into the state until they get data availability (DA) and validity (ZKPs) confirmations\n\n### SequenceSender\n\nThe SequenceSender\u2019s role is to send the ordered list of transactions, known as a sequence, to the Ethereum mainnet. It also collaborates with the Data Availability layer, ensuring that all transaction data is accessible off-chain. It plays a pivotal role in finalizing the rollup:\n\n- Sequence Transmission: Sends a fingerprint of the ordered transaction batches to the Ethereum mainnet.\n- Data Availability: Works in tandem with the Data Availability layer to ensure off-chain data is accessible.\n- L1 Interaction: Utilizes the EthTxManager to handle L1 transaction nuances like nonce management and gas price adjustments.\n\n### Synchronizer\n\nThe Synchronizer keeps the node\u2019s local state in sync with the Ethereum mainnet. It listens for events emitted by the smart contract on the mainnet and updates the local state to match. The Synchronizer acts as the bridge between the Ethereum mainnet and the node:\n\n- Event Listening: Monitors events emitted by the smart contract on the Ethereum mainnet.\n- Data Availability: downloads data from the Data Availability layer based on L1 events\n- State Updating: Aligns the local state with the mainnet, ensuring consistency.\n- Reorg Handling: Detects and manages blockchain reorganizations to maintain data integrity.\n\n### Data Availability Configuration\n\nThe Data Availability (DA) layer is a crucial component that ensures all transaction data is available when needed. This off-chain storage solution is configurable, allowing operators to set parameters that best suit their needs. The DA layer is essential for the Validium system, where data availability is maintained off-chain but can be made available for verification when required. In general:\n\n- Off-Chain Storage: Stores all transaction data off-chain but ensures it\u2019s readily available for verification.\n- Configurability: Allows chain operators to customize data storage parameters.\n- Data Verification: Provides mechanisms for data integrity checks, crucial for the Validium model.\n\n### Executor\n\nThe Executor is the state transition implementation, in this case a EVM implementation:\n\n- Batch execution: receives requests to execute batch of transactions.\n- EVM Implementation: Provides an EVM-compatible implementation for transaction processing.\n- Metadata Retrieval: Retrieves necessary metadata like state root, transaction receipts, and logs from the execution.\n\nNote that this software is not implemented in this repo, but in [this one](https://github.com/0xPolygonHermez/zkevm-prover)\n\n### EthTxManager\n\nThe EthTxManager is crucial for interacting with the Ethereum mainnet:\n\n- L1 Transaction Handling: Manages requests from the SequenceSender and Aggregator to send transactions to L1.\n- Nonce Management: Takes care of the nonce for each account involved in a transaction.\n- Gas Price Adjustment: Dynamically adjusts the gas price to ensure that transactions are mined in a timely manner.\n\n### State\n\nThe State component is the backbone of the node\u2019s data management:\n\n- State Management: Handles all state-related data, including batches, blocks, and transactions.\n- Executor Integration: Communicates with the Executor to process transactions and update the state.\n- StateDB: used for persistance\n\n### Pool\n\nThe Pool serves as a temporary storage for transactions:\n\n- Transaction Storage: Holds transactions submitted via the RPC.\n- Sequencer Interaction: Provides transactions to the Sequencer for ordering and batch creation.\n\n### JSON RPC\n\nThe JSON RPC serves as the HTTP interface for user interaction:\n\n- User Interface: Allows users and dApps to interact with the node, following the Ethereum standard:\n    - [Endpoint compatibility](./docs/json-rpc-endpoints.md)\n    - [Custom endpoints](./docs/zkEVM-custom-endpoints.md)\n- State Interaction: Retrieves data from the state and processes transactions.\n- Pool Interaction: Stores transactions in the pool.\n\n### L2GasPricer\n\nThe L2GasPricer is responsible for calculating the gas price on L2 based on the L1 gas price:\n\n- L1 Gas Price Fetching: Retrieves the current L1 gas price.\n- Gas Price Calculation: Applies a formula to calculate the suggested L2 gas price.\n- Pool Storage: Stores the calculated L2 gas price in the pool for consumption by the rpc.\n\n## Contribute\n\nBefore opening a pull request, please read [this guide](./CONTRIBUTING.md).\n\n## License\n\nThe cdk-validium-node project is licensed under the GNU Affero General Public License free software license.\n", "release_dates": []}, {"name": "chain-indexer-framework", "description": "Chain-indexer-framework", "language": "TypeScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Chain Indexer Framework - Blockchain Data Indexer\n\n[![GitHub version](https://badge.fury.io/gh/maticnetwork%2Fchain-indexer-framework.svg)](https://badge.fury.io/gh/maticnetwork%2Fchain-indexer-framework)\n![Build Status](https://github.com/0xPolygon/chain-indexer-framework/workflows/CI/badge.svg?branch=main)\n[![npm version](https://badge.fury.io/js/%40maticnetwork%2Fchain-indexer-framework.svg)](https://www.npmjs.com/package/@maticnetwork/chain-indexer-framework)\n![GitHub](https://img.shields.io/github/license/0xPolygon/chain-indexer-framework)\n[![TEST](https://github.com/0xPolygon/chain-indexer-framework/actions/workflows/tests.yml/badge.svg)](https://github.com/0xPolygon/chain-indexer-framework/actions/workflows/tests.yml)\n\n\nChain Indexer Framework, is a powerful framework designed for the development of flexible event-driven data pipelines on EVM blockchains. Built on the reliable foundation of Kafka, Chain Indexer Framework empowers developers to build robust and scalable applications that seamlessly process blockchain events and enable real-time data integration.\n\nIn today's rapidly evolving blockchain ecosystem, the need for efficient and reliable data processing is paramount. EVM (Ethereum Virtual Machine) blockchains, such as Ethereum itself and its compatible networks, have gained significant traction due to their smart contract capabilities and decentralized nature. However, working with blockchain data at scale and in real time presents unique challenges.\n\nChain Indexer Framework addresses these challenges by providing a comprehensive Node.js package that simplifies the development of event-driven data pipelines. With its intuitive design and seamless integration with Kafka, one of the most popular and battle-tested distributed streaming platforms, Chain Indexer Framework offers a robust and reliable infrastructure for processing blockchain events efficiently.\n\n## Key Features\n\n- **Event-driven Architecture:** Chain Indexer Framework embraces the power of event-driven architecture, allowing developers to create pipelines that react to blockchain events in real time. By leveraging this approach, applications built with Chain Indexer Framework can easily respond to changes on the blockchain, enabling near-instantaneous data processing.\n- **Flexible Data Pipelines:** Chain Indexer Framework offers a flexible and extensible framework for building data pipelines that suit your specific needs. Developers can easily define their desired data flow, including event filtering, transformation, and aggregation, by utilizing the feature set of Chain Indexer Framework.\n- **Seamless Integration with Kafka:** As the backbone of Chain Indexer Framework, Kafka provides the necessary infrastructure for handling high-throughput, fault-tolerant, and scalable data streams. Chain Indexer Framework's integration with Kafka ensures reliable data processing and enables seamless interoperability with other Kafka-based systems, further enhancing the versatility of your data pipelines.\n- **EVM Blockchain Compatibility:** Chain Indexer Framework is specifically designed for EVM blockchains, enabling developers to harness the power of smart contracts and decentralized applications. Whether you are working with Ethereum or any other EVM-compatible blockchain, Chain Indexer Framework provides a unified and consistent approach to processing blockchain events across different networks. Chain Indexer Framework can also be used for other chains with custom implementations of the provided interfaces and abstract classes.\n\nWith Chain Indexer Framework, you can unlock the true potential of EVM blockchains by seamlessly integrating them into your data infrastructure. Whether you are building real-time analytics, decentralized applications, or any other data-driven solution, this documentation will guide you through the intricacies of using Chain Indexer Framework's packages and assist you in developing robust and efficient event-driven data pipelines on EVM blockchains.\n\n## Installation\n\nYou can install the package using [NPM](https://www.npmjs.com/package/@maticnetwork/chain-indexer-framework)\n\n### Using NPM\n\n```bash\nnpm install @maticnetwork/chain-indexer-framework\n```\n\n## Usage\n\n```typescript\n// Import the chain-indexer-framework module\nconst chain-indexer-framework = require('@maticnetwork/chain-indexer-framework');\n```\n\nYou will learn more about usage as we go through the doc below.\n\n## Architecture\n\nChain Indexer Framework's architecture is composed of three main layers: block producers, transformers, and consumers. Each layer plays a crucial role in processing and transforming blockchain events to facilitate various use cases.\n\n1. **Block Producers:**\nBlock producers are responsible for publishing raw block data to Kafka topics. This raw block data serves as the primary source of events for all use cases within Chain Indexer Framework. The block producers handle important tasks such as handling blockchain reorganizations (re-orgs) and backfilling of blocks. This ensures that developers only need to focus on specifying the desired RPC (Remote Procedure Call) endpoints they want to provide, without worrying about the intricacies of block management.\n\nBy leveraging the block producers, developers can easily connect to blockchain networks, retrieve the latest blocks, and publish them to Kafka topics, establishing a reliable and continuous stream of blockchain events.\n\n2. **Transformers:**\nThe transformers layer plays a critical role in transforming the raw block data into domain-specific events. These transformers take the raw block data from the Kafka topics and apply various transformations to create meaningful and specialized events. The transformations can encompass a wide range of operations such as data enrichment, filtering, aggregation, or any other processing logic required for a specific use case.\n\nThe transformed domain-specific events are then published to their respective Kafka topics, enabling easy replay of events whenever required. This layer allows for flexibility and customization, ensuring that the events generated align with the specific needs of the applications and services built on top of Chain Indexer Framework.\n\n3. **Consumers:**\nConsumers are responsible for responding to the events published by the transformers layer. They are designed to receive and process the domain-specific events, triggering actions based on the requirements of the specific service or application. The consumers can be tailored to perform various tasks such as building API endpoints, indexing data to data warehouses for analytics, or notifying events to frontend applications.\n\nBy leveraging the events generated by the transformers, consumers can react in real time to changes on the blockchain, enabling seamless integration with other services and systems. This layer empowers developers to build powerful and dynamic applications that respond to blockchain events efficiently.\n\nTogether, these three layers form the foundation of Chain Indexer Framework, providing a comprehensive framework for building flexible and scalable event-driven data pipelines on EVM blockchains. Whether it's building real-time analytics, decentralized applications, or any other data-driven solution, Chain Indexer Framework's architecture offers the necessary tools and abstractions to streamline the development process and unlock the full potential of EVM blockchains.\n\n## Examples\nTo gain a clearer understanding of the entire process, let's consider straightforward [examples](./examples/README.md) \n- First example involves indexing [MATIC transfer](./examples/matic_transfer/README.md) events from the Ethereum blockchain.\n- Second example involes indexing NFT Transfer and maintaining [NFT Balance](./examples/nft_balancer/README.md)\n\nBoth these examples encompasses all the layers involved, starting from producers, moving through transformers, and concluding with consumers.\n\n## Producers\n\n### Block Producers\n\nBlock producers play a critical role in the seamless publishing of raw block data to Kafka topics. It is essential to recognize that each blockchain network has its own unique implementation, and the functionality of Remote Procedure Calls (RPCs) can differ across these networks. In the current implementation, all block producers leverage the SynchronousProducerClass from the Kafka wrapper classes.\n\nChain Indexer Framework block producers encompass three distinct types of producers, each designed to cater to the specific requirements of different blockchain networks.\n\n1. **BlockPollingProducer**: This producer employs a polling method to continuously check the blockchain for new blocks. If the blockchain is already in sync, it waits for the completion of a predefined timeout before initiating the polling process again. This method is straightforward and involves utilizing basic RPC calls commonly found in every node. Additionally, it serves as an ideal choice when the web socket (wss) node is unavailable for a particular blockchain network.\n\n    ```typescript\n    // Import the required module\n    import { BlockPollerProducer } from \"@maticnetwork/chain-indexer-framework/block_producers/block_polling_producer\";\n\n    // Set up and start the Block Poller Producer\n    const producer = new BlockPollerProducer({\n        startBlock: '<START_BLOCK as number>',\n        rpcWsEndpoints: ['<HTTP_PROVIDER_1>', '<HTTP_PROVIDER_2>'],\n        blockPollingTimeout: '<BLOCK_POLLING_TIMEOUT as string>',\n        topic: '<PRODUCER_TOPIC>',\n        maxReOrgDepth: '<MAX_REORG_DEPTH>',\n        maxRetries: '<MAX_RETRIES>',\n        mongoUrl: '<MONGO_DB_URL>',\n        \"bootstrap.servers\": '<KAFKA_CONNECTION_URL>',\n        \"security.protocol\": \"plaintext\"\n    })\n\n    // Handle fatal error\n    producer.on(\"blockProducer.fatalError\", (error) => {\n        console.error(`Block producer exited. ${error.message}`);\n        process.exit(1); // Exiting process on fatal error. Process manager needs to restart the process.\n    });\n    \n    // Start the producer\n    producer.start().catch((error) => {\n        console.error(error);\n    });\n\n\n    // Or use the functional API\n\n\n    // Import the required module\n    import { produce } from \"@maticnetwork/chain-indexer-framework/kafka/producer/produce\";\n    import { BlockPollerProducer } from \"@maticnetwork/chain-indexer-framework/block_producers/block_polling_producer\";\n\n    // Set up the Block Poller Producer\n    const producer = produce<BlockPollerProducer>({\n        startBlock: '<START_BLOCK as number>',\n        rpcWsEndpoints: ['<HTTP_PROVIDER_1>', '<HTTP_PROVIDER_2>'],\n        blockPollingTimeout: '<BLOCK_POLLING_TIMEOUT as string>',\n        topic: '<PRODUCER_TOPIC>',\n        maxReOrgDepth: '<MAX_REORG_DEPTH>',\n        maxRetries: '<MAX_RETRIES>',\n        mongoUrl: '<MONGO_DB_URL>',\n        \"bootstrap.servers\": '<KAFKA_CONNECTION_URL>',\n        \"security.protocol\": \"plaintext\",\n        blockSubscriptionTimeout: 120000,\n        type: 'blocks:poller',\n        {\n            error: (error: KafkaError | BlockProducerError) => {},\n            closed: () => {} // On broker connection closed\n        }\n    });\n    ```\n\n2. **ErigonBlockProducer**: This particular block producer operates by subscribing to blocks through a web socket providers and utilizes an erigon node to produce data for Kafka. What sets the erigon node apart from other nodes is its unique characteristic of not requiring separate calls for obtaining receipts for each transaction. Instead, it efficiently retrieves block details with just two calls: one to retrieve the overall block information and another to fetch all the transaction details within that block. While the former call is commonly used across various nodes, the latter, that used the `eth_getBlockReceipts` method, distinguishes the erigon node from others in the network.\n\n    ```typescript\n    // Import the required module\n    import { ErigonBlockProducer } from \"@maticnetwork/chain-indexer-framework/block_producers/erigon_block_producer\";\n\n    // Set up the Erigon Block Producer\n    const producer = new ErigonBlockProducer({\n        startBlock: '<START_BLOCK as number>',\n        rpcWsEndpoints: ['<RPC_WS_ENDPOINT_URL_LIST_1>', '<RPC_WS_ENDPOINT_URL_LIST_2>'],\n        blockPollingTimeout: '<BLOCK_POLLING_TIMEOUT as string>',\n        topic: '<PRODUCER_TOPIC>',\n        maxReOrgDepth: '<MAX_REORG_DEPTH>',\n        maxRetries: '<MAX_RETRIES>',\n        mongoUrl: '<MONGO_DB_URL>',\n        \"bootstrap.servers\": '<KAFKA_CONNECTION_URL>',\n        \"security.protocol\": \"plaintext\"\n    })\n\n    // Handle fatal error\n    producer.on(\"blockProducer.fatalError\", (error) => {\n        console.error(`Block producer exited. ${error.message}`);\n        process.exit(1); // Exiting process on fatal error. Process manager needs to restart the process.\n    });\n    \n    // Start the producer\n    producer.start().catch((error) => {\n        console.error(error);\n    });\n\n\n    // Or use the functional API\n\n\n    // Import the required module\n    import { produce } from \"@maticnetwork/chain-indexer-framework/kafka/producer/produce\";\n    import { ErigonBlockProducer } from \"@maticnetwork/chain-indexer-framework/block_producers/erigon_block_producer\";\n\n    // Set up the Erigon Block Producer\n    const producer = produce<ErigonBlockProducer>({\n        startBlock: '<START_BLOCK as number>',\n        rpcWsEndpoints: ['<HTTP_PROVIDER_1>', '<HTTP_PROVIDER_2>'],\n        blockPollingTimeout: '<BLOCK_POLLING_TIMEOUT as string>',\n        topic: '<PRODUCER_TOPIC>',\n        maxReOrgDepth: '<MAX_REORG_DEPTH>',\n        maxRetries: '<MAX_RETRIES>',\n        mongoUrl: '<MONGO_DB_URL>',\n        \"bootstrap.servers\": '<KAFKA_CONNECTION_URL>',\n        \"security.protocol\": \"plaintext\",\n        blockSubscriptionTimeout: 120000,\n        type: 'blocks:erigon',\n        {\n            error: (error: KafkaError | BlockProducerError) => {},\n            closed: () => {} // On broker connection closed\n        }\n    });\n    ```\n\n3. **QuickNodeBlockProducer**: The highly optimized block producer exclusively runs on QuickNode RPCs, utilizing the same web socket provider. However, what sets it apart is that QuickNode exposes the `qn_getBlockWithReceipts` method, which streamlines the process of retrieving both block details and transaction information within a single call. Unlike the ErigonBlockProducer, which necessitates two separate calls, or the BlockPollingProducer, which requires multiple calls, this method significantly simplifies the data retrieval process. To utilize this producer effectively, it is essential for QuickNode to support the `qn_getBlockWithReceipts` method for the desired chain.\n\n    ```typescript\n    // Import the required module\n    import { QuickNodeBlockProducer } from \"@maticnetwork/chain-indexer-framework/block_producers/quicknode_block_producer\";\n\n    // Set up the QuickNode Block Producer\n    const producer = new QuickNodeBlockProducer({\n        startBlock: '<START_BLOCK as number>',\n        rpcWsEndpoints: ['<RPC_WS_ENDPOINT_URL_LIST_1>', '<RPC_WS_ENDPOINT_URL_LIST_2>'],\n        blockPollingTimeout: '<BLOCK_POLLING_TIMEOUT as string>',\n        topic: '<PRODUCER_TOPIC>',\n        maxReOrgDepth: '<MAX_REORG_DEPTH>',\n        maxRetries: '<MAX_RETRIES>',\n        mongoUrl: '<MONGO_DB_URL>',\n        \"bootstrap.servers\": '<KAFKA_CONNECTION_URL>',\n        \"security.protocol\": \"plaintext\"\n    })\n\n    producer.on(\"blockProducer.fatalError\", (error) => {\n        console.error(`Block producer exited. ${error.message}`);\n        process.exit(1); // Exiting process on fatal error. Process manager needs to restart the process.\n    });\n\n    producer.start().catch((error) => {\n        console.error(error);\n    });\n\n\n    // Or use the functional API\n\n\n    // Import the required module\n    import { produce } from \"@maticnetwork/chain-indexer-framework/kafka/producer/produce\";\n    import { QuickNodeBlockProducer } from \"@maticnetwork/chain-indexer-framework/block_producers/quicknode_block_producer\";\n\n    // Set up the QuickNode Block Producer\n    const producer = produce<QuickNodeBlockProducer>({\n        startBlock: '<START_BLOCK as number>',\n        rpcWsEndpoints: ['<HTTP_PROVIDER_1>', '<HTTP_PROVIDER_2>'],\n        blockPollingTimeout: '<BLOCK_POLLING_TIMEOUT as string>',\n        topic: '<PRODUCER_TOPIC>',\n        maxReOrgDepth: '<MAX_REORG_DEPTH>',\n        maxRetries: '<MAX_RETRIES>',\n        mongoUrl: '<MONGO_DB_URL>',\n        \"bootstrap.servers\": '<KAFKA_CONNECTION_URL>',\n        \"security.protocol\": \"plaintext\",\n        blockSubscriptionTimeout: 120000,\n        type: 'blocks:quicknode',\n        {\n            error: (error: KafkaError | BlockProducerError) => {},\n            closed: () => {} // On broker connection closed\n        }\n    });\n\n    ```\n\n### Synchronous Producer\n    \nThe **Synchronous Producer** class is recommended when the utmost importance is placed on data integrity and the tolerance for potential delays is higher than the risk of data loss. This class ensures that even if the service encounters downtime or disruptions, it will resume publishing from the exact point it left off. By maintaining a synchronous nature, it guarantees the completion of each message before moving on to the next, prioritizing data integrity and consistency.\n\n```typescript\n// Import the required modules\nimport { SynchronousProducer } from \"@maticnetwork/chain-indexer-framework/kafka/producer/synchronous_producer\";\nimport { Coder } from \"@maticnetwork/chain-indexer-framework/coder/protobuf_coder\";\n\n// Initialize the Kafka producer\nconst producer = new SynchronousProducer(\n    {\n        topic: \"<PRODUCER_TOPIC>\",\n        \"bootstrap.servers\": \"<KAFKA_CONNECTION_URL>\",\n        \"security.protocol\": \"plaintext\",\n        \"message.max.bytes\": 26214400,\n        coder: {\n            fileName: \"matic_transfer\",\n            packageName: \"matictransferpackage\",\n            messageType: \"MaticTransferBlock\"\n        }\n    }\n);\n\n// Starting the Producer\nproducer.start();\n\n// Send an event to the Kafka topic\nproducer.produceEvent(\"<key: string>\", \"<message: object>\");\n\n\n// Or use the functional API\n\n\n// Import the required modules\nimport { produce } from \"@maticnetwork/chain-indexer-framework/kafka/producer/produce\";\nimport { SynchronousProducer } from \"@maticnetwork/chain-indexer-framework/kafka/producer/synchronous_producer\";\n\n// Initialize and start the Kafka producer\nconst producer = produce<SynchronousProducer>(\n    {\n        topic: \"<PRODUCER_TOPIC>\",\n        \"bootstrap.servers\": \"<KAFKA_CONNECTION_URL>\",\n        \"security.protocol\": \"plaintext\",\n        \"message.max.bytes\": 26214400,\n        coder: {\n            fileName: \"matic_transfer\",\n            packageName: \"matictransferpackage\",\n            messageType: \"MaticTransferBlock\",\n        },\n        type: \"synchronous\", // use 'synchronous'. if synchronous producer is needed\n        {\n            emitter: () => {\n                this.produceEvent(\"<key: string>\", \"<message: object>\");\n            },\n            error: (error: KafkaError | BlockProducerError) => {},\n            closed: () => {} // On broker connection closed\n        }\n    }\n)\n```\n\n### Asynchronous Producer\n\nThe **Asynchronous Producer** class is designed for scenarios where maximizing throughput and minimizing latency are paramount. This class prioritizes speed and efficiency by allowing rapid publication of data. However, it should be noted that using the asynchronous approach carries a higher risk of data loss. If the service experiences an interruption, the previous batch will be considered published, even if it wasn't fully transmitted.\n\n```typescript\n// Import the required modules\nimport { AsynchronousProducer } from \"@maticnetwork/chain-indexer-framework/kafka/producer/asynchronous_producer\";\n\n// Initialize the asynchronous Kafka producer\nconst producer = new AsynchronousProducer(\n    {\n        topic: \"<PRODUCER_TOPIC>\",\n        \"bootstrap.servers\": \"<KAFKA_CONNECTION_URL>\",\n        \"security.protocol\": \"plaintext\",\n        \"message.max.bytes\": 26214400,\n        coder: {\n            fileName: \"matic_transfer\",\n            packageName: \"matictransferpackage\",\n            messageType: \"MaticTransferBlock\"\n        }\n    }\n);\n\n// Start the producer\nproducer.start();\n\n// Send an event to the Kafka topic\nproducer.produceEvent(\"<key: string>\", \"<message: object>\");\n\n\n// Or use the functional API\n\n\n// Import the required modules\nimport { produce } from \"@maticnetwork/chain-indexer-framework/kafka/producer/produce\";\nimport { AsynchronousProducer } from \"@maticnetwork/chain-indexer-framework/kafka/producer/asynchronous_producer\";\n\n// Initialize and start the Kafka producer\nconst producer = produce<AsynchronousProducer>(\n    {\n        topic: \"<PRODUCER_TOPIC>\",\n        \"bootstrap.servers\": \"<KAFKA_CONNECTION_URL>\",\n        \"security.protocol\": \"plaintext\",\n        \"message.max.bytes\": 26214400,\n        coder: {\n            fileName: \"matic_transfer\",\n            packageName: \"matictransferpackage\",\n            messageType: \"MaticTransferBlock\"\n        },\n        type: \"asynchronous\",\n        {\n            emitter: () => {\n                this.produceEvent(\"<key: string>\", \"<message: object>\");\n            },\n            error: (error: KafkaError | BlockProducerError) => {},\n            closed: () => {} // On broker connection closed\n        }\n    }\n)\n```\n\n\n## Transformers\n\nTransformers play a vital role as the essential second layer in the architecture, tasked with the crucial task of converting blockchain data into domain-specific events. Each application has its own unique requirements and preferences for event indexing, necessitating the creation of distinct transformers for each event type. These transformers consume the blockchain data produced by the producer layer from Kafka topics, diligently filter the blocks based on the targeted events for indexing, and proceed to transform the data in a manner that aligns with the application's desired storage format. The transformed data is then published to new Kafka topics, specifically dedicated to those specific events. This process ensures that the transformed data is organized and made available exclusively for the intended events, catering to the specific needs of different applications.\n\nThe Transformer layer serves as a pivotal component where both the producer and consumer wrapper classes of Kafka are utilized. This layer is composed of three integral parts:\n\n1. Consuming the block events from Kafka that were produced by the Block Producer layer.\n2. Mapping and transforming the events/data from the raw block format into a domain-specific structure, tailored to meet the required format or schema.\n3. Publishing the transformed and filtered data into the next Kafka topic, enabling the consumer layer to consume and process it.\n\nIn the Transformer layer, the data flow starts by consuming the block events previously published to Kafka by the Block Producer layer. These events are then passed through a mapping and transformation process, where the raw block data is converted into a more meaningful and structured format. This transformation ensures that the data aligns with the specific requirements of the domain or application.\n\nOnce the data is transformed and mapped appropriately, it is published to the next Kafka topic, making it readily available for consumption by the consumer layer. This enables downstream systems or processes to retrieve and utilize the transformed data, leveraging it for various purposes such as analytics, further processing, or storage.\n\nIn summary, the Transformer layer acts as a vital intermediary in the data pipeline, facilitating the consumption of block events, their transformation into a domain-specific format, and the subsequent publication of the transformed data for consumption by downstream components.\n\n```typescript\nimport { SynchronousDataTransformer } from \"@maticnetwork/chain-indexer-framework/data_transformation/synchronous_data_transformer\";\nimport { IConsumerConfig } from \"@maticnetwork/chain-indexer-framework/interfaces/consumer_config\";\nimport { IProducerConfig } from \"@maticnetwork/chain-indexer-framework/interfaces/producer_config\";\nimport { ITransformedBlock } from \"@maticnetwork/chain-indexer-framework/interfaces/transformed_block\";\n\n// <T> is the consuming data type interface\n// <Y> is the producer data type interface\nexport class TransformerClass extends SynchronousDataTransformer<T, Y> {\n    constructor(consumerConfig: IConsumerConfig, producerConfig: IProducerConfig) {\n        super(consumerConfig, producerConfig);\n    }\n\n    protected async transform(block: T): Promise<ITransformedBlock<Y>> {\n        return {\n            blockNumber: block.number,\n            timestamp: block.timestamp,\n            data: this.map(block)\n        };\n    }\n\n    private map(block: IBlock): Y[] {\n        let transformedData: Y[] = [];\n\n        block.transactions.forEach((transaction: ITransaction) => {\n            // Here logic can be changed based on the application requirement \n            // and even based on the domain event that is transformed\n            const getFormattedTransaction = transform_X_to_Y_Function(transaction);\n            transformedData = transformedData.concat(getFormattedTransaction);\n        });\n\n        return transformedData;\n    }\n}\n\nlet transformer = new TransformerClass({...consumerConfig}, {...producerConfig});\ntransformer.start();\n\n// or you can use the functional implementation\n\n// Import the required modules\nimport transform from \"@maticnetwork/chain-indexer-framework/data_transformation/transform\";\n\n// Configure the trasnformer\ntransform(\n    {\n        consumerConfig, // consumer config object\n        producerConfig, // producer config object\n        type: 'asynchronous'\n    },\n    {\n        transform: () => {},\n        error: () => {}\n    }\n);\n\n```\n\n## Consumers\n\n### Synchronous Consumer\n    \nThe **Synchronous Consumer** class is the preferred option when ensuring data integrity and avoiding any loss is of paramount importance, even if it means sacrificing speed. With this class, each event is consumed in a synchronous manner, guaranteeing that no data is missed or left unprocessed. By prioritizing reliability, it provides a slower but more dependable consumption process.\n\n```typescript\n// Import the required modules\nimport { SynchronousConsumer } from \"@maticnetwork/chain-indexer-framework/kafka/consumer/synchronous_consumer\";\nimport { Coder } from \"@maticnetwork/chain-indexer-framework/coder/protobuf_coder\";\n\n// Initialize the synchronous Kafka consumer\nconst consumer = new SynchronousConsumer(\n    {\n        \"bootstrap.servers\": '<KAFKA_CONNECTION_URL>',\n        \"group.id\": '<GROUP_ID>',\n        \"security.protocol\": \"plaintext\",\n        \"message.max.bytes\": 26214400,\n        \"fetch.message.max.bytes\": 26214400,\n        topic: '<CONSUMER_TOPIC>',\n        coders: {\n            fileName: \"block\",\n            packageName: \"blockpackage\",\n            messageType: \"Block\"\n        }\n    }\n);\n\n// Start the consumer\nconsumer.start({\n    next: () => {},\n    error: () => {},\n    closed: () => {}\n});\n\n// or you can use the functional implementation\n\n// Import the required modules\nimport { consume } from \"@maticnetwork/chain-indexer-framework/kafka/consumer/consume\";\nimport { Coder } from \"@maticnetwork/chain-indexer-framework/coder/protobuf_coder\";\n\n// Configure the Kafka consumer\nconsume(\n    {\n        \"metadata.broker.list\": '<KAFKA_CONNECTION_URL>',\n        \"group.id\": '<GROUP_ID>',\n        \"security.protocol\": \"plaintext\",\n        \"topic\": '<CONSUMER_TOPIC>',\n        \"coderConfig\": {\n            fileName: \"block\",\n            packageName: \"blockpackage\",\n            messageType: \"Block\"\n        },\n        type: 'synchronous'\n    },\n    {\n        next: () => {},\n        error: () => {},\n        closed: () => {}\n    }\n);\n```\n\n### Asynchronous Consumer\n\nThe **Asynchronous Consumer** class is designed for scenarios where the speed of data consumption takes precedence over potential data loss. If the timely processing of events is critical and the occasional loss of some events is acceptable within defined limits, the asynchronous approach offers enhanced performance. By consuming events in a non-blocking manner, it allows for faster processing and higher throughput, albeit with a higher risk of occasional data loss.\n\n```typescript\n// Import the required modules\nimport { AsynchronousConsumer } from \"@maticnetwork/chain-indexer-framework/kafka/consumer/asynchronous_consumer\";\nimport { Coder } from \"@maticnetwork/chain-indexer-framework/coder/protobuf_coder\";\n\n// Initialize the asynchronous Kafka consumer\nconst consumer = new AsynchronousConsumer(\n    {\n        \"bootstrap.servers\": '<KAFKA_CONNECTION_URL>',\n        \"group.id\": '<GROUP_ID>',\n        \"security.protocol\": \"plaintext\",\n        \"message.max.bytes\": 26214400,\n        \"fetch.message.max.bytes\": 26214400,\n        topic: '<CONSUMER_TOPIC>',\n        coders: {\n            fileName: \"block\",\n            packageName: \"blockpackage\",\n            messageType: \"Block\"\n        }\n    }\n);\n\n// Start the consumer\nconsumer.start({\n    next: () => {},\n    error: () => {},\n    closed: () => {}\n});\n```\n\n\nThe final layer of the architecture possesses both simplicity and intelligence. It can be considered \"dumb\" in the sense that it directly consumes Kafka topics generated by the transformers and saves them into the database without any modification. On the other hand, it demonstrates its intelligence by managing reorganization processes. This layer monitors the event stream for blocks, and if a block appears again, it recognizes it as a reorganization event and updates the database accordingly. Additionally, this layer exposes an endpoint that can be accessed by clients. When called, the endpoint queries the database to retrieve the required data and sends it back to the client. It is worth mentioning that this layer has the capability to consume multiple Kafka topics from the transformer layer and update the database accordingly. To facilitate event consumption, it employs a consumer wrapper class from Kafka.\n\n```typescript\nimport { SynchronousConsumer } from \"@maticnetwork/chain-indexer-framework/kafka/consumer/synchronous_consumer\";\nimport { DeserialisedMessage } from \"@maticnetwork/chain-indexer-framework/interfaces/deserialised_kafka_message\";\nimport { ITransformedBlock } from \"@maticnetwork/chain-indexer-framework/interfaces/transformed_block\";\n\nexport class ConsumerClass {\n    constructor(\n        consumer1: SynchronousConsumer,\n        consumer2: SynchronousConsumer,\n        serviceClass: CustomServiceClass // Replace '<CUSTOM_SERVICE_CLASS>' with the actual CustomServiceClass type\n    ) {\n        this.consumer1 = consumer1;\n        this.consumer2 = consumer2;\n        this.serviceClass = serviceClass;\n    }\n\n    public async execute(): Promise<void> {\n        await this.consumer1.start({\n            next: this.onConsumer1Data.bind(this),\n            error(err: Error) {\n                Logger.error('something wrong occurred: ' + err);\n            },\n            closed: () => {\n                Logger.info('subscription is ended');\n                this.onFatalError(new Error(\"Consumer stopped.\"));\n            },\n        });\n\n        await this.consumer2.start({\n            next: this.onConsumer2Data.bind(this),\n            error(err: Error) {\n                Logger.error('something wrong occurred: ' + err);\n            },\n            closed: () => {\n                Logger.info('subscription is ended');\n                this.onFatalError(new Error(\"Consumer stopped.\"));\n            },\n        });\n    }\n\n    private async onConsumer1Data(message: DeserialisedMessage): Promise<void> {\n        const transformedBlock = message.value as ITransformedBlock<TYPE>; // Replace 'TYPE' with the actual data type for Consumer 1\n        if (transformedBlock.data && transformedBlock.data.length > 0) {\n            // This service class is used to add data to the DB and also exposes\n            // functions to call the DB for API calls.\n            await this.serviceClass.save(transformedBlock);\n        }\n    }\n\n    private async onConsumer2Data(message: DeserialisedMessage): Promise<void> {\n        // Implement the data processing logic for Consumer 2 here\n        // Similar to onConsumer1Data, you can use this.serviceClass to interact with the DB\n    }\n\n    private onFatalError(error: Error): void {\n        Logger.error(`Consumer encountered a fatal error: ${error.message}`);\n        process.exit(1); // Exiting process on fatal error. Process manager needs to restart the process.\n    }\n}\n\n// Replace '<CONSUMER_1>', '<CONSUMER_2>', and '<SERVICE>' with actual instances of SynchronousConsumer and CustomServiceClass respectively.\n// You should create instances of SynchronousConsumer and CustomServiceClass with proper configurations before passing them to ConsumerClass constructor.\n\nlet consumer1 = new SynchronousConsumer(/*...*/); // Configure consumer1 with appropriate settings and topic\nlet consumer2 = new SynchronousConsumer(/*...*/); // Configure consumer2 with appropriate settings and topic\nlet service = new CustomServiceClass(/*...*/); // Replace CustomServiceClass with your actual service class and configure it accordingly\n\nlet consumer = new ConsumerClass(consumer1, consumer2, service);\nconsumer.execute();\n\n\n// or you can use the functional implementation\n\nimport { ITransformedBlock } from \"@maticnetwork/chain-indexer-framework/interfaces/transformed_block\";\nimport { DeserialisedMessage } from \"@maticnetwork/chain-indexer-framework/interfaces/deserialised_kafka_message\";\nimport { consume } from \"@maticnetwork/chain-indexer-framework/kafka/consumer/consume\";\nimport transferService from \"path_to_service_file\";\n\n// Configure your consumerConfig object with the required Kafka settings and topic.\n// For example:\nconst consumerConfig = {\n    \"metadata.broker.list\": \"<BROKER_LIST>\",\n    \"group.id\": \"<GROUP_ID>\",\n    \"security.protocol\": \"plaintext\",\n    \"topic\": \"<CONSUMER_TOPIC>\",\n    \"coderConfig\": {\n        fileName: \"block\",\n        packageName: \"blockpackage\",\n        messageType: \"Block\",\n    },\n    type: \"synchronous\"\n};\n\n// Start consuming messages from the Kafka topic.\nconsume(consumerConfig, {\n    next: async (message: DeserialisedMessage) => {\n        const transformedBlock = message.value as ITransformedBlock<TYPE>; // Replace 'TYPE' with the actual data type for the transformed block.\n\n        if (transformedBlock.data && transformedBlock.data.length > 0) {\n            // Use the transferService to save the data to the database or perform other operations.\n            await transferService.save(transformedBlock);\n        }\n    },\n    error: (err: Error) => {\n        console.error('Something wrong occurred: ' + err);\n    },\n    closed: () => {\n        Logger.info(`Subscription is ended.`);\n        throw new Error(\"Consumer stopped.\");\n    }\n});\n\n\n```\n\n## Helpers\n\n### Database\n\nThe Database class is a singleton class that provides a simple and straightforward method to connect and disconnect from a database with a particular collection. It is designed to work with Mongoose, a popular MongoDB object modeling tool for Node.js. The class is initialized with a database URL, and it manages the connection state and creation of models for the database.\n\nThe `Database` class encapsulates the following key functionalities:\n\n1. **Singleton Pattern**: The class follows the Singleton design pattern to ensure that only one instance of the database connection exists throughout the application.\n\n2. **Database Connection**: The class handles the connection to the database specified by the provided URL.\n\n3. **Connection Status**: It allows checking the connection status and ensures that no unnecessary connections are established.\n\n4. **Model Creation**: The class provides a method to define and retrieve Mongoose models associated with a specific collection.\n\n```typescript\nimport { Database } from \"@maticnetwork/chain-indexer-framework/mongo/database\";\n\n// Create an instance of the Database class\nconst database = new Database(\"mongodb://localhost/mydatabase\");\n\n// Connect to the database\nawait database.connect();\n\n// Define a Mongoose schema\nconst userSchema = new Schema({\n    name: String,\n    age: Number,\n});\n\n// Create a Mongoose model for the \"users\" collection\nconst UserModel = database.model(\"User\", userSchema, \"users\");\n\n// Perform database operations using the UserModel\n// ...\n\n// Disconnect from the database when done\nawait database.disconnect();\n```\n\n### Logger\n\nThe Logger class is a singleton class designed to provide a centralized and straightforward approach to log application events. It utilizes the winston library, a popular logging tool for Node.js, and integrates with Sentry for error reporting and DataDog for log aggregation. The class allows developers to create a singleton logger instance with custom configurations and log events at various severity levels, such as \"info,\" \"debug,\" \"error,\" and \"warn.\"\n\n```typescript\n// Import the Logger class and LoggerConfig interface\nimport { Logger } from \"@maticnetwork/chain-indexer-framework/logger/logger\";\n\n// Configuration for the logger\nconst loggerConfig = {\n    console: {\n        level: \"debug\", // Log all messages with severity level \"debug\" and above to the console\n    },\n    sentry: {\n        dsn: \"your-sentry-dsn\", // Sentry DSN for error reporting\n        level: \"error\", // Log messages with severity level \"error\" and above to Sentry\n    },\n    datadog: {\n        api_key: \"your-datadog-api-key\", // DataDog API key for log aggregation\n        service_name: \"your-service-name\", // Service name for log identification in DataDog\n    },\n    winston: {\n        // Any additional Winston configuration options can be provided here\n    }\n};\n\n// Create the singleton logger instance with the specified configuration\nLogger.create(loggerConfig);\n\n// Log events with different severity levels\nLogger.info(\"This is an information message.\");\nLogger.debug({ foo: \"bar\", baz: \"qux\" }); // Logging a JSON object\nLogger.error(new Error(\"An error occurred.\"));\nLogger.warn(\"This is a warning message.\");\n\n// Log events with custom severity level\nLogger.log(\"custom\", \"This is a custom log message.\");\n\n```\n\n### ABI Coder\n\nThe ABICoder class is a helper class for web3.js-related functionalities. It provides methods to encode and decode data based on the Ethereum contract Application Binary Interface (ABI). The class utilizes the web3-eth-abi library to perform the encoding and decoding operations. Developers can use this class to handle ABI-related tasks, such as encoding parameters for contract function calls, decoding function call outputs, and decoding logs emitted by smart contracts.\n\n```typescript\n// Import the ABICoder class\nimport { ABICoder } from \"@maticnetwork/chain-indexer-framework/coder/abi-coder\";\n\n// Example function call encoding and decoding\nconst types = [\"address\", \"uint256\"];\nconst values = [\"0x1234567890123456789012345678901234567890\", \"42\"];\n\n// Encode the parameters\nconst encodedParams = ABICoder.encodeParameters(types, values);\nconsole.log(\"Encoded Parameters:\", encodedParams);\n\n// Decode the parameters\nconst decodedParams = ABICoder.decodeParameters(types, encodedParams);\nconsole.log(\"Decoded Parameters:\", decodedParams);\n\n// Example log decoding\nconst eventInputs = [\n    { type: \"address\", name: \"from\" },\n    { type: \"address\", name: \"to\" },\n    { type: \"uint256\", name: \"amount\" },\n];\n\nconst logData = \"0x1234567890123456789012345678901234567890\";\nconst logTopics = [\"0xabcdef1234567890\", \"0x0123456789abcdef\"];\n\nconst decodedLog = ABICoder.decodeLog(eventInputs, logData, logTopics);\nconsole.log(\"Decoded Log:\", decodedLog);\n\n// Example method call data decoding\nconst methodTypes = [\"address\", \"uint256\"];\nconst methodData = \"0x0123456789abcdef\"; // Assuming this is the data received from a method call\n\nconst decodedMethodData = ABICoder.decodeMethod(methodTypes, methodData);\nconsole.log(\"Decoded Method Data:\", decodedMethodData);\n\n```\n\n### Bloom Filter\n\nThe `BloomFilter` class is a wrapper around the `ethereum-bloom-filters` package, providing methods to check for the presence of contract addresses and topics in a Bloom filter. A Bloom filter is a probabilistic data structure used for efficient set membership tests. It allows fast and memory-efficient checks to determine whether an element is likely to be present in the set, with a certain probability of false positives.\n\n```typescript\n// Import the ABICoder class\nimport { BloomFilter } from \"@maticnetwork/chain-indexer-framework/filter/bloom_Filter\";\n\n// Example Bloom filter and addresses/topics to check\nconst bloomFilter = \"0x0123456789abcdef\";\nconst contractAddress = \"0x1234567890abcdef\";\nconst topic = \"0x7890abcdef1234567890abcdef\";\n\n// Check if the contract address is in the Bloom filter\nconst isAddressInBloom = BloomFilter.isContractAddressInBloom(bloomFilter, contractAddress);\nconsole.log(`Is Contract Address in Bloom Filter? ${isAddressInBloom}`); // true or false\n\n// Check if the topic is in the Bloom filter\nconst isTopicInBloom = BloomFilter.isTopicInBloom(bloomFilter, topic);\nconsole.log(`Is Topic in Bloom Filter? ${isTopicInBloom}`); // true or false\n\n```\n\n## Conclusion \n\nThis horizontal architectural approach allows to handle increasing workloads by adding more resources and not like vertical where you increase the resources of existing servers. It enables the system to maintain performance and handle larger amounts of data and traffic as the demands grow.Data in Kafka is organized into topics, and producers write data to specific topics. Consumers subscribe to topics and process the data in real-time. This decoupling of producers and consumers allows for a flexible and scalable architecture, as more producers and consumers can be added without affecting the existing ones.\n\nKafka is a distributed streaming platform that allows you to publish and subscribe to streams of records. It is designed to handle high-throughput, real-time data feeds and provides features such as fault tolerance, scalability, and message persistence, making it well-suited for handling blockchain data.\n\nHere's how abstracting historic blockchain data through Kafka is providing reliability and performance benefits:\n\n1. **Separating data producers and consumers**: By using Kafka, you can separate the blockchain data producers (e.g., blockchain nodes) from the consumers (e.g., applications specific data). This allows each component to work independently and reducing the risk of data loss or any delay.\n2. **Reliable message delivery**: If a consumer fails in kafka, it can resume from where it got stopped once it recovers, avoiding any data loss. This is important when working with historical blockchain data that needs to be processed accurately.\n3. **Multiple consumers and data replay**: Kafka allows same topics from the producer to be consumed by multiple consumers any number of times. this helps in preventing producing of blockchain data multiple times into the kafka stream. A consumer can even replay the processing of same topic if any change in required.\n\n## Building\n\n### Requirements\n\n-   [Node.js](https://nodejs.org)\n-   [npm](https://www.npmjs.com/)\n\n### Install\n\nInstall all the dependencies:\n\n```bash\nnpm install\n```\n\n### Building\n\nBuild the chain-indexer-framework package:\n\n```bash\nnpm run build\n```\n\n### Testing (jest)\n\n```bash\nnpm run tests\n```\n\n### Linking\n\nusing package locally\n\n```bash\nnpm run build:link\n```\n\n## Support\n\nOur [Discord](https://discord.gg/0xPolygonDevs) is the best way to reach us \u2728.\n\n## Contributing\n\nYou are very welcome to contribute, please see contributing guidelines - [[Contribute](./CONTRIBUTING.md)].\n\nThank you to all the people who already contributed to chain-indexer-framework!\n\n<a href=\"https://github.com/maticnetwork/chain-indexer-framework/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=maticnetwork/chain-indexer-framework\" />\n</a>\n\nMade with [contributors-img](https://contrib.rocks).\n\n## License\n\n[MIT](./LICENSE)\n", "release_dates": ["2023-10-13T12:00:25Z", "2023-10-04T10:32:42Z", "2023-10-04T10:10:11Z"]}, {"name": "cometbft", "description": "CometBFT (fork of Tendermint Core): A distributed, Byzantine fault-tolerant, deterministic state machine replication engine", "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# CometBFT\n\n[Byzantine-Fault Tolerant][bft] [State Machine Replication][smr]. Or\n[Blockchain], for short.\n\n[![Version][version-badge]][version-url]\n[![API Reference][api-badge]][api-url]\n[![Go version][go-badge]][go-url]\n[![Discord chat][discord-badge]][discord-url]\n[![License][license-badge]][license-url]\n[![Sourcegraph][sg-badge]][sg-url]\n\n| Branch  | Tests                                          | Linting                                     |\n|---------|------------------------------------------------|---------------------------------------------|\n| main    | [![Tests][tests-badge]][tests-url]             | [![Lint][lint-badge]][lint-url]             |\n| v0.38.x | [![Tests][tests-badge-v038x]][tests-url-v038x] | [![Lint][lint-badge-v038x]][lint-url-v038x] |\n| v0.37.x | [![Tests][tests-badge-v037x]][tests-url-v037x] | [![Lint][lint-badge-v037x]][lint-url-v037x] |\n| v0.34.x | [![Tests][tests-badge-v034x]][tests-url-v034x] | [![Lint][lint-badge-v034x]][lint-url-v034x] |\n\nCometBFT is a Byzantine Fault Tolerant (BFT) middleware that takes a\nstate transition machine - written in any programming language - and securely\nreplicates it on many machines.\n\nIt is a fork of [Tendermint Core][tm-core] and implements the Tendermint\nconsensus algorithm.\n\nFor protocol details, refer to the [CometBFT Specification](./spec/README.md).\n\nFor detailed analysis of the consensus protocol, including safety and liveness\nproofs, read our paper, \"[The latest gossip on BFT\nconsensus](https://arxiv.org/abs/1807.04938)\".\n\n## Documentation\n\nComplete documentation can be found on the\n[website](https://docs.cometbft.com/).\n\n## Releases\n\nPlease do not depend on `main` as your production branch. Use\n[releases](https://github.com/cometbft/cometbft/releases) instead.\n\nIf you intend to run CometBFT in production, we're happy to help. To contact\nus, in order of preference:\n\n- [Create a new discussion on\n  GitHub](https://github.com/cometbft/cometbft/discussions)\n- Reach out to us via [Telegram](https://t.me/CometBFT)\n- [Join the Cosmos Network Discord](https://discord.gg/cosmosnetwork) and\n  discuss in\n  [`#cometbft`](https://discord.com/channels/669268347736686612/1069933855307472906)\n\nMore on how releases are conducted can be found [here](./RELEASES.md).\n\n## Security\n\nTo report a security vulnerability, see our [bug bounty\nprogram](https://hackerone.com/cosmos). For examples of the kinds of bugs we're\nlooking for, see [our security policy](SECURITY.md).\n\n## Minimum requirements\n\n| CometBFT version | Requirement | Notes             |\n|------------------|-------------|-------------------|\n| main             | Go version  | Go 1.20 or higher |\n| v0.38.x          | Go version  | Go 1.20 or higher |\n| v0.37.x          | Go version  | Go 1.20 or higher |\n| v0.34.x          | Go version  | Go 1.19 or higher |\n\n### Install\n\nSee the [install guide](./docs/guides/install.md).\n\n### Quick Start\n\n- [Single node](./docs/guides/quick-start.md)\n- [Local cluster using docker-compose](./docs/networks/docker-compose.md)\n\n## Contributing\n\nPlease abide by the [Code of Conduct](CODE_OF_CONDUCT.md) in all interactions.\n\nBefore contributing to the project, please take a look at the [contributing\nguidelines](CONTRIBUTING.md) and the [style guide](STYLE_GUIDE.md). You may also\nfind it helpful to read the [specifications](./spec/README.md), and familiarize\nyourself with our [Architectural Decision Records\n(ADRs)](./docs/architecture/README.md) and [Request For Comments\n(RFCs)](./docs/rfc/README.md).\n\n## Versioning\n\n### Semantic Versioning\n\nCometBFT uses [Semantic Versioning](http://semver.org/) to determine when and\nhow the version changes. According to SemVer, anything in the public API can\nchange at any time before version 1.0.0\n\nTo provide some stability to users of 0.X.X versions of CometBFT, the MINOR\nversion is used to signal breaking changes across CometBFT's API. This API\nincludes all publicly exposed types, functions, and methods in non-internal Go\npackages as well as the types and methods accessible via the CometBFT RPC\ninterface.\n\nBreaking changes to these public APIs will be documented in the CHANGELOG.\n\n### Upgrades\n\nIn an effort to avoid accumulating technical debt prior to 1.0.0, we do not\nguarantee that breaking changes (i.e. bumps in the MINOR version) will work with\nexisting CometBFT blockchains. In these cases you will have to start a new\nblockchain, or write something custom to get the old data into the new chain.\nHowever, any bump in the PATCH version should be compatible with existing\nblockchain histories.\n\nFor more information on upgrading, see [UPGRADING.md](./UPGRADING.md).\n\n### Supported Versions\n\nBecause we are a small core team, we have limited capacity to ship patch\nupdates, including security updates. Consequently, we strongly recommend keeping\nCometBFT up-to-date. Upgrading instructions can be found in\n[UPGRADING.md](./UPGRADING.md).\n\nCurrently supported versions include:\n\n- v0.38.x: CometBFT v0.38 introduces ABCI 2.0, which implements the entirety of\n  ABCI++\n- v0.37.x: CometBFT v0.37 introduces ABCI 1.0, which is the first major step\n  towards the full ABCI++ implementation in ABCI 2.0\n- v0.34.x: The CometBFT v0.34 series is compatible with the Tendermint Core\n  v0.34 series\n\n## Resources\n\n### Libraries\n\n- [Cosmos SDK](http://github.com/cosmos/cosmos-sdk); A framework for building\n  applications in Golang\n- [Tendermint in Rust](https://github.com/informalsystems/tendermint-rs)\n- [ABCI Tower](https://github.com/penumbra-zone/tower-abci)\n\n### Applications\n\n- [Cosmos Hub](https://hub.cosmos.network/)\n- [Terra](https://www.terra.money/)\n- [Celestia](https://celestia.org/)\n- [Anoma](https://anoma.network/)\n- [Vocdoni](https://docs.vocdoni.io/)\n\n### Research\n\nBelow are links to the original Tendermint consensus algorithm and relevant\nwhitepapers which CometBFT will continue to build on.\n\n- [The latest gossip on BFT consensus](https://arxiv.org/abs/1807.04938)\n- [Master's Thesis on Tendermint](https://atrium.lib.uoguelph.ca/xmlui/handle/10214/9769)\n- [Original Whitepaper: \"Tendermint: Consensus Without Mining\"](https://tendermint.com/static/docs/tendermint.pdf)\n\n## Join us\n\nCometBFT is currently maintained by [Informal\nSystems](https://informal.systems). If you'd like to work full-time on CometBFT,\n[we're hiring](https://informal.systems/careers)!\n\nFunding for CometBFT development comes primarily from the [Interchain\nFoundation](https://interchain.io), a Swiss non-profit. Informal Systems also\nmaintains [cometbft.com](https://cometbft.com).\n\n[bft]: https://en.wikipedia.org/wiki/Byzantine_fault_tolerance\n[smr]: https://en.wikipedia.org/wiki/State_machine_replication\n[Blockchain]: https://en.wikipedia.org/wiki/Blockchain\n[version-badge]: https://img.shields.io/github/v/release/cometbft/cometbft.svg\n[version-url]: https://github.com/cometbft/cometbft/releases/latest\n[api-badge]: https://camo.githubusercontent.com/915b7be44ada53c290eb157634330494ebe3e30a/68747470733a2f2f676f646f632e6f72672f6769746875622e636f6d2f676f6c616e672f6764646f3f7374617475732e737667\n[api-url]: https://pkg.go.dev/github.com/cometbft/cometbft\n[go-badge]: https://img.shields.io/badge/go-1.20-blue.svg\n[go-url]: https://github.com/moovweb/gvm\n[discord-badge]: https://img.shields.io/discord/669268347736686612.svg\n[discord-url]: https://discord.gg/cosmosnetwork\n[license-badge]: https://img.shields.io/github/license/cometbft/cometbft.svg\n[license-url]: https://github.com/cometbft/cometbft/blob/main/LICENSE\n[sg-badge]: https://sourcegraph.com/github.com/cometbft/cometbft/-/badge.svg\n[sg-url]: https://sourcegraph.com/github.com/cometbft/cometbft?badge\n[tests-url]: https://github.com/cometbft/cometbft/actions/workflows/tests.yml\n[tests-url-v038x]: https://github.com/cometbft/cometbft/actions/workflows/tests.yml?query=branch%3Av0.38.x\n[tests-url-v037x]: https://github.com/cometbft/cometbft/actions/workflows/tests.yml?query=branch%3Av0.37.x\n[tests-url-v034x]: https://github.com/cometbft/cometbft/actions/workflows/tests.yml?query=branch%3Av0.34.x\n[tests-badge]: https://github.com/cometbft/cometbft/actions/workflows/tests.yml/badge.svg?branch=main\n[tests-badge-v038x]: https://github.com/cometbft/cometbft/actions/workflows/tests.yml/badge.svg?branch=v0.38.x\n[tests-badge-v037x]: https://github.com/cometbft/cometbft/actions/workflows/tests.yml/badge.svg?branch=v0.37.x\n[tests-badge-v034x]: https://github.com/cometbft/cometbft/actions/workflows/tests.yml/badge.svg?branch=v0.34.x\n[lint-badge]: https://github.com/cometbft/cometbft/actions/workflows/lint.yml/badge.svg?branch=main\n[lint-badge-v034x]: https://github.com/cometbft/cometbft/actions/workflows/lint.yml/badge.svg?branch=v0.34.x\n[lint-badge-v037x]: https://github.com/cometbft/cometbft/actions/workflows/lint.yml/badge.svg?branch=v0.37.x\n[lint-badge-v038x]: https://github.com/cometbft/cometbft/actions/workflows/lint.yml/badge.svg?branch=v0.38.x\n[lint-url]: https://github.com/cometbft/cometbft/actions/workflows/lint.yml\n[lint-url-v034x]: https://github.com/cometbft/cometbft/actions/workflows/lint.yml?query=branch%3Av0.34.x\n[lint-url-v037x]: https://github.com/cometbft/cometbft/actions/workflows/lint.yml?query=branch%3Av0.37.x\n[lint-url-v038x]: https://github.com/cometbft/cometbft/actions/workflows/lint.yml?query=branch%3Av0.38.x\n[tm-core]: https://github.com/tendermint/tendermint\n", "release_dates": ["2024-03-01T12:11:13Z"]}, {"name": "contracts", "description": null, "language": "Solidity", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Template Repo (Foundry)\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![CI Status](../../actions/workflows/test.yaml/badge.svg)](../../actions)\n\nThis template repo is a quick and easy way to get started with a new Solidity project. It comes with a number of features that are useful for developing and deploying smart contracts. Such as:\n\n- Pre-commit hooks for formatting, auto generated documentation, and more\n- Various libraries with useful contracts (OpenZeppelin, Solady) and libraries (Deployment log generation, storage checks, deployer templates)\n\n#### Table of Contents\n\n- [Setup](#setup)\n- [Deployment](#deployment)\n- [Docs](#docs)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Setup\n\nFollow these steps to set up your local environment:\n\n- [Install foundry](https://book.getfoundry.sh/getting-started/installation)\n- Install dependencies: `forge install`\n- Build contracts: `forge build`\n- Test contracts: `forge test`\n\nIf you intend to develop on this repo, follow the steps outlined in [CONTRIBUTING.md](CONTRIBUTING.md#install).\n\n## Deployment\n\nThis repo utilizes versioned deployments. For more information on how to use forge scripts within the repo, check [here](CONTRIBUTING.md#deployment).\n\nSmart contracts are deployed or upgraded using the following command:\n\n```shell\nforge script script/Deploy.s.sol --broadcast --rpc-url <rpc_url> --verify\n```\n\n## Docs\n\nThe documentation and architecture diagrams for the contracts within this repo can be found [here](docs/).\nDetailed documentation generated from the NatSpec documentation of the contracts can be found [here](docs/autogen/src/src/).\nWhen exploring the contracts within this repository, it is recommended to start with the interfaces first and then move on to the implementation as outlined [here](CONTRIBUTING.md#natspec--comments)\n\n## Contributing\n\nIf you want to contribute to this project, please check [CONTRIBUTING.md](CONTRIBUTING.md) first.\n\n## License\n\n\u200b\nLicensed under either of\n\u200b\n\n- Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)\n- MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n  \u200b\n\nat your option.\n\nUnless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.\n\n---\n\n\u00a9 2023 PT Services DMCC\n", "release_dates": []}, {"name": "cosmos-sdk", "description": ":chains: A Framework for Building High Value Public Blockchains :sparkles:", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<div align=\"center\">\n  <h1> Cosmos SDK </h1>\n</div>\n\n![banner](https://github.com/cosmos/cosmos-sdk-docs/blob/main/static/img/banner.jpg)\n\n<div align=\"center\">\n  <a href=\"https://github.com/cosmos/cosmos-sdk/blob/main/LICENSE\">\n    <img alt=\"License: Apache-2.0\" src=\"https://img.shields.io/github/license/cosmos/cosmos-sdk.svg\" />\n  </a>\n  <a href=\"https://pkg.go.dev/github.com/cosmos/cosmos-sdk\">\n    <img src=\"https://pkg.go.dev/badge/github.com/cosmos/cosmos-sdk.svg\" alt=\"Go Reference\">\n  </a>\n  <a href=\"https://goreportcard.com/report/github.com/cosmos/cosmos-sdk\">\n    <img alt=\"Go report card\" src=\"https://goreportcard.com/badge/github.com/cosmos/cosmos-sdk\" />\n  </a>\n  <a href=\"https://sonarcloud.io/summary/overall?id=cosmos_cosmos-sdk\">\n    <img alt=\"Code Coverage\" src=\"https://sonarcloud.io/api/project_badges/measure?project=cosmos_cosmos-sdk&metric=coverage\" />\n  </a>\n  <a href=\"https://sonarcloud.io/summary/overall?id=cosmos_cosmos-sdk\">\n    <img alt=\"SonarCloud Analysis\" src=\"https://sonarcloud.io/api/project_badges/measure?project=cosmos_cosmos-sdk&metric=alert_status\">\n  </a>\n</div>\n<div align=\"center\">\n  <a href=\"https://discord.gg/cosmosnetwork\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/discord/669268347736686612.svg\" />\n  </a>\n  <a href=\"https://sourcegraph.com/github.com/cosmos/cosmos-sdk?badge\">\n    <img alt=\"Imported by\" src=\"https://sourcegraph.com/github.com/cosmos/cosmos-sdk/-/badge.svg\" />\n  </a>\n    <img alt=\"Sims\" src=\"https://github.com/cosmos/cosmos-sdk/workflows/Sims/badge.svg\" />\n    <img alt=\"Lint Satus\" src=\"https://github.com/cosmos/cosmos-sdk/workflows/Lint/badge.svg\" />\n</div>\n\nThe Cosmos SDK is a framework for building blockchain applications. [CometBFT (BFT Consensus)](https://github.com/cometbft/cometbft) and the Cosmos SDK are written in the Go programming language. Cosmos SDK is used to build [Gaia](https://github.com/cosmos/gaia), the implementation of the Cosmos Hub.\n\n**WARNING**: The Cosmos SDK has mostly stabilized, but we are still making some breaking changes.\n\n**Note**: Always use the latest maintained [Go](https://go.dev/dl) version for building Cosmos SDK applications.\n\n## Quick Start\n\nTo learn how the Cosmos SDK works from a high-level perspective, see the Cosmos SDK [High-Level Intro](https://docs.cosmos.network/main/learn/intro/overview).\n\nIf you want to get started quickly and learn how to build on top of Cosmos SDK, visit [Cosmos SDK Tutorials](https://tutorials.cosmos.network). You can also fork the tutorial's repository to get started building your own Cosmos SDK application.\n\nFor more information, see the [Cosmos SDK Documentation](https://docs.cosmos.network).\n\n## Contributing\n\nSee [CONTRIBUTING.md](./CONTRIBUTING.md) for details on how to contribute and participate in our [dev calls](./CONTRIBUTING.md#teams-dev-calls).\nIf you want to follow the updates or learn more about the latest design then join our [Discord](https://discord.gg/cosmosnetwork).\n\n## Tools and Frameworks\n\nThe Cosmos ecosystem is vast.\n[Awesome Cosmos](https://github.com/cosmos/awesome-cosmos) is a community-curated list of notable frameworks, modules and tools.\n\n### Cosmos Hub Mainnet\n\nThe Cosmos Hub application, `gaia`, has its own [cosmos/gaia repository](https://github.com/cosmos/gaia). Go there to join the Cosmos Hub mainnet and more.\n\n### Inter-Blockchain Communication (IBC)\n\nThe IBC module for the Cosmos SDK has its own [cosmos/ibc-go repository](https://github.com/cosmos/ibc-go). Go there to build and integrate with the IBC module.\n\n## Disambiguation\n\nThis Cosmos SDK project is not related to the [React-Cosmos](https://github.com/react-cosmos/react-cosmos) project (yet). Many thanks to Evan Coury and Ovidiu (@skidding) for this Github organization name. As per our agreement, this disambiguation notice will stay here.\n", "release_dates": []}, {"name": "dapp-launchpad", "description": "dApp Launchpad is a CLI tool to quickly initialise a fully-integrated EVM-compatible dApp, create a development environment, and deploy everything to production.", "language": "TypeScript", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# dApp Launchpad\n\n## Table of Contents\n- [Introduction](#introduction)\n- [Installation](#installation)\n- [Usage](#usage)\n    - [Initialising a project](#initialising-a-project)\n    - [Setting up enviroment variables](#setting-up-enviroment-variables)\n    - [Starting a dev environment](#starting-a-dev-environment)\n    - [Deploying](#deploying)\n    - [Help](#help)\n- [Project structure](#project-structure)\n    - [Frontend](#frontend)\n    - [Smart contracts](#smart-contracts)\n- [Contributing](#contributing)\n- [FAQs](#faqs)\n\n## Introduction\ndApp Launchpad is a CLI tool to quickly initialise a fully-integrated EVM-compatible DApp, create a development environment, and deploy everything to production.\n\nEvery step of the way is automated!\n\n## Node version\nNode >v16.14.x is supported, although Node v18.x.x is recommended.\n\nBefore going on with installation, make sure to switch to a supported Node version.\n\nTo easily manage different npm versions on your system, we recommend using [nvm](https://github.com/nvm-sh/nvm).\n\n## Installation\nInstall the package globally, and the tool will be accessible anywhere.\n```\nnpm install -g @polygonlabs/dapp-launchpad\n```\n\n## Usage\n\n### Initialising a project\nTo initialise a project, simply run:\n```\ndapp-launchpad init [YOUR PROJECT NAME]\n```\n\nThis will create a new directory in your current directory, and initialise a minimal dApp project inside it, then proceed to install all required packages.\n\nBy default, the scaffolded project is in javascript. To use typescript or any other template, use `--template NAME` option.\n\nTo get a list of available templates (for use in above option), run `list scaffold-templates`.\n\n### Setting up enviroment variables\nBefore starting anything, set up the environment variables in both the `frontend` and `smart-contracts` suub-folders, in a `.env` file. Example env files are provided for each, in `.env.example`.\n\n#### WalletConnect Project ID\nTo get a WalletConnect Project ID, Head over to [WalletConnect Cloud](https://cloud.walletconnect.com/) and create a new project. This will generate a project ID which you can then use.\n\n### Starting a dev environment\nTo start a development environment, use:\n```\ndapp-launchpad dev\n```\n\nAnd this will start a fully integrated dev environment - a local dev blockchain and a local Frontend dev server! Any change in the code automatically updates both the frontend and the smart contracts; no manual reload is necessary!\n\nThis will also generate some funded test wallets for you in this test chain, which you can use to develop your dApp.\n\nYou may also start this local chain by forking Ethereum or any EVM-compatible chains. Just run:\n```\ndapp-launchpad dev -n polygonZkevm\n```\n\nTo see all available options, run:\n```\ndapp-launchpad dev -h\n```\n\nSee [Project structure](#project-structure) to learn about how the dev environment is structured.\n\n### Deploying\nTo deploy your project to production, run:\n```\ndapp-launchpad deploy -n CHAIN_NAME\n```\n\nThis will do 2 things:\n- Deploy all your smart contracts to the selected chain, and log the deployment results.\n- Deploy your frontend to Vercel, and log the deployment URL.\n\nTo deploy only the smart contracts, run:\n```\ndapp-launchpad deploy -n CHAIN_NAME --only-smart-contracts\n```\n\nAnd to deploy only the frontend, run:\n```\ndapp-launchpad deploy -n CHAIN_NAME --only-frontend\n```\n\nThe frontend deployment requires that smart contracts to have been deployed before. So if you are only deploying the frontend, make sure that you did run the smart contracts deploy command successfully before this.\n\n### Help\nTo see all available options of any command at any time, use:\n```\ndapp-launchpad [COMMAND NAME] -h\n```\n\n## Project structure\n\nThe project is divided into two parts - Frontend (inside `./frontend`) and Smart contracts (inside `./smart-contracts`).\n\n### Frontend\n\n#### Node version\nNode >v16.14.x is supported, although Node v18.x.x is recommended.\n\nA `.nvmrc` has been provided if you use `nvm`. You can use this by:\n```\nnvm use # in ./frontend\n```\n\n#### Framework\n\nThe frontend runs on a Next.js server. If you're new to Next.js but know React.js, getting used to Next.js would be trivial. To get started, modify the component file at `./frontend/src/pages/index`.\n\nTo learn more about Next.js, [read their docs](https://nextjs.org/docs).\n\n#### Environment variables\n\nBefore you start, you need to setup the environment variables. Look at the `.env.example` to know what to setup. Env variables required are:\n```\nNEXT_PUBLIC_WALLETCONNECT_PROJECT_ID=\"\" \n```\n\nNote, all env variable names which are supposed to be exposed used in client requests should be prefixed with `NEXT_PUBLIC_`.\n\n#### Connecting wallet\n\nTo connect user wallets, [Web3Modal v3](https://web3modal.com/) has been integrated and pre-configured for you.\n\nUse the provided `useWallet` hook to interact with Web3Modal and wallets. This contains utilities to simplify anything you need related to wallets.\n\n#### Sending transactions to smart contracts\n\nTo send transactions to either a locally deployed smart contract or a smart contract on a prod chain, use the `useSmartContract` hook. This contains utilities to simplify getting and interacting with a Ethers.js contract instance.\n\nWhen [deploying to local or production](#deploying), this hook will automatically use the correct chain and contracts.\n\n#### Deploying to local test server\n\nThe `dev` command automates everything for you to setup a local Next.js test server.\n\n#### Deploying to Vercel\n\nTo deploy this, follow the [Deploying](#deploying) guide.\n\nWith the `deploy` command, the Frontend deployment is fully automated. Vercel is used for deployments. Vercel offers free quotas to developers to get started.\n\nNo pre-configuration is necessary to run the `deploy` command. You'll be taken through all relevant steps upon running it.\n\n### Smart Contracts\n\n#### Node version\nNode >v16.14.X is supported, although Node v18.17.X is recommended.\n\nA `.nvmrc` has been provided if you use `nvm`. You can use this by:\n```\nnvm use # in ./smart-contracts\n```\n\n#### Environment variables\n\nBefore you start, you need to setup the environment variables. Look at the `.env.example` to know what to setup. Env variables required are:\n```\nPRIVATE_KEY_DEPLOYER=\"\" \n```\n\n#### Framework\n\nThe smart contracts run on a Hardhat environment.\n\nThe smart contracts are written in [Solidity](https://docs.soliditylang.org/), and are in the `contracts` directory.\n\nTests are written in JS/TS, and are in `tests` directory. An example test is written for you here.\n\nScripts are also written in JS/TS, and are in `scripts` directory. Some mandatory scripts are already there to get started with.\n\n#### Deploying on local test chain\n\nThe `dev` command automates everything for you to setup a local test chain.\n\nThis will also generate some funded test wallets for you in this test chain, which you can use to develop your dApp.\n\nYou may also start this local chain by forking Ethereum or any EVM-compatible chain. Just run:\n```\ndapp-launchpad dev -n polygonZkevm -b [BLOCK_NUMBER_TO_FORK_AT]\n```\n\nTo see all available options, run:\n```\ndapp-launchpad dev -h\n```\n\nThe `dev` command internally runs the provided `scripts/deploy_localhost` script to deploy all contracts in the correct sequence. When working on your own smart contracts, make sure to update this script.\n\n#### Local test chain explorer\n\nOptionally, you can also enable a local blockchain explorer, which auto-indexes all transactions, and provides a feature-loaded dashboard for you to get an overview of this chain.\n\nTo use it, run the `dev` command with `-e`, optionally with a few more args.\n\nFor this to work, you need to sign up on [Ethernal](https://app.tryethernal.com/), and create a workspace. Then you put your login email, password and workspace name inside the `.env` in `smart-contracts`. (checkout the `.env.example`)\n\nThe above config can also be mentioned with `dev` command params `--ethernal-login-email`, `--ethernal-login-password` and `--ethernal-workspace`, which overrides the env variables.\n\nOnce started, you can access the chain explorer at the same URL as mentioned before!\n\n#### Deploying to production\n\nThe `deploy` command automates everything for you to deploy to Ethereum or any EVM-compatible chain.\n\nThe `deploy` command internally runs the provided `scripts/deploy_prod` script to deploy all contracts in the correct sequence. When working on your own smart contracts, make sure to update this script.\n\nTo see all available options, run:\n```\ndapp-launchpad deploy -h\n```\n\n## Contributing\n\n### Building\n\nTo build the CLI tool, run:\n\n```\nnpm run build\n```\n\nThis will generate `cli.js` inside `bin` directory, which can they be installed globally with:\n\n```\nnpm run install-global\n```\n\nAfter this, `dapp-launchpad` will be available as a global command.\n\n### Dev environment\n\nTo modify this tool, a dev environment can be started by running:\n\n```\nnpm run dev\n```\n\nThis watches the source files, and bundles up the CLI app on every change, and installs it globally. In other words, the global `dapp-launchpad` is always updated with your changes in the code.\n\n### Reporting bugs / Feature requests\n\nTo report a bug or request a feature, [create an issue](https://github.com/0xPolygon/dapp-launchpad/issues), and describe what you want.\n\n## FAQs\n\n### Why does Metamask fail in sending transactions in dev environment with a nonce error?\nEverytime the dev environment is started, a new local test chain is started. Metamask internally maintains a cache of \"latest block number\" and \"account transaction nonce\". Since every run of `dev` creates a new chain, it never matches with this cache.\n\nTo know how to clear the cache, [read this](https://support.metamask.io/hc/en-us/articles/360015488891-How-to-clear-your-account-activity-reset-account).\n\n### Why does Metamask fail in sending transactions with a nonce error when using \"reset on change\" option in dev environment?\nThe reset on change option resets the blockchain on every code change. Metamask internally maintains a cache of \"latest block number\" and \"account transaction nonce\". After resetting the chain, the latest block number and account transaction nonce should go back to initial state as well, but Metamask does not update this cache on its own.\n\nTo know how to clear the cache, [read this](https://support.metamask.io/hc/en-us/articles/360015488891-How-to-clear-your-account-activity-reset-account).\n", "release_dates": []}, {"name": "deployer-kit", "description": null, "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Deployer Kit\n\nThe deployer kit script is a tool to streamline the deployment of contracts between scripts and unit tests. Sharing a single script during testing and production deployment reduces the risk of errors and allows to test the deployment process in advance.\n\n## Requirements\n\nThe script utilizes Node.js to run. We recommend the node version defined in the `.nvmrc` file.\n\n## Installation\n\n```bash\nforge install 0xPolygon/deployer-kit\n```\n\n## Usage Example\n\nThe following command will create a deployer contract for the `MyExample` contract from the `src/Example.sol` file in the `test/deployers/MyExampleDeployer.s.sol` file.\n\n```bash\nnode lib/deployer-kit src/Example.sol -o test/deployers -n MyExample\n```\n\n## Flags\n\n| --flag    | -flag | Description                                               |\n| --------- | ----- | --------------------------------------------------------- |\n| --output  | -o    | Output directory (default: script/deployers)              |\n| --name    | -n    | Name of the contract (default: name of the contract file) |\n| Options   |       |                                                           |\n| --help    | -h    | Print help                                                |\n| --version | -v    | Print the version number                                  |\n\n## License\n\n\u200b\nLicensed under either of\n\u200b\n\n- Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)\n- MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n  \u200b\n\nat your option.\n\nUnless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.\n\n---\n\n\u00a9 2023 PT Services DMCC\n", "release_dates": ["2023-12-11T12:59:55Z", "2023-12-08T13:46:14Z"]}, {"name": "edge-contracts", "description": null, "language": "Solidity", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Core Contracts\n\n[![Solidity CI](https://github.com/maticnetwork/v3-contracts/actions/workflows/ci.yml/badge.svg)](https://github.com/maticnetwork/v3-contracts/actions/workflows/ci.yml)\n[![Coverage Status](https://coveralls.io/repos/github/maticnetwork/v3-contracts/badge.svg?branch=main&t=ZTUm69)](https://coveralls.io/github/maticnetwork/v3-contracts?branch=main)\n\nThis repository contains the smart contract suite used in Polygon's ecosystems. Recent iterations have focused on features for the Edge project. Edge-specific contracts may be spun off into their own repo in the future.\n\n**_Note: You do not need to clone this repo in order to interact with Polygon POS or any other Polygon ecosystem._**\n\n## Contents\n\n- [Core Contracts](#core-contracts)\n  - [Contents](#contents)\n  - [Repo Architecture](#repo-architecture)\n    - [Contracts](#contracts)\n    - [General Repo Layout](#general-repo-layout)\n  - [Using This Repo](#using-this-repo)\n    - [Requirements](#requirements)\n    - [General Repo Layout](#general-repo-layout-1)\n    - [Installation](#installation)\n    - [Deployment](#deployment)\n      - [Root contracts](#root-contracts)\n    - [Environment Setup](#environment-setup)\n    - [Compiling Contracts](#compiling-contracts)\n    - [Running tests](#running-tests)\n    - [Linting](#linting)\n    - [Check Test Coverage](#check-test-coverage)\n    - [Run Slither](#run-slither)\n    - [Continuous Integration](#continuous-integration)\n    - [Documentation](#documentation)\n\n## Repo Architecture\n\n### Contracts\n\nThere are a number of different contracts with different roles in the suite, as such an architecture diagram of the contents of `contracts/` should be useful in understanding where to find what you're looking for.\n\nOne piece of terminology that is useful in understanding the layout and contracts themselves is the references to `root` and `child`. Chains such as POS and Edge assume that there is a base layer chain that data from other chains is committed to. In the case of POS, the root chain is Ethereum mainnet and the child is Polygon POS, while in the case of Edge, the root chain is Edge and the child chains are the various Supernets.\n\n```ml\n\u2502 child/ \"contracts that live on the child chain\"\n\u251c\u2500 tokens - \"contracts for the bridging/management of native and ERC20/721/1155 assets\"\n\u251c\u2500 EIP1559Burn - \"allows child native token to be burnt on root\"\n\u251c\u2500 ForkParams - \"configurable softfork features read by the client each epoch\"\n\u251c\u2500 L2StateSender - \"arbitrary message bridge (child -> root)\"\n\u251c\u2500 NetworkParams - \"configurable network parameters read by the client each epoch\"\n\u251c\u2500 StateReceiver \u2014 \"child chain component of a message bridge\"\n\u251c\u2500 System - \"various infra/precompile addresses on the child chain\"\n\u251c\u2500 \u2502 validator/ \"contracts relating directly to validating\"\n   \u251c\u2500 RewardPool - \"reward distribution to validators for committed epochs\"\n   \u251c\u2500 ValidatorSet - \"validator voting power management, commits epochs for child chains\"\n\u2502 common/ \"libraries used on both the child and root chains\"\n\u251c\u2500 BLS - \"BLS signature operations\"\n\u251c\u2500 BN256G2 - \"elliptic curve operations on G2 for BN256 (used for BLS)\"\n\u251c\u2500 Merkle - \"checks membership of a hash in a merkle tree\"\n\u2502 interfaces/ \"interfaces for all contracts\"\n\u251c\u2500 Errors - \"commonly reused errors\"\n\u2502 lib/ \"libraries used for specific applications\"\n\u251c\u2500 AccessList - \"checks address membership in protocol-level access controls\"\n\u251c\u2500 ChildManagerLib - \"library for managing child chains on root\"\n\u251c\u2500 EIP712MetaTransaction - \"template for process EIP712 structures\"\n\u251c\u2500 EIP712Upgradeable - \"adapted from OpenZeppelin, allows for upgradeable EIP712 structures\"\n\u251c\u2500 GenesisLib - \"facilitates generation of the validator set at genesis\"\n\u251c\u2500 ModExp \u2014 \"modular exponentiation (from Hubble Project, for BLS)\"\n\u251c\u2500 SafeMathInt - \"casts int256 to uint256 and vice versa with over/underflow checks\"\n\u251c\u2500 StakeManagerLib - \"manages validator stake (deposit, withdrawal, etc)\"\n\u251c\u2500 WithdrawalQueue \u2014 \"lib of operations for the rewards withdrawal queue\"\n\u2502 mocks/ \"mocks of various contracts for testing\"\n\u2502 root/ \"contracts that live on the root chain (Ethereum mainnet)\"\n\u251c\u2500 root predicates - \"templates for processing asset bridging on root\"\n\u251c\u2500 | staking \"contracts comprising the hub for staking on any child chain\"\n   \u251c\u2500 CustomSupernetManager - \"manages validator access, syncs voting power\"\n   \u251c\u2500 StakeManager - \"manages stake for all child chains\"\n   \u251c\u2500 SupernetManager - \"abstract template for managing Supernets\"\n\u251c\u2500 CheckpointManager - \"receives and executes messages from child\"\n\u251c\u2500 ExitHelper - \"processes exits from stored event roots in CheckpointManager\"\n\u251c\u2500 StateSender - \"sends messages to child\"\n```\n\n### General Repo Layout\n\nThis repo is a hybrid [Hardhat](https://hardhat.org) and [Foundry](https://getfoundry.sh/) environment. There are a number of add-ons, some of which we will detail here. Unlike standard Foundry environments, the contracts are located in `contracts/` (as opposed to `src/`) in order to conform with the general Hardhat project architecture. The Foundry/Solidity tests live in `test/forge/` whereas the Hardhat/Typescript tests are at the root level of `test/`. (For more details on the tests, see [Running Tests](#running-tests) in the [Using This Repo](#using-this-repo) section.) This can result in the actual test coverage on the repo being hard to read, as `solidity-coverage` and Foundry's native coverage tools do not natively communicate with each other. (In addition, Foundry's tool does not currently reflect branch coverage in Solidity libraries properly ([source](https://github.com/foundry-rs/foundry/issues/4854)), though this will likely be remediated in the future.)\n\nPart of the rationale is that while Foundry provides a set of options well suited to testing bridges, there are still aspects of the codebase which cannot be tested in native Solidity, particularly\n\nThe following is a brief overview of some of the files and directories in the project root:\n\n```ml\n\u2502 .github/workflows/ - \"CI (Github Actions) script: formats, lints, runs tests, coverage, Slither\"\n\u2502 contracts/ - \"all smart contracts, including mocks, but excluding Foundry tests and libs\"\n\u2502 docs/ - \"smart contract docs autogenerated from natspec\"\n\u2502 lib/ - \"smart contract libraries utilized by Foundry\"\n\u2502 scripts/ - \"Hardhat scripts, currently not updated, may contain deployment scripts in the future\"\n\u2502 test/ - \"both HH/TS and Foundry/Sol tests\"\n\u2502 ts/ - \"Typescript libraries for BLS/Elliptic Curves for testing BLS/BN256G2\"\n\u2502 types/ - \"Typescript types\"\n\u2502 .env.example - \"example env var file for using the HH env to connect with public nets/testnets\"\n\u2502 .eslint.js - \"JavaScript/TypeScript linter settings\"\n\u2502 .nvmrc - \"recommended Node version using nvm\"\n\u2502 .prettierrc - \"code formatting settings\"\n\u2502 .solcover.js - \"solidity-coverage settings\"\n\u2502 .solhint.json - \"Solidity linter settings\"\n\u2502 foundry.toml - \"Foundry configuration file\"\n\u2502 hardhat.config.ts - \"Hardhat configuration file\"\n\u2502 slither.config.json - \"settings for the Slither static analyzer\"\n```\n\nThe `package-lock.json` is also provided to ensure the ability to install the same versions of the npm packages used in development and testing.\n\n## Using This Repo\n\n### Requirements\n\nIn order to work with this repo locally, you will need Node (preferably using [nvm](https://github.com/nvm-sh/nvm)) in order to work with the Hardhat part of the repo.\n\nIn addition, to work with Foundry, you will need to have it installed. The recommended method is to use their `foundryup` tool, which can be installed (and automatically install Foundry) using this command:\n\n```bash\ncurl -L https://foundry.paradigm.xyz | bash\n```\n\nNote that this only works on Linux and Mac. For Windows, or if `foundryup` doesn't work, consult [their documentation](https://book.getfoundry.sh/getting-started/installation).\n\n### General Repo Layout\n\nThis repo is a hybrid [Hardhat](https://hardhat.org) and [Foundry](https://getfoundry.sh/) environment. There are a number of add-ons, some of which we will detail here. Unlike standard Foundry environments, the contracts are located in `contracts/` (as opposed to `src/`) in order to conform with the general Hardhat project architecture. The Foundry/Solidity tests live in `test/forge/` whereas the Hardhat/Typescript tests are at the root level of `test/`. (For more details on the tests, see [Running Tests](#running-tests) in the [Using This Repo](#using-this-repo) section.)\n\nInstall Foundry libs:\n\nIn addition, to work with Foundry, you will need to have it installed. The recommended method is to use their `foundryup` tool, which can be installed (and automatically install Foundry) using this command:\n\n```bash\ncurl -L https://foundry.paradigm.xyz | bash\n```\n\nNote that this only works on Linux and Mac. For Windows, or if `foundryup` doesn't work, consult [their documentation](https://book.getfoundry.sh/getting-started/installation).\n\n### Installation\n\n**You do not need to clone this repo in order to interact with the Polygon core contracts**\n\nIf you would like to work with these contracts in a development environment, first clone the repo:\n\n```bash\ngit clone git@github.com:maticnetwork/v3-contracts.git\n```\n\nIf you have [nvm](https://github.com/nvm-sh/nvm) installed (recommended), you can run `nvm use #` to set your version of Node to the same as used in development and testing.\n\nInstall JS/TS (Hardhat) dependencies:\n\n```bash\nnpm i\n```\n\nInstall Foundry libs:\n\n```bash\nforge install\n```\n\n### Deployment\n\nDeploying these contracts in the context of a production blockchain is out of the scope of this repo, as it requires a client that has support of the Edge specification integrated. At current, Edge maintains its own client [here](https://github.com/0xPolygon/polygon-edge), which can be consulted.\n\nOne point that is worth emphasizing in this context is that from the perspective of launching a Supernet is understanding genesis contracts. Another is that for at least the time being, the decision has been made to proxify all genesis contracts in order to facilitate upgrades/updates without necessitating a hardfork or regenesis. All deployment scripts in `script/deployment` use OpenZeppelin's `TransparentUpgradeableProxy`.\n\n#### Root contracts\n\nDeployment scripts have been provided for each of the root chain contracts. (The child chain contracts are genesis contracts, and are not deployed traditionally; they are deployed by the client as a part of the genesis of the child chain.)\n\nSome contracts in the Edge suite need be deployed only once on root. These contracts can deployed using the [`DeploySharedRootContracts`](script/deployment/DeploySharedRootContracts.sol) script, after [`sharedRootContractsConfig`](script/deployment/sharedRootContractsConfig.json) has been filled with appropriate values.\n\nOther contracts are deployed on root once per Supernet. These contracts can deployed using the [`DeployNewRootContractSet.s.sol`](script/deployment/DeployNewRootContractSet.s.sol) script, after [`rootContractSetConfig.json`](script/deployment/rootContractSetConfig.json) has been filled with appropriate values.\n\nNote that the script does not initialize `CheckpointManager`. Instead, it protects it to be initializable only by the `INITIATOR` address later.\n\nNot all root contracts are deployed at this point, however. There are parts of the bridge that need the addresses of various child contracts in order to be initialized. These contracts can deployed using the [`DeployRootTokenContracts.s.sol`](script/deployment/DeployRootTokenContracts.s.sol) script, after [`rootTokenContractsConfig.json`](script/deployment/rootTokenContractsConfig.json) has been filled with appropriate values.\n\nScripts are run by invoking:\n\n```bash\nforge <SCRIPT_NAME> \\\n  --broadcast \\\n  <SIGNING_METHOD> \\\n  --rpc-url <RPC_URL> \\\n  --verify \\\n  --sig \"run()\"\n```\n\nFor the signing method and other options, consult [Foundry Book](https://book.getfoundry.sh/reference/forge/forge-script).\n\n### Environment Setup\n\nThere are a few things that should be done to set up the repo once you've cloned it and installed the dependencies and libraries. An important step for various parts of the repo to work properly is to set up a `.env` file. There is an `.example.env` file provided, copy it and rename the copy `.env`.\n\nThe v3 contract set is meant to be deployed across two blockchains, which are called the root chain and child chain. In the case of Polygon POS v3 itself, Ethereum mainnet is the root chain, while Polygon POS v3 is the child chain. In order to give users the ability to work with these contracts on the chains of their choice, four networks are configured in Hardhat: `root`, `rootTest`, `child`, and `childTest`. To interact with whichever networks you would like to use as root and/or child, you will need to add a URL pointing to an RPC endpoint on the relevant chain in your `.env` file. This can be a RPC provider such as Ankr or Alchemy, in which case you would put the entire URL including the API key into the relevant line of the `.env`, or could be a local node, in which case you would put `https://localhost:<PORT_NUMBER>` (usually 8545).\n\nA field for a private key is also provided in the `.env`. You will need to input this if you are interacting with any public networks (for example, deploying the contracts to a testnet).\n\nLastly, there are fields for an Etherscan and Polygonscan for verifying any deployed contracts on the Ethereum or Polygon mainnets or testnets. (Some additional configuration may be required, only Eth mainnet, Goerli, Polygon POS v1, and Mumbai are configured as of this writing.)\n\n### Compiling Contracts\n\n**Hardhat:**\n\n```bash\nnpx hardhat compile --show-stack-traces\n```\n\n`hardhat-ts` automatically generates typings for you after compilation, to use in tests and scripts. You can import them like: `import { ... } from \"../typechain-types\";`\n\nSimilarly, the `hardhat-dodoc` package autogenerates smart contract documentation in `docs/` every time Hardhat compiles the contract. If you wish to disable this, uncomment the `runOnCompile: false` line in the `dodoc` object in `hardhat.config.ts`.\n\n**Foundry:**\n\n```bash\nforge build\n```\n\n### Running tests\n\nAs mentioned previously, there are two separate test suites, one in Hardhat/Typescript, and the other in Foundry/Solidity. The HH tests are structured more as scenario tests, generally running through an entire interaction or process, while the Foundry tests are structured more as unit tests. This is coincidental, and is not a set rule.\n\n**Hardhat:**\n\n```bash\nnpx hardhat test\n```\n\nThe Hardhat tests have gas reporting enabled by default, you can disable them from `hardhat.config.ts` by setting `enabled` in the `gasReporter` object in `hardhat.config.ts` or by setting `REPORT_GAS` to `false` in the `.env`.\n\n**Foundry:**\n\n```bash\nforge test\n```\n\nSimple gas profiling is included in Foundry tests by default. For a more complete gas profile using Foundry, see [their documentation](https://book.getfoundry.sh/forge/gas-reports).\n\nSimple gas profiling is included in Foundry tests by default. For a more complete gas profile using Foundry, see [their documentation](https://book.getfoundry.sh/forge/gas-reports).\n\n### Linting\n\nThe linters run from inside the Hardhat/JS environment.\n\n```bash\nnpm run lint      # runs all linters at once\n\nnpm run lint:sol  # only runs solhint and prettier\nnpm run lint:ts   # only runs prettier and eslint\n```\n\n### Check Test Coverage\n\nWe do not know of a way to see the general coverage from the TS and Solidity tests combined at this juncture. Instead, the coverage of each suite can be checked individually.\n\n**Hardhat:**\n\n```bash\nnpx hardhat coverage\n\n# or\n\nnpm run coverage\n```\n\n**Foundry:**\n\n```bash\nforge coverage\n```\n\n### Run Slither\n\nFirst, install slither by following the instructions [here](https://github.com/crytic/slither#how-to-install).\nThen, run:\n\n```bash\nslither .\n\n# or\n\nnpm run slither\n```\n\n### Continuous Integration\n\nThere is a CI script for Github Actions in `.github/workflows/`. Currently it runs:\n\n- linters\n- both test suites (fails if any tests fail)\n- coverage report (currently only HH)\n- Slither\n\n### Documentation\n\nThis repo makes use of [Dodoc](https://github.com/primitivefinance/primitive-dodoc), a Hardhat plugin from Primitive Finance which generates Markdown docs on contracts from their natspec. The docs are generated on every compile, and can be found in the `docs/` directory.\n", "release_dates": ["2022-09-19T12:29:29Z"]}, {"name": "eth-event-tracker", "description": "Tracker of Ethereum events", "language": "Go", "license": null, "readme": "# Eth-event-tracker\n\n## Usage\n\nTODO\n\n## CLI\n\nQuery the Ethereum 2.0 deposit contract:\n\n```\ngo run cli/event-tracker/main.go --endpoint https://mainnet.infura.io/v3/... --start-block 11052984 --target 0x00000000219ab540356cbb839cbe05303d7705fa --storage my.db\n```\n", "release_dates": []}, {"name": "eth-state-transition", "description": "Ethereum state transition", "language": "Go", "license": null, "readme": "\n# Eth-state-transition\n\nEthereum state transition function.\n\n## Usage\n\n```golang\n\nimport (\n    itrie \"github.com/0xPolygon/eth-state-transition/immutable-trie\"\n    \"github.com/0xPolygon/eth-state-transition/runtime\"\n    state \"github.com/0xPolygon/eth-state-transition\"\n)\n\nfunc main() {\n    // get a reference for the state\n    state := itrie.NewArchiveState(itrie.NewMemoryStorage())\n    snap := s.NewSnapshot()\n\n    // create a transition object\n    forks := runtime.ForksInTime{}\n    config := runtime.TxContext{}\n    transition := state.NewTransition(forks, config, snap)\n\n    // process a transaction\n    result, err := transition.Write(&state.Transaction{})\n    if err != nil {\n        panic(err)\n    }\n\n    fmt.Printf(\"Logs: %v\\n\", result.Logs)\n    fmt.Printf(\"Gas used: %d\\n\", result.GasUsed)\n\n    // retrieve the state data changed\n    objs := transition.Commit()\n\n    // commit the data to the state\n    if _, err := snap.Commit(objs); err != nil {\n        panic(err)\n    }\n}\n```\n", "release_dates": []}, {"name": "eth-txn-spam", "description": "Multi-threaded transaction spam", "language": "TypeScript", "license": null, "readme": "# txn-spam \nThis is a stress test script you can run against v3. It will spam various kinds of transactions from multiple base mnemonics and all their child accounts.\n\nEnsure you have at least 12 cores if you want to spam from more than 3-4 base mnemonics at once. Ideally 16 cores, otherwise your machine will be the limiting factor and you might not spam as many transactions as you would hope for. \n\n# Setup\n\n[Optional]:\n    Make a file `mnemonics` in this repo and populate it with 12 word mnemonics, one for each line. Fund the base accounts for each mnemonic  with a bunch of ETH. You can use https://iancoleman.io/bip39/ to generate mnemonics and derive the base account. \n\n`$ ./init.sh` - initializes and/or starts v3 instances before we can do actual spamming\n  - `$ ./init.sh --help` to see all the commands and options\n  - `$ ./init.sh setup` creates main spam account, validator accounts and config.json.\n  - `$ ./init.sh docker` creates docker image and deploys smart contract to v3 instance\n\n`config.json` is a configuration for txn spam. By default it is saved to the `~/borv3` directory. it has properties:\n  - `privateKey` private key for main txn spam account\n  - `spamContractAddress` address of smart contract used for modes 1 and 2\n  - `rpcUrl` rpc url of v3 instance we want to spam\n\n# Usage\n\nRun script with four arguments\n\n`$ ./start.sh 2 100 0` starts actual test\n\n- `RPC_URL` is the RPC URL of the node you wish to target\n- `PARALLELISM` is the amount of individual processes you want to run\n- `RANGE` is how many derivations (sub accounts) of each mnemonic you want to use at a time. Essentially another parallelism parameter. For example, a range of 10 means that for each mnemonic, 10 child accounts will be sending transactions in parallel. *The transactions do not wait for the previous one to finish, they simply increase the nonce for the next one in line, greatly improving the speed*.\n- `MODE` is the type of transaction you wish to send. Follow modes are supported:\n  - 0 is regular transfer (default if not passed)\n  - 1 approximates ERC20 approval or ERC20 transfer (~100k gas)\n  - 2 is a complex transaction (~250k gas)\n  - 3 is a contract deploy (~600k gas)\n\nYou shouldn't have to top up the gas frequently as it is hard-coded to send all transactions with 1 gwei. Moreover, mode 0 essentially just sends money back and forth between accounts. \n\n*Note that there is some startup time for the script to distribute $ if necessary to its sub accounts*\n\nStopping:\n\n`$ ./stop.sh`\n\n# Without docker\n`$ npm run compile && env RANGE=100 MODE=0 node ./dist/index.js`\n\n\n\n\n\n", "release_dates": []}, {"name": "forge-chronicles", "description": null, "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Forge Chronicles\n\nThe Forge Chronicles library is a tool that generates a json file and a human readable markdown log file with the deployment information about contracts deployed using `forge script`. It can keep track of upgrades for transparent proxies and generates a history of deployments. An example for such a log can be found [here](https://github.com/0xPolygon/pol-token/blob/main/deployments/5.md). Logs are generated by extracting information from the `run-latest.json` file generated by `forge script` in the `broadcast` directory.\n\n## Requirements\n\nThe script utilizes Node.js to run. We recommend the node version defined in the `.nvmrc` file.\n\n## Installation\n\n```bash\nforge install 0xPolygon/forge-chronicles\n```\n\n## Usage Example\n\nThe following command will create a log for the contracts deployed in the `Deploy.s.sol` script on Ethereum mainnet.\n\n```bash\nnode lib/forge-chronicles Deploy.s.sol --chain-id 1 --rpc-url https://mainnet.infura.io/v3/{your-infura-key}\n```\n\n## Usage\n\nThe deployment markdown & json log file will be placed within the `deployments` directory. The name of the file will be the chain id of the network where the script was executed (e.g., for Ethereum mainnet the files will be `deployments/1.md` and `deployments/json/1.json`).\n\nSupplying the RPC url _can_ be optional. When contracts are deployed, the RPC url is used to check whether the deployed contracts implement a `version()` function. If they do, the version is extracted and recorded within the log file. If no RPC url is supplied, the version check is skipped.\n\nSupplying the RPC url is _required_ when an upgrade is deployed for a transparent proxy. The script can recognize when an upgrade is deployed, however, most of the times, only an implementation is deployed and the actual upgrade is performed by a multisig or governance. When an upgrade is detected, the RPC url will be used to verify that the upgrade was performed on chain directly and only then, deployment log files will be generated. If the upgrade was not performed on chain yet, the script will abort.\n\n## Flags\n\n| --flag      | -flag | Description                                                                |\n| ----------- | ----- | -------------------------------------------------------------------------- |\n| --rpc-url   | -r    | RPC url used for versioning and upgrade verification (default: $RPC_URL)   |\n| --chain-id  | -c    | Chain id of the network where the script was executed (default: 31337)     |\n| --skip-json | -s    | Skip generation of json file and only generate markdown from existing json |\n| Options     |       |                                                                            |\n| --help      | -h    | Print help                                                                 |\n| --version   | -v    | Print the version number                                                   |\n\n## License\n\n\u200b\nLicensed under either of\n\u200b\n\n- Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)\n- MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n  \u200b\n\nat your option.\n\nUnless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.\n\n---\n\n\u00a9 2023 PT Services DMCC\n", "release_dates": ["2023-12-11T13:01:42Z", "2023-12-08T13:46:52Z"]}, {"name": "foundry-template", "description": "Contracts team, template repo", "language": "Solidity", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Template Repo (Foundry)\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![CI Status](../../actions/workflows/test.yaml/badge.svg)](../../actions)\n\nThis template repo is a quick and easy way to get started with a new Solidity project. It comes with a number of features that are useful for developing and deploying smart contracts. Such as:\n\n- Pre-commit hooks for formatting, auto generated documentation, and more\n- Various libraries with useful contracts (OpenZeppelin, Solady) and libraries (Deployment log generation, storage checks, deployer templates)\n\n#### Table of Contents\n\n- [Setup](#setup)\n- [Deployment](#deployment)\n- [Docs](#docs)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Setup\n\nFollow these steps to set up your local environment:\n\n- [Install foundry](https://book.getfoundry.sh/getting-started/installation)\n- Install dependencies: `forge install`\n- Build contracts: `forge build`\n- Test contracts: `forge test`\n\nIf you intend to develop on this repo, follow the steps outlined in [CONTRIBUTING.md](CONTRIBUTING.md#install).\n\n## Deployment\n\nThis repo utilizes versioned deployments. For more information on how to use forge scripts within the repo, check [here](CONTRIBUTING.md#deployment).\n\nSmart contracts are deployed or upgraded using the following command:\n\n```shell\nforge script script/Deploy.s.sol --broadcast --rpc-url <rpc_url> --verify\n```\n\n## Docs\n\nThe documentation and architecture diagrams for the contracts within this repo can be found [here](docs/).\nDetailed documentation generated from the NatSpec documentation of the contracts can be found [here](docs/autogen/src/src/).\nWhen exploring the contracts within this repository, it is recommended to start with the interfaces first and then move on to the implementation as outlined [here](CONTRIBUTING.md#natspec--comments)\n\n## Contributing\n\nIf you want to contribute to this project, please check [CONTRIBUTING.md](CONTRIBUTING.md) first.\n\n## License\n\n\u200b\nLicensed under either of\n\u200b\n\n- Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)\n- MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n  \u200b\n\nat your option.\n\nUnless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.\n\n---\n\n\u00a9 2023 PT Services DMCC\n", "release_dates": ["2023-12-10T02:52:58Z"]}, {"name": "fx-portal", "description": "FxPortal for Polygon (Previously Matic) chain. No mapping. Seamless.", "language": "Solidity", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# FX-Portal (Flexible Portal)\n\nFxPortal for Polygon (prev Matic) Chain. No mappings. Seamless communication with the Ethereum Network.\n\n### Audits\n\n- [Halborn](audits/Polygon_FX_Portal_Smart_Contract_Security_Audit_Halborn_v1_0.pdf)\n- [ChainSecurity](audits/ChainSecurity_Polygon_Fx_Portal_audit.pdf)\n\n### Contents\n\n- [What is FX-Portal](#what-is-fx-portal)\n  - [Some usecases](#some-usecases-of-fx-portal)\n  - [What about POS-Portal](#what-about-pos-portal)\n  - [Can I Build My Own Custom Bridge](#can-i-build-my-own-custom-bridge)\n- [What Can I Build With FX-Portal](#what-can-i-build-with-fx-portal)\n- [What are fxChild and fxRoot](#what-are-fxchild-and-fxroot)\n- [Deployment Addresses](#deployment-addresses)\n- [Development](#development)\n- [Proof Generation](#proof-generation)\n- [Future Plans](#future-plans)\n\n### What is FX-Portal?\n\n**A powerful yet simple implementation of Polygon's [state sync](https://wiki.polygon.technology/docs/category/state-sync/) mechanism. (The Polygon [POS-Portal bridge](https://github.com/maticnetwork/pos-portal/) is also based on it, but relies on a centralized party mapping tokens on one chain to their contract on the other.) There are examples of how to use the bridge in the `contracts/examples` directory. You can use these examples to build your own implementations or own custom bridge.**\n\nIn short, this bridge allows arbitrary message bridging without mapping, with built-in support for a number of token standards.\n\nIn more detail, FX-Portal makes use of a message passing mechanism built into the Polygon POS chain, leveraging it to pass messages from one chain to another. Using a mechanism for generating deterministic token addresses, this can be used to create asset bridges without the need for a centralized party documenting the relationship between assets on the two chains (commonly referred to as a 'mapping' here).\n\n#### Some usecases of FX-Portal\n\n- [ERC20 token tranfer from Ethereum to Polygon POS without mapping request](https://github.com/0xPolygon/fx-portal/tree/main/contracts/examples/erc20-transfer)\n- [Lazy minting of ERC20 tokens on Polygon POS](https://github.com/0xPolygon/fx-portal/tree/main/contracts/examples/mintable-erc20-transfer)\n- [State Transfer between Ethereum mainnet and Polygon POS](https://github.com/0xPolygon/fx-portal/tree/main/contracts/examples/state-transfer)\n\n#### What about [POS-Portal](https://github.com/maticnetwork/pos-portal/)?\n\nPOS-Portal is another bridge, but it works only for few ERC standards and requires mappings. It is more developer-friendly in some ways, and allows customization without much headache.\n\nWhile FX-Portal focuses on permissionlessness and flexibility, a developer might have to write more code than POS-Portal. On the other hand, FX-Portal requires no mapping, meaning that there is no need to rely on an authorized party to submit the mapping with FX-Portal.\n\n#### Can I build my own custom bridge?\n\nYes. You can check docs here: https://wiki.polygon.technology/docs/pos/design/bridge/l1-l2-communication/fx-portal/\n\n### What can I build with FX-Portal?\n\n- Arbitrary state bridge (examples/state-transfer)\n- Normal ERC20 bridge (examples/erc2-transfer)\n- ERC20 token generator bridge (example/mintable-erc20-transfer)\n\n### What are FxChild and FxRoot?\n\n`FxChild` (`FxChild.sol`) and `FxRoot` (`FxRoot.sol`) are the main contracts on which the bridge works. It calls and passes data to user-defined methods on another chain without needing a mapping. You can deploy your own `FxChild` and `FxRoot`, but there is no need. If you pass the data to the deployed instances of `FxChild` or `FxRoot`, the clients will pick up the data and pass it to the other chain.\n\n### Deployment Addresses\n\n**Mumbai**\n\n| Contract                                                                                                                | Deployed address                             |\n| :---------------------------------------------------------------------------------------------------------------------- | :------------------------------------------- |\n| [FxRoot (Goerli)](https://goerli.etherscan.io/address/0x3d1d3E34f7fB6D26245E6640E1c50710eFFf15bA#code)                  | `0x3d1d3E34f7fB6D26245E6640E1c50710eFFf15bA` |\n| [FxChild (Mumbai)](https://explorer-mumbai.maticvigil.com/address/0xCf73231F28B7331BBe3124B907840A94851f9f11/contracts) | `0xCf73231F28B7331BBe3124B907840A94851f9f11` |\n\n**Mainnet**\n\n| Contract                                                                                                                         | Deployed address                             |\n| :------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------- |\n| [FxRoot (Ethereum Mainnet)](https://etherscan.io/address/0xfe5e5d361b2ad62c541bab87c45a0b9b018389a2#code)                        | `0xfe5e5D361b2ad62c541bAb87C45a0B9B018389a2` |\n| [FxChild (Matic Mainnnet)](https://explorer-mainnet.maticvigil.com/address/0x8397259c983751DAf40400790063935a11afa28a/contracts) | `0x8397259c983751DAf40400790063935a11afa28a` |\n\n### Development\n\nThis project can be compiled and tested with Hardhat and Foundry. Hardhat unit tests are located in [/hardhat](/hardhat) and Foundry unit + invariant tests can be found in the [/test](/test) directory.\n\n- Setup: `yarn install` and/or `forge install`\n- Compile: `yarn build` and/or `forge build`\n- Test: `yarn test` and/or `forge test -vvv`\n- Coverage: `yarn coverage`\n- Lint: `yarn lint`, `yarn prettier:write`\n\n#### Proof Generation\n\nA common question is how to generate proofs for the bridge. We'll explain what that means, and then list solutions.\n\nTo withdraw tokens on the root chain, first we call the relevant `withdraw()` method the child tunnel contract (which burn the respective tokens on child); and we use this transaction hash to generate proof of inclusion which acts as the argument to receiveMessage() in the respective root tunnel contract. Please see [here](https://wiki.polygon.technology/docs/pos/design/bridge/l1-l2-communication/fx-portal/#withdraw-tokens-on-the-root-chain).\n\nTo generate the proof, you can either use the [proof generation API](https://proof-generator.polygon.technology/api/v1/matic/exit-payload/%7BburnTxHash%7D?eventSignature=%7BeventSignature%7D) hosted by Polygon or you can also spin up your own proof generation API by following the instructions [here](https://github.com/maticnetwork/proof-generation-api).\n\nThe test suite also replicates proof generation (located at [hardhat/tunnel/payload](hardhat/tunnel/payload)), the following _hardhat task_ can help generating the proof for custom chains.\n\n**Usage**: `npx hardhat exit-proof --help`\n\n```\nhardhat [GLOBAL OPTIONS] exit-proof [--sig <STRING>] --tx <STRING>\n\nOPTIONS:\n\n--sig log event hex signature (defaults to 0x8c5261668696ce22758910d05bab8f186d6eb247ceac2af2e82c7dc17669b036 for `MessageSent(bytes)`)\n--tx burn transaction hash\n\nexit-proof: Generates exit proof for the given burn transaction hash\n```\n\n**Example**: `npx hardhat exit-proof --network polygon --tx 0x1cfc2658719e6d1753e091ce4515507711fe649269e75023c0f1123cd6e37c1a`\n\n```\n\u279c npx hardhat exit-proof --network polygon --tx 0x1cfc2658719e6d1753e091ce4515507711fe649269e75023c0f1123cd6e37c1a\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\n```\n### Future Plans\n\nPolygon has [announced](https://polygon.technology/blog/polygon-2-0-polygon-pos-zk-layer-2) its intent to migrate the chain to a zk validium. This would mean changes to the architecture and infrastructure. One solution that has been discussed is an [LXLY Bridge](https://www.youtube.com/watch?v=z44yN5ApOE8), a bridge structure developed for zkEVM. ([Alternative presentation](https://www.youtube.com/watch?v=sIbw5qqiXFk&t=2079s) timestamped at the section about the bridge)", "release_dates": ["2022-09-07T06:33:22Z", "2022-01-27T13:37:42Z", "2021-10-13T09:37:42Z", "2021-08-12T08:51:33Z", "2021-08-12T08:48:45Z", "2021-08-12T08:45:36Z", "2021-01-22T11:48:13Z", "2021-01-17T15:22:33Z"]}, {"name": "go-bls", "description": "Go wrapper for a BLS12-381 Signature Aggregation implementation in C++", "language": null, "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "readme": "# Go-BLS Signature Aggregation\n\nThis repository is a go-wrapper around [@herumi](https://github.com/herumi)'s BLS implementation using the underlying [herumi/mcl](https://github.com/herumi/mcl) cryptography library written in C++. The go wrapper is also based on [@herumi](https://github.com/herumi)'s original BLS C++ project [herumi/bls](https://github.com/herumi/bls). The code was modified by Prysmatic Labs in order to integrate it into Ethereum's Serenity upgrade including Proof of Stake and Sharding.\n\n## Docs\n\nTo see examples on how to use herumi's BLS library, see the `example/` directory in the repo containing single signature verification in addition to aggregate signature verification.\n\nSee more information on Godoc [here](https://godoc.org/github.com/prysmaticlabs/go-bls).\n\n## Installing\n\n```bash\ngit clone https://github.com/prysmaticlabs/go-bls\n```\n\n#### Running With Go\n\nMake sure you have the latest version of Go installed along with a c/c++ compiler and [libgmp](https://gmplib.org/) required for precision arithmetic. Then, you can run the local bls tests using the go tool:\n\n```bash\ngo test -bench .\ngoos: linux\ngoarch: amd64\npkg: github.com/prysmaticlabs/go-bls\nBenchmarkPubkeyFromSeckey-4   \t   10000\t    221235 ns/op\nBenchmarkSigning-4            \t    5000\t    274591 ns/op\nBenchmarkValidation-4         \t    1000\t   1305703 ns/op\nPASS\nok  \tgithub.com/prysmaticlabs/go-bls\t7.814s\n```\n\n#### Running With Bazel\n\nInstall Google's Bazel build tool [here](https://docs.bazel.build/versions/master/install-ubuntu.html) for your architecture. Then, run tests as follows:\n\n```bash\nbazel test //...\nINFO: Analysed 3 targets (1 packages loaded).\nINFO: Found 2 targets and 1 test target...\nINFO: Elapsed time: 6.543s, Critical Path: 5.53s\nINFO: 17 processes: 17 linux-sandbox.\nINFO: Build completed successfully, 18 total actions\n//:go_default_test                                                       PASSED in 2.0s\n\nExecuted 1 out of 1 test: 1 test passes.\nThere were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option INFO: Build completed successfully, 18 total actions\n```\n\n## License\n\nThe original BLS code was written by @herumi under the BSD-3 software license.", "release_dates": []}, {"name": "go-ethereum", "description": "Official Go implementation of the Ethereum protocol", "language": "Go", "license": {"key": "lgpl-3.0", "name": "GNU Lesser General Public License v3.0", "spdx_id": "LGPL-3.0", "url": "https://api.github.com/licenses/lgpl-3.0", "node_id": "MDc6TGljZW5zZTEy"}, "readme": "## Go Ethereum\n\nOfficial Golang execution layer implementation of the Ethereum protocol.\n\n[![API Reference](\nhttps://camo.githubusercontent.com/915b7be44ada53c290eb157634330494ebe3e30a/68747470733a2f2f676f646f632e6f72672f6769746875622e636f6d2f676f6c616e672f6764646f3f7374617475732e737667\n)](https://pkg.go.dev/github.com/ethereum/go-ethereum?tab=doc)\n[![Go Report Card](https://goreportcard.com/badge/github.com/ethereum/go-ethereum)](https://goreportcard.com/report/github.com/ethereum/go-ethereum)\n[![Travis](https://travis-ci.com/ethereum/go-ethereum.svg?branch=master)](https://travis-ci.com/ethereum/go-ethereum)\n[![Discord](https://img.shields.io/badge/discord-join%20chat-blue.svg)](https://discord.gg/nthXNEv)\n\nAutomated builds are available for stable releases and the unstable master branch. Binary\narchives are published at https://geth.ethereum.org/downloads/.\n\n## Building the source\n\nFor prerequisites and detailed build instructions please read the [Installation Instructions](https://geth.ethereum.org/docs/install-and-build/installing-geth).\n\nBuilding `geth` requires both a Go (version 1.16 or later) and a C compiler. You can install\nthem using your favourite package manager. Once the dependencies are installed, run\n\n```shell\nmake geth\n```\n\nor, to build the full suite of utilities:\n\n```shell\nmake all\n```\n\n## Executables\n\nThe go-ethereum project comes with several wrappers/executables found in the `cmd`\ndirectory.\n\n|    Command    | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| :-----------: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n|  **`geth`**   | Our main Ethereum CLI client. It is the entry point into the Ethereum network (main-, test- or private net), capable of running as a full node (default), archive node (retaining all historical state) or a light node (retrieving data live). It can be used by other processes as a gateway into the Ethereum network via JSON RPC endpoints exposed on top of HTTP, WebSocket and/or IPC transports. `geth --help` and the [CLI page](https://geth.ethereum.org/docs/interface/command-line-options) for command line options.          |\n|   `clef`    | Stand-alone signing tool, which can be used as a backend signer for `geth`.  |\n|   `devp2p`    | Utilities to interact with nodes on the networking layer, without running a full blockchain. |\n|   `abigen`    | Source code generator to convert Ethereum contract definitions into easy-to-use, compile-time type-safe Go packages. It operates on plain [Ethereum contract ABIs](https://docs.soliditylang.org/en/develop/abi-spec.html) with expanded functionality if the contract bytecode is also available. However, it also accepts Solidity source files, making development much more streamlined. Please see our [Native DApps](https://geth.ethereum.org/docs/dapp/native-bindings) page for details. |\n|  `bootnode`   | Stripped down version of our Ethereum client implementation that only takes part in the network node discovery protocol, but does not run any of the higher level application protocols. It can be used as a lightweight bootstrap node to aid in finding peers in private networks.                                                                                                                                                                                                                                                                 |\n|     `evm`     | Developer utility version of the EVM (Ethereum Virtual Machine) that is capable of running bytecode snippets within a configurable environment and execution mode. Its purpose is to allow isolated, fine-grained debugging of EVM opcodes (e.g. `evm --code 60ff60ff --debug run`).                                                                                                                                                                                                                                                                     |\n|   `rlpdump`   | Developer utility tool to convert binary RLP ([Recursive Length Prefix](https://ethereum.org/en/developers/docs/data-structures-and-encoding/rlp)) dumps (data encoding used by the Ethereum protocol both network as well as consensus wise) to user-friendlier hierarchical representation (e.g. `rlpdump --hex CE0183FFFFFFC4C304050583616263`).                                                                                                                                                                                                                                 |\n|   `puppeth`   | a CLI wizard that aids in creating a new Ethereum network.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n\n## Running `geth`\n\nGoing through all the possible command line flags is out of scope here (please consult our\n[CLI Wiki page](https://geth.ethereum.org/docs/interface/command-line-options)),\nbut we've enumerated a few common parameter combos to get you up to speed quickly\non how you can run your own `geth` instance.\n\n### Hardware Requirements\n\nMinimum:\n\n* CPU with 2+ cores\n* 4GB RAM\n* 1TB free storage space to sync the Mainnet\n* 8 MBit/sec download Internet service\n\nRecommended:\n\n* Fast CPU with 4+ cores\n* 16GB+ RAM\n* High-performance SSD with at least 1TB of free space\n* 25+ MBit/sec download Internet service\n\n### Full node on the main Ethereum network\n\nBy far the most common scenario is people wanting to simply interact with the Ethereum\nnetwork: create accounts; transfer funds; deploy and interact with contracts. For this\nparticular use case, the user doesn't care about years-old historical data, so we can\nsync quickly to the current state of the network. To do so:\n\n```shell\n$ geth console\n```\n\nThis command will:\n * Start `geth` in snap sync mode (default, can be changed with the `--syncmode` flag),\n   causing it to download more data in exchange for avoiding processing the entire history\n   of the Ethereum network, which is very CPU intensive.\n * Start the built-in interactive [JavaScript console](https://geth.ethereum.org/docs/interface/javascript-console),\n   (via the trailing `console` subcommand) through which you can interact using [`web3` methods](https://github.com/ChainSafe/web3.js/blob/0.20.7/DOCUMENTATION.md) \n   (note: the `web3` version bundled within `geth` is very old, and not up to date with official docs),\n   as well as `geth`'s own [management APIs](https://geth.ethereum.org/docs/rpc/server).\n   This tool is optional and if you leave it out you can always attach it to an already running\n   `geth` instance with `geth attach`.\n\n### A Full node on the G\u00f6rli test network\n\nTransitioning towards developers, if you'd like to play around with creating Ethereum\ncontracts, you almost certainly would like to do that without any real money involved until\nyou get the hang of the entire system. In other words, instead of attaching to the main\nnetwork, you want to join the **test** network with your node, which is fully equivalent to\nthe main network, but with play-Ether only.\n\n```shell\n$ geth --goerli console\n```\n\nThe `console` subcommand has the same meaning as above and is equally\nuseful on the testnet too.\n\nSpecifying the `--goerli` flag, however, will reconfigure your `geth` instance a bit:\n\n * Instead of connecting to the main Ethereum network, the client will connect to the G\u00f6rli\n   test network, which uses different P2P bootnodes, different network IDs and genesis\n   states.\n * Instead of using the default data directory (`~/.ethereum` on Linux for example), `geth`\n   will nest itself one level deeper into a `goerli` subfolder (`~/.ethereum/goerli` on\n   Linux). Note, on OSX and Linux this also means that attaching to a running testnet node\n   requires the use of a custom endpoint since `geth attach` will try to attach to a\n   production node endpoint by default, e.g.,\n   `geth attach <datadir>/goerli/geth.ipc`. Windows users are not affected by\n   this.\n\n*Note: Although some internal protective measures prevent transactions from\ncrossing over between the main network and test network, you should always\nuse separate accounts for play and real money. Unless you manually move\naccounts, `geth` will by default correctly separate the two networks and will not make any\naccounts available between them.*\n\n### Full node on the Rinkeby test network\n\nGo Ethereum also supports connecting to the older proof-of-authority based test network\ncalled [*Rinkeby*](https://www.rinkeby.io) which is operated by members of the community.\n\n```shell\n$ geth --rinkeby console\n```\n\n### Full node on the Ropsten test network\n\nIn addition to G\u00f6rli and Rinkeby, Geth also supports the ancient Ropsten testnet. The\nRopsten test network is based on the Ethash proof-of-work consensus algorithm. As such,\nit has certain extra overhead and is more susceptible to reorganization attacks due to the\nnetwork's low difficulty/security.\n\n```shell\n$ geth --ropsten console\n```\n\n*Note: Older Geth configurations store the Ropsten database in the `testnet` subdirectory.*\n\n### Configuration\n\nAs an alternative to passing the numerous flags to the `geth` binary, you can also pass a\nconfiguration file via:\n\n```shell\n$ geth --config /path/to/your_config.toml\n```\n\nTo get an idea of how the file should look like you can use the `dumpconfig` subcommand to\nexport your existing configuration:\n\n```shell\n$ geth --your-favourite-flags dumpconfig\n```\n\n*Note: This works only with `geth` v1.6.0 and above.*\n\n#### Docker quick start\n\nOne of the quickest ways to get Ethereum up and running on your machine is by using\nDocker:\n\n```shell\ndocker run -d --name ethereum-node -v /Users/alice/ethereum:/root \\\n           -p 8545:8545 -p 30303:30303 \\\n           ethereum/client-go\n```\n\nThis will start `geth` in snap-sync mode with a DB memory allowance of 1GB, as the\nabove command does.  It will also create a persistent volume in your home directory for\nsaving your blockchain as well as map the default ports. There is also an `alpine` tag\navailable for a slim version of the image.\n\nDo not forget `--http.addr 0.0.0.0`, if you want to access RPC from other containers\nand/or hosts. By default, `geth` binds to the local interface and RPC endpoints are not\naccessible from the outside.\n\n### Programmatically interfacing `geth` nodes\n\nAs a developer, sooner rather than later you'll want to start interacting with `geth` and the\nEthereum network via your own programs and not manually through the console. To aid\nthis, `geth` has built-in support for a JSON-RPC based APIs ([standard APIs](https://ethereum.github.io/execution-apis/api-documentation/)\nand [`geth` specific APIs](https://geth.ethereum.org/docs/rpc/server)).\nThese can be exposed via HTTP, WebSockets and IPC (UNIX sockets on UNIX based\nplatforms, and named pipes on Windows).\n\nThe IPC interface is enabled by default and exposes all the APIs supported by `geth`,\nwhereas the HTTP and WS interfaces need to manually be enabled and only expose a\nsubset of APIs due to security reasons. These can be turned on/off and configured as\nyou'd expect.\n\nHTTP based JSON-RPC API options:\n\n  * `--http` Enable the HTTP-RPC server\n  * `--http.addr` HTTP-RPC server listening interface (default: `localhost`)\n  * `--http.port` HTTP-RPC server listening port (default: `8545`)\n  * `--http.api` API's offered over the HTTP-RPC interface (default: `eth,net,web3`)\n  * `--http.corsdomain` Comma separated list of domains from which to accept cross origin requests (browser enforced)\n  * `--ws` Enable the WS-RPC server\n  * `--ws.addr` WS-RPC server listening interface (default: `localhost`)\n  * `--ws.port` WS-RPC server listening port (default: `8546`)\n  * `--ws.api` API's offered over the WS-RPC interface (default: `eth,net,web3`)\n  * `--ws.origins` Origins from which to accept WebSocket requests\n  * `--ipcdisable` Disable the IPC-RPC server\n  * `--ipcapi` API's offered over the IPC-RPC interface (default: `admin,debug,eth,miner,net,personal,txpool,web3`)\n  * `--ipcpath` Filename for IPC socket/pipe within the datadir (explicit paths escape it)\n\nYou'll need to use your own programming environments' capabilities (libraries, tools, etc) to\nconnect via HTTP, WS or IPC to a `geth` node configured with the above flags and you'll\nneed to speak [JSON-RPC](https://www.jsonrpc.org/specification) on all transports. You\ncan reuse the same connection for multiple requests!\n\n**Note: Please understand the security implications of opening up an HTTP/WS based\ntransport before doing so! Hackers on the internet are actively trying to subvert\nEthereum nodes with exposed APIs! Further, all browser tabs can access locally\nrunning web servers, so malicious web pages could try to subvert locally available\nAPIs!**\n\n### Operating a private network\n\nMaintaining your own private network is more involved as a lot of configurations taken for\ngranted in the official networks need to be manually set up.\n\n#### Defining the private genesis state\n\nFirst, you'll need to create the genesis state of your networks, which all nodes need to be\naware of and agree upon. This consists of a small JSON file (e.g. call it `genesis.json`):\n\n```json\n{\n  \"config\": {\n    \"chainId\": <arbitrary positive integer>,\n    \"homesteadBlock\": 0,\n    \"eip150Block\": 0,\n    \"eip155Block\": 0,\n    \"eip158Block\": 0,\n    \"byzantiumBlock\": 0,\n    \"constantinopleBlock\": 0,\n    \"petersburgBlock\": 0,\n    \"istanbulBlock\": 0,\n    \"berlinBlock\": 0,\n    \"londonBlock\": 0\n  },\n  \"alloc\": {},\n  \"coinbase\": \"0x0000000000000000000000000000000000000000\",\n  \"difficulty\": \"0x20000\",\n  \"extraData\": \"\",\n  \"gasLimit\": \"0x2fefd8\",\n  \"nonce\": \"0x0000000000000042\",\n  \"mixhash\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\n  \"parentHash\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\n  \"timestamp\": \"0x00\"\n}\n```\n\nThe above fields should be fine for most purposes, although we'd recommend changing\nthe `nonce` to some random value so you prevent unknown remote nodes from being able\nto connect to you. If you'd like to pre-fund some accounts for easier testing, create\nthe accounts and populate the `alloc` field with their addresses.\n\n```json\n\"alloc\": {\n  \"0x0000000000000000000000000000000000000001\": {\n    \"balance\": \"111111111\"\n  },\n  \"0x0000000000000000000000000000000000000002\": {\n    \"balance\": \"222222222\"\n  }\n}\n```\n\nWith the genesis state defined in the above JSON file, you'll need to initialize **every**\n`geth` node with it prior to starting it up to ensure all blockchain parameters are correctly\nset:\n\n```shell\n$ geth init path/to/genesis.json\n```\n\n#### Creating the rendezvous point\n\nWith all nodes that you want to run initialized to the desired genesis state, you'll need to\nstart a bootstrap node that others can use to find each other in your network and/or over\nthe internet. The clean way is to configure and run a dedicated bootnode:\n\n```shell\n$ bootnode --genkey=boot.key\n$ bootnode --nodekey=boot.key\n```\n\nWith the bootnode online, it will display an [`enode` URL](https://ethereum.org/en/developers/docs/networking-layer/network-addresses/#enode)\nthat other nodes can use to connect to it and exchange peer information. Make sure to\nreplace the displayed IP address information (most probably `[::]`) with your externally\naccessible IP to get the actual `enode` URL.\n\n*Note: You could also use a full-fledged `geth` node as a bootnode, but it's the less\nrecommended way.*\n\n#### Starting up your member nodes\n\nWith the bootnode operational and externally reachable (you can try\n`telnet <ip> <port>` to ensure it's indeed reachable), start every subsequent `geth`\nnode pointed to the bootnode for peer discovery via the `--bootnodes` flag. It will\nprobably also be desirable to keep the data directory of your private network separated, so\ndo also specify a custom `--datadir` flag.\n\n```shell\n$ geth --datadir=path/to/custom/data/folder --bootnodes=<bootnode-enode-url-from-above>\n```\n\n*Note: Since your network will be completely cut off from the main and test networks, you'll\nalso need to configure a miner to process transactions and create new blocks for you.*\n\n#### Running a private miner\n\nMining on the public Ethereum network is a complex task as it's only feasible using GPUs,\nrequiring an OpenCL or CUDA enabled `ethminer` instance. For information on such a\nsetup, please consult the [EtherMining subreddit](https://www.reddit.com/r/EtherMining/)\nand the [ethminer](https://github.com/ethereum-mining/ethminer) repository.\n\nIn a private network setting, however, a single CPU miner instance is more than enough for\npractical purposes as it can produce a stable stream of blocks at the correct intervals\nwithout needing heavy resources (consider running on a single thread, no need for multiple\nones either). To start a `geth` instance for mining, run it with all your usual flags, extended\nby:\n\n```shell\n$ geth <usual-flags> --mine --miner.threads=1 --miner.etherbase=0x0000000000000000000000000000000000000000\n```\n\nWhich will start mining blocks and transactions on a single CPU thread, crediting all\nproceedings to the account specified by `--miner.etherbase`. You can further tune the mining\nby changing the default gas limit blocks converge to (`--miner.targetgaslimit`) and the price\ntransactions are accepted at (`--miner.gasprice`).\n\n## Contribution\n\nThank you for considering helping out with the source code! We welcome contributions\nfrom anyone on the internet, and are grateful for even the smallest of fixes!\n\nIf you'd like to contribute to go-ethereum, please fork, fix, commit and send a pull request\nfor the maintainers to review and merge into the main code base. If you wish to submit\nmore complex changes though, please check up with the core devs first on [our Discord Server](https://discord.gg/invite/nthXNEv)\nto ensure those changes are in line with the general philosophy of the project and/or get\nsome early feedback which can make both your efforts much lighter as well as our review\nand merge procedures quick and simple.\n\nPlease make sure your contributions adhere to our coding guidelines:\n\n * Code must adhere to the official Go [formatting](https://golang.org/doc/effective_go.html#formatting)\n   guidelines (i.e. uses [gofmt](https://golang.org/cmd/gofmt/)).\n * Code must be documented adhering to the official Go [commentary](https://golang.org/doc/effective_go.html#commentary)\n   guidelines.\n * Pull requests need to be based on and opened against the `master` branch.\n * Commit messages should be prefixed with the package(s) they modify.\n   * E.g. \"eth, rpc: make trace configs optional\"\n\nPlease see the [Developers' Guide](https://geth.ethereum.org/docs/developers/devguide)\nfor more details on configuring your environment, managing project dependencies, and\ntesting procedures.\n\n## License\n\nThe go-ethereum library (i.e. all code outside of the `cmd` directory) is licensed under the\n[GNU Lesser General Public License v3.0](https://www.gnu.org/licenses/lgpl-3.0.en.html),\nalso included in our repository in the `COPYING.LESSER` file.\n\nThe go-ethereum binaries (i.e. all code inside of the `cmd` directory) are licensed under the\n[GNU General Public License v3.0](https://www.gnu.org/licenses/gpl-3.0.en.html), also\nincluded in our repository in the `COPYING` file.\n", "release_dates": []}, {"name": "go-ibft", "description": "A minimal and compact IBFT 2.0 implementation, written in Go", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "[![codecov](https://codecov.io/gh/0xPolygon/go-ibft/branch/main/graph/badge.svg?token=0vLkmaEq3h)](https://codecov.io/gh/0xPolygon/go-ibft)\n# go-ibft README\n\n## Overview\n\n`go-ibft` is a simple, straightforward, IBFT state machine implementation.\n\nIt doesn't contain fancy synchronization logic, or any kind of transaction execution layer.\nInstead, `go-ibft` is designed from the ground up to respect and adhere to the `IBFT 2.0` specification document.\n\nInside this package, you\u2019ll find that it solves the underlying liveness and persistence issues of the original IBFT specification, as well as that it contains a plethora of optimizations that make it faster and more lightweight. For a complete specification overview on the package, you can check out the official documentation.\n\nAs mentioned before, `go-ibft` implements basic IBFT 2.0 state machine logic, meaning it doesn\u2019t have any kind of transaction execution or block building mechanics. That responsibility is left to the `Backend` implementation.\n\n## Installation\n\nTo get up and running with the `go-ibft` package, you can pull it into your project using:\n\n`go get github.com/0xPolygon/go-ibft`\n\nCurrently, the minimum required go version is `go 1.17`.\n\n## Usage Examples\n\n```go\npackage main\n\nimport \"github.com/0xPolygon/go-ibft\"\n\n// IBFTBackend is the structure that implements all required\n// go-ibft Backend interfaces\ntype IBFTBackend struct {\n\t// ...\n}\n\n// IBFTLogger is the structure that implements all required\n// go-ibft Logger interface\ntype IBFTLogger struct {\n\t// ...\n}\n\n// IBFTTransport is the structure that implements all required\n// go-ibft Transport interface\ntype IBFTTransport struct {\n\t// ...\n}\n\n// ...\n\nfunc main() {\n\tbackend := NewIBFTBackned(...)\n\tlogger := NewIBFTLogger(...)\n\ttransport := NewIBFTTransport(...)\n\n\tibft := NewIBFT(logger, backend, transport)\n\n\tblockHeight := uint64(1)\n\tctx, cancelFn := context.WithCancel(context.Background())\n\n\tgo func () {\n\t\t// Run the consensus sequence for the block height.\n\t\t// When the method returns, that means that\n\t\t// consensus was reached\n\t\ti := RunSequence(ctx, blockHeight)\n\t}\n\n\t// ...\n\n\t// Stop the sequence by cancelling the context\n\tcancelFn()\n}\n```\n\n## License\n\nCopyright 2022 Polygon Technology\nLicensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n\n### http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u201c AS IS\u201d BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n", "release_dates": []}, {"name": "go-profile-chart-generator", "description": null, "language": "Python", "license": null, "readme": "# go-profile-chart-generator\n\nA tool to generate time series chart from profiles in Go.\n\nSample result:\n![sample](https://user-images.githubusercontent.com/19720977/128287228-99bc3fac-7b21-40a1-81f5-f8a32abf1efd.png)\n\n## Requirements\n+ Python\n+ Golang\n\n## How to start\n\nPlease get profiles before use this tool (Please check: https://blog.golang.org/pprof)\n\n```shell\n$ pip install -r requirements.txt\n$ python main.py [DIRECTORY_FOR_GO_PROFILES]\n```\n\n## Command Option Arguments\n\n+ --out: path of output file\n+ --binary: binary file for analysis target\n+ --type: type of data, available only if the type of profiles is heap [inuse_space/alloc_space/inuse_objects/alloc_objects]\n+ --kind: kind of aggregation\n  - flat: value only in the function\n  - cum: value in the function and callee functions\n+ --num: number of functions to display\n+ --title: chart title\n+ --ymin: Max limit of y-axis\n+ --ymax: Min limit of y-axis\n", "release_dates": []}, {"name": "kryptology", "description": null, "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# Important\nThis library has been archived and is no longer supported. As such it should not be used, and it is not used by Coinbase.\n\n# Kryptology\nCoinbase's advanced cryptography library\n\n## Quickstart\nUse the latest version of this library:\n```$xslt\ngo get github.com/coinbase/kryptology\n```\n\nPin a specific release of this library:\n```$xslt\ngo get github.com/coinbase/kryptology@v1.6.0\n```\n\n## Documentation\n\nPublic documentations can be found at https://pkg.go.dev/github.com/coinbase/kryptology\n\nTo access the documentation of the local version, run `godoc -http=:6060` and open\nthe following url in your browser.\n\nhttp://localhost:6060/pkg/github.com/coinbase/kryptology/\n\n## Developer Setup\n**Prerequisites**: `golang 1.17`, `make`\n\n```$xslt\ngit clone git@github.com/coinbase/kryptology.git && make \n``` \n\n## Components\n\nThe following is the list of primitives and protocols that are implemented in this repository.\n\n### Curves\n\nThe curve abstraction code can be found at [pkg/core/curves/curve.go](pkg/core/curves/curve.go)\n\nThe curves that implement this abstraction are as follows.\n\n- [BLS12377](pkg/core/curves/bls12377_curve.go)\n- [BLS12381](pkg/core/curves/bls12381_curve.go)\n- [Ed25519](pkg/core/curves/ed25519_curve.go)\n- [Secp256k1](pkg/core/curves/k256_curve.go)\n- [P256](pkg/core/curves/p256_curve.go)\n- [Pallas](pkg/core/curves/pallas_curve.go)\n\n### Protocols\n\nThe generic protocol interface [pkg/core/protocol/protocol.go](pkg/core/protocol/protocol.go).\nThis abstraction is currently only used in DKLs18 implementation.\n\n- [Cryptographic Accumulators](pkg/accumulator)\n- [Bulletproof](pkg/bulletproof)\n- Oblivious Transfer\n  - [Verifiable Simplest OT](pkg/ot/base/simplest)\n  - [KOS OT Extension](pkg/ot/extension/kos)\n- Threshold ECDSA Signature\n  - [DKLs18 - DKG and Signing](pkg/tecdsa/dkls/v1)\n  - GG20: The authors of GG20 have stated that the protocol is obsolete and should not be used. See [https://eprint.iacr.org/2020/540.pdf](https://eprint.iacr.org/2020/540.pdf).\n    - [GG20 - DKG](pkg/dkg/gennaro)\n    - [GG20 - Signing](pkg/tecdsa/gg20)\n- Threshold Schnorr Signature\n  - [FROST threshold signature - DKG](pkg/dkg/frost)\n  - [FROST threshold signature - Signing](pkg/ted25519/frost)\n- [Paillier encryption system](pkg/paillier)\n- Secret Sharing Schemes\n  - [Shamir's secret sharing scheme](pkg/sharing/shamir.go)\n  - [Pedersen](pkg/sharing/pedersen.go)\n  - [Feldman](pkg/sharing/feldman.go)\n- [Verifiable encryption](pkg/verenc)\n- [ZKP Schnorr](pkg/zkp/schnorr)\n\n\n## Contributing\n- [Versioning](https://blog.golang.org/publishing-go-modules): `vMajor.Minor.Patch`\n    - Major revision indicates breaking API change or significant new features\n    - Minor revision indicates no API breaking changes and may include significant new features or documentation\n    - Patch indicates no API breaking changes and may include only fixes\n \n \n## [References](docs/)\n- [[GG20] _One Round Threshold ECDSA with Identifiable Abort._](https://eprint.iacr.org/2020/540.pdf)\n- [[specV5] _One Round Threshold ECDSA for Coinbase._](docs/Coinbase_Pseudocode_v5.pdf)\n- [[EL20] _Eliding RSA Group Membership Checks._](docs/rsa-membership.pdf) [src](https://www.overleaf.com/project/5f9c3b0624a9a600012037a3)\n- [[P99] _Public-Key Cryptosystems Based on Composite Degree Residuosity Classes._](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.4035&rep=rep1&type=pdf)\n", "release_dates": []}, {"name": "maticjs-fxportal", "description": "Library for interacting with fx-portal bridge.", "language": "TypeScript", "license": null, "readme": "[![TEST](https://github.com/fx-portal/maticjs-fxportal/actions/workflows/test.yml/badge.svg)](https://github.com/fx-portal/maticjs-fxportal/actions/workflows/test.yml)\n[![npm version](https://badge.fury.io/js/@fxportal%2Fmaticjs-fxportal.svg)](https://badge.fury.io/js/@fxportal%2Fmaticjs-fxportal)\n# maticjs-fxportal\n\nFxPortal bridge plugin for maticjs. It provides FxPortalClient to interact with fxportal bridge.\n\n# Installation\n\n```\nnpm i @fxportal/maticjs-fxportal\n```\n\n## Install ethers library\n\nCurrently `matic.js` support two ethers library - \n\n### 1. web3.js\n\n```\nnpm i @maticnetwork/maticjs-web3\n```\n\n### 2. Ethers\n\n```\nnpm i @maticnetwork/maticjs-ethers\n```\n\n# DOCS\n\n## Initiate client\n\n```\nconst { use } = require(\"@maticnetwork/maticjs\");\nconst { Web3ClientPlugin } = require(\"@maticnetwork/maticjs-web3\");\nconst { FxPortalClient } = require(\"@fxportal/maticjs-fxportal\");\n\nconst HDWalletProvider = require(\"@truffle/hdwallet-provider\");\n\n// add Web3Plugin\n\nuse(Web3ClientPlugin);\n\nconst fxPortalClient = new FxPortalClient();\n\nawait fxPortalClient.init({\n    network: 'testnet',\n    version: 'mumbai',\n    parent: {\n        provider: new HDWalletProvider(privateKey, rootRPC),\n        defaultConfig: {\n            from\n        }\n    },\n    child: {\n        provider: new HDWalletProvider(privateKey, childRPC),\n        defaultConfig: {\n            from\n        }\n    }\n});\n\n```\n\n## ERC20\n\nMethod `erc20` allows you to interact with erc20 token.\n\n```\nconst erc20 = fxPortalClient.erc20(<tokenAddress>, <isRoot>);\n```\n\n### getBalance\n\nGet balance of a user by supplying user address\n\n```\nconst balance = await erc20.getBalance(<user address>);\n```\n### getName\n\nGet name of token\n\n```\nconst name = await erc20.getName();\n```\n### getDecimals\n\nGet decimals of token\n\n```\nconst decimals = await erc20.getDecimals();\n```\n### getSymbol\n\nGet symbol of token\n\n```\nconst decimals = await erc20.getSymbol();\n```\n\n### approve\n\nApprove required amount for depositing to polygon chain\n\n```\nconst approveResult = await erc20.approve(<amount>);\nconst txHash = await approveResult.getTransactionHash();\nconst receipt = await approveResult.getReceipt();\n```\n\n### approveMax\n\nApprove max amount for depositing to polygon chain\n\n```\nconst approveResult = await erc20.approveMax();\nconst txHash = await approveResult.getTransactionHash();\nconst receipt = await approveResult.getReceipt();\n```\n\n### getAllowance\n\nGet approve amount of a user by supplying user address\n\n```\nconst balance = await erc20.getAllowance(<user address>);\n```\n\n### deposit\n\nDeposit required amount from ethereum to polygon\n\n```\nconst result = await erc20.deposit(<amount>, <user address>);\nconst txHash = await result.getTransactionHash();\nconst receipt = await result.getReceipt();\n```\n\n### withdrawStart\n\nInitiate withdraw process by burning the required amount. \n\n```\nconst result = await erc20.withdrawStart(<amount>);\nconst txHash = await result.getTransactionHash();\nconst receipt = await result.getReceipt();\n```\n### withdrawToStart\n\nInitiate withdrawTo process by burning the required amount. \n\n```\nconst result = await erc20.withdrawToStart(<amount>, <to address>);\nconst txHash = await result.getTransactionHash();\nconst receipt = await result.getReceipt();\n```\n### withdrawExit\n\nExit withdraw process by providng txHash received in `withdrawStart` process.\n\n**Note:-** `withdrawExit` can be called after checkpoint has been submitted for `withdrawStart`.\n\n```\nconst result = await erc20.withdrawExit(<burn tx hash>);\nconst txHash = await result.getTransactionHash();\nconst receipt = await result.getReceipt();\n```\n### withdrawExitFaster\n\nFaster exit withdraw process by providng txHash received in `withdrawStart` process.\n\n> It is faster because it uses api to create the proof.\n\n```\nconst result = await erc20.withdrawExitFaster(<burn tx hash>);\nconst txHash = await result.getTransactionHash();\nconst receipt = await result.getReceipt();\n```\n\n## isCheckPointed\n\nCheck if transaction has been checkpointed or not.\n\n```\nawait fxPortalClient.isCheckPointed(<tx hash>);\n```\n\n\n## isWithdrawExited\n\nCheck if withdraw process has been completed by supplying burn transaction hash.\n\n```\nconst balance = await erc20.isExited(<burn tx hash>);\n```\n\n\n## isDeposited\n\nCheck if deposit is completed.\n\n```\nconst balance = await erc20.isDeposited(<tx hash>);\n```\n", "release_dates": []}, {"name": "pbft-consensus", "description": "Practical Byzantium Fault Tolerant (PBFT) algorithm", "language": "Go", "license": null, "readme": "\n# Practical-BFT consensus\n\n## Tracing\n\nYou can use OpenTracing to trace the execution of the protocol. Each trace span represents a height/sequence.\n\n## E2E\n\nThis repo includes integration tests under [/e2e](./e2e)\n", "release_dates": []}, {"name": "pol-token", "description": null, "language": "Solidity", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Polygon Ecosystem Token (POL)\n\n![Test Status](https://github.com/github/docs/actions/workflows/test.yml/badge.svg)\n\nThe Polygon Ecosystem Token is intended as an upgrade to the [MATIC token](https://etherscan.io/address/0x7d1afa7b718fb893db30a3abc0cfc608aacfebb0). It consists of a [token contract](https://github.com/0xPolygon/pol-token/tree/main/src/PolygonEcosystemToken.sol), [migration contract](https://github.com/0xPolygon/pol-token/tree/main/src/PolygonMigration.sol), and an [emission manager contract](https://github.com/0xPolygon/pol-token/tree/main/src/DefaultEmissionManager.sol). Together, this set of contracts is proposed in [PIP-17](https://github.com/maticnetwork/Polygon-Improvement-Proposals/blob/main/PIPs/PIP-17.md) to Polygon Governance as a step forward in functionality for the polygon ecosystem.\n\n## POL Token Contract\n\nPOL is broadly based on the MIT-licensed OpenZeppelin ERC-20 implementations which provide support for the default ERC-20 standard, along with some non-standard functions for allowance modifications. The implementation also provides support for [EIP-2612: Signature-Based Permit Approvals](https://eips.ethereum.org/EIPS/eip-2612)-style is supported).\n\nThe POL token contract is not upgradable.\n\n[Source Code](https://github.com/0xPolygon/pol-token/tree/main/src/PolygonEcosystemToken.sol)\n\n## Migration Contract\n\nThe migration contract allows 1-to-1 migrations between MATIC and POL using the `migrate` and `unmigrate` functions respectively. This migration contract is ownable, and the owner has the ability to disable the `ummigrate` functionality. For both actions, [EIP-2612 Permit](https://eips.ethereum.org/EIPS/eip-2612)-style is supported.\n\n[Source Code](https://github.com/0xPolygon/pol-token/tree/main/src/PolygonMigration.sol)\n\n## Emission Manager Contract\n\nThe role of the Emission Manager is to have the exclusive ability to mint new POL tokens. It has the ability to calculate token emissions based upon a yearly rate, and then dispurse them linearly to a configured target `StakeManager`. For safety, there is a cap on the number of tokens mintable per second as defined by [`mintPerSecondCap`](https://github.com/0xPolygon/pol-token/blob/main/src/PolygonEcosystemToken.sol#L16) on the token implementation.\n\nA default implementation is included and this contract will be proxy upgradable by Polygon Governance.\n\n[Source Code](https://github.com/0xPolygon/pol-token/tree/main/src/DefaultEmissionManager.sol)\n\n## Development\n\n### Setup\n\n- [Install foundry](https://book.getfoundry.sh/getting-started/installation)\n- Install Dependencies: `forge install`\n- Build: `forge build`\n- Test: `forge test`\n\n### Deployment\n\nForge scripts are used to deploy or upgrade contracts and an additional extract.js script can be used to generate a JSON and Markdown file with coalesced deployment information. (see [deployments](./deployments/))\n\n1. Ensure .env file is set, `cp .env.example`\n2. `source .env`\n\n3. Deploy using foundry\n\n- (mainnet): `forge script script/Deploy.s.sol --broadcast --verify --rpc-url $RPC_URL --etherscan-api-key $ETHERSCAN_API_KEY`\n- (testnet, goerli for example): `forge script script/Deploy.s.sol --broadcast --verify --rpc-url $RPC_URL --verifier-url https://api-goerli.etherscan.io/api --chain-id 5`\n\n4. Run `node script/util/extract.js <chainId> [version = 1.0.0] [scriptName = Deploy.s.sol]` to extract deployment information from forge broadcast output (broadcast/latest-run.json).\n\n## Reference Deployments\n\n- Ethereum Mainnet [0x455e53CBB86018Ac2B8092FdCd39d8444aFFC3F6](https://etherscan.io/address/0x455e53CBB86018Ac2B8092FdCd39d8444aFFC3F6)\n- Goerli [0x4f34BF3352A701AEc924CE34d6CfC373eABb186c](https://goerli.etherscan.io/address/0x4f34BF3352A701AEc924CE34d6CfC373eABb186c)\n\n---\n\nCopyright (C) 2023 PT Services DMCC\n", "release_dates": ["2023-10-25T11:01:53Z", "2023-10-04T11:21:19Z"]}, {"name": "polygon-docs", "description": "Polygon Technical Docs", "language": "HTML", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# Polygon Knowledge Layer\n\nWelcome to the Polygon Knowledge Layer.\n\nThese docs use [the Material theme for MkDocs](https://squidfunk.github.io/mkdocs-material/). Our goal is to establish a high-quality, curated, and comprehensive \"source of truth\" for Polygon's technology. \n\nThis includes sections on:\n\n- Polygon CDK\n- Polygon zkEVM\n- Polygon PoS\n- Polygon Miden\n- Developer tools \n\n## Run locally\n\n### Prerequisites\n\n1. [Python 3.12](https://www.python.org/downloads/).\n2. [`virtualenv`](https://pypi.org/project/virtualenv/): Install using `pip3 install virtualenv`.\n\n### Setup\n\n1. Clone the repository.\n2. `cd` to the root.\n3. Run the `run.sh` script. You may need to make the script executable: `chmod +x run.sh`\n\n```sh\n./run.sh\n```\n\nThe site comes up at http://127.0.0.1:8000/ \n\n### Docker \n\nIf you prefer Docker, you can build and run the site using the following commands:\n\n```sh\ndocker build -t polygon-docs .\ndocker compose up\n```\n\n## Contributing\n\n### Getting started\n\n1. **Fork and branch**: Fork the `main` branch into your own GitHub account. Create a feature branch for your changes.\n2. **Make changes**: Implement your changes or additions in your feature branch.\n3. **Contribution quality**: Ensure that your contributions are:\n   - **Atomic**: Small, self-contained, logical updates are preferred.\n   - **Well documented**: Use clear commit messages. Explain your changes in the pull request description.\n   - **Tested**: Verify your changes do not break existing functionality.\n   - **Styled correctly**: Follow the [Microsoft Style Guide](https://learn.microsoft.com/en-us/style-guide/welcome/).\n\n### Creating a pull request\n\n1. **Pull request**: Once your changes are complete, create a pull request against the main branch of Polygon Knowledge Layer.\n2. **Review process**: Your pull request will be reviewed by the maintainers. They may request changes or clarifications.\n3. **Responsibility**: Contributors are expected to maintain their contributions over time and update them as necessary to ensure continued accuracy and relevance.\n\n### Best practices\n\n- **Stay informed**: Keep up-to-date with the latest developments in Polygon technologies.\n- **Engage with the community**: Participate in discussions and provide feedback on other contributions.\n- **Stay consistent**: Ensure your contributions are coherent with the rest of the documentation and do not overlap or contradict existing content.\n\n## Contact and support\n\n- For docs issues (technical or language) open an issue here.\n- For technical issues with the software, either raise an issue here and we will follow up, or check https://support.polygon.technology/support/home. \n\n## The team\n\n- Anthony Matlala (@EmpieichO)\n- Katharine Murphy (@kmurphypolygon)\n- Hans (@hsutaiyu) \n", "release_dates": []}, {"name": "polygon-edge", "description": "A Framework for Building Ethereum-compatible Blockchain Networks", "language": "Go", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "\n![Banner](.github/banner.jpg)\n\n## Update: Edge v1.0.0 is here!  \n\nDevelopers at Polygon Labs have been hard at work gathering and incorporating community feedback into the Edge client and a new version of Edge, v1.0.0, is here with several new features! Check out the Release Notes included with the release to find out more! \n\n## Polygon Edge\n\nPolygon Edge is a modular and extensible framework for building Ethereum-compatible blockchain networks.\n\nTo find out more about Polygon, visit the [official website](https://polygon.technology/).\n\nWARNING: This is a work in progress so architectural changes may happen in the future. The code is still being audited, so please contact the Polygon team if you would like to use it in production.\n\n## Documentation \ud83d\udcdd\n\nIf you'd like to learn more about the Polygon Edge, how it works and how you can use it for your project,\nplease check out the **[Polygon Supernets Documentation](https://wiki.polygon.technology/docs/edge/)**.\n\n## Disclaimer\n\nAs this project evolves, the Polygon Labs developer team will focus on the latest version of the Edge client and does not plan to support Edge 0.6 or lower. It is highly recommended that you upgrade to the newest version with the most up-to-date features and fixes. Users that want to stay on 0.6 or below, can continue to do so. The repo will continue to exist and users can fork it and do with it as they please, subject to applicable open-source license terms. \n\n---\n\nCopyright 2022 Polygon Technology\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n", "release_dates": ["2024-01-24T16:10:13Z", "2023-10-12T09:20:53Z", "2023-10-04T09:23:26Z", "2023-10-03T21:02:21Z", "2023-07-24T20:33:47Z", "2023-07-04T18:37:37Z", "2023-06-27T20:12:27Z", "2023-06-05T13:04:01Z", "2023-05-05T09:30:46Z", "2023-03-31T17:08:54Z", "2023-03-20T18:05:13Z", "2023-03-16T14:21:39Z", "2023-03-06T10:07:02Z", "2023-02-23T16:45:29Z", "2023-02-23T11:35:49Z", "2023-01-26T12:00:52Z", "2023-01-12T15:07:54Z", "2023-01-10T08:47:59Z", "2023-01-12T12:00:06Z", "2022-12-14T12:16:10Z", "2022-12-01T11:31:32Z", "2022-10-27T10:50:25Z", "2022-09-27T10:38:47Z", "2022-09-22T15:04:13Z", "2022-08-03T10:52:47Z", "2022-06-03T08:24:27Z", "2022-05-17T10:36:52Z", "2022-04-21T15:03:46Z", "2022-03-16T15:53:32Z", "2022-03-09T18:26:13Z"]}, {"name": "polygon-edge-assm", "description": "Helper program for bootstrapping Polygon Edge deployments utilizing AWS Session Manager", "language": "Go", "license": null, "readme": "# Polygon Edge secrets manager initializer\n\nOn running automated Polygon Edge deployments this API helps with creation of `genesis.json` file that the Polygon Edge server needs to run a chain.\nWhen the private keys are saved in some secrets manager solution ( right now only AWS SSM is supported ) this API fetches these keys and converts them to network_id and validator address.\n\n### Prequestites\n* Dedicated node that will clone this repo, compile binary and run it.\n* This node needs to have access to both ASSM and S3 that will hold `genesis.json` file ( instance IAM polices ).  \n* All polygon edge nodes need to be able to access this node at TCP 9001 by default ( security groups ).\n\n## How to use\nThe genesis creation process involves several stages, consisting of hitting the API and delivering the required data to it.  \n\n### Total number of nodes API\nIf there are 4 validator nodes in total: `/total-nodes?total=4`  \nNow the API knows that there are 4 nodes that we would like to initialize as validator nodes.\n\n### Initialization done\nEach validator node needs to send the following api when it finishes the `secrets init` stage: ` /node-done?name=node1&ip=10.150.1.4`   \nNode sends its name, which coresponds to the name in secrets manager, and its IP address.  \nOnce the program receives enough calls to this API ( for 4 nodes, the program expects 4 calls to this API ), it moves to the next stage.   \n\n### Fetch keys and generate genesis.json file\nOnce all validator nodes reported that they have successfully completed the `secrets init` stage, this program fetches validator secrets from the secrets store, generates `genesis.json` file and puts it in the S3 bucket.   \nThe API call that triggers this action is: `/init`   \n\nEach node can be configured to send all 3 API calls.   \nOnce the last node hits `/init` api, the `genesis.json` file generation will start.\n\n### Configuration options\nFlags that can be set for this program are:   \n* `aws-region` - sets the AWS region for the SSM. Default: `us-west-2`\n* `s3-name` - sets S3 bucket name in which to place the `genesis.json` file. Default: `polygon-edge-shared`\n* `log-file` - sets the log file output. Default: `/var/log/edge-assm.log`\n* `genesis-log-file` - sets log file output for genesis module. Default: `/var/log/edge-assm-genesis.log`\n* `chain-name` - sets chain name. Default: pulled from `polygon-edge genesis` command\n* `pos` - sets PoS consensus. Default: false\n* `epoch-size` - sets epoch size. Default: pulled from `polygon-edge genesis` command\n* `premine` - premine accounts. For multiple accounts, separate them with `,`. Format: `<account>:<ammount>`\n* `chain-id` - sets chain id. Default: pulled from `polygon-edge genesis` command\n* `block-gas-limit` - sets block gas limit. Default: pulled from `polygon-edge genesis` command\n* `max-validator-count` - sets maximum validator count, only for PoS consensus. Default: pulled from `polygon-edge genesis` command\n* `min-validator-count` - sets minimum validator count, only for PoS consensus. Default: pulled from `polygon-edge genesis` command\n\n\n", "release_dates": []}, {"name": "polygon-edge-docs", "description": "The Official documentation for Polygon Edge", "language": "JavaScript", "license": null, "readme": "## Polygon Edge Docs\n\nThe Polygon Edge docs have been migrated to https://github.com/maticnetwork/matic-docs\n\nThis repo is archived.\n", "release_dates": []}, {"name": "polygon-sdk-old", "description": "Deprecated Polygon SDK Repo", "language": "Go", "license": {"key": "lgpl-3.0", "name": "GNU Lesser General Public License v3.0", "spdx_id": "LGPL-3.0", "url": "https://api.github.com/licenses/lgpl-3.0", "node_id": "MDc6TGljZW5zZTEy"}, "readme": "\n# Polygon SDK\n\n\nPolygon SDK is a modular and extensible framework for building Ethereum-compatible blockchain networks. \n\nThis repository is the first implementation of Polygon SDK, written in Golang. Other implementations, written in other programming languages might be introduced in the future. If you would like to contribute to this or any future implementation, please reach out to [Polygon team](mailto:contact@polygon.technology).\n\nTo find out more about Polygon, visit the [official website](https://polygon.technology/).\n\nWARNING: This is a work in progress so architectural changes may happen in the future. The code has not been audited yet, so please contact [Polygon team](mailto:contact@polygon.technology) if you would like to use it in production.\n\n## Commands\n\n### Agent\n\nStarts the Ethereum client for the mainnet:\n\n```\n$ go run main.go agent [--config ./config.json]\n```\n\nThe configuration file can be specified either in HCL or JSON format:\n\n```\n{\n    \"data-dir\": \"/tmp/data-dir\"\n}\n```\n\nSome attributes can be also set from the command line:\n\n```\n$ go run main.go agent --config ./config.json --data-dir /tmp/local --port 30304 --log-level TRACE\n```\n\nThe values from the CLI have preference over the ones in the configuration file.\n\n### Dev\n\nStart a development chain with instant sealing:\n\n```\n$ go run main.go dev\n```\n\n### Genesis\n\nGenerates a test genesis file:\n\n```\n$ go run main.go genesis\n```\n", "release_dates": []}, {"name": "polygon2.0-economic-model", "description": "polygon2.0-economic-model", "language": "Jupyter Notebook", "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "readme": "# Polygon Economic Model\n\n[![Python package](https://github.com/CADLabs/ethereum-model/actions/workflows/python.yml/badge.svg)](https://github.com/CADLabs/ethereum-model/actions/workflows/python.yml)\n\nA simulation model of Polygon 2.0 to study validator incentives and ecosystem security. It forked from [ethereum-economic-model](https://github.com/CADLabs/ethereum-economic-model) and has been constructed using the open-source Python library [radCAD](https://github.com/CADLabs/radCAD), which is an enhancement of [cadCAD](https://cadcad.org).\n\n\n\n## Environment Setup\n\n1. Clone or download the Git repository: `git clone https://github.com/0xPolygon/polygon2.0-economic-model.git` or using GitHub Desktop\n2. Set up your development environment using one of the following three options:\n\n### Option 1: Anaconda Development Environment\n\nThis option guides you through setting up a cross-platform, beginner-friendly (yet more than capable enough for the advanced user) development environment using Anaconda to install Python 3 and Jupyter. There is also a video that accompanies this option and walks through all the steps: [Model Quick-Start Guide](https://www.cadcad.education/course/masterclass-ethereum)\n\n1. Download [Anaconda](https://www.anaconda.com/products/individual)\n2. Use Anaconda to install Python 3\n3. Set up a virtual environment from within Anaconda\n4. Install Jupyter Notebook within the virtual environment\n5. Launch Jupyter Notebook and open the [environment_setup.ipynb](environment_setup.ipynb) notebook in the root of the project repo\n6. Follow and execute all notebook cells to install and check your Python dependencies\n\n### Option 2: Custom Development Environment\n\nThis option guides you through how to set up a custom development environment using Python 3 and Jupyter.\n\nPlease note the following prerequisites before getting started:\n* Python: tested with versions 3.7, 3.8, 3.9\n* NodeJS might be needed if using Plotly with Jupyter Lab (Plotly works out the box when using the Anaconda/Conda package manager with Jupyter Lab or Jupyter Notebook)\n\nFirst, set up a Python 3 [virtualenv](https://docs.python.org/3/library/venv.html) development environment (or use the equivalent Anaconda step):\n```bash\n# Create a virtual environment using Python 3 venv module\npython3 -m venv venv\n# Activate virtual environment\nsource venv/bin/activate\n```\n\nMake sure to activate the virtual environment before each of the following steps.\n\nSecondly, install the Python 3 dependencies using [Pip](https://packaging.python.org/tutorials/installing-packages/), from the [requirements.txt](requirements.txt) file within your new virtual environment:\n```bash\n# Install Python 3 dependencies inside virtual environment\npip install -r requirements.txt\n```\n\nTo create a new Jupyter Kernel specifically for this environment, execute the following command:\n```bash\npython3 -m ipykernel install --user --name python-pol-model --display-name \"Python (Polygon Economic Model)\"\n```\n\nYou'll then be able to select the kernel with display name `Python (Polygon Economic Model)` to use for your notebook from within Jupyter.\n\nTo start Jupyter Notebook or Lab (see notes about issues with [using Plotly with Jupyter Lab](#Known-Issues)):\n```bash\njupyter notebook\n# Or:\njupyter lab\n```\n\nFor more advanced Unix/macOS users, a [Makefile](Makefile) is also included for convenience that simply executes all the setup steps. For example, to setup your environment and start Jupyter Lab:\n```bash\n# Setup environment\nmake setup\n# Start Jupyter Lab\nmake start-lab\n```\n\n### Option 3: Docker Development Environment\n\nAlternatively, you can set up your development environment using the pre-built Docker image with all the dependencies you need: [CADLabs Jupyter Lab Environment](https://github.com/CADLabs/jupyter-lab-environment)\n\n### Known Issues\n\n#### Plotly doesn't display in Jupyter Lab\n\nTo install and use Plotly with Jupyter Lab, you might need NodeJS installed to build Node dependencies, unless you're using the Anaconda/Conda package manager to manage your environment. Alternatively, use Jupyter Notebook which works out the box with Plotly.\n\nSee https://plotly.com/python/getting-started/\n\nYou might need to install the following \"lab extension\": \n```bash\njupyter labextension install jupyterlab-plotly@4.14.3\n```\n\n#### Windows Issues\n\nIf you receive the following error and you use Anaconda, try: `conda install -c anaconda pywin32`\n> DLL load failed while importing win32api: The specified procedure could not be found.\n\n## Simulation Experiments\n\nThe [experiments/notebooks/Polygon_analysis](experiments/notebooks/Polygon_analysis) directory contains the most recent released cryptoeconomics simulation results. Specifically, [Polygon2.0 Economics v0.1](experiments/notebooks/Polygon_analysis/Polygon2.0_economics_v0.1.ipynb)presents a comprehensive analysis of the primary validator economics in the current model iteration. The simulation specs in this notebook is aligned with the POL whitepaper, ensuring that the simulation specifications are consistent with the official documentation.  The notebook shows\n* $POL token price trajectory\n* Three chains adoption rate scenarios\n* Validator yields over time per scenario\n* Validator profit break down analysis in both yields and dollars.\n\n\n\n\n\n\n\n\n\n\n", "release_dates": []}, {"name": "polygon2.0-economic-model-fork", "description": "A modular dynamical-systems model of  Polygon's validator economics", "language": null, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "readme": "# CADLabs Ethereum Economic Model\n\n[![Python package](https://github.com/CADLabs/ethereum-model/actions/workflows/python.yml/badge.svg)](https://github.com/CADLabs/ethereum-model/actions/workflows/python.yml)\n\nA modular dynamical-systems model of Ethereum's validator economics, based on the open-source Python library [radCAD](https://github.com/CADLabs/radCAD), an extension to [cadCAD](https://cadcad.org).\n\n* Latest model release version: [Subgraph / v1.1.7](https://github.com/CADLabs/ethereum-economic-model/releases/tag/v1.1.7)\n* Implements the official Ethereum [Altair](https://github.com/ethereum/eth2.0-specs#altair) spec updates in the [Blue Loop / v1.1.0-alpha.7](https://github.com/ethereum/eth2.0-specs/releases/tag/v1.1.0-alpha.7) release\n\n## Table of Contents\n\n* [Introduction](#Introduction)\n  * [Model Features](#Model-Features)\n  * [Directory Structure](#Directory-Structure)\n  * [Model Architecture](#Model-Architecture)\n  * [Model Assumptions](#Model-Assumptions)\n  * [Mathematical Model Specification](#Mathematical-Model-Specification)\n  * [Differential Model Specification](#Differential-Model-Specification)\n* [Environment Setup](#Environment-Setup)\n* [Simulation Experiments](#Simulation-Experiments)\n* [Model Extension Roadmap](#Model-Extension-Roadmap)\n* [Tests](#Tests)\n* [Change Log](#Change-Log)\n* [Acknowledgements](#Acknowledgements)\n* [Contributors](#Contributors-)\n* [License](#License)\n\n---\n\n## Introduction\n\nThis open-source model was developed in collaboration with the Ethereum Robust Incentives Group and funded by an Ethereum ESP (Ecosystem Support Program) grant. While originally scoped with purely modelling-educational intent as part of the cadCAD Edu online course \"[cadCAD Masterclass: Ethereum Validator Economics](https://www.cadcad.education/course/masterclass-ethereum)\", it has evolved to become a highly versatile, customizable and extensible research model that includes a list of [model extension ideas](#Model-Extension-Roadmap). The model is focused on epoch- and population-level Ethereum validator economics across different deployment types and \u2013 at least in its initial setup \u2013 abstracts from slot- and agent-level dynamics. Please see [Model Assumptions](ASSUMPTIONS.md) for further context.\n\n### Model Features\n\n* Configurable to reflect protocol behaviour at different points in time of the development roadmap (referred to as \"upgrade stages\"):\n  * Post Beacon Chain launch, pre EIP-1559, pre PoS (validators receive PoS incentives, EIP-1559 disabled, and PoW still in operation)\n  * Post Beacon Chain launch, post EIP-1559, pre PoS (validators receive PoS incentives, EIP-1559 enabled with miners receiving priority fees, and PoW still in operation)\n  * Post Beacon Chain launch, post EIP-1559, post PoS (validators receive PoS incentives, EIP-1559 enabled with validators receiving priority fees, and PoW deprecated)\n* Flexible calculation granularity: By default, State Variables, System Metrics, and System Parameters are calculated at epoch level and aggregated daily (~= 225 epochs). Users can easily change epoch aggregation using the delta-time (`dt`) parameter. The model can be extended for slot-level granularity and analysis if that is desired (see [Model Extension Roadmap](#Model-Extension-Roadmap)).\n* Supports [state-space analysis](https://en.wikipedia.org/wiki/State-space_representation) (i.e. simulation of system state over time) and [phase-space analysis](https://en.wikipedia.org/wiki/Phase_space) (i.e. generation of all unique system states in a given experimental setup).\n* Customizable processes to set important variables such as ETH price, ETH staked, and EIP-1559 transaction pricing.\n* Modular model structure for convenient extension and modification. This allows different user groups to refactor the model for different purposes, rapidly test new incentive mechanisms, or update the model as Ethereum implements new protocol improvements.\n* References to official [Eth2 specs](https://github.com/ethereum/eth2.0-specs) in Policy and State Update Function logic. This enables seamless onboarding of protocol developers and allows the more advanced cadCAD user to dig into the underlying protocol design that inspired the logic.\n\n### Directory Structure\n\n* [data/](data/): Datasets and API data sources (such as Etherscan.io and Beaconcha.in) used in the model\n* [docs/](docs/): Misc. documentation such as auto-generated docs from Python docstrings and Markdown docs\n* [experiments/](experiments/): Analysis notebooks and experiment workflow (such as configuration and execution)\n* [logs/](logs/): Experiment runtime log files\n* [model/](model/): Model software architecture (structural and configuration modules)\n* [tests/](tests/): Unit and integration tests for model and notebooks\n\n### Model Architecture\n\nThe [model/](model/) directory contains the model's software architecture in the form of two categories of modules: structural modules and configuration modules.\n\n#### Structural Modules\n\nThe model is composed of several structural modules in the [model/parts/](model/parts/) directory:\n\n| Module | Description |\n| --- | --- |\n| [ethereum_system.py](model/parts/ethereum_system.py) | General Ethereum mechanisms, such as managing the system upgrade process, the EIP-1559 transaction pricing mechanism, and updating the ETH price and ETH supply |\n| [pos_incentives.py](model/parts/pos_incentives.py) | Calculation of PoS incentives such as attestation and block proposal rewards and penalties |\n| [system_metrics.py](model/parts/system_metrics.py) | Calculation of metrics such as validator operational costs and yields |\n| [validators.py](model/parts/validators.py) | Validator processes such as validator activation, staking, and uptime |\n| [utils/ethereum_spec.py](model/parts/utils/ethereum_spec.py) | Relevant extracts from the official Eth2 spec |\n\n#### Configuration Modules\n\nThe model is configured using several configuration modules in the [model/](model/) directory:\n\n| Module | Description |\n| --- | --- |\n| [constants.py](model/constants.py) | Constants used in the model, e.g. number of epochs in a year, Gwei in 1 Ether |\n| [state_update_blocks.py](model/state_update_blocks.py) | cadCAD model State Update Block structure, composed of Policy and State Update Functions |\n| [state_variables.py](model/state_variables.py) | Model State Variable definition, configuration, and defaults |\n| [stochastic_processes.py](model/stochastic_processes.py) | Helper functions to generate stochastic environmental processes |\n| [system_parameters.py](model/system_parameters.py) | Model System Parameter definition, configuration, and defaults |\n| [types.py](model/types.py) | Various Python types used in the model, such as the `Stage` Enum and calculation units |\n| [utils.py](model/utils.py) | Misc. utility and helper functions |\n\n### Model Assumptions\n\nThe model implements the official Ethereum Specification wherever possible, but rests on a few default network-level and validator-level assumptions detailed in the [ASSUMPTIONS.md](ASSUMPTIONS.md) document.\n\n### Mathematical Model Specification\n\nThe [Mathematical Model Specification](https://hackmd.io/@CADLabs/ryLrPm2T_) articulates the relevant system dynamics as a state-space representation, the mathematical modelling paradigm underlying the cadCAD simulation library. It can be understood as a minimum viable formalism necessary to enable solid cadCAD modelling.\n\n### Differential Model Specification\n\nThe [Differential Model Specification](https://hackmd.io/@CADLabs/HyENPQ36u) depicts the model's overall structure across System States, System Inputs, System Parameters, State Update Logic and System Metrics.\n\n## Environment Setup\n\n1. Clone or download the Git repository: `git clone https://github.com/CADLabs/ethereum-model` or using GitHub Desktop\n2. If completing the cadCAD Edu Masterclass MOOC, find and check out the latest [\"Masterclass \ud83c\udf93\" release version](https://github.com/CADLabs/ethereum-economic-model/releases): e.g. `git checkout v.1.1.7`\n3. Set up your development environment using one of the following three options:\n\n### Option 1: Anaconda Development Environment\n\nThis option guides you through setting up a cross-platform, beginner-friendly (yet more than capable enough for the advanced user) development environment using Anaconda to install Python 3 and Jupyter. There is also a video that accompanies this option and walks through all the steps: [Model Quick-Start Guide](https://www.cadcad.education/course/masterclass-ethereum)\n\n1. Download [Anaconda](https://www.anaconda.com/products/individual)\n2. Use Anaconda to install Python 3\n3. Set up a virtual environment from within Anaconda\n4. Install Jupyter Notebook within the virtual environment\n5. Launch Jupyter Notebook and open the [environment_setup.ipynb](environment_setup.ipynb) notebook in the root of the project repo\n6. Follow and execute all notebook cells to install and check your Python dependencies\n\n### Option 2: Custom Development Environment\n\nThis option guides you through how to set up a custom development environment using Python 3 and Jupyter.\n\nPlease note the following prerequisites before getting started:\n* Python: tested with versions 3.7, 3.8, 3.9\n* NodeJS might be needed if using Plotly with Jupyter Lab (Plotly works out the box when using the Anaconda/Conda package manager with Jupyter Lab or Jupyter Notebook)\n\nFirst, set up a Python 3 [virtualenv](https://docs.python.org/3/library/venv.html) development environment (or use the equivalent Anaconda step):\n```bash\n# Create a virtual environment using Python 3 venv module\npython3 -m venv venv\n# Activate virtual environment\nsource venv/bin/activate\n```\n\nMake sure to activate the virtual environment before each of the following steps.\n\nSecondly, install the Python 3 dependencies using [Pip](https://packaging.python.org/tutorials/installing-packages/), from the [requirements.txt](requirements.txt) file within your new virtual environment:\n```bash\n# Install Python 3 dependencies inside virtual environment\npip install -r requirements.txt\n```\n\nTo create a new Jupyter Kernel specifically for this environment, execute the following command:\n```bash\npython3 -m ipykernel install --user --name python-cadlabs-eth-model --display-name \"Python (CADLabs Ethereum Economic Model)\"\n```\n\nYou'll then be able to select the kernel with display name `Python (CADLabs Ethereum Economic Model)` to use for your notebook from within Jupyter.\n\nTo start Jupyter Notebook or Lab (see notes about issues with [using Plotly with Jupyter Lab](#Known-Issues)):\n```bash\njupyter notebook\n# Or:\njupyter lab\n```\n\nFor more advanced Unix/macOS users, a [Makefile](Makefile) is also included for convenience that simply executes all the setup steps. For example, to setup your environment and start Jupyter Lab:\n```bash\n# Setup environment\nmake setup\n# Start Jupyter Lab\nmake start-lab\n```\n\n### Option 3: Docker Development Environment\n\nAlternatively, you can set up your development environment using the pre-built Docker image with all the dependencies you need: [CADLabs Jupyter Lab Environment](https://github.com/CADLabs/jupyter-lab-environment)\n\n### Known Issues\n\n#### Plotly doesn't display in Jupyter Lab\n\nTo install and use Plotly with Jupyter Lab, you might need NodeJS installed to build Node dependencies, unless you're using the Anaconda/Conda package manager to manage your environment. Alternatively, use Jupyter Notebook which works out the box with Plotly.\n\nSee https://plotly.com/python/getting-started/\n\nYou might need to install the following \"lab extension\": \n```bash\njupyter labextension install jupyterlab-plotly@4.14.3\n```\n\n#### Windows Issues\n\nIf you receive the following error and you use Anaconda, try: `conda install -c anaconda pywin32`\n> DLL load failed while importing win32api: The specified procedure could not be found.\n\n## Simulation Experiments\n\nThe [experiments/](experiments/) directory contains modules for configuring and executing simulation experiments, as well as performing post-processing of the results.\n\nThe [experiments/notebooks/](experiments/notebooks/) directory contains initial validator-level and network-level experiment notebooks and analyses. These notebooks and analyses do not aim to comprehensively illuminate the Ethereum protocol, but rather to suggest insights into a few salient questions the Ethereum community has been discussing, and to serve as inspiration for researchers building out their own, customized analyses and structural model extensions.\n\nThe [Experiment README notebook](experiments/notebooks/0_README.ipynb) contains an overview of how to execute existing experiment notebooks, and how to configure and execute new ones.\n\n#### Notebook 1. Model Validation\n\nThe purpose of this notebook is to recreate selected simulations from the widely acknowledged Hoban/Borgers Ethereum 2.0 Economic Model using the CADLabs model, and to compare the results. We suggest that the CADLabs model has a high degree of validity.\n\n#### Notebook 2. Validator Revenue and Profit Yields (Validator-level Analysis)\n\nThe purpose of this notebook is to explore the returns validators can expect from staking in the Ethereum protocol across different time horizons, adoption scenarios, ETH price scenarios and validator environments.\n\n* Analysis 1: Revenue and Profit Yields Over Time\n* Analysis 2: Revenue and Profit Yields Over ETH Staked\n* Analysis 3: Revenue and Profit Yields Over ETH Price\n* Analysis 4: Profit Yields Over ETH Staked vs. ETH Price (Yield Surface)\n* Analysis 5: Profit Yields By Validator Environment Over Time\n\n#### Notebook 3. Network Issuance and Inflation Rate (Network-level Analysis)\n\nThe purpose of this notebook is to explore ETH issuance and the resulting annualized inflation rate across different time horizons and scenarios.\n\n* Analysis: Inflation Rate and ETH Supply Over Time\n\n## Model Extension Roadmap\n\nThe modular nature of the model makes structural and experiment-level extensions straightforward. The [Model Extension Roadmap](ROADMAP.md) provides some inspiration.\n\n## Tests\n\nWe use Pytest to test the `model` module code, as well as the notebooks.\n\nTo execute the Pytest tests:\n```bash\nsource venv/bin/activate\npython3 -m pytest tests\n```\n\nTo run the full GitHub Actions CI Workflow (see [.github/workflows](.github/workflows)):\n```bash\nsource venv/bin/activate\nmake test\n```\n\n## Change Log\n\nSee [CHANGELOG.md](CHANGELOG.md) for notable changes and versions.\n\n## Acknowledgements\n\n* [Ethereum Ecosystem Support Program](https://esp.ethereum.foundation/en/) for sponsoring this work.\n* Barnab\u00e9 Monnot and the Ethereum Robust Incentives Group for the invaluable guidance.\n* Lakshman Sankar and Danny Ryan for milestone reviews.\n* Tanner Hoban and Thomas Borgers for their extensive work on [Ethereum 2.0 Economic Review. July 16, 2020. \"An Analysis of Ethereum\u2019s Proof of Stake Incentive Model](https://drive.google.com/file/d/1pwt-EdnjhDLc_Mi2ydHus0_Cm14rs1Aq/view), which inspired many design decisions and assumptions we adopted, and the generous time spent on the phone with us. \n* Other notable Ethereum Models (list not comprehensive):\n  * Barnab\u00e9 Monnot's **BeaconRunner** model: https://github.com/barnabemonnot/beaconrunner\n  * Justin Drake's **Modelling Ultrasound Money**: https://www.pscp.tv/w/1LyxBdyOdMbGN?s=09, https://docs.google.com/spreadsheets/d/1ZN444__qkPWPjMJQ_t6FfqbhllkWNhHF-06ivRF73nQ/edit#gid=0, https://docs.google.com/spreadsheets/d/1TsrdbdusUop4NJbvjGBbOWTUwYH-Jgg1QBkQ5CtY_-k/edit#gid=0, https://docs.google.com/spreadsheets/d/1FslqTnECKvi7_l4x6lbyRhNtzW9f6CVEzwDf04zprfA/edit#gid=0  \n  * Pintail's **Beacon Chain Validator Rewards** model: https://pintail.xyz/posts/beacon-chain-validator-rewards/\n  * Flashbots **Eth2 Research** model - \"Assessing the nature and impact of MEV in eth2.\": https://github.com/flashbots/eth2-research\n\n## Contributors \u2728\n\nThanks goes to these wonderful contributors (see [emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/AntoineRondelet\"><img src=\"https://avatars.githubusercontent.com/u/17513145?v=4?s=100\" width=\"100px;\" alt=\"Antoine Rondelet\"/><br /><sub><b>Antoine Rondelet</b></sub></a><br /><a href=\"https://github.com/CADLabs/ethereum-economic-model/pulls?q=is%3Apr+reviewed-by%3AAntoineRondelet\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://barnabemonnot.com\"><img src=\"https://avatars.githubusercontent.com/u/4910325?v=4?s=100\" width=\"100px;\" alt=\"Barnab\u00e9 Monnot\"/><br /><sub><b>Barnab\u00e9 Monnot</b></sub></a><br /><a href=\"https://github.com/CADLabs/ethereum-economic-model/commits?author=barnabemonnot\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/pulls?q=is%3Apr+reviewed-by%3Abarnabemonnot\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"#ideas-barnabemonnot\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://bitsofether.com\"><img src=\"https://avatars.githubusercontent.com/u/13078998?v=4?s=100\" width=\"100px;\" alt=\"Benjamin Scholtz\"/><br /><sub><b>Benjamin Scholtz</b></sub></a><br /><a href=\"https://github.com/CADLabs/ethereum-economic-model/commits?author=BenSchZA\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#infra-BenSchZA\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">\ud83d\ude87</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/pulls?q=is%3Apr+reviewed-by%3ABenSchZA\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/commits?author=BenSchZA\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/issues?q=author%3ABenSchZA\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"#ideas-BenSchZA\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://danlessa.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/15021144?v=4?s=100\" width=\"100px;\" alt=\"Danillo Lessa Bernardineli\"/><br /><sub><b>Danillo Lessa Bernardineli</b></sub></a><br /><a href=\"https://github.com/CADLabs/ethereum-economic-model/pulls?q=is%3Apr+reviewed-by%3Adanlessa\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"#ideas-danlessa\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/JGBSci\"><img src=\"https://avatars.githubusercontent.com/u/35999312?v=4?s=100\" width=\"100px;\" alt=\"JGBSci\"/><br /><sub><b>JGBSci</b></sub></a><br /><a href=\"https://github.com/CADLabs/ethereum-economic-model/commits?author=JGBSci\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/pulls?q=is%3Apr+reviewed-by%3AJGBSci\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/commits?author=JGBSci\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/issues?q=author%3AJGBSci\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"#ideas-JGBSci\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://joranhonig.nl\"><img src=\"https://avatars.githubusercontent.com/u/8710366?v=4?s=100\" width=\"100px;\" alt=\"JoranHonig\"/><br /><sub><b>JoranHonig</b></sub></a><br /><a href=\"https://github.com/CADLabs/ethereum-economic-model/pulls?q=is%3Apr+reviewed-by%3AJoranHonig\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/rogervs\"><img src=\"https://avatars.githubusercontent.com/u/4959125?v=4?s=100\" width=\"100px;\" alt=\"RogerVs\"/><br /><sub><b>RogerVs</b></sub></a><br /><a href=\"https://github.com/CADLabs/ethereum-economic-model/commits?author=rogervs\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#infra-rogervs\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">\ud83d\ude87</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/pulls?q=is%3Apr+reviewed-by%3Arogervs\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/commits?author=rogervs\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/issues?q=author%3Arogervs\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"#ideas-rogervs\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nardleram\"><img src=\"https://avatars.githubusercontent.com/u/18208637?v=4?s=100\" width=\"100px;\" alt=\"Toby Russell\"/><br /><sub><b>Toby Russell</b></sub></a><br /><a href=\"#content-nardleram\" title=\"Content\">\ud83d\udd8b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://marthendalnunes.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/18421017?v=4?s=100\" width=\"100px;\" alt=\"Vitor Marthendal Nunes\"/><br /><sub><b>Vitor Marthendal Nunes</b></sub></a><br /><a href=\"https://github.com/CADLabs/ethereum-economic-model/commits?author=marthendalnunes\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#infra-marthendalnunes\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">\ud83d\ude87</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/pulls?q=is%3Apr+reviewed-by%3Amarthendalnunes\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/commits?author=marthendalnunes\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/CADLabs/ethereum-economic-model/issues?q=author%3Amarthendalnunes\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"#ideas-marthendalnunes\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/carlwafe\"><img src=\"https://avatars.githubusercontent.com/u/87176407?v=4?s=100\" width=\"100px;\" alt=\"carlwafe\"/><br /><sub><b>carlwafe</b></sub></a><br /><a href=\"https://github.com/CADLabs/ethereum-economic-model/pulls?q=is%3Apr+reviewed-by%3Acarlwafe\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/casparschwa\"><img src=\"https://avatars.githubusercontent.com/u/31305984?v=4?s=100\" width=\"100px;\" alt=\"casparschwa\"/><br /><sub><b>casparschwa</b></sub></a><br /><a href=\"https://github.com/CADLabs/ethereum-economic-model/pulls?q=is%3Apr+reviewed-by%3Acasparschwa\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/uta0x89\"><img src=\"https://avatars.githubusercontent.com/u/122957026?v=4?s=100\" width=\"100px;\" alt=\"uta\"/><br /><sub><b>uta</b></sub></a><br /><a href=\"https://github.com/CADLabs/ethereum-economic-model/commits?author=uta0x89\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#maintenance-uta0x89\" title=\"Maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://clayming.space\"><img src=\"https://avatars.githubusercontent.com/u/3201174?v=4?s=100\" width=\"100px;\" alt=\"witwiki\"/><br /><sub><b>witwiki</b></sub></a><br /><a href=\"https://github.com/CADLabs/ethereum-economic-model/pulls?q=is%3Apr+reviewed-by%3Awitwiki\" title=\"Reviewed Pull Requests\">\ud83d\udc40</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!\n\n## License\n\nThe code repository `CADLabs/ethereum-economic-model` is licensed under the GNU General Public License v3.0.\n\nPermissions of this strong copyleft license are conditioned on making available complete source code of licensed works and modifications, which include larger works using a licensed work under the same license. Copyright and license notices must be preserved. Contributors provide an express grant of patent rights.\n\nIf you'd like to cite this code and/or research, we suggest the following format:\n\n> CADLabs, Ethereum Economic Model, (2021), GitHub repository, https://github.com/CADLabs/ethereum-economic-model\n\n```latex\n@misc{CADLabs2021,\n  author = {CADLabs},\n  title = {Ethereum Economic Model},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/CADLabs/ethereum-economic-model}},\n  version = {v1.1.7}\n}\n```\n", "release_dates": []}, {"name": "pos-contracts", "description": null, "language": "JavaScript", "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "readme": "# Matic contracts\n\n![Build Status](https://github.com/maticnetwork/contracts/workflows/CI/badge.svg)\n\nEthereum smart contracts that power the [Matic Network](https://polygon.technology/polygon-pos).\n\n### Install dependencies with\n\n```\nnpm install\n```\n\n### Setup\n\n```\npre-commit install\n```\n\n### Compile\n\n```\nnpm run template:process -- --bor-chain-id 15001\n```\n\nbor-chain-id should be:  \n**local: 15001**  \nMainnet = 137  \nTestnetV4 (Mumbai) = 80001\n\n### Main chain and side chain\n\n- Main chain\n\nAll tests are run against a fork of mainnet using Hardhat's forking functionality. No need to run any local chain!\n\n- Start Matic side chain. Requires docker.\n\n```\nnpm run bor:simulate\n```\n\n- Stop with\n\n```\nnpm run bor:stop\n```\n\n- If you want a clean chain, this also deletes your /data folder containing the chain state.\n\n```\nnpm run bor:clean\n```\n\n### Run tests\n\nRun Hardhat test\n\n```\nnpm test:hardhat\n```\n\nRun Foundry test\n\n```\nnpm test:foundry\n```\n\n### Coverage\n\nRun coverage with\n\n```\nnpm run coverage\n```\n", "release_dates": []}, {"name": "rust", "description": "Rust work for nomad actors", "language": null, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "<img src=\".github/images/Logo-White.svg\" alt=\"Nomad logo\" style=\"width: 100%; background: black;\"/>\n\n## Nomad\n\nNomad is a cross-chain communication standard that supports passing messages between blockchains easily and inexpensively. Like [IBC](https://ibcprotocol.org) light clients and similar systems, Nomad establishes message-passing channels between chains. Once a channel is established, any application on that chain can use it to send messages to others chains.\n\nNomad is an implementation and extension of the [Optics protocol](https://medium.com/celoorg/announcing-optics-a-gas-efficient-interoperability-standard-for-cross-chain-communication-e597163b2) (**OPT**imistic **I**nterchain **C**ommunication), originally developed at Celo.\n\nCompared to light clients, Nomad has weaker security guarantees and a longer latency period. However, these tradeoffs allow Nomad to be implemented on any smart contract chain without expensive light client development. Unlike light clients, Nomad does not use gas verifying remote chain block headers.\n\nNomad is designed to prioritize:\n\n- Operating costs: No gas-intensive header verification or state management\n- Implementation speed and cost: Uses simple smart contracts without complex cryptography\n- Ease of use: Straightforward interface for maintaining xApp connections\n- Security: Relies on a 1/n honest watcher assumption for security\n\nYou can read more about Nomad's architecture at our [main documentation site](https://docs.nomad.xyz).\n\n## Nomad Rust Repository\n\nNomad's off-chain systems are written in Rust for speed, safety and reliability. (Nomad's on-chain systems are written in Solidity and are available [here](https://github.com/nomad-xyz/monorepo).)\n\n### Rust Setup\n\n- Install `rustup` from [here](https://rustup.rs/) and run it\n\nNote: You should be running at least version `1.52.1` of the rustc compiler. Check it with `rustup --version`\n\n```\n$ rustup --version\nrustup 1.24.2 (755e2b07e 2021-05-12)\ninfo: This is the version for the rustup toolchain manager, not the rustc compiler.\ninfo: The currently active `rustc` version is `rustc 1.52.1 (9bc8c42bb 2021-05-09)`\n```\n\nRust uses `cargo` for package management, building, testing and other essential tasks.\n\nFor Ethereum and Celo connections we use\n[ethers-rs](https://github.com/gakonst/ethers-rs). Please see the docs\n[here](https://docs.rs/ethers/0.2.0/ethers/).\n\nNomad uses the tokio async runtime environment. Please see the docs\n[here](https://docs.rs/tokio/1.1.0/tokio/).\n\n### Running the Test Suite\n\n- `cargo test --workspace --all-features`\n\nThis will run the full suite of tests for this repository.\n\n### Generate Documentation\n\n- `cargo doc --open`\n\nThis will generate this repos documentation and open it in a web browser.\n\n### Agent Architecture\n\nThe off-chain portion of Nomad is a set of agents each with a specific role:\n\n- `updater`\n  - Signs update attestations and submits them to the home chain\n- `watcher`\n  - Observes the home chain\n  - Observes one or more replica chains\n  - Check for fraud\n  - Submits fraud to the home chain\n  - If configured, issues emergency stop transactions\n- `relayer`\n  - Relays signed updates from the home chain to the replicas\n- `processor`\n  - Retrieves Merkle leaves from home chain\n  - Observes one or more replica chains\n  - Generates proofs for passed messages\n  - Submits messages with proofs to replica chains\n\n### Repository Layout\n\n- `nomad-base`\n  - A VM-agnostic toolkit for building agents\n    - Common agent structs\n    - Agent traits\n    - Agent settings\n    - NomadDB (RocksDB)\n    - Concrete contract objects (for calling contracts)\n    - VM-agnostic contract sync\n    - Common metrics\n- `nomad-core`\n  - Contains implementations of core primitives\n    - Core primitives\n    - Core data types\n    - Contract and chain traits\n- `nomad-types`\n  - Common types used throughout the stack\n- `chains`\n  - A collection of crates for interacting with different VMs\n    - Ethereum\n    - More coming...\n- `accumulator`\n  - Contains Merkle tree implementations\n- `agents`\n  - A collection of VM-agnostic agent implementations\n- `configuration`\n  - An interface for persisting and accessing config data\n    - JSON config files (for development, staging, production)\n    - An interface for retrieving agent and system config\n    - An interface for retrieving agent secrets\n\n## Contributing\n\nAll contributions, big and small, are welcome. All contributions require signature verification and contributions that touch code will have to pass linting and formatting checks as well as tests.\n\n### Commit signature verification\n\nCommits (and tags) for this repository require signature verification. You can learn about signing commits [here](https://docs.github.com/en/enterprise-server@3.3/authentication/managing-commit-signature-verification/signing-commits).\n\nAfter signing is set up, commits can be signed with the `-S` flag.\n\n- `git commit -S -m \"your commit message\"`\n\n### Testing, Linting and Formatting\n\nIf your commits have changed code, please ensure the following have been run and pass before submitting a PR:\n\n```\ncargo check --workspace\ncargo test --workspace --all-features\ncargo fmt --all\ncargo clippy --workspace --all-features -- -D warnings\n```\n\n## Release Process\n\n### Overview\n\nWe make releases within the `rust` repository specific to the crate(s) that will be consumed. This includes the below crates/groups of crates.\n\n- agents@X.Y.Z\n- configuration@X.Y.Z\n- accumulator@X.Y.Z\n\nWe follow [Semantic Versioning](https://semver.org/), where breaking changes constitute changes that break agent configuration compatibility.\n\nReleases are managed on GitHub [here](https://github.com/nomad-xyz/rust/releases).\n\n### Aggregating the Changelog\n\n- Output a patch file by running `git diff <tag of last release> --no-prefix --output <location to output patch txt file> -- */CHANGELOG.md */*/CHANGELOG.md`\n- Organize and format patch file into release notes (see [template](./RELEASE-TEMPLATE.md))\n\n### Bumping Versions\n\n- Bump package versions in all relevant `Cargo.toml` files\n- Bump the package versions in all relevant `CHANGELOG.md` files\n- E.g. for an `agents` release, this would entail bumping all agents in `rust/agents`\n- Make/merge a PR declaring the new version you are releasing (e.g. \"Bumping agents for release agents@1.0.1\")\n\n### Making a New Release\n\n- Tag newly-merged PR by running `git tag -s <package(s)-release-name>@<new-package-version>`, using your compiled changelog as the tag message\n- Push tags by running `git push --tags`\n- Visit the [releases page](https://github.com/nomad-xyz/rust/releases) for the `rust` repo\n- Draft a new release using the newly published tag\n- Publish release with your included release notes\n\n## Advanced Usage\n\n### Building Agent Images\n\nThere exists a docker build for the agent binaries. These docker images are used for deploying the agents in a production environment.\n\n```\n./build.sh <image_tag>\n./release.sh <image_tag>\n```\n\n### Adding a New Agent\n\n- Run `cargo new $AGENT_NAME`\n- Add the new directory name to the workspace `Cargo.toml`\n- Add dependencies to the new directory's `Cargo.toml`\n  - Copy most of the dependencies from `nomad-base`\n- Create a new module in `src/$AGENT_NAME.rs`\n  - Add a new struct\n  - Implement `nomad_base::NomadAgent` for your struct\n  - Your `run` function is the business logic of your agent\n- Create a new settings module `src/settings.rs`\n  - Reuse the `Settings` objects from `nomad_base::settings`\n  - Add your own new settings\n  - Make sure to read the docs :)\n- In `$AGENT_NAME/src/main.rs`:\n  - Add `mod` declarations for your agent and settings modules\n  - Create `main` and `setup` functions\n  - Use the implementation in `agents/kathy/src/main.rs` as a guide\n- Add required config to `configuration/configs/*` for the agent\n\n## Miscellaneous\n\n### Useful `cargo` Extensions\n\n- `tree`\n  - Show the dependency tree. Allows searching for specific packages\n  - Install: `cargo install cargo-tree`\n  - Invoke: `cargo tree`\n- `clippy`\n  - Search the codebase for a large number of lints and bad patterns\n  - Install: `rustup component add clippy`\n  - Invoke: `cargo clippy`\n- `expand`\n  - Expand macros and procedural macros. Show the code generated by the\n    preprocessor\n  - Useful for debugging `#[macros]` and `macros!()`\n  - Install: `cargo install cargo-expand`\n  - Invoke `cargo expand path::to::module`\n", "release_dates": []}, {"name": "staking-contracts", "description": "The smart contracts used in the IBFT PoS implementation", "language": "TypeScript", "license": null, "readme": "# staking-contracts\n\nSmart contracts for IBFT PoS\n\n## How to start\n\n```shell\n$ git clone https://github.com/0xPolygon/staking-contracts.git\n$ cd staking-contracts\n$ npm i\n```\n\n### Build contracts\n\n```shell\n$ npm run build\n```\n\n### Run unit tests\n\n```shell\n$ npm run test\n```\n\n### Deploy contract to Polygon Edge\n\n```shell\n$ npm run deploy\n```\n\n### Stake balance to contract\n\nPlease make sure required values are set in .env to use this command\n\n```shell\n$ npm run stake\n```\n\n### Unstake from contract\n\n```shell\n$ npm run unstake\n```\n\n### Check current total staked amount and validators in contract\n\n```shell\n$ npm run info\n```\n", "release_dates": []}, {"name": "staking-hub", "description": null, "language": "Solidity", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "## Template Repo (Foundry)\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![CI Status](../../actions/workflows/test.yaml/badge.svg)](../../actions)\n\nThis template repo is a quick and easy way to get started with a new Solidity project. It comes with a number of features that are useful for developing and deploying smart contracts. Such as:\n\n- Pre-commit hooks for formatting, auto generated documentation, and more\n- Various libraries with useful contracts (OpenZeppelin, Solady) and libraries (Deployment log generation, storage checks, deployer templates)\n\n#### Table of Contents\n\n- [Setup](#setup)\n- [Deployment](#deployment)\n- [Docs](#docs)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Setup\n\nFollow these steps to set up your local environment:\n\n- [Install foundry](https://book.getfoundry.sh/getting-started/installation)\n- Install dependencies: `forge install`\n- Build contracts: `forge build`\n- Test contracts: `forge test`\n\nIf you intend to develop on this repo, follow the steps outlined in [CONTRIBUTING.md](CONTRIBUTING.md#install).\n\n## Deployment\n\nThis repo utilizes versioned deployments. For more information on how to use forge scripts within the repo, check [here](CONTRIBUTING.md#deployment).\n\nSmart contracts are deployed or upgraded using the following command:\n\n```shell\nforge script script/Deploy.s.sol --broadcast --rpc-url <rpc_url> --verify\n```\n\n## Docs\n\nThe documentation and architecture diagrams for the contracts within this repo can be found [here](docs/).\nDetailed documentation generated from the NatSpec documentation of the contracts can be found [here](docs/autogen/src/src/).\nWhen exploring the contracts within this repository, it is recommended to start with the interfaces first and then move on to the implementation as outlined [here](CONTRIBUTING.md#natspec--comments)\n\n## Contributing\n\nIf you want to contribute to this project, please check [CONTRIBUTING.md](CONTRIBUTING.md) first.\n\n## License\n\n\u200b\nLicensed under either of\n\u200b\n\n- Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)\n- MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n  \u200b\n\nat your option.\n\nUnless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.\n\n---\n\n\u00a9 2023 PT Services DMCC\n", "release_dates": []}, {"name": "storage-delta", "description": "\u27c1 Audit storage layout changes during contract upgrades.", "language": "JavaScript", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "readme": "# \u27c1 Storage Delta\n\nStorage Delta is a tool for auditing storage layout changes during contract upgrades.\n\n![Demo](./demo.gif)\n\n## Install\n\nPowered by [Foundry](https://github.com/foundry-rs/foundry).\n\n```bash\nforge install 0xPolygon/storage-delta\n```\n\n## Run\n\nStorage Delta analyzes the entire contract suite against any previous version.\n\n```bash\nbash lib/storage-delta/run.sh <COMMIT_OR_TAG>\n```\n\n`./storage_delta` will be generated if there are findings. Open `OLD` and `NEW` files side by side for the best experience.\n\n## Detectors\n\n|     | Description                                |\n| --- | ------------------------------------------ |\n| \ud83c\udf31  | [New](#new)                                |\n| \ud83c\udff4  | [Problematic](#problematic)                |\n| \ud83c\udff3\ufe0f  | [Moved](#moved)                            |\n| \ud83c\udfc1  | [Moved & problematic](#moved--problematic) |\n| \ud83e\udea6  | [Removed](#removed)                        |\n|     | [Dirty bytes](#dirty-bytes)                |\n\n### New\n\nWhen a variable with a unique name and type is added.\n\n```solidity\n    uint256 a\n```\n\n```solidity\n    uint256 a\n\ud83c\udf31  bool b\n```\n\n### Problematic\n\nWhen a new variable is added, but conflicts with the existing storage.\n\n```solidity\n    uint256 a\n```\n\n```solidity\n\ud83c\udff4  bool b\n```\n\n### Moved\n\nWhen an existing variable is moved.\n\n```solidity\n    uint256 a\n    ...\n```\n\n```solidity\n    ...\n\ud83c\udff3\ufe0f  uint256 a\n```\n\n### Moved & problematic\n\nWhen an existing variable is moved and conflicts with the existing storage.\n\n```solidity\n    uint256 a\n    bool b\n```\n\n```solidity\n\ud83c\udfc1  bool b\n\ud83c\udfc1  uint256 a\n```\n\n### Removed\n\nWhen a variable no longer exists.\n\n```solidity\n    uint256 a\n    bool b\n```\n\n```solidity\n    uint256 a\n\ud83e\udea6\n```\n\n### Dirty bytes\n\nWhen the storage is not clean.\n\n```solidity\n    uint256 a\n```\n\n```solidity\n\ud83c\udff4  uint128 a\n    16 dirty bytes\n```\n\n## Configuration\n\n| Option   | Values | Description                                        |\n| -------- | ------ | -------------------------------------------------- |\n| `--omit` | `new`  | Do not generate a report with only those findings. |\n\n## Requirements\n\nFiles should be named after the contract they hold. `Example` \u2192 `Example.sol`\n\nThe script utilizes Node.js to run. We recommend the node version defined in the `.nvmrc` file.\n\n## License\n\n\u200b\nLicensed under either of\n\u200b\n\n- Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)\n- MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n  \u200b\n\nat your option.\n\nUnless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.\n\n---\n\n\u00a9 2023 PT Services DMCC\n", "release_dates": ["2024-01-11T11:48:29Z", "2023-12-11T13:02:34Z", "2023-12-09T23:24:23Z", "2023-12-09T15:34:36Z"]}, {"name": "zkevm-wrapper", "description": null, "language": "Solidity", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "readme": "# ZkEVM Wrapper [![test](https://github.com/0xPolygon/zkevm-wrapper/actions/workflows/test.yml/badge.svg)](https://github.com/0xPolygon/zkevm-wrapper/actions/workflows/test.yml)\nThis repository contains the ZkEVM wrapper contracts that allows any user (including smart contracts) to transfer ETH and\nERC20 tokens to ZkEVM in a single transaction.\n\n## Features\n* Transfer ETH and ERC20 tokens to ZkEVM using traditional ERC-20 approval flow\n* Transfer ETH and ERC20 tokens to ZkEVM using EIP-2612 permit signatures\n* Transfer ETH and ERC20 tokens to ZkEVM using DAI-style permit signatures\n> \u26a0\ufe0f **_NOTE:_**  The wrapper does not support fee-on-transfer tokens and will automatically revert if the token has a transfer fee.\n\n## Usage\n* Make an approval tx to the wrapper contract with the amount of tokens you want to transfer to ZkEVM\n*or*\n* Sign a EIP-2612 permit signature with the amount of tokens and appropriate deadline you want to transfer to ZkEVM\n*or*\n* Sign a Dai-style permit signature with the amount of tokens and appropriate expiry and nonce you want to transfer to ZkEVM\n\nThen,\n* Call the appropriate `deposit()` function on the wrapper contract with the ether, amount of ERC-20 tokens (and signature, if any) that you want to transfer to ZkEVM\n\n## Deployment addresses\n| Network | Address | Etherscan link |\n| ------- | ------- | -------------- |\n| Ethereum mainnet | `0x047E0b64743071b897A6177F1796E98b4C3f344E` | https://etherscan.io/address/0x047e0b64743071b897a6177f1796e98b4c3f344e |\n| Testnet (Goerli) | `0xDb5328c50B166545d1e830BB509944d4B98CBb23` | https://goerli.etherscan.io/address/0xDb5328c50B166545d1e830BB509944d4B98CBb23 |\n\n## Development\nThis repository makes use of [Foundry](https://github.com/foundry-rs/foundry) for compilation and dependency management. You can find the installation instructions [here](https://book.getfoundry.sh/getting-started/installation).\n\nTo build the project, run:\n```bash\nforge build\n```\nTo run the tests, run:\n```bash\nforge test\n```\n## Licensing\nAll source code is released under the MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT).\n\nUnless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the MIT license, shall be licensed as above, without any additional terms or conditions.\n", "release_dates": []}]