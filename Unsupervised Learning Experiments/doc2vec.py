# -*- coding: utf-8 -*-
"""Doc2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J59lezONFqH7Ahp3uZXCmR-n8UfOlPzQ

We trained Gensim Doc2Vec using ercs and eip documents to generate the model for our classification objective. Then we used this model to generate embeddings.
"""

! pip install spacy
! python -m spacy download en_core_web_sm
! pip install gensim
! pip install scikit-learn
! pip install transformers torch sklearn

import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
import smart_open
from gensim.parsing.preprocessing import strip_punctuation, remove_stopwords

import nltk
nltk.download('punkt')

# build corpus using ercs & eip documents ("ercs')
import os
corpus = []
folder_path = '/content/drive/MyDrive/ercs'
for filename in os.listdir(folder_path):
  file_path = os.path.join(folder_path, filename)
  with open(file_path, 'r') as f:
    corpus.append(remove_stopwords(strip_punctuation(f.read())))

documents = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[i]) for i, doc in enumerate(corpus)]

model = Doc2Vec(documents, vector_size=50, window=4, min_count=1, workers=4, epochs=100)

# use built model for embedding repo readme
import spacy

nlp = spacy.load('en_core_web_sm')

import json

json_file_path = '/content/ethereum_repos_extracted_text_cleaned.json'

with open(json_file_path, 'r') as file:
  json_data = json.load(file)


from gensim.models.doc2vec import Doc2Vec, TaggedDocument

l = list()
for repo in json_data:

  text = repo["name"]+(repo["desc"] or " ")+(repo["readme"] or " ")
  text = remove_stopwords(strip_punctuation(text))
  doc = nlp(text)
  tokens = [token.text for token in doc]
  vector = model.infer_vector(tokens)

  l.append(vector)

import pickle
with open("vectors", "wb") as f:
  pickle.dump(l, f)

with open("vectors", "rb") as f:
   l = pickle.load(f)

X = np.array(l)

kmeans = KMeans(n_clusters=5, random_state=0)

kmeans.fit(X)

labels = kmeans.labels_

centroids = kmeans.cluster_centers_

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
reduced_data = pca.fit_transform(X)
centroids_reduced = pca.transform(centroids)

plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap='viridis')
plt.scatter(centroids_reduced[:, 0], centroids_reduced[:, 1], c='red', s=100, marker='x')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA: High-dimensional Data Projection')
plt.show()